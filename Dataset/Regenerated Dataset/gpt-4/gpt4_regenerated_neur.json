{"wS23xAeKwSN": "The paper introduces PolarMix a novel point cloud augmentation technique designed to enhance the data efficiency and training performance of deep neural networks (DNNs) for tasks involving LiDAR point clouds. LiDAR sensors are pivotal for various perception tasks in autonomous navigation due to their ability to capture highfidelity geometric representations of the environment. However the effective and efficient exploitation of LiDAR data for network training is hindered by the laborious and timeconsuming process of collecting and annotating large datasets. PolarMix addresses this challenge by proposing a simple yet effective method for augmenting LiDAR scans through crossscan mixing strategies notably scenelevel swapping and instancelevel rotationpasting aligned with the unique data properties captured by LiDAR sensors such as partial visibility and density variation. Experimental evaluations across several benchmarks demonstrate PolarMixs robustness and versatility reporting consistent performance improvements in tasks like semantic segmentation and object detection and facilitating unsupervised domain adaptation with minimal computational overhead. Strengths: 1. Innovation and Relevance: The paper tackles a significant and relatively underexplored problem in the domain of 3D deep learning and LiDAR point cloud processing. It innovatively applies the principles of data augmentation widely used in 2D vision to LiDAR point clouds in a way that respects their unique properties. 2. Robust Evaluation: The evaluation is comprehensive covering a variety of network architectures perception tasks and datasets. Such thorough validation not only demonstrates the efficacy of PolarMix but also underscores its general applicability and the reproducibility of its results. 3. Ease of Integration: PolarMix is presented as a plugandplay technique that can be easily incorporated into existing 3D network architectures without needing significant adjustments encouraging broader adoption in the research community and possibly accelerating progress in related fields. Weaknesses: 1. Lack of Comparison with DomainSpecific Methods: While the paper positions PolarMix against general data augmentation techniques it falls short of directly comparing it with other augmentation strategies that are specifically designed for LiDAR point clouds or 3D data. 2. Limited Discussion on Computational Efficiency: Although the paper briefly mentions the negligible computational overhead of PolarMix a more detailed analysis of its computational requirements compared to baseline methods would provide deeper insights especially for applications in resourceconstrained environments. 3. Potential for Overfitting: There is a limited discussion on whether the augmented data generated by PolarMix could lead to overfitting especially in scenarios with extremely limited original datasets. Insights into how variability is ensured in the augmented data would be beneficial. Questions and Suggestions for the Authors: 1. How does PolarMix compare in computational efficiency and augmentation effectiveness with other 3Dspecific augmentation techniques if any have been proposed in the literature 2. The paper would benefit from a more detailed explanation or exploration of how PolarMix avoids potential overfitting issues especially considering that augmentation is being used to address data scarcity. 3. It would be interesting to see the application or potential adaptation of PolarMix for other types of 3D sensors or data representations such as structured light or stereo vision systems. Could the authors speculate on or discuss any adjustments that might be necessary for such applications Limitations and Societal Impact: The paper acknowledges the computational efficiency of PolarMix and provides its codebase for accessibility. However the implications of generating and using synthetic data in highstakes applications (e.g. autonomous driving) are not thoroughly explored. While data augmentation is a powerful tool for improving the robustness of machine learning models reliance on augmented data should be balanced with considerations of realworld variability and safety requirements. The authors are encouraged to discuss potential pitfalls or ethical considerations related to deploying models trained heavily on augmented data in safetycritical applications. Overall PolarMix represents a significant step forward in the domain of LiDAR point cloud processing with promising applications in facilitating more efficient and effective training of deep learning models for 3D perception tasks.", "l2CVt1ySC2Q": "The paper studied the excess capacity of deep neural networks in supervised classification tasks focusing on how this capacity can be constrained without significantly affecting empirical error. The authors extended and unified prior Rademacher complexity bounds to address modern network architectures including residual networks and convolutions thereby introducing novel bounds based on the Lipschitz constants of layers and a particular group norm distance of convolution weights from their initializations. They conducted extensive experiments across datasets of varying difficulty demonstrating significant excess capacity in neural networks that can be constrained without impacting performance suggesting potential for network compression beyond parameter pruning. The source code for their experiments is made available for further research.  Strengths 1. Theoretical Contributions: This paper successfully extends the theoretical framework around empirical Rademacher complexity to accommodate modern deep learning architectures. The introduction of novel singlelayer covering number bounds and the modular approach to multilayer architectures are significant contributions that provide a deeper understanding of the actual capacity needs of neural networks. 2. Empirical Validation: The thorough experimental setup across multiple benchmark datasets and the use of a standardized architecture under varying task difficulties strengthen the paper\u2019s claims. The experiments are welldesigned to assess the practical implications of the theoretical findings showcasing the potential for capacity constraint without loss of performance. 3. Practical Implications: The paper not only advances theoretical understanding but also has clear practical implications by demonstrating how networks can maintain performance at significantly reduced capacity levels. This compression potential is crucial for deploying models in resourceconstrained environments.  Weaknesses 1. Generalization to Other Architectures: While the paper focuses on residual networks and convolutions the generalization of the findings to a broader range of architectures is not directly addressed. Given the diversity of architectures used in practice insights into how the findings apply to other models would be valuable. 2. Analysis of Optimization Dynamics: The paper outlines a method for constraining model capacity during training but does not deeply explore how these constraints affect the optimization process itself. An analysis of how training dynamics and convergence are impacted by these constraints could provide further insights into the practicality of implementing such capacity reductions. 3. Discussion on the Role of Data: The studys focus is on model capacity with respect to model architecture. However an exploration of how data characteristics (such as dataset size dimensionality and complexity) interact with model capacity and the efficacy of the proposed constraints would be beneficial. Understanding whether certain data traits exacerbate or mitigate the observed excess capacity could inform more tailored compression strategies.  Questions and Suggestions for Improvement 1. Extension to Other Architectures: Can the authors elaborate on how their findings might extrapolate to architectures not covered in the study such as transformers or recurrent neural networks Additionally experimentation or theoretical analysis involving these architectures could strengthen the papers generality. 2. Impact on Optimization: It would be helpful if the authors provided more detail on how capacity constraints affect optimization particularly in terms of convergence rates and whether certain constraint combinations pose challenges. Expanding the experimental analysis to include these aspects could offer a more comprehensive view of capacity reductions practical implications. 3. Interplay with Data Characteristics: A deeper discussion on how data attributes influence the observed excess capacity and the effectiveness of constraints would be enlightening. Including experiments or simulations that manipulate dataset properties (e.g. size noise level) might reveal interesting dynamics between data and model capacity.  Limitations and Societal Impact The paper mentions source code availability which is commendable however the discussion on limitations primarily focuses on model architectures without considering broader implications. For instance the practicality of implementing the proposed constraints in various realworld applications remains unclear. Additionally while the potential for model compression is highlighted the paper does not discuss the potential negative impacts such as increased computational cost during training due to projection operations or the implications of reduced model capacity in critical applications. Suggestions for Improvement:  A section discussing the computational overhead introduced by capacity constraints and strategies to mitigate it would be valuable especially for applications in resourcelimited settings.  A more thorough consideration of the ethical and societal implications of deploying compressed models in highstakes domains would enhance the paper. Specifically discussing whether capacity reduction could affect model fairness or reliability would be pertinent. Overall this paper provides valuable theoretical and empirical insights into the excess capacity of deep neural networks presenting a solid basis for future work on model compression and efficiency.", "tlUnxtAmcJq": "Title: SO3KRATES: Efficiently Modeling Nonlocal Electronic Effects in Quantum Chemistry with Spherical Harmonic Coordinates 1) Summary and Contributions: The paper presents a novel approach to address the computational challenges in quantum chemistry especially in modeling nonlocal electronic effects in molecular and material simulations. It introduces the SO3KRATES model which combines a selfattention based messagepassing neural network (MPNN) with spherical harmonic coordinates (SPHCs) to separately process atomic features and higherorder geometric information. This separation enables the efficient capturing of nonlocal effects a significant challenge for existing methods. Key contributions include the adapted attention mechanism with spherical filters a reduction in parameter complexity resulting in speedups in both training and inference times and the demonstration of stateoftheart (SOTA) performance on popular benchmarks. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The integration of SPHCs to model nonlocal electronic effects is innovative and addresses a significant gap in current quantum chemistry simulations.  Efficiency: Demonstrated efficiency improvements both in terms of computational requirements (parameter reduction) and speedup (training and inference) are substantial.  Generalizability: The paper shows that SO3KRATES not only excels in specific tasks but also generalizes well across different benchmarks and molecular sizes including structures larger than those in the training dataset. Weaknesses:  Complexity and Usability: The complexity of the proposed method could be a barrier to wider adoption especially for researchers not familiar with the intricacies of SPHCs or the underlying mathematical framework.  Evaluation Against Competitors: While the paper provides a comparison with existing models a deeper analysis of how SO3KRATES performs in diverse realworld scenarios outside of benchmark datasets could further validate its applicability.  Discussion on Tradeoffs: The paper could benefit from a more detailed discussion on potential tradeoffs particularly regarding the chosen spherical neighborhoods and how they impact model performance and efficiency. 3) Questions and Suggestions for Improvement:  Can the authors provide insights on how SO3KRATES performs with datasets of varying complexity and sizes not covered in the benchmarks  A detailed comparison with recent advancements in the field could enhance the papers relevance and positioning. How does SO3KRATES compare to other novel methods not included in the current benchmarking  Suggestions for making the methodology more accessible to a broader audience could include providing opensource implementation details extensive documentation and illustrative examples of SO3KRATES in action beyond quantum chemistry. 4) Limitations and Potential Negative Societal Impact:  The authors have briefly discussed potential negative uses of their work such as in designing chemical weapons. However a more indepth analysis of ethical considerations including misuse and dualuse concerns could strengthen the paper.  Addressing limitations the current implementations reliance on computing spherical neighborhoods from all pairwise distances could be a bottleneck for larger molecules. Exploring efficient space partitioning schemes might be a valuable future direction.  The potential impact on computational resource demand while reduced compared to existing models could still be significant for extensive simulations and might not align with sustainable computing goals. Highlighting approaches to further mitigate this impact would be beneficial. Overall the paper presents a significant step forward in the efficient modeling of nonlocal electronic effects in quantum chemistry showcasing promising results that could pave the way for accelerated drug discovery and material design amidst a wellbalanced discussion on its limitations and potential societal impacts.", "nosngu5XwY9": "Review of \"Dynamic Inverse Reinforcement Learning for Characterizing Exploratory Behavior in Complex Environments\" 1. Summary The paper introduces a novel framework named Dynamic Inverse Reinforcement Learning (DIRL) which extends traditional Inverse Reinforcement Learning (IRL) approaches to account for timevarying intrinsic rewards a crucial aspect of animal decisionmaking not accommodated by existing IRL models. The authors propose modeling the timevarying reward function of an agent as a linear combination of spatial reward maps or \"goal maps\" modulated by timevarying weights. This approach allows for the characterization of complex behaviors in animals such as navigation and exploration by inferring the underlying dynamic reward functions from observed trajectories. To showcase their model the authors apply DIRL to both simulated environments and realworld datasets involving mice navigating through a labyrinth. The results reveal distinct reward functions for two cohorts of mice (waterrestricted and unrestricted) providing insights into their exploratory behavior. The comparison with static reward functionbased IRL frameworks demonstrates DIRL\u2019s superior performance in explaining observed animal behaviors underscoring the importance of accommodating timevarying rewards in models of animal decisionmaking. 2. Assessment Strengths:  DIRL addresses a significant gap in the application of IRL to neuroscience by incorporating timevariance in the reward functions which is prevalent in many realworld decisionmaking scenarios.  The framework is supported by rigorous theoretical underpinnings and an efficient inference method making it applicable to complex environments and large datasets.  The empirical validation of the model using both simulated data and realworld experiments with mice provides strong evidence of its utility in explaining animal behavior in complex settings.  The ability to recover interpretable reward functions and the demonstration of distinct behaviors (exploitative vs. exploratory) in the studied cohorts showcase DIRLs potential for broader neuroscience applications. Weaknesses:  While the paper discusses the computational efficiency of DIRL detailed benchmarks comparing its runtime performance against existing IRL methods would provide a clearer insight into its scalability and practical limitations.  The application of DIRL is demonstrated mainly in the context of animal behavior. Expanding its application to human decisionmaking tasks could further validate its versatility and effectiveness in different domains.  The discussion on the potential negative societal impact is somewhat limited. Given the increasing application of artificial intelligence in decisionmaking systems a more indepth analysis of how DIRLs findings could influence AI behavior privacy and ethical considerations would be beneficial. 3. Questions and Suggestions for the Authors  Could the authors elaborate on how DIRL could be adapted or extended to model decisionmaking behaviors in humans especially in tasks involving social interactions or economic decisions  It would be interesting to know the authors perspective on integrating DIRL with deep learning models especially in terms of learning goal maps from highdimensional data (e.g. images or videos) for tasks beyond navigation.  A technical point that could use further clarification is the choice of prior for the timevarying weights. How sensitive is the models performance to the choice of this prior and are there principled ways to select or tune this prior based on the specific application or dataset 4. Limitations and Societal Impact The authors have made a commendable effort in detailing the implications and applications of DIRL within neurological and behavioral sciences. However discussions around potential limitations particularly concerning the scalability to very highdimensional state spaces and the generalizability of the model to different species or contexts seem somewhat brief. An expanded section on potential negative societal impacts especially considering the broader use of reinforcement learning algorithms in society would also be beneficial. Suggestions for improvement include more detailed benchmarks for scalability considerations for extensions to more complex decisionmaking scenarios and a deeper exploration of ethical considerations particularly in the application of DIRLinspired algorithms in AI systems that may interact with humans or influence decisionmaking in societal contexts.", "uNYqDfPEDD8": " Peer Review of \"A Neural MixedSize Placement and Routing Pipeline for VLSI Design\"  Summary of the Paper This paper proposes a novel Reinforcement Learning (RL)based model and a conditional generative model to solve mixedsize macro placement and routing in the field of Very Large Scale Integration (VLSI) design. Unlike traditional heuristic solvers the proposed framework leverages RL for macro placement considerations and utilizes a oneshot conditional generative model for efficient routing marking a departure from stepbystep grid point level routing. By integrating these models into a unified neural network pipeline the authors claim to present the first pure neural approach for placement and routing without relying on conventional heuristic solvers. The approach demonstrates promising results in terms of costefficiency and competitive performance when evaluated against standard chip design benchmarks.  Assessment of Strengths and Weaknesses  Strengths 1. Innovative Approach: The paper introduces a pioneering RL and generative modelbased framework for VLSI design potentially setting a new direction in Electronic Design Automation (EDA) technology. 2. Comprehensive Methodology: The methodology is thorough combining mixedsize macro placement and oneshot generative routing which innovatively addresses the traditional placement and routing challenges in IC design. 3. Experimental Validation: The authors have conducted extensive experiments using standard benchmarks showcasing the effectiveness and efficiency of their proposed models compared to RLbased routing solvers. 4. Reproducibility and Openness: By making the source code publicly available the paper encourages reproducibility and further research in the area.  Weaknesses 1. Dependence on Classical Router Outputs for Training: The generative models training relies on routes generated by classical routers which may cap the models ability to outperform or discover new routing strategies beyond current benchmarks. 2. Limited Exploration of Limitations: While the paper discusses the innovative aspects extensively there is sparse discussion about the potential limitations or failure modes of the proposed models especially in edgecase scenarios or under constraints not covered by the benchmarks. 3. Potential Negative Societal Impact: The paper briefly mentions the possibility of AI in EDA leading to job losses but does not delve into ethical considerations nor does it propose ways to mitigate such impacts.  Questions and Suggestions for the Authors 1. Generalization Capability: How does the model perform on unforeseen VLSI designs or under constraints not represented in the training data It could be beneficial to test the models adaptability to various and possibly mixed design rule sets. 2. Comparison with Other AI Approaches: Apart from traditional heuristic solvers how does this neural approach compare with other AIbased methods if any exist in the literature Including such comparisons could enhance the papers value. 3. Limitation Discussion: The authors could enrich the paper by providing a deeper analysis of possible limitations or scenarios where the model might not perform as expected. Additionally discussing potential strategies to address these limitations would be insightful. 4. Ethical Considerations: Its suggested that the authors expand on the ethical considerations especially focusing on the automations impact on the job market and proposing a framework for ethical AI deployment in EDA that includes upskilling programs for affected professionals.  Limitations and Potential Negative Societal Impact The authors briefly mention the potential for job losses in the EDA industry due to the automation capabilities of AIbased placement and routing. However a more detailed exploration of this aspect alongside mitigation strategies is needed. To improve the authors could incorporate a section dedicated to ethical considerations discussing how AI innovations in chip design could coexist with the workforce. Suggesting industryacademia collaborations to retrain or upskill the affected workforce could be one way forward. Moreover discussing the limitations regarding the models scalability adaptability to different design rules and performance in realworld applications beyond the benchmarks would strengthen the paper.", "q__FmUtPZd9": " Review of \"Inverse DecisionMaking with Task Migrations in Social Contagion Management\"  1. Summary This paper proposes a novel problem in the domain of decisionmaking termed \"inverse decisionmaking with task migrations\" with a specific focus on social contagion management tasks. The primary question it addresses is whether one can solve a target management task by utilizing querydecision pairs from a different management task despite the underlying diffusion model being unknown. To tackle this the authors introduce \"SocialInverse\" a generic framework designed for migrating between tasks of diffusion enhancement and diffusion containment. Theoretical analyses support the learning performance of the framework while empirical studies validate its practical applicability and superiority to existing methods. Through rigorous experimentation it is demonstrated that the proposed solution can effectively manage task migrations and leverage data from diverse sources to solve complex decisionmaking problems potentially revolutionizing social contagion management strategies.  2. Assessment of Strengths and Weaknesses Strengths:  Originality: The paper addresses an innovative problem space combining inverse decisionmaking with task migrations within the realm of social contagion. This contributes a fresh perspective to decisionmaking literature and opens up new avenues for research.  Theoretical Grounding: The introduction of a theoretical framework alongside a generalization analysis adds depth and credibility providing a robust foundation for the empirical findings.  Practical Significance: Given the practical relevance of social contagion management from marketing strategies to misinformation containment this work has considerable potential for realworld impact.  Empirical Validation: Extensive empirical tests showcase the frameworks effectiveness across various scenarios convincingly arguing for its practical utility and superiority over baseline approaches. Weaknesses:  Model Assumptions: The assumption of access to approximation solutions for training data might limit the applicability in more dynamic or less structured environments where such data is unavailable.  Generalization: While showing promising results the experiments are constrained to specific types of social networks. The performance and adaptability of the SocialInverse framework in highly dynamic largescale or irregular network structures remain uncertain.  Complexity Analysis: Although the paper touches on NPhardness it lacks a detailed complexity analysis of the proposed algorithms which is crucial for understanding scalability issues.  3. Questions and Suggestions for the Authors  Generalization to Other Networks: Have the authors considered extending their experimental validation to more complex or largerscale networks Doing so could significantly strengthen claims of the frameworks versatility and utility.  Complexity and Scalability: Can the authors provide a more indepth discussion on the computational complexity of their algorithms especially when applied to realworld largescale data  Robustness to Noisy Data: How does the SocialInverse framework perform in the presence of noisy or incomplete data which is common in realworld social networks  Extension Ideas: Could the authors speculate on how their framework might adapt to or benefit from the incorporation of recent advancements in graph neural networks or other deep learning architectures  4. Limitations and Societal Impact The authors have made an initial attempt to discuss the limitations and future directions in Section 5 of their paper notably acknowledging the need for further experimental validation and the exploration of deep architecture designs. However a deeper analysis on the scalability of their approach to very large networks or more varied network topologies could enhance the paper. Additionally while the potential negative societal impacts are less direct a discussion around ethical considerations particularly when managing sensitive information cascades (e.g. political misinformation) would be pertinent. Suggestions for Improvement:  Expanding experimental validation to a broader array of networks and including a performance benchmark against substantial network sizes would help in understanding the frameworks scalability.  A detailed overview of computational requirements and potential optimization strategies would add another layer of depth to the methodology.  Addressing potential misuse especially in the context of information diffusion manipulation could enrich the discussion on societal impacts. Overall the paper presents a valuable contribution to the field of social contagion management and decisionmaking research.", "pgF-N1YORd": "Review of \"ClonExSAC: Improved Transfer in Continual Reinforcement Learning through Actor Critic and Exploration Techniques\" 1. Summary of the Paper This paper addresses the challenge of transfer in continual learning (CL) systems specifically within the domain of continual reinforcement learning (CRL). It aims to deepen our understanding of how transfer can be optimized by exploring the effects of different components of the Soft ActorCritic (SAC) algorithm (such as the actor and critic networks exploration strategies and data management) on the efficacy of transfer. The authors propose a novel approach termed ClonExSAC which incorporates a strategic combination of behavioral cloning for the actor bestreturn exploration strategy no regularization for the critic and multioutput heads. Their method is evaluated against existing ones using the Continual World benchmark showing superior performance in terms of final success rate and transfer metrics. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty and Relevance: The paper tackles an understudied area in CRL by conducting a systematic study on the factors affecting transfer. The proposed ClonExSAC method is both novel and highly relevant to the field.  Comprehensive Methodology: The methodology employed is thorough and robust analyzing various components individually and in combination to understand their impact on transfer. The multistage experimental setup from a simplified twotask scenario to full CRL scenarios adds depth to their analysis.  Significant Empirical Results: ClonExSAC outperforms existing methods on the Continual World benchmark indicating a substantial step forward in improving transfer learning in CRL. The improvement in forward transfer metrics is particularly noteworthy. Weaknesses:  Limited Generalizability: The study is based solely on the SAC algorithm and the Continual World suite. While the authors acknowledge this as a limitation it raises concerns regarding the applicability of their findings across different reinforcement learning algorithms and environments.  Data Retention Concerns: The requirement to retain data from previous tasks for ClonExSAC to function effectively might limit its practicality in some scenarios such as those with storage constraints or privacy concerns. 3. Questions and Suggestions for the Authors  Algorithm Generalizability: Can the authors speculate on how components of ClonExSAC might affect transfer in other reinforcement learning algorithms such as DQN or PPO Are there challenges anticipated in adapting ClonExSACs principles to these other algorithms  Further Experimental Validation: Could further experimentation in additional domains or with more structured state spaces potentially validate the robustness and generality of ClonExSAC It would be beneficial to see its performance in other benchmarks.  Addressing Data Retention Issues: Have the authors considered approaches to mitigate the data retention requirement of ClonExSAC Possible strategies could include synthetic data generation or more efficient data compression and selection mechanisms. 4. Limitations and Improvement Suggestions While the authors have discussed limitations including the focus on SAC and the Continual World suite a more detailed discussion on the potential negative societal impacts would strengthen the paper. For example addressing concerns around privacy and data security with retaining task data could make their work more resilient and applicable to realworld scenarios. Additionally exploring mitigations for these limitations could provide a more comprehensive view of ClonExSACs applicability. In conclusion the paper makes substantial contributions to the field of continual reinforcement learning by enhancing our understanding of transfer mechanisms and proposing an effective method that outperforms existing strategies. Despite limitations regarding generalizability and practical application concerns the work opens up exciting avenues for future research in CRL.", "rUc8peDIM45": "Review of \"Explaining the SGD Noise Structure and Its Impact on the Selection of Flat Minima\" 1) Summary: This paper offers a comprehensive analysis of how Stochastic Gradient Descent (SGD) noise influences the selection of flat minima during the training of overparameterized models with square loss. The authors establish a direct relationship between the noise structure of SGD\u2014particularly its alignment with sharp directions in the loss landscape\u2014and the models linear stability. They argue that for a minimum to remain linearly stable under the dynamics of SGD its Hessians Frobenius norm must be below a certain threshold. This threshold does not depend on model or sample size but on the learning rate batch size and the structural alignment of SGD noise. The authors validate their theoretical findings through experiments on several models including linear and nonlinear networks trained on the CIFAR10 dataset. 2) Strengths and Weaknesses: Strengths:  The paper addresses a significant problem in understanding the implicit regularization effects of SGD contributing to the deeper knowledge of why SGD often converges to flat minima which generalize better.  The introduction of a \"lossscaled alignment factor\" is innovative and provides a quantifiable measure to assess the alignment property of SGD noise broadening the scope for future investigations.  Extensive experimental validation including welldesigned experiments on a variety of models and datasets solidifies the relevance and applicability of the theoretical findings in practical scenarios. Weaknesses:  While the paper provides theoretical insights and empirical evidence to support the claims the explanations of the mechanisms behind the observed phenomena could be further detailed especially on how exactly the noises alignment property influences the selection of flat minima.  The paper largely focuses on overparameterized models with square loss. The generalizability of the findings to underparameterized models or those trained with different loss functions remains unclear.  The discussion on limitations and potential negative societal impacts is either missing or inadequately addressed. Given the broad applicability of SGD in machine learning a more thorough consideration of these aspects would enhance the papers contribution. 3) Questions and Suggestions:  Could the authors elaborate on the potential impact of their findings on the training of underparameterized models or models trained with nonsquare loss functions This extension could significantly broaden the applicability of the research.  I suggest a more detailed discussion on the intuitions behind why noises geometrical alignment with the loss landscape leads to the preference for flat minima. Including simplified illustrative examples could make the concepts more accessible.  The paper could benefit from a section discussing the limitations of the study more explicitly including considerations of the scenarios where the theoretical claims may not hold. Additionally reflecting on any potential negative societal impacts stemming from the findings or their misuse would be valuable. 4) Limitations and Societal Impact: The limitations of the study such as its focus on overparameterized models and reliance on the square loss function are acknowledged to some extent but a more comprehensive examination of these and other limitations would strengthen the paper. Moreover discussing the potential negative societal impacts is essential to ensure the responsible advancement of AI. For instance if SGDs implicit bias toward flatter minima can be manipulated negatively in adversarial settings or if it exacerbates issues of model interpretability these should be explored. Constructive suggestions might include recommending practices for mitigating any adverse impacts identified or areas for future research to address these limitations. Conclusion: This paper makes a significant contribution to understanding the implicit regularization effect of SGD via a novel perspective on the noise structure. While the study has some limitations and areas that could be further expanded upon its theoretical insights and extensive empirical validations offer valuable advancements in the field. Future work addressing the generalizability of these findings and exploring the outlined suggestions could enhance the robustness and applicability of the research.", "nRcyGtY2kBC": "Review of \"Improving Conditional Average Treatment Effect Estimation Through Heterogeneous Transfer Learning\" 1. Summary: This paper addresses the crucial problem of estimating the Conditional Average Treatment Effect (CATE) in target domains with limited data and heterogeneous feature spaces compared to a source domain. This scenario is common in healthcare and other fields where interventions need to be personalized based on observational data. The novelty of this work lies in its approach to leverage data from a related but not identical source domain to improve CATE estimation in the target domain. The authors introduce a method involving representation learning for handling feature heterogeneity and a multitask architecture that allows for information sharing across domains. They define a novel problem setup propose a set of building blocks for heterogeneous transfer causal effect learners (HTCElearners) and demonstrate their efficacy through a semisynthetic data simulation. The paper contributes to the existing literature on causal inference transfer learning and domain adaptation by providing a flexible framework that extends common CATE estimation methods to the heterogeneous transfer learning setting. 2. Assessment: Strengths:  Novelty: The paper tackles an underexplored but practically significant problem of heterogeneous transfer learning specifically for CATE estimation. The proposed HTCElearners represent an innovative approach to leveraging data across domains with diverse feature spaces and underlying causal structures.  Technical Soundness: The methodology is technically sound encompassing rigorous problem formalization clearly defined assumptions and a wellstructured approach to constructing the HTCElearners. The use of semisynthetic data simulation for evaluation is appropriate given the challenge of accessing real counterfactual data.  Practical Relevance: This research has high practical relevance especially for healthcare applications where often there is a need to infer the effectiveness of treatments for new patient populations based on existing datasets with different covariates.  Thorough Evaluation: The experimental setup is comprehensive with detailed discussions on the impacts of various factors such as the amount of shared information between domains dataset sizes and selection bias. These provide valuable insights into the conditions under which the proposed method excels. Weaknesses:  Clarity in Contribution Differentiation: While the paper positions itself within the intersection of several research domains it could further clarify how HTCElearners advance beyond the stateoftheart in each relevant area specifically in contrast to existing CATE learners and transfer learning models.  Discussion on Limitations: The limitations section while acknowledging the focus on binary treatments and the necessity for theoretical performance guarantees somewhat underplays the potential challenges in realworld implementation particularly in identifying and quantifying the \"shared structure\" between source and target domains which is critical for the methods success.  Potential for Broader Applicability: The discussion is primarily framed within healthcare applications. Expanding on how the proposed method could be adapted or applied to other domains (beyond education and public policy which are briefly mentioned) might broaden the papers impact. 3. Questions and Suggestions:  Regarding Feature Space Heterogeneity: The paper efficiently addresses feature heterogeneity through representation learning. However it would be beneficial to explore and discuss further how the method can be adapted or extended when facing not just heterogeneous but dynamically evolving feature spaces as seen in longitudinal studies.  Treatment Effect Heterogeneity: While the paper focuses on binary treatment settings extending the framework to accommodate other types of treatments (continuous multivalued or temporal treatments) could significantly enhance its utility. Any thoughts or preliminary ideas on achieving this would be valuable.  Robustness to Model Misspecification: Given the reliance on semisynthetic data and predefined simulation settings how robust are the HTCElearners to model misspecification in realworld scenarios where the true datagenerating processes are unknown or complex 4. Limitations and Suggestions for Improvement: While the paper effectively addresses its stated goals it would benefit from a deeper discussion on operational challenges and broader applicability. To further solidify its contributions I suggest:  A more thorough comparison including quantitative benchmarks against stateoftheart models in both causal inference and transfer learning domains.  Expanding the discussion on the practical implementation challenges particularly in healthcare requiring regulatory approvals and clinical validations to provide a grounded perspective on deploying HTCElearners.  Considering the societal implications the paper could further engage with the ethical considerations surrounding treatment effect estimation especially in sensitive applications like healthcare to foster responsible research and usage. Overall this paper presents a compelling and technically sound approach to a challenging problem with clear potential for significant impact within the field of causal inference and beyond.", "s1yaWFDLxVG": " Peer Review 1) Brief Summary: The paper addresses a critical aspect of firstorder optimization  the sensitivity of optimization algorithms to objective function assumptions that ultimately influence algorithmic efficiency and conditions for optimality. Dissecting the limitation with widely assumed conditions of strong convexity and smoothness the authors turn their focus towards a more robust set of conditions  the lower restricted secant inequality (RSI\u2212) and upper error bounds (EB). Through an analytical exploration and leveraging the Performance Estimation Problem (PEP) framework the paper establishes that gradient descent (GD) tuned appropriately performs optimally in the context of these conditions without the possibility of further acceleration  a revelation that adds a new dimension to understanding algorithmic optimality. Key sections include the introduction of RSI\u2212 and EB related works defining interpolation conditions and proving the lower bound on convergence rates leading to conclusions that highlight GDs exact optimality under the given conditions. 2) Assessment of Strengths and Weaknesses: Strengths:  The paper pioneers in addressing the insufficiency of traditional assumptions (strong convexity and smoothness) in optimization thus pushing the boundary for a more robust framework.  Methodologically sound leveraging the PEP framework to analytically solve and establish a lower bound on convergence rates presents a transparent and replicable approach.  Empirical verification on the optimization path of a ResNet18 provides a practical lens to theoretical claims enhancing the credibility and applicability of the results within deep learning contexts. Weaknesses:  The transition from highlighting issues with established notions of optimality to introducing RSI\u2212 and EB as alternatives seems swift. A deeper motivational groundwork would enhance comprehension and engagement.  While empirical verification on a single neural network architecture offers practical insights it restricts generalizability. Additional architectures and tasks could provide a broader validation scope.  The discussion on limitations and societal impact appears underexplored. Given the profound implications of optimization in AI a more thorough contemplation on how these findings translate to broader societal impacts or address ethical concerns is warranted. 3) Questions and Suggestions for the Authors:  Clarity on Translating Theoretical Insights to Practice: How do the findings inform the tuning of firstorder optimization algorithms in heterogeneous realworld scenarios especially in nonconvex settings prevalent in deep learning  Broader Empirical Evaluation: Could additional experiments involving other network architectures or optimization problems be incorporated to validate the universality of the proposed conditions applicability  Exploration of Alternative Conditions: Given the focus on RSI\u2212 and EB as robust conditions is there room to explore or introduce other conditions that might similarly evade the limitations of strong convexity and smoothness  Limitations and Societal Impact: The authors are encouraged to elaborate on potential limitations especially in terms of scalability and computational efficiency. Moreover a discussion on the potential societal impacts considering the ubiquitous role of optimization algorithms in technology would add value. 4) Limitations: The paper successfully navigates the introduction of robust optimization conditions but marginally addresses the limitations associated with these conditions practical application. The computational efficiency and scalability aspects especially when considering highdimensional optimization problems remain lightly touched. Furthermore the paper does not delve into the potential negative societal impacts \u2013 an important consideration given the significant role optimization plays in algorithmically driven decisions within societal systems. Constructive Suggestions for Improvement:  A dedicated section discussing potential scalability and efficiency challenges when implementing the proposed conditions in largescale optimization problems could provide a more holistic view.  Expanding the empirical validation to include a diverse set of neural network architectures and optimization tasks would bolster the papers assertions on practical applicability.  A thoughtful discussion on potential negative societal impacts such as biases that optimization algorithms might entrench or exacerbate when applied in realworld AI systems could add an important dimension to the conversation.", "plu6AK3qs5T": " Peer Review of \"AutoClipping: Automatic Clipping Threshold for Differential Privacy in Deep Learning\"  1) Summary of the Contributions The paper introduces AutoClipping a novel technique designed to automatize the selection of the clipping threshold (R) in differential privacy (DP) deep learning algorithms. The authors propose two variants AUTOV and AUTOS as replacements for the standard perexample gradient clipping method that requires manual tuning of R. They claim these methods maintain the same privacy guarantees computational efficiency and require no DPspecific hyperparameters for various DP optimizers. Through rigorous theoretical analysis and empirical validation across computer vision and language tasks the paper argues that AutoClipping matches or outperforms stateoftheart methods while significantly simplifying the DP training process.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The approach to automatically determine the clipping threshold without sacrificing privacy or computational efficiency is innovative. This could significantly lower the barrier to adopting DP in deep learning.  Theoretical Foundation: The convergence analysis of automatic DPSGD in nonconvex settings provides a solid theoretical basis for the effectiveness of AutoClipping.  Empirical Validation: Comprehensive experiments across different domains (vision and language tasks) and models (including large models like ResNet RoBERTa GPT2) convincingly demonstrate the methods effectiveness.  Ease of Integration: The inclusion of code snippets and claims of easy adaptation to existing codebases promote its practical applicability. Weaknesses:  Hyperparameter \u03b3: The introduction of the stability constant \u03b3 in AUTOS while critical to maintaining gradient magnitude information adds a hyperparameter that may need tuning in practice which somewhat contradicts the goal of reducing hyperparameter reliance.  Scale Invariance Concerns: While the paper addresses scale invariance and the potential \"lazy region\" issue with AUTOV the practical impact of these theoretical concerns on training dynamics is not thoroughly explored.  Limitation Discussion: The paper could benefit from a deeper discussion on the potential limitations or scenarios where AutoClipping might not perform as expected. For instance the impact of extremely noisy or sparse gradients on AutoClippings effectiveness is not addressed.  3) Questions and Suggestions for the Authors  Stability Constant \u03b3: How sensitive are the models performance and convergence properties to the choice of \u03b3 Could you provide guidelines or insights on setting \u03b3 across different tasks and models  Comparison with Other Techniques: Could you detail how AutoClipping performs relative to other approaches aimed at overcoming the manual tuning of R such as adaptive clipping methods  Longterm Impact: Can you speculate on or provide preliminary evidence for how AutoClipping might interact with other DP improvements such as advanced privacy accounting methods or privacy amplification techniques  4) Addressing Limitations and Societal Impact The authors briefly mention the societal impact of their work by facilitating the more widespread adoption of DP techniques in deep learning. However a more thorough discussion on the limitations especially the introduction of the \u03b3 hyperparameter in AUTOS and potential scenarios under which AutoClipping might not yield the desired privacyutility tradeoff would enhance the paper. Additionally while lowering the barrier to DPs application is beneficial considering and discussing the broader implications of potentially enabling more intrusive models due to ease of training could prompt important ethical considerations. Suggestions for Improvement:  Providing a more detailed discussion on selecting and tuning \u03b3 across different settings could help practitioners better utilize AutoClipping.  Expanding the experimentation section to include scenarios with challenging gradient distributions might showcase the methods robustness further.  Engaging with the broader implications of making DP training more accessible including potential misuse and the need for ongoing ethical considerations in deploying models trained with AutoClipping. In summary the paper presents a potentially transformative approach to simplifying differential privacy applications in deep learning. Addressing the noted weaknesses and questions can further strengthen the works impact and applicability.", "uLYc4L3C81A": "Review of \"Confident Adaptive Language Modeling (CALM): A Framework for Efficiently Accelerating Transformerbased Large Language Models\" 1. Summary The paper presents Confident Adaptive Language Modeling (CALM) a novel framework designed to reduce computational costs associated with Transformerbased large language models (LLMs) by dynamically adjusting the amount of computation allocated per input and generation timestep. Recognizing that not all predictions require the full model capacity CALM employs early exit strategies where predictions can be made at intermediate layers based on confidence measures. The authors address several challenges in early exit decoding including selecting appropriate confidence measures connecting global sequence constraints with local token exit decisions and handling the absence of hidden states due to previous early exits. Empirical validation across text summarization machine translation and question answering tasks demonstrates CALMs ability to significantly reduce computational requirements (up to 3x speedup) while preserving high task performance. 2. Assessment Strengths:  Novelty and Relevance: The idea of dynamically allocating computational resources based on the difficulty of specific tokens during generation is novel and highly relevant given the growing energy and cost concerns related to LLM inference.  Theoretical and Empirical Rigor: The paper provides a solid theoretical foundation for the CALM framework including a method for calibrating early exit decisions to maintain sequencelevel quality guarantees. The comprehensive empirical evaluation across multiple tasks and setups further strengthens the paper.  Practicality: The methods proposed can be integrated into existing large language models with minimal modifications offering a practical path toward reducing the computational overhead of such models in realworld applications. Weaknesses:  Generalization of the Confidence Measure: While the paper presents a class of confidence measures and their calibration it is unclear how these measures generalize across different languages domains or even fundamentally different tasks beyond the ones tested. Further exploration in diverse settings would strengthen the findings.  Limitation in Model Diversity: The experiments are conducted using variants of the T5 model which may limit the generalizability of the proposed CALM framework to other Transformer architectures or language models with significantly different characteristics.  Analysis of Error Propagation: While the paper discusses error propagation due to early exiting the depth of analysis particularly in how these errors affect downstream tasks or longer generation sequences appears limited. A more detailed exploration could provide further insights into the robustness of the CALM framework. 3. Questions and Suggestions for the Authors  Generalization Across Languages and Domains: Can the authors provide insights into how the confidence measures and their calibration might be adapted for languages with significantly different syntactic or morphological characteristics or for highly specialized domains  Impact on Different Model Architectures: Have any experiments been conducted with nonT5based models such as GPT or BERT variants If so how do the results compare and if not what are the potential challenges in applying CALM to these models  Detailed Analysis of Error Types: Could the authors elaborate on the types of errors introduced by early exits and their impact on tasks that may be sensitive to subtle nuances in language (e.g. sentiment analysis humor detection)  Suggestions for Improvement: It may be beneficial to incorporate a mechanism for dynamic reevaluation of confidence thresholds based on continuous or periodic feedback from actual task performance which could potentially address the issue of generalization across domains and languages. 4. Limitations and Societal Impact The authors discuss the computational efficiency gains of CALM which is a positive step toward making LLMs more environmentally friendly and costeffective. However the potential for inconsistent performance across languages and domains is briefly mentioned but not thoroughly addressed. A more comprehensive discussion of these limitations along with strategies for ensuring equitable performance improvements across a broader spectrum of applications would be valuable. Additionally exploring the societal implications of making powerful LLMs more accessible and efficient both positive and negative could provide a more holistic understanding of CALMs impact.", "opw858PBJl6": "Review of \"Rethinking Saliency Methods: Completeness and Soundness in Explaining Model Decisions\" 1) Summary of Contributions: This paper tackles the critical issue of interpreting deep neural network decisions through saliency methods which generate heatmaps to highlight input features deemed important for a models decision. The authors argue that current saliency evaluation methods focus primarily on completeness (ensuring all relevant features are included) but neglect soundness (ensuring no irrelevant features are included). To address this the paper introduces new evaluation metrics for saliency methods that consider both completeness and soundness without requiring external judgments or separately trained models. A noteworthy contribution is the development of a novel saliency method that surpasses or matches the performance of existing methods according to these new metrics. Additionally the paper sheds light on the intrinsic value of heuristic techniques such as TV regularization and upsampling by showing their role in enhancing soundness. 2) Assessment of Strengths and Weaknesses:  Strengths: a. Innovative Metrics: The introduction of soundness and completeness metrics for evaluating saliency methods is a substantial advancement in explainable AI. These metrics address a noticeable gap in the evaluation of saliency methods ensuring a balanced assessment of the explanations provided by these methods. b. Comprehensive Analysis: The paper presents a thorough comparison of the proposed saliency method against existing techniques demonstrating its effectiveness across various evaluation metrics. c. Theoretical and Empirical Justification: The paper provides a strong motivation for including soundness in saliency method evaluations supported by both theoretical insights and empirical results. This strengthens the argument for the proposed evaluation framework. d. Practical Contribution: By enhancing the intrinsic justification for heuristic techniques and proposing an effective saliency method the work has direct applicability in improving the interpretability of neural network models.  Weaknesses: a. Limited Focus on Human Interpretability: The papers intrinsic evaluation framework does not directly tackle human interpretability which is a crucial aspect of explainability in AI. While intrinsic metrics are valuable future work should also consider how these explanations align with human understanding. b. Dependency on Model Outputs: The evaluation metrics proposed depend on the models probability outputs which can vary significantly across models and tasks. This may limit the generalizability of the findings to different settings or models with highly uncertain outputs. c. Potential for Overfitting to Metrics: There is a risk that focusing on optimizing these new metrics might lead to overfitting saliency methods to these measures without actual improvement in providing meaningful explanations. 3) Questions and Suggestions for Authors:  Clarification on Human Evaluations: Could the authors elaborate on how the proposed saliency method and evaluation metrics might correlate with human judgment of model explanations Are there plans to incorporate humancentered studies  Adaptability Across Different Models: How adaptable is the proposed saliency method across different architectures and tasks especially outside the realm of image processing  Addressing Overfitting Concerns: What measures can be taken to ensure that optimization for completeness and soundness does not lead to overfitting to these metrics at the expense of producing genuinely informative explanations  Exploring Further Applications: Could the notion of soundness be applied to other explanation methods beyond saliency such as feature attribution or counterfactual explanations 4) Limitations and Societal Impact: The authors have made an effort to address the limitations of their work particularly in terms of focusing on intrinsic evaluations and potentially overlooking human interpretability aspects. However additional consideration could be given to the implications of overly relying on computational metrics for evaluating saliency which might not always align with user expectations or understanding. Constructive suggestions include incorporating diverse humancentric evaluations to validate the utility of the proposed methods and exploring the adaptability of the framework to a wider range of tasks and data types. Furthermore considering the potential negative societal impact its vital to ensure that these methods do not inadvertently obscure or misrepresent the decisionmaking process of models especially in highstakes applications. This concern underscores the importance of maintaining transparency about the limitations of saliency methods and the need for ongoing research to bridge the gap between computational and human interpretations of model decisions.", "pFqgUJxXXz": "Review of \"Adversarial Task Upsampling for Enhanced MetaLearning Performance\" 1) Summary: This paper introduces an innovative approach to task augmentation known as Adversarial Task Upsampling (ATU) targeted at enhancing the generalization ability of metalearning algorithms. The principal contribution of the proposed framework lies in its ability to generate augmented tasks by upsampling existing metatraining tasks through a task upsampling network. This approach ensures the generation of tasks that are both aligned with the true task distribution and sufficiently challenging to facilitate the improvement of the metalearner. The ATU method also capitalizes on an adversarial loss to optimize the generation process producing tasks that present a meaningful challenge to the current state of the metalearner. The authors empirically demonstrate the effectiveness of ATU across both regression and classification problems utilizing five different datasets to substantiate claims of superiority over existing stateoftheart task augmentation strategies. 2) Strengths and Weaknesses: Strengths:  Originality: The concept of a tasklevel augmentation network that simultaneously targets taskawareness taskimaginary and modeladaptive properties is novel. The use of adversarial loss to generate challenging tasks for the metalearner is an innovative approach that distinguishes ATU from other task augmentation methods.  Theoretical Justification: The paper provides a theoretical analysis supporting the efficacy of ATU in promoting taskawareness a significant theoretical contribution that solidifies the foundation of the proposed method.  Empirical Validation: Thorough experimental work across a range of datasets and problem types (regression and classification) substantiates the papers claims. The comparison with stateoftheart methods illustrates ATUs superior performance in enhancing the generalization capability of metalearners. Weaknesses:  Complexity and Scalability: The paper does not adequately address the computational complexity of the ATU framework or its scalability to more complex metalearning scenarios with larger datasets. Given the addition of an adversarial task generation component potential increases in training time or resource requirements could limit practical applicability.  Generalization across Diverse Domains: While the paper presents results across five datasets it does not extensively explore the generalizability of ATU to domains with highly idiosyncratic task distributions or to tasks beyond the fewshot setting. Further exploration in this area would strengthen the papers assertions about the versatility of ATU. 3) Questions and Suggestions for Improvement:  Generalization to Other MetaLearners: It would be beneficial to see the performance of ATU when applied to a wider array of metalearning algorithms beyond the gradientbased methods considered. How does ATU fare with methods that fundamentally differ in how they approach task adaptation  Impact on Computational Requirements: Given the introduction of adversarial task generation what is the computational overhead introduced by ATU It would be useful for practical considerations to have an analysis or discussion regarding any significant increases in computational resources or training time.  Extension to More Diverse Domains: The paper could benefit from experiments in domains with fundamentally different task characteristics such as natural language processing or reinforcement learning tasks to validate the general applicability and robustness of ATU.  Understanding the Role of Adversarial Loss: More indepth theoretical insights or empirical analysis on how varying the strength or formulation of the adversarial loss impacts the quality and diversity of generated tasks would provide valuable guidance for tuning ATU in practical implementations. 4) Addressing Limitations and Societal Impact: The authors acknowledge limitations related to the theoretical assumptions made and the scope of empirical validation. It would strengthen the paper to discuss any potential negative societal impacts particularly in applications where augmented tasks generated by ATU may inadvertently introduce or exacerbate biases in the metalearner. Suggestions for improvement include implementing mechanisms to detect and mitigate such biases during task generation and more broadly considering the ethical implications of deploying ATU in sensitive applications such as healthcare or autonomous systems. Additionally adding a discussion on strategies to manage the computational complexity and adapt ATU to more diverse and complex task distributions would address scalability concerns and broaden the frameworks applicability.", "mSiPuHIP7t8": " Peer Review for \"GraphDE: A Generative Probabilistic Framework for Debiased Learning and OutofDistribution Detection in Graphs\"  1) Summary of the Paper This paper presents GraphDE a novel generative probabilistic framework focused on addressing two critical but underexplored challenges in the domain of graph representation learning: debiased learning and outofdistribution (OOD) detection. The authors mathematically formulate and define these problems under a unified probabilistic model leveraging a variational inference approach to infer latent environment variables that indicate whether data are indistribution (ID) or not (outliersOOD). They introduce a mixture model composition for both the graph structure and classification models to differentiate and appropriately weight ID and OOD data during training. Their evaluation across various datasets demonstrates GraphDEs superiority over existing baselines in both debiasing learning tasks and OOD detection substantiating their contributions to the advancement of trustworthy GNN applications.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper tackles largely unexplored problems in graph data with a novel approach that combines debiased learning and OOD detection under a generative probabilistic framework. This approach is theoretically justified and wellmotivated.  Methodological Rigor: The authors provide a robust mathematical formulation for their model grounded in probabilistic generative learning. The use of variational inference to estimate model parameters and the environment variable is particularly noteworthy.  Experimental Validation: The comprehensive experiments across diverse datasets showcase GraphDEs effectiveness compared to strong baselines. The quantitative analysis is accompanied by qualitative insights particularly in visualizations that highlight the models proficiency in identifying and attenuating the influence of outliers.  Reproducibility: Making the source code publicly available enhances the reproducibility of their work and facilitates further research based on their proposed framework. Weaknesses:  Lack of Comparative Discussion: The paper could benefit from a more detailed comparison with related works especially discussing how GraphDEs probabilistic model compares to other methods not explicitly designed for graph data but potentially adaptable.  Limitations and Societal Impact Not Fully Explored: While the paper briefly mentions the limitations and potential for generalization to other data formats a deeper discussion on the limitations particularly regarding scalability and computational efficiency would be valuable. The societal implications especially in sensitive applications are not discussed.  Hyperparameter Sensitivity: The paper suggests some dependence on hyperparameter settings (e.g. prior ratios) but does not fully explore the sensitivity or provide guidelines for tuning these hyperparameters.  3) Questions and Suggestions for the Authors  Scalability and Computational Efficiency: Could the authors discuss the scalability of GraphDE concerning graph size and density Given the computational complexity of variational inference and GNNs how does GraphDE perform on largescale graphs  Hyperparameter Sensitivity: Can the authors provide more insights or potential guidance on setting hyperparameters especially the prior ratio of the environment variable to optimize performance across different tasks and datasets  Generalization to Other Data Formats: While the authors mention the potential generalization of their framework to other data types such as images or texts it would be beneficial to discuss any preliminary findings or anticipated challenges in adapting GraphDE to these formats.  4) Limitations and Constructive Suggestions The papers limitations regarding a detailed discussion on scalability and potential computational inefficiencies when applied to very large graphs were noted. Additionally the societal impacts particularly negative consequences of potentially misclassifying OOD samples in critical applications were not adequately addressed. Suggestions for Improvement:  Including a Scalability Study: Provide experimental results or a theoretical analysis of GraphDEs performance on larger graphs to address scalability concerns.  Detailed Discussion on Societal Impact: Expand on the potential societal impacts both positive and negative of deploying GraphDE in realworld scenarios. Discuss mitigating strategies for any identified risks.  Guidelines on Hyperparameters: Offer more comprehensive guidance or automated techniques for selecting and tuning hyperparameters aiding practitioners in effectively deploying the model across varied scenarios. These suggestions aim to enhance the clarity utility and ethical consideration of deploying GraphDE in practical settings further strengthening the papers contributions to the field.", "vExdPu73R2z": "PeerReview Report 1) Brief Summary: The paper introduces an extended task for Referring Video Object Segmentation (RVOS) called Robust RVOS (R2VOS) which removes the assumption that a referred object must exist in the video linked with a referring linguistic expression. To address this challenge the authors propose a model that leverages textvideotext cycle consistency to determine semantic consensus between video clips and linguistic expressions. A novel evaluation dataset named R2YoutubeVOS is introduced to measure model robustness against unpaired videoexpression inputs. The method is shown to significantly reduce false positive rates (referred to as the false alarm problem) for negative pairs and improves segmentation performance for positive pairs achieving stateoftheart results on RefYoutubeVOS RefDAVIS17 and the R2YoutubeVOS dataset. 2) Assessment of Strengths and Weaknesses: Strengths:  Originality: The paper tackles a relatively unexplored problem in RVOS by questioning the assumption of objectlevel semantic consensus between video and linguistic expressions. The introduction of R2VOS and the use of cycle consistency for semantic consensus discrimination are novel and significant contributions.  Methodology: The approach of leveraging cycle consistency between the original and reconstructed text embeddings to filter out negative pairs and improve the segmentation of positive pairs is wellfounded. The early grounding module and parallel optimization of primary and dual problems are welldesigned and contribute to the effectiveness of the method.  Evaluation: The comprehensive evaluation methodology including the creation of the R2YoutubeVOS dataset for robustness measurement against unpaired inputs is commendable. The superior performance over stateoftheart methods on multiple datasets demonstrates the effectiveness of the proposed method.  Presentation: The paper is wellstructured and clearly written with a logical flow that makes it easy to understand the proposed method and its contributions. Weaknesses:  Limitations Discussion: While the authors briefly mention limitations in the additional appendix the paper could benefit from a more thorough discussion of limitations in the main text. For instance the challenge of object occlusion and its impact on perframe segmentation accuracy could be highlighted as an area for future improvement.  Societal Impact: The paper lacks a detailed discussion on potential negative societal impacts of the work. While its acknowledged that most applications could be positive exploring possible misuse and privacy considerations especially in scenarios involving surveillance footage would make the paper more comprehensive.  Reproducibility: The absence of shared code and specific implementation details may hinder the reproducibility of the proposed method. Acknowledging plans to release code and datasets upon acceptance is good but immediate availability would strengthen the paper\u2019s impact. 3) Questions and Suggestions for the Authors:  Question: Could the authors elaborate on how the model performs in scenarios with complex backgrounds or multiple objects that might fit the textual description but are not the target object  Suggestion: To address the limitation related to occlusion incorporating temporal consistency checks over sequences where the presence of the object is inferred based on its historical appearance could be a fruitful direction.  Suggestion: To ensure broader applicability and ethical use of the technology the authors could discuss guidelines or mechanisms to prevent misuse of RVOS technologies especially in privacysensitive environments.  Suggestion: For enhancing reproducibility alongside releasing code providing detailed hyperparameter settings software versions and hardware configurations utilized in experiments would be beneficial. 4) Limitations and Societal Impact: The papers discussion on limitations mainly focused on technical aspects and overlooked broader societal implications. Addressing how the developed technology might be misused or impact privacy negatively would make the discussion more rounded. Future works should also consider fairness and bias particularly in how models may perform across diverse video datasets. Improvement Suggestions:  Incorporate a broader discussion on potential negative societal impacts privacy and misuse.  Discuss technical limitations in the main text and propose potential solutions or future research directions.  Enhance reproducibility by providing comprehensive details on the implementation and experimenting with configurations upon code release.", "uKYvlNgahrz": "This paper introduces a novel architecture for constructing monotonic deep neural networks a topic of great relevance given the increasing importance of explainability and constraints in machine learning models. The key contribution is a straightforward modification to existing methods for enforcing monotonicity in neural networks. By employing both a function and its point reflection within neuron layers the authors propose a method able to approximate complex nonconvex functions while maintaining the monotonic characteristic. This development is significant as it sidesteps the limitations of previous architectures that could only handle convex functions thereby broadening the applicability of monotonic networks in realworld tasks.  Strengths: 1. Innovative Approach: The papers method ingeniously combines activation functions and their point reflections to enforce monotonicity tackling the nonconvex approximation problem that plagued earlier models. 2. Simplicity and Efficiency: The architecture is lauded for its simplicity not requiring any additional parameters or changes to the learning algorithm thus making it computationally attractive. 3. Empirical Validation: The authors provide a thorough empirical evaluation that demonstrates their methods superior or comparable performance to stateoftheart techniques on various datasets emphasizing its practical utility. 4. Theoretical Support: A rigorous proof of the universal approximation property for monotone functions adds a solid theoretical foundation to the proposed method.  Weaknesses: 1. Limited Activation Function Scope: The methods restriction to monotonic activation functions might limit its flexibility or applicability in scenarios where other types of nonlinear activations are preferred. 2. Potential Overfitting Risk: While the models simplicity is a strength there could be scenarios especially with complex datasets where this simplicity leads to underfitting or an inability to capture the underlying data distribution adequately. 3. Lack of Broader Impact Discussion: The paper could benefit from a more detailed discussion on the societal and ethical implications of employing monotonic models given their potential use in critical domains like finance and healthcare.  Questions and Suggestions for Authors: 1. Activation Function Limitations: Can you elaborate on potential approaches to extend your methodology to nonmonotonic activation functions or detail how significant the limitation is in practical applications 2. Regularization Techniques: While the method guarantees monotonicity by construction could incorporating regularization techniques improve the models robustness or generalization to unseen data 3. Comparison Across Diverse Domains: How does the performance of your method compare across different domains especially where the assumption of monotonicity might be more or less strong Further experimentation could provide more nuanced insights. 4. Interpretability and Explainability: Beyond monotonicity are there avenues for your method to enhance other aspects of neural network interpretability or explainability This is an increasingly critical area in machine learning research.  Limitations and Societal Impact: While the paper briefly discusses the importance of monotonicity in ensuring models do not violate intuitive expectations a deeper exploration of the limitations and the methods societal impacts would enrich the manuscript. For instance how might the reliance on monotonic activation functions affect model performance in highly noisy or complex datasets Additionally as the method is poised for application in sensitive areas a cautious examination of potential biases or misinterpretations arising from enforced monotonicity and suggested mitigations would be highly valuable.  Constructive Suggestions for Improvement: 1. Experimental Diversity: Expanding the experimental evaluation to include more varied domains could better illustrate the methods versatility and limitations. 2. Activation Function Exploration: Further theoretical analysis or empirical experimentation with a broader set of activation functions could potentially enhance the methods applicability. 3. Indepth Limitation Discussion: A more detailed discussion on potential scenarios where the proposed method might not perform optimally accompanied by possible mitigations or warnings for practitioners would be beneficial. Overall this paper makes significant strides in the design and application of monotonic deep neural networks presenting a solution that is both practically and theoretically compelling. The suggested improvements and additional considerations could further solidify its contributions to the field.", "qPb0m0NXt4j": "PeerReview Report: 1) Brief Summary of the Paper: The paper presents an innovative exploration into the evolution of graphical conventions through emergent communication between two neural agents engaged in a visual communication game. In this setup a sender communicates with a receiver by sketching images on a canvas aiming to convey certain concepts without the use of linguistic symbols. The primary contribution of this work lies in modeling the transition from iconic (directly resembling the referent) to symbolic (arbitrarily related to the referent) communication through a novel reinforcement learning framework. This framework facilitates the agents\u2019 joint evolution towards successful communication abstract graphical conventions and an examination of these conventions based on three defined properties: iconicity symbolicity and semanticity. Empirical results demonstrate that the evolved sketches and communication strategies align with observations in human graphical conventions highlighting phenomena such as the preservation of semantic continuum and the switch to more iconic communication for unfamiliar concepts. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The paper takes a novel step in exploring the evolution of communication from iconic to symbolic representations through neural agents addressing a gap in emergent communication research that often focuses on symbolic language.  Comprehensive Evaluation: The authors define and systematically evaluate the emerged graphical conventions with meticulously designed metrics for iconicity symbolicity and semanticity providing a thorough understanding of the agents evolved communication abilities.  Alignment with Human Communication: The empirical results not only validate the effectiveness of the proposed model but also align well with observations from human studies suggesting relevance and applicability to cognitive science theories of communication evolution.  Technical Rigor: The technical development of the multiagent reinforcement learning framework the integration of continuous sketch parametrization and the novel training surrogate demonstrate significant computational innovation. Weaknesses:  Limited Generalization Discussion: While the paper presents tests on unseen instances and classes the discussion on the agents ability to generalize to completely new contexts (beyond the tested unseen categories) is somewhat limited.  Model Complexity and Computational Demands: The complexity of the proposed framework and the computational resources required might limit its accessibility and applicability in broader research contexts or practical implementations.  Scope for Enhancing Realism: The simulations though informative operate in a somewhat idealized setting. Incorporating more realistic environmental variables and challenges could provide deeper insights into the robustness of the emergent conventions. 3) Questions and Suggestions for the Authors:  Generalization Capability: Could the authors elaborate on potential strategies to enhance the agents ability to generalize to new contexts or domains beyond the tested unseen classes and instances  Model Accessibility: Given the computational demands do the authors have suggestions for making the approach more accessible to researchers with limited computational resources  Realism in Simulations: Have the authors considered introducing more variability or noise into the visual communication game to test the robustness of the emerged conventions in more realistic scenarios 4) Limitations and Societal Impact: The paper briefly touches on the generalization capability of the emerged conventions which is crucial for assessing the models applicability to realworld scenarios. More detailed exploration in this direction could strengthen the work. Additionally while the focus on graphical communication is commendable discussions on the potential implications (both positive and negative) of these findings on societal aspects such as privacy concerns or misinformation through visual communication are absent. Constructive suggestions include:  Enhancing Transparency: Developing methods to ensure the interpretability and transparency of the emergent conventions could mitigate potential negative impacts such as misinterpretation or manipulation.  Diversity and Bias: Further research should consider the diversity of the input data and explore biases in the evolved conventions to ensure fair and inclusive communication systems. Overall the paper presents a significant advancement in understanding emergent graphical communication offering valuable insights for both AI and cognitive science fields.", "wXNPMS11aUb": " Peer Review for \"Developmental ProcessDriven Locality Sensitive Hashing for Nearest Neighbour Search\"  1) Brief Summary This paper introduces a novel bioinspired methodology for enhancing the performance of locality sensitive hashing (LSH) namely through a developmental process that mimics the connectivity formation observed in the insect mushroom body. Labeled as DevFly this method eschews random connectivity for a more structured approach wherein connections between projection neurons (PNs) and Kenyon cells (KCs) are established based on the strength of input dimensions across samples. The performance of DevFly is evaluated against the traditional FlyLSH model in terms of accuracy and speed in finding nearest neighbours across several datasets. The paper reports that DevFly not only outperforms FlyLSH in most cases but also demonstrates significant improvements in learning efficiency and insensitivity to sample order alongside potential benefits for hardware implementation due to its onetime update mechanism.  2) Strengths and Weaknesses Strengths:  Innovation: The move from random to developmentallydriven connectivity is a novel approach that stands to significantly benefit the field of locality sensitive hashing.  Biological Inspiration: The method\u2019s grounding in neuroscientific findings adds depth to the models conceptual underpinnings and suggests potential across diverse applications.  Performance: Improved accuracy in nearest neighbour searches and better learning efficiency present strong cases for DevFly\u2019s utility.  Computational Efficiency: The paper offers a compelling argument for the models superiority in terms of computation time and data usage particularly in largescale applications. Weaknesses:  Dataset Dependency: The performance gains reported are somewhat variable across different datasets hinting at possible limitations in the methods generalizability or the need for datasetspecific adaptations.  Optimality of Connections: While DevFly shows improved performance the paper does not fully address whether the developmental process leads to the optimal set of connections for the intended tasks.  Lack of Comparative Analysis: The paper would benefit from a more comprehensive comparison with other stateoftheart LSH methods especially those that incorporate learning in their connectivity schemes.  Potential Hardware Limitations: While the paper briefly discusses hardware implementation advantages it somewhat glosses over the potential challenges or limitations of deploying such a model in practical applications.  3) Questions and Suggestions for Authors  Datasetspecific Performance: Can the authors provide more insights into why DevFly performs variably across different datasets Are there specific characteristics of these datasets that influence the effectiveness of developmental connectivity  Comparison with Other Methods: It would be beneficial if the authors could include a comparison with other LSH methods especially those that use learningbased or nonrandom connectivity strategies to contextualize DevFlys performance.  Hardware Implementation Details: More details on the proposed hardware implementation and any encountered or anticipated challenges would enrich the discussion and practical applicability of the model.  Future Directions: The paper alludes to the potential applications of DevFly in areas like reinforcement learning. Could the authors expand on how they envision applying their model to these areas and any preliminary findings or hypotheses they might have  4) Limitations and Societal Impact The authors have outlined the limitations related to dataset specificity and have cautioned about the nonoptimal nature of the developmentally generated connections. However a deeper discussion on the implications of these limitations and strategies for mitigation would add value. Moreover while the paper briefly touches on potential positive societal impacts a more thorough examination of the ethical considerations particularly in terms of privacy and bias in automated decisionmaking systems would be prudent. Suggestions for Improvement:  Develop a more nuanced understanding of how DevFlys performance varies with dataset characteristics and explore adaptations to enhance generalizability.  Expand the comparison section to include a wider range of LSH techniques particularly those incorporating learningbased approaches to better position DevFly within the current technological landscape.  Provide a detailed discussion on the challenges and considerations of hardware implementation to guide future practical deployments. In summary the paper presents an intriguing and potentially impactful approach to locality sensitive hashing through the lens of biological development processes. Addressing the mentioned questions and concerns will likely make a strong contribution to the field.", "x0LCDsbJ5JF": "The paper presents a novel approach to learning lightweight yet highly expressive networks for image synthesis and recognition tasks. It introduces the SpatiallyAdaptive SqueezeExcitation (SASE) module extending the lightweight SqueezeExcitation (SE) framework to include spatial adaptiveness for better data specificity. This innovation is tested across various applications showing improvements over existing methods in terms of performance with significantly reduced model sizes. The experiments span lowshot image synthesis with FastGANs oneshot synthesis using SinGANs ImageNet1000 classification with ResNets and MSCOCO object detection using Mask RCNN.  Strengths: 1. Innovative Approach: The introduction of SASE modules for making SE networks spatially adaptive is novel and well justified addressing a gap in the current literature. 2. Comprehensive Evaluation: The paper rigorously evaluates the proposed approach across a variety of tasks and benchmarks showing consistent improvements over existing methods. 3. Significant Performance Gains: Demonstrates better or competitive performance with much smaller model sizes especially in terms of accuracy in image recognition tasks and quality in synthesis tasks. 4. Practical Value: Achieving better performance with significantly smaller models is highly beneficial for practical applications including those in resourceconstrained environments.  Weaknesses: 1. Lack of Theoretical Insight: While empirical results are strong the paper could benefit from deeper theoretical insights into why spatial adaptiveness significantly improves the effectiveness of SE modules. 2. Limited Discussion on Limitations: While the paper mentions weaker performance in object detection and segmentation tasks compared to stateoftheart Transformer models there is scant discussion on the underlying reasons or potential directions for improvement. 3. Potential Overfitting Concerns: The paper could further address concerns related to overfitting especially in lowshot and oneshot learning scenarios beyond the provided memorization assessment.  Questions and Suggestions for Authors: 1. Theoretical Insights: Could the authors provide more theoretical reasoning or intuitions behind why integrating spatial adaptiveness with SE modules leads to such significant improvements 2. Limitations and Future Work: The discussion on limitations and potential for future improvements is somewhat brief. Could the authors elaborate on specific challenges encountered or anticipated in extending this work especially regarding the weaker performance in MSCOCO tasks 3. Generalization across Datasets: How does the proposed method generalize across more varied datasets particularly those with more complex or varied structures than those tested  Limitations and Societal Impact: The authors make a brief mention of the limitations in terms of the models weaker performance on certain tasks and the increased training time for highresolution image synthesis. More comprehensive discussion and potential remedies for these limitations would strengthen the paper. On societal impact while direct negative consequences are not discussed its imperative for future work to consider the broader implications of enhancing image synthesis capabilities including concerns around deepfakes and misinformation.  Suggestions for Improvement:  Expanding upon the theoretical underpinnings of the approach to provide deeper insights into its effectiveness.  More extensive discussion on how the approach can be adapted or improved to tackle its weaker performance in some domains such as object detection and segmentation.  Assessing the approachs performance and robustness across a broader range of datasets and potentially more challenging synthesis and recognition tasks.", "kUnHCGiILeU": " Peer Review  1) Brief Summary The paper presents a novel framework CageNeRF which proposes an innovative approach for deforming and animating neural radiance fields (NeRF) for arbitrary 3D objects without relying on datadependent priors. The method employs a categoryagnostic cagebased representation as a deformation prior utilizing sparsely defined vertices within a cage to perform barycentric interpolation for deforming and animating objects. This maintains geometric details of the target object and is demonstrated to be effective in geometry editing object animation and deformation transfer. The framework consists of two main modules: the neural rendering module that optimizes the radiance field in a canonical space and the cage deformer module that performs the deformation. The authors provide extensive experiments to validate the effectiveness of CageNeRF across various tasks including geometry editing neural animation and pose reenactment.  2) Assessment of Strengths and Weaknesses Strengths:  Generalization and Flexibility: CageNeRF does not require specific datadependent priors making it broadly applicable to various objects and tasks. This enhances the methods generalization capabilities and operational flexibility.  Novel Methodology: The cagebased approach for deforming and animating implicit fields is innovative and effectively addresses the limitations of existing methods that rely on datadependent models.  Comprehensive Evaluation: The authors have conducted extensive experiments to empirically validate the effectiveness of CageNeRF. The comparison with stateoftheart methods and detailed ablation study on MVC sampling strategy enrich the evaluation.  HighQuality Results: The presented results demonstrate CageNeRFs capability to deform and animate objects while preserving geometric details effectively. Weaknesses:  Limited to Noncomplex Deformations: As acknowledged by the authors the method may not be wellsuited for modeling complex deformations such as facial expressions. This limits its applicability to scenarios requiring high fidelity in subtle movements.  Dependence on Accurate Cage Optimization: The quality of deformation heavily relies on the optimization of the cage. Inaccuracies in cage optimization could lead to suboptimal deformations.  Rendering Speed and Computational Resources: The method could be computationally intensive especially for highresolution objects affecting its practical applicability in realtime applications. The paper lacks a discussion on computational efficiency and potential optimizations.  3) Questions and Suggestions for the Authors  Complex Deformations: Could integrating CageNeRF with advanced deformation techniques tailored for complex movements (e.g. facial expressions) mitigate the current limitations in handling subtle deformations  Computational Efficiency: Have the authors considered methods to improve the computational efficiency of CageNeRF particularly for realtime applications Insights into potential optimizations or lighterversion implementations could broaden its applicability.  Automated Cage Construction: Is there scope for automating the cage construction process especially for complex objects to reduce manual effort and potential inaccuracies in cage optimization  Dynamic Textures and Lighting Conditions: Given the limitations in handling dynamic objects with changing colors or varying lighting conditions what are the authors views on integrating CageNeRF with techniques that allow for dynamic texture and lighting adaptation  4) Limitations and Societal Impact The paper acknowledges the limitations related to modeling complex deformations and dynamic objects with changing colors or lighting conditions. A constructive suggestion would be to explore the integration of CageNeRF with existing methods capable of capturing subtle deformations and dynamic scene changes enhancing its applicability and performance across varied scenarios. On the discussion of potential negative societal impact the paper lacks a dedicated section addressing this aspect. Given the broad applicability of 3D content generation in media entertainment and virtual reality its crucial to consider the ethical implications including the potential for misuse in creating misleading or harmful content. Constructive suggestions include implementing safeguards and promoting guidelines for the ethical use of the technology.", "zfQrX05HzBO": " Review of \"A Framework for Continuous Category Discovery (CCD) in Dynamic Settings\"  Summary This paper introduces an innovative approach to address the Continuous Category Discovery (CCD) problem where a model continually receives unlabeled data necessitating simultaneous classification of known categories and discovery of new ones. Unlike existing methods that operate under a static setting this work focuses on dynamic scenarios presenting significant challenges due to the requirement for both classdiscriminative features for classification and diverse features for novel category discovery. To overcome these challenges the authors propose the Grow and Merge (GM) framework that employs a twophase learning process \u2013 growing and merging \u2013 to enhance feature diversity for category discovery while maintaining performance on known classes. The framework distinctly manages to avoid catastrophic forgetting a common issue in continuous training setups by maintaining a set of labeled and pseudolabeled samples from both known and novel categories. The research is validated through extensive experiments under various settings illustrating the frameworks superiority over stateoftheart methods in balancing classification performance and category discovery.  Strengths 1. Novelty and Relevance: The paper addresses a highly relevant and underexplored problem of CCD in realworld dynamic settings a step forward from the traditional static setups in novel category discovery literature. 2. Comprehensive Methodology: The proposed GM framework is well formulated integrating selfsupervised learning for feature diversity and a merging strategy to safeguard against catastrophic forgetting. This methodology is both innovative and methodologically sound. 3. Extensive Evaluation: The authors have conducted an exhaustive evaluation of the GM framework across multiple datasets and scenarios providing a robust proof of the frameworks efficacy. The comparison with stateoftheart methods adds value by clearly demonstrating improvements. 4. Theoretical and Practical Implications: The research bridges a significant gap in the novel category discovery domain by introducing a dynamic learning paradigm. This has substantial implications for developing intelligent systems capable of adapting over time without manual intervention.  Weaknesses 1. Scalability Concerns: While the paper presents extensive experimental results there is limited discussion on the scalability of the GM framework especially in extremely largescale settings or under rapid data inflow scenarios. 2. Domain Adaptability: The adaptability of the proposed method across diverse domains (beyond image datasets) or in scenarios with highly imbalanced data distribution remains unclear. This could limit the general applicability of the findings. 3. Computational Efficiency: The dualbranch architecture and the continuous training phases may introduce significant computational overhead. However the paper lacks a detailed analysis of the computational complexity and efficiency of the proposed framework.  Questions and Suggestions 1. Scalability: Can the authors elaborate on how the GM framework scales with extremely large datasets or with highly frequent unlabeled data streams Are there any optimizations or adaptations recommended for such scenarios 2. CrossDomain Applicability: It would be beneficial if the authors discussed or tested the frameworks performance across different data types or more challenging datasets including imbalanced classes or nonvisual data to understand its versatility. 3. Computational Efficiency: Given the potential computational demands of the proposed method suggestions on optimizing computational efficiency or utilizing more resourceefficient architectures would be valuable. 4. Realworld Applications: Future directions could explore the incorporation of GM into realworld applications highlighting the operational considerations and potential adjustments needed to deploy the framework effectively outside of experimental settings.  Limitations and Societal Impact The paper does not fully address potential scalability issues and the adaptability of the GM framework across various domains. Additionally there is a lack of discussion on the computational costs associated with the proposed method. Constructively exploring methods to reduce computational demands and extending the frameworks applicability would be beneficial paths forward. Moreover discussing the implications of automatic category discovery including the risks of reinforcing biases or incorrect clustering in sensitive applications would strengthen the paper. Suggestions for mitigating such risks could enhance the societal impact of the research.", "pfI7u0eJAIr": " Peer Review  1) Briefly summarize the paper and its contributions The paper addresses the challenge of modeling tabular data with deep learning (DL) techniques particularly focusing on the embedding of numerical features a relatively underexplored area compared to the existing models primarily designed for gradientboosted decision trees (GBDT). It highlights the performance gap between DL models and GBDT in handling tabular data suggesting that the embedding of numerical features can be a pivotal factor in improving DL models performance. The authors introduce two novel approaches for embedding numerical features: piecewise linear encoding and utilization of periodic activation functions. They empirically demonstrate these approaches superiority over traditional embedding methods (e.g. linear layers) across various architectures including MLP and Transformerlike models achieving stateoftheart results on several public benchmarks.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The paper pioneers in focusing on the embeddings of numerical features for tabular data providing fresh insights into enhancing DL models performance.  Comprehensiveness: It offers a thorough comparison between two novel embedding techniques and traditional methods demonstrating the formers effectiveness through extensive experiments.  Accessibility of Resources: By making the source code available the authors ensure transparency and facilitate future research in this direction.  Generalizability: The paper shows that the proposed embedding techniques are not only beneficial for Transformerlike architectures but also enhance simpler models like MLPs. Weaknesses:  Lack of Theoretical Explanation: While the empirical results are compelling the paper lacks a deep theoretical discussion on why and how these novel embedding techniques work better than traditional methods.  Computational Efficiency: The paper does not thoroughly address the computational requirements and efficiency of the proposed methods compared to conventional approaches which is crucial for their adoption in practical applications.  Potential for Overfitting: Given the substantial performance improvement theres a concern regarding the models generalizability and whether they might overfit the datasets used for benchmarking.  3) Please list up and carefully describe any questions and suggestions for the authors Questions:  Can the authors provide more insights into the theoretical foundations behind the effectiveness of the proposed embedding methods  How do the computational costs of these embedding techniques compare with traditional embedding methods  Were any measures taken to prevent the potential overfitting of models given their significantly improved performance on benchmark datasets Suggestions:  It might be beneficial to include a more detailed theoretical discussion on the novel embedding methods potentially offering intuitions or mathematical proofs of their effectiveness.  A thorough analysis of the computational efficiency and scalability of the proposed methods would make the paper more comprehensive and practical.  Consider experimenting with additional datasets including those outside the public benchmark sets to test the generalizability of the proposed methods further.  Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work If not please include constructive suggestions for improvement. The paper does not extensively discuss the limitations or the potential negative societal impact of the proposed embedding techniques. While the methods show promise in enhancing DL models for tabular data understanding their limitations such as computational efficiency and potential biases in data representation is crucial for responsible application. Future iterations of the work could benefit from a section dedicated to these aspects ensuring a balanced view of the novel techniques capabilities and their implications in realworld settings.", "xl39QEYiB-j": " PeerReview of \"Embodied SceneAware Human Pose Estimation\"  1) Summary: The paper introduces an innovative approach named Embodied SceneAware Human Pose Estimation aimed at estimating 3D human poses through a simulated agents understanding of its environment proprioception and thirdperson observations. Unlike conventional methods which rely on multistage optimization noncausal inferencing and complex contact modeling this onestage causal approach leverages a physics simulation and environmental sensors to navigate and interact within scanned scenes accurately. A distinguishing component is the Multistep Projection Gradient (MPG) allowing the simulated agent to adapt movements based on 2D keypoint observations without direct camera pose input. The method achieves excellent results on the H36M and PROX datasets showcasing its robustness particularly on the latter without needing training on its motion sequences. The approach opens new possibilities for realtime applications in mixed reality robotics and surveillance due to its subrealtime operation and reliance on synthetic data for training.  2) Assessment: Strengths:  Innovativeness: The approach is genuinely innovative combining physics simulation environmental sensing and thirdperson observation to estimate human pose in a simulated environment. This integration allows for accurately capturing humanscene interactions a notable advancement over existing methods.  Simplicity and Efficiency: By employing a onestage causal process that works in subreal time the method simplifies the complex process of human pose estimation while ensuring efficiency making it suitable for timecritical applications.  Robustness: The techniques robustness to occlusions and its ability to infer poses from consecutive frames of missing observations are commendable. The methods high performance on the challenging PROX dataset particularly without training on its motion sequences further emphasizes its effectiveness. Weaknesses:  Generalizability: While the results are promising the generalizability of the method across various environments and datasets outside those tested (H36M and PROX) remains unclear. The reliance on accurate 3D scene scans might limit applicability in less structured environments.  Dependence on Synthetic Data: The exclusive use of synthetic data for training though effective raises questions about the methods performance on realworld data with potentially noisier and more complex characteristics.  Complexity in Implementation: Despite the methods efficiency the complexity of setting up the simulation environment tuning the physics engine and generating suitable synthetic data could be challenging for broader adoption without extensive documentation or tools.  3) Questions and Suggestions:  Generalizability: Can the authors comment on the methods performance or potential modifications required for realworld environments where accurate 3D scans are not available  Reallife Data Training: It would be interesting to see experiments incorporating reallife data into training. Could a hybrid training approach using both synthetic and real data improve the methods robustness and applicability  Userfriendly Implementation: Given the complexity in implementing such a system particularly for those not deeply familiar with physics simulation would the authors consider releasing userfriendly tools or detailed guides to facilitate broader use  4) Limitations and Societal Impact: The authors have adequately presented the methods strengths and its potential for realtime applications. However discussing the limitations regarding generalizability and dependence on synthetic data could be expanded. Its also important to consider the ethical implications of deploying such technology in surveillance without safeguards against misuse. Suggestions for Improvement:  A deeper exploration of the methods limitations and potential biases especially in diverse realworld scenarios.  Include an assessment of potential negative societal impacts particularly in privacysensitive applications and propose guidelines or safeguards to mitigate these risks. In summary the paper presents a significant advancement in human pose estimation offering a robust efficient and innovative method that opens new avenues for research and application. However considering its broader applicability and addressing the outlined limitations and ethical considerations will be crucial for its future development and deployment.", "xL7B5axplIe": " Summary The paper introduces Conservative Dual Policy Optimization (CDPO) a novel provably efficient ModelBased Reinforcement Learning (MBRL) algorithm that addresses key limitations in current MBRL algorithms related to overexploration and aggressive policy updates when dealing with models with large generalization error. By implementing a dual procedure of policy optimization  consisting of a Referential Update using a stable reference model and a subsequent Conservative Update that maximizes expected model value within a defined range of randomness  CDPO aims to provide stability global optimality and monotonic policy improvement absent in existing methods like PSRL and algorithms based on optimism in the face of uncertainty (OFU). The theoretical claims are validated with empirical results particularly highlighting CDPOs exploration efficiency and practical advantages in improving convergence speed and achieving lower regret.  Strengths 1. Novelty and Theoretical Contributions: CDPO creatively addresses the challenge of overexploration and aggressive policy updates inherent in nonlinear models within MBRL. The dual update strategy is both innovative and welljustified with rigorous theoretical backing that convincingly demonstrates its potential to achieve global optimality and monotonic policy improvement concurrently a notable feat not guaranteed by prior approaches. 2. Rigorous Analysis: The paper provides a comprehensive and mathematically rigorous analysis of the proposed CDPO framework including proofs of statistical equivalence to PSRL in terms of BayesRegret iterative policy improvement and global optimality. These analyses are thorough and strengthen the argument for CDPOs efficacy. 3. Empirical Validation: The empirical results presented in the paper including comparisons to PSRL and OFUbased algorithms on a variety of tasks effectively demonstrate CDPOs superior exploration efficiency faster convergence and competitive or superior performance against both modelbased and modelfree baselines.  Weaknesses 1. Complexity and Scalability Concerns: While CDPO shows promising results the complexity of implementing the dual update process and the computational demands of optimizing under a reference model and then performing a conservative update could pose scalability issues for extremely large state or action spaces. The paper could benefit from a discussion on computational complexity and strategies to mitigate potential scalability issues. 2. Limited Exploration of Hyperparameter Sensitivity: Although an ablation study on the constraint hyperparameter \\xce\\xb7 is presented a more extensive exploration of the sensitivity of CDPOs performance to various hyperparameters would provide a deeper understanding of the algorithms robustness and ease of tuning. 3. Generalization to Various Environments: While the empirical evaluations cover a decent range of tasks further analysis on a broader spectrum of environments especially those with highly complex dynamics not well captured by current models would help validate the general applicability of CDPO.  Questions and Suggestions  Scalability and Efficiency: Could the authors elaborate on the computational demands of CDPO and potential strategies for improving scalability and efficiency in extremely large state or action spaces  Hyperparameter Sensitivity Analysis: More detailed sensitivity analysis on key hyperparameters including the trust region constraint hyperparameter \\xce\\xb7 and any solverspecific parameters could elucidate their impact on CDPOs performance and robustness.  Application to Complex Environments: Expanding empirical evaluations to include environments with highly complex or chaotic dynamics could further demonstrate CDPOs generalization capabilities and efficacy in challenging scenarios.  Limitations and Societal Impact The authors have adequately discussed theoretical limitations and provided empirical validation though a deeper exploration of computational scalability is warranted. On the potential negative societal impact the paper lacks a discussion. While the direct societal implications of improved MBRL algorithms may not be apparent considering broader applications in autonomous systems healthcare or other areas with significant ethical considerations could enrich the discourse. Constructively the authors could suggest guidelines or safeguards for ethical application and deployment of CDPO in sensitive areas to preempt potential misuse or unintended consequences.", "ptUZl8xDMMN": " Review of \"A Framework for Generalized Graph Scattering Transforms\"  Summary The paper under review proposes a novel framework for designing and analyzing graph scattering networks that extend beyond the conventional graph wavelet approach aiming to bridge the gap with more general graph convolutional networks (GCNs). The main contributions include: 1. A general mathematical framework for constructing graph scattering transforms with variable branching ratios and functional calculus filters rather than relying on predefined wavelet frames. 2. Spectrumagnostic stability guarantees for both nodelevel and graphlevel perturbations including the treatment of vertexset nonpreserving cases using mathematicalphysicsbased tools. 3. Introduction of new methods for graphlevel feature aggregation and the extension of scattering transforms to handle edgelevel and tensorial inputs. 4. Demonstration through numerical experiments that the proposed scattering network design outperforms traditional waveletbased graph scattering approaches and is competitive or superior to current graphbased learning methods in specific tasks.  Strengths 1. Mathematical Rigor: The paper is mathematically comprehensive establishing a solid theoretical foundation for the proposed generalized graph scattering framework. The spectrumagnostic stability guarantees and the treatment of vertexset nonpreserving perturbations are particularly noteworthy contributions that enhance the understanding of scattering network behaviors on graphs. 2. Practical Relevance: Beyond theoretical contributions the paper demonstrates the practical utility of the proposed framework through numerical experiments. The performance improvements observed in social network classification and quantum chemical energy regression tasks indicate the potential of the proposed framework to address realworld challenges. 3. Innovative Methods for Feature Aggregation: The introduction of new graphlevel feature aggregation methods and the extension to edgelevel and higherorder tensorial inputs are significant contributions that expand the applicability of graph scattering transforms making them more versatile in handling different types of graph data.  Weaknesses 1. Complexity and Accessibility: The mathematical complexity of the framework while a strength could also be a barrier to accessibility for practitioners who may find it challenging to implement and adapt the proposed methods without a strong mathematical background. 2. Limited Experimentation: Despite the promising experimental results the scope of the numerical investigations is relatively limited. Expanding the range of datasets and tasks particularly to encompass a broader array of realworld applications could better underscore the versatility and performance advantages of the framework. 3. Comparison with StateoftheArt Models: While the paper demonstrates the superiority of the proposed methods over traditional graphwaveletbased approaches and some graphbased learning methods a more exhaustive comparison with a wider range of stateoftheart models including the latest GCNs could strengthen the argument for the proposed framework\u2019s competitiveness.  Questions and Suggestions for the Authors 1. Implementational Details: Could the authors provide more insights or guidelines on implementing the proposed framework especially for practitioners not deeply versed in the mathematical intricacies Any software or codebase support planned 2. Extension to More Tasks and Datasets: Are there plans to explore the performance of the proposed methods on a wider variety of graphrelated tasks and datasets including largescale problems to further validate the frameworks efficacy and versatility 3. Comparison and Integration with GCNs: How do the authors envision the proposed scattering networks integrating with or complementing existing GCN architectures especially in applications where trainable filters provided by GCNs present clear advantages  Limitations and Societal Impact The authors have made commendable efforts in addressing the limitations related to the structural differences between scattering networks and more general GCNs. However further work is suggested to explore the applicability of the vertexset nonpreserving perturbation framework to a broader range of coarsegraining procedures and clarify the practical steps needed to integrate the proposed methods with existing graphbased learning models. On the societal impact front the caution regarding potential biases in datasets is appreciated and further emphasis on ethical considerations and fairness in applications of the proposed framework would be valuable.  Constructive Suggestions for Improvement 1. Expand the experimental evaluation to include more diverse datasets and tasks providing a broader validation of the frameworks capabilities. 2. Provide clearer guidelines or support tools for implementing the framework lowering the barrier to entry for practitioners. 3. Enhance the discussion on integrating the proposed framework with current GCN architectures including potential synergies and practical implementation pathways. In conclusion the paper makes significant contributions to the field of graphbased learning proposing a novel and theoretically sound framework for graph scattering networks. With further development and broader validation the proposed framework has the potential to significantly impact the design and analysis of graphbased learning models.", "lXuZaxEaI7": " Review of \"Batch SizeInvariance in Policy Optimization Algorithms\"  Summary of the Paper The paper under review presents an innovative approach to enhancing policy optimization algorithms such as Proximal Policy Optimization (PPO) to achieve batch sizeinvariance. The authors identify a critical improvement over traditional policy optimization methods by decoupling the proximal policy which controls the size of policy updates from the behavior policy which is utilized for offpolicy corrections. They introduce variants named PPOEWMA and PPGEWMA (for PPO and Phasic Policy Gradient respectively) which employ an exponentiallyweighted moving average (EWMA) of the policy networks weights to serve as the proximal policy. Through a series of experiments the research validates that this modification allows for more efficient use of stale data and makes the algorithms inherently batch sizeinvariant enabling hyperparameter adjustments to compensate for changes in batch size.  Assessment of Strengths and Weaknesses  Strengths 1. Innovation in Policy Optimization: The concept of decoupling the proximal policy from the behavior policy represents a significant theoretical advancement in policy optimization literature. It challenges traditional notions and opens up new avenues for efficient algorithm design. 2. Batch SizeInvariance: Providing a solution to achieve batch sizeinvariance in policy optimization algorithms is a substantial practical contribution. This property is extremely valuable for adapting to computational constraints while maintaining performance. 3. Experimental Validation: The extensive experiments conducted provide solid evidence supporting the authors claims. The use of the Procgen Benchmark for these experiments adds to the credibility of the findings due to the benchmarks diversity and difficulty.  Weaknesses 1. Hyperparameter Dependency: While the introduction of an exponentiallyweighted moving average (EWMA) addresses batch sizeinvariance it also introduces an additional hyperparameter which may complicate the tuning process. 2. Computational Overhead: Implementing the EWMA variant incurs extra computational overhead. Although addressed the tradeoff between computational efficiency and the benefits of batch sizeinvariance may not always justify the adoption of the proposed approach in every application. 3. Generalization Concerns: The papers focus on two specific algorithms (PPO and PPG) raises questions about how widely applicable this method is across other policy optimization frameworks.  Questions and Suggestions for Authors 1. Hyperparameter Sensitivity: It would be beneficial if the authors could discuss how sensitive the proposed algorithms are to the selection of the decay rate for the EWMA and provide guidance or methodologies for tuning this new hyperparameter. 2. Comparison with Other Methods: Can the authors provide comparisons with other approaches aimed at improving the efficiency and scalability of policy optimization algorithms to contextualize the benefits and drawbacks of their method 3. LongTerm Impact: The paper briefly mentions theoretical implications but a more indepth discussion on how this innovation might influence future research and algorithm design in policy optimization would be valuable.  Limitations and Society Impact The limitations of the study are acknowledged in terms of the added computational overhead and focus on specific algorithms. However the paper could further explore the broader applicability of the method across various policy optimization strategies and its potential negative impacts such as increased energy consumption due to additional computation. Suggestions for future research could include exploring optimized implementations that minimize computational costs and assessing the method\u2019s efficiency and effectiveness in different application areas beyond the Procgen Benchmark. Moreover a discussion on the potential societal impact of deploying batch sizeinvariant algorithms in realworld applications considering both ethical considerations and environmental implications would enrich the paper.", "wlEOsQ917F": " Review of \"A Novel Framework for Stochastic Bilevel Optimization with Global Variance Reduction\"  1) Summary This paper introduces a novel framework for addressing bilevel optimization problems which are prevalent in various machine learning contexts such as hyperparameter selection data augmentation and neural architecture search. The main contribution revolves around the formulation of a new methodology that integrates the simultaneous evolution of the solution to the inner problem the solution to the relevant linear system and the primary variable. This approach named SABA (Stochastic Average Bilevel Algorithm) extends the SAGA algorithm to bilevel optimization achieving unbiased stochastic estimates that facilitate variance reduction across all variables involved. The authors conduct a theoretical analysis demonstrating O(1T) convergence and linear convergence under the Polyak\\xc5\\x81ojasciewicz assumption. Numerical experiments highlight the methods effectiveness compared to existing algorithms.  2) Assessment Strengths:  Novelty: The framework is innovative in its approach to handling bilevel optimization by evolving several variables simultaneously and integrating variance reduction techniques a significant step forward in stochastic optimization methods.  Theoretical Depth and Rigor: The paper provides a solid theoretical foundation for the proposed methods including detailed convergence analysis and conditions under which linear convergence is achieved. This thorough analysis contributes to the credibility and reliability of the presented approach.  Empirical Validation: The extensive numerical experiments across multiple relevant tasks demonstrate the practical effectiveness of the proposed framework especially the superiority of the SABA algorithm in achieving faster convergence and lower suboptimality gaps compared to existing methods. Weaknesses:  Complexity and Scalability: While the proposed approach shows promise the paper could benefit from a more detailed discussion on the computational complexity especially concerning the memory requirements for storing previous variable states in variance reduction. An analysis of scalability to extremely large problem sizes would be useful.  Generalizability: The paper demonstrates the effectiveness of the proposed methods primarily in the context of machine learning applications. Further exploration into the applicability and performance of the framework in other domains of bilevel optimization would be valuable.  Comparison Baseline: Although the paper presents a comparison with various existing methods incorporating more recent advancements in the field of bilevel optimization as baselines could further strengthen the empirical validation section.  3) Questions and Suggestions  Memory Requirement Analysis: Could the authors provide some insights or theoretical analysis regarding the memory overhead involved in storing previous states for the variance reduction mechanism employed by SABA  Application Beyond Machine Learning: Have the authors considered testing the proposed framework on bilevel optimization problems outside the machine learning domain If so sharing preliminary results or insights would broaden the frameworks applicability scope.  Hyperparameter Sensitivity: The performance of stochastic methods often depends significantly on the choice of hyperparameters (e.g. step sizes). Could the authors discuss the sensitivity of SOBA and SABA to their respective hyperparameters and provide guidelines or strategies for their selection  4) Limitations and Societal Impact The authors have outlined the theoretical and empirical strengths of their framework. However discussions regarding the limitations related to computational resources and potential barriers to adoption in realworld applications are somewhat lacking. Addressing these considerations including the computational complexity and memory requirements in more detail would provide a more rounded view of the frameworks practicality. There is no explicit discussion about the potential negative societal impacts. Given the wide applications of bilevel optimization it would be prudent to consider any broader implications especially in sensitive areas such as data privacy in hyperparameter selection or biases in neural architecture search. Improvement Suggestions:  Expanding the discussion on computational and memory complexity possibly including optimizations or approximations that could alleviate these issues without significantly impacting performance.  Incorporating a broader range of problem domains in the experimental validation to demonstrate the frameworks generalizability.  Adding a section discussing ethical considerations and potential negative societal impacts providing a balanced view of the proposed methods.", "vgIz0emVTAd": "Review of \"aDversarIal defenSe with local impliCit functiOns (DISCO): A Novel Defensive Strategy against Adversarial Image Attacks\" 1. Summary and Contributions: This paper addresses the critical area of adversarial defenses in image classification by introducing a novel defensive approach named DISCO (aDversarIal defenSe with local impliCit functiOns). The overarching hypothesis is that adversarial examples lie slightly outside the natural image manifold and thus a defense strategy based on localized manifold projections can robustly clean these images. DISCO employs an encoder to produce perpixel features and a local implicit module that utilizes these features to predict clean RGB values for perturbed pixels. Extensively tested across various benchmarks DISCO not only achieves stateoftheart robustness against known and adaptive adversarial attacks but also showcases efficiency in terms of data and parameters alongside transferability across datasets classifiers and attack types. 2. Assessment of Strengths and Weaknesses: Strengths: a. Novelty and Practicality: DISCO\u2019s novel use of local implicit functions for adversarial defense is both innovative and practical offering a fresh perspective in the adversarial learning domain. b. Robustness and Efficiency: The paper demonstrates DISCOs robust performance against various adversarial attacks and its efficiency concerning computational resources and training data. c. Extensive Evaluation: The thorough experimental setup including a variety of datasets and attacks provides strong evidence for DISCOs effectiveness and versatility. d. Code Availability: Offering the source code enhances reproducibility and facilitates further research exploration. Weaknesses: a. Limited Attack Context: The primary focus is on normbounded attacks. Extensions to other sophisticated attack paradigms such as patch or functional attacks could further validate the robustness of DISCO. b. Potential for Bias: While efficiency in data usage is highlighted there could be underlying bias or overfitting issues not fully addressed especially when transferring the defense across vastly different datasets. c. Computational Demands at Inference: Despite being efficient in training the perpixel processing nature of DISCO might introduce latency during inference particularly problematic for realtime applications. 3. Questions and Suggestions for the Authors: a. Considering Unconventional Attacks: Have you considered evaluating DISCO against more abstract adversarial attacks such as spatially transformed or patch attacks b. Latency Concerns: Can you comment on the inference time of DISCO particularly in comparison to existing methods and suggest optimizations for realtime application scenarios c. Bias and Overfitting Preventive Measures: Could you delve deeper into how DISCO avoids overfitting especially in the context of its demonstrated transferability Additionally offering insights into potential dataset biases that could impact DISCOs performance would be beneficial. 4. Limitations and Societal Impact: The authors acknowledge the limitation of DISCOs evaluation primarily to normbounded attacks implying a need for broader testing against a more diverse array of adversarial strategies. Addressing this limitation could involve extending the evaluation to include nonnormbound attacks and realworld scenarios. On the societal impact front while the defensive capabilities of DISCO could significantly enhance the reliability and security of AI systems theres the dualedged risk that such technologies can also be leveraged by malicious entities to evade detection. Proposing an ethical framework around the deployment of such defenses could serve as a constructive suggestion. In conclusion this paper contributes an important and innovative approach to adversarial defenses in image classification. Despite outlined weaknesses and limitations the strengths in robustness efficiency and thorough evaluation make DISCO a notable advancement in the field warranting further exploration and development.", "mjVZw5ADSbX": "Review for \"Contrastive Neural Text Generation with CONT\" 1) Summary: This paper presents CONT a new contrastive learning framework tailored for neural text generation tasks. Contrastive learning has shown promise in addressing exposure bias and enhancing sequencelevel training signals which are critical for autoregressive decoding in tasks like machine translation and summarization. However its application in text generation has been challenging due to issues with constructing contrastive examples choosing an appropriate loss function and decoding strategies. CONT innovates by generating contrastive examples from the models own predictions employing a Npairs contrastive loss that accounts for the quality difference between examples and integrating the learned sequence similarity score into the decoding stage. The authors validate CONTs effectiveness across five generation tasks on ten benchmarks demonstrating significant improvements over both conventional training frameworks and prior contrastive learning approaches. 2) Assessment: Strengths:  The paper identifies and tackles three critical challenges preventing the broader adoption of contrastive learning in text generation tasks contributing important insights to the field.  The proposed CONT framework shows notable performance improvements across diverse tasks including achieving new stateoftheart results which underscores its practical applicability and potential impact.  The experimental setup is comprehensive covering a wide range of generation tasks and benchmarks and comparing against multiple baseline and stateoftheart models which provide robust evidence for CONTs effectiveness. Weaknesses:  The paper could provide more depth on how the contrastive examples are generated from the models own predictions. While this is a novel aspect of the approach more technical details could help in replicating the method or understanding its potential limitations.  The discussion on the societal impact and limitations of the research is somewhat lacking. While the paper briefly mentions training inefficiency it does not delve into potential misuse of the technology or inherent bias within the contrastive examples it generates.  Although the paper mentions that CONT does not negatively impact decoding speed a more rigorous analysis of computational costs including comparisons with other models and baselines would strengthen the claims regarding its efficiency. 3) Questions and Suggestions:  Could the authors provide more details about the diversity of contrastive examples generated from the models own predictions Specifically how does the quality and variety of these examples influence the models learning  It would be beneficial to explore the impact of the Npairs contrastive loss in more depth. For instance how sensitive is the models performance to the choice of the margin value in the Npairs loss  A discussion on the potential for bias in the generated text due to biased contrastive examples would be insightful. Do the authors have suggestions for mitigating such bias  Considering the significant improvement in text generation tasks it would be interesting to see if CONTs approach could be adapted or extended to other NLP tasks beyond generation such as classification or question answering. 4) Limitations and Societal Impact:  The paper does not sufficiently address the limitations of the CONT framework beyond training inefficiency. For instance there could be limitations related to the scalability of generating contrastive examples or the robustness of the approach against adversarial examples.  The potential negative societal impacts such as misuse for generating misleading information or amplifying biases present in the data are not discussed. Given the significant improvements in text generation capabilities presented it is critical to consider these aspects.  Suggestions for improvement include adding a section that thoroughly examines and discusses possible biases in the generated text and contrastive examples and outlining strategies for minimizing harmful societal impacts such as incorporating fairness constraints or ethical guidelines in the generation process.", "wwWCZ7sER_C": "Review of \"Algorithms with Multiple MachineLearned Predictions\" 1. Summary: The paper delves into the innovative concept of integrating multiple machinelearned predictors into the algorithmic design to enhance performance across a variety of computational problems including mincost perfect matching online load balancing and nonclairvoyant scheduling. These applications have traditionally been explored within the single predictor framework focusing on leveraging predictive models to either surpass worstcase scenario bounds or maintain them when predictions fail. The key contribution of this work is the introduction and analysis of methodologies that not only accommodate multiple predictors but also dynamically select the most efficient predictor during runtime. This aims to optimize algorithm performance based on the best available predictor balancing the tradeoff between the accuracy of predictions and the computational overhead of managing multiple predictors. 2. Assessment: Strengths:  The paper addresses a pertinent and underexplored question within the domain of algorithms enhanced by machine learning predictions: how to effectively harness multiple predictors. This investigation is timely and significant given the burgeoning integration of machine learning with algorithm design.  The authors present a solid theoretical foundation for their proposed algorithms offering rigorous performance bounds that underscore the value of utilizing multiple predictions. This theoretical rigor enhances the credibility and novelty of their work.  Preliminary empirical validation specifically for the mincost perfect matching problem provides a tangible glimpse into the practical relevance of their theoretical findings. This dual approach strengthens the paper\u2019s overall impact. Weaknesses:  The main limitation lies in the breadth of empirical validation. Given the paper\u2019s strong theoretical contributions a more extensive set of experimental results across all discussed problems would have significantly elevated its contribution bridging the gap between theory and practice more comprehensively.  The discussion on the computational overhead introduced by managing and selecting among multiple predictors seems somewhat cursory. A deeper exploration of the tradeoffs involved especially in practice would add valuable insights.  While the paper acknowledges the relevance of expert and bandit learning literatures a more indepth comparison or integration of these ideas could provide a richer methodological background and possibly open up new avenues of exploration. 3. Questions and Suggestions:  Can the authors extend their empirical evaluation to cover the online load balancing and nonclairvoyant scheduling problems Such results would offer a more rounded view of the utility and limitations of their proposed algorithms.  It would be intriguing to see an exploration of how the algorithms perform when predictions are not only inaccurate but systematically biased. Would the proposed mechanisms for selecting among predictors mitigate or exacerbate the impact of such biases  A deeper discussion on the computational and memory overhead associated with maintaining multiple predictors would be beneficial. Are there specific scenarios where the cost of this overhead might outweigh the benefits of improved accuracy 4. Limitations and Societal Impact: The paper provides a balanced view of its contributions and limitations particularly emphasizing the theoretical nature of the work and the limited scope of empirical validation. However the potential societal impact especially the implications of more efficient algorithms for data processing and decisionmaking in various sectors remains unexplored. The authors are encouraged to consider:  Discussing the ethical considerations or possible negative impacts such as unfair advantage or bias amplification that might arise from the application of their algorithms in realworld scenarios.  Providing guidance on the responsible deployment of their algorithms particularly in sensitive applications where prediction errors could have serious consequences. Overall the paper makes a compelling case for the utility of incorporating multiple machinelearned predictions into algorithmic design. Expanding the empirical evidence and addressing the computational costs in more detail would significantly enrich the paper\u2019s contributions.", "zzDrPqn57DL": "Peer Review for \"BEVFusion: A Robust LiDARCamera Fusion Framework for Autonomous Driving\" 1) Summary: This paper introduces BEVFusion a novel framework for fusing LiDAR and camera data in autonomous driving applications specifically for 3D object detection. Unlike existing methods that heavily rely on LiDAR point clouds as queries for fusing image features  which becomes a limitation in case of LiDAR malfunctions  BEVFusion proposes an independent processing of camera and LiDAR streams before their features are fused. This setup allows for robust 3D object detection even when LiDAR data is compromised or missing. BEVFusion has been empirically validated on the nuScenes dataset showing significant improvements over stateoftheart methods under both normal and robust training settings especially in scenarios mimicking various LiDAR malfunctions. 2) Assessment: Strengths:  Novelty and Contribution: The paper addresses a critical limitation in current LiDARcamera fusion methods by proposing a framework that remains functional even in the absence of LiDAR inputs. This significantly enhances the robustness of autonomous driving systems against sensor failures.  Empirical Validation: The comprehensive evaluation on the nuScenes dataset convincingly demonstrates the superiority of BEVFusion over existing stateoftheart methods. The inclusion of robust training settings and scenarios simulating LiDAR malfunctions further underlines the practical relevance of the contributions.  Generalizability: BEVFusions architecture which can incorporate various single modality BEV models showcases flexibility and suggests potential applicability across different architectures and scenarios within autonomous driving tasks.  Open Source: Making the codebase available at the provided GitHub link fosters reproducibility and allows for community contributions and enhancements to the proposed framework. Weaknesses:  Potential Limitations in RealWorld Application: While the paper highlights improved performance in simulated LiDAR malfunction scenarios the correspondence of these simulations to realworld malfunctions and the frameworks performance in varied unpredicted sensor failure scenarios remain less explored.  Evaluation Depth: The paper could benefit from an expanded discussion on the tradeoffs involved in using BEVFusion particularly regarding computational efficiency latency and potential impacts on realtime performance in autonomous driving systems.  Comparative Analysis: A more detailed comparative analysis with concurrent works addressing similar problems could provide better insights into BEVFusions unique advantages and situational preferences. 3) Questions and Suggestions:  Question: How does BEVFusion handle scenarios where both LiDAR and camera inputs might be partially compromised or provide conflicting data  Suggestion: Consider including a more detailed analysis of the computational overhead introduced by BEVFusion especially in comparison to singlestream approaches. This will help evaluate its suitability for realtime applications.  Suggestion: Extending the evaluation to include realworld malfunction scenarios possibly through field trials or more realistic simulations could significantly enhance the papers contributions.  Suggestion: Exploring and discussing the potential of integrating additional sensing modalities (e.g. radar) within the BEVFusion framework could be an interesting future direction. 4) Limitations and Societal Impact: The authors have made commendable efforts to address the limitations of current LiDARcamera fusion methodologies and clearly articulated the advantages of BEVFusion. However a more thorough discussion on the potential limitations particularly regarding realworld deployment challenges (such as computational demands and scenarios with conflicting sensor data) would be beneficial. As for societal impact while the paper itself does not raise direct concerns the broader application of this research in autonomous vehicles must consider the imperative of safety and reliability ensuring that advancements do not introduce new vectors for catastrophic failure. Recommendations for future studies to further investigate and mitigate these potential limitations and risks would strengthen the paper. Conclusion: \"BEVFusion: A Robust LiDARCamera Fusion Framework for Autonomous Driving\" makes significant contributions to the autonomous driving research domain by addressing a key limitation in sensor fusion methods. The framework promises enhanced robustness against sensor failures a critical requirement for the safe deployment of autonomous vehicles. While the evaluation demonstrates impressive improvements a deeper inquiry into the frameworks realworld applicability and broader integration considerations would further solidify its contributions.", "lYHUY4H7fs": "Review of \"EnvyFree Policy Teaching in Markov Decision Processes\" Summary: This paper introduces a novel and intriguing problem at the nexus of policy teaching and fairness termed envyfree (EF) policy teaching. The authors explore a setting where various agents independently navigate a shared Markov decision process (MDP) with unique reward functions and discount rates. A teacher aims to impart a target policy upon these agents by altering their reward functions imposing additional rewards or penalties to encourage or discourage certain actions. Central to the authors\u2019 study is the issue of fairness: ensuring that the modifications to the reward functions do not cause any agent to feel less favorably treated than another i.e. maintaining envyfreeness. The paper meticulously addresses three core aspects of this problem: the existence of EF solutions the efficiency of computing costminimizing EF solutions and the price of fairness (PoF) associated with incorporating fairness into policy teaching. Their findings reveal that while an EF solution may not always exist without penalties such a solution is always attainable when penalties are permissible. Furthermore they demonstrate that finding a costminimizing EF solution is a convex optimization problem and assess how the PoF scales with the geometric sum of discount factors the size of the MDP and the number of agents. Strengths: 1. Originality: The paper addresses a unique and socially relevant issue within the context of multiagent systems and reinforcement learning. The application of the envyfree concept from fair division to policy teaching is both novel and innovative. 2. Theoretical Insights: The development of a theoretical framework to understand characterize and compute EF solutions in policy teaching is rigorous and thorough. The results regarding the existence of solutions computational efficiency and PoF add significant value to the field. 3. Practical Relevance: By illustrating how fairness can be integrated into policy teaching without imposing significant computational or cost burdens the paper offers practical insights that could have implications for designing fair teaching programs in various application areas. Weaknesses: 1. Assumption of Rationality: The paper assumes agent rationality which might not always hold in practical scenarios especially in complex environments or where agents\u2019 perceptions of fairness are influenced by factors beyond their immediate utility. 2. Scope of Fairness Notions: While the paper provides a solid foundation by focusing on envyfreeness there are other dimensions of fairness (like equity efficiency or needbased considerations) that could affect perceptions of fairness in policy teaching but are not explored. 3. Experimental Validation: The paper is predominantly theoretical. Including empirical studies or simulations that demonstrate the practical applicability and impact of the proposed EF solutions across different settings would strengthen the paper. Questions and Suggestions: 1. Extension to Nonrational Agents: Have the authors considered how their framework would adapt to scenarios with boundedrationality agents or those with subjective perceptions of fairness 2. Broadening the Fairness Framework: Could the authors comment on how other fairness principles might be incorporated into their framework or suggest pathways for future research to explore these dimensions 3. Empirical Investigation: Given the theoretical nature of the paper an empirical study or a simulated environment demonstrating the implementation and outcomes of EF policy teaching would be helpful. Can the authors provide insights into potential challenges or considerations in such empirical work Limitations: The authors have explored the limitations and potential negative societal impacts of their work to a reasonable extent acknowledging the parallel between policy teaching and reward poisoning. However a deeper discussion on the limitations of applying the EF concept exclusively and how this might skew outcomes in practice would be beneficial. For instance elaborating on scenarios where overall reward equity might be as significant as procedural fairness in amendments could provide a more nuanced understanding of the model\u2019s applicability. Additionally suggestions for empirical testing and potential obstacles in practical implementation would offer more comprehensive coverage of the limitations. Constructive Suggestions for Improvement: 1. Future iterations of the paper could benefit from a broader conceptualization of fairness considering other principles beyond envyfreeness to accommodate a wider array of teaching scenarios and agent models. 2. The discussion on limitations could be expanded to address potential discrepancies between theoretical models and realworld applications including the impact of agent heterogeneity beyond reward functions and discount rates (e.g. cognitive biases). 3. Integrating a case study or simulated experiments into the paper could significantly enhance its impact by providing concrete evidence of the model\u2019s viability and effectiveness in realworld or nearreal scenarios.", "vRwCvlvd8eA": " Review of \"Chefs\u2019 Random Tables for Approximating Gaussian and SoftmaxKernels\"  Summary The paper introduces a novel approach for approximating Gaussian and softmax kernels through a mechanism called chefs\u2019 random tables (CRTs) which serves as a nontrigonometric alternative to the widely used Random Kitchen Sink (RKS) method. Key innovations include the introduction of Optimal Positive Random Features (OPRFs) which unlike previous methods provide unbiased estimation of softmaxGaussian kernels with positive and bounded random features. The authors present comprehensive theoretical results showcasing the variance reduction capabilities and boundedness of OPRFs and extend their approach to achieve additional variance reduction by incorporating orthogonal random features. Empirical evaluations across various domains including text speech and image data show that the proposed methods achieve new stateoftheart results for lowrank attention Transformers all while maintaining linear space and time complexities.  Strengths 1. Novelty: The paper introduces an innovative set of methods (CRTs) that extend the application of random features (RFs) to include positive and bounded RFs. This is a significant departure from existing methods that either do not guarantee positivity or boundedness. 2. Theoretical Foundation: The authors provide a solid theoretical foundation for the proposed methods. The variance reduction capabilities are thoroughly analyzed and the boundedness of OPRFs is demonstrated both of which are crucial for the stability of Transformer training. 3. Empirical Results: The comprehensive suite of empirical evaluations showcases the practical value of the CRTs. The results indicate significant improvements over existing methods particularly in the context of lowrank attention Transformers without compromising on computational efficiency. 4. Applicability: The techniques applicability to a broad range of tasks including text speech and image classifications indicates its versatility and potential to serve as a foundational technique in numerous machine learning and deep learning applications.  Weaknesses 1. Parameter Optimization: The paper does not provide a clear methodology for choosing theoretically optimal parameters for some of the mechanisms proposed such as GERF and GeomRF. This ambiguity might hinder the practical application and optimization of the proposed methods. 2. Comparison with Recent Methods: While the paper compares the proposed methods with traditional RF approaches a more exhaustive comparison with recent advancements in kernel approximation would offer a clearer picture of the relative advantages and shortcomings. 3. Limitations on Datasets: The empirical evaluation mainly showcases success on benchmark datasets. Further experimentation on diverse and challenging datasets would strengthen the case for the generalizability of the CRTs.  Questions and Suggestions for the Authors 1. Could the authors elaborate on choosing optimal parameters for the GERF and GeomRF mechanisms Are there practical guidelines or automated methods for parameter selection 2. A more detailed comparison with other stateoftheart methods in kernel approximation including nonRF based techniques would be valuable. Are there scenarios where CRTs might underperform 3. It would be beneficial to explore the application of CRTs on more diverse datasets possibly with higher dimensionality or noise levels. Could further experimentation in these areas be considered for future work  Limitations and Societal Impact While the authors touch upon the broader impact of their work particularly in relation to the carbon footprint and societal issues surrounding massive Transformer models a deeper discussion on the possible negative implications such as misuse or bias amplification in AI applications could make the paper more comprehensive. Constructively it would be instructive to see recommendations for responsible use potential safeguards against misuse and exploration of energyefficient training methods. In summary the paper presents a compelling advancement in the field of kernel approximation with positive and bounded RFs supported by strong theoretical analysis and empirical validation. Addressing the outlined limitations and suggestions could further solidify the works impact and applicability.", "lTZBRxm2q5": "Review of \"Efficient Approximation and Model Fitting for Generalized White Noise in Neural Stochastic Differential Equations\" 1. Summary of the Paper: The paper introduces a revolutionary approach to integrate a generalized form of white noise specifically fractional white noise into Neural Stochastic Differential Equations (SDEs). This is achieved by proposing an efficient approximation method for noise sample paths that leverages classical integration methods and sparse Gaussian processes. The authors demonstrate that their model can better capture noise characteristics across various timeseries data improving model fit over existing approaches. Additionally they explore the application of their method in scorebased generative models uncovering instances where their generalized noise leads to superior image generation metrics. The paper is thoroughly grounded in theoretical analysis providing a convergence analysis and touching on potential limitations and extensions of the presented work. 2. Assessment of Strengths and Weaknesses:  Strengths: a. Innovativeness: The paper pioneers the integration of generalized fractional white noise into neural SDE models addressing a gap in modeling more complex noise types beyond traditional Brownian motion. b. Theoretical Rigor: They offer a robust theoretical foundation including a clear formulation of the problem a comprehensive convergence analysis and a thoughtful discussion on the limitations of their approach. c. Practical Implications: The empirical evaluations demonstrate the practical value of the proposed method highlighting its ability to more accurately represent noise characteristics in realworld data and potentially enhance generative model performance. d. Efficiency: The approximations based on sparse Gaussian processes provide a scalable way to model complex noises making the approach practically viable for a wide range of applications.  Weaknesses: a. Limitation in Scope: The current study primarily focuses on a fixed step size in numerical SDE solvers and on the Hurst exponent range. This may restrict the applicability of the proposed methods in scenarios requiring variable step sizes or different stochastic processes. b. Convergence Rate Uncertainty: While the authors provide an initial justification for the convergence of their approximations the precise order of convergence remains unclear which could pose challenges in determining the efficiency of the method in practice. c. Model Complexity: The reliance on sparse Gaussian processes while beneficial for scalability introduces additional complexity in terms of choosing and optimizing inducing points which might require domain expertise in Gaussian processes. 3. Questions and Suggestions for the Authors:  Clarification on Practical Implementations: Could the authors provide more insights or guidelines on how to select the number and distribution of inducing points in sparse Gaussian processes for various applications  Exploration of Variable Step Sizes: Have the authors considered extending their methodology to accommodate variable step sizes in SDE solvers If so what are the potential challenges and solutions identified  Comparison with Other Noise Models: It would be valuable to see a comparative analysis against other models of noise beyond fractional Brownian motion exploring the models adaptability and performance across a broader spectrum of stochastic processes.  Extensions to Multidimensional Cases: Could the authors discuss how their approach could be generalized to multidimensional SDEs and the potential implications for highdimensional data modeling 4. Limitations and Societal Impact: The authors briefly touch on the limitations related to solver step sizes and the specific range of Hurst exponents considered. However a more detailed discussion on the limitations especially regarding the computational complexity and the conditions under which the models assumptions hold would strengthen the paper. As for societal impact the paper primarily focuses on theoretical and methodological contributions without exploring potential negative societal impacts directly. Considering the broad applicability of the proposed method including in financial modeling it would be prudent for the authors to address ethical considerations model misuse scenarios or the impact on decisionmaking processes based on the models predictions. Suggestions for future improvements could include research on adaptive methods for choosing inducing points exploration of the models applicability in broader domains such as climate modeling or healthcare and a deeper investigation into ethical and societal implications.", "unb1wyXf-aC": "The paper under review introduces a novel perceptual superresolution (PSR) technique for 3D neuroimaging named Concurrent Super Resolution and Segmentation (CSRS) aimed at enhancing the detection of brain changes due to neurodegenerative diseases. The CSRS method is distinguished by its dual capability to upsample image intensity channels and associated segmentation labels simultaneously. This characteristic not only enhances the resolution of images but also significantly improves the segmentation accuracy making it particularly suitable for application to existing labeled datasets. A notable challenge the paper addresses is the evaluation of superresolution (SR) methods without highresolution ground truth in clinical neuroimages. The authors counter this by assessing CSRSs effectiveness in quantifying crosssectional and longitudinal changes across various neurodegenerative disorders that demonstrate distinct patterns of brain atrophy.  Strengths 1. Innovative Approach: The integration of superresolution and segmentation in a single framework (CSRS) for 3D neuroimaging is a significant methodological advancement. It shows potential for broader application in medical image analysis. 2. Comprehensive Evaluation: The empirical evaluation of the CSRS method across different neurodegenerative diseases contrasting various 3D PSR loss functions is thorough and methodologically sound. 3. Publicly Available Resources: The commitment to making model weights sample data and training code publicly available postreview is commendable and will facilitate reproducibility and further research.  Weaknesses 1. Lack of Comparison: The paper does not sufficiently compare CSRS with existing stateoftheart methods in 3D neuroimaging SR and segmentation which would have provided a clearer understanding of its relative performance and contribution. 2. Computational Efficiency: Though the paper mentions the computational requirements there is insufficient discussion on the practicality of applying CSRS in clinical settings considering the computational costs mentioned. 3. Limited Validation on RealWorld Data: While the authors have attempted to evaluate CSRS under clinically relevant conditions the validation primarily relies on data from controlled studies. Additional validation on more diverse realworld clinical datasets would strengthen the paper.  Questions and Suggestions for the Authors 1. Comparison with Existing Methods: Could the authors provide additional benchmarks comparing CSRS against other stateoftheart methods in 3D neuroimaging SR and segmentation 2. Robustness to Diverse Data: How robust is CSRS to variations in data quality given the heterogeneous nature of clinical neuroimaging data 3. Potential for Clinical Application: Can the authors elaborate on the potential clinical applications of CSRS and any pilot studies on clinical data  Limitations and Societal Impact The paper discusses the computational demands of the CSRS method and acknowledges the existing limitation of requiring highquality segmentation inputs for optimal performance. However there is a lack of detailed discussion on the potential negative societal impact especially regarding the methods accessibility and practicality in resourcelimited clinical settings. To mitigate this the authors could explore ways to optimize the computational efficiency of CSRS and evaluate its performance on lowerquality inputs which are more common in realworld clinical contexts. Additionally a discussion on ethical considerations especially in the context of diagnosing and monitoring neurodegenerative diseases would be beneficial to ensure the responsible deployment of such technologies.", "wk5zDkuSHq": "Review for the Paper titled \"Extension of Online Agnostic Boosting to Multiclass Classification\" 1) Summary of the Paper: This paper introduces the first algorithm for online agnostic boosting in multiclass classification settings expanding the theory and methodology of boosting from binary to multiclass outcomes. The authors build upon the work by Brukhim et al. (reference [6]) who first extended boosting to online agnostic binary classification using a reduction approach to online convex optimization (OCO). The novelty in this work is the extension of this reduction framework to tackle multiclass problems thereby providing a method to enhance weak learners in scenarios where multiple categories are involved. The approach also facilitates algorithms for statistical agnostic online realizable and statistical realizable multiclass boosting. Empirical analyses demonstrate competitive performance against existing stateoftheart multiclass boosting algorithms offering a compelling case for the efficacy and efficiency of the presented method. 2) Assessment of Strengths and Weaknesses: Strengths: i. The paper addresses a significant gap in the machine learning literature by extending online agnostic boosting to multiclass classification a muchneeded advance given the plethora of realworld applications requiring multiclass decisions. ii. The theoretical foundation is solid building on previous work and extending it thoughtfully. The connection to online convex optimization (OCO) offers a concrete methodological framework that is likely to be of high interest. iii. Empirical results are promising indicating that the proposed algorithm is not only theoretically valid but also practically competitive. iv. The exploration of the methods application to other settings such as statistical agnostic and realizable learning demonstrates the versatility and wide applicability of the proposed approach. Weaknesses: i. The paper could better articulate the implications and limitations of the assumptions made particularly those related to the weak learners and the online convex optimization framework. ii. While the empirical results are encouraging a more extensive comparison with a broader set of benchmark algorithms could further strengthen the evidence for the proposed methods efficacy. iii. The discussion section could offer more insights into the practical implications of the findings including potential application domains where the proposed method could be especially beneficial. iv. The presentation of the theoretical constructs particularly around the formulation of the weak learning conditions could be more accessible to readers less familiar with the mathematical foundations of boosting and OCO. 3) Questions and Suggestions for the Authors: i. Could the authors provide more detailed explanations or examples to elucidate the assumptions related to the weak learners performance in the multiclass setting ii. It would be useful to see comparisons or discussions on how the proposed method performs relative to other stateoftheart multiclass classification approaches beyond boosting algorithms. iii. Is there potential to further optimize the computational efficiency of the algorithm especially for very large datasets or highdimensional multiclass problems iv. Could the authors discuss any considerations or modifications needed when applying their algorithm to domains with imbalanced class distributions 4) Limitations and Societal Impact: The paper somewhat addresses limitations in terms of empirical validation but could benefit from a more thorough discussion on the potential challenges and limitations encountered when extending the framework to multiclass classification. Specifically considerations around scalability applicability to nonstandard class structures (e.g. hierarchical or imbalanced classes) and the robustness of the weak learner assumptions could be further explored. Regarding societal impact the paper does not explicitly discuss the potential negative implications of its findings. While the extension of boosting to multiclass classification is largely beneficial any machine learning technology can have unintended consequences. For example in sensitive applications (e.g. facial recognition hiring tools) the misuse or overreliance on automated decisionmaking systems could amplify biases or introduce unfairness. It would be constructive for the authors to reflect on these aspects offering guidelines or cautionary advice on the responsible use of their proposed methodology. Suggestions for Improvement:  Expand the empirical comparison to include a wider range of multiclass classification algorithms and detailed analysis on computational efficiency.  Enhance the discussion on practical implications potential application domains and consideration for imbalanced datasets.  Incorporate a section on ethical considerations and responsible use to highlight the societal impacts of the technology. Overall this paper makes a significant contribution to the field of machine learning by extending online agnostic boosting to multiclass classification. With some enhancements particularly in discussing limitations applications and societal impacts this work has the potential to serve as a foundational reference in the area.", "voV_TRqcWh": "Review for \"Poisson Flow Generative Models (PFGM)\" 1) Summary The paper introduces a novel deep generative model named Poisson Flow Generative Model (PFGM) which is underpinned by physical principles related to the behavior of electric charges and fields. Leveraging the concept of transforming charge distributions via motion in a viscous fluid it formulates a generative model that transmutes a uniform distribution on a highdimensional hemisphere into any target data distribution. This process is facilitated by interpreting data points as electrical charges in a higherdimensional space and directing their flow through an electric field derived from Poissons equation. The model achieves a bijective mapping between the data and latent space through forward and backward ordinary differential equations (ODEs). Empirically PFGM showcases exceptional performance on image generation benchmarks including CIFAR10 demonstrating superior throughput and robustness compared to existing stateoftheart models. Furthermore it provides a model implementation that proves its efficacy across different resolutions and datasets. 2) Strengths and Weaknesses Strengths:  Innovative Concept: The foundation of PFGM on wellestablished physics principles (the behavior of electric charges in a field) and its application to generative models is highly novel. This approach offers a fresh perspective on designing generative models distinguishing PFGM from other models grounded in statistical or purely mathematical frameworks.  Strong Empirical Results: PFGM achieves toptier performance on recognized benchmarks such as CIFAR10 outperforming current normalizing flow and scorebased models. The accelerated image generation capability (10x to 20x quicker than SDE approaches) without compromising quality is particularly notable.  Robustness and Versatility: The models robustness to estimation errors and its adaptability to various network architectures and step sizes in Eulers method are commendable. These characteristics enhance the practical utility of PFGM in diverse settings. Weaknesses:  Model Complexity: The introduction of an additional dimension and the reliance on backward and forward ODEs contribute to a more complex model setup. This complexity might pose challenges in understanding implementing and optimizing the model especially for practitioners new to the field.  Limited Discussion on Limitations and Societal Impact: While the paper excels in presenting the models strengths it offers limited insight into potential limitations or negative societal impacts. A deeper exploration of these aspects including cases where PFGM might fail or its implications in generating misleading or harmful content would provide a more balanced view. 3) Questions and Suggestions for the Authors  Model Interpretability: Can the authors provide more insights into how the physical analogy of charge movements translates into learned representations in the data space Understanding this could offer further theoretical underpinnings for the models effectiveness.  Scalability and Efficiency: While the paper demonstrates PFGMs scalability to higher resolution datasets can the authors comment on the computational efficiency and resource requirements relative to other models especially in largescale applications  Broader Applicability: Given the impressive results in image generation are there plans to explore PFGMs application in other domains such as text or audio Insights into potential modifications required for these domains would be valuable.  Limitations and Societal Impact: I recommend the authors explicitly discuss any limitations of their approach and consider potential negative societal impacts including misuse scenarios. Adding mitigation strategies for identified risks would strengthen the papers contributions. Limitations and Societal Impact The authors have not adequately addressed the limitations and potential negative societal impacts of their work. It is crucial to explore and candidly discuss scenarios where PFGM might not perform as expected or could be misused for instance in generating deepfakes or propagating misinformation. Constructively the authors could propose guidelines or technical safeguards to prevent misuse and encourage responsible use of their model. Additionally exploring the models performance on diverse or underrepresented datasets could help assess its inclusivity and fairness.", "sWNT5lT7l9G": "Review for \"Constrained GPI for Improved ZeroShot Transfer of Successor Features\" 1) Summary: This paper introduces a novel approach to the zeroshot transfer problem in reinforcement learning focusing on successor features (SFs) and generalized policy improvement (GPI) to enhance transfer to novel tasks that share environment dynamics but have varying reward functions. The authors propose a theorem for bounding the optimal values of novel task vectors that can be expressed as linear combinations of source task vectors. Building on this they develop a constrained GPI approach that improves transfer by constraining actionvalue approximation errors without additional training. The effectiveness of this approach is demonstrated through experiments in several environments showing significant performance gains over traditional GPI. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty and Theoretical Contribution: The introduction of a theorem that allows for lower and upper bounding of optimal values for novel tasks is a significant theoretical contribution. This provides a foundation for the constrained GPI method which is novel in its approach to improving transfer in reinforcement learning.  Practical Application Without Training Modification: The constrained GPI approach innovatively improves transfer performance without modifying the training procedure making it a practical solution that can be applied to existing models.  Empirical Validation: The authors provide comprehensive experimental evidence showing that constrained GPI significantly outperforms traditional GPI across multiple environments. This robust validation strongly supports the proposed methods effectiveness.  Availability of Code: By making their code available the authors facilitate reproducibility and further experimentation by the community enhancing the impact of their work. Weaknesses:  Discussion on Failed Cases: The paper could benefit from a more detailed discussion on scenarios where constrained GPI might fail or underperform providing a more balanced view of the methods applicability.  Comparison with Related Work: While the paper briefly mentions universal successor features approximators (USFAs) and their limitations a deeper comparison with other related approaches could provide readers with a clearer understanding of the proposed methods unique advantages and limitations.  Broader Range of Environments: Although the selected environments are suitable for evaluation extending the experiments to a wider variety of settings including those with more complex dynamics or reward structures could further validate the approachs generality and robustness. 3) Questions and Suggestions for the Authors:  Clarification on Theorem Application: Could the authors elaborate on how the theorems application might vary across different types of environments especially those with complex or nonlinear dynamics  Potential for Broader Applicability: Are there other areas within reinforcement learning beyond transfer learning where the bounded approximation error approach could be beneficial For instance could this method be adapted for improving exploration strategies or learning efficiency  Hyperparameter Sensitivity Analysis: The paper mentions the potential instability in constrained training due to hyperparameter choices. Providing an analysis of hyperparameter sensitivity and recommendations for their selection could enhance the methods practical usability. 4) Limitations and Constructive Suggestions: While the paper presents a compelling approach to improving zeroshot transfer in reinforcement learning the authors could strengthen their work by addressing the following:  Addressing Limitations in Diverse Environments: Expanding the experimental evaluation to include more diverse environments would not only demonstrate the approachs versatility but also help identify any limitations or conditions under which the method may not perform as expected.  Potential Negative Societal Impact: Although the authors do not identify direct negative societal impacts further discussion on the responsible use of reinforcement learning particularly in realworld applications would be valuable. For example considerations around bias in decisionmaking or safety in autonomous systems could add depth to the discussion.  Improvements in Methodology for Bounded Errors: Building on the suggestion for adaptive upper bounding using learned minimum actionvalue functions further exploration into methods for dynamically adjusting bounds based on observed task or environment characteristics could enhance the methods effectiveness and adaptability. Overall this paper makes a valuable contribution to the field of reinforcement learning particularly in addressing the challenge of zeroshot transfer. With the suggested improvements the work could offer even greater insights and applicability.", "ok-SB1kz67Z": "Summary and Contributions: The paper introduces a novel framework known as introspective learning for neural networks structured around two distinct stages: a quick sensing stage typical of existing feedforward neural networks and a more deliberate reflection stage. The reflection stage is innovative in that it uses the gradients of trained neural networks to measure introspection employing a simple threelayered Multi Layer Perceptron (MLP) that predicts based on gradient features extracted from the sensing stage. Key contributions include the definition of implicit introspective questions an efficient methodology for extracting introspective loss gradients and the demonstration of the MLPs robustness and calibration improvement in downstream applications such as active learning outofdistribution detection uncertainty estimation and Image Quality Assessment (IQA). Assessment of Strengths and Weaknesses: Strengths: 1. Innovative Approach: The concept of introducing a reflection stage to neural networks for introspection is novel and offers a promising avenue for improving network robustness and calibration. 2. Comprehensive Testing: The authors executed thorough experiments across various datasets and applications clearly demonstrating the benefits of their approach in terms of increased robustness and improved calibration. 3. Practical Implications: By improving network robustness and calibration the introspective learning framework could be significantly beneficial in realworld applications where reliability and accuracy are critical. Weaknesses: 1. Complexity and Computation Overhead: The reflection stage while beneficial adds complexity and could potentially introduce significant computational overhead especially for larger networks and datasets. 2. Lack of RealWorld Validation: While the paper presents tests on multiple datasets it lacks a realworld case study or application that could better illustrate the practical benefits and limitations of the approach. 3. Potential Bias Reinforcement: The paper mentions that biases in the original feedforward network could be strengthened in the reflection stage but does not provide a detailed discussion or solutions to mitigate this critical issue. Questions and Suggestions for the Authors: 1. Computational Efficiency: Could the authors comment on the computational efficiency of the introspective learning process especially when scaling to larger models and datasets 2. Bias Mitigation: It would be beneficial if the paper could delve deeper into the issue of bias reinforcement in the reflection stage and propose potential strategies for addressing this. 3. Realworld Application: Can the authors provide insights or potential studies on the application of introspective learning in a realworld scenario to better understand its practical implications and limitations Addressing Limitations and Potential Negative Societal Impact: The authors briefly touch on the limitation related to the space complexity of the approach and the potential reinforcement of biases but do not thoroughly explore solutions for these issues. Suggested improvements could include:  Optimization Techniques: Propose and test optimization techniques or architectures that reduce the computational and space complexity of the introspective learning process making it more feasible for largerscale applications.  Bias Detection and Mitigation: Develop and integrate methods for detecting and mitigating biases within the reflection stage to prevent the reinforcement of existing biases thus ensuring fairer and more equitable outcomes.  Experimental Details: To better understand the practical applications and limitations include more detailed experiments or case studies in realworld settings particularly in sensitive areas where model bias and calibration errors could have significant societal impacts.", "o4neHaKMlse": "Peer Review of \"FIBER: A FusionintheBackbone Architecture for Efficiently Pretrained VisionLanguage Models\" 1. Summary: The paper introduces FIBER (FusionIntheBackbonebased transformER) a novel approach in visionlanguage (VL) pretraining that enables efficient handling of both imagelevel and regionlevel understanding tasks without the need for separate model architectures. Unlike previous works FIBER integrates multimodal fusion directly into the image and text backbones through crossattention modules allowing for memory and performance gains. It also proposes a twostage pretraining strategy utilizing both coarsegrained imagetext and finegrained imagetextbox data. The paper presents comprehensive experiments across a variety of tasks demonstrating FIBER\u2019s superiority over existing models in efficiency and performance. 2. Assessment: Strengths:  The integration of crossattention modules directly within the backbones for multimodal fusion is innovative. It simplifies the model architecture and enhances efficiency addressing the limitations of previous VL models.  The twostage pretraining strategy is a significant contribution enabling the model to leverage diverse data types and scales progressively from coarsegrained to finegrained understanding.  FIBER shows improved performance across a wide range of VL tasks with reduced computational costs presenting a compelling argument for its adoption in various applications. Weaknesses:  The paper could benefit from a more detailed discussion on the implications of the twostage pretraining strategy particularly on how it impacts the transferability of the model across tasks with significantly different characteristics.  While the paper addresses environmental concerns and societal biases briefly a more thorough exploration of these aspects especially potential mitigation strategies would strengthen its contributions.  The generalizability of FIBER beyond the tasks and datasets evaluated in the experiments remains unclear. Additional insights into its performance in more diverse or outofdistribution contexts would be valuable. 3. Questions and Suggestions for the Authors:  Could the authors elaborate on potential challenges in applying FIBER to other modalities (e.g. video audio) and suggest avenues for future exploration in this direction  It would be interesting to see a detailed analysis of the model\u2019s behavior on tasks with limited or noisy training data. Are there specific strategies within FIBER that help it adapt to such scenarios  A comparison with recent largescale pretrained models in terms of not only performance but also efficiency and environmental impact would provide a clearer perspective on FIBER\u2019s advantages and areas for improvement.  The paper mentions the potential for societal biases. Could the authors discuss specific examples observed during experimentation if any and propose concrete steps for bias mitigation 4. Limitations and Societal Impact: The authors have provided a brief acknowledgment of the limitations related to societal biases and environmental costs. However a more comprehensive discussion on these challenges is warranted. For societal biases outlining specific instances discovered during the research and detailing the steps planned or taken to address these would enhance the papers impact. Regarding environmental costs exploring strategies for more energyefficient training and deployment practices could set a precedent for future research in the field. Moreover assessing the model\u2019s adaptability to mitigate biases in realworld applications would contribute significantly to its practical utility and societal acceptance. In summary the paper presents a groundbreaking approach to visionlanguage pretraining offering meaningful advancements in model efficiency and performance. Addressing the identified weaknesses and expanding on the limitations and societal impacts could further solidify its contributions to the field.", "mamv07NQWk": " Review of \"Theoretical Insights into Multilabel Classification: Regret Bounds and Algorithms\"  1) Summary and Contributions The paper provided a comprehensive theoretical investigation into multilabel classification (MLC) focusing on establishing upper and lower regret bounds for the Hamming loss and Precision\u03ba metrics. The authors considered two fundamental learning setups: a nonparametric approach using knearest neighbors (kNN) and a parametric one employing empirical risk minimization with convex surrogate loss functions. Central to their analysis was the examination of how label sparsity and low noise conditions affect learning rates and the intricate ways these aspects interact across the two learning frameworks. The papers primary contributions are the establishment of finitesample regret bounds that highlight the computational efficiency of the MLC methods examined. Additionally the work derives regret lower bounds demonstrating the optimality of their results under certain conditions.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Rigor: The paper fills a significant gap in the MLC literature by providing a thorough theoretical analysis of regret bounds that had not been previously established in such depth. The methodology is rigorous and builds logically on existing work.  Practical Relevance: By pinpointing the importance of label sparsity and low noise conditions the research sheds light on practical aspects that can significantly influence the performance of MLC algorithms in realworld scenarios.  Clarity in Results: The main results are clearly stated and contribute both upper and lower bounds for two widely used performance metrics providing a balanced view of the theoretical limitations and capabilities of MLC methods. Weaknesses:  Assumptions Generality: While the assumptions made (e.g. label sparsity low noise) are reasonable and grounded in practical considerations the paper could benefit from a discussion on the generalizability of the results under less restrictive assumptions.  Algorithmic Specificity: Although the paper successfully explores two distinct settings (nonparametric and parametric) it focuses heavily on kNN and surrogate loss functions. An exploration of how the presented results generalize to other MLC algorithms would add value.  Experimental Validation: The paper is theoretical in nature and does not include empirical validations of the theorized regret bounds. Including such validations could strengthen the papers impact by demonstrating applicability to realworld datasets.  3) Questions and Suggestions for the Authors  Generalizability of Assumptions: Can the authors discuss the potential impact of relaxing some of the assumptions made such as label sparsity or the low noise condition on the regret bounds  Extension to Other MLC Methods: How might the theoretical framework presented be extended to analyze other popular MLC algorithms not covered in the paper such as decision trees or neural networks  Empirical Validation: While the papers focus is theoretical providing empirical validation of the derived bounds could significantly enhance its contribution. Is there an intent to include such validation in future work  4) Limitations and Societal Impact The authors have provided a thorough theoretical treatise on MLCs regret bounds and effectively discussed the implications of label sparsity and noise conditions. However there is a lack of discussion on the broader limitations of their findings particularly regarding the practical applicability and robustness of the theory when assumptions do not strictly hold. Additionally the paper does not explore the potential negative societal impacts of MLC algorithms such as biases that might arise from label sparsity in sensitive applications. It would be beneficial for the authors to address these aspects proposing ways to mitigate any adverse effects and suggesting directions for future research that might explore these limitations further. In summary this paper makes significant contributions to the theoretical understanding of MLC though it could be improved by addressing the generalizability of its findings extending its analysis to a broader set of MLC algorithms and providing empirical validation of the theoretical results.", "rWgfLdqVVl_": " Peer Review of \"Visual Concept Tokenization for Disentangled Representation Learning and Scene Decomposition\"  1) Summary This paper introduces a novel unsupervised transformerbased framework named Visual Concepts Tokenization (VCT) designed to abstract visual concepts from images into disentangled visual concept tokens. Each token in this framework represents an independent visual concept achieved through a novel mechanism that leverages crossattention to extract visual concepts from images without the interference between the concept tokens. To ensure and enhance the disentanglement of these tokens the authors introduce a Concept Disentangling Loss. Extensive experiments demonstrate that VCT achieves stateoftheart results in disentangled representation learning and scene decomposition on several popular datasets. Notably the authors also explore the alignment of visual concept tokens with language representations using the CLIP image encoder facilitating image content manipulation via text inputs.  2) Assessment  Strengths:  Innovativeness: The approach of using crossattention without selfattention between concept tokens to maintain their independence appears novel and is a meaningful departure from existing methods.  Comprehensiveness: A thorough experimental setup testing the VCT framework on multiple datasets and against various metrics shows a deep understanding of the challenges and the landscape of disentangled representation learning.  Utility and Generality: The ability of VCT to align visual concept tokens with language representations is of significant practical value opening up avenues for applications that require image content manipulation through natural language.  Weaknesses:  Lack of Detailed Analysis: The paper does not sufficiently discuss the mechanisms that lead to the observed alignment between visual concept tokens and language representations nor does it explore the limitations or potential failure modes of this alignment.  Experimental Transparency: While the results are impressive the paper could benefit from more details regarding the experimental conditions such as hyperparameter settings training details and failure cases to ensure reproducibility and a full understanding of the frameworks capabilities and limitations.  Potential Bias in Data Handling: The paper could further address how the VCT framework might perform in more adversarial or noisy dataset conditions questioning the robustness of the presented approach.  3) Questions and Suggestions for the Authors:  Mechanism Insight: Can the authors provide more insight into why and how the crossattention mechanism without selfattention leads to such effective disentanglement of visual concepts  Failure Modes: Are there specific types or categories of images where VCT does not perform as well Discussing these would help understand the methods limitations.  Language Alignment Extension: How might the alignment with language representations extend to languages other than English or to more complex text inputs beyond simple descriptors  Data Diversity: Could the authors comment on the robustness of VCT to datasets with significant intraclass variance or with noisy labels  4) Limitations and Societal Impact: The authors briefly touch on the potential negative societal impacts through malicious uses which is appreciated. However a more detailed discussion on specific negative impacts such as deepfakes or misinformation stemming from the ease of manipulating image content through language would be valuable. Additionally its not clear if the limitations of the approach particularly regarding data diversity and potential biases in visual concept abstraction are thoroughly addressed. Suggestions for improvement include:  Expanding the datasets used in experiments to include more diverse and potentially noisy datasets to assess the frameworks robustness.  Proposing methods or modifications to mitigate potential biases in visual concept abstraction which might arise due to the training datas limitations. Overall this paper presents a significant contribution to the machine learning domain by proposing a novel approach for disentangled representation learning and scene decomposition. The integration with language representation is particularly intriguing but warrants a more detailed investigation to fully understand its implications and limits.", "lArVAWWpY3": "Review of \"Statistical Robustness and Computational Aspects of Sliced Wasserstein Distances\" 1) Summary of the Paper: This paper investigates the scalability of sliced Wasserstein distances specifically focusing on their empirical convergence rates robustness to data contamination and computational efficiency. The authors address the challenges in estimating and computing sliced Wasserstein distances in high dimensions offering theoretical contributions and numerical validations. They establish fast empirical convergence rates for sliced distances that explicitly consider the dimensionality of the data. For robust estimation the paper characterizes minimax optimal robust estimation rates that demonstrate improved scalability by removing dependency on the datas dimensionality. Additionally the study provides formal guarantees for popular computational methods employed to calculate sliced Wasserstein distances demonstrating surprising behaviors in convergence rates and efficiency improvements in higher dimensions. These contributions are supported by numerical experiments enhancing the understanding of sliced Wasserstein distances in statistical inference and machine learning applications. 2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper fills significant gaps in the literature by deriving sharp empirical convergence rates for sliced Wasserstein distances proving minimax robust estimation rates and offering formal guarantees for computational methods. These contributions significantly advance the understanding of sliced distances in highdimensional spaces.  Practical Implications: The findings have direct implications for various applications in machine learning and statistical inference enhancing the utility and scalability of sliced Wasserstein distances in these domains.  Rigorous Analysis: The theoretical results are supported by rigorous proofs and a thoughtful exploration of the assumptions such as logconcavity that underpin the analysis.  Empirical Validation: The numerical experiments not only confirm the theoretical findings but also reveal insightful behaviors such as the dimensionalitys impact on convergence and computation. Weaknesses:  Assumption Constraints: While the logconcavity assumption is central to many of the derived rates and bounds it limits the papers findings applicability to distributions that satisfy this condition. The practical impact of this constraint is not deeply explored.  Complexity of Computation Methods: The computational guarantees provided primarily focus on convergence rates and do not fully address the practical computational cost or scalability especially for realworld largescale datasets.  Limited Scope of Numerical Experiments: Although the empirical results are enlightening the experimental scope is somewhat narrow. Expanding the diversity of datasets and settings could strengthen the validation of the theoretical claims. 3) Questions and Suggestions for the Authors:  Could the authors comment on the practical implications of the logconcavity assumption for realworld datasets that might not satisfy this criterion  It would be helpful if the authors could provide more insight into the practical computational costs of the methods analyzed possibly through a broader set of numerical experiments or more detailed theoretical analysis regarding algorithmic complexity in realworld scenarios.  The paper could benefit from a discussion on potential extensions or modifications to the explored estimation and computational methods to accommodate distributions not satisfying the logconcavity assumption.  Given the advancements made are there specific machine learning or statistical inference tasks where the authors anticipate their findings will have the most significant impact 4) Limitations and Societal Impact: The authors have made an effort to address the limitations inherent in their work particularly through the assumptions made regarding distributional properties like logconcavity. However a more detailed exploration of the practical limitations of their methods when applied to datasets that do not meet these assumptions would be beneficial. Additionally while the paper does not explicitly discuss potential negative societal impacts it is worth noting that any tool or method that enhances data analysis capabilities must be used responsibly with awareness of privacy bias and ethical considerations especially in sensitive applications. Suggestions for improvement include expanding the discussion on the broader applicability of the results and incorporating considerations for ethical usage and potential biases in statistical and machine learning tasks leveraging sliced Wasserstein distances. Conclusion: This paper is a substantial contribution to the body of literature on Wasserstein distances addressing critical questions regarding their scalability robustness and computability in highdimensional spaces. Despite some limitations and areas for further exploration the theoretical insights and empirical validations presented significantly advance the understanding and application of sliced Wasserstein distances in statistical inference and machine learning.", "klElp42K9U0": "Review Report: 1) Summary of the Paper: This paper introduces a novel pipeline named SplitSelectRetrain (SSR) designed for automatic algorithmhyperparameter (AH) selection in the context of offline reinforcement learning (RL) particularly focusing on scenarios with limited data. The foundation of SSR lies in applying repeated random subsampling (RRS) to split the dataset multiple times to counteract the variability issues faced in training and validating RL policies due to data sporadicity and partition sensitivity. By averaging the offline policy evaluation (OPE) estimates across these splits the pipeline selects the AH pair that demonstrates the best average performance and uses this chosen configuration to retrain on the entire dataset for the final policy deployment. The robustness and effectiveness of the SSR pipeline are illustrated through extensive experiments across varied domains including healthcare (sepsis treatment) education (TutorBot) and robotics (Robomimic and D4RL datasets). 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty and Generality: The SSR pipeline represents a novel approach to handling the challenge of AH selection in offline RL especially in scenarios plagued by limited data availability. Its general applicability across a wide range of domains enhances its appeal.  Empirical Validation: The comprehensive evaluation spanning multiple domains and the comparison against several baseline methods (including OneSplit OPE BCa CV and BVFTFQE) demonstrate the superiority of the SSR pipeline in consistently selecting more effective policies.  Addressing a Critical Challenge: The paper tackles the critical issue of dataset partition variability and its impact on offline RL training and evaluation providing a practical solution that can significantly benefit scenarios where online policy tuning is impractical. Weaknesses:  Computational Demand: The SSR approach particularly due to the need for repeated sampling and evaluation across multiple splits might introduce significant computational overhead especially as the dataset size or the number of AH pairs increases.  Methodological Assumptions: While the paper emphasizes the generality of the SSR pipeline there might be implicit assumptions (e.g. about the distribution of the data or the responsiveness of certain RL algorithms to the evaluation metrics used) that could limit its efficacy in some realworld scenarios not covered in the experimental domains.  Lack of Theoretical Bounds: The paper could benefit from a deeper theoretical analysis particularly in providing bounds on the performance improvement or the reliability of the policy selected using the SSR method. 3) Questions and Suggestions for the Authors:  Clarification on Computational Requirements: Can the authors provide more details on the computational overhead of the SSR pipeline especially in comparison to the baseline methods Suggestions for mitigating these computational demands could enhance the practicality of the approach.  Extension to Larger Datasets: How does the SSR pipeline perform with significantly larger datasets An analysis or discussion on scaling the approach and its impact on performance and computational efficiency would be valuable.  Exploration of Theoretical Properties: Including theoretical discussions on the robustness of the SSR approach such as bounds on performance variance across different data splits could enrich the paper. Are there theoretical guarantees for the selection of an optimal or nearoptimal policy  Consideration of Different OPE Methods: The choice of OPE method is critical in the SSR pipeline. Could the authors elaborate on how the pipeline might adapt or respond to the use of different OPE estimators especially in more complex or less conventional domains 4) Limitations and Societal Impact: The authors should consider discussing the limitations related to the scalability of the SSR pipeline including computational demand and potential biases introduced by the choice of OPE estimators. While the paper presents a promising approach for offline RL acknowledging these limitations openly could guide future research efforts more effectively. Moreover a discussion on the potential negative societal impacts such as biases in policy outcomes due to data imbalances or misuse in sensitive applications would be a responsible addition. To mitigate these suggestions could include promoting transparency in the AI selection process and integrating fairness considerations into the pipeline\u2019s evaluation criteria.", "p0LJa6_XHM_": " Peer Review for \"Sleeper Agent: A Hidden Trigger Backdoor Attack for Neural Networks\"  1) Brief Summary The paper presents \"Sleeper Agent\" an innovative backdoor attack for deep neural networks. Unlike traditional backdoor attacks that insert a trigger directly into training data Sleeper Agent embeds a hidden trigger without altering training labels or making visible modifications. The attack is based on gradient matching data selection poison selection and adaptive retraining aiming to craft poisons effective against neural networks trained from scratch. This method demonstrates significant success in bypassing existing defenses on challenging datasets like ImageNet and CIFAR10 in both blackbox and graybox settings.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Methodology: The gradient matching and adaptive retraining mechanisms are novel contributions that significantly advance the efficacy of hidden trigger backdoor attacks in fromscratch training scenarios.  Effectiveness: Demonstrated through rigorous experiments Sleeper Agent proves to be effective across a variety of architectures and data sets significantly outperforming existing methods.  Defense Robustness: The paper provides extensive analysis of the attacks resilience against multiple defense mechanisms highlighting the challenge it poses to current detection strategies. Weaknesses:  Complexity and Overhead: The requirement for multiple rounds of optimization and retraining adds computational overhead which may limit scalability or practical application in larger or more complex models.  Potential Ethical Concerns: While the paper discusses the broader impact there\u2019s a need for a deeper analysis of ethical implications and misuse especially considering the effectiveness of the method.  Limited Discussion on Countermeasures: Though various defenses are tested there\u2019s limited exploration of potential countermeasures specifically strategies that might effectively mitigate the attack without significant performance degradation.  3) Questions and Suggestions for Authors  Scalability and Efficiency: Can the authors elaborate on the computational resources required for the Sleeper Agent attack compared to traditional methods Suggestions for optimization could enhance the papers impact.  Ethical Safeguards: Given the attacks potential for misuse could the authors discuss guidelines or safeguards that could prevent or limit its unethical application  Expand Countermeasure Analysis: It would be beneficial to explore or propose specific countermeasures or defense strategies tailored to Sleeper Agent providing a more rounded view of the attackdefense landscape.  4) Limitations and Societal Impact The authors present an initial discussion on the broader implications of their work but may not fully cover the spectrum of ethical considerations and potential negative societal impacts. Especially considering the high effectiveness against current defenses it\u2019s crucial to deeply reflect on how such a powerful method could be misused in realworld scenarios. Constructive suggestions include:  Implementing Ethical Frameworks: Propose an ethical framework or set of guidelines for responsibly researching and deploying such attack methodologies.  Engaging with the Security Community: Collaboration with cybersecurity researchers to develop robust defenses against Sleeper Agent and similar hidden trigger attacks.  Transparency and Disclosure: Encourage transparency in sharing findings with affected parties including developers of potentially vulnerable systems to proactively mitigate risks. Overall while the paper introduces a significant advancement in the domain of dataset manipulation and neural network vulnerabilities further discussion around mitigation strategies and ethical considerations would strengthen its contribution to the field.", "sFapsu4hYo": " PeerReview  1) Brief Summary The paper introduces a novel dynamic sparse training algorithm focused on sparse models with shuffled block structures. Such structures are not only hardwarefriendly but also claim to reduce training overhead significantly. By dynamically adjusting the model with a blockaware drop and grow criteria based on shuffled blocks of active parameters the authors attempt to maintain or even surpass the accuracy of dense models while achieving sparsity levels up to 0.75. The method is tested across several networks (ResNet50 ResNet18 VGG16 etc.) and datasets (ImageNet CIFAR10100) showing promising results in maintaining dense accuracy levels at certain sparsity ratios and improving training efficiency.  2) Strengths and Weaknesses Strengths:  Innovative Approach: The papers method of integrating shuffled blocks within the dynamic sparse training regime is novel and addresses both the efficiency and the accuracy concerns seen in previous sparse training attempts.  Comprehensive Evaluation: It provides extensive experimental validation across multiple architectures and datasets showing the algorithms effectiveness and robustness.  Hardware Efficiency: By focusing on structured sparsity it significantly improves hardware efficiency which is crucial for practical deployment of large models. Weaknesses:  Lack of Theoretical Backing: While the empirical results are strong the paper does not provide a theoretical foundation for why shuffled block structures perform well.  Comparisons and Baselines: Although the paper compares its results with several other methods it somewhat lacks a detailed discussion on how it compares with the most recent stateoftheart sparse training methods.  Potential Overfitting to Tested Networks and Datasets: The methodology is tested on popular networks and datasets but its unclear how generalizable the method is to other architectures or novel tasks.  3) Questions and Suggestions for Authors  Regarding Generalizability: How does the algorithm perform on other architectures or datasets especially those that might have different characteristics from ImageNet or CIFAR  Theoretical Grounding: Can the authors provide any theoretical insights or intuitions into why shuffled block structures might be more effective compared to other sparsifying strategies  Hardware Adaptability: Given the methods hardware efficiency claim how does the method perform across different hardware setups including older or nonNvidia GPUs  Future Directions: What are the authors views on further improving the proposed algorithm Are there specific areas (e.g. block size optimization layerwise sparsity adjustments) that could be explored  4) Limitations and Improvement Suggestions The authors have made an effort to discuss the limitations regarding the need for specific block sizes for optimal performance and the overhead of shuffled blocking. However a more profound discussion on the potential negative societal impacts such as environmental considerations from training large models even if made more efficient and the risk of promoting models that might be efficient yet contribute to biased outcomes due to sparse training could enrich the paper. Improvement Suggestions:  Broader Limitation Discussion: Expand the discussion on limitations to include broader societal impacts and potential mitigation strategies.  Algorithmic Generalizability: It would be beneficial to explore the methods adaptability to newer or less conventional network architectures and tasks to demonstrate its versatility further.  Further Optimization: Investigating automated methods for determining optimal block sizes or a more dynamic approach to adjusting sparsity levels during training could yield even better efficiency and accuracy results. In conclusion the paper presents a significant step forward in dynamic sparse training offering a novel method that strikes a balance between model efficiency and accuracy. With some enhancements in theoretical justifications broader comparisons and a more indepth analysis of limitations this work could pave the way for more efficient and practical implementations of large neural networks.", "nrksGSRT7kX": "Peer Review of \"Robust Adversarial ModelBased Offline RL (RAMBO): A Novel Approach to Offline Reinforcement Learning\" 1) Summary of the Paper: This paper introduces Robust Adversarial ModelBased Offline Reinforcement Learning (RAMBO) a novel approach to offline RL that addresses model exploitation and distributional shift issues inherent to offline settings. RAMBO utilizes a twoplayer zerosum game formulation against an adversarially optimized environment model guiding policy optimization to favor conservative actions in areas not covered by the dataset. The key contributions include the integration of adversarial dynamics modification to enforce conservatism in learned policies and the theoretical grounding of RAMBO which provides PAC performance guarantees and introduces a pessimistic value function that lower bounds true environment values. The authors demonstrate RAMBOs superiority over stateoftheart baselines through evaluations on the D4RL benchmarks. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: RAMBOs approach to modifying transition dynamics adversarially represents a novel contribution to offline RL offering a fresh perspective beyond uncertainty estimation and reward penalties.  Theoretical Foundations: The paper provides a strong theoretical basis for RAMBO with PAC guarantees and a theoretically justified pessimistic value function enhancing the reliability of the approach.  Empirical Performance: Experiments demonstrate that RAMBO outperforms existing stateoftheart methods on widely used benchmarks indicating its practical effectiveness.  Lack of Uncertainty Estimation Requirement: By not relying on uncertainty estimation which can be unreliable especially in neural network models RAMBO addresses a significant limitation of prior modelbased offline RL methods. Weaknesses:  Complexity and Implementation Challenges: The alternating optimization of policy and adversarial model alongside maintaining model constraints could introduce complexity in implementation and tuning.  Limited Evaluation on HighQuality Datasets: RAMBO appears less effective on highquality datasets as noted in the comparison with simpler approaches like behavior cloning on these datasets.  Domain Limitations: While RAMBO shows promising results in locomotion and control benchmarks its performance on more complex tasks with sparse rewards such as AntMaze is less compelling. This might limit its applicability across a broader range of offline RL problems. 3) Questions and Suggestions for the Authors:  Hyperparameter Sensitivity: Given the performance sensitivity to rollout length and adversarial weighting mentioned can the authors elaborate on the robustness of RAMBO across a wider range of settings and offer guidance on hyperparameter selection  Performance on HighQuality Datasets: Could the authors discuss potential enhancements to RAMBO that might improve its utility on highquality datasets without compromising its advantages in other settings  Comparison with StateoftheArt: While the empirical evaluations are comprehensive further insights into RAMBOs performance against closely related methods on tasks where RAMBO is less effective could provide deeper understanding. Could such analyses be included  Feasibility and Implications of Extensions: The conclusion mentions potential extensions (e.g. application to imagespace domains). Can the authors further speculate on the feasibility and expected challenges of these extensions 4) Limitations and Societal Impact: The authors have not thoroughly discussed the limitations and potential negative societal impacts of their work. While RAMBO presents a sound theoretical and empirical advance in offline RL discussions around its adaptability to realworld applications where conservative policy optimization is crucial and potential unintended consequences in those scenarios are lacking. For example overly conservative policies in critical systems could result in suboptimal responses to rare but significant events. Constructive Suggestions for Improvement:  Broader Evaluations: Extend evaluations to include more complex and practical scenarios possibly through collaborations with domainspecific experts to assess RAMBO\u2019s applicability and limitations in realworld settings.  Impact Discussion: Augment the paper with a section discussing potential societal impacts including scenarios where RAMBOs conservative nature might both benefit and limit decisionmaking in critical applications.  Transparency on Implementation: Provide detailed pseudocode and considerations for practical implementation assisting in the reproducibility and adoption of RAMBO in diverse settings. Overall this paper presents a significant contribution to the field of offline reinforcement learning introducing a theoretically grounded and empirically validated approach that advances the state of the art. Addressing the identified weaknesses and considerations could further enhance its value and applicability.", "rOimdw0-sx9": "Peer Review for \"Adaptive Distributional Robustness in MetaReinforcement Learning\" 1) Summary of the Paper This paper presents a novel framework designed to improve the resiliency and adaptability of metareinforcement learning (MetaRL) in the face of distribution shifts in task environments. Given the dynamic and unpredictable nature of realworld scenarios the ability for MetaRL algorithms to generalize beyond their training distribution is crucial. The authors propose a strategy for training a variety of metapolicies against different levels of anticipated distribution shift thereby enabling the algorithm to select the most appropriate metapolicy under varying test conditions. This approach is grounded in the concept of distributional robustness extended here to operate adaptively across different levels of task distribution shifts. They introduce a specific instantiation of their framework named DiAMetR and assess its performance against existing MetaRL strategies in simulated robotics tasks. Their experiments demonstrate DiAMetRs superior adaptability to both insupport and outofsupport distribution shifts without sacrificing performance on tasks within the training distribution. 2) Strengths and Weaknesses Strengths:  Novelty and Relevance: The paper addresses a significant and timely challenge in MetaRL namely the robustness of learned policies under varying task distribution shifts. This is a critical issue for the deployment of RL agents in realworld applications.  Methodological Rigor: The approach is theoretically grounded in distributional robustness principles and extends these with a practical and innovative adaptation mechanism for MetaRL.  Empirical Evaluation: The comprehensive test suite across multiple simulated robotic environments demonstrates the effectiveness and generalizability of the proposed method. Weaknesses:  Complexity and Scalability: The method involves training and maintaining multiple metapolicies and using a bandit algorithm for policy selection at test time. This complexity might pose scalability and efficiency issues in more extensive or more complex task settings.  Assumption on Distribution Shift Range: The method assumes the distribution shift range is known a priori which may not always be practical. This assumption might limit the methods applicability in entirely unpredictable environments.  Analysis Depth: While the paper presents a quantitative analysis of performance improvements a deeper exploration into why certain policies are selected under specific distribution shifts and how these decisions affect overall adaptability would enrich the paper. 3) Questions and Suggestions for the Authors  Clarification on Scalability: Can the authors elaborate on how the proposed method scales with the number of tasks or complexity of the task environment It would be beneficial to understand any computational or practical limitations.  Beyond Assumed Distribution Shifts: How does DiAMetR perform when the actual distribution shift exceeds the range anticipated during training Exploring this scenario could highlight the methods robustness limits.  Indepth Analysis of Policy Selection: Providing more insights into the policy selection mechanism during test time\u2014specifically examples or case studies where certain policies were more effective for specific shifts\u2014would be valuable for understanding the adaptation process. 4) Limitations and Societal Impact The paper could benefit from a more detailed discussion of the limitations especially regarding the assumptions made about the knowledge of distribution shift range and the potential scalability issues for complex or highdimensional tasks. Additionally while the focus on improving adaptability in MetaRL is commendable consideration of potential negative societal impacts\u2014such as misuse in autonomous systems or in scenarios where overly conservative behavior could have unintended consequences\u2014would make the discussion more comprehensive. Suggestions for future work could include exploring more efficient algorithms for policy selection and expanding the framework to handle unknown distribution shifts dynamically. Overall this paper contributes significantly to the field of MetaRL by addressing a critical challenge of distributional shift robustness. With further exploration of its limitations and broader implications this work has the potential to impact the deployment of RL agents in a variety of realworld settings.", "nJt27NQffr": " Peer Review of \"Maximum Entropy Coding for Generalizable SelfSupervised Learning\"  1) Summary of the Paper and its Contributions The paper introduces Maximum Entropy Coding (MEC) for selfsupervised learning (SSL) to address the biases introduced by conventional pretext tasks aiming to enhance the generalizability of learned representations to various downstream tasks. Unlike traditional approaches that rely on handcrafted pretext tasks leading to biases MEC utilizes the principle of maximum entropy from information theory proposing that generalizable representations should maximize entropy among plausible representations. To operationalize this the authors optimize the minimal coding length in lossy data coding as a surrogate for entropy presenting a scalable reformulation for efficient computation. The methodology is validated across a broad range of tasks showing superior performance and offering a unified perspective on existing SSL techniques suggesting that batchwise and featurewise objectives are loworder approximations of MEC.  2) Assessment of Strengths and Weaknesses  Strengths:  Theoretical Novelty: The paper is theoretically innovative extending the principle of maximum entropy to SSL a move that could influence future research directions within the field.  Generalizability: MEC convincingly demonstrates improved generalization across a wide spectrum of downstream tasks which is a substantial advancement over existing SSL approaches.  Scalability: The reformulation of the coding length function for tractable entropy estimation enables largescale pretraining addressing a critical computational challenge.  Comprehensive Evaluation: The exhaustive experimental setup covers diverse tasks showcasing MECs broad applicability and setting a robust benchmark for future SSL methods.  Weaknesses:  Lack of Theoretical Boundaries: While the reformulation for computational efficiency is practical the paper does not thoroughly explore the theoretical implications or limitations of approximating entropy with coding length.  Potential Overfitting: The paper does not discuss the risk of overfitting to the new objective especially in contexts with limited or highly specific data distributions.  Broader Impact Considerations: The assessment of societal impacts or misuse of generalizable representations learned via MEC is notably absent.  3) Questions and Suggestions for the Authors  Theoretical Insights: Can the authors provide more insight or theoretical backing on the choice of minimal coding length as the optimal surrogate for entropy in the context of SSL  Framework Applicability: How does MEC perform in scenarios with significantly limited data or in tasks with unconventional data distributions Are there any limitations in these contexts  Mitigation of Overfitting: Could the authors suggest any strategies or modifications to the MEC framework to prevent potential overfitting to the maximum entropy objective  Integration with Other SSL Approaches: Given MECs compatibility with existing SSL methods could further research explore a more integrated or hybrid approach combining MEC with other novel SSL objectives for enhanced performance  4) Limitations and Societal Impact The authors did not thoroughly discuss the theoretical limitations of their surrogate entropy measurement or the potential overfitting when optimizing for maximum entropy. Additionally the paper lacks a detailed exploration of MECs societal impacts both positive and negative particularly concerning data privacy and the potential for misuse in generating deepfakes or other deceptive media.  Constructive Suggestions for Improvement  Theoretical Exploration: A deeper dive into the theoretical foundations of using minimal coding length as an entropy surrogate could strengthen the paper\u2019s contributions and address potential criticisms.  Discussion on Societal Impacts: Adding a section that proactively discusses the ethical considerations potential negative uses and strategies for mitigating undesired impacts would make the research more holistic and responsible.  Guidelines for Avoiding Overfitting: Providing explicit guidelines or additional mechanisms within the MEC framework to prevent overfitting especially in specialized applications would enhance its robustness and applicability. Overall the paper makes significant contributions to the field of SSL with its innovative approach towards learning generalizable representations. Addressing the highlighted theoretical and practical concerns could further elevate its impact and applicability.", "uIXyp4Ip9fG": "Review of \"Active Surrogate Estimators for LabelEfficient Model Evaluation\" 1. Summary of the Paper The paper introduces a novel method named Active Surrogate Estimators (ASEs) for labelefficient model evaluation emphasizing the challenge in evaluating model performance when labels are scarce and expensive to acquire. ASEs leverage a surrogatebased estimation approach that interpolates errors of points with unknown labels aiming for active testing with enhanced label efficiency. The proposed method distinct from Monte Carlo estimatorbased approaches actively learns a surrogate model to directly predict losses for both observed and unobserved points. Additionally the paper introduces XWED an innovative acquisition strategy designed to augment the learning process by focusing on points most beneficial for the final estimation task. Through theoretical insights and application to deep neural network evaluation on image classification tasks the paper demonstrates that ASEs outperform current stateoftheart methods in label efficiency. 2. Assessment of the Paper Strengths:  Originality: The concept of ASEs introduces a fresh perspective to labelefficient model evaluation targeting both theoretical advancement and practical efficiency in label use.  Methodological Rigor: The paper presents a comprehensive approach encapsulating the development of ASEs the novel acquisition strategy XWED and a thorough theoretical analysis of ASE errors. The inclusion of adaptive refinement of ASEs through active learning strategies further demonstrates methodological depth.  Practical Relevance: The application of ASEs to the challenging arena of deep neural network evaluation presents a significant advance. The empirical results showcasing superior performance over existing methods underline ASEs potential practical impact.  Theoretical Contributions: The theoretical insights into ASE errors broken down into components and assessed separately provide a solid foundation for understanding the estimators performance nuances. Weaknesses:  Limited Exploration of Surrogate Model Constraints: While the paper mentions the flexibility in surrogate model selection a more detailed discussion on the limitations or constraints imposed by certain types of surrogate models would provide a more rounded viewpoint.  Evaluation Depth: The experimental evaluation though compelling is somewhat limited in the scope of application scenarios and datasets explored. Extending this to more varied scenarios or tasks could underline the versatility of ASEs further.  Discussion on Computational Complexity: Although briefly touched upon a more indepth discussion on the computational implications of ASEs especially in comparison to existing methods would add valuable insight for potential users weighing the costbenefit aspects. 3. Questions and Suggestions for the Authors  Surrogate Model Selection: Could the authors elaborate on the considerations or limitations that might influence the choice of surrogate models for ASEs Are there cases where certain models may significantly underperform or even be inapplicable  Broader Evaluation: While the focus on deep neural networks for image classification is insightful are there plans to test ASEs efficacy on different types of models (e.g. regression models) or in less conventional settings (e.g. text or timeseries data)  Algorithmic Scalability: How do ASEs scale with increasingly large datasets or more complex models Any insights or preliminary results in this direction would be illuminative. 4. Limitations and Societal Impact The authors provide a solid theoretical foundation for ASEs and demonstrate their utility in labelefficient model evaluation. However the broader limitations concerning the scalability and computational demand of the proposed method especially in resourceconstrained settings could be better addressed. Regarding potential societal impact while the paper primarily focuses on technical contributions a brief discussion on how ASEs might influence practices in domains where data labeling is costprohibitive or ethically sensitive could enrich the paper. Considerations around fairness privacy or potential biases within the surrogate models could further highlight the thoughtful application of ASEs. Suggestions for Improvement:  Expand the discussion on the implications of surrogate model selection and potential constraints or pitfalls.  Broaden the empirical evaluation to include additional tasks domains or dataset complexities to showcase ASEs applicability.  Provide a more comprehensive analysis of computational demands and scalability challenges.  Include a section discussing the broader ethical and societal considerations relevant to ASEs particularly in sensitive or constrained contexts. In summary the paper makes significant contributions to model evaluation methodologies with ASEs presenting a promising avenue for research and application. With further elaboration on the discussed points the work has the potential to set a new standard in labelefficient model evaluation practices.", "l1WlfNaRkKw": "The paper under review investigates the theoretical performance of data augmentation (DA) in machine learning focusing on settings with transformation invariances such as image rotations or color changes. It explores how DA compares to other learning methods in various realizability settings defined by the relationship between hypotheses the natural and transformed data distribution. The authors make several contributions identifying conditions under which DA helps or even harms learning accuracy. They propose two new complexity measures to characterize the optimal sample complexity under different settings and provide optimal algorithms according to these metrics. Furthermore adaptive algorithms are introduced to bridge the gaps between different invariance settings of the hypothesis.  Strengths: 1. Theoretical Depth: The paper dives deep into a theoretical analysis of DA covering a gap in the literature regarding its performance in the presence of transformation invariances. The definitions of different realizability settings and the complexity measures introduced are rigorous and provide a solid framework for understanding the nuances of learning under transformation invariances. 2. Originality: Proposing the combinatorial measures for characterizing the optimal sample complexity and identifying suboptimality of DA in certain settings are novel contributions that push the boundaries of current understanding. 3. Practical Implications: Although theoretical this work has important implications for the practical application of DA in machine learning especially in areas like image processing and natural language processing where transformation invariances are common.  Weaknesses: 1. Assumption Clarity: While the paper introduces specific settings and assumptions for its theoretical analysis it does not extensively discuss the realism of these assumptions in practical scenarios. For example the assumption that the set of transformations forms a group may not always hold and the paper briefly mentions this but does not delve into the implications. 2. Algorithmic Details: The paper provides optimal algorithms based on their proposed complexity measures but lacks detailed explanation or demonstrations of these algorithms implementation and performance in practical or simulated settings. 3. Bridging Theory and Practice: Despite the practical implications of their findings theres a missed opportunity in not providing any empirical validation or discussion on how these theoretical insights could be translated into realworld applications effectively.  Questions and Suggestions for Authors: 1. Can the authors provide further discussion or examples on the realism of their assumptions in practical machine learning tasks Specifically how common is the group property among transformations encountered in real datasets 2. It would be beneficial for readers if the paper included a more detailed explanation of the proposed algorithms possibly supplemented with pseudocode or a case study on a simple dataset. 3. Considering the findings how should practitioners reconsider their use of DA and are there specific types of transformation invariances or datasets where DA should be avoided or used differently  Limitations and Societal Impact: The authors briefly touch upon scenarios where the transformation set does not form a group which is a limitation in extending their theoretical results to more general cases. Addressing nongroup transformations more thoroughly could enhance the papers relevance for a broader range of applications. Regarding societal impact the paper is primarily theoretical and does not directly address potential negative impacts. However understanding and improving DAs effectiveness can influence the deployment of machine learning models in critical areas such as autonomous driving or medical imaging where robustness to transformations is crucial. It would be beneficial if the authors included a discussion on ensuring that the improvements in learning algorithms do not inadvertently exacerbate biases in data.  Suggestions for Improvement:  Expand on the discussion of nongroup transformations and their implications for DAs effectiveness and the proposed complexity measures.  Include more algorithmic details and potential for a supplemental section with implementation details or simulated results to demonstrate the practical applicability of the theoretical findings.  Consider adding a section discussing the translation of these theoretical insights into practical recommendations for machine learning practitioners.", "tVbJdvMxK2-": "Summary The paper proposes a novel approach to combating catastrophic forgetting in continual learning by dynamically optimizing memory construction in Experience Replay (ER) environments. The central innovation termed Global Pseudotask Simulation (GPS) addresses the combinatorial optimization challenge of memory construction by simulating future catastrophic forgetting patterns through pseudotasks. This approach significantly improves model accuracy across four standard vision benchmarks. The paper not only demonstrates the efficiency of GPS but also its potential as a unified framework for incorporating various memory construction policies in ER methods. Strengths 1. Innovative Approach: The dynamic optimization of memory construction using the Global Pseudotask Simulation (GPS) is a unique solution to catastrophic forgetting. It addresses a fundamental challenge in continual learning with a novel perspective that could inspire future research. 2. Empirical Validation: The comprehensive experiments across multiple benchmarks provide strong evidence of the methods effectiveness. By outperforming existing baselines and showing closeness to offline solutions the paper presents a compelling case for the practical utility of GPS. 3. Framework Flexibility: The ability of GPS to integrate with other ER variants and improve their performance signifies its flexibility and potential as a unifying framework. This characteristic makes the proposed method valuable for a wide range of continual learning applications. Weaknesses 1. Complexity and Computational Demand: While the paper mentions the efficiency of GPS the detailed computational overhead and complexity associated with simulating pseudotasks are not thoroughly discussed. For practical applications understanding the tradeoffs between computational demand and performance benefits is crucial. 2. Generalization to NonVision Tasks: The experimental validation is limited to vision benchmarks. While the results are promising the generalization of GPS to nonvision tasks and more heterogeneous data remains an open question. Further studies in diverse domains would enhance the methods credibility. 3. Analysis of Failure Modes: The paper could benefit from a more detailed analysis of scenarios where GPS might fail or perform suboptimally. Understanding the conditions under which the models assumptions do not hold would be valuable for practitioners. Questions and Suggestions 1. Computational Costs: Could the authors provide more details on the computational costs associated with GPS especially in comparison to traditional ER methods This insight would help readers assess the practicality of implementing GPS in different settings. 2. Extension to NonVision Tasks: Have the authors considered extending their experiments to nonvision tasks Insights from tasks involving sequential data or natural language processing could broaden the impact of the work. 3. Failure Mode Analysis: It would be beneficial if the authors could discuss potential failure modes or limitations of GPS. For instance under what scenarios might the pseudotasks fail to accurately mimic future forgetting patterns Limitations and Societal Impact The authors have made efforts to address the limitations of their work by discussing the scope of their experiments and the potential extension to other learning setups. However a more detailed discussion on the computational efficiency of GPS in realworld applications would strengthen the paper. Regarding societal impact while the paper focuses on the technical aspects of the method acknowledging the broader implications of improving continual learning systems such as enhanced personalized learning or more adaptable AI systems could enrich the discussion. Constructively to address these limitations the authors could explore efficient simulation techniques to reduce computational demand and engage in interdisciplinary research to understand the broader impacts of their work.", "ttQ_3CiZqd3": " Review of \"Learning Equilibrium Systems through Optimal Control\"  Paper Summary This paper presents a novel principle for learning systems modeled as equilibrium states including both feedforward and recurrent neural networks which has been difficult due to the need for biologicallyplausible efficient learning algorithms. The authors introduce a learning principle based on optimal control where learning is formulated as minimizing the control needed to reach a state of minimal loss thereby incorporating learning directly into the dynamics of the system. This approach allows for local gradientbased learning without storing intermediate states or relying on small learning signals offering a more efficient and plausible model for neural computation in the brain. The authors apply their principle to tasks involving recurrent neural networks and metalearning showing competitive performance with existing gradientbased methods and propose their method as a potentially more biologically relevant way of learning in neural systems.  Assessment Strengths: 1. Novelty: The proposed principle of learning through leastcontrol is a significant departure from traditional gradientbased or backpropagation methods providing a fresh perspective on learning in dynamical systems. 2. Generality and Flexibility: The principle is applicable to a wide array of learning systems not being restricted to specific network architectures. This flexibility demonstrates the potential of the principle to model complex learning phenomena. 3. Biological Plausibility: By embedding error information directly into network activity and avoiding the storage of intermediate states the paper paves the way for more plausible models of learning in the brain. 4. Empirical Validation: The experiments conducted show that the principle can be successfully applied to various architectures and learning problems achieving competitive performance with current gradientbased learning methods. Weaknesses: 1. Assumptions and Simplifications: While the theoretical concept is compelling the assumption of equilibrium and the simplification used in modeling learning as optimal control could limit the applicability to realworld biological systems where such equilibria might not be easily attainable or identifiable. 2. Complexity of Implementation: While the authors claim that their learning rule is local and does not require storing intermediate states the practical complexity of implementing such a system especially in neural circuitry is not thoroughly discussed. 3. Limited Exploration of Limitations: The paper could benefit from a more thorough discussion on the limitations of the proposed approach especially in terms of scalability to larger problems and its comparison with stateoftheart methods in more challenging settings.  Questions and Suggestions 1. Extension Beyond Equilibrium Systems: Can the authors discuss how their principle might be adapted or extended to learning in nonequilibrium systems which are prevalent in biological and artificial neural networks 2. Comparison with Stateoftheart Learning Methods: It would be beneficial for the paper to include a more rigorous comparison with current leading methods especially in terms of computational efficiency and scalability. 3. Biological Validation: While the model is inspired by neuroscience direct evidence or theoretical support from the biological community would strengthen the papers claims on biological plausibility.  Limitations and Societal Impact The paper does not delve deeply into the limitations of the approach particularly in terms of scalability and efficiency in larger more complex tasks. Furthermore the potential negative societal impact is not addressed. While purely theoretical research might seem distant from direct societal impact the development of more efficient and biologically plausible learning systems could eventually influence areas such as autonomous systems which come with their own set of ethical and societal considerations. Improvement suggestions include a more detailed exploration of how the proposed learning principle scales with network size and complexity as well as a section dedicated to discussing potential negative impacts such as misuse of advanced machine learning models in surveillance automation leading to job displacement and ensuring the ethical use of biologically inspired models.", "mn1MWh0iDCA": "The paper presents an indepth analysis of the underlying mechanisms that contribute to the success or failure of stateoftheart (SOTA) offline Reinforcement Learning (RL) agents. Through a series of meticulously designed experiments the authors evaluate these agents based on the quality of learned representations accuracy of value functions and the efficacy of policies. Surprisingly the study reveals that highperforming offline RL agents do not necessarily rely on highquality representations or accurate value functions. Instead their capability to select suboptimal actions effectively sets them apart. Building on these insights the authors propose a novel offline RL algorithm RIQL which notably surpasses existing SOTA performances by adjusting the policy improvement step in IQL. Additionally the paper explores the impact of learned dynamics models on modelfree offline RL agents and introduces an uncertaintybased sample selection method to handle modelinduced noises. Code for this research is made publicly available encouraging replication and further exploration.  Strengths: 1. Comprehensive Experimentation: The paper stands out for its thorough experimental design that meticulously evaluates offline RL agents across multiple dimensions. This approach not only facilitates a deeper understanding of these agents but also identifies crucial areas for improvement. 2. Innovative Contributions: The introduction of RIQL and the uncertaintybased sample selection method are significant advancements. These contributions not only address identified issues but also set new benchmarks in the field pushing the envelope of whats achievable in offline RL. 3. Clarity and Reproducibility: The inclusion of code and detailed experimental setups enhances the papers clarity and reproducibility. It enables other researchers to validate the findings and use the proposed methods as a foundation for future research. 4. Insightful Findings: The discovery that highquality representations and accurate value functions are not prerequisites for successful offline RL agents is counterintuitive and challenges prevailing beliefs. These insights are invaluable for guiding future RL research and algorithm development.  Weaknesses: 1. Generalization of Findings: While the experiments provide novel insights they are primarily conducted on the D4RL benchmark datasets. Additional experiments across a broader range of environments could help in understanding whether these findings generalize across different settings. 2. Discussion on Computational Requirements: The paper lacks a detailed discussion on the computational resources required for RIQL and the uncertaintybased method. Given their potential complexity understanding their computational overhead is essential for practical applications. 3. Limited Analysis on Policy Evaluation Methods: While the paper introduces a novel policy improvement method a more detailed comparison and analysis of existing policy evaluation methods could provide a richer understanding of their relative strengths and weaknesses.  Questions and Suggestions for Authors: 1. Generalization to Different Environments: Can the authors provide insights or preliminary results on how RIQL and the proposed sample selection method perform in environments outside the D4RL datasets 2. Computational Costs: Could the authors elaborate on the computational requirements of the proposed methods compared to existing SOTA algorithms 3. Impact on Policy Evaluation: Given the focus on policy improvement in RIQL how do the authors envision the role and potential improvements in policy evaluation methods within the context of their findings  Limitations and Societal Impact: The authors have made a commendable effort in addressing the limitations related to the scope of their experiments and the potential computational demands of their methods. However a more detailed discussion on the broader applicability of their findings and the computational efficiency of their proposed methods would further strengthen the paper. Additionally while the paper briefly mentions the potential realworld applications of offline RL an expanded discussion on the ethical implications and societal impact particularly in sensitive applications (e.g. healthcare autonomous driving) would be beneficial. Constructive strategies to ensure that the deployment of these technologies aligns with ethical standards and societal values would be a valuable addition.", "uP9RiC4uVcR": " Peer Review of \"MoralExceptQA: A Challenge Set for Moral Exception Question Answering with Large Language Models\"  1. Summary The paper introduces MoralExceptQA a novel challenge set designed to assess the capability of Large Language Models (LLMs) in handling moral exception question answering a critical aspect of ethical AI development. This challenge is rooted in understanding the flexible nature of human moral judgments wherein certain rules can be bent or broken under specific circumstances. Through a unique combination of moral psychology and advanced computational techniques the authors have developed MORALCOT an LLMbased model that incorporates cognitive theories of moral reasoning to predict humanlike moral judgments. The model showcases a 6.2 F1 improvement over existing stateofart LLMs in the MoralExceptQA benchmark. Additionally a comprehensive error analysis is provided to pinpoint areas of failure and suggest avenues for future research in AI safety and ethical decisionmaking mechanisms.  2. Assessment Strengths:  Originality and Relevance: The study attacks a very complex and underexplored domain within AI which has significant implications for the development of ethically aligned AI systems. Combining cognitive science theories with computational models to tackle moral flexibility demonstrates a novel interdisciplinary approach.  Technical Soundness: The methodology particularly the development of the MORALCOT prompting strategy and the preparation of the MoralExceptQA challenge set is rigorously designed. The use of stateoftheart LLMs as a foundation and improving upon them using insights from moral philosophy and cognitive science is technically sound and welljustified.  Potential Impact: Given the growing deployment of AI in various societal functions the paper addresses an urgent need for models that can navigate the complex terrain of human ethics. Successful integration of models like MORALCOT could enhance AI safety and reliability fostering trust in AI systems. Weaknesses:  Dataset Diversity and Size: The challenge set while innovative is based primarily on U.S. residents limiting its cultural and sociomoral perspective. The relatively small size of the dataset might also constrain the generalizability of the findings.  Model Generalizability: While MORALCOT shows improvement on the MoralExceptQA task it\u2019s unclear how this model performs on moral reasoning tasks in unstructured realworld scenarios beyond the dataset. The specific nature of the prompts used might limit the applicability of the model in diverse contexts.  Societal and Ethical Considerations: The paper touches on the potential societal impact but could benefit from a deeper exploration of the ethical implications of deploying such models. Questions around biases accountability and the replacement of human moral judgment with AI decisions warrant further discussion.  3. Questions and Suggestions for the Authors  Expanding Cultural Representation: Could the authors elaborate on plans or methodologies to include moral judgments from a broader array of cultural and ethical backgrounds  Model Adaptability: How might MORALCOT adapt to realtime moral dilemmas in varied contexts Are there plans to test the model in dynamic environments  Addressing Bias and Accountability: How do the authors propose to tackle inherent biases in moral judgment data Additionally what accountability mechanisms can be put in place should these AI models make ethically questionable decisions  4. Limitations and Constructive Suggestions for Improvement The considerations around the potentially narrow cultural scope of the dataset and the ethical implications of AIdriven moral decisions are critical limitations. To improve the authors should consider the following:  Dataset Expansion: Actively work on diversifying the data sources to include a wider range of cultural perspectives and moral beliefs. This could involve collaborative efforts with researchers across the globe.  Robust Testing in Varied Contexts: Future iterations of this work could benefit from deploying MORALCOT in simulated realworld scenarios to assess its adaptability and performance in dynamic moral dilemmas.  Ethical Framework: Develop a comprehensive ethical framework to guide the deployment of such models ensuring that biases are mitigated and accountability mechanisms are in place for decisionmaking processes. In conclusion this paper makes a significant contribution to the field of AI ethics presenting a groundbreaking approach to modeling humanlike moral judgments. Despite its limitations the work opened valuable discussions and avenues for future research to build safer and more ethically aligned AI systems.", "m6DJxSuKuqF": "Review of \"KeypointGuided Optimal Transport for Heterogeneous Domain Adaptation\" Summary: This paper presents a novel KeypointGuided model by Relation preservation (KPGRL) aimed at enhancing the performance of Optimal Transport (OT) methods by leveraging annotated keypoints to guide correct matching in OT applications. Specifically the authors introduce a maskbased constraint to ensure the matching of keypoint pairs and propose to preserve the relation of each data point to the keypoints to guide the matching process. The effectiveness of KPGRL is demonstrated through applications to heterogeneous domain adaptation (HDA) where it is shown to outperform existing methods. The approach is versatile applicable even when distributions are supported in different spaces and can be solved efficiently using Sinkhorn\u2019s algorithm. The authors also extend the model to the partial OT setting and provide comprehensive experiments to validate the models effectiveness. Assessment: Strengths: 1. Originality: The introduction of keypoint annotations as guidance in OT settings is a novel and significant contribution. The maskbased constraint for preserving the matching of keypoint pairs and the relation preservation for guiding the matching process are innovative approaches that address limitations in existing OT methods. 2. Relevance to Applications: The paper addresses a practical challenge in many applications where incorrect matching of samples could occur in OT frameworks. By leveraging a few annotated keypoints the method considerably enhances matching accuracy showcasing its applicability in realworld scenarios such as HDA and potentially crossdomain data generationtranslation. 3. Theoretical Soundness: The KPGRL model is wellformulated with clear mathematical representation. The adaptations of the model to handle partial OT and its extension to other OT models like KP and GW with proofs and theoretical justification strengthen the paper. 4. Empirical Evaluation: The comprehensive experiments conducted on HDA tasks including the comparison with baseline and stateoftheart methods and the illustrative experiments on toy data effectively demonstrate the superiority of the proposed approach. The openaccess code further contributes to the paper\u2019s transparency and reproducibility. Weaknesses: 1. Limitations Discussion: While the paper briefly mentions its applicability in different spaces and extensions to partial OT settings a more detailed discussion on the limitations particularly regarding the selection and availability of keypoints in practical applications would enhance the paper. How the method performs in scenarios with very sparse or possibly incorrect keypoint annotations remains unclear. 2. Societal Impact: The paper does not discuss the potential negative societal impacts of the proposed method. Given its application to HDA there could be concerns regarding bias amplification or privacy issues when transferring knowledge across domains especially in sensitive applications. Addressing these concerns and suggesting mitigation strategies would be a responsible addition. 3. Comparative Analysis: The experimental evaluation while comprehensive could benefit from a more nuanced analysis of why the KPGRL model outperforms others beyond the effectiveness of keypoints. Insights into the models robustness to the number and quality of keypoints or comparison with other known techniques for incorporating side information in OT would enrich the discussion. Questions and Suggestions for the Authors: 1. Could the authors elaborate on the potential limitations of the KPGRL model concerning the availability and selection of keypoints in different application scenarios 2. It would be beneficial if the paper included a discussion on the ethical considerations and potential negative societal impacts of leveraging OT in domain adaptation particularly concerning the proposed method. 3. The experiments showcased the model\u2019s superiority in the HDA task. Can the authors discuss or hypothesize how KPGRL might perform in other applications such as in unsupervised domain adaptation or crossdomain image translation without explicit keypoint annotations 4. A more detailed analysis comparing the models sensitivity to the number and correctness of keypoint annotations would be insightful. How does the models performance vary with inaccurate or noisy keypoint annotations Limitations and Potential Negative Societal Impact: The authors have not fully addressed the limitations of their approach particularly regarding the selection and availability of keypoints in different applications. Additionally the potential negative societal impacts such as bias amplification in domain adaptation tasks are not discussed. As a suggestion for improvement the authors should include a section discussing these aspects providing guidelines for responsible application of their method and strategies for mitigating possible negative impacts. Overall the paper makes a significant contribution to the field of OT and its application in HDA presenting a novel method that leverages keypoints to improve matching accuracy effectively. With further discussion on the outlined points the paper could provide a more comprehensive insight into the applicability and responsible use of the proposed method.", "xWvI9z37Xd": "Title: Enhancing Unsupervised Feature Selection with Sparse Autoencoders: A WAST Method Overview 1) Summary and Contributions: The paper introduces a novel unsupervised method for feature selection leveraging sparse autoencoders optimized via a fresh sparse training algorithm dubbed WAST (Where to pay Attention during Sparse Training). The key innovation is the methods ability to adapt the sparse topology of the autoencoder dynamically focusing on informative features early during training. This approach greatly enhances computational efficiency by requiring significantly fewer iterations to converge compared to existing neural networkbased methods. The authors conducted exhaustive testing across ten diverse datasets showcasing WASTs superior performance in identifying informative features its robustness in noisy conditions and its efficacy for both high dimensional data and datasets with few samples. Contributions are summarized as the proposal of WAST for efficient unsupervised feature selection extensive experimentation across varied datasets demonstrating its superior performance and efficiency and a demonstration of robustness in noisy environments. 2) Assessment of Strengths and Weaknesses:  Strengths: 1. Innovative Approach: The introduction of an attentionbased sparse topology adjustment during training is a significant contribution addressing the computational inefficiency seen in previous neural networkbased feature selection methods. 2. Extensive Evaluation: The paper provides a thorough evaluation covering a wide array of data types and conditions including extreme noise scenarios. 3. Performance and Efficiency: WAST not only surpasses the performance of stateoftheart unsupervised feature selection methods but also demonstrates substantial reductions in training iterations and computational costs. 4. Robustness: The method\u2019s effectiveness in highly noisy environments and across various dataset characteristics is commendable.  Weaknesses: 1. Lack of Realworld Application: Though the method is validated across several datasets the paper lacks a demonstration of WAST in a realworld application or case study. 2. Limited Comparison with Supervised Methods: While the paper briefly compares WAST with supervised methods a deeper analysis or discussion on how it might be adapted or compared more extensively in supervised contexts could be beneficial. 3. Hardware Utilization: The current benefits in computational efficiency might not be fully realized in practice due to the lack of hardware that supports efficient computation of sparse networks. 3) Questions and Suggestions for the Authors:  Q1: Can the authors provide insights or preliminary findings on applying WAST for supervised feature selection  Q2: The paper mentions the limitation related to hardware support for sparse models. Could you discuss any ongoing developments or future directions that might mitigate this limitation  Q3: A realworld application showcasing the practical benefits of WAST would be highly valuable. Are there plans to integrate WAST into specific domains such as bioinformatics or realtime analytics where feature selection is pivotal  Suggestion 1: Exploring the integration of WAST with recent advancements in hardware capable of exploiting sparse networks could further boost its applicability and efficiency.  Suggestion 2: A comparative analysis or adaptation of WAST in supervised learning contexts could broaden the methods utility and appeal to a wider research community. 4) Limitations and Societal Impact: The paper prudently acknowledges its limitations chiefly the lack of hardware to fully exploit the computational benefits of sparse networks. The authors optimism toward growing support for sparsity in hardware and software is justified though a timeline or example developments would enrich this discussion. Regarding societal impacts the potential for WAST to facilitate energyefficient systems and enhance the interpretability of models is rightly highlighted. The absence of any negative societal impacts is noted but a deeper dive into potential biases in feature selection especially in sensitive applications could strengthen this section. Constructive Suggestions for Improvement:  The authors should consider elaborating on possible adaptations of WAST for supervised scenarios providing a roadmap or speculated benefits.  Detailed discussion or a dedicated section on emerging hardware technologies that could support the computational efficiency of sparse networks would be a valuable addition.  A case study illustrating WAST\u2019s application in a practical setting would greatly enhance the paper\u2019s impact and demonstrate its realworld utility.", "l7aekTjF6CO": " Review of \"Exploring the Tradeoff between Representation and Fairness in SortitionBased Democracy\"  1) Summary The paper investigates the concepts of representation and fairness within the framework of sortitionbased democracy where citizens assemblies are selected randomly. Focusing on the method of uniform selection it questions whether such a random selection is indeed representative of the population. To address this the authors introduce a representation metric to quantify how well a panel represents the broader population using the concept of the qth closest representative and the social cost of representation. They find that for values of q greater than half the panel size (k2  q \u2264 k) uniform selection does indeed provide adequate representation. However for q \u2264 k2 no algorithm can provide both fair and representative selection under their model. In response they propose a new algorithm RANDOMREPLACE which offers a balance between representation and fairness achieving a nontrivial representation guarantee of 1(q  1) for any value of q. The study also includes an empirical analysis using demographic datasets showing how uniform selection and RANDOMREPLACE perform in practice.  2) Assessment Strengths:  Novelty and Relevance: The paper addresses a significant gap in the literature on democratic selection processes particularly in the context of sortition or random selection democracies. The introduction of a representation metric is an innovative approach to quantitatively assess representation.  Theoretical Insight: The distinction between representation quality for qvalues greater and less than half the panel size is a valuable theoretical contribution. This bifurcation sheds light on inherent limitations and potentials within sortition systems.  Practical Implications: The empirical analysis leveraging real demographic data adds practical value demonstrating the models applicability in realworld scenarios. This connection between theory and practical implementation is commendable. Weaknesses:  Assumptions on Representation Metric: The papers foundational assumption on the existence and constructability of a universally applicable representation metric might oversimplify the complex social political and psychological factors that define \"representation.\"  Algorithmic Complexity and Scalability: The complexity of the RANDOMREPLACE algorithm and its scalability in extremely large populations or highly diversified societies is not discussed in detail. Practical application in realworld democratic systems could be challenging.  Broader Implications on Democratic Systems: The analysis mainly focuses on citizens assemblies without delving into how such findings may affect broader democratic systems elections and governance. The societal implications particularly in nonideal or politically contentious environments would benefit from further exploration.  3) Questions and Suggestions  Representation Metric Clarification: Could the authors provide more details on how the representation metric could be constructed in diverse societies with complex social fabrics How would you account for nonquantifiable factors such as cultural representation  Algorithmic Adjustments for Larger Populations: Are there any proposed modifications to the RANDOMREPLACE algorithm that could ensure its scalability and efficiency in larger populations or more granular representation demands  Extensions to Other Democratic Forms: How might the results of this study inform the design or reform of electoral democracy mechanisms beyond sortitionbased approaches Could the representation metric find application in electoral system design or reform  Impact on Public Trust: It would be insightful to discuss how different levels of representation and fairness as achieved by various algorithms might impact public trust in the democratic process. Can you theorize the relationship between your findings and public perception of democratic legitimacy  4) Limitations and Societal Impact The authors moderately address the limitations associated with the theoretical and practical application of their model and algorithms. However a deeper analysis of the assumptions underlying the representation metric and their implications in highly diverse societies would strengthen the paper. Further while the paper hints at the positive societal impact of improving representation and fairness in democratic selections a more thorough discussion on the potential negative effects\u2014such as algorithmic bias disenfranchisement of minority groups or exploitation of algorithmic loopholes\u2014would provide a more balanced view. Constructive Suggestions for Improvement:  Expand the discussion on the construction and application of the representation metric to incorporate multidimensional social and political identities.  Offer a more detailed analysis of the scalability challenges and potential solutions for the proposed algorithms.  Include a comparative study or discussion on how these findings might translate to or influence other forms of democracy particularly electoral systems.  Provide an indepth exploration of the societal impact considering both the potential benefits and drawbacks of implementing such algorithms in democratic processes. Overall the paper contributes valuable insights to the ongoing discourse on the optimization of democratic processes through sortition striking a balance between theoretical innovation and practical application. Addressing the outlined questions and suggestions could significantly enhance its contributions to the field.", "yZ_JlZaOCzv": " Peer Review for \"Adversarial Attacks on Go AIs with Meaningless Moves\"  1. Summary This paper introduces a novel method to systematically generate adversarial states for AlphaZerolike agents in the game of Go revealing vulnerabilities in both PolicyValue Neural Networks (PVNNs) and Monte Carlo Tree Search (MCTS). By adding one or two \"meaningless\" moves to the game board the proposed method can force a Go AI including stateoftheart models like KataGo to make suboptimal decisions that are easily recognized as inferior by amateur human players. The authors define a perturbation set that maintains semantic equivalence between the original state and the perturbed state ensuring the generated adversarial examples are valid. Furthermore the paper describes an efficient search algorithm for identifying these adversarial states significantly outperforming a bruteforce approach in terms of speed. The effectiveness of the method is demonstrated across various datasets involving different Go AI agents. The paper concludes with a suggestion to use these findings to improve AI agents making them more robust against such attacks.  2. Assessment  Strengths  Novelty: The approach to generating adversarial examples in the discrete and complex domain of the game of Go is original and contributes significantly to understanding the robustness of Go AI agents.  Practical Relevance: Identifying vulnerabilities in Go AIs not only provides insights into their weaknesses but also suggests pathways for improving their performance and robustness.  Comprehensive Evaluation: The experiments conducted across multiple datasets and AI agents demonstrate the methods effectiveness and generalizability.  Efficiency of Proposed Method: The devised search algorithm significantly accelerates the identification of adversarial examples which effectively addresses the computational challenge posed by the games vast state space.  Human Verification: Including human players in the evaluation process provides a meaningful benchmark for the adversarial examples quality emphasizing the practical impact of the proposed attack.  Weaknesses  Limited Scope of Perturbations: The perturbations are restricted to adding \"meaningless\" moves. Exploring a broader range of perturbations might reveal additional vulnerabilities.  Verification by Human Players: While the paper benefits from human verification of adversarial examples the sample size and the range of player expertise could affect the generalizability of these findings.  Societal Impact Consideration: The discussion on potential negative societal impacts is sparse. Given the increasing deployment of AI in strategic decisionmaking understanding how such vulnerabilities might be exploited warrants further exploration.  Questions and Suggestions 1. Expansion of Perturbation Types: Have the authors considered extending the method to include more perturbation types such as removing or moving existing stones on the board 2. Broader Human Participant Study: Could future work involve a larger and more diverse set of human players to verify the adversarial examples adding robustness to the findings 3. Improvement Suggestions: Can the authors suggest specific mechanisms by which Go AI developers might incorporate these findings to enhance the agents resilience to such attacks 4. Societal Impact: Could the authors elaborate on potential malicious applications of their findings and suggest guidelines for the ethical use of adversarial attack research in AI  Limitations and Societal Impact While the paper makes significant contributions it acknowledges limitations such as not providing a direct method for guiding AI agents towards adversarial states in actual gameplay. Addressing this could make adversarial training more practical for improving AI robustness. Furthermore a more detailed discussion on the potential misuse of adversarial attacks in competitive contexts or securityrelated applications would enhance the papers consideration of societal impacts. Suggestions for mitigating risks associated with adversarial machine learning techniques could also be beneficial to the research community and society at large.  Conclusion The paper presents a valuable addition to the literature on adversarial machine learning particularly in the realm of games with complex state spaces like Go. The findings offer insights into the vulnerabilities of cuttingedge AI systems and lay the groundwork for developing more resilient AI agents. Further research that broadens the scope of adversarial examples and includes more extensive human verification would strengthen the claims and applicability of the proposed method.", "ogNrYe9CJlH": " 1) Summary The paper introduces a novel ReductiontoBinary (R2B) approach aimed at enforcing demographic parity in multiclass classification scenarios with nonbinary sensitive attributes by reducing the problem to a series of binary debiasing tasks. The authors provide theoretical guarantees for optimality and bias reduction with their method and empirically demonstrate its superiority over two primary baselines: independent label debiasing treated as multilabel problems and the transformation of features instead of labels. The effectiveness of R2B is validated through experiments on synthetic and realworld datasets from diverse domains including social science computer vision and healthcare.  2) Assessment Strengths:  Innovative Approach: R2Bs reduction to binary tasks is a creative solution to the challenging problem of multiclass classification with nonbinary sensitive attributes addressing a gap in existing literature and practice.  Theoretical and Empirical Validation: The authors back their claims with both theoretical guarantees and comprehensive empirical evidence enhancing the credibility of their methodology.  Versatility across Domains: The empirical validation across multiple domains suggests R2Bs broad applicability potentially influencing a wide range of applications.  Comparative Analysis: The thorough comparison with existing baselines demonstrates the advantages of R2B in maintaining demographic parity adding valuable insights to the field of algorithmic fairness. Weaknesses:  Limited Scope of Fairness Metrics: The focus is restricted to demographic parity which might not capture all fairness concerns in multiclass classification problems.  Preprocessingonly Approach: R2B being a preprocessing method limits its applicability in scenarios where inprocessing or postprocessing interventions are preferable or necessary.  Lack of Focus on Computational Complexity: While the authors touch upon the computational efficiency of R2B detailed analysis or comparison in terms of computational resources and scalability is missing which is crucial for largescale applications.  3) Questions and Suggestions  Extension to Other Fairness Metrics: Can the R2B approach be extended or modified to address other fairness criteria such as equalized odds or individual fairness Doing so could broaden its utility.  Comparative Performance with Deep Learning Models: Given that deep learning models are prevalent in the domains of application how does R2B interact with such models and what is its comparative performance against the baselines in deeplearningspecific settings  Computational Efficiency Analysis: Indepth analysis or a more detailed discussion on computational efficiency and scalability possibly comparing R2B with the baselines in terms of computational time and resources would be beneficial.  4) Limitations and Societal Impact The authors have attempted to address the limitations inherent to the R2B approach especially its focus on demographic parity and its preprocessing nature. However the discussion on potential negative societal impacts is somewhat cursory. Considering the sensitive nature of fairness in AI a more thorough exploration of how misuse or misinterpretation of demographic parity (e.g. potentially obscuring individuallevel disparities) could lead to unintended consequences would be valuable. Constructive suggestions include:  Broader Consideration of Fairness Metrics: Encourage research into integrating additional fairness metrics within the R2B framework or similar future methodologies.  Impact Case Studies: Including case studies or theoretical scenarios exploring potential negative societal impacts would better prepare practitioners for responsible implementation.  Continual Monitoring Recommendation: Advocate for the implementation of mechanisms for continual monitoring of deployed models for emerging biases or fairness issues particularly in rapidly changing societal contexts. Overall the R2B approach makes a significant contribution to advancing fairness in algorithmic decisionmaking particularly in complex multiclass classification problems. However careful consideration and further research into its broader implications limitations and potential for adaptation are necessary to maximize its positive impact on society.", "pOEN7dDC0d": " Review of \"Investigating the Applicability of Harris\u2019s Articulation Scheme in Emergent Languages\"  1. Summary of the Paper This paper sets out to explore the application of Harris\u2019s Articulation Scheme (HAS) in emergent languages a statistical universal from natural languages to segment words based on phoneme statistics without Semantic interpretations. Emergent languages are artificial communication systems between agents in simulated environments significant for their implications in AI interaction and linguistics. Employing HAS as an unsupervised segmentation method the authors investigate whether it accurately segments meaningful units in emergent languages from signaling games. They discover that while emergent languages satisfy some conditions for implementing HAS the segmentation does not reliably reflect semantically meaningful boundaries.  2. Assessment of Strengths and Weaknesses Strengths:  Novel Investigation Arena: Research expanding on emergent languages and their structural comparison to natural languages is relatively scarce. This paper addresses a novel and intriguing question about the applicability of HAS in emergent languages.  Comprehensive Methodological Framework: The authors adopt a rigorous approach from formulation of their hypothesis grounded in established linguistic theory to detailed experimental setups designed to test various aspects of the hypothesis.  Clear Presentation of Results: The experimental outcomes albeit mixed are presented transparently with proper statistical validation. The graphical representations aid in understanding the findings effectively. Weaknesses:  Lack of Semantic Analysis: While the paper adeptly explores the statistical alignment of emergent language segments with HAS predictions theres a noticeable gap in analyzing the semantic cohesiveness of the segmented units. Given the emphasis on semantics in language incorporating semantic validity could have strengthened the papers conclusions.  Limited Discussion on HAS Limitations: Although HASs universality is a focal point the paper could benefit from a deeper discussion on the known limitations and critiques of HAS in the context of natural language to frame its applicability to emergent languages more robustly.  Potential Overlook of Multimodality: The studys focus is primarily on phonemelike units in emergent languages. Considering emergent languages in AI environments often transcend purely linguistic modalities examining HAS within multimodal communication could provide further insights.  3. Questions and Suggestions  Semantic Validation Suggestion: Could the authors elaborate on possible methods to assess the semantic accuracy of the segments determined by HAS in emergent languages This could offer a more rounded analysis of HASs effectiveness.  HAS Limitation Discussion: It would enrich the discussion to include a subsection addressing the limitations and critiques of HAS within natural linguistics and hypothesize their implications for emergent languages.  Exploring Multimodality: Recognizing emergent languages potential for multimodality how might HAS apply across different modes of communication A speculative section discussing these prospects could broaden the paper\u2019s impact.  Limitations The authors moderately address the limitations and potential societal impacts of their work. The computational demands and environmental impact associated with GPU usage for simulations are acknowledged though somewhat briefly. The societal implications are deemed minimal due to the artificial nature of the subject matter which is a reasonable assumption. However expanding on potential future applications that could bridge humanmachine interactions and their societal ramifications might offer a more comprehensive view on the implications and limitations of this research.", "tro0_OqIVde": " Review of \"Recursive Gated Convolution for Generic Vision Backbones: HorNet\"  1) Summary This paper introduces Recursive Gated Convolution (gnConv) a novel operation designed to implement highorder spatial interactions efficiently within a convolutionbased framework. The operation leverages gated convolutions and recursive designs to extend the order of spatial interactions in visual models aiming to combine the successful features of vision Transformers and traditional convolutional neural networks (CNNs). Building upon gnConv the authors propose a new family of vision model backbones termed HorNet demonstrating superior performance over existing models like Swin Transformers and ConvNeXt across several benchmarks (ImageNet classification COCO object detection and ADE20K semantic segmentation). The paper argues that the effective modeling of highorder spatial interactions previously a strength of selfattention mechanisms in Transformers can be achieved through their convolutional framework with added benefits such as efficiency extendability and translation equivariance.  2) Assessment Strengths:  The paper tackles an important problem in the field of computer vision namely the efficient and effective modeling of highorder spatial interactions which is crucial for various vision tasks.  The recursive gated convolution (gnConv) is innovative combining the benefits of vision Transformers and CNNs efficiently and showing adaptability across different tasks and model scales.  Extensive experiments demonstrate the superiority of HorNet over existing stateoftheart models providing a compelling argument for the approachs effectiveness.  The provision of source code promotes reproducibility and further exploration by the research community. Weaknesses:  While the paper provides extensive experimental validation there is a lack of theoretical analysis or deeper insights into why gnConv performs better than both Transformers and conventional CNNs beyond its ability to model highorder interactions.  The impact of varying the recursion depth (n) on different types of vision tasks (beyond the primary tasks tested) is not thoroughly explored.  The paper does not sufficiently discuss the implications of gnConv on computational efficiency beyond FLOPs such as memory consumption which is crucial for deployment in resourceconstrained environments.  The comparison lacks an analysis against hybrid CNNTransformer models other than those constructed solely based on Conv or Transformer blocks which would have provided a more comprehensive picture of the proposed methods position in the landscape of vision models.  3) Questions and Suggestions for Authors  Question: Could the authors provide more theoretical insight or intuition behind why the recursive design and gated convolutions contribute significantly to the performance improvements observed  Question: How does the models performance and computational efficiency vary with changes in the recursion depth `n` and is there an optimal range that works universally across different tasks  Suggestion: It would be beneficial to include a more detailed analysis of the models memory usage considering that this is an essential aspect of computational efficiency especially for deployment in edge devices.  Suggestion: A comparison with hybrid models that incorporate both convolution and selfattention mechanisms within their architecture could offer valuable insights into the unique benefits of the proposed gnConv method.  4) Limitations and Societal Impact The authors have not sufficiently addressed the computational and memory efficiency aspects beyond FLOP analysis. In practical applications especially in mobile and edge computing environments these factors are critical for the adoption of new models like HorNet. Furthermore the societal impact of deploying such models including potential biases in vision recognition tasks and energy consumption considerations has not been discussed. Constructive Suggestions for Improvement:  The authors should consider extending their analysis to include memory usage and latency on different platforms (e.g. GPUs vs. edge devices) to provide a more holistic view of the models efficiency.  A discussion on the potential biases in datasets and model predictions along with strategies for mitigation would address concerns regarding the negative societal impacts of deploying advanced vision models.", "mfxq7BrMfga": " Peer Review of \"Generalized Oneshot GAN Adaptation for Style and Entity Transfer\"  1) Briefly summarize the paper and its contributions The paper presents a novel framework for adapting pretrained Generative Adversarial Networks (GANs) to new domains with limited data focusing on the challenging oneshot scenario. Unlike previous works that primarily addressed style transfer this study introduces a method capable of transferring both the style and entities while maintaining crossdomain correspondence. The proposed framework employs sliced Wasserstein distance for style and entity adaptation style fixation for preliminary style acquisition and variational Laplacian regularization to ensure smooth transition and content preservation. The paper demonstrates the effectiveness of its approach through extensive experiments showing superior performance over stateoftheart methods in various domains.  2) Assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The paper addresses a lesserexplored but critical problem in GAN adaptation focusing on both style and entity transfer in a oneshot learning context which is commendable for its novelty.  Comprehensive Methodology: The use of sliced Wasserstein distance style fixation and variational Laplacian regularization is welljustified and elaborately explained providing a clear understanding of the proposed solution.  Extensive Evaluation: The paper presents a thorough evaluation with both qualitative and quantitative experiments comparing the framework against several stateoftheart methods across different domains which strengthens the claim of its effectiveness. Weaknesses:  Limited Discussion on Failure Cases: While the paper discusses its strengths theres a lack of focus on the conditions or scenarios where the proposed method may not perform as expected which is crucial for understanding its applicability limits.  Complexity and Accessibility: The complexity of the proposed framework might limit its accessibility and applicability for broader usage especially for novices in the field. A discussion on simplifications or potential optimizations could make the work more impactful.  3) Questions and suggestions for the authors  Handling of Complex Entities: How does the framework perform with highly complex entities and are there limitations to the complexity of entities that can be effectively transferred  Adaptation to Drastically Different Domains: Could the authors elaborate on the frameworks performance when adapting GANs to domains that are substantially different from the source domain  Realworld Applications: It would be beneficial if the authors could provide insights or demonstrations of realworld applications or potential use cases where their method shows significant advantages.  4) Limitations and Potential Societal Impact The authors have not thoroughly discussed the limitations of their approach particularly regarding its performance on complex entities and drastically different domains. Addressing these concerns would give a more balanced view of the frameworks applicability. Furthermore theres a lack of discussion on the potential negative societal impacts such as the misuse of the technology for creating misleading content or deepfakes. Constructive suggestions include:  Detailed Analysis on Limitations: Providing a comprehensive analysis of scenarios where the framework might underperform and proposing potential directions for future research to overcome these limitations.  Ethical Considerations: Incorporating discussions on ethical considerations and potential safeguards to prevent misuse of the technology could make the paper more responsible and impactful. In summary the paper introduces a significant advancement in the field of GAN adaptation especially for oneshot learning scenarios involving both style and entity transfer. Addressing the raised questions and concerns could further strengthen the paper.", "uPdS_7pdA9p": " Peer Review of \"Contrastive Adapter Training for Improved Group Robustness in Foundation Model Zeroshot Classification\"  1) Brief Summary and Contributions This paper addresses the challenge of improving group robustness in zeroshot classification tasks performed by large pretrained foundation models (FMs) such as CLIP. The authors identify that while FMs demonstrate impressive datasetlevel distribution shift robustness in zeroshot scenarios their robustness to subpopulation or group shifts is notably weak with accuracy gaps up to 80.7 percentage points between average and worstgroup performances. The core contributions of the paper include:  A thorough empirical analysis demonstrating the limitations of existing methods (linear probes standard adapters) in enhancing group robustness for FMs.  The introduction of a novel \"contrastive adapting\" technique which improves group robustness by training adapters through a contrastive learning approach effectively reducing worstgroup accuracy gaps significantly without requiring finetuning of the FM itself.  Extensive experimental validation across nine robustness benchmarks showing that contrastive adapting consistently outperforms zeroshot classification and existing adaptation methods in terms of worstgroup accuracy sometimes by a large margin (8.5 to 56.0 percentage points).  2) Assessment of Strengths and Weaknesses Strengths:  Significance: The paper tackles a crucial but underexplored problem of group robustness in zeroshot classification with foundation models which has profound implications for the fairness and applicability of AI models in diverse realworld settings.  Novelty: The contrastive adapting method represents a novel approach to leveraging pretrained FM embeddings for improved group robustness contributing significantly to the literature on efficient FM adaptation techniques.  Experimental Validation: The extensive benchmarks and comparative analysis provide solid evidence of the effectiveness and efficiency of the proposed method enriching its credibility. Weaknesses:  Generalization Concerns: While the paper showcases impressive results across a range of datasets and FMs theres limited discussion on how the method would generalize beyond the specific group shifts studied. More insights into its applicability to unseen group shifts or domain shifts would enhance its impact.  Complexity Analysis: The paper could benefit from a deeper analysis of the computational and practical complexities involved in deploying the contrastive adapting method especially in comparison to standard adaptation methods.  Broader Implications: The discussion on the potential negative societal impacts of the work is somewhat brief. A more detailed exploration of how misapplications of this technology could exacerbate existing biases or create new ones would be valuable.  3) Questions and Suggestions for the Authors  On Generalization: Could the authors provide additional insights or preliminary experiments on how the contrastive adapting method might perform with entirely unseen group or domain shifts  Complexity and Scalability: How does the computational complexity of contrastive adapting compare to the standard adapter or linear probe training especially in scenarios involving very large datasets or models  Societal Impacts: I encourage the authors to expand their discussion on potential negative societal impacts specifically considering scenarios where improved group robustness could unintentionally mask underlying biases in dataset construction or FM training methodologies.  4) Limitations and Societal Impact The authors acknowledge the limitations of computational approaches in addressing subgroup performance disparities emphasizing the necessity of situating FMs within broader sociotechnical systems. However the paper could further explore specific scenarios where improved group robustness might inadvertently contribute to the magnification of biases or overlooking critical ethical considerations in AI deployment. Constructive suggestions include:  Incorporating an ethical review step in the model deployment pipeline to evaluate the broader implications of deploying enhanced FMs in sensitive applications.  Encouraging collaboration with domain experts and affected communities to ensure that improved robustness aligns with ethical and socially beneficial outcomes. In summary this paper makes a significant contribution to the field by addressing the group robustness challenge in FMs with an innovative and effective method. With some enhancements in addressing the mentioned weaknesses and further exploration of limitations and societal impacts this work has the potential to set a new standard in the deployment of robust and fair AI systems.", "n4wnZAdBavx": " 1) Briefly summarize the paper and its contributions This paper introduces a novel framework MultiAgent communication via Selfsupervised Information Aggregation (MASIA) aimed at enhancing coordination in cooperative Multiagent Reinforcement Learning (MARL) through efficient message aggregation. Traditional approaches often mishandle the aggregation of multiple messages from teammates leading to suboptimal policy learning efficiency. MASIA addresses this by aggregating received messages into compact relevant representations through a permutation invariant message encoder optimized via selfsupervised learning objectives including state reconstruction and future state prediction. The evaluation of MASIA across various MARL tasks demonstrates its superiority over existing methods by showing significant improvements in communication performance.  2) Thorough assessment of strengths and weaknesses Strengths:  Novelty in Approach: The paper presents an innovative method for the aggregation of messages in MARL settings which is a less explored area. The use of selfsupervised learning for optimizing message representation is particularly notable.  Comprehensive Evaluation: Extensive experiments across multiple benchmarks and communication settings robustly demonstrate the effectiveness of MASIA. The comparisons with strong baselines and illustrative visualizations provide clear evidence of performance improvements.  Theoretical Justification: The design choices such as the permutation invariant message encoder and focusing network for relevant information extraction are welljustified with theoretical underpinnings contributing to the robustness of the framework. Weaknesses:  Complexity and Scalability: The described method appears to be computationally intensive given the complexity of the aggregation encoding and selfsupervised learning objectives. Concerns regarding scalability especially in environments with a large number of agents are not adequately addressed.  Lack of Detailed Analysis on Focusing Mechanism: While the paper discusses the focusing network mechanism a more detailed analysis or ablation study specifically focusing on this aspect could better elucidate its contribution to the overall effectiveness of MASIA.  3) Questions and suggestions  How does the computational complexity of MASIA compare with the baseline methods especially in scenarios with very high numbers of agents  An ablation study focusing on the impact of the permutation invariant message encoder and the focusing network independently could provide deeper insights into the contribution of these components to the overall systems effectiveness.  Future work could explore adaptation and scalability of MASIA to extremely largescale MARL environments. Investigating techniques like agent grouping or hierarchical message aggregation could be potential directions.  4) Limitations and potential negative societal impact The authors have not explicitly discussed the potential limitations related to the scalability of MASIA or its computational requirements. Exploring and addressing these concerns could make MASIA more practical for realworld applications involving a large number of agents. The paper also lacks a discussion on the potential negative societal impacts of their work. Given the broad applications of MARL in autonomous driving surveillance and others it is essential to consider and mitigate any negative implications such as privacy concerns and misuse in surveillance. Constructive suggestion for improvement would include a dedicated section discussing these limitations and potential negative impacts offering guidelines for responsible use and suggestions for mitigating possible risks.", "zb-xfApk4ZK": " Peer Review of \"Exploring the Efficiency of ExploreExploit Markov Chain Monte Carlo Strategies\"  1) Brief Summary of the Paper This paper introduces an innovative Markov chain Monte Carlo (MCMC) strategy named ExploreExploit MCMC (Ex2MCMC) and its adaptive counterpart FlEx2MCMC. These strategies aim to improve sampling efficiency by combining local and global sampling methods leveraging both for their distinct advantages. The authors present theoretical guarantees for the convergence and efficiency of these strategies including proving Vuniform geometric ergodicity of Ex2MCMC without requiring a uniform adaptation of the global sampler. They also explore the use of normalizing flows in the adaptive version (FlEx2MCMC) to dynamically adjust the global proposal distribution during sampling. The performance and advantages of Ex2MCMC and FlEx2MCMC are demonstrated through numerical evaluations on classical benchmarks and highdimensional distributions defined by Generative Adversarial Networks (GANs) seen as Energy Based Models (EBMs).  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Contributions: The paper makes significant theoretical contributions by providing explicit bounds on the mixing rate and proving Vuniform geometric ergodicity for the proposed MCMC strategies. These findings contribute to a deeper understanding of the behavior of MCMC methods in challenging sampling contexts.  Adaptive Strategy with Normalizing Flows: The adaptive version FlEx2MCMC uses normalizing flows to dynamically improve the global proposal distribution which is a novel approach to enhance sampling efficiency. This demonstrates an innovative integration of deep learning techniques with traditional MCMC methods.  Comprehensive Evaluation: The authors conduct a thorough evaluation of Ex2MCMC and FlEx2MCMC including comparisons against standard local and global MCMC methods. The experiments cover a range of scenarios from classical sampling problems to highdimensional GANdefined distributions showcasing the versatility and effectiveness of the proposed strategies. Weaknesses:  Limitation in HighDimensional Spaces: While the paper addresses the challenge of sampling in high dimensions there is an acknowledgment that the acceptance rate of independent proposals decreases dramatically with dimensions. This hints at potential limitations of the proposed methods in extremely highdimensional problems which could benefit from further exploration and solutions.  Computational Complexity: The paper could provide a clearer discussion on the computational complexity of Ex2MCMC and FlEx2MCMC especially in comparison to traditional MCMC methods. Given the incorporation of learningbased global proposals understanding the tradeoffs in computational requirements would be beneficial for potential users.  Generalization of Results: While the paper presents promising results the generalization of these findings to a broader range of distributions and domains could be further investigated. Additional empirical studies or theoretical analyses could enhance the robustness of the proposed methods.  3) Questions and Suggestions for the Authors  Could the authors elaborate on how the proposed methods perform in extreme highdimensional settings (e.g. dimensions in the order of thousands) and suggest potential modifications or extensions to address any identified challenges  It would be beneficial to include a more detailed discussion on the computational complexity and runtime comparison of Ex2MCMC and FlEx2MCMC against baseline MCMC methods. Can the authors provide insights or empirical data on this aspect  Considering the adaptive nature of FlEx2MCMC how sensitive are the results to the choice of hyperparameters (e.g. stepsizes and the sequence in the adaptive rule) Could the paper include guidelines or recommendations for hyperparameter selection in practical implementations  Limitations and Constructive Suggestions for Improvement  Addressing HighDimensional Limitations: The authors are encouraged to explore strategies or modifications that could mitigate the reduced efficiency in extremely highdimensional spaces potentially through more advanced learning models or alternative global proposal mechanisms.  Explicit Computational Cost Analysis: A more thorough examination and presentation of the computational costs associated with the proposed strategies would enhance the paper. This could include comparisons of computational time and resource requirements relative to traditional MCMC methods.  Enhanced Generalization Discussion: The paper would benefit from a broader discussion on the applicability and potential limitations of the proposed methods across various domains and distribution types. Including additional experiments or theoretical considerations could support this. Overall the paper presents valuable contributions to the MCMC literature offering innovative strategies that combine local and global sampling approaches effectively. Addressing the identified weaknesses and considering the provided suggestions could further strengthen the impact of this work.", "zUbMHIxszNp": " Peer Review for \"MicroMacro Objective for Graph Generation with GraphVAEs\"  1. Summary of the Paper This paper introduces a novel multilevel framework for graph data generation that considers both nodelevel properties and global graphlevel statistics simultaneously. By proposing a micromacro (MM) objective function and incorporating it into a Graph Variational AutoEncoder (GraphVAE) architecture the authors demonstrate significant improvements in the realism and diversity of generated graphs across several benchmark datasets. The MM framework enriches the GraphVAE model by enabling it to capture both detailed local structures and overarching global patterns efficiently maintaining fast generation speeds characteristic of VAEbased models.  2. Assessment of the Paper Strengths:  Novelty of the MM Framework: The introduction of a joint probabilistic model that pairs local graph properties with global graphlevel statistics under a unified training objective is a significant contribution. It paves the way for more nuanced graph generative models.  Quantifiable Improvements: The paper provides clear quantifiable evidence that adding the MM objective to GraphVAEs improves graph generation quality dramatically shown through improvements in several graph quality metrics.  Applicability: The methods versatility is illustrated by its application across various datasets demonstrating its potential utility in domains where accurate graph generation is crucial such as molecule design and recommendation systems.  Opensource Contribution: By providing implementation and datasets publicly the authors contribute to the reproducibility of their work and facilitate further research in the field. Weaknesses:  Scalability Concerns: While the paper addresses training time overhead minimally impacting performance for mediumsized graphs concerns about scalability to very large graphs remain. The paper could benefit from a deeper exploration of computational efficiency and potential scalability solutions.  Limited Discussion on Societal Impact and Ethics: While the paper briefly mentions the potential positive societal impact and ethical considerations in network data collection a deeper discussion on potential misuse and ways to mitigate negative societal impacts would enrich the paper.  Comparative Analysis: The paper showcases the superiority of the proposed method mainly against autoregressive models and could strengthen its position by including comparisons with more recent or varied graph generative frameworks.  3. Questions and Suggestions  Scalability: Could the authors elaborate on potential strategies to address scalability concerns for very large graphs Is there ongoing work to optimize the computational efficiency of calculating graph statistics or the MM objective function  Ethical Considerations: Can the authors discuss in more detail the potential negative societal impacts of their work particularly in terms of privacy and propose concrete measures to prevent misuse  Comparative Analysis: Would it be possible to provide a comparison with other types of graph generative models beyond autoregressive ones particularly those employing different architectural paradigms to showcase the strengths and potential limitations of the MM approach in a broader context  4. Limitations and Societal Impact The authors have made an effort to address the limitations related to training overhead and scalability issues although more detailed work is needed for very large graphs. Additionally while there is a brief mention of ethical considerations in network modeling a more comprehensive discussion on the potential negative societal impacts and safeguards against misuse (e.g. ensuring privacy and fairness) would make the paper more robust. Suggestions include employing privacypreserving data generation techniques and developing guidelines for ethical use of generative graph models to mitigate risks of misuse.", "tHK5ntjp-5K": " 1) Brief Summary of the Paper and Its Contributions The paper introduces LION a novel denoising diffusion model (DDM) for 3D shape generation leveraging a hierarchical VAE framework with two levels of latent space: a global shape latent and a pointstructured latent space. These innovations enable LION to produce highquality flexible and diverse 3D shapes with smooth surface outputs. LION achieves stateoftheart performance on ShapeNet benchmarks showcases its capabilities in varied applications like multimodal shape denoising voxelconditioned synthesis and also enables effective shape interpolations and reconstructions of smooth 3D meshes. The authors approach provides significant advancements for digital artistry in 3D shape generation offering a versatile tool that excels in multiple generative tasks and output types.  2) Assessment of Strengths and Weaknesses Strengths:  High Generation Quality: LION sets a new benchmark for 3D shape generation with results demonstrating superior quality over existing methods. Its ability to produce smooth meshes directly caters to the needs of digital artists.  Flexibility: The dual usage of variational autoencoder (VAE) and DDMs allows LION to adapt easily to various generative tasks making it an incredibly versatile tool. The model can be finetuned for different conditions without retraining facilitating tasks like voxelguided synthesis and denoising with remarkable efficiency.  Expressivity and Performance: Through the hierarchical VAE design and the combination with SAP for surface reconstruction LION showcases enhanced expressivity and performance on benchmark datasets solidifying its stateoftheart status. Weaknesses:  Complexity and Computation: The hierarchical model architecture and the incorporation of multiple DDMs may lead to increased computational demands potentially limiting its accessibility or utility for realtime applications without significant hardware resources.  Limited in Scope: Given its current design LION focuses on generating untextured single objects. Extending its capabilities to generate textured shapes or entire scenes could greatly enhance its utility.  3) Questions and Suggestions for the Authors Questions: 1. Have the authors considered integrating neural or differentiable rendering techniques to extend LIONs capabilities to generate textured shapes 2. What are the computational demands of LION compared to other stateoftheart models and are there strategies in place to optimize its efficiency for realtime applications 3. Could LION be adapted for generating entire 3D scenes and if yes what modifications would be necessary Suggestions:  Incorporating Texture Generation: Exploring methods to include texture generation alongside shape synthesis could significantly enhance the applicability of LION in digital content creation making it a more comprehensive tool for artists.  Optimization for RealTime Use: Investigating accelerated sampling methods or optimizing model architecture could make LION more accessible for realtime applications expanding its usecase scenarios.  Extended Applications: Further research into adapting LION for full scene generation could broaden its impact offering a powerful tool for not just single object creation but complex scene development.  4) Limitations and Societal Impact The authors have made commendable efforts in addressing the limitations of their work particularly in terms of generation quality and versatility. However the potential for LION to generate realistic 3D models rapidly could have negative implications such as facilitating the creation of deceptive imagery or unauthorized replication of copyrighted designs. While the paper does not discuss these aspects in depth recognizing and proposing measures to prevent misuse would be beneficial. Constructive suggestions include implementing watermarking techniques in generated outputs or restricting model access for verified users only to help mitigate potential misuse while fostering positive contributions to digital artistry and content creation.", "zkQho-Jxky9": " Review of \"Counterfactual Reasoning for Reducing Harm in Algorithmic Decision Making\"  Summary of the Paper The paper introduces a novel formal definition of harm and benefit through causal models addressing a significant gap in the statistical methods used for measuring harm in algorithmic decisions. The authors convincingly argue that standard machine learning algorithms which rely purely on factual inferences are inherently limited in their capacity to avoid harmful actions especially following distributional shifts. They propose a framework for incorporating harmaversion into decision making by optimizing a counterfactual objective function demonstrating its application in determining optimal drug doses from randomized control trial data. Their approach is shown to identify less harmful doses without compromising efficacy compared to standard methods that focus on treatment effects.  Assessment of Strengths and Weaknesses Strengths: 1. Innovative Conceptual Framework: One of the papers most notable strengths is its formal definition of harm through counterfactual reasoning which is a significant advancement in the ethical application of machine learning algorithms. 2. Practical Application to Medicine: The application of the proposed framework to optimizing drug doses illustrates the practical value and potential wideranging impact of the research addressing an urgent issue in medical decision making. 3. Clarification of Counterfactual Harm: The paper thoroughly explains how counterfactual reasoning can disentangle the complexities of harm and benefit in decision making offering a clear methodology for evaluating potential harms before taking actions. Weaknesses: 1. Assumption of Known Causal Models: The framework assumes a detailed understanding and the availability of structural causal models (SCMs) which might not always be practically feasible especially in complex realworld scenarios. 2. Limited Demonstrations: While the drug dosage example is compelling the paper could benefit from additional examples across diverse domains to illustrate the frameworks broad applicability and limitations. 3. Technological Readiness and Accessibility: The transition from theory to practice especially in integrating harmaversion into existing machine learning algorithms is not sufficiently addressed. The technological readiness and accessibility for broad implementation of this framework remain unclear.  Questions and Suggestions for the Authors 1. Handling Unknown or Incomplete Causal Models: Can the authors propose methods or heuristics for applying their framework in situations where the causal model is not fully known or is too complex to be practically usable 2. Extension to Other Domains: Could the framework be adapted or extended to decisionmaking processes outside of medicine such as in criminal justice or financial planning where harm avoidance is also critical 3. Technological Integration: What are the practical challenges in integrating the proposed harmaversion framework with existing machine learning algorithms and how might these be overcome  Limitations and Constructive Suggestions for Improvement The paper does not thoroughly address the limitations regarding the practical applicability of its framework particularly the assumption that detailed SCM knowledge is always available and usable. Also the potential negative societal impacts such as biases in causal models leading to harm misestimation are not discussed. Suggestions for Improvement:  It would enhance the paper to discuss approaches for approximating harm when complete SCMs are unavailable perhaps through simulations or partial models.  The authors should consider exploring and discussing the ethical considerations and potential negative societal impacts of misestimating harm particularly in sensitive applications like healthcare and criminal justice.  Providing guidelines or a roadmap for technology developers to adopt and integrate this harmaverse decisionmaking framework into existing and future machine learning systems could significantly enhance its practical impact.", "osPA8Bs4MJB": " Review of \"Local Temporalaware Transformerbased Deepfake Detection (LTTD) Framework\"  Summary of the Paper The paper introduces a novel framework named Local Temporalaware Transformerbased Deepfake Detection (LTTD) that aims to address the challenge of detecting deepfake videos. The proposed framework incorporates a Local Sequence Transformer (LST) that focuses on enhancing the temporal consistency of local patch sequences with an emphasis on lowlevel information through shallow learned 3D filters. The LTTD framework achieves final classification using global contrastive supervision and aggregates local information through the CrossPatch Inconsistency (CPI) loss and the CrossPatch Aggregation (CPA) module. The authors validate their approach through extensive experiments on popular deepfake datasets demonstrating stateoftheart performance in terms of generalizability and robustness against deepfake videos.  Assessment of Strengths 1. Innovative Methodology: The integration of the LST module for modeling the temporal consistency within local sequences presents an innovative approach. This localtoglobal learning strategy effectively captures subtle local forgery cues enhancing the models capability to detect deepfakes. 2. Robustness and Generalizability: One of the key strengths highlighted is the frameworks robustness to common postprocessing perturbations and its generalizability across various deepfake generating methods. This has been a persistent challenge in the field and the proposed framework offers promising solutions. 3. Quantitative and Qualitative Analysis: The paper presents comprehensive quantitative results augmented with qualitative insights such as feature representation visualizations and forgery localization. Such thorough analysis aids in understanding why the proposed method outperforms existing techniques.  Questions and Suggestions for the Authors 1. Temporal Token Inquiry: The paper introduces a learnable temporal token in the Local Sequence Embedding stage. It would be beneficial if the authors could elaborate on the role and impact of this temporal token in enhancing the temporal modeling capabilities of the framework. 2. Performance on Highly Compressed Videos: While the framework shows robustness against several perturbations its important to discuss its performance on highly compressed videos a common scenario in realworld applications. Could the authors comment on or include results for this scenario 3. Future Work on Adversarial Resistance: Given the frameworks emphasis on lowlevel temporal patterns how might it perform against deepfakes generated through adversarial methods specifically aimed at defeating temporal consistency checks Suggestions for future enhancements to improve resistance against such adversarially crafted deepfakes would be valuable.  Limitations and Societal Impact Considerations The authors have acknowledged potential limitations related to the advancing deepfake generation techniques and the unexplored performance of their approach against adversarially enhanced deepfakes. However a deeper discussion on the frameworks scalability and computational efficiency especially when deployed in realworld environments with high video throughput would complement the current limitation discussion. Regarding potential negative societal impacts while the immediate goal of combating deepfakes is commendable its crucial to consider and discuss the dualuse nature of such detection technologies. For instance overreliance on automated systems can lead to false positives that may unfairly target content creators. Furthermore evolving deepfake technologies might adapt more sophisticated methods to bypass detection frameworks necessitating a continuous effort in research. Constructive suggestions include advocating for transparency in the usage of detection technologies and promoting broader awareness and educational campaigns on the implications of deepfakes.", "wph_3smhuec": " Review of \"Bilevel Optimization Problems with Nonconvex Lower Levels: A GameTheoretical Approach\"  1) Summary of the Paper and Its Contributions This paper presents a comprehensive study on bilevel optimization problems where the lowerlevel problem is nonconvex leading to ambiguities due to the presence of multiple critical points. The core contribution is the introduction of Bilevel Games with Selection (BGS) a novel framework that utilizes a selection map to anticipate the solution of the lowerlevel problem by the upperlevel decisionmaker. This approach effectively resolves the ambiguity inherent in traditional bilevel problems. By employing Morse theory and extending implicit differentiation techniques beyond the implicit function theorems applicability the paper establishes a firm analytical foundation for the BGS framework. Furthermore it demonstrates how existing bilevel optimization algorithms can be adapted to solve BGS by incorporating a simple correction term thereby improving their practical utility and accuracy.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Innovation: The introduction of BGS as a means to tackle the nonconvex lowerlevel problem in bilevel optimization is novel and provides a significant theoretical advancement in the field.  Analytical Rigor: The use of Morse theory to extend the application of implicit differentiation is both innovative and rigorously executed. This approach provides valuable insights into dealing with degenerate critical points.  Practical Relevance: The correction proposed for standard bilevel optimization algorithms addresses a critical gap in current methodologies offering a direct path to enhancing their practical effectiveness. Weaknesses:  Complexity of Implementation: While the theoretical foundations are strong the practical implementation of the proposed solutions (especially Morse theory and the gradient flowbased selection) may be complex and computationally intensive.  Limited Experimental Validation: Although the paper discusses the theoretical implications of the proposed methods and their potential to correct biases in existing algorithms it lacks comprehensive experimental results that validate these claims across diverse scenarios.  Assumption Sensitivity: The effectiveness of the proposed approach heavily relies on the assumptions made such as the differentiability of the selection map and the parametric MorseBott function property. The performance in scenarios where these assumptions are not fully met remains uncertain.  3) Questions and Suggestions for the Authors  Experimental Results: Can the authors provide more extensive empirical evidence to substantiate the practical effectiveness of the proposed corrective approach in solving BGS Specifically how does it perform compared to existing algorithms on standard bilevel optimization benchmarks  Implementational Details: Could the authors elaborate on the computational complexity of implementing their proposed solutions particularly regarding the selection map and handling degenerate critical points  Robustness Across Assumptions: How robust is the proposed BGS framework in situations where the underlying assumptions (e.g. MorseBott property smoothness assumptions) may not hold strictly It would be beneficial to explore the impact of relaxing these assumptions on the solution quality.  4) Limitations and Societal Impact The authors have made a commendable effort in addressing the theoretical limitations inherent in bilevel optimization problems with nonconvex lower levels. However the practical implications of computational complexity and the robustness of their approach under relaxed assumptions require further elucidation. Suggestions for Improvement:  Detailed Guidance for Practitioners: Given the complexity of the proposed methods a more detailed implementation guide or opensource reference implementation would significantly lower the barrier to adoption.  Expanded Experimental Analysis: Additional experiments including comparisons with stateoftheart algorithms on a wider array of problems could further validate the effectiveness and efficiency of the proposed approach. Societal Impact: The paper does not explicitly discuss potential negative societal impacts. However as with any optimization method the societal implications of its applications should be carefully considered especially in sensitive domains like healthcare finance or autonomous systems where optimization decisions might have farreaching consequences. In summary this paper makes a substantial theoretical contribution to the field of bilevel optimization offering innovative approaches to overcoming the challenges posed by nonconvex lowerlevel problems. With enhancements to its practical application guidance and further empirical validation its methods could see significant use in addressing complex decisionmaking problems in various domains.", "qx51yfvLnE": "Review of the paper \u201cGreedy Online Contention Resolution Schemes and Applications in Combinatorial Optimization\u201d 1) Summary: The paper provides an indepth examination of Online Contention Resolution Schemes (OCRSs) particularly focusing on greedy OCRSs for addressing realworld combinatorial optimization problems under uncertainty. The study introduces algorithms for optimally rounding an optimal solution to a linear program in an online manner for different sets of constraints showcasing significant advancements in ad allocation matching markets and stochastic probing. The core contributions include the development of optimal predictable algorithms for singleitem settings partition matroids and transversal matroids enhancing the performance of greedy OCRSs beyond the current stateoftheart. The paper demonstrates the strength and applicability of these schemes in various scenarios including situations where the adversary knows future events and random elements thereby facing the \"almighty adversary.\" 2) Strengths and Weaknesses: Strengths:  The paper addresses a critical area in machine learning and AI particularly in problems plagued by uncertainty providing a robust mathematical framework and empirically validated solutions.  The introduction of optimal greedy OCRSs for a range of constraints presents a significant leap forward from previous works with the potential to influence various applied domains.  The theoretical grounding and the proofs provided for the selectability of OCRSs are thorough and compelling offering a solid foundation for the claims made.  The empirical evaluation section with experiments on generated instances and worstcase scenarios convincingly supports the theoretical claims enhancing the papers credibility. Weaknesses:  While the paper excels in presenting new theoretical advancements there is a noticeable gap in discussing the practical implications and potential applications of these OCRSs outside simulated or theoretical environments.  The discussion on the societal impact and ethical considerations of this research is notably lacking or underdeveloped. Given the broad applicability of these schemes further analysis in this area could have strengthened the paper.  The experimental setup although comprehensive could benefit from comparisons with realworld datasets or applications to demonstrate the practical efficacy of the proposed schemes more convincingly. 3) Questions and Suggestions for the Authors:  Can the authors provide more insights into how these greedy OCRSs might be adapted or applied to realworld problems especially in domains not traditionally associated with combinatorial optimization  It would be beneficial if the authors could elaborate on the potential negative societal impacts of their work especially in scenarios where algorithms could exacerbate existing biases in decisionmaking processes.  A suggestion for future work could involve collaborating with industry practitioners to test the realworld applicability of these schemes in online ad allocation ecommerce and beyond. This could also help in identifying unforeseen ethical and societal challenges. 4) Limitations and Societal Impact: The authors have made notable contributions to the field of online stochastic optimization however they have not thoroughly addressed the limitations and potential negative societal impacts of their work. In future iterations exploring and elaborating on the broader implications of deploying such OCRSs especially in sensitive areas like healthcare or finance would be invaluable. Constructively the authors could engage with interdisciplinary experts to evaluate and mitigate any potentially negative consequences ensuring that the applications of their research contribute positively to society. In conclusion this paper represents a significant step forward in the study of Online Contention Resolution Schemes offering valuable insights and tools for tackling uncertainty in combinatorial optimization. Improvements in discussing realworld applications societal impacts and ethical considerations would make the contributions even more compelling and relevant.", "zt4xNo0lF8W": " Review of \"Mask Matching Transformer (MMFormer) for FewShot Segmentation\"  1. Summary and Contributions This paper introduces a novel approach for tackling the challenge of fewshot segmentation called Mask Matching Transformer (MMFormer). The primary innovation of MMFormer is its twostage architecture which first generates classagnostic mask proposals for a query image through a Potential Objects Segmenter (POS) and then efficiently matches these proposals to support images using a simple matching mechanism in the Mask Matching Module (MM Module). This framework shifts away from the prevalent paradigms that tightly couple the learning of segmentation and matching tasks aiming to offer improved flexibility reduced training complexity and better generalization across diverse scenarios. The approach is validated through extensive experiments on COCO20i and Pascal5i benchmarks showing competitive results and highlighting MMFormers strengths in generalization and performance.  2. Assessment of Strengths and Weaknesses Strengths:  Innovative Twostage Framework: The decomposed learning process which separates the tasks of segment proposal generation and mask matching is a significant departure from existing methods. This design choice not only simplifies the learning process but also likely contributes to the methods strong generalization capability observed across different datasets.  Generalization Across Datasets: MMFormer demonstrates robust performance when models trained on COCO are tested on Pascal5i which is a robust indication of its effective learning and generalization mechanism.  Publicly Available Code: Providing code at a public repository greatly facilitates reproducibility and further experimentation by the community accelerating progress in the fewshot segmentation domain. Weaknesses:  Limited Improvement on Pascal5i: When trained on the Pascal dataset exclusively the MMFormers improvements are not as notable compared to other stateoftheart methods. This may point towards potential optimization or architectural adjustments specific to smaller datasets that could be explored further.  Gap to Oracle Performance: Despite achieving stateoftheart results there remains a significant performance gap compared to the oracle. While this indicates room for improvement the current methods to bridge this gap are not adequately discussed or proposed within the paper.  Computational Efficiency: While the twostage training strategy is highlighted as beneficial for model performance there is limited discussion on its implications for computational efficiency training time and resource consumption which are critical considerations for practical deployment.  3. Questions and Suggestions for the Authors  Could the authors elaborate on potential strategies to close the performance gap between the current results and the oracle Are there specific components within the MMFormer architecture that are suspected to be limiting factors  In the context of limited improvement on certain datasets such as Pascal5i have the authors considered datasetspecific optimizations or augmentations that might enhance performance  Regarding the computational efficiency of the twostage training strategy could the authors provide insights into the comparative training time and resource requirements vis\u00e0vis comparable singlestage fewshot segmentation approaches  4. Limitations and Constructive Suggestions for Improvement The authors have acknowledged the gap between current results and the oracle suggesting areas for future research. However a detailed discussion on addressing this performance gap is lacking. It might be beneficial for the authors to explore and discuss potential enhancements to their MMFormer architecture or training strategy that could further narrow this gap. Additionally considering the mixed performance improvements across datasets investigating datasetspecific modifications or more sophisticated data augmentation techniques could prove fruitful. While the paper briefly touches upon the societal impact of their work a more thorough exploration of potential negative consequences such as privacy concerns related to more effective segmentation tools could provide a more balanced view of the technologys societal implications. Overall the MMFormer paper presents a compelling new approach to fewshot segmentation with strong theoretical foundations and promising experimental results. Addressing its acknowledged limitations and exploring the raised questions could further strengthen the work\u2019s impact and applicability.", "qZUHvvtbzy": "PeerReview Report 1) Summary of the Paper and Its Contributions This paper tackles the quantum manybody problem a pivotal challenge across various fields of physics. It introduces an innovative approach leveraging neural network wave functions specifically restricted Boltzmann machines (RBMs) for variational estimation of quantum states. The main contributions lie in the enhancement of RBM wave functions through symmetry projection and a Lanczos recursion method resulting in stateoftheart accuracy for the ground state representation of the Heisenberg J1  J2 model. The paper outlines the underlying challenges of the quantum manybody problem reviews existing solutions and details their methodology which incorporates combined RBMs symmetry restoration and Krylov basis expansion via Lanczos recursion. The results including benchmarking against exact diagonalization and comparison with quantum Monte Carlo and other numerical methods demonstrate the superiority of their approach in both accuracy and computational efficiency for selected lattice sizes. 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper proposes a novel method combining the simplicity and computational efficiency of RBMs with the concept of symmetry projection and Lanczos recursion enabling systematic improvements in wave function accuracy with moderate computational efforts.  Significant Results: It achieves stateoftheart accuracy in representing the ground state of the Heisenberg J1  J2 model on a square lattice outperforming existing neural network methods and other numerical techniques in specific regimes.  Comprehensive Methodology: The authors provide a rigorous and detailed description of their technique from the basic formalism of neural network wave functions to the implementation of symmetry considerations and the optimization process. Weaknesses:  Limited Scope in Application: The paper focuses predominantly on the Heisenberg J1  J2 model. Although this model is crucial for studying quantum manybody systems the applicability and effectiveness of the proposed method on a broader range of models remain to be demonstrated.  Potential Overfitting Risk: While the Lanczos recursion improves accuracy theres a potential risk of overfitting the ground state to the specific Hamiltonians considered raising questions about the generalizability of the approach.  Computational Complexity: Despite improvements the computational complexity can still be a challenge for larger lattices. Its mentioned that the computational bottleneck may limit broader application and error rates in correlation functions suggest limitations in the methods efficacy for all observables. 3) Questions and Suggestions for the Authors  Broadening the Methods Applicability: Have the authors considered applying their method to models beyond the Heisenberg J1  J2 model to test its generalizability  Exploring Other Neural Network Architectures: Given the success of RBMs enhanced by symmetry projection and Lanczos recursion might other neural network architectures (e.g. CNNs or GNNs) further improve accuracy or computational efficiency  Addressing the Overfitting Concern: Could the authors discuss potential strategies for avoiding or minimizing overfitting when applying Lanczos recursion ensuring that improvements in accuracy are indeed indicative of a more universally applicable solution 4) Limitations and Potential Negative Societal Impact The authors have acknowledged the computational challenges and potential limitations in error rates for certain observables. They note that while Lanczos steps provide significant improvements these diminish as system size increases or as additional steps are added hinting at scalability issues. A suggestion for improvement would be to explore parallel computation techniques or more efficient sampling methods to mitigate these computational challenges. Regarding societal impact the authors correctly note that their work is theoretical with a focus on quantum simulations and thus does not directly entail negative societal consequences. However a broader discussion on the implications of quantum computing advancements including issues of accessibility and the digital divide in computational resources could enrich the papers contextual understanding. Acknowledgments and Disclosure of Funding: The inclusion of funding sources and acknowledgments is appropriate and enhances the transparency of the research.", "yts7fLpWY9G": " Review of \"Graph Neural Networks with Adaptive and Differentiable Readouts\"  1) Summary: The paper investigates adaptive and differentiable readout functions for Graph Neural Networks (GNNs) to enhance graphlevel representation learning for tasks such as molecule binding affinity prediction. Traditionally GNNs employ simple nonadaptive readouts (sum mean max) that are permutation invariant to aggregate node features into a graphlevel representation. However these may not effectively capture complex embeddings required for certain tasks. This work introduces neural networkbased adaptive readouts which unlike traditional readouts can accommodate permutation variability to some extent. The authors empirically show across over 40 datasets that their approach consistently outperforms standard readouts across various metrics neighborhood aggregation iterations and convolutional operators. They categorize their readout functions into three classes: Set Transformer Readouts Janossy Poolingbased Readouts and Plain FeedforwardRecurrent Readouts detailing their potential to learn more expressive models for graphstructured data.  2) Assessment: Strengths:  The paper addresses a significant gap in GNN literature by focusing on the adaptiveness of readout functions which has been less explored compared to neighborhood aggregation schemes.  Extensive empirical evaluation supports the claim that adaptive readouts can lead to better performance across a wide range of datasets offering a robust validation of the proposed methods.  The differentiation between types of readout functions and the exploration of their relative advantages over standard readouts provide valuable insights into designing more effective GNN models. Weaknesses:  While the paper claims that their methods outperform standard readouts the evaluation lacks a discussion on the computational efficiency and scalability of adaptive readouts versus standard ones.  The practical implications of relaxing permutation invariance are not fully explored particularly regarding how this might affect model interpretability and transferability across tasks.  There is little discussion about the failures and limitations of the proposed methods. Detailed analysis of instances where adaptive readouts do not perform as expected would provide a more balanced view.  3) Questions and Suggestions:  Question: Could you elaborate on the computational overhead introduced by adaptive readouts compared to standard readouts  Suggestion: It would be beneficial to include a deeper analysis of the tradeoffs between permutation invariance and model performance possibly by comparing the adaptive and standard readouts in settings where graph isomorphism plays a critical role.  Suggestion: The paper could benefit from a detailed discussion on the implications of these findings for model interpretability especially since permutation variability is introduced.  Question: How do the different adaptive readouts compare in terms of their resilience to overfitting given their varying degrees of complexity  4) Limitations: The authors have not fully addressed the computational efficiency and potential scalability challenges of adaptive readouts. Moreover the societal impacts of potentially reduced model interpretability due to relaxing permutation invariance are not discussed. Lastly although the paper presents an extensive empirical validation it lacks a thorough theoretical analysis explaining why adaptive readouts perform better which could be crucial for understanding the limitations of these approaches. Suggestions for Improvement:  A comparative analysis focused on computational costs and model scalability would provide more clarity on the practical applicability of the proposed methods.  Including a discussion on the potential challenges and strategies for maintaining model interpretability when employing adaptive readouts would be valuable.  Expanding the theoretical foundation of adaptive readouts could enhance the papers contributions potentially explaining scenarios where these methods may not offer significant improvements.", "o762mMj4XK": "The paper introduced a novel algorithm Balanced Neural Ratio Estimation (BNRE) designed to improve the reliability of posterior surrogates within the context of simulationbased inference (SBI) particularly in regimes of limited simulation budgets. SBI leverages computer simulations to describe complex stochastic systems for which the likelihood function is implicitly defined and thus classical inference techniques are inapplicable. Previous efforts such as Neural Ratio Estimation (NRE) approximate parts of Bayes rule to facilitate inference but have demonstrated tendencies to produce overconfident posterior estimators. The core contribution of this paper is the development of the BNRE approach which aims to generate more conservative and thus reliable posterior approximations by enforcing a balancing condition without adding computational burden or compromising the asymptotic accuracy of the estimators.  Strengths: 1. Originality and Significance: The paper addresses a critical challenge in the domain of SBI by proposing an innovative remedy for the problem of overconfidence in posterior estimators. Given the increasing reliance on SBI across various scientific disciplines the potential impact of this work is considerable. 2. Theoretical Justification: The theoretical underpinnings of BNRE presented through a series of wellarticulated theorems offer a solid foundation for the proposed approach. The logical flow from problem identification to solution is both coherent and compelling. 3. Experimental Validation: The authors conduct a rigorous empirical evaluation of BNRE across diverse benchmarks demonstrating its ability to produce conservative posteriors consistently. The comparative analysis with NRE lays out clear improvements reinforcing the utility of BNRE.  Weaknesses: 1. Generalization to Highdimensional Parameters: The paper acknowledges the limitations in evaluating the performance of BNRE in highdimensional parameter spaces due to the reliance on discretized grids for computing credibility regions. This limitation may restrict the applicability of the method in complex scenarios common in scientific investigations such as climate modeling or computational biology. 2. Balance Condition Estimation: While the method is theoretically sound the balancing conditions enforcement through regularization is estimated via Monte Carlo sampling. This approximation introduces potential inaccuracies in ensuring the balance condition which the paper recognizes but does not fully explore in terms of its impact on classifier performance. 3. Comparative Depth: Although the paper presents an extensive evaluation of BNRE it predominantly compares it against NRE. Including comparisons with other SBI techniques or alternative approaches to ensuring conservative posterior estimates could enhance the papers contributions by situating BNRE within a broader methodological landscape.  Questions and Suggestions: 1. Enhancing Applicability to Highdimensional Spaces: Can the authors consider adaptive sampling techniques or advances in highdimensional Bayesian inference to mitigate the computational challenges of applying BNRE in complex parameter spaces 2. Robustness to Model Misspecification: Given the assumption of a wellspecified simulator model how sensitive is BNRE to model misspecifications Exploring this aspect could further validate the robustness of BNRE. 3. Exploration of Alternative Loss Functions: The paper utilizes binary crossentropy loss. Could exploring alternative loss functions potentially with intrinsic properties favoring conservativeness offer additional benefits or improvements in performance 4. Impact of Hyperparameter \\( \\lambda \\): While \\( \\lambda  100 \\) was empirically found to work well across benchmarks the sensitivity of BNRE\u2019s performance to changes in \\( \\lambda \\) is not extensively discussed. Providing more insights into how \\( \\lambda \\) influences the balance conditions fulfillment might help practitioners in effectively tuning BNRE.  Limitations and Societal Impact: The paper rightly identifies important limitations associated with the application of BNRE in highdimensional spaces and in scenarios where the enforcement of the balance condition might not be perfectly achieved. Addressing these limitations through future research is crucial for the broader applicability of BNRE. While the paper does not extensively discuss the potential negative societal impact its worth noting that any improvement in the reliability of simulationbased inferences especially in critical fields such as epidemiology or environmental science inherently contributes positively to societal wellbeing. However the overreliance on conservative estimates could potentially slow down scientific advancements if not properly balanced with the need for precise and actionable insights. Overall this work is a valuable contribution to the field addressing a wellidentified gap in simulationbased inference with a solid theoretical foundation and promising empirical results. Further exploration in the areas suggested could augment the robustness and applicability of BNRE making it a significant tool in the arsenal of computational scientists and engineers.", "zBlj0Cs6dw1": " Peer Review for \"RLCG: Accelerating Column Generation with Reinforcement Learning\"  1. Brief Summary The manuscript introduces RLCG an innovative approach that integrates Reinforcement Learning (RL) with Column Generation (CG) to solve largescale linear programs (LPs). CG is an iterative algorithm widely used for tackling LPs with a large number of variables where only a subset of variables (columns) is considered in each iteration. Traditional column selection methods in CG often rely on myopic heuristics like selecting the column with the most negative reduced cost. Contrarily RLCG conceptualizes column selection as a sequential decisionmaking problem wherein decisions on column inclusion influence future selections and the overall convergence of the CG process. Utilizing Deep Reinforcement Learning with Graph Neural Networks (GNNs) to represent LP structure RLCG outperforms traditional greedy and lookahead heuristics by a significant margin on benchmark instances of the Cutting Stock Problem (CSP) and the Vehicle Routing Problem with Time Windows (VRPTW).  2. Strengths and Weaknesses Strengths:  Novel Integration of RL with CG: The marriage of RL to CG to form RLCG is both novel and significant addressing the limitations of previous myopic heuristics by considering the sequential and cumulative impact of column selection decisions.  Superior Performance: Demonstrated improvements in convergence speed (22.4 for CSP and 40.9 for VRPTW) over traditional heuristics showcase the practical effectiveness and potential of RLCG in solving complex LPs efficiently.  Strong Methodological Foundation: The use of GNNs for capturing the variableconstraint structure of LPs is methodologically sound and innovative leveraging recent advances in machine learning for optimization.  Extensive Experimental Validation: The authors provide a comprehensive evaluation of RLCG against traditional heuristics using standard benchmarks showing robustness and generalization capabilities. Weaknesses:  Scalability Concerns: While RLCG shows impressive performance on benchmark problems its scalability to even larger problem instances remains unclear especially as the complexity and size of the bipartite graph representation grow.  Computational Overhead of RL Training: The training of the RL agent particularly the curriculum learning aspect could introduce significant computational overhead potentially offsetting the gains in convergence speed during the initial deployments.  Limited Discussion on Practical Implementation Challenges: The paper could benefit from a more thorough discussion on the practical challenges of integrating RLCG within existing optimization workflows particularly in terms of computational resources and expertise required.  3. Questions and Suggestions for the Authors  Scalability: Could the authors elaborate on potential scalability issues with RLCG and suggest strategies for mitigating computational challenges as problem sizes increase  Comparison with Other ML Approaches: It would be beneficial if the authors could compare RLCG with other machine learningguided heuristics for solving large LPs providing context on where RLCG stands in the spectrum of existing methods.  Implementation Details: Additional details on the training infrastructure required (e.g. computational resources time) would provide valuable insights into the practicality of deploying RLCG.  Further Applications: Are there other problem domains aside from CSP and VRPTW where RLCGs approach could be particularly beneficial Exploring and discussing potential applications could highlight the versatility of the method.  4. Limitations and Societal Impact The manuscript could further articulate potential limitations such as scalability issues and the computational overhead of training the RL agent. While RLCG shows promise in speeding up LP convergence a candid discussion on the environmental impact of potentially increased computational demands especially during agent training would be pertinent. Suggestions for improvement include exploring more efficient training algorithms or incremental learning approaches that could mitigate these concerns. Additionally addressing how RLCG can be adapted or simplified for practitioners without deep expertise in RL could maximize its positive societal impact by broadening its accessibility and applicability.", "vt516zga8m": " Peer Review for \"CacheAugmented Inbatch Importance Resampling for Training Recommender Retrievers\"  1. Summary of the Paper The paper introduces a novel methodology called CacheAugmented Inbatch Importance Resampling (IR) aimed at improving the training of recommender retrievers particularly for the widely used twotower model architecture. Recommender retrievers are essential in filtering highly relevant items from a vast item corpus in response to user queries a critical step for coping with information overload in recommendation systems. Traditional approaches to optimizing these retrievers have harnessed inbatch sampling for efficiency leveraging items within the same minibatch as negative samples for estimating the softmax function. However these methods suffer from biases due to their inability to differentiate between user queries and the static nature of the sampling distribution. The proposed IR method innovates by resampling inbatch items based on modified softmax weights offering a dynamic and adaptive strategy that accounts for queryspecific differences and captures the training models dynamic changes. A cache mechanism is employed to store frequently sampled items providing a rich source of informative negatives to further refine the recommender model. The effectiveness of IR over competitive methods is demonstrated through experiments on five public datasets showcasing significant improvements in model convergence and recommendation accuracy.  2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper addresses a significant limitation in the traditional inbatch sampling methods by incorporating a dynamic cacheaugmented resampling strategy that adapts to queryspecific needs and model changes.  Theoretical Justification: The methodology is wellsupported by theoretical analysis including a lemma and theorems that articulate the methods ability to more accurately approximate the softmax distribution.  Empirical Validation: The performance of the proposed method is validated on multiple realworld datasets providing a comprehensive comparison with existing methods and demonstrating superior performance in terms of NDCG and RECALL metrics. Weaknesses:  Complexity and Scalability: While the paper provides a complexity analysis the additional computational cost introduced by the cache mechanism and importance resampling might pose scalability issues for very large item corpora or in environments with strict latency requirements.  Generalizability: The experiments focus on the twotower model architecture within the context of recommender systems. It remains unclear how the proposed methods would adapt or perform in varying architectures or different application domains outside of recommendation systems.  3. Questions and Suggestions for the Authors  Scalability Concerns: Could the authors elaborate on how the proposed method scales with extremely large datasets not covered in the experiments and suggest potential optimizations to mitigate any efficiency losses  Impact of Cache Size: The paper briefly explores the effect of cache size on model performance. More detailed analysis or guidelines on choosing an optimal cache size could be beneficial for practitioners implementing this method.  Extension to Other Architectures: It would be valuable to discuss or hypothesize how the IR method could be adapted to other model architectures or recommendation domains beyond the twotower models and whether similar performance improvements can be expected.  4. Limitations and Societal Impact The authors have adequately addressed the potential limitations regarding the complexity and generalizability of their method. To further improve they could consider discussing the implications of introducing a dynamic cache mechanism on realtime system performance and user experience. Additionally exploring the broader applicability of the proposed method across various recommendation systems and domains would be insightful. Regarding societal impact while the paper does not explicitly address potential negative effects ensuring fairness and avoiding reinforcement of biases within recommendation systems using dynamic resampling methods is essential. The authors could consider including a discussion on ethical considerations and bias mitigation strategies to ensure their contributions positively impact society.  Conclusion The paper makes a significant contribution to the field of recommender systems by addressing a critical issue in the training of recommender retrievers with an innovative cacheaugmented resampling strategy. While there are areas for further exploration regarding scalability and generalizability the proposed methods theoretical foundation and empirical validation make it a valuable advancement in recommendation system research.", "kyY4w4IgtM8": " Review of \"MetaLearning with Natural Language Feature Descriptions for Improved Predictive Performance on Unseen Tasks\"  1. Summary of the Paper This paper introduces a novel metalearning approach that leverages natural language feature descriptions to share knowledge across supervised learning tasks aiming to enhance predictive performance on unseen tasks with limited labeled data. By encoding feature descriptions with a pretrained language model (e.g. BERT) the method can draw on vast human knowledge embedded in language corpora facilitating the discovery of relationships across tasks with different feature spaces. The proposed model embeds instances in a taskspecific space which when combined with Gaussian Process (GP) regression results in a tailored prediction model for each task. The authors evaluate their method on a diverse set of realworld datasets from the statistical offices of the EU and Japan demonstrating superior performance compared to existing metalearning methods.  2. Strengths and Weaknesses Strengths:  Novelty: The integration of natural language feature descriptions into the metalearning framework is innovative and welljustified addressing a significant gap in current methods that ignore such valuable information.  Empirical Evaluation: The comprehensive experiments using realworld datasets and a variety of comparative baselines demonstrate the methods effectiveness in improving predictive performance on unseen tasks.  Potential for Generalization: The methods ability to handle tasks with different feature spaces and its empirical success suggest it could be broadly applicable across various domains. Weaknesses:  Model Complexity: The increased complexity from encoding feature descriptions and translating them for supervised learning tasks may lead to higher computational costs and longer training times which was not thoroughly discussed.  Dependency on Language Model Quality: The performance of the proposed method heavily relies on the quality of the pretrained language model which might limit its effectiveness in languages or domains where such models are not welldeveloped.  Limited Evaluation on Task Types: The experiments mainly focus on tasks with categorical features and numerical labels. The performance and applicability of the method to other types of tasks (e.g. tasks with all numerical features and categorical labels) were not extensively evaluated.  3. Questions and Suggestions for the Authors  Generalizability to Other LanguagesDomains: Have the authors considered evaluating the method in a domain or language where pretrained language models might not be as effective or widely available This could help understand the methods robustness and applicability further.  Impact of Feature Description Quality: How sensitive is the method to the quality and specificity of feature descriptions Adding a study to assess this sensitivity could provide valuable insights into the methods practical implementation.  Scalability Considerations: Can the authors elaborate on the methods scalability especially concerning the computational resources required as the number and complexity of tasks increase  4. Limitations and Negative Societal Impact The authors touch upon the limitations in empirical evaluation but do not fully discuss the potential computational inefficiency or the methods reliance on the availability of highquality language models. Additionally there is no mention of any potential negative societal impacts. Considering the methods dependency on language models which might encode biases present in the training data it would be prudent for the authors to acknowledge and suggest ways to mitigate bias amplification in metalearning tasks. Constructively future work could explore strategies for efficient computation evaluate robustness across languagesdomains less represented in language models and develop mechanisms to detect and correct for biases in feature descriptions.", "lgNGDjWRTo-": " Brief Summary of the Paper This paper introduces the PeriodicGraph Disentangled Variational Autoencoder (PGDVAE) a novel deep generative model specifically designed to handle the generation of periodic graphs which are characterized by their repetitive local structures. Unlike traditional models that rely heavily on domainspecific knowledge or geometrybased rules for periodic graph generation PGDVAE learns to automatically disentangle and generate both local and global patterns that characterize these graphs. The approach involves a novel encoderdecoder architecture that ensures the disentanglement of these patterns and employs a unique learning objective that promotes the invariance of localsemantic representations. Extensive experiments demonstrate the effectiveness of PGDVAE over existing methods on realworld and synthetic datasets.  Assessment of Strengths and Weaknesses Strengths: 1. Novelty in Approach: PGDVAE addresses the unique challenges of periodic graph generation such as ensuring graph periodicity and efficiently learning repetitive patterns. Its methodology for disentangling local and global patterns is innovative and wellconceived. 2. Comprehensive Architecture: The encoderdecoder architecture with specific components designed to encode and decode local and global patterns is meticulously designed. This architecture coupled with the assembler component effectively ensures the generation of coherent periodic structures. 3. Experimental Validation: The paper presents comprehensive experiments that validate the effectiveness efficiency and scalability of PGDVAE compared to stateoftheart methods. The inclusion of realworld and synthetic datasets enhances the credibility of the evaluation. Weaknesses: 1. Complexity and Implementation Challenge: The sophisticated architecture while a strength may also pose a barrier to understanding and implementing the model. Simplifying or providing more intuitive explanations could be beneficial. 2. Limited Discussion on Limitations: While the paper mentions conducting extensive experiments there is a lack of detailed discussion regarding the limitations of PGDVAE particularly in scenarios where it might underperform. 3. Societal Impact Considerations: The paper does not discuss the potential negative societal impacts of periodic graph generation such as misuse in designing harmful materials.  Questions and Suggestions for the Authors 1. Regarding Model Generalizability: How does PGDVAE perform on graphs that have irregular periodic patterns or on datasets vastly different from those tested 2. On Computational Efficiency: Given the models complexity can the authors comment on its computational demands especially when scaling up to much larger graphs than those tested 3. Future Directions: Could the authors elaborate on potential extensions of PGDVAE to incorporate node and edge features How might this affect the models current architecture and performance  Addressing Limitations and Societal Impact The authors have not extensively discussed the limitations of PGDVAE or its potential negative societal impacts. Suggestions for Improvement:  Detailing Limitations: The authors should offer a thorough examination of scenarios where PGDVAE may face challenges possibly including graphs with nonuniform patterns or when applied to domains with sparse data.  Societal Impact Discourse: It would be beneficial to include a section discussing how the technology could be misused such as in the creation of materials with negative environmental impacts and suggesting mitigating strategies.  Enhancing Transparency and Accessibility: Providing more resources such as simplified implementation guides or interactive demonstrations could help a broader audience appreciate the nuances of PGDVAE. In conclusion while PGDVAE marks a significant advancement in the generative modeling of periodic graphs addressing its shortcomings and potential misuse will enhance its contribution to the field.", "qVtbqSwOxy6": "Review of \"Fast MultiView AnchorCorrespondence Clustering (FMVACC): Tackling the AnchorUnaligned Problem for Scalable MultiView Graph Clustering\" 1. Summary: The paper introduces a novel framework named Fast MultiView AnchorCorrespondence Clustering (FMVACC) that addresses scalability issues in multiview graph clustering by reducing complexity through the use of representative anchors. The main innovation lies in solving the AnchorUnaligned Problem (AUP) which arises due to disparities in anchor sets across different views leading to inaccuracies during graph fusion and subsequently degrading clustering performance. The authors achieve this through a flexible anchor graph fusion mechanism that ensures correct correspondences between anchors by utilizing both feature and structure information. Methodologically FMVACC aligns anchor graphs columnwise after establishing crossview consistency thereby significantly enhancing clustering accuracy. Extensive experiments across seven benchmark datasets demonstrate the superiority of FMVACC over existing approaches also showing that the introduced alignment module considerably improves performance on these tasks. 2. Assessment: Strengths:  The paper addresses a significant and practical challenge in multiview graph clustering specifically the scalability issue by innovatively focusing on the oftenneglected aspect of aligning anchor sets across views.  The proposed FMVACC framework is novel and is the first study to formalize the concept of flexible anchor correspondence fusion adding a significant contribution to the domain.  The comprehensive empirical assessment demonstrates that FMVACC not only improves clustering performance significantly on several benchmark datasets but also enhances the performance of existing multiview anchor graph clustering methods through the use of the alignment module.  The theorypractice linkage is strong with the authors providing both theoretical analyses showcasing the connection of FMVACC with existing approaches and practical algorithms for implementation. Weaknesses:  While the paper provides a robust foundation it could benefit from deeper theoretical insights into why the alignment of anchor graphs results in superior clustering performance beyond the empirical evidence.  The scalability of FMVACC particularly on extremely large datasets beyond those tested remains somewhat unclear due to lack of explicit discussion on limitations with respect to computational resources.  The paper underestimates the potential challenges in the practical application of FMVACC such as the sensitivity of clustering performance to the quality of initial anchors and does not provide extensive discussion or solutions for scenarios where highquality anchor selection may not be straightforward. Questions and Suggestions for the Authors:  Could the authors provide more detailed theoretical reasoning or a formal proof to illustrate why ensuring correct anchor correspondences leads to significant improvements in clustering performance  It would be beneficial if the authors could discuss the impact of the nature and dimensionality of the data on the effectiveness of FMVACC offering guidance on scenarios where FMVACC may not perform optimally.  A deeper exploration into the sensitivity of FMVACCs performance to the initial selection of anchors and methods to optimize this selection process could add value to the paper.  Considering the future direction suggested by the authors incorporating deep learning for anchor selection a preliminary discussion on how this might be implemented and its potential challenges would be intriguing. 3. Limitations: The authors have not fully explored the inherent limitations and potential negative societal impacts of their work. While FMVACC shows significant improvements in multiview graph clustering tasks understanding the frameworks performance boundaries particularly in handling extremely large or highly imbalanced datasets is crucial. Additionally a discussion on the ethical implications especially in privacysensitive applications would enrich the paper. Suggestions for improvement include a dedicated section addressing these limitations and offering guidelines for mitigating potential biases or unfair outcomes should FMVACC be employed in sensitive applications. Conclusion: The paper presents a significant advancement in addressing the scalability and alignment challenges in multiview graph clustering. FMVACC stands out for its novel approach to the AnchorUnaligned Problem. However an extended discussion on theoretical underpinnings the impacts of data characteristics and a broader consideration of limitations could further enhance the manuscript.", "syU-XvinTI1": " PeerReview  1) Brief Summary: The paper presents a critical examination of deep learning (DL)based models of brain circuits specifically focusing on models of grid cells in the entorhinalhippocampal circuit. It challenges the claims that DL models by virtue of being trained on specific optimization tasks can reveal the underlying optimization problems the brain solves or predict novel neural tuning patterns. Through extensive experiments including architectural and hyperparameter sweeps the authors argue that the emergence of gridlike units in these models is heavily dependent on specific nonfundamental posthoc implementation choices rather than revealing any fundamental truths about neural circuits or their optimization functions. The paper calls for a more circumspect and transparent approach to building and interpreting DL models in neuroscience emphasizing the need for biological knowledge and inductive biases in model construction.  2) Assessment: Strengths:  Rigorous Empirical Analysis: The paper undertakes a comprehensive and systematic examination of the claims made by DL models regarding grid cells. The largescale architectural and hyperparameter sweeps coupled with theorydriven experimentation provide a robust evidence base for the papers claims.  Critical Perspective: It contributes to the neuroscience community by prompting a critical reevaluation of the current enthusiasm around DL models as tools for understanding brain functions.  Transparency and Reproducibility: By releasing their code the authors embody the principles of open science facilitating further research and verification of their results. Weaknesses:  Limited Scope: The focus is predominantly on grid cells within the entorhinalhippocampal circuit. While this serves to make a pointed critique it might limit the generalizability of the findings across neuroscience.  Potential for Broader Discussion on Methodological Implications: The paper effectively criticizes the use of DL in modeling grid cells but could further explore the methodological implications for the broader field of computational neuroscience.  3) Questions and Suggestions:  Generalizability: Can the authors comment on the extent to which their critiques may apply to other areas of neuroscience where DL models are employed  Biological Plausibility: The paper mentions the need for inductive biases to produce more accurate models. Could the authors elaborate on specific examples of such biases that could be implemented in DL models to enhance their biological relevance  Future Directions: What do the authors envision as the next steps in leveraging DL within neuroscience while accounting for the critiques laid out in the paper Are there particular methodological changes or paradigms that could address these concerns  4) Limitations and Societal Impact: The authors have thoroughly discussed the limitations inherent to DL models of grid cells emphasizing the need for incorporating biological knowledge and inductive biases. However a further discussion of the broader implications of these limitations for neuroscience research would strengthen the paper. While the authors note no direct negative societal impact from their work its worth considering the indirect consequences of overrelying on possibly flawed DL models in neuroscience. Misinterpretations of the brains functioning could potentially mislead future research directions technology development or even clinical applications. Suggestions for Improvement:  Expanding the discussion to include broader implications of the reliance on DL models in neuroscience could provide valuable context.  Including a section on potential corrective measures or methodological approaches that could mitigate the identified issues would be beneficial. The critique and the insights provided in this paper are important for advancing the reliability and validity of DL models in neuroscience fostering a more critical and transparent approach to modeling the brains complexities.", "rg_yN3HpCp": "Review of \"Graph Labelinvariant Augmentation for Enhancing Contrastive Learning on Graph Structured Data\" 1. Summary: This paper introduces a novel approach to graph contrastive learning by proposing a labelinvariant augmentation strategy called Graph Labelinvariant Augmentation (GLA). The key innovation lies in the generation of labelconsistent augmented data in the representation space rather than through traditional graph data augmentation methods like node dropping or edge perturbation. This strategy allows for the production of difficult augmented samples that retain their original labels thereby enhancing model generalization while preserving label integrity. The authors test their method against classical graph neural network methods and recent graph contrastive learning approaches over eight graph benchmark datasets in a semisupervised setting demonstrating superior performance. Additionally the paper explores the effects of various factors such as negative pairs augmentation space and strategy on the GLA method. 2. Assessment: Strengths:  The innovative approach of conducting augmentation in the representation space and ensuring label invariance presents a significant advancement in graph contrastive learning. This method cleverly circumvents the challenge of directly augmenting graph data which often risks altering the graphs inherent structure.  The comprehensive evaluation across eight benchmark datasets under semisupervised conditions showcases the robustness and generalizability of the proposed method offering compelling evidence of its superior performance compared to existing methods.  The methodological exploration into the impacts of negative pairs augmentation space and the strategy on the performance of GLA provides valuable insights for future research and application of graph contrastive learning approaches. Weaknesses:  The paper assumes a level of performance dependency on the quality of the decision boundary generated from the downstream classifier which may limit its effectiveness in scenarios where label information is sparse or unreliable.  While the paper claims no direct societal impacts the broader implications of enhancing machine learning models on graphstructured data\u2014especially in sensitive areas like social network analysis or bioinformatics\u2014warrant a more detailed discussion on potential negative impacts including privacy and ethical considerations. Questions and Suggestions:  Question 1: Could the authors elaborate on how the GLA method might perform in an unsupervised setting or with extremely limited label information  Suggestion 1: Exploring approaches to mitigate the dependency on the quality of the decision boundary possibly through incorporating unsupervised or selfsupervised elements could further strengthen the methods applicability.  Question 2: Given the advancements in generative adversarial networks (GANs) have the authors considered integrating GANs to further enhance the generation of difficult augmented samples in the representation space  Suggestion 2: A deeper discussion on the ethical and privacy implications of graph contrastive learning particularly in sensitive applications would not only fulfill the responsibility of acknowledging potential societal impacts but also guide future research towards more ethical practices. 3. Limitations and Societal Impact: The paper acknowledges that the effectiveness of the GLA method is contingent upon the availability and reliability of label information from downstream tasks which could be a limiting factor in its applicability across different domains. Additionally while the potential negative societal impacts are briefly discussed a more thorough exploration into the ethical considerations especially in sensitive areas of application would enhance the papers comprehensiveness and responsibility towards societal concerns. Suggestions for Improvement:  Investigating GLAs performance in varying scenarios of label availability including unsupervised settings could broaden the methods applicability and address one of the identified limitations.  Expanding the discussion on potential societal impacts and ethical considerations would not only enrich the paper but also contribute to the responsible advancement of machine learning technologies.", "tJBYkwVDv5": "Review of \"Variance Estimation in Dynamic Linear Regression with Finite Sample Complexity Guarantees\" 1) Summary: The paper addresses the challenge of variance estimation in dynamic linear regression where the predictor vector changes over time. This problem is formulated within a linear dynamical system framework with a nonconstant observation operator focusing on estimating the variance of process and observation noise parameters. Unlike prior work that lacks explicit rates or offers only asymptotic guarantees this study achieves a significant breakthrough by providing the first known finite sample complexity bounds for variance estimators. By examining the global system operator and its spectrum the authors introduce explicit geometric parameters related to data that influence estimation error bounds. Their results are valid for arbitrary subGaussian noise distributions. The method Spectrum Thresholding Variance Estimator (STVE) is evaluated on both synthetic and realworld data showcasing its utility across diverse applications. 2) Strengths and Weaknesses:  Strengths: a. The paper tackles a wellacknowledged yet underexplored problem within dynamic linear regression contributing substantial theoretical advancements with finite sample complexity bounds. b. A significant theoretical leap is made by linking spectral properties of the system operator to the convergence rates of variance estimates offering practical insights into the dynamics of estimation errors. c. The methodology is thoroughly evaluated through synthetic experiments and a realworld case study concerning electricity consumption and temperature demonstrating the approachs efficacy and versatility.  Weaknesses: a. While the theoretical foundation is robust the practical implications and limitations for realworld scenarios especially with complex nonlinear dynamics are not deeply explored. b. The paper assumes independence and subGaussian properties of noise which may be restrictive or overly idealistic in certain realworld applications. c. The exposition on dealing with missing data outliers or nonideal conditions is somewhat limited and could benefit from a more detailed discussion. 3) Questions and Suggestions for Authors:  How does the methodology perform under nonideal conditions such as dependent noise terms nonGaussian distributions or the presence of outliers Any insights or preliminary results in these directions would be valuable.  Could the authors clarify the practical impacts of the geometric parameters related to the data spectrum on the estimation process For instance what specific properties or characteristics of the data could potentially lead to poorer performance  A more detailed exploration of the methods applicability to nonlinear dynamical systems could significantly enhance the papers contributions. Are there plans or ongoing work in this direction that could be briefly mentioned or discussed 4) Limitations and Suggestions: The authors have made an earnest attempt to address the limitations of their work by focusing on subGaussian noise distributions and providing a theoretical framework suitable for a broad range of applications. However the potential negative societal impacts especially in sensitive or critical application areas (e.g. energy management healthcare monitoring) where inaccurate variance estimations could lead to significant consequences are not discussed. It would be beneficial for the paper to include a brief section on ethical considerations potential misuse and any mitigative measures to ensure responsible application of the proposed methodology. Additionally expanding the discussion on limitations related to the assumptions of noise properties and independence could provide a more holistic view of the methods applicability and robustness.", "wVc4Qg5Bhah": "Review of \u201cA Universal and Adaptive SecondOrder Algorithm for Convex Optimization\u201d 1. Summary The paper introduces EXTRANEWTON a novel secondorder optimization algorithm designed for minimizing smooth convex functions. EXTRANEWTON stands out for its universal and adaptive characteristics providing a convergence rate of O(\u03c3g\\xe2\\x88\\x9aT  \u03c3HT(32)  1T3) without prior knowledge of smoothness constants variance bounds or the diameter of the constraint set. It functions effectively under both stochastic and deterministic oracle feedback adapting its stepsize dynamically to accommodate variance in gradient and Hessian evaluations. This technique substantially extends existing works in secondorder optimization bridging gaps in universal algorithm designs within convex minimization contexts. 2. Strengths and Weaknesses Strengths:  Universality and Adaptivity: The authors successfully tackle the challenge of designing a secondorder optimization algorithm that does not require prior information on problem parameters. Its adaptive nature allows it to handle variance in oracles effectively adapting the stepsize accordingly.  Theoretical Foundation: The paper lays a solid theoretical foundation by providing rigorous proofs of the global convergence rates under stochastic and deterministic oracles. Such thorough analysis enhances the credibility and applicability of EXTRANEWTON.  Practical Performance: Through extensive experiments EXTRANEWTON is shown to outperform or match contemporary first and secondorder methods across a variety of setups demonstrating its practical utility and efficiency. Weaknesses:  Complexity and Implementation Challenge: While achieving remarkable theoretical implications the algorithm itself appears to be complex which may pose challenges to practical implementations especially in applications requiring realtime decisionmaking or having stringent computational resource limitations.  Limited Exploration of NonConvex Cases: The focus of the paper is strictly on convex optimization problems. Given the current interest in nonconvex optimization within the machine learning community expanding the methodology to nonconvex scenarios could significantly enhance the papers relevance. 3. Questions and Suggestions  On Scalability: How does EXTRANEWTON scale with the dimensionality of the problem and the size of data It would be informative to see scalability tests and a discussion on computational complexity relative to the dimensionality and data size.  Extensions to NonConvex Problems: Can the authors comment on potential adjustments or theoretical barriers to extending EXTRANEWTON\u2019s methodology to nonconvex optimization problems Any insights or preliminary results would enrich the paper.  On Hyperparameter Sensitivity: The paper asserts minimal dependency on prior knowledge of problem parameters yet the performance of optimization algorithms can often be sensitive to choices of hyperparameters such as the initial stepsize. A deeper exploration into the sensitivity of EXTRANEWTON to its initialization and hyperparameters would be valuable. 4. Limitations and Societal Impact While the authors have thoroughly addressed the algorithmic contributions and theoretical implications there is a lack of discussion concerning the limitations and potential negative societal impacts. Optimization algorithms can potentiate biases in decisionmaking processes especially when employed in datadriven applications prone to inheriting biases from historical data. It would be beneficial to see an explicit discussion on:  Ethical Implications: A reflection on how the use of EXTRANEWTON in largescale decision systems might inadvertently perpetuate or amplify biases and suggestions for mitigating such risks.  Environmental Considerations: Given the computational requirements of secondorder methods a discussion on the potential environmental impact due to increased energy consumption in largescale applications would be pertinent. Improvement Suggestions: To address these limitations we propose:  Adding a section discussing the ethical implications and environmental considerations associated with deploying advanced optimization algorithms like EXTRANEWTON.  Investigating the integration of fairnessaware constraints or regularization terms to mitigate potential biases in optimization problems solved by EXTRANEWTON. In summary this paper contributes significantly to the convex optimization field by introducing a versatile theoretically sound and practically efficient secondorder method. Addressing the highlighted questions and suggestions could further enhance its contribution and applicability.", "oNWqs_JRcDD": " Review of \"Efficient Scalable and Generalizable Framework for Optimizer Search in AutoML Systems\"  1) Summary of the Paper and its Contributions The paper presents an innovative framework for automated optimizer search within AutoML systems aimed at addressing the limitations of previous methods in terms of scalability generalization and sample efficiency. Recognizing that optimizer updates are fundamentally mathematical expressions with an innate tree structure the authors propose rearranging the space of optimizers into a supertree. Each path in this supertree encodes an optimizer transforming the search into a pathfinding problem that is amenable to efficient tree traversal methods. The implementation utilizes an adaptation of the Monte Carlo method for tree search enhanced by rejection sampling and equivalentform detection techniques to improve sample efficiency. The framework is evaluated across diverse tasks demonstrating the ability to discover optimizers that outperform humandesigned counterparts and previous methods with a fraction of the evaluations. The code is made publicly available encouraging further research and practical applications.  2) Assessment of the Papers Strengths and Weaknesses Strengths:  Innovativeness: The conceptualization of optimizer search as a pathfinding problem in a supertree of mathematical expressions is novel and creatively leverages the intrinsic structure of optimizer updates.  Efficiency and Scalability: The framework significantly reduces the computational resources required for optimizer search making it practical for a wider variety of tasks and potentially democratizing the application of AutoML.  Generalizability: Demonstrating superior performance across numerous tasks indicates a high level of generalizability addressing a critical limitation of previous methods.  Publicly Available Implementation: Sharing the codebase is commendable fostering transparency and facilitating future enhancements and applications by the community. Weaknesses:  Depth of Evaluation: While the framework is evaluated on a diverse set of tasks the analysis of how it performs under varying conditions (e.g. different data distribution shifts tasks of varying complexity) is not deeply explored.  Comparison with StateoftheArt (SOTA): The paper positions its method against previous optimizer search methods but lacks a thorough comparison with the current SOTA AutoML frameworks in terms of performance and resource efficiency.  Discussion on Failure Cases: Insight into scenarios where the proposed method might underperform or fail is limited which could be valuable for practitioners and researchers aiming to apply or extend the work.  3) Questions and Suggestions for the Authors  Clarification on Generalizability: Could the authors provide more insights or empirical evidence on how the proposed method adapts to tasks with significantly different characteristics than those presented in the evaluations  Scalability Limits: It would be beneficial to discuss the theoretical and practical limits of scalability for the proposed framework. Are there tasks or scenarios where this method becomes impractical and if so what are they  Comparison with SOTA: Further elaboration on how the framework compares with the latest AutoML systems beyond optimizer search in terms of efficiency and performance could strengthen the paper.  Potential Improvements: The paper could benefit from a discussion on potential improvements or extensions to the method. Are there specific areas within the current framework where the authors see opportunities for significant enhancements  4) Limitations and Societal Implications The authors do not fully address the limitations of their work especially in terms of the robustness of discovered optimizers across widely varying tasks and potential computational overhead for extremely largescale applications. Additionally the discussion on potential negative societal impacts is absent which is crucial for a comprehensive understanding of the implications of widespread adoption of the proposed framework. Suggestions for Improvement:  Include a section discussing the limitations in more detail particularly regarding scalability to exceptionally large tasks and robustness across diverse application domains.  Discuss the potential negative societal impacts such as how reliance on automated optimizer search might influence the skill sets required for machine learning practitioners or potentially exacerbate issues of computational resource inequality between different research entities. In conclusion this paper presents a promising approach to automated optimizer search in AutoML systems that addresses key limitations of previous methods. While the framework demonstrates significant strengths including innovation efficiency and generalizability addressing the mentioned suggestions could further enhance its contributions to the field.", "oprTuM8F3dt": "Peer Review 1) Summary of the Paper The paper introduces CoCoINR an innovative model for implicit neural 3D representation that significantly reduces the dependency on a dense set of input views for highquality scene reconstruction and novel view synthesis. Traditional coordinatebased multilayer perceptrons (MLPs) like Neural Radiance Field (NeRF) and its variants typically require between 50150 calibrated images to achieve satisfactory results. CoCoINR mitigates this requirement by incorporating codebook attention and coordinate attention modules which leverage prior information to enrich the feature representation of coordinates thus allowing for realistic rendering from fewer input views. The method demonstrates its efficacy through extensive experiments on various datasets (DTU BlendedMVS H3DS) showing superior robustness and detail preservation in generated 3D views compared to stateoftheart methods. 2) Strengths and Weaknesses Strengths:  Innovation and Relevance: The introduction of CoCoINR addresses a significant limitation in the field of implicit neural 3D representation providing a novel approach that combines codebook and coordinate attention mechanisms.  Experimental Validation: The comprehensive experiments across different datasets along with a robust comparison with existing methods thoroughly validate the model\u2019s capabilities.  Detail Preservation: The ability of CoCoINR to preserve fine details and produce photorealistic results under sparse input views is commendable and highlights the effectiveness of leveraging prior information. Weaknesses:  Scalability: While the method shows superior performance under sparse input conditions the scalability of this approach when applied to even larger and more complex scenes or datasets is not discussed.  Computational Efficiency: The paper does not address the computational requirements or efficiency of CoCoINR which is critical for practical applications and comparisons to existing methods.  Codebook Generalization: The reliance on a pretrained codebook may introduce limitations on the models generalizability across vastly different scenes or domains not wellrepresented in the codebook\u2019s training data. 3) Questions and Suggestions for the Authors  Scalability: Could the authors discuss how CoCoINR scales with very large or complex scenes Are there any practical limitations in terms of memory or computational resources  Computational Efficiency: It would be beneficial to include a comparison of computational requirements and runtimes between CoCoINR and the methods it outperforms.  Codebook Generalization: Can the authors provide insights on how the choice of codebook affects the generalization of CoCoINR across diverse scenes or datasets Additionally exploring adaptive or scenespecific codebooks could be a worthwhile direction.  Potential Biases: Considering the reliance on pretrained models (VQGAN) have the authors considered the potential biases this might introduce especially in terms of texture and color representations 4) Limitations and Societal Impact The paper briefly touches on the limitations regarding the necessity for a dense set of input views by previous methods which CoCoINR aims to overcome. However the discussion on its own limitations such as scalability computational efficiency and potential biases from the use of pretrained codebooks is missing. Addressing these could provide a more comprehensive understanding of CoCoINR\u2019s applicability and areas for future improvement. On the societal impact while the paper presents an advancement in 3D scene reconstruction it does not delve into potential negative implications such as privacy concerns or misuse in creating deepfakes. Acknowledging these concerns and suggesting mitigation strategies (e.g. ethical guidelines detection methods) would strengthen the papers consideration of its broader impacts. Suggestions for improvement:  Expand the discussion on the models scalability and computational efficiency providing concrete data or comparisons where possible.  Explore the impact of different codebooks on the generalizability of CoCoINR and consider the introduction of adaptive codebook strategies.  Address potential negative societal impacts offering guidance or solutions to mitigate these concerns while promoting positive applications of this technology.", "qSYVigfakqS": " Peer Review  1) Brief Summary of the Paper and Its Contributions The paper presents SimFormer a novel framework aimed at addressing the problem of weakshot semantic segmentation. It leverages the concept of dual similarity transfer based on MaskFormer to enable learning dense pixellevel masks for novel classes using imagelevel labels a scenario necessitated by the high annotation costs of expanding semantic segmentation to more classes. MaskFormer\u2019s architecture is utilized to disentangle semantic segmentation into proposal classification and segmentation facilitating the transfer of proposalpixel and pixelpixel similarities from base to novel classes. This approach complemented with a new loss function significantly enhances learning efficiency and accuracy on two major datasets COCOStuff10K and ADE20K. The key contributions can be summarized as:  The proposal of a dual similarity transfer mechanism within the MaskFormer framework for weakshot semantic segmentation  Introduction of pixelpixel similarity transfer and a complementary loss function to improve learning of novel classes  Extensive validation of the method\u2019s practicality and effectiveness through experiments.  2) Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The dual similarity transfer mechanism is a novel approach that creatively addresses the expensive labeling issue in semantic segmentation for novel classes.  Comprehensiveness of Experiments: The authors conducted thorough experiments on two challenging datasets demonstrating the substantial improvements over existing techniques.  Practicality: The proposed method directly tackles the high cost and scalability issue of pixellevel annotations in semantic segmentation offering a relevant solution for realworld applications. Weaknesses:  Generalization Concerns: While the paper presents results on two significant datasets there might be concerns about how this method generalizes to other datasets or under other domainspecific scenarios.  Complexity and Efficiency: The dual similarity transfer mechanism while effective introduces additional complexity to the semantic segmentation task. Theres limited discussion on the computational efficiency or practicality in deployment scenarios.  Limited Focus on Failure Analysis: The paper could benefit from a more detailed analysis of failure cases or limitations of the proposed method offering a deeper understanding of its practical limitations.  3) Questions and Suggestions for the Authors  Generalization to Other Domains: Can the authors provide insights into how SimFormer might perform or be adapted for datasets beyond COCOStuff10K and ADE20K especially in distinct domains  Efficiency Considerations: A deeper discussion on the computational efficiency of SimFormer especially in comparison with MaskFormer would be valuable. Are there optimizations or future directions that could maintain effectiveness while reducing complexity  Extended Analysis of Failure Cases: It would be enlightening to see more comprehensive analysis on why SimFormer might fail in certain scenarios or with certain classes and how these issues could be mitigated.  Limitations and Societal Impact While the paper briefly touches on challenges and failure cases there is room for a more thorough discussion on the limitations particularly regarding the models adaptability and efficiency across varied scenarios. Additional insights into potential biases introduced through transfer learning mechanisms and their implications would also strengthen the paper. Suggestions for Improvement:  Consider expanding the discussion on generalization and efficiency providing clearer pathways for future research to address these challenges.  Explore and elucidate any potential ethical considerations or biases that may arise from the proposed method especially when expanding to novel classes in diverse application domains. Overall the paper introduces a highly relevant and innovative approach to weakshot semantic segmentation demonstrating significant potential in reducing the annotation burden while maintaining high accuracy. Addressing the identified weaknesses and exploring the suggested questions could further solidify the impact and applicability of SimFormer.", "pNEisJqGuei": "Summary of the Paper The paper presents a novel approach to improving the design process of reinforcement learning (RL) agents by integrating value decomposition into actorcritic algorithms. Value decomposition breaks down the composite reward function into individual components facilitating insights into the agents decisionmaking and learning processes. The authors introduce SACD a soft actorcritic adaptation that employs value decomposition to maintain performance while providing additional diagnostic capabilities. They also introduce a reward influence metric to evaluate the impact of individual reward components on agent decisions. Demonstrations showcase how the proposed method can identify and address common design and learning problems making the approach a valuable tool for RL practitioners. Strengths and Weaknesses Strengths: 1. Innovative Integration: The paper pioneers in integrating value decomposition into actorcritic RL methods. This approach addresses the opaque nature of RL learning processes by providing more granularity in the reward functions role in agent learning and decisionmaking. 2. Diagnostic Capabilities: The introduction of decompositionbased diagnostic tools especially the reward influence metric presents a significant advancement in understanding and troubleshooting RL agents learning challenges. 3. Practical Demonstrations: The application examples effectively illustrate the methods utility in diagnosing and mitigating common issues reinforcing its practical significance. Weaknesses: 1. Implementation Complexity: While the paper demonstrates the feasibility and benefits of the SACD implementation the added complexity in managing multiple value functions and ensuring their proper integration could be a barrier for broader adoption. 2. Limited Exploration of NonLinear Decompositions: The focus on linear reward decomposition limits understanding the methods applicability or adjustments needed for nonlinear scenarios. 3. Comparison Depth: Although performance comparisons are made with the SAC algorithm a broader analysis against other value decomposition or diagnostic approaches could further validate the methods efficacy. Questions and Suggestions for the Authors 1. NonLinear Decompositions: Could the authors discuss potential adaptations of their method for nonlinear reward decompositions and the expected challenges or performance implications 2. Scalability Concerns: How does the value decomposition approach scale with the number and complexity of reward components especially in more complex environments or tasks 3. Generalization Across Domains: Can the authors provide insights or empirical evidence on how well the presented method generalizes across different application domains beyond the demonstrated examples 4. LongTerm Influence Metric Evolution: Have the authors considered studying the longterm evolution of the reward influence metric during prolonged training sessions and could such insights offer further improvements in agent design Addressing Limitations and Societal Impact The authors acknowledge the limitations related to the reliance on composite reward functions and the focus on linear reward decomposition. Expanding the methodology to accommodate nonlinear reward functions and providing more nuanced guidance for handling complex or numerous reward components could enhance the methods applicability. Additionally reflecting on the broader societal implications of their work especially in terms of transparency and accountability in automated decisionmaking systems could strengthen the papers impact. The acknowledgment that value estimates represent beliefs rather than truths is crucial encouraging further research into mitigating potential biases in these estimates can contribute positively to the responsible development and deployment of RL technologies.", "tbdk6XLYmZj": " Peer Review of \"Learning the Best Combination for Efficient N:M Sparsity\" (LBC)  Summary of the Paper The paper presents a novel approach for implementing N:M sparsity in neural networks aimed at improving model efficiency without the heavy computational burden typically associated with similar sparsity methods. By conceptualizing N:M sparsity as a combinatorial problem the authors propose an efficient divideandconquer strategy dubbed \"Learning Best Combination\" (LBC) which selects the most optimal combination of weights from a predefined set thereby achieving finegrained sparsity during the normal training phase. This is different from prior methods that require expensive pretraining phases or densegradient computations. The crux of LBC involves dividing the weight vector into combination subsets and then applying a learnable score to each combination optimizing it alongside the weights. Extensive experiments demonstrate that LBC not only outperforms existing N:M sparsity methods across different networks but also illustrates its applicability and benefits in computer vision tasks without the drawbacks of previous approaches.  Strengths 1. Innovative Methodology: The paper introduces a unique perspective on achieving N:M sparsity by treating it as a combinatorial problem. The divideandconquer approach coupled with a learnable scoring system for weight combinations is a significant innovation. 2. Comprehensive Empirical Validation: The authors have conducted a wide range of experiments across multiple networks and tasks showing the universal applicability and effectiveness of LBC in achieving high performance and efficiency. 3. OpenSource Code: Providing access to the projects source code is commendable as it enhances reproducibility and allows for community improvement.  Weaknesses 1. Limited Exploration Outside Vision Tasks: While the paper provides substantial evidence of LBCs effectiveness in computer vision tasks its performance in other domains such as NLP or audio processing is unexplored. 2. EarlyStage Efficiency Concerns: The paper mentions that LBC still requires relatively dense computation in the initial stages of training. This could be a potential efficiency bottleneck that needs addressing.  Questions and Suggestions for the Authors 1. Expansion to Other Domains: Have the authors considered testing LBCs effectiveness on tasks outside of computer vision such as NLP to truly gauge its versatility 2. Improving Initial Efficiencies: Could the authors propose a modified approach or additional techniques that could minimize the dense computation required in the early stages of LBCs training process 3. Comparison with Recent Methods: Given the rapid development in sparsity and compression techniques how does LBC compare with the very latest methods not covered in the paper  Limitations and Societal Impact The authors have briefly mentioned the limitations regarding the dense computation requirement in the early stages of training and the focus on computer vision tasks exclusively. Further elaboration on potential solutions or ongoing work to address these limitations would be beneficial. Additionally the potential negative societal impact of deploying highly efficient sparsityoptimized models is not discussed. While efficiency generally has positive implications it could also lead to increased deployment of large models possibly exacerbating issues like energy consumption and electronic waste. Suggestions for improvement include a more detailed exploration of LBCs applicability across a broader range of tasks and an indepth discussion on the ethical implications and environmental impact of deploying optimized models at scale.", "zK6PjBczve": " 1) Brief Summary and Contributions The paper introduces NeurHap a novel method that integrates graph representation learning with combinatorial optimization to tackle the haplotype assembly problem for both polyploid species and viral quasispecies. This problem is challenging due to the need to infer multiple haplotypes from highthroughput noisy sequencing data which is further complicated by polyploidy and the dynamic nature of viral quasispecies. Traditional neural networkbased methods have struggled with this task due to their insufficient modeling of the complex relations among sequencing reads. NeurHap is distinctive in formulating the haplotype assembly problem as a graph coloring issue where sequencing reads are vertices in a graph and edges represent pairwise relationships (consistent or conflicting) based on single nucleotide polymorphisms (SNPs). The method employs a messagepassing neural network to optimize vertex color assignments indicative of haplotype groupings followed by a local refinement strategy to further enhance solution accuracy as measured by the minimum error correction (MEC) score. Key contributions of the paper include:  A firstofitskind formulation of the haplotype phasing problem as a graph coloring challenge  The development of an integrated graph neural network search framework (NeurHapsearch) combined with a local optimization strategy (NeurHaprefine) for haplotype assembly  Demonstrated superiority of NeurHap over existing stateoftheart methods on both synthetic and realworld datasets concerning polyploid species and viral quasispecies based on improved MEC scores.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty of Approach: The paper proposes a unique and innovative method to address the haplotype assembly problem effectively. Its graphbased formulation and combination of global search with local refinement are particularly noteworthy.  Improved Performance: The empirical results presented convincingly with comprehensive experiments on both simulated and real datasets show that NeurHap outperforms existing methods significantly indicating its practical applicability to genomics and virology.  Detailed Methodology: The authors provide a thorough explanation of their approach including the algorithms theoretical basis the rationale behind design choices and a clear description of the optimization process. Weaknesses:  Scalability Concerns: While the method shows improved performance the paper lacks a detailed discussion on scalability especially concerning very large datasets or extremely high ploidy levels which are pertinent in some contexts.  Limitations on Read Lengths: The authors mention that NeurHap struggles with long reads due to the increased search space. This limitation is significant given the ongoing shift towards longread sequencing technologies in genomics.  Automatic Haplotype Number Detection: The current inability of NeurHap to automatically determine the number of haplotypes is a notable drawback requiring prior knowledge or additional analysis to set this parameter.  3) Questions and Suggestions for Authors  Future Work on Scalability: Can the authors elaborate on potential extensions or modifications to NeurHap that could improve its scalability and efficiency especially for larger genomes or higher ploidy levels  Long Reads Handling: Could further integration with or development of preprocessing methods that segment long reads into more manageable fragments without losing critical SNP information be a viable direction for addressing the limitations with long reads  Determining Haplotype Number: Have the authors considered incorporating or developing algorithms within NeurHap that could estimate the number of haplotypes directly from the data thereby automating this aspect of the haplotype assembly process  4) Addressing Limitations and Societal Impact The authors briefly touch upon limitations concerning the handling of long reads and the manual determination of haplotype numbers. These limitations could restrict the broader application of NeurHap in current genomic research workflows especially with the increasing prevalence of longread sequencing technologies and complex polyploid genomes. A discussion of potential negative societal impacts such as misuse of genomic data or dualuse concerns in virology is absent but would be beneficial. Suggestions for Improvement:  Exploration of Scalability Enhancements: Further research into algorithm optimizations or parallel processing techniques could help in scaling NeurHap to handle larger datasets more efficiently.  Integration with Preprocessing Strategies: Developing or recommending specific preprocessing strategies for long reads could make NeurHap more versatile and applicable to a broader range of sequencing data.  Algorithmic Estimation of Haplotype Numbers: Incorporating methods for automatically determining the number of haplotypes within NeurHap could significantly enhance its userfriendliness and applicability.  Societal Impact Considerations: Although the focus is on technical and scientific contributions future work should also consider discussing the ethical and societal dimensions of haplotype analysis technology including privacy data security and ethical use guidelines.", "mxzIrQIOGIK": " Review of \"A Systematic Study of MultiObjective Online Learning\"  Summary This paper introduces a comprehensive examination of multiobjective online learning proposing a novel framework termed MultiObjective Online Convex Optimization (MOOCO). Within this framework the authors introduce two new forms of multiobjective regret catered specifically for the dynamic nature of online learning environments. Notably a significant contribution of the paper is the development of the Doubly Regularized Online Mirror Multiple Descent (DROMMD) algorithm. The design of this algorithm addresses a fundamental issue identified in employing standard minnorm methods for gradient composition in a multiobjective online setting which could lead to linear regrets. Through rigorous theoretical analysis the authors derive static and dynamic regret bounds for DROMMD proving its efficacy. The practical relevance and efficiency of DROMMD are further validated through extensive experiments on both simulated and realworld datasets.  Strengths 1. Innovative Framework and Algorithm: The proposal of MOOCO and DROMMD fills a critical gap in the study of multiobjective online learning. The delineation of unique regret metrics and the novel minregularizednorm solver presents a significant advancement in methodology. 2. Rigorous Theoretical Analysis: The derivation of both static and dynamic regret bounds for DROMMD not only substantiates the theoretical contribution of this work but also aligns with optimal bounds in singleobjective settings which is commendable. 3. Empirical Validation: The comprehensive experiments conducted offer solid evidence for the superiority of DROMMD over existing methods. The use of various datasets including realworld applications adds to the robustness of the empirical evaluation.  Weaknesses 1. Complexity and Practical Implementation Issues: While the paper provides an extensive theoretical foundation and empirical evidence of effectiveness it falls short in discussing the practical complexities involved in the implementation and operation of DROMMD especially in highly nonstatic environments. 2. Limitation and Societal Impact: The discussion on limitations is somewhat vague particularly regarding the nonconvex settings. Furthermore the paper does not engage with the broader societal or ethical implications associated with the deployment of multiobjective online learning algorithms which could be significant depending on the application domain.  Questions and Suggestions for Authors 1. Algorithm Complexity: Can the authors provide more insight into the computational complexity of DROMMD especially in comparison with traditional minnorm solvers and its practical feasibility for largescale problems 2. Extensibility to NonConvex Problems: Given the promising results in convex settings how does DROMMD perform in nonconvex scenarios Are there modifications or considerations required to maintain its efficacy 3. Societal Impact Consideration: Could the authors elaborate on potential negative societal impacts particularly concerning fairness and bias in decisionmaking processes when deploying such algorithms in multiobjective online learning applications  Limitations and Suggestions for Improvement The authors should consider extending the discussion on the applicability and potential adjustments necessary for DROMMD in nonconvex problems. Additionally it would be beneficial to include a more thorough examination of the algorithms computational complexity and practical deployment challenges. Moreover integrating an explicit analysis of potential ethical and societal impacts alongside suggested mitigation strategies would vastly enrich the paper. This could involve considerations around fairness privacy and unintended consequences of multiobjective optimization decisions in critical domains such as healthcare finance or autonomous driving.  Conclusion This paper makes a significant contribution to the field of multiobjective online learning through the introduction of a novel framework algorithm and corresponding theoretical analysis. Despite its strengths there are areas particularly concerning practical complexity and broader societal impacts where additional discussion could provide a more comprehensive understanding. Nevertheless this work undoubtedly advances the stateoftheart and sets a foundation for future research endeavors in multiobjective online learning.", "s_mEE4xOU-m": " Peer Review  1) Brief Summary of the Paper This paper addresses the challenge of network load balancing in data centers with multiple load balancers (LBs) by adopting a multiagent reinforcement learning (MARL) approach. The core difficulties tackled include heterogeneous processing architectures dynamic environments and limited observability by individual LB agents. The authors formulate the load balancing problem as a Markov potential game and introduce a distributed MARL algorithm aimed at approximating the Nash equilibrium with workload distribution fairness serving as the potential function. Through eventdriven simulations and realworld system experiments the proposed MARL algorithm demonstrates superior performance when compared to existing inproduction load balancers showcasing both closetooptimal behavior in simulations and improved realworld outcomes.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper presents a novel approach using MARL for network load balancing addressing the challenge with a wellmotivated game theoretical formulation.  Practical Relevance: Given the importance of efficient load balancing in cloud data centers the proposed solution has significant practical relevance. The demonstrated improvement over inproduction LBs underscores this point.  Experimental Evaluation: The thorough evaluation including both simulated environments and a realworld DC network provides a solid foundation for the claims of efficiency and effectiveness.  Opensourced Resources: By providing access to both the source code and the data the paper facilitates reproducibility and future research in the area. Weaknesses:  Complexity and Scalability: While the paper touches on scalability there is limited insight into the algorithms complexity and its operational scalability across very largescale data centers.  Limited Discussion on Limitations or Failures: The paper would benefit from a more thorough discussion on scenarios where the algorithm might not perform as expected or could potentially fail.  Partial Observability Tackled Silently: Despite partial observability being a substantial challenge the paper does not deeply delve into how this limitation impacts the overall system performance or the specific strategies used to mitigate its effects.  3) Questions and Suggestions for the Authors  Scalability Details: Can the authors provide more details on the computational complexity of the algorithm and its scalability especially in the context of data centers with a very large number of servers and LBs  Algorithm Failure Scenarios: It would be beneficial to discuss potential failure scenarios or limitations of the proposed algorithm. In what situations might it perform poorly and are there any fallback strategies  Comparison with Other MARL Approaches: Given the richness of MARL research a more detailed comparison with other recent MARL approaches could highlight the unique advantages or limitations of the proposed method.  Impact of Partial Observability: How does the partial observability of the system affect decisionmaking and what specific techniques or strategies are employed to mitigate its negative effects  Limitations and Suggestions for Improvement The authors have touched upon the limitations related to the distributed networking systems design preferences inviting future work on reducing communication and management overhead among agents. However a more detailed discussion on the algorithm\u2019s limitations especially in handling extreme dynamics in traffic and server capabilities could enhance the papers depth. Furthermore the societal impact particularly concerning energy consumption and the potential for reinforcing existing biases through misconfigurations should be addressed. Suggested improvements include:  Exploring Hybrid Approaches: Combining the proposed MARL with other decisionmaking paradigms to handle edge cases or failure scenarios efficiently.  Enhanced Analysis on Societal Impact: A conscientious analysis regarding the potential for negative societal impacts including environmental considerations and data bias would provide a more comprehensive view.  Broader Contextualisation Within MARL: Situating the proposed solution within the broader scope of MARL research identifying gaps in the current approach and suggesting pathways for future research. In summary while the paper presents a compelling approach to network load balancing using MARL bolstering the discussion on scalability more nuanced treatment of its limitations and a broader contextualization within the field could amplify its impact.", "s71h4wo9bFI": "Review of \"Design of Optimal Bayesian Data Acquisition Mechanisms for PrivacySensitive Users\" 1. Summary and Contributions The paper addresses the intricate challenge of designing data acquisition mechanisms for platforms aiming to estimate parameters (e.g. mean of a distribution) from data shared by privacyconscious users. Users exhibit heterogeneous sensitivity towards two types of privacy losses: local (privacy loss from sharing data with the platform) and central (privacy loss from the platforms public estimates). The platforms objective is to design a mechanism that elicits true user preferences for privacy guarantees (local and central) compensates users for their privacy losses and optimally balances the mean estimation error with the total expected payment to users. The authors propose a Bayesian approach where the platform knows the distribution of user privacy sensitivities but not the exact values. Key contributions include:  Establishing minimax lower bounds for estimation error given privacy guarantees and proving that a linear estimator with adjusted Gaussian noise additions provides nearoptimal performance.  Modeling the optimal data acquisition mechanism arrangement as a nonconvex optimization problem with a focus on linear estimators.  Demonstrating that finding the optimal mechanism for such estimators is computationally feasible via a Polynomial Time Approximation Scheme (PTAS). 2. Assessment of Strengths and Weaknesses Strengths:  Originality and Relevance: The paper addresses a highly relevant and original problem intersecting privacy economic incentives and statistical estimation contributing significantly to the literature on data markets and privacypreserving data analysis.  Theoretical Contributions: The establishment of minimax lower bounds the modeling of the mechanism design as a nonconvex optimization problem and the PTAS for finding the optimal mechanism are substantial theoretical advancements.  Policy Implications: The framework and findings could inform the design of realworld data markets and platforms balancing user privacy concerns with data utility. Weaknesses:  Model Assumptions: The paper assumes knowledge of the distribution of users privacy sensitivities and that user data can be represented as privacy losses adequately managed through differential privacy mechanisms. These assumptions may not hold in all realworld scenarios potentially limiting the applicability of the findings.  Potential for Extension: While the focus on linear estimators is justified the paper could explore or discuss extensions to other types of estimators or multidimensional parameters which might be relevant for some applications.  Empirical Validation: The theoretical findings while strong are not complemented with empirical validation. Including case studies or simulations could strengthen the paper by demonstrating the practical effectiveness of the proposed mechanisms. 3. Questions and Suggestions for the Authors  Empirical Validation: Can the authors provide realworld data or simulated scenarios to validate the effectiveness and practicality of the proposed data acquisition mechanisms  Extensions to Nonlinear Estimators: Could the authors discuss how their proposed mechanism might be adapted for or perform with nonlinear estimators or estimation of vector parameters  Impact of Model Assumptions: How sensitive are the proposed mechanisms to deviations from the model assumptions such as inaccuracies in the distribution of privacy sensitivities or noncompliance of users with the mechanism 4. Limitations and Societal Impact The paper somewhat overlooks the discussion on the limitations and potential negative societal impacts of its proposed mechanisms. For instance the effectiveness of the mechanism heavily relies on accurate prior knowledge of the distribution of users privacy sensitivities which might not always be feasible or ethical to obtain. Moreover there is limited discussion on the potential for misuse of such mechanisms or the consequences of their failure. Suggestions for Improvement:  The authors should more thoroughly discuss the limitations of their assumptions particularly regarding the distribution of privacy sensitivities and the models applicability across different data types and user populations.  A section addressing the potential negative societal impacts including privacy risks or inequities that could arise from the implementation of the proposed mechanisms would be a valuable addition.  Including recommendations for safeguarding against misuse and addressing potential failures of the mechanism would enhance the papers practical and ethical considerations.", "ytnwPTrpl38": " Peer Review  Summary of the Paper This paper introduces GeneRAlizing by DiscovERing (GRADER) a novel framework that enhances the generalization capabilities of GoalConditioned Reinforcement Learning (GCRL) by integrating causal reasoning through causal graphs as latent variables. The authors argue that conventional RL algorithms often overlook reasoning leading to data inefficiency and performance degradation. To mitigate this GRADER formulates the GCRL problem into a variational likelihood maximization task using causal graphs as a means to iteratively discover and represent causeandeffect relationships thereby improving transition modeling policy training and ultimately the generalization of the RL agents across various tasks. Through rigorous theoretical analyses and comprehensive experiments across nine designed tasks in three environments the paper demonstrates GRADERs superior performance relative to five baselines underlining the frameworks data efficiency interpretability and enhanced generalization capabilities.  Strengths and Weaknesses Strengths: 1. Originality: The integration of causal reasoning into GCRL through a latent variable model is a significant and novel contribution. This approach addresses a critical gap in enhancing RL generalization capabilities. 2. Theoretical Foundation: The paper provides a detailed theoretical background to support the GRADER framework including proofs of unique identifiability of true causal graphs and the theoretical performance guarantees of the optimization framework which adds depth and credibility to the proposed methodology. 3. Experimental Validation: Extensive experiments and a new set of tasks specifically designed to test reasoning capability underscore GRADERs effectiveness. This is further bolstered by comprehensive comparisons with several baselines showcasing GRADERs superiority in various generalization settings. Weaknesses: 1. Scalability Concerns: While the paper discusses the effectiveness of the proposed method there is a potential concern regarding its scalability particularly in environments with a large number of nodes or complex causal structures. 2. Assumptions Limitations: The assumptions of a factorized state and action space may restrict the applicability of GRADER to cases where semantic representation is readily accessible or can be abstracted without significant information loss. 3. Generalization to RealWorld Applications: Although the paper provides a strong empirical foundation within designed environments it lacks a clear demonstration or discussion on the application and effectiveness of GRADER in realworld scenarios or domains outside the tested environments.  Questions and Suggestions for the Authors 1. Scalability: Could the authors elaborate on potential strategies to enhance the scalability of GRADER particularly in handling environments with more complex causal structures or a greater number of nodes 2. RealWorld Application: It would be valuable if the authors could discuss or provide insights into how the GRADER framework could be adapted or applied to realworld problems particularly those involving complex decisionmaking processes. 3. Assumption Relaxation: Are there ongoing efforts or future plans to relax some of the methods assumptions such as the factorized state and action space to broaden the applicability of GRADER  Limitations and Suggestions for Improvement The authors have made an effort to address the limitations of their work by discussing scalability and the assumption of factorized state and action spaces. However they could further expand on potential strategies to overcome these limitations or explore alternative approaches that could allow for the application of GRADER in more complex or realistic environments. Moreover a direct comparison or analysis of GRADERs performance in realworld tasks if feasible within the scope of this research could significantly enhance the papers impact. Potential Negative Societal Impact: The paper briefly touches on negative societal impacts in the appendix but a more thorough contemplation of these aspects especially considering misuse privacy and ethical ramifications of applying GRADER in sensitive or critical domains could be beneficial. Constructive suggestions for mitigating such risks would also be appreciated.", "nyBJcnhjAoy": "Summary of the Paper and Its Contributions The paper introduces ProtoX a novel prototypebased policy explainer designed to elucidate the decisions made by blackbox deep reinforcement learning (DRL) agents. ProtoX distinguishes itself through its ability to map an agents actions to a set of prototypical scenarios that represent common patterns of behavior improving the interpretability of the agents decisionmaking process. The approach leverages selfsupervised learning for pretraining an encoder to recognize visual similarity and introduces an isometry layer for the adaptation of scenario similarities. The key contributions of the paper are threefold: Firstly it pioneers the use of prototypebased explanations for DRL agents without requiring access to the agents internal workings or interaction with the environment. Secondly it proposes a novel methodology combining selfsupervised learning with an isometry layer to grasp both visual and scenario similarities. Lastly compared to existing methods ProtoX shows superior explanation fidelity and applicability to unstructured data setting a new standard for posthoc explanation approaches in reinforcement learning. Strengths and Weaknesses Strengths: 1. Innovative Approach: The prototypebased method for explaining blackbox agents marks a significant advancement in interpretability research within DRL. 2. Broad Applicability: ProtoX can be applied directly to unstructured data a common hurdle in DRL maximizing its utility across various applications. 3. High Explanation Fidelity: The methodology demonstrates high fidelity in its explanations closely mirroring the behaviors of the original agents it seeks to explain. 4. Selfsupervised Learning for Visual Similarity: The use of selfsupervised learning to pretrain an encoder on visual similarity is both novel and effective. Weaknesses: 1. Complex Interpretation for Nonexperts: While the paper aims at interpretability the interpretation of prototypes might still require a baseline understanding of the task potentially limiting accessibility to nonexpert users. 2. Overlapping Prototypes: The occurrence of similar or redundant prototypes suggests a need for more efficient pruning or merging strategies to enhance model simplicity and interpretability. Questions and Suggestions for Authors 1. Could you elaborate on how ProtoX might be adapted or extended to continuous action spaces common in many realworld applications 2. The paper mentions the potential for redundant prototypes. Would incorporating a more dynamic prototype merging or pruning technique during training help reduce redundancy and improve interpretability 3. Have the authors considered the impact of adversarial manipulations or outliers in the training data on the explanation fidelity of ProtoX 4. A suggestion for improvement involves exploring ways to make the prototype interpretation more accessible to nonexpert users potentially through visualization tools or simplifying the logic behind prototype significance. Limitations and Societal Impacts The authors have acknowledged that understanding ProtoXs prototypes requires a minimum understanding of the task at hand which could limit its applicability to a broader audience. Additionally while the paper briefly discusses the redundancy issue in prototypes more detailed strategies for addressing this limitation would strengthen the paper. There is also a need for further discussion on the potential for malicious use cases where explanations could be exploited to game or deceive users about an agent\u2019s true nature. Constructive suggestions include incorporating a more robust discussion on strategies for enhancing the accessibility of explanations to nonexpert audiences and exploring the incorporation of robustness checks against adversarial examples in the training data. Addressing these concerns would not only strengthen the paper but also contribute to a more ethical and socially responsible deployment of DRL agents.", "nP6e73uxd1": "Review of \"Sampling from LogConcave Distributions with InfinityDistance Bounds for Differentially Private Optimization\" 1. Summary: The presented paper tackles the challenge of efficiently sampling from logconcave distributions particularly focusing on generating samples that are \\epsilonclose in infinitydistance to a target distribution fi. This is crucial for applications in differentially private optimization where existing methods for achieving such sampling within the total variation or weaker metrics like Wasserstein or KL divergence are insufficient for ensuring pure differential privacy. Prior algorithms for sampling in \\epsilon infinitydistance suffered from runtime bounds that scale polynomially with \\epsilon. The authors present a novel algorithm that remarkably reduces the dependency on \\epsilon to logarithmic while achieving \\epsilon infinitydistance accuracy. This is accomplished by directly converting continuous samples from a convex body with totalvariation bounds to samples with infinity bounds bypassing the need for gridbased Markov chains. The authors further demonstrate how their technique offers runtime improvements in scenarios where the convex body is a polytope by leveraging the Dikin Walk Markov chain. The paper also presents immediate applications of this method to differentially private empirical risk minimization and lowrank approximation showing significant improvements over existing methods. 2. Strengths and Weaknesses: Strengths:  The paper addresses a significant gap in the literature by proposing an efficient sampling algorithm for obtaining samples \\epsilonclose in infinitydistance to a target logconcave distribution which is pivotal for applications demanding pure differential privacy.  The methods logarithmic dependence on \\epsilon marks a considerable advance over prior polylogarithmic approaches potentially leading to significant speedups in practical applications.  The theoretical contributions particularly the demonstration of how totalvariation bounds can be converted to infinitydistance bounds within the algorithms design are both novel and of high significance to the field.  The paper is wellstructured providing a comprehensive background clear definition of the problem detailed descriptions of the algorithm and its theoretical underpinnings followed by practical implications and potential applications. Weaknesses:  While the paper outlines the improvements in runtime and discusses potential applications the experimental evaluation of the proposed algorithm\u2019s practical performance especially in comparison with existing methods is missing. This would be crucial for understanding its realworld implications.  The discussion on limitations largely focuses on the theoretical aspect of extending the current bounds when f is not Lipschitz. A more detailed examination of limitations in practical scenarios especially regarding the algorithms performance on highdimensional data or under different configurations of the convex body K would be beneficial.  The societal impact section predominantly concentrates on the positive attributes of enhancing differential privacy. However it would be advantageous to also consider scenarios where the misuse of such efficient sampling mechanisms might lead to unwarranted privacy invasions or other ethical concerns. 3. Questions and Suggestions for the Authors:  Could you provide more insights into the practical performance of your proposed algorithm possibly including comparisons with existing methods under various settings and applications  The paper could benefit from a deeper exploration of the limitations particularly in terms of the practical scalability of the algorithm. Are there specific scenarios or configurations of the input where the algorithm might not perform as efficiently  It would be interesting to see further discussion on the broader implications of your findings for the field of differential privacy perhaps including potential future directions or applications that could be unlocked by your contributions. 4. Limitations: The authors have made a commendable effort in addressing the limitations of their work notably the polynomial dependence on LR in the absence of a Lipschitz constraint on f. However the discussion could be enriched by considering more practical limitations such as potential challenges in highdimensional settings or variations in the convex bodys configurations. The societal impact section while positive could be expanded to reflect a broader range of implications including possible negative outcomes in scenarios of misuse. Constructive suggestions would include conducting experimental evaluations to showcase the algorithms practical efficiency and exploring mitigation strategies for any identified limitations.", "wt7cd9m2cz2": " Peer Review for \"Learning SingleIndex Models with Shallow Neural Networks via Gradient Flow\"  Abstract Summary The paper addresses the problem of learning singleindex models using shallow neural networks. It introduces a specific class of shallow networks where biases are frozen and analyzes their capacity to learn through gradient flow. The main innovation lies in demonstrating that this architecture allows for recovering the hidden subspace with an approximation quality and sample complexity that are competitive with dedicated semiparametric methods traditionally used for this type of task. The analysis is supported by both theoretical guarantees and experimental evidence.  Strengths 1. Novelty: The approach of using a shallow neural network with biases fixed at initialization represents a significant departure from traditional ways of learning singleindex models. It provides a fresh perspective on adapting neural networks for highdimensional data exhibiting lowdimensional structures. 2. Theoretical Insight: The paper successfully translates the nonconvex optimization challenge of training such neural networks into a theoretically tractable problem. The results concerning the optimization landscape and the nearoptimal sample complexity are particularly noteworthy offering deeper insights into why and how shallow networks can efficiently learn singleindex models. 3. Practical Implications: By showing that shallow neural networks can match the sample complexity of more specialized semiparametric methods the work opens up new avenues for employing neural networks in scenarios where interpretability and computational efficiency are crucial such as in highdimensional data analysis.  Weaknesses 1. Assumption Rigidity: The analysis is based on several restrictive assumptions such as the random initialization of biases that remain fixed and the tied nature of the firstlayer weights. While these conditions are critical for the theoretical analysis they may limit the practical applicability of the results. 2. Model Generality: The focus on singleindex models although valuable leaves the applicability of the proposed approach to multiindex or more complex models an open question. Extending the analysis to cover a broader class of models would significantly enhance the impact of the work. 3. Experimental Validation: While the paper provides some experimental results a more thorough empirical evaluation covering a wider range of scenarios and datasets would be beneficial to understand the practical effectiveness and limitations of the proposed approach.  Questions and Suggestions 1. Relaxation of Assumptions: Can the authors comment on the possibility and implications of relaxing some of the assumptions notably the fixed biases and tied weights How sensitive are the theoretical results to these assumptions 2. Extension to Multiindex Models: It would be interesting to see how the proposed architecture and theoretical analysis could be adapted to multiindex models. Are there inherent limitations in the approach that would make such an extension nontrivial 3. Empirical Evaluation: The experimental section could be strengthened by including comparisons with traditional semiparametric methods explicitly. Such comparisons would highlight the practical advantages or shortcomings of the proposed neural network approach in realworld scenarios.  Limitations and Societal Impacts The authors have provided a solid foundation for the strengths of their proposed method however a more detailed discussion on the limitations and potential improvements would strengthen the paper. Specifically addressing how the method could be adapted to more complex models and whether there are practical limitations in standard training environments would be beneficial. On the societal impact the paper does not delve into potential negative implications. Given the focus on highdimensional data an exploration into the methods robustness against noisy or adversarial data inputs and the implications of misinterpretation could be insightful. Constructive suggestions for future work could include exploring these robustness aspects and extending the analysis and methodology to a broader class of models to enhance practical utility.", "xpdaDM_B4D": "Review of \"Fewshot Learning with Hard Mixup (FeLMi)\" 1) Summary of the Paper: This paper introduces a novel fewshot learning (FSL) strategy known as Fewshot Learning with hard Mixup (FeLMi) specifically designed to address the data scarcity problem in FSL tasks. It bridges the gap in traditional FSL efforts by integrating a mixupbased data augmentation strategy that involves generating synthetic samples through a technique the authors refer to as hard mixup. This approach is distinct because it chooses mixup samples based on a marginbased uncertainty measure aiming to select hard mixup examples that potentially provide a stronger learning signal. The authors validate FeLMi across several FSL benchmarks (FC100 CIFARFS miniImageNet tieredImageNet) and a crossdomain FSL benchmark (miniImageNet \u2192 CUB) demonstrating its efficacy by outperforming existing stateoftheart methods in both 1shot and 5shot learning scenarios. 2) Assessment of Strengths and Weaknesses: Strengths:  The paper addresses a critical issue in FSL \u2013 data scarcity \u2013 using an innovative mixup strategy that significantly enhances learning from limited data.  The introduction of hard mixup based on uncertainty measures is a novel contribution offering a fresh perspective on generating and utilizing synthetic samples for effective learning.  Extensive experiments across multiple standard and crossdomain benchmarks along with comparative analysis robustly validate the proposed approach.  The methodology is clearly articulated and the code has been made publicly available facilitating reproducibility and further research in this direction. Weaknesses:  The paper lacks a deeper theoretical grounding on why the hard mixup strategy specifically improves performance in FSL tasks beyond empirical evidence.  While the authors mention the significant improvements achieved there is limited discussion on the computational cost and efficiency implications of implementing FeLMi especially in terms of the mixup sample generation and selection process.  The evaluations primarily focus on image classification tasks leaving the applicability of FeLMi in other domains of fewshot learning such as natural language processing or speech recognition unexplored. 3) Questions and Suggestions for the Authors:  Could the authors provide more theoretical insights or intuition behind the effectiveness of the hard mixup strategy particularly in the context of FSL  An evaluation of the computational overhead introduced by the hard mixup sample generation and selection process would be highly beneficial. Can the authors comment on the efficiency of FeLMi in practical scenarios  It would be interesting to see how FeLMi performs in fewshot learning tasks outside of image classification. Do the authors plan to extend their evaluations to other domains  A discussion on the sensitivity of FeLMi to the choice of hyperparameters especially those related to mixup (e.g. lambda values) would offer deeper insights into the robustness of the proposed method. 4) Limitations and Constructive Suggestions:  The authors have not thoroughly discussed the scalability of applying FeLMi to very large datasets due to the requirement of accessing base examples during metatesting. A direction for future work could include exploring strategies to minimize or eliminate the reliance on base examples during the adaptation phase for novel classes.  FeLMis potential negative societal impacts particularly in contexts where generating synthetic data could lead to biases or ethical concerns (e.g. synthetic data reinforcing existing stereotypes) are not discussed. Future iterations of the work could benefit from an explicit evaluation of these aspects along with strategies to mitigate any adverse effects.  To broaden the practical applicability of FeLMi it would be constructive to explore its performance and adaptability in diverse FSL scenarios beyond image classification addressing the limitation regarding its current domainspecific evaluations.", "tIZtD2kZ6zx": "Summary and Contributions The paper introduces a novel neurosymbolic generative model named Drawing out of Distribution (DooD) focusing on learning generalpurpose representations from perceptual inputs regarding strokebased drawing tasks. Uniquely DooD requires no supervision operates directly on images and performs unsupervised amortized inference with a symbolic stroke model enhancing both interpretability and generalization over previous models. DooDs evaluation demonstrates its superior generalization capabilities across data and tasks by outperforming baselines and matching stateoftheart (SOTA) on the Omniglot challenge tasks showcasing its ability to capture generalpurpose representations and take a step towards creating general and robust conceptlearning systems. Assessment of Strengths and Weaknesses Strengths: 1. Unsupervised Learning: DooDs capability to operate without supervision or preprocessing is a significant advancement making it easier to deploy and more versatile. 2. Generalization Across Data and Tasks: The models ability to generalize across different datasets and tasks without retraining or finetuning indicates a strong underlying representation learning mechanism. 3. Interpretability and Efficiency: The adoption of a symbolic stroke model for generating drawings contributes to the models interpretability and efficiency addressing a common critique of purely neural approaches. 4. Comprehensive Evaluation: The paper provides a thorough evaluation including zeroshot transfer ablation studies and comparison with stateoftheart models to validate DooDs performance claims effectively. Weaknesses: 1. Complexity and Understandability: The intricate design combining neurosymbolic elements might make the model challenging to understand and replicate without a detailed study potentially hindering broader adoption. 2. Specificity to StrokeBased Data: While DooD excels in strokebased drawing tasks its applicability to other types of data or tasks (e.g. 3D object generation) remains unexplored. 3. Scalability and Performance Cost: The paper lacks discussion on the scalability of DooD and the computational performance cost which are critical for realworld applications. Questions and Suggestions for the Authors 1. Extension to Other Data Types: Can DooD be adapted to work with other types of data or tasks beyond strokebased drawings such as texture synthesis or 3D object modeling 2. Scalability and Performance: Could the authors elaborate on the scalability of the proposed model and its performance cost especially compared to the baselines 3. Interpretability in Practice: The paper mentions the interpretability of DooDs symbolic stroke model. Could the authors provide more concrete examples or qualitative studies demonstrating how interpretability benefits users or improves outcomes Limitations and Potential Negative Societal Impact The authors have acknowledged the importance of interpretability and generalization in their model which is commendable. However they have not explicitly discussed the models limitations or potential negative societal impacts such as the propagation of biases in the datasets it is trained on. Constructive suggestions for improvement include:  Bias and Fairness: Conduct an analysis of biases in the strokebased data DooD is trained on and propose mechanisms to detect and mitigate these biases.  Scalability Concerns: Offer insights into the models scalability and potential performance bottlenecks encouraging future research to further optimize the architecture for largerscale applications.  Broader Applicability: Encourage research into extending the models approach to a wider range of tasks and data types potentially increasing its societal impact by addressing more diverse needs.", "prQT0gN81oG": "Review of SHAKE: Bridging Offline and Online Knowledge Distillation via Shadow Knowledge Transfer 1. Summary and Contributions The paper introduces SHAKE (SHAdow KnowlEdge transfer) a novel framework aimed at bridging the performance gap between offline and online knowledge distillation (KD) methods for neural network compression. The core insight driving this work is the identification of reversed distillation (i.e. studenttoteacher knowledge transfer) as the key factor in the performance superiority of online over offline KD methods rather than the training fashion per se. SHAKE operates by creating a proxy teacher model augmented with an additional \"shadow head\" that mimics the pretrained teachers predictions. This setup enables efficient bidirectional knowledge transfer between the student and the proxy teacher without the substantial training costs typically associated with finetuning pretrained teacher models. The authors support their claims with extensive experimental results across different tasks (classification object detection) datasets and architectures demonstrating significant gains in efficiency and accuracy over existing KD methods. 2. Strengths and Weaknesses Strengths:  Innovation: SHAKE introduces an original approach to KD by combining the strengths of both offline and online methods through the concept of a shadow head. This concept allows for more efficient training and better performance without substantial computational overhead.  Empirical Validation: The paper provides a comprehensive set of experiments across a range of tasks architectures and datasets showing that SHAKE consistently outperforms both baseline KD methods and recent advanced alternatives.  Practical Utility: By achieving competitive results with lower training costs and without extra inference overhead SHAKE presents a practical solution for deploying lightweight models especially for applications with strict computational budgets. Weaknesses:  Method Complexity: While SHAKE offers efficiency improvements the addition of shadow heads and the requirement for bidirectional distillation could increase the complexity of the training setup potentially hindering ease of use and adaptability in different scenarios.  Limited Analysis on Failure Cases: The paper could have further strengthened its findings by discussing conditions under which SHAKE may not perform as expected or may be less effective compared to other methods. 3. Questions and Suggestions  Regarding Generalizability: How does SHAKE perform in domains beyond image classification and object detection particularly where data structures are more complex (e.g. sequential or graph data)  Ablation Study on Shadow Heads: Could the authors provide an ablation study specifically focusing on the design and impact of the shadow head For instance how critical is the architecture of the shadow head to the overall performance gain and what are the tradeoffs involved in its design  Potential for Further Efficiency Gains: Are there opportunities to optimize the training process further perhaps by dynamically adjusting the involvement of the shadow head based on the training phase or student performance 4. Limitations and Societal Impact The authors have made commendable efforts to address the limitations of their work particularly in terms of the computational efficiency of SHAKE compared to traditional KD methods. However a deeper discussion on the potential negative societal impacts of their work such as misuse of efficient model compression techniques for malicious purposes or unintended biases in distilled models could provide a more balanced view. Constructively future iterations of this work could explore methods to ensure the ethical use of KD techniques and mechanisms to detect and mitigate biases that might be amplified through the distillation process. Conclusion Overall SHAKE represents a significant advancement in knowledge distillation techniques offering a viable path toward combining the benefits of offline and online methods. With its demonstrated efficiency and performance gains SHAKE has the potential to impact the deployment of neural network models across various realworld applications particularly those constrained by computational resources. Future work expanding on the applicability of SHAKE to a broader range of tasks and further optimization of the shadow head mechanism would greatly enhance the utility and applicability of this promising approach.", "xZmjH3Pm2BK": " Review of \"Wavelet Scorebased Generative Models for Accelerated Sampling of Multiscale Processes\"  1) Summary of the Paper and its Contributions This paper presents an innovative approach to scorebased generative models (SGMs) through the introduction of Wavelet Scorebased Generative Models (WSGM). Traditional SGMs synthesize data samples from Gaussian white noise using a timereversed Stochastic Differential Equation (SDE) but suffer from high computational costs due to the large number of time steps required for discretization. The key contribution of the WSGM is its ability to generate wavelet coefficients from coarse to fine scales with a controlled number of time steps reducing the overall time complexity of the synthesis process. This is achieved by factorizing the data distribution into a product of conditional probabilities of wavelet coefficients across scales rendering the process more efficient. The paper mathematically proves the acceleration obtained through this method for Gaussian distributions and empirically demonstrates its efficacy on phase transition physical processes and natural image datasets.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Technical Merit: The introduction of WSGMs represents a novel approach to addressing the computational inefficiencies inherent in traditional SGMs with strong theoretical underpinnings and empirical validation.  Theoretical Contributions: The paper provides rigorous proofs that demonstrate how WSGMs can achieve an acceleration in the sampling process by minimizing discretization errors and keeping the number of steps linear with respect to image size.  Empirical Validation: The authors empirically validate their model on a variety of datasets including physical processes at phase transition and natural images showcasing the practical benefits and applicability of their approach.  Clarity and Rigor: The paper is wellstructured with a clear exposition of the theoretical principles behind WSGMs and a detailed presentation of the experimental results that support the theoretical claims. Weaknesses:  Limited Scope: The models applicability is highlighted predominantly for multiscale processes where conditional wavelet probabilities approximate white Gaussian noise which may limit its generalizability.  Comparison with Other Models: While the paper does compare WSGMs with traditional SGMs a broader comparison with other stateoftheart generative models could provide a clearer understanding of its position within the current landscape.  Potential for Overfitting: The reliance on wavelet coefficients and their properties for generation could introduce risks of overfitting to particular data characteristics a point not extensively discussed in the paper.  3) Questions and Suggestions for the Authors  Generalizability to NonGaussian Processes: Can the authors elaborate on the potential of WSGMs to handle nonGaussian multiscale processes beyond those empirically studied  Impact of Wavelet Choice: How sensitive are the results to the choice of wavelet filters Could an investigation into the impact of different wavelet families strengthen the paper  Scalability Concerns: Given the computational improvements how does WSGM scale with significantly larger datasets than those tested  Potential for Overfitting: Could the authors discuss any mechanisms or safeguards within WSGM to prevent overfitting especially when working with sparse datasets  4) Limitations and Societal Impact The authors have successfully demonstrated the theoretical and empirical strengths of WSGMs but have not extensively discussed potential limitations or negative societal impacts. Specifically there is a need for a more detailed exploration of the models generalizability across diverse datasets and its behavior under different conditions (e.g. noisy or incomplete data). From a societal impact perspective while generative models hold tremendous potential their misuse for generating misleading or harmful content necessitates a discussion on ethical considerations and safeguards. Suggestions for Improvement:  Expand the discussion on the models limitations particularly concerning its adaptability to a broader range of datasets and resilience against data anomalies.  Incorporate an ethical considerations section detailing potential negative uses of generative models and proposing guidelines or measures to mitigate such risks.  Conclusion Overall this paper makes a significant contribution to the field of generative models by introducing WSGMs which promise to accelerate the sampling process significantly. While the model demonstrates strong theoretical and empirical performance further exploration into its limitations broader applicability and ethical implications would enhance its contribution to the field.", "tmUGnBjchSC": " Review of \"H`AEntropy Search: A DecisionTheoretic Approach to Bayesian Optimization\"  1) Summary of the Paper This paper presents an innovative approach to Bayesian Optimization (BO) by introducing a framework that generalizes traditional informationtheoretic BO methods through the lens of Bayesian decision theory. The central premise revolves around the use of a generalized entropy concept denoted as H`Aentropy which is parameterized by a loss function (`) and an action set (A) allowing it to closely align with the specific objectives of a downstream decisionmaking task. This flexibility enables the development of tailored acquisition functions that are optimized for a wide variety of optimization scenarios extending the applicability of BO beyond its traditional confines. Key contributions of this work include:  The introduction of a unifying framework that bridges the gap between decisiontheoretic and uncertaintyreduction paradigms in BO offering a broader perspective on how acquisition functions can be devised.  The proposition of a novel set of customizable acquisition functions derived from the proposed H`Aentropy concept effective for both traditional optimization tasks and novel problem settings.  The development of gradientbased methods to efficiently optimize the new family of acquisition functions demonstrating their effectiveness across several sequential decisionmaking tasks that vary in nature and complexity.  2) Strengths and Weaknesses Strengths:  Conceptual Novelty: The paper successfully extends the traditional BO framework by integrating a decisiontheoretic perspective offering a more generalized and versatile approach to acquisition function design.  Practical Applicability: The authors demonstrate the utility of their method across a diverse range of tasks showcasing its potential to address complex realworld problems that would be challenging to handle with conventional BO techniques.  WellSupported Claims: The empirical evaluations supported by comparisons with baselines and visualizations provide convincing evidence of the superiority of the proposed method in various contexts. Weaknesses:  Limited Discussion on Scalability: While the paper presents a robust framework and strong empirical results there is scant discussion on the scalability of the method to extremely highdimensional problems or very large datasets.  Assumption Clarity: The paper could benefit from a clearer discussion of the assumptions made within the proposed framework particularly regarding the properties of the loss function and action set that ensure the tractability and efficiency of the optimization procedure.  3) Questions and Suggestions  Question on Generalization: Can the authors elaborate on how the proposed methodology could be adapted or extended to handle dynamic optimization problems where the underlying function changes over time  Suggestion for Future Work: The paper could explore incorporating explicit constraint handling mechanisms within the H`Aentropy framework to address optimization problems with complex feasibility constraints.  Clarity on Limitations: It would be valuable to include a more detailed analysis of the limitations of the proposed method. For instance discussing situations or problem characteristics that might challenge the effectiveness or efficiency of the technique would provide readers with a clearer understanding of its applicability scope.  4) Addressing Limitations and Potential Negative Societal Impact The paper does not extensively address the limitations or the potential negative societal impacts of the proposed method. Given the broad applicability of Bayesian optimization unintended consequences or biases in decisionmaking scenarios could arise if the method were misapplied. Constructive Suggestions for Improvement:  The authors could enhance the discussion on limitations by exploring scenarios where the performance of the proposed method might degrade such as in nonstationary environments or for problems with nonsmooth objectives.  Addressing potential negative societal impacts requires a thorough assessment of the methods behavior in various domains especially those with significant ethical considerations. Incorporating fairness and bias assessment measures into the framework evaluation could mitigate some of these concerns.  To complement the proposed frameworks versatility and power the inclusion of guidelines or principles for responsible use in sensitive applications (e.g. healthcare policymaking) would be beneficial for ensuring ethical deployment. Overall this paper presents a significant advancement in the field of Bayesian optimization by introducing a novel decisiontheoretic approach that could pave the way for solving a wider array of optimization problems more effectively.", "sn6BZR4WvUR": "This paper introduces Multiscale Perturbed Gradient Descent (MPGD) a novel optimization framework designed to incorporate chaotic components into Gradient Descent (GD) in a controlled manner. By extending the GD recursion with external deterministic dynamical systems exhibiting chaotic characteristics the authors aim to improve generalization performance in nonconvex optimization problems. They analytically demonstrate through rough paths theory and homogenization techniques that the MPGD recursion weakly converges to a stochastic differential equation (SDE) driven by a heavytailed L\u00e9vystable process. Furthermore they derive a generalization bound for the limiting SDE and elucidate the implicit regularization effect of MPGD. Empirical results across various settings support the theoretical findings showcasing MPGDs advantage over traditional GD and stochastic GD (SGD).  Strengths: 1. Theoretical Contribution: The incorporation of rough paths theory to establish the connection between deterministic dynamics with chaotic perturbations and stochastic processes is novel and theoretically sound. This paper makes significant theoretical contributions by demonstrating that controlled chaotic behavior in GD can lead to improved optimization dynamics modeled by heavytailed processes. 2. Generalization Bound: The authors successfully derive a generalization bound relative to the parameters of MPGD providing insights into how chaotic perturbations contribute to generalization in deep learning models. This enriches our understanding of the relationship between optimization dynamics and learning theory. 3. Empirical Validation: The experiments conducted offer concrete evidence of MPGDs ability to outperform traditional GD and SGD under certain conditions particularly in settings involving nonconvex loss landscapes. This serves to validate the theoretical claims and underscores the practical utility of MPGD.  Weaknesses: 1. Assumption Clarity: The paper relies on several assumptions especially in deriving the generalization bounds. However the practical implications of these assumptions are not thoroughly discussed potentially limiting the reader\u2019s ability to assess the applicability of MPGD in realworld scenarios. 2. Complexity and Scalability: While the theoretical underpinnings are rigorously presented there is a lack of detailed discussion on the computational complexity of implementing MPGD compared to GD or SGD. Moreover the scalability of MPGD especially in very largescale deep learning tasks remains unclear. 3. Parameter Selection: The process of selecting the parameters for MPGD including stepsize and the coefficients modulating the chaotic perturbations could be elaborated further. Given the problemdependent nature of these parameters guidelines or strategies for their selection would enhance the papers practical contribution.  Questions and Suggestions for the Authors: 1. Exploration of Parameter Sensitivity: Could the authors provide additional insights or empirical results on how sensitive MPGDs performance is to the choice of its parameters (e.g. stepsize \u00b5 and \u03c3) 2. Comparison with Other Optimization Techniques: It would be beneficial if the paper included a broader comparison of MPGD against other optimization techniques that introduce randomness or perturbations such as momentumbased methods or adaptive learning rate algorithms. 3. Application to Wider Range of Problems: Can the authors speculate or provide preliminary results on how MPGD might perform in other machine learning tasks beyond those tested such as unsupervised learning or reinforcement learning scenarios  Limitations and Societal Impact: The authors have made an effort to address the limitations of their work primarily by underlying the theoretical nature of the paper and its focus on exploring deterministic dynamics within GD through chaos. However there\u2019s a need for more explicit discussion on the computational demands and practical applicability of MPGD in a broader spectrum of realworld applications. Regarding potential negative societal impact the paper does not directly address this aspect. While the theoretical nature of the work may imply minimal direct societal impact a brief discussion on how optimization techniques like MPGD could influence the development and deployment of AI systems potentially affecting fairness privacy or security would be constructive. Overall this paper makes a significant theoretical contribution to understanding and harnessing chaotic dynamics in optimization for machine learning. Expanding the empirical analysis and providing more practical guidance for implementing MPGD would make the findings more accessible to a wider audience within the machine learning community.", "wwW-1k1ljIg": " Review of \"Detecting and Mitigating Backdoor Attacks in Convolutional Neural Networks via Differential Entropy and BN Statistics\"  1. Summary of the Paper and its Contributions This paper presents a novel approach to identifying and neutralizing backdoor attacks in Convolutional Neural Networks (CNNs). Backdoor attacks involve embedding a hidden behavior within a model that is triggered by specific input patterns posing severe security risks. The authors discovered that neurons associated with backdoor behaviors manifest significantly different preactivation distribution properties when compared to benign neurons. Utilizing this observation the paper introduces two defensive strategies based on differential entropy and KullbackLeibler (KL) divergence targeting the abnormal preactivation distributions of neurons influenced by poisoned data. The effectiveness of these strategies is validated through comprehensive experiments demonstrating superior performance in mitigating backdoor attacks on CNNs without significantly compromising the models accuracy on clean data.  2. Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The paper addresses the detection of backdoor neurons with an original perspective by analyzing discrepancies in preactivation distributions a novel contribution to the field.  Effectiveness: The proposed methods are thoroughly evaluated against various backdoor attacks showing stateoftheart performance in reducing the attacks success rate while retaining high accuracy on clean samples.  Efficiency: The strategies introduced are computationally efficient allowing for faster backdoor mitigation which is practical for realworld applications.  Generalizability: These methods demonstrate robustness across different attack types and datasets indicating their potential applicability to a wide range of CNN models and contexts. Weaknesses:  Assumptionbased: The methodologies heavily rely on the assumptions made regarding the distribution characteristics of backdoor neurons. Any deviations in realworld attack scenarios from these assumptions might limit the effectiveness of the proposed solutions.  Hyperparameter Sensitivity: The need for a universal hyperparameter (`u`) implies potential challenges in tuning to achieve optimum defense performance across varied scenarios without prior knowledge of the attacks.  Limited to CNNs: While the focus is presently on CNN models expanding the applicability to other neural network architectures would broaden the impact of the research.  3. Questions and Suggestions for the Authors  Extending to Other Architectures: Have the authors considered testing their methods on neural network architectures beyond CNNs It would be beneficial to see if the observed distributional discrepancies in backdoor neurons hold for other models.  Robustness under Adaptive Attacks: How would the proposed defense strategies perform against adaptive adversaries aware of the defense mechanism It would be interesting to evaluate the robustness of these strategies under such scenarios.  Hyperparameter Tuning: Could the authors provide more insights or guidelines on selecting the universal hyperparameter `u` Exploring automated or datadriven approaches to optimize this parameter could significantly enhance usability.  4. Limitations and Societal Impact The paper could benefit from a deeper exploration of potential limitations particularly concerning the variability in the effectiveness of proposed strategies when faced with novel or sophisticated backdoor attacks. Addressing the robustness of these methods against adaptive adversaries who might exploit knowledge of the defense mechanisms employed would strengthen the paper. Additionally while the societal impact of mitigating backdoor attacks is inherently positive a discussion on the ethical considerations surrounding defensive strategies against AI vulnerabilities would be enriching. Suggestions for safeguarding against misuse and ensuring equitable access to defense technologies could further contribute to the papers societal impact contributions.  Overall this paper makes significant contributions to securing CNNs against backdoor attacks offering practical and effective defense strategies. Addressing the highlighted questions and potential limitations would further solidify its contributions to the field.", "sQ2LdeHNMej": "Review of \"FATHOM: Federated Automatic Hyperparameter Optimization\" 1. Summary: The manuscript introduces FATHOM (Federated Automatic Hyperparameter Optimization) a novel approach aimed at optimizing hyperparameters in federated learning (FL) settings without the trialanderror process typical in hyperparameter tuning methods. Specifically FATHOM focuses on the client learning rate number of local steps and batch size. By exploiting full knowledge of the FL training process FATHOM calculates analytical gradients with respect to these hyperparameters. This enables adaptive optimization of hyperparameters alongside the FL model parameters themselves promising more efficient communication and computation within FL environments compared to the conventional Federated Averaging (FedAvg) approach. The authors validate FATHOMs effectiveness through experiments on the Federated EMNIST62 and Federated Stack Overflow datasets showcasing its superiority in terms of communication and computational efficiency. 2. Assessment:  Strengths: 1. Innovation: FATHOM introduces a principled yet practical approach to hyperparameter optimization in FL addressing significant challenges such as communication efficiency and computational constraints. 2. Analytical Rigor: The paper provides a detailed theoretical foundation for FATHOM including assumptions derivation of gradients and convergence analysis under certain conditions. 3. Empirical Validation: Extensive testing on wellrecognized datasets with comparisons to FedAvg serves to underscore the practical benefits of the proposed method.  Weaknesses: 1. Scope of Application: While the focus on client learning rate local steps and batch size is justified the algorithms dependence on analytical gradients might limit its applicability to other hyperparameters or optimization contexts where such gradients are not easily obtainable. 2. Comparison Baseline: The paper primarily compares FATHOM with FedAvg and does not offer a broad comparison against other hyperparameter optimization methods mentioned in the related work which may offer different benefits under certain conditions. 3. Discussion on Generalizability: While FATHOM shows impressive results on two datasets there is a lack of discussion regarding its performance across a broader spectrum of tasks models or more diverse FL settings. 3. Questions and Suggestions for the Authors:  Could the authors elaborate on how FATHOM might be extended or adapted to optimize additional hyperparameters beyond those explored in this work  It would be beneficial to see comparisons with other stateoftheart hyperparameter optimization methods in federated learning to better position FATHOM within the existing ecosystem.  The discussion on limitations primarily focuses on algorithmic constraints. Can the authors comment on the scalability of FATHOM to larger more heterogeneous federated networks 4. Limitations and Societal Impact:  The authors have not fully explored the limitations and potential negative societal impacts of their work. For instance while FATHOM enhances communication and computational efficiency there could be implications regarding bias or fairness when deployed in realworld FL applications due to its adaptive optimization strategy. It could inadvertently prioritize clients that contribute more effectively to the global model potentially marginalizing those with less typical data distributions.  It is suggested that future iterations of the paper include a more thorough examination of these potential downsides alongside strategies for mitigation. For instance incorporating fairness constraints or bias detection mechanisms into the FATHOM framework could be a valuable step forward. Overall this paper presents a significant contribution to the FL community by introducing a novel hyperparameter optimization method that addresses critical challenges in communication and computational efficiency. With further validation across diverse settings and considerations for broader implications FATHOM has the potential to make a substantial impact on federated learning research and applications.", "mhP6mHgrg1c": "Review of \"ORIENT: An SMIBased Subset Selection Framework for Efficient and Effective Supervised Domain Adaptation\" 1) Summary of Contributions The paper presents ORIENT a novel subset selection framework designed to enhance the computational efficiency of supervised domain adaptation (SDA) methods without sacrificing performance quality. The framework uses Submodular Mutual Information (SMI) to select a source data subset that closely resembles target data thereby facilitating faster model training. The authors demonstrate how ORIENT harmonizes with established robust subset selection strategies like GLISTER GRADMATCH and CRAIG thereby establishing a unified approach to subset selection in SDA tasks. Through empirical evaluation on two realworld datasets the paper shows that ORIENT can significantly reduce training times (up to 3x speed up) while either maintaining or improving the model\u2019s performance on the target data. 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The utilization of SMI to select data subsets is a novel approach that potentially revolutionizes the efficiency of SDA. It addresses crucial drawbacks of existing SDA methods related to computational expense and environmental impact.  Unified Framework: ORIENTs ability to integrate with previous robust subset selection methods (GLISTER GRADMATCH CRAIG) shows the framework\u2019s versatility and potential for broad applicability in diverse SDA scenarios.  Empirical Validation: The comprehensive experiments carried out on two datasets provide strong empirical evidence supporting the frameworks effectiveness. The substantial reduction in training time without loss of performance is particularly compelling. Weaknesses:  Memory Requirements: While ORIENT reduces computational time the increased memory demand for storing the similarity kernel could limit its usability in resourceconstrained environments which is contrary to one of the objectives the authors aimed to address.  Limited Evaluation Scope: The evaluation is limited to only two datasets and focuses primarily on computational efficiency and performance. A broader evaluation including aspects like robustness to extremely noisy data or varying levels of domain shift would provide a more comprehensive validation of ORIENTs capabilities.  Generalizability and Scalability Concerns: Though promising its uncertain how ORIENT scales with extremely large datasets or applies across domains beyond those tested. Further experimentation in these directions would add to the frameworks credibility. 3) Questions and Suggestions for Authors  Memory Efficiency: Could the authors propose strategies to mitigate the high memory requirements for implementing ORIENT making it more accessible for use in lowresource settings  Evaluation on Varied Domains: Have the authors considered extending their empirical evaluation to include more datasets especially those from distinctly different domains to assess the generalizability of ORIENT  Comparison with Unsupervised Domain Adaptation (UDA) Methods: Given the computational efficiency focus how does ORIENT compare with UDA approaches especially in contexts where labeled target data is scarce or unavailable 4) Limitations and Potential Negative Societal Impact The authors have acknowledged the main limitation regarding memory requirements for the similarity kernel which could hinder the use of ORIENT in memoryconstrained environments. To address this suggesting potential compression or approximation techniques to reduce memory footprint without significantly affecting performance would be valuable. Additionally exploring the development of lightweight versions of ORIENT capable of running on edge devices could overcome this limitation. Regarding potential negative societal impacts the paper does not explicitly discuss them. Given the focus on reducing the environmental impact of model training this aspect is implicitly positive. However it might be beneficial for the authors to consider the broader ethical implications particularly concerning the datasets used for model training and the risk of perpetuating biases. Suggesting guidelines or safeguards to ensure the responsible use of ORIENT in sensitive applications (e.g. surveillance law enforcement) could contribute to mitigating any potential negative impacts. In conclusion ORIENT represents a significant advancement in the field of supervised domain adaptation by offering a solution that is both computationally efficient and performancewise competitive. Addressing the outlined questions and considerations could further enhance its contribution to the research community and its applicability in realworld scenarios.", "xK6wRfL2mv7": " Review of \"SharpnessAware Training for Free: A Novel Approach to Efficient and Effective Deep Neural Network Training\"  Summary This paper presents a novel methodology named SharpnessAware Training for Free (SAF) aimed at efficient deep neural network (DNN) training while maintaining superior generalization performance. Recognizing the computational inefficiency in existing SharpnessAware Minimization (SAM) techniques which double the computational overhead the proposed SAF method introduces a trajectory loss. This loss is computed based on the KullbackLeibler (KL) divergence between the outputs of a neural network at current and past weights aiming to track and minimize sharpness along the weight update trajectory at almost zero additional computational cost. The authors demonstrate the efficacy of SAF against SAM and its variants through experiments on the ImageNet dataset. Additionally they introduce MESA a memoryefficient version of SAF to address potential outofmemory issues on very large datasets.  Strengths 1. Innovation: The concept of leveraging the trajectory of weights to approximate sharpness without additional computational overhead is very innovative and addresses a critical limitation of SAM. 2. Empirical Results: The extensive experimental validation including comparisons with SAM and its followups on reputable datasets like ImageNet adds significant credibility to the proposed method. The improved performance and reduced computational overhead of SAF and MESA are commendable. 3. Methodological Clarity: The paper provides a clear explanation of the methodology from the theoretical construct of the trajectory loss to its application within the SAF and MESA frameworks.  Weaknesses 1. Generalization Across Tasks: While the paper shows remarkable results on image classification tasks its adaptability and effectiveness in other domains such as natural language processing or speech recognition remain unspecified. 2. Analysis of Temperature Parameter \u03c4: The role and sensitivity analysis of the temperature parameter \u03c4 in the KLdivergencebased trajectory loss is not discussed. This parameter could significantly influence the loss landscape and the resultant generalization performance. 3. Comparison with NonSAM Based Methods: The paper predominantly compares SAF with SAM and its variants. Incorporating comparisons with other stateoftheart training methodologies not based on sharpness awareness could provide a more rounded evaluation of SAFs performance.  Questions and Suggestions for the Authors 1. Expand Domain Applicability: Can the authors discuss or hypothesize the potential effectiveness of SAF in tasks outside image classification Any preliminary results or ongoing work in these directions would be insightful. 2. Temperature Parameter Sensitivity: It would be beneficial if the authors could provide an analysis of how the selection of \u03c4 affects the trajectory loss and the training outcome. Are there guidelines for setting this parameter across different problems 3. Further Comparisons: While the focus on SAMrelated methods is understandable given the papers scope adding comparisons with other efficient training strategies might further illustrate SAFs advantages. Could such comparisons be considered in future extensions of this work  Limitations and Societal Impact The paper acknowledges potential memory issues in extremescale datasets and proposes MESA as a solution albeit with a slight increase in computational cost. However the broader societal implications such as the environmental impact of deploying SAF in largescale applications remain unaddressed. Given the significant energy consumption associated with training large DNNs discussing the potential energy savings from using SAF could enhance the papers relevance.  Constructive Suggestions for Improvement  Providing a deeper theoretical analysis or proof to solidify the intuition behind why minimizing the trajectory loss effectively approximates minimizing the sharpness.  Exploring the incorporation of automatic \u03c4 adjustment strategies could make the SAF approach more adaptable across varied tasks and datasets.  Expanding the experimental section to include a broader range of tasks and comparing SAF with a wider array of existing efficient training methods would substantially strengthen the papers contributions.  Conclusion The paper introduces an innovative and efficient approach to DNN training tackling the computational overhead challenge of sharpnessaware techniques. The proposed SAF method with its compelling empirical results represents a significant step forward. However expanding its application scope thorough examination of its components like \u03c4 and broader comparisons could elevate its contribution to the field further.", "w1CF57sLstO": " Peer Review  Summary of the Paper This paper rigorously investigates the generalization properties of ModelAgnostic MetaLearning (MAML) within an overparameterized regime particularly when trained using Stochastic Gradient Descent (SGD). The authors delve into a mixed linear regression model to explore the bounds of MAMLs excess risk the impact of learning parameters (notably the adaptation learning rate) on generalization capabilities and discern between data and task distributions that favorably or adversely affect the generalization error. The theoretical analysis is backed by empirical experiments to validate the findings.  Assessment of Strengths and Weaknesses  Strengths: 1. Rigorous Theoretical Analysis: The paper provides both upper and lower bounds for the excess risk associated with overparameterized MAML. This dualbound approach adds depth to the understanding of how overparameterization affects MAMLs generalization capabilities. 2. Comprehensive Parameter Examination: The impact of various learning parameters on MAML\u2019s performance especially the adaptation learning rate is examined in detail offering insights into how these parameters can be optimized for better performance. 3. Empirical Validation: The empirical experiments conducted to validate the theoretical claims are a major strength. They not only corroborate the theory but also demonstrate the practical relevance of the findings. 4. Novel Contributions: The identification of specific data and task distributions that lead to diminishing generalization error in an overparameterized setting is a novel contribution providing actionable insights for designing efficient MAML systems.  Weaknesses: 1. Assumption Limitations: The study is primarily centered around a mixed linear regression model which although providing a solid framework for analysis may limit the applicability of the findings to more complex nonlinear realworld scenarios. 2. Complexity of Results: The theoretical findings characterized by various parameters and conditions can be overwhelming and difficult to digest for practitioners. A more simplified presentation or additional practical guidelines could enhance the papers accessibility. 3. Broader Implications and Applications: The paper could further explore the practical implications of its findings beyond the scope of mixed linear regression particularly in how these insights can be generalized or adapted to other metalearning models and applications.  Questions and Suggestions for Authors 1. Extension to Nonlinear Models: Can the theoretical framework and results presented for the mixed linear regression model be extended or adapted to nonlinear models If so what modifications or additional considerations would be necessary 2. Impact of Overparameterization: While the paper discusses the generalization capability in an overparameterized regime it would be valuable to include a comparative analysis of how these findings contrast with underparameterized settings offering a more comprehensive view of MAMLs behavior across different parameterization scales. 3. Practical Recommendations: Could the authors provide specific recommendations or guidelines on selecting learning parameters (e.g. adaptation learning rate) based on their analysis Insights on practical application could greatly enhance the papers utility for a broader audience.  Limitations and Societal Impact The paper cautiously addresses the limitations inherent in its focus on a mixed linear regression model which might not encapsulate the full spectrum of potential applications. However further discussion on how these limitations could be overcome or what implications they might have for broader applications of MAML in complex realworld scenarios would be beneficial. Additionally the paper does not delve deeply into the potential negative societal impacts of its findings. Given the increasing application of metalearning in sensitive areas (e.g. healthcare autonomous systems) a thoughtful consideration of how misinterpretations or misapplications of these findings could lead to adverse outcomes would be prudent. Incorporating suggestions for overcoming the highlighted limitations and a discussion on ethical considerations and societal impact could substantially enrich the papers contribution.", "lCGYC7pXWNQ": " Review of \"FrameMaker: A MemoryEfficient Video ClassIncremental Learning Approach\"  1. Summary of the Paper The paper introduces FrameMaker a novel approach for video classincremental learning that significantly reduces memory consumption by condensing multiple frames of a video into a single informationrich frame. This is achieved through two main components: Frame Condensing which merges multiple frames into one while trying to preserve the original videos essential features and InstanceSpecific Prompt a mechanism to restore any spatiotemporal details lost during the condensation process. The paper claims that FrameMaker outperforms existing stateoftheart methods on three benchmark datasets (HMDB51 UCF101 and SomethingSomething V2) while only utilizing 20 of the memory compared to traditional methods. Extensive experiments are conducted to validate the effectiveness of FrameMaker highlighting its superiority in memory efficiency and performance.  2. Assessment of Strengths and Weaknesses  Strengths:  Memory Efficiency: One of FrameMakers significant contributions is its ability to substantially reduce the required memory for video classincremental learning while still maintaining or even improving the performance compared to existing techniques.  Innovativeness: The concept of Frame Condensing coupled with InstanceSpecific Prompting offers a new pathway for handling video data preserving essential information in a memoryconstrained environment.  Experimental Validation: The authors conducted comprehensive experiments across several datasets and compared with stateoftheart methods to demonstrate the effectiveness of FrameMaker.  Weaknesses:  Limited Scope on Types of Videos: The paper primarily focuses on action recognition datasets. The effectiveness of FrameMaker on other types of video data such as surveillance or live broadcast footage remains unexplored.  Dependence on Initial Frame Selection: The efficiency of FrameMaker might significantly depend on the selection of frames for condensing. How FrameMaker performs under less ideal circumstances (e.g. poor initial frame selection) was not discussed.  Lack of Realworld Application Scenarios: Although the proposed method is tested on benchmark datasets a discussion on realworld applications and the challenges therein are missing.  3. Questions and Suggestions for the Authors  Generalizability: How does the FrameMaker approach perform on video data outside the action recognition context Exploring its applicability on diverse video types could enhance its practicality.  Robustness to Frame Selection: Could the authors provide insights on how the performance of FrameMaker might vary with different strategies for initial frame selections An analysis could strengthen the paper.  Realworld Deployment: It would be beneficial if the paper included discussions or case studies on deploying FrameMaker in realworld scenarios particularly how it could tackle privacy concerns more effectively.  Comparison with Nonvideo Incremental Learning Methods: An interesting study could involve comparing FrameMakers approach to conventional imagebased or nonvideo incremental learning methods. This would highlight the necessity and efficiency of FrameMaker in handling video data.  4. Limitations and Societal Impact The paper adeptly discusses its limitations particularly concerning potential failures under highly constrained memory budgets. However it could further explore the performance impact under varied operational conditions such as differing video qualities or compression rates. Regarding societal impact the authors briefly mention privacy concerns related to storing condensed frames. Its imperative to delve deeper into the ethical implications offering concrete solutions or mitigations for privacy preservation in applications of FrameMaker. In conclusion FrameMaker is a pioneering step towards efficient video classincremental learning. Its memoryefficient approach coupled with the innovative use of frame condensing and instancespecific prompts sets a new benchmark in the field. However future work should address its limitations and explore broader applicability and ethical considerations in realworld scenarios.", "oWqWiazEb62": " Peer Review on \"Distributionally Robust Stochastic Optimization with DataDependent Constraints\"  1) Brief Summary: This paper presents a novel approach for addressing stochastic optimization problems with datadependent constraints a critical challenge in machine learning applications such as fairness and resource allocation. The authors propose a distributionally robust reformulation that guarantees expectedvalue constraints are satisfied with a userspecified probability. This is achieved by adjusting the empirical constraints with an \"divergence ball leading to a pivotal and computationally efficient method that is demonstrated through application to fairnesssensitive classification tasks and a resourceconstrained newsvendor problem.  2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper introduces an innovative distributionally robust approach that extends the realm of stochastic optimization by integrating datadependent constraints and offering a high probability guarantee of satisfying expectedvalue constraints.  Practical Application: Demonstrating the methodology through a fairnesssensitive classification task and a newsvendor problem effectively showcases the approachs applicability and relevance to realworld scenarios.  Computational Efficiency: The developed algorithm compatible with standard stochastic optimization algorithms addresses a significant barrier in the field \u2014 the computational complexity of fairness or resourceconstrained machine learning problems. Weaknesses:  Assumption Transparency: While the method proposes robustness it relies heavily on assumptions regarding smoothness uniqueness strict complementarity and positive definiteness which may limit applicability. There is insufficient discussion on the practical implications when these assumptions are not met.  Generalization to NonLinear Constraints: The paper primarily focuses on linear expectedvalue constraints leaving a gap in the exploration of nonlinear functions of expected values which are prevalent in complex machine learning tasks.  Empirical Validation: Although the paper includes simulation studies and an application scenario the experimental validation appears limited. A broader set of benchmarks comparison with stateoftheart methods and indepth analysis of performance across varied datasets would strengthen the paper.  3) Questions and Suggestions for the Authors:  On Assumptions: Can the authors provide further insights into the robustness of the proposed method when the smoothness and strictly positive definiteness assumptions are relaxed Are there potential adaptations of the method in such scenarios  Extension to NonLinear Constraints: Would it be possible to extend the proposed method to handle nonlinear constraints directly Any preliminary thoughts on how this could be approached would be valuable.  Empirical Benchmarking: It is suggested to expand the experimental evaluation to include more datasets particularly those with known challenges in fairness or resource constraints. Comparing the proposed methods performance against existing approaches under these conditions would offer a clearer insight into its advantages and limitations.  Clarification on Computational Complexity: While the method is touted as computationally efficient a detailed complexity analysis or benchmarking against existing methods in terms of computational time and resource usage could further validate these claims.  4) Limitations and Societal Impact: The authors do not adequately discuss the limitations associated with the assumptions underpinning their method. Moreover there is scant analysis of the potential negative societal impact particularly in fairnesssensitive applications. For instance in misestimating the constraints or parameters there might be unintended biases reinforced through the models predictions. Suggestions for Improvement:  Provide a detailed discussion of the limitations and practical implications of the assumptions required for the method\u2019s application.  Conduct a thorough analysis of the potential negative societal impacts especially in fairnesssensitive scenarios to ensure transparency and awareness.  Propose mitigation strategies for potential biases or negative outcomes including guidance on responsible implementation and monitoring measures postdeployment. Overall the paper makes notable contributions to the field of stochastic optimization with datadependent constraints offering a promising direction for future research and practical applications. Addressing the highlighted areas could significantly enhance its value and impact.", "r6_zHM2POTd": "Review of the manuscript entitled: \"Enhancing Information Encoding in Recurrent Neural Networks through Tight Balance of Excitatory and Inhibitory Currents\" 1) Summary of the Paper and its Contributions: This manuscript presents a detailed investigation into the dynamics of balanced recurrent neural networks (RNNs) driven by timevarying input signals. The authors develop a novel nonstationary dynamic meanfield theory to analyze how a tight balance of excitatory currents by recurrent inhibition improves the accuracy and robustness of information encoding in recurrent networks. They demonstrate both theoretically and through numerical simulations that the mutual information rate between the input signal and network response increases linearly with the tightness of inhibitory and excitatory balance in the presence of noise and network chaos. The paper further elucidates through training experiments that RNNs naturally adopt a more tightly balanced state to enhance signal encoding efficiency with implications for recurrent network architecture design in biological and artificial systems. Significant contributions include:  A nonstationary dynamic meanfield theory for the dynamics of balanced RNNs driven by timevaried signals.  A mathematical framework linking the tightness of balance with the mutual information rate.  Analysis of network training dynamics revealing a spontaneous move towards tightly balanced states for efficient information encoding. 2) Assessment of Strengths and Weaknesses: Strengths:  The paper addresses a significant gap in the literature regarding the theory of recurrent rate networks under dynamic stimulus conditions extending the understanding of information encoding beyond the static input analysis prevalent in prior studies.  Novel theoretical contributions are solidly supported by explicit mathematical formulations analytic results and comprehensive numerical simulations that crossvalidate the theoretical predictions.  Practical implications for neural network design and training methodologies are clearly drawn contributing valuable insights into the principles underlying efficient information encoding in both biological systems and artificial neural architectures.  The manuscript is wellstructured providing a logical flow of the investigation from theory to simulations to practical implications which enhances the clarity and impact of the findings. Weaknesses:  Despite the extensive theoretical and numerical analysis the application scope appears limited to RNNs with specific architectures and activation functions. The generalizability of the results to a broader class of neural networks might require further investigation.  The discussion on potential negative societal impacts is notably absent which could be considered a significant oversight in light of the increasing scrutiny on AI ethics and safety.  While the authors briefly acknowledge limitations concerning the extension to multidimensional signals and biophysically detailed neuron models a more thorough exploration of these limitations and their potential resolutions would enrich the paper. 3) Questions and Suggestions for the Authors:  The authors might explore the potential generalizability of their findings to networks with different architectures or activation functions. Can the theoretical framework be extended or adapted to offer insights into such networks  Considering the significant implications for neural network training and architecture design how do the authors envision the practical application of their findings in current machine learning and neural network optimization practices  Could the authors comment on any potential negative societal impacts that could arise from the misuse or misinterpretation of their findings especially in the context of artificial intelligence applications and suggest mitigations 4) Limitations and Constructive Suggestions: The paper effectively addresses many technical limitations and challenges inherent in understanding dynamic stimulus encoding in recurrent networks. However the discussion on limitations could be expanded to include considerations of the broader applicability of the findings and potential ethical implications. For constructive improvement:  The authors could supplement their discussion by evaluating the robustness of their findings across a wider range of network architectures and external conditions providing insights into the universality and limitations of the proposed theories.  A section dedicated to the ethical considerations and potential negative societal impacts of the research could enhance the manuscripts relevance and responsibly guide future work in the field.  Further exploration of how these theories scale with network complexity and realworld tasks could offer valuable guidance for applying these principles in practical neural network designs and training methodologies. Overall this manuscript makes significant theoretical and practical contributions to the understanding of information encoding dynamics in recurrent neural networks presenting a robust framework that bridges the gap between dynamic stimulus encoding and network balance theory.", "pbILUUf_hBN": " PeerReview of \"Learning with Feedback Graphs: NearOptimal BestofBothWorlds Guarantees\"  1) Brief Summary The paper presents an algorithm for online learning with feedback graphs offering a framework where learner feedback is determined by a directed graph over the action set. This work extends the boundaries of learning in both stochastic and adversarial environments by achieving nearoptimal regret bounds. The algorithm named EXP3.G employs exploration schemes exploiting the feedback graphs structure to minimize exploration while maintaining robustness in stochastic and adversarial scenarios. Notably the algorithm is efficient and extends to scenarios with dynamically changing feedback graphs.  2) Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The paper introduces a novel exploration scheme integrated into the EXP3.G algorithm addressing online learning with feedback graphs. Its significant as it bridges the gap between full information and bandit settings offering a flexible and robust solution.  Theoretical Contribution: The derivation of nearoptimal regret bounds for both stochastic and adversarial environments is a strong theoretical contribution. These bounds are carefully derived with respect to the independence number of the feedback graph and the suboptimality gaps showcasing deep analytical work.  Practical Applicability: The algorithm doesn\u2019t require prior knowledge of the environment nor the time horizon enhancing its practicality for realworld applications where such parameters are unknown.  Extension to Dynamic Graphs: Extending the algorithm to accommodate timevarying feedback graphs adds a layer of complexity and broadens the applicability of the proposed solution in dynamic environments. Weaknesses:  Computational Complexity: While the algorithm is touted as efficient a detailed analysis of its computational complexity especially in scenarios with large action sets or dynamic graphs is missing. This analysis is crucial for understanding its feasibility in largescale applications.  Empirical Validation: The paper lacks empirical evaluation to support the theoretical findings. Realworld or simulated experiments could significantly strengthen the case for the algorithm\u2019s effectiveness across the claimed environments.  Comparison with StateoftheArt: There is insufficient comparison with existing algorithms. A more detailed benchmarking against stateoftheart solutions in both stochastic and adversarial settings could provide clearer insights into the relative advantages or potential tradeoffs.  3) Questions and Suggestions for the Authors  Empirical Evaluation: Could the authors provide empirical evidence supporting the efficiency and effectiveness of the EXP3.G algorithm across different environments Realworld case studies or simulations could enrich the paper.  Complexity Analysis: An indepth discussion regarding the computational complexity of the algorithm particularly for largescale problems would be valuable. How does the algorithm scale with the number of actions or the complexity of the feedback graph  Benchmarking: How does EXP3.G compare to existing algorithms within the same domain particularly in terms of regret bounds computational efficiency and practical applicability Including comparative analyses could further establish the significance of the proposed algorithm.  Impact of Approximations: The paper makes several assumptions for theoretical analysis. Could the authors discuss the potential impact of these assumptions on the algorithms performance in practice How sensitive is EXP3.G to variations in these assumptions  4) Limitations and Societal Impact The authors have thoroughly outlined the theoretical foundations of their proposed algorithm but have not extensively addressed its computational complexity or provided empirical validations. These aspects are crucial for assessing the algorithms limitations in practical implementations. Furthermore discussions on the potential negative societal impacts such as privacy concerns or misuse in adversarial settings are absent. Constructive suggestions for improvement include:  Detailed Complexity and Scalability Analysis: Offer a clearer view of the algorithms computational demands to assess its practical limitations.  Expansion on Societal Impact: Engage in a thoughtful examination of how the proposed method might be used or misused within societal contexts along with guidelines to mitigate potential negative impacts.  Empirical Validation: Augment the paper with empirical studies that not only validate the theoretical claims but also explore the algorithm\u2019s behavior under various realworld conditions. Inclusion of these elements would significantly strengthen the paper providing a more comprehensive understanding of the algorithm\u2019s practicality and its ethical considerations within broader societal contexts.", "n0dD3d54Wgf": "1. Summary of the Paper and Its Contributions This paper presents Sparse Continual Learning (SparCL) a novel framework aimed at making continual learning (CL) more efficient in resourcelimited scenarios such as edge devices. SparCL addresses not just the traditional goal of CL in mitigating catastrophic forgetting (i.e. the deterioration of model performance on past tasks when learning new tasks) but focuses on improving training efficiency in terms of memory footprint and computational demands. The core innovations of SparCL lie in its integration of taskaware dynamic masking (TDM) dynamic data removal (DDR) and dynamic gradient masking (DGM) which collectively enable significant reductions in training FLOPs while either preserving or improving accuracy compared to stateoftheart methods. Notably the framework is compatible with various existing rehearsalbased CL methods and demonstrates improved performance on benchmark datasets and realworld mobile devices. 2. Assessment of the Paper Strengths:  Originality and Novelty: The approach of integrating sparsity into the continual learning process is unique addressing the often overlooked aspect of computational efficiency.  Comprehensive Evaluation: The paper provides a thorough evaluation of SparCL comparing its performance with existing methods across multiple benchmarks and realworld scenarios which convincingly demonstrates its effectiveness.  Practical Impact: By focusing on efficiency and deploying on edge devices SparCL addresses a significant gap in the realworld application of CL systems emphasizing its potential for broad adoption. Weaknesses:  Generalization of DDR and DGM: While the paper articulates the mechanisms and benefits of DDR (dynamic data removal) and DGM (dynamic gradient masking) it may benefit from deeper exploration into how these components generalize across tasks that vary greatly in difficulty or complexity.  Extensibility: The paper could further discuss how SparCL could be extended or adapted to other domains of CL such as unsupervised or reinforcement learning which could greatly widen its applicability. 3. Questions and Suggestions  On the Choice of Sparsity Ratios: How sensitive are the models performance gains to the specific sparsity ratios chosen and could an adaptive mechanism for choosing these ratios enhance performance  Extending SparCL to Other Learning Paradigms: Is there potential for adapting SparCL\u2019s principles (TDM DDR DGM) to other forms of continual learning such as unsupervised learning or reinforcement learning scenarios Insights into potential modifications for such adaptations would be beneficial.  Impact of Hyperparameter Tuning: It would be interesting to see an analysis of the sensitivity of SparCL\u2019s performance to the tuning of its novel hyperparameters (e.g. intratask and intertask adjustment ratios). Could automated hyperparameter tuning methods like Bayesian optimization enhance SparCLs efficiency 4. Limitations and Societal Impact The authors discuss limitations and potential negative societal impacts in Sections 7 and 8 but there seems to be room for further elaboration. Specifically while SparCL significantly reduces training FLOPs the energy consumption and environmental impact during the training phase particularly on a large scale remain concerns that could be addressed more comprehensively. Furthermore considering the deployment on edge devices a discussion on the potential for misuse in surveillance or privacysensitive applications would be pertinent. Suggestions for Improvement:  Energy Consumption Analysis: Including a more detailed analysis of energy consumption during training and inference on edge devices could provide a clearer picture of the environmental impact of deploying SparCL.  Addressing Misuse: Proposing guidelines or safeguards to prevent misuse of SparCL in applications where continual learning could infringe on privacy rights or contribute to surveillance overreach would be a valuable addition.", "x3JsaghSj0v": " Review of \"Adaptive Node Sampling for Graph Transformer (ANSGT)\"  Brief Summary This paper addresses the limitations faced by existing Graph Transformer models when applied to large graphstructured data. The core issues identified include the agnostic nature of node sampling towards graph characteristics and an oversight of longrange dependencies due to a focus on local neighbors. To mitigate these the authors propose Adaptive Node Sampling for Graph Transformer (ANSGT) employing an adversary bandit problem optimized node sampling strategy alongside a hierarchical attention scheme incorporating graph coarsening for global context capture. The paper claims remarkable improvements over both Graph Transformer architectures and popular Graph Neural Networks (GNNs) across several realworld datasets.  Assessment of Strengths and Weaknesses Strengths: 1. Novel Approach: The proposal of incorporating an adversarial bandit problem framework for adaptively selecting informative nodes is innovative. This addresses the static nature of existing sampling strategies effectively. 2. Hierarchical Attention Scheme: The integration of local and global information through hierarchical attention coupled with graph coarsening is well thought out. It systematically reduces computational complexity while capturing essential graph characteristics. 3. Empirical Evaluation: Extensive experiments and ablation studies provide comprehensive insights into the methods efficacy. The improvements in node classification tasks across six benchmark datasets are substantial. Weaknesses: 1. Comparison Baseline Selection: While the paper compares ANSGT against several baselines it lacks comparison with the latest advancements in graph neural networks that might employ similar transformative or adaptive sampling approaches. 2. Potential Overfitting Concern: The paper does not extensively discuss the risk of overfitting due to adaptive node sampling especially in graphs with erratic structural properties or in cases of sparse labels. 3. Complexity Analysis: Although claimed linear in computational complexity a more detailed breakdown or comparison of runtime and resource utilization against baselines would strengthen the argument regarding scalability and efficiency.  Questions and Suggestions 1. Regarding Overfitting: Have the authors conducted experiments on overfitting measurement or added regularization explicitly to prevent overfitting arising from very adaptive sampling strategies 2. Comparison with Latest GNNs: How does ANSGT perform against the newest GNN models not mentioned in the paper Including these comparisons could provide a clearer picture of the proposed methods standing. 3. Detailed Complexity Analysis: Given that the method involves several layers of computation including adaptive sampling and graph coarsening can the authors provide a deeper analysis or empirical evidence of computational savings in practice  Limitations and Societal Impact The authors have not explicitly discussed potential limitations and negative societal impacts of ANSGT. Considering the application domain of GNNs extends to sensitive areas like social network analysis and recommender systems its crucial to understand how adaptive sampling may introduce biases or unfair predictions. The methods transparency and interpretability are also areas that could be explored further. A constructive suggestion would be to conduct fairness and bias analysis across different domains to ensure the adaptability does not inadvertently amplify undesirable characteristics of the data.  Conclusion Adaptive Node Sampling for Graph Transformer presents an innovative approach to addressing fundamental challenges in applying transformers to graph data. Its strengths in adaptively sampling informative nodes and incorporating global context through hierarchical attention are commendable. However addressing the outlined questions and potential concerns could further solidify its contributions and applicability across a broader spectrum of graphbased learning tasks.", "u4KagP_FjB": " 1) Summary of the Paper and Contributions The paper introduces Spartan a novel approach for training sparse neural network models focusing on achieving predetermined levels of sparsity. Spartan incorporates a blend of two key techniques: (1) soft topk masking based on a regularized optimal transportation problem for promoting sparsity during the training process and (2) dual averagingbased parameter updates coupled with hard sparsification in the forward pass which allows for a controlled explorationexploitation tradeoff throughout the training phase. This method is distinctively flexible supporting various sparsity patterns and costsensitive sparsity allocations. Empirical validation on the ImageNet1K dataset with ResNet50 and ViTB16 models demonstrates Spartans effectiveness achieving significant top1 accuracy with substantial sparsity. The paper makes pivotal contributions by introducing a sparsityinducing training algorithm that bridges the gap between two existing sparsity induction methods offering a robust empirical evaluation and exploring the nuanced effects of sparsity hyperparameters on model performance.  2) Assessment of Strengths and Weaknesses Strengths:  Innovativeness: Spartan utilizes an original combination of soft topk masking and dual averaging for sparsity presenting a fresh perspective on sparse neural network training.  Flexibility: The methods support for various sparsity policies including unstructured and blockstructured sparsity is a significant strength catering to a wide range of model architectures and application requirements.  Empirical Results: Spartans performance on the ImageNet1K classification task with minimal accuracy loss at high sparsity levels underscores its efficacy and practical utility compared to fully dense training. Weaknesses:  Complexity and Overhead: The implementation complexity and computational overhead particularly related to the optimal transportation problem and Sinkhorn iteration might limit Spartans applicability in resourceconstrained environments.  Lack of Theoretical Analysis: While Spartan shows promising empirical results the paper lacks a deep theoretical analysis of why the method outperforms existing approaches which could provide valuable insights into its effectiveness and limitations.  Generalization beyond ImageNet1K: The evaluation focuses primarily on ImageNet1K. Additional studies on diverse datasets and tasks would be beneficial to fully understand Spartan\u2019s generalization capabilities.  3) Questions and Suggestions for the Authors Questions:  How does Spartans training time compare to fully dense models and other stateoftheart sparsityinducing methods particularly on more extensive datasets or more complex networks  Could the authors elaborate on Spartans potential extension to other model architectures beyond ResNet50 and ViTB16 especially in language processing or generative models Suggestions:  Theoretical Insights: Future work could include a more thorough theoretical discussion on the tradeoffs between exploration and exploitation in sparse training and how Spartan effectively manages this balance.  Broadening Evaluation: It would be beneficial to evaluate Spartans performance on a wider array of datasets and tasks including those outside the vision domain to better understand its general applicability and limitations.  Detailed Resource Utilization Analysis: Providing a more detailed analysis of the computational resources and time required by Spartan compared to other methods would offer valuable insights for practical adoption.  4) Limitations and Societal Impact The authors have not sufficiently discussed Spartans limitations particularly regarding its computational overhead and potential applicability challenges in different contexts. While Spartan presents a significant advancement in sparse training detailing these limitations could provide a more balanced view of its practical utility. Additionally the paper does not delve into the potential negative societal impacts though it implicitly contributes to energyefficient AI research. Future iterations could benefit from a detailed discussion on these aspects including constructive suggestions to mitigate any identified limitations such as optimizing the computational efficiency of the soft topk masking and dual averaging process for broader applicability.", "ldl2V3vLZ5": "The paper presents S3GC a novel approach for graph clustering that incorporates side information in the form of node features alongside graph structure. This method employs a onelayer Graph Neural Network (GNN) and contrastive learning to learn features that are more clusterable compared to existing stateoftheart methods. Specifically S3GC addresses the limitations of prior methods by effectively combining both graph structure and node features even in cases where one of the sources is insufficient to learn accurate clusters on its own. Through extensive empirical evaluation on various benchmark datasets including largescale graphs with up to 100 million nodes S3GC demonstrated significant improvement in clustering accuracy (up to 5 gain in Normalized Mutual Information) and scalability over previous methods.  Strengths:  Innovative Methodology: S3GCs use of a simplified GNN encoder along with contrastive learning for promoting clusterability marks a significant advancement in the graph clustering domain. This method ensures the model is scalable to very large graphs without sacrificing performance.  Scalability: The paper demonstrates S3GCs ability to handle extremely large datasets a critical aspect that has been a major challenge for existing graph clustering methods. The fact that it can be deployed on a single virtual machine for graphs with 100M nodes is particularly impressive.  Empirical Evaluation: The comprehensive testing across multiple benchmark datasets including comparisons with stateoftheart methods across various metrics substantiates the methods effectiveness and robustness.  Weaknesses:  Limited Theoretical Underpinning: While the empirical results are strong the paper lacks a detailed theoretical analysis explaining why S3GC outperforms existing methods particularly in terms of the contrastive learning component and its impact on clusterability.  Generalization Concerns: The paper predominantly focuses on synthetic and structured benchmark datasets. There is a lack of exploration on heterogeneous or noisy realworld graphs which might present different challenges not captured in the current evaluation.  Parameter Sensitivity Analysis Missing: The paper does not discuss the sensitivity of the methods performance to its hyperparameters such as the contrastive learning margin or the architecture of the GNN encoder which could be important for practical implementation.  Questions and Suggestions for the Authors: 1. Theoretical Analysis: Could the authors provide more insight into the theoretical foundations of S3GC especially regarding how contrastive learning contributes to the clusterability of the features 2. Impact of Heterogeneity: How does S3GC perform on heterogeneous graphs with different types of nodes and edges Including such evaluations could significantly enhance the papers contributions. 3. Hyperparameter Sensitivity: It would be beneficial for the authors to include an analysis of the sensitivity of the models performance to its various hyperparameters. This could offer guidelines for practitioners on how to effectively deploy S3GC. 4. Comparison with Noncontrastive Methods: While the paper compares S3GC with several stateoftheart methods it would be interesting to see how it fares against noncontrastive graph clustering approaches particularly those leveraging different machine learning paradigms.  Limitations and Societal Impact: The authors have not thoroughly discussed the limitations of their work or its potential negative societal impacts. For instance graph clustering algorithms can be used in sensitive applications such as social network analysis which may raise privacy concerns. A discussion on the ethical implications and privacypreserving measures would be valuable. Additionally the paper could benefit from a more detailed discussion on the scenarios where S3GC may fail or underperform such as graphs with highly noisy node features or irregular structure.  Constructive Suggestions for Improvement:  Incorporating a theoretical analysis or providing intuition behind why contrastive learning significantly improves the clusterability in the graph clustering context.  Extending the evaluation to include realworld heterogeneous graphs and conducting sensitivity analysis on hyperparameter choices to guide implementation.  Discussing the ethical implications and limitations of the proposed method especially in applications with potential privacy concerns would round out the paper and provide a more comprehensive understanding of S3GCs impact.", "mvbr8A_eY2n": "Review of \"A SemiDiscrete Optimal Transport Approach for Fair Allocation with Envy Constraints\" 1. Summary: This paper tackles the challenge of fairly allocating a distribution of items among n recipients each requiring a fixed fraction of items while limiting envy among recipients. The authors propose a novel approach by framing this as a variant of the semidiscrete optimal transport (OT) problem which allows for a continuous tradeoff between efficiency and envy. Unlike prior works that either ignore envy or treat it as a hard constraint this work introduces a model where envy can be budgeted. Key contributions include the formulation of this allocation problem within the OT framework introduction of a stochastic optimization algorithm with proven convergence investigation of the statistical properties through PAClike sample complexity bounds and demonstration of the models effectiveness with artificial and semireal data. 2. Assessment: Strengths:  The paper addresses an important problem in fair allocation by considering envy a common issue in distributive justice and providing a manageable way to balance it with efficiency.  The formulation of the problem as a semidiscrete OT problem is innovative and provides a more structured way to approach the allocation problem which is appreciated for its computational efficiency.  The authors present a strong theoretical foundation with provable guarantees on the convergence rate of their optimization algorithm and a polynomial bound on sample complexity offering reliable methods for practical implementation.  The use of artificial and semireal data for numerical demonstrations showcases the models applicability and effectiveness providing a concrete understanding of how the proposed methods work in real scenarios. Weaknesses:  While the paper provides a novel formulation and promising results the discussion on the realworld applicability and limitations especially in diverse allocation scenarios other than blood donations is somewhat limited.  The potential negative societal impacts of envybased allocation and how they might be mitigated are not thoroughly explored. For example the consequences of envy allocation in scenarios with significant disparities among recipients could have been addressed more critically.  The experimental section although informative could benefit from a broader range of datasets and more rigorous comparison with existing methods to fully establish the models superiority and versatility. 3. Questions and Suggestions for the Authors:  Could the authors provide more insight into how the model might perform or be adapted to other fair allocation problems outside the context of blood donations  It would be beneficial if the authors could discuss the potential negative societal impacts of their model in more depth. Specifically how might allocations that incorporate envy affect societal perceptions of fairness or contribute to inequality  A comparison with existing allocation methods not based on OT especially in terms of scalability and efficiency could help highlight the advantages of the proposed approach more clearly. Could such comparisons be included 4. Limitations and Societal Impact: The authors mention the importance of backtesting in critical applications like blood donation which is wise. However discussions on limitations and the broader societal implications of the envybudgeting approach are somewhat lacking. Future implications particularly in scenarios where resources are scarce or highly contested could exacerbate inequalities or introduce new forms of envy not considered by the model. The paper would benefit from a more detailed analysis of these aspects including constructive suggestions on how these potential issues could be addressed. Expanding on these limitations and considering diverse application contexts would strengthen the papers contribution to equitable resource allocation research.", "riIaC2ivcYA": " Review of the Paper: \"Improved Algorithms for Neural Active Learning in NonParametric Streaming Settings\"  1) Summary of the Paper and Its Contributions The paper presents a significant advancement in the domain of neural network (NN)based active learning algorithms specifically tailored for the nonparametric streaming setting. The authors introduce INeurAL an improved algorithm that addresses several limitations of stateoftheart (SOTA) works by introducing two new regret metrics aimed at better reflecting the algorithms generalization capability. The proposed INeurAL algorithm showcases a novel approach in exploiting neural networks for exploration and exploitation ensuring efficient parameter updates through minibatch Stochastic Gradient Descent (SGD) with warm start and incorporating a query decisionmaker for kclass classification problems. The algorithms design not only achieves a reduction in regret but also removes the curse of input dimensionality essentially advancing both the theoretical underpinnings and empirical applicability of neural active learning. Empirical validations demonstrate the superiority of INeurAL over existing baselines.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of two regret metrics that better cater to the needs of active learning in streaming settings is commendable. This reflects a deep understanding of the inadequacies in current methodologies.  Comprehensive Approach: The algorithm effectively combines neural networks representation power with active learnings efficiency addressing both theoretical and practical challenges.  Empirical Evidence: The extensive experiments across multiple datasets underscore the algorithms superior performance compared to SOTA methods providing robust empirical evidence for its effectiveness.  Theoretical Guarantees: Providing an instancedependent performance guarantee that also addresses historical challenges like the curse of dimensionality adds significant value to the proposed method. Weaknesses:  Complexity of Implementation: Although the paper is wellwritten the intricate details concerning the implementation and the algorithms complexity could pose challenges for replication and broader adoption.  Limited Discussion on Practicality for Realworld Applications: Despite excellent empirical performance the paper could benefit from a discussion on the feasibility of implementing INeurAL in realworld largescale applications particularly in terms of computational resources and scalability.  3) Questions and Suggestions for the Authors  Scalability Concerns: Can the authors provide insights or empirical evidence on the scalability of INeurAL especially when dealing with extremely large datasets in practical scenarios  Application Breadth: How transferable is the proposed algorithm across different domains It would be beneficial if the authors could discuss or demonstrate the versatility of INeurAL beyond the datasets used in the experiments.  Comparison with Human Annotation: Given the active learning context how does INeurALs performance relate to scenarios with humanintheloop annotation Any insights into the algorithms adaptability to varying annotation quality would be valuable.  4) Limitations and Suggestions for Improvement The authors have made commendable efforts to address the limitations inherent in existing active learning approaches using neural networks. However more clarity on the potential negative societal impacts particularly concerning the deployment of such technologies in sensitive domains (e.g. healthcare security) would make the paper more comprehensive. The ethical considerations potential biases in the data and the algorithms behavior when confronted with such biases should also be discussed. A recommendation would be to include a section dedicated to exploring these dimensions offering guidelines or mitigation strategies to handle possible adverse effects.", "q-FRENiEP_d": "Review of \"SageMix: A SaliencyGuided Mixup for Point Clouds to Preserve Salient Local Structures\" 1. Summary of the Paper This paper introduces SageMix a novel data augmentation technique specifically designed for 3D vision tasks involving point clouds. Unlike existing Mixup methods that are primarily developed for images SageMix is a saliencyguided Mixup method that aims to preserve the salient local structures in point clouds by extracting salient regions from two point clouds and combining them into one continuous structure. The paper claims that this approach not only improves generalization performance but also enhances model robustness and uncertainty calibration. SageMixs effectiveness is demonstrated through extensive experiments on various benchmark point cloud datasets and tasks including 3D shape classification part segmentation and even 2D image classification with competitive performance. 2. Assessment of Strengths and Weaknesses Strengths: a. Novelty: SageMix is one of the first attempts to apply a saliencyguided Mixup approach to point clouds addressing the unique challenges of the unordered and nongrid nature of point cloud data. b. Comprehensive Evaluation: The authors conduct an extensive evaluation of the proposed method across multiple datasets and tasks showing consistent improvements over existing methods in terms of generalization robustness and uncertainty calibration. c. Applicability: Despite being designed for point clouds SageMixs applicability to 2D image classification and its competitive performance therein demonstrate the methods versatility. d. Efficiency: The proposed method avoids the computational burden of solving additional optimization problems by implementing a simple sequential sampling based on reweighted saliency scores. Weaknesses: a. Lack of Theoretical Rationale: The paper lacks a theoretical explanation or rationale for why SageMix works better than other Mixup methods especially in preserving salient features in point clouds. b. Potential Overfitting to Salient Features: While focusing on salient regions might improve performance metrics theres a risk that models might overfit to these features and potentially ignore other informative albeit less salient parts of the data. c. Limited Evaluation on RealWorld Applications: Although the method is tested on several benchmark datasets the paper does not present applications to realworld point cloud tasks (e.g. autonomous driving robotics) which could significantly benefit from such data augmentation techniques. 3. Questions and Suggestions for the Authors a. Can the authors provide a more detailed theoretical insight or rationale for the observed improvements particularly concerning the preservation of salient features b. It would be beneficial to see a comparison or exploration of how SageMix performs when applied to realworld point cloud tasks perhaps through a case study or additional experiments. c. Considering the risk of potential overfitting to salient features could the authors explore mechanisms or modifications to SageMix that ensure a balanced learning from both salient and nonsalient regions 4. Limitations and Societal Impact The paper briefly mentions the limitations regarding the theoretical explanations and demonstrates an awareness of the potential negative implications (e.g. overfitting to salient regions). However it would be advantageous to discuss in more depth how these limitations could affect the applicability of SageMix in various contexts. Additionally the authors should consider elaborating on potential negative societal impacts especially biases that may arise from focusing intensely on salient features which might not be uniformly present across different realworld datasets. This can be partially addressed by encouraging diversity in the training datasets and considering the inclusion of fairness metrics in the evaluation process.", "thgItcQrJ4y": "Review of \"Understanding the Dynamics of Gradient Descent in the Edge of Stability Regime\" 1) Summary: The manuscript delves into the Edge of Stability (EOS) phenomenon observed in neural networks trained using fullbatch gradient descent. The authors first empirically identify that the norm of the weight vector in the output layer is a reliable indicator of sharpness dynamics throughout the training process. Through a mixture of theoretical insights and empirical evidence the paper categorizes the gradient descent trajectory into four distinct phases based on the behavior of the sharpness value. Further it provides an indepth analysis of these phases detailing the dynamics at play which contribute to the changing sharpness and loss values. A rigorous proof is offered for a special case involving twolayer fully connected linear neural networks showcasing the progression through the identified phases. 2) Assessment: Strengths:  The paper addresses a fascinating aspect of the training dynamics of neural networks that diverges from traditional optimization theory contributing significantly to the understanding of deep learning behavior.  The division of the gradient descent trajectory into four phases depending on sharpness dynamics is both novel and insightful presenting a clear framework for future studies to explore the phenomenon.  The empirical identification of the output layer weight norm as an indicator of sharpness dynamics is a valuable discovery that may lead to practical applications for monitoring and adjusting the training process.  Theoretical rigor is evident in the concise proof provided for the behavior of sharpness in the twolayer fullyconnected linear neural network scenario. Weaknesses:  The claims heavily rely on empirical observations and assumptions that may not hold across different architectures or datasets. The generalizability of the findings needs further exploration.  While the paper provides a theoretical proof for a specific case the broader applicability to more complex networks or realworld scenarios is less clear. The leap from the theoretical model to practical nonlinear and deeper networks may be significant.  The paper discusses the limitations and potential negative societal impacts of the work in general terms but a more detailed analysis tailored to the specific contributions of the paper could provide better clarity and guidance. 3) Questions and Suggestions for Authors:  Can the authors provide additional empirical evidence or theoretical analysis to strengthen the claim that the identified phases of training dynamics are universally observable across different network architectures and datasets  Would including a broader range of network models in the empirical analysis help validate the assumptions made in the paper Specifically how do these dynamics play out in networks with normalization layers or residual connections  The discussion on societal impacts is appreciated but it feels rather generic. Could the authors speculate on more specific scenarios where the misuse of this understanding (e.g. optimizing malicious AI behaviors) could have negative impacts offering suggestions on mitigations 4) Limitations and Societal Impact: The authors have made an initial attempt to address the limitations of their work by pointing out the reliance on specific assumptions and the limited scope of their theoretical proof. However a more detailed discussion on the limitations regarding the generalizability of their findings to other models and realworld applications would enhance the paper. Additionally while the paper mentions potential negative societal impacts in broad terms a deeper analysis of specific scenarios where these insights could be misused or lead to unintended consequences along with proposed safeguards would be beneficial. Suggestions for improvement include expanding the empirical validation across a wider range of architectures and exploring the implications of optimizing networks with malicious intent alongside proposing guidelines for responsible use of these insights in AI development.", "rjbl59Qkf_": "The paper under review explores the efficacy of Generalized Reweighting (GRW) algorithms specifically in addressing the issue of nonrobust empirical risk minimization (ERM) under distributional shifts which is the variation between training and testing data distributions. GRW approaches including popular methods such as importance weighting and variants of distributionally robust optimization (DRO) aim to iteratively update model parameters based on reweighted training samples to achieve better generalization on unseen data distributions. The authors provide a comprehensive theoretical analysis showing that when applied to overparameterized models GRW approaches yield models with performance and implicit biases similar to those obtained via ERM suggesting that GRW does not offer significant improvements in distributional robustness. They also demonstrate that minor regularization efforts do not alter this conclusion significantly. The study concludes by suggesting alternate pathways to enhance distributional robustness which might involve moving beyond GRW strategies or developing new loss functions tailored to these algorithms. Assessment of Strengths: 1. Significant Theoretical Contributions: The paper successfully establishes a critical theoretical understanding regarding the limitations of GRW approaches in achieving distributional robustness. The detailed analysis covering both linear models and wide neural networks adds depth to the discourse on generalization under distributional shifts. 2. Comprehensive Exploration: The study makes a comprehensive effort to cover varied aspects of GRWlike algorithms pondering both regression and classification tasks and engaging with different model complexities. This broad scope makes its findings relevant to a wide array of applications in machine learning. 3. Clarity and Organization: The paper is wellorganized with clear definitions and statements of theoretical results. The progression from the problem statement through to the theoretical findings and implications for future work is logical and easy to follow. Assessment of Weaknesses: 1. Lack of Empirical Validation: While the theoretical insights are valuable the paper lacks empirical analysis to validate these insights. Demonstrating the theoretical findings through practical experiments would have strengthened the papers claims. 2. Assumption Limitations: The assumptions required for their theoretical results particularly concerning overparameterized models and the specific conditions under which GRW and ERM converge to similar results may not always hold in practical scenarios. The paper does discuss limitations but a more thorough exploration of these aspects and their implications on the generality of the results would be beneficial. 3. Consideration of Alternative Approaches: The conclusion pivots towards the need for alternative or novel approaches outside the realm of GRW algorithms. However the discussion does not deeply engage with existing or emerging methodologies that could potentially address distributional robustness more effectively. Questions and Suggestions for Authors: 1. Empirical Evidence: Could the authors provide empirical evaluations possibly on synthetic or benchmark datasets to corroborate their theoretical findings This would significantly enhance the papers credibility and applicability. 2. Broader Applicability: How do the authors envision their findings influencing the development of new algorithms aimed at overcoming the limitations of GRW approaches Are there specific characteristics or features that future algorithms should aim to incorporate 3. Impact of Assumptions: The papers theoretical results rely on specific model assumptions. It would be helpful to discuss the robustness of these findings under slight deviations from these assumptions. How sensitive are the results to variations in model parameters and training dynamics Limitations and Societal Impact: The paper acknowledges its limitations regarding the assumptions made and the focus on overparameterized models. Further considering that distributional robustness is a critical aspect of deploying machine learning models in sensitive and impactful domains (e.g. healthcare finance) exploring the potential negative societal impacts of models that fail to generalize under distribution shifts would add an important dimension to the discussion. Expanding on this by providing guidelines or recommendations on mitigating such risks could make a valuable contribution. In conclusion while the paper casts a valuable theoretical examination of the limitations inherent to GRW algorithms in achieving distributional robustness validating these insights through empirical studies and expanding on the discussion around realworld impacts and alternative approaches would significantly enhance its contribution to the field.", "q85GV4aSpt": "Review of \"Understanding Square Loss in Deep Classifiers\": 1) Summary: This paper systematically investigates the theoretical and practical aspects of employing square loss for training deep learning classifiers specifically focusing on overparameterized neural networks in the neural tangent kernel (NTK) regime. The authors provide theoretical contributions surrounding the generalization error robustness and calibration error of square loss in classification distinguishing between separable and nonseparable class scenarios. They establish fast convergence rates in the nonseparable case and exponentially fast rates in the separable case a significant theoretical foundation for square losss robustness and model calibration benefits. Empirically the paper demonstrates these theoretical findings with experiments on both synthetic lowdimensional data and realimage datasets showing square losss advantages over the traditional crossentropy loss in terms of generalization error robustness and calibration. 2) Strengths:  The paper fills a significant gap in the literature by providing a comprehensive theoretical analysis of square loss in deep learning classifiers elaborating on its unique advantages.  The distinction between separable and nonseparable scenarios and the thorough derivation of fast to exponentially fast convergence rates represent robust contributions to understanding square losss theoretical underpinnings.  The empirical evaluations effectively illustrate the theoretical findings demonstrating square losss practical benefits over crossentropy loss particularly in model robustness and calibration error.  The discussion on the implications beyond the NTK regime introduces potential for broader applicability and impact on various classifier architectures. 3) Weaknesses:  While the paper addresses both theoretical and empirical aspects the empirical studies mainly focus on generalization robustness and calibration without detailed exploration of computational efficiency or scalability issues when comparing square loss with crossentropy loss.  The analysis predominantly revolves around overparameterized networks in the NTK regime. An expanded discussion on the limitations and potential applicability of these findings to underparameterized models or other learning regimes might provide a more nuanced perspective.  The potential negative societal impacts of the work are not thoroughly discussed. Considering deep learnings pervasive application a nuanced discussion on possible misuse or unintended consequences especially in sensitive areas where robustness and calibration are critical would have been valuable. 4) Questions and Suggestions for the Authors:  Could the authors extend the theoretical analysis to include more complex data structures or multiple layers providing insights into how square loss might perform under these conditions  An expansion on the computational efficiency and scalability of employing square loss versus crossentropy loss might offer valuable practical insights for largerscale applications.  It would be beneficial to include a section discussing the broader impacts of this work particularly potential negative consequences in critical applications (e.g. healthcare autonomous driving) where overconfidence or misclassification can have severe outcomes.  Could future work explore the integration of square loss with more recent deep learning innovations such as attention mechanisms or newer architectures to further evaluate its versatility and effectiveness Limitations and Potential Negative Societal Impact: The authors provide a solid foundation for understanding the theoretical advantages of square loss in classification tasks however the exploration of limitations is somewhat lacking particularly regarding the methods scalability and efficiency. Moreover the paper could benefit from a comprehensive analysis of the potential negative societal impacts. For instance while enhanced model calibration and robustness are generally positive overreliance on these models in critical decisionmaking areas without considering their limitations could lead to unintended consequences. Suggestions for improvement include a more detailed exploration of the conditions under which square loss may not offer advantages or might even underperform compared to traditional losses. Further an explicit discussion on the areas of application where caution is warranted could help mitigate potential overoptimism regarding the applicability of these findings.", "zSkYVeX7bC4": "Review of \"Exploring Length Generalization in TransformerBased Language Models\" 1) Brief Summary This paper investigates the ability of transformerbased large language models (LLMs) to generalize from solving short problem instances to longer ones\u2014a crucial aspect of outofdistribution generalization in reasoning tasks. The authors focus on two algorithmic tasks parity and variable assignment to systematically study the interaction and efficacy of three techniques on length generalization: finetuning few shot prompting (incontext learning) and the use of a scratchpad (chainofthought reasoning). The paper demonstrates that while standard finetuning methods fail to achieve length generalization regardless of model scale combining incontext learning with scratchpad prompting significantly improves performance on longer tasks. Furthermore the paper examines whether additional finetuning on models that have been fewshot prompted with scratchpad inputs could further enhance their ability to generalize over length yielding mixed results. 2) Assessment of Strengths and Weaknesses Strengths:  Originality: The paper tackles an underexplored but critical area in the generalization capabilities of LLMs. By holistically assessing the effectiveness of various approaches (finetuning incontext learning scratchpad use) it provides new insights into the mechanisms that can foster length generalization.  Methodological Rigor: The empirical studies are carefully designed and the choice of tasks (parity and variable assignment) is appropriate for dissecting length generalization capabilities without being confounded by taskspecific nuances.  Practical Relevance: Demonstrating that incontext learning combined with scratchpad prompting can significantly improve length generalization has practical implications for a wide range of applications from theorem proving to novel summarization where longer instance problems are common. Weaknesses:  Generalization Beyond Tested Tasks: While the paper provides compelling evidence of improved length generalization for the specific tasks tested it remains unclear how these findings generalize to more complex or realworld tasks.  Investigation of Underlying Mechanisms: Although the paper identifies effective strategies for length generalization it does not delve deeply into the theoretical or mechanistic reasons behind why certain methods (e.g. scratchpad prompting) are more effective than others.  Performance Benchmarks: The paper could benefit from clearer benchmarks or comparisons against traditional models known for their generalization capabilities providing a more comprehensive picture of where transformerbased LLMs stand. 3) Questions and Suggestions for the Authors  Could the authors speculate on or provide preliminary insights into why scratchpad prompting significantly enhances length generalization capabilities  It would be beneficial to explore a broader range of tasks or domains to assess the generalizability of the findings. Are there plans to extend the investigation to more complex reasoning tasks or realworld datasets  The interaction between the different techniques (finetuning incontext learning scratchpad use) is fascinating. Could more light be shed on how these interactions might change with different model architectures or sizes beyond what was tested 4) Limitations and Societal Impact The authors have made an effort to address limitations regarding the scope of tasks and generalizability of findings. However a more thorough discussion on potential negative societal impacts (e.g. misuse of enhanced reasoning capabilities by LLMs in generating manipulative narratives) is warranted. As a suggestion for improvement the paper could include a section discussing ethical considerations and measures to mitigate potential adverse effects emphasizing responsible usage and deployment of LLMs with advanced generalization capabilities. Overall this paper makes a notable contribution to understanding and improving the length generalization capabilities of transformerbased language models pointing towards promising directions for future research in AI reasoning tasks.", "ySB7IbdseGC": "Review of \"Structured Recognition and Variational Autoencoding of Neural Data via Gaussian Processes\" 1. Summary The paper introduces a novel framework for unsupervised learning that emphasizes the structured recognition of latent variable models particularly focusing on the role of Gaussian Processes and variational autoencoders (VAEs) in identifying and modeling complex highdimensional data. It presents \"Structured Recognition Variational Autoencoder (SRVAE)\" that extends amortized variational inference to incorporate structured factors in latent variables enabling the capture of complex observations through a probabilistic graphical model. The framework is tested on both synthetic data and highdimensional neural spike data from the hippocampus of freely moving rodents showcasing its ability to identify latent signals corresponding to behavioral covariates thus providing a promising tool for unsupervised analysis of neural data and beyond. 2. Assessment Strengths:  Innovation: The paper innovatively combines Gaussian processes with structured recognition principles within a variational autoencoding framework. This novel combination addresses the significant challenge of capturing the posterior correlation between latent variables (\"explaining away\") in unsupervised learning contexts.  Utility in Neuroscience: The application to neural spike data from rodents is highly relevant. The frameworks capacity to uncover latent variables that correlate with behavioral covariates without supervision is of substantial value for neuroscience research providing insights into neural coding mechanisms.  Comprehensive Evaluation: The evaluation on synthetic and real datasets demonstrates the methods superiority over competitors in both settings. The extensive comparative analysis including baselines and proposed methodologies effectively showcases the improvements offered by SRVAE. Weaknesses:  Complexity and Computational Demands: The paper while thorough presents a complex framework that might be computationally demanding especially when applied to large datasets. The authors should discuss the scalability of their approach and potential optimization strategies.  Accessibility to a Broader Audience: Due to its highly technical nature and specialized application domain the paper might be less accessible to readers unfamiliar with Gaussian processes or variational autoencoders. Simplifying explanations or providing a more intuitive understanding of the methods and their significance could broaden its impact. 3. Questions and Suggestions for the Authors  Scalability and Optimization: Can the authors provide more insights into the scalability of their method Specifically are there strategies that could be employed to optimize computational efficiency without compromising the models ability to capture complex dependencies among latent variables  Comparative Analysis Depth: While the results are compelling could the authors elaborate on the limitations and strengths of the baseline models used for comparison This depth could help articulate the unique contributions of SRVAE more clearly.  Generalizability Across Domains: The paper focuses primarily on neural spike data. How well does the SRVAE framework generalize to other types of highdimensional data or unsupervised learning tasks outside neuroscience 4. Limitations and Societal Impact The authors sufficiently address the methodological limitations particularly focusing on computational demands. However they could further comment on the potential for misinterpretation or misuse of the unsupervised learning models in sensitive applications beyond neuroscience such as surveillance or personal data analysis. Ensuring ethical use and addressing potential biases in model application would strengthen the papers consideration of societal impacts. Conclusion The paper presents a highly innovative and promising approach to unsupervised learning with significant implications for neuroscience and other fields grappling with highdimensional data. Despite its complexity and computational demands the structured recognition framework offers valuable insights into latent variable interactions. Future iterations could benefit from greater focus on scalability accessibility and ethical considerations of unsupervised modeling.", "zVglD2W0EAS": "Review of \"DrugRec: A Causal InferenceBased Drug Recommendation Model to Mitigate Bias and Coordinate Multiple Drugs\" Summary: The paper introduces DrugRec an innovative model designed for drug recommendation tasks within a healthcare context leveraging Electronic Health Records (EHRs). It addresses three critical challenges in drug recommendation: elimination of bias due to limited observational data effective utilization of historical health records for drug prescribing and coordination of multiple drugs to ensure safety specifically reducing harmful drugdrug interactions (DDIs). Utilizing a causal inference framework DrugRec deconfounds recommendation bias through frontdoor adjustment and models multivisit scenarios for better historical health condition consideration. Furthermore it formulates the DDI problem as a 2SAT problem to optimize drug coordination safely. The efficacy of DrugRec is validated on MIMICIII and MIMICIV datasets where it demonstrates superior performance over existing models in terms of recommendation accuracy and safety. Strengths and Weaknesses: Strengths: 1. The paper addresses a highly relevant and complex problem in healthcare\u2014providing safe and effective drug recommendations by considering historical health data and minimizing DDIs. 2. The innovative application of causal inference to debias the recommendation process and the novel use of 2SAT for controlling DDIs are significant theoretical contributions. 3. Comprehensive experiments and evaluation using wellknown datasets MIMICIII and MIMICIV substantiate the models effectiveness and provide a strong argument for its practical utility. Weaknesses: 1. While the abstraction to a 2SAT problem for DDIs is innovative the complexity and scalability of this approach in realworld healthcare systems with extensive drug databases and intricate interaction networks might be challenging. 2. The paper underexplores the potential implications of false positives and false negatives in drug recommendations which are crucial for patient safety. 3. There is a lack of discussion on how DrugRec integrates or interoperates with existing healthcare IT systems which is crucial for its adoption in realworld settings. Questions and Suggestions: 1. Have the authors considered the scalability of the 2SAT formulation for DDIs in settings with much larger drug databases and more complex DDI networks 2. It would be beneficial if the paper addressed the integration aspect of DrugRec with current healthcare IT infrastructures. Can the authors elaborate on potential strategies or challenges in this integration 3. A deeper exploration into the models handling of false positives and negatives in drug recommendations and its impact on patient safety would be insightful. Could the authors provide more details on this 4. Suggestion: Including a case study or qualitative analysis involving healthcare professionals could add valuable practical insights into how DrugRecs recommendations compare with current clinical practices. Limitations and Societal Impact: The authors have made an effort to address the limitations of observable information in drug recommendation tasks. However the paper could benefit from a more detailed discussion on the limitations related to the scalability of the proposed methods and the inherent risks of automated drug recommendation systems such as overreliance and potential errors. A constructive suggestion would be to propose ongoing validation or monitoring mechanisms that ensure the systems recommendations remain safe and effective as new drugs and DDIs are discovered. The societal impact of improving drug recommendations is potentially significant offering to enhance patient safety and treatment efficacy. Nevertheless a discussion on the ethical considerations such as privacy concerns related to patient data and the decisionmaking power of AI systems in healthcare could enrich the paper. It would be beneficial to suggest frameworks or guidelines to maintain transparency accountability and ethical considerations in deploying such AIdriven recommendation systems in healthcare settings.", "wZk69kjy9_d": " Review of \"Director: Learning Hierarchical Behaviors from Pixels in Sparse Reward Environments\"  Summary of the Paper The paper introduces \"Director\" a novel method for learning hierarchical behaviors in reinforcement learning (RL) tasks directly from pixels. It leverages a learned world model to plan in a latent space enabling efficient exploration and goal achievement in sparse reward environments. The method aims to mimic the human ability to decompose complex tasks into subgoals and addresses the limitations of current artificial intelligence (AI) systems that struggle with decisionmaking over long horizons. Key contributions include:  The development of Director a general and interpretable algorithm that operates within a world model trained from pixels.  The introduction of two sparse reward benchmarks demonstrating the limitations of traditional flat RL approaches.  Validation of Directors efficacy across a variety of domains including visual control Atari games and DMLab levels.  Visualization of latent goals for understanding Directors decisionmaking process.  Assessment of Strengths and Weaknesses  Strengths 1. Novelty: Director presents an innovative approach to hierarchical reinforcement learning (HRL) by planning in the latent space of a learned world model without manual specification of subtasks or goals. 2. Generality and Applicability: The method successfully generalizes across a wide variety of environments showcasing its robustness and flexibility in handling different types of challenges without domainspecific knowledge. 3. Interpretability: By decoding latent goals into images Director offers a level of transparency uncommon in many RL approaches providing insights into the hierarchical decisionmaking process. 4. Benchmark Contribution: The introduction of new sparse reward benchmarks is a valuable contribution to the field enabling more rigorous testing of HRL algorithms.  Weaknesses 1. Complexity and Resource Requirements: The implementation details suggest Director requires significant computational resources (e.g. V100 GPU and extensive training time). This might limit its accessibility and usability for researchers with fewer resources. 2. Lack of Comparative Baselines in HRL domain: While the paper compares Director with DreamerV2 and Plan2Explore it lacks a comparison with other stateoftheart HRL approaches which could provide a clearer picture of its performance and innovative aspects. 3. Limited Investigation into the ManagerWorker Dynamic: The paper could benefit from a deeper analysis of the interaction between the manager and worker components specifically how this dynamic evolves during training and affects overall performance.  Questions and Suggestions for the Authors 1. Comparative Analysis: Could the authors compare Director with other HRL methods focusing on tasks with long decision horizons Such comparison would highlight Directors unique contributions more clearly. 2. Computational Efficiency: Are there ways to make Director more computationally efficient or accessible to researchers with limited resources Potential optimizations or lighter versions of the algorithm could significantly increase its impact. 3. Extension to MultiLevel Hierarchies: Is it feasible to extend Director to support multilevel hierarchies enabling more complex task decompositions This could be an exciting direction for future work potentially amplifying its effectiveness in even more challenging environments. 4. Human Interpretability: The paper mentions that the latent goals are interpretable because they can be decoded into images. It would be valuable to include user studies or expert evaluations on how these visual representations aid in understanding the AIs decisionmaking process.  Limitations and Potential Negative Societal Impact The authors have made efforts to address limitations by offering thorough evaluations across various tasks and providing benchmark contributions for future research. However the energy and computational resource requirements of Director could pose environmental concerns and accessibility issues potentially contributing to the digital divide in AI research. As for societal impact while the paper does not explicitly discuss negative implications any powerful AI technology has the potential for misuse especially in the automation of complex tasks which could have economic and ethical ramifications such as job displacement or biased decisionmaking in sensitive applications.  Constructive Suggestions for Improvement 1. Resource Efficiency: Investigate and document strategies for reducing the computational footprint of Director making the method more sustainable and widely accessible. 2. Ethical Considerations: Engage in a discussion about ethical safeguards and responsible use principles that could guide the deployment of Director in realworld applications ensuring it contributes positively to society. In conclusion \"Director\" represents a significant advancement in hierarchical reinforcement learning with promising applications and a commendable focus on interpretability and generality. Addressing its limitations and considering broader societal impacts could further its contribution to AI research and applications.", "mux7gn3g_3": "Review for \"Revisiting MetaReinforcement Learning: MultiTask Pretraining vs. MetaRL\" 1) Briefly summarize the paper and its contributions: This paper critically evaluates the effectiveness of metareinforcement learning (metaRL) algorithms against a simpler approach of multitask pretraining followed by finetuning across diverse and challenging task environments. The study is motivated by the observation that unlike in computer vision simple approaches have not been previously proven to outperform more complex metaRL strategies in reinforcement learning scenarios. The authors rigorously test three leading metaRL algorithms (Reptile PEARL and RL2) against multitask pretraining with finetuning across three diverse visionbased RL benchmarks (Procgen RLBench and Atari) covering a wide range of tasks. Key findings include that multitask pretraining followed by finetuning achieves comparable or superior performance to metaRL approaches with the added benefits of simplicity and reduced computational cost. The paper advocates for considering multitask pretraining with finetuning as a strong baseline in future research and suggests that the metaRL community should broaden their evaluations to include a wider array of challenging tasks. 2) Strengths and Weaknesses: Strengths:  Rigor and breadth of evaluation: The study spans three benchmarks with varied tasks providing a comprehensive assessment of multitask pretraining versus metaRL.  Clarity of exposition: The paper is wellwritten and presents its methodology experiments and findings clearly.  Practical implications: Highlighting the competitiveness of a simpler less computationally intensive approach has direct implications for future research directions and algorithm selection in practical RL applications. Weaknesses:  Limited exploration of causative factors: While the paper highlights the effectiveness of multitask pretraining over metaRL it does not delve deeply into analyzing why certain approaches succeed or fail across different tasks. This could include discussions on the role of task diversity dimensionalities and intrinsic complexities.  Generalization of findings: With a focus on visionbased RL benchmarks its uncertain how these findings generalize to other forms of RL tasks such as those involving different sensory modalities or nonvisual state representations. 3) Questions and Suggestions for the Authors:  Exploring Task Diversity: Could the authors provide further insights into how task diversity within the training set impacts the performance of pretraining plus finetuning versus metaRL Are there diminishing returns as the number of tasks increases or does performance continue to improve  Failure Analysis: Were there specific tasks or conditions under which metaRL significantly outperformed multitask pretraining with finetuning Analyzing such cases could yield insights into the niches where metaRL still holds strong advantages.  Extension to Other Domains: How do the authors envision their findings applying to RL tasks outside of the visionbased domain Would the same trends hold for say tasks that heavily rely on auditory inputs or abstract state representations 4) Limitations and Societal Impact: The paper does an adequate job of discussing its own limitations particularly highlighting the scope of benchmarks as a potential limiting factor. However it could further elaborate on the implications of recommending simpler approaches over more complex metaRL algorithms in terms of broader applicability potential biases and impacts on the RL communitys research directions. Furthermore the potential negative societal impact is not addressed. While not directly applicable to all studies considering and discussing how these findings might influence the deployment of RL systems in realworld scenarios especially concerning safetycritical applications or areas with profound ethical considerations would enhance the papers depth. Overall the paper makes valuable contributions to the field by challenging prevailing assumptions and providing clear evidence to support its claims. The path forward it suggests has the potential to streamline research and application of reinforcement learning in diverse settings.", "tNXumks8yHv": "The paper presents a novel statistical framework aimed at unifying and understanding the principles behind popular dimensionality reduction (DR) methods such as tSNE LargeVis and UMAP. It introduces a probabilistic model that describes the observations in both input and latent spaces as structured by latent graphs enabling the comparison of input and latent structures directly. The main contributions are showing that existing DR methods can be seen as special cases within this framework identifying a probabilistic explanation for their challenges in preserving global structure due to statistical degeneracy and establishing a connection with PCA as a nondegenerate graph coupling model.  Strengths: 1. Theoretical Novelty and Unification: A significant theoretical contribution is the unification of various DR methods within a single probabilistic framework. This aids in a deeper understanding of these methods from a statistical viewpoint. 2. Identification of Statistical Degeneracy: The paper elucidates the issue of statistical degeneracy when using shiftinvariant kernels offering a theoretical basis for the observed difficulty these methods have in preserving largescale data structure. 3. Connection with PCA: The papers linkage of PCA with the proposed framework as a nondegenerate special case is insightful. It not only provides a strong theoretical foundation for PCA within the context of graphcoupling but also suggests directions for mitigating the limitations of current DR methods.  Weaknesses: 1. Complexity and Accessibility: The theoretical nature and the complexity of the presented framework might limit its accessibility to practitioners or readers not wellversed in probabilistic models and statistical theory. 2. Empirical Validation: While the theoretical contributions are solid the paper could benefit from more extensive empirical validation. The experimental section particularly around hierarchical graph coupling (ccPCA) provides interesting insights but feels somewhat limited in scope and diversity of datasets. 3. Practical Implementation Details: The paper lacks detail on the practical implementation of its propositions especially concerning the computational efficiency and scalability of the proposed methods. Given the complexity of the described models insights into their practical applicability would be beneficial.  Questions and Suggestions: 1. Extension to Other Kernels: How might the framework adapt to newer or less common kernels not discussed in the paper Expanding the frameworks applicability could further enhance its utility. 2. Clarification on ccPCA: The ccPCA initialization process is intriguing however its description and rationale could benefit from further elaboration. Additionally comparing ccPCA with other initialization methods across more varied datasets could underline its effectiveness more distinctly. 3. Societal Impact and Ethical Considerations: The paper briefly touches upon the potential for improved interpretability through this unifying framework. Further discussing the implications of enhanced interpretability such as in areas with significant societal impacts (e.g. healthcare finance) could enrich the paper.  Limitations and Societal Impact: The authors have made an effort to address the limitations of their work particularly around the theoretical complexity and the initial empirical validation. However further discussion on the practical implications of implementing these models such as computational demands and the necessity for specialized knowledge would be beneficial. Moreover while the paper briefly mentions the interpretability advantages a more detailed exploration of the potential negative implications such as privacy concerns in sensitive data or biases in DR visualizations could provide a more balanced view. Suggestions for improvement include a comprehensive section on ethical considerations and potential societal impacts alongside guidelines or principles for responsible use of the proposed framework.", "wYGIxXZ_sZx": "Review of \"Exploring Generalization in Stochastic Minimax Optimization: A New Metric and Comparisons\" 1. Summary: This paper tackles the underexplored area of generalization guarantees within the domain of stochastic minimax optimization. It critically evaluates the suitability of existing metrics  the primal risk and primaldual risk  in understanding the generalization of minimax problems particularly in nonconvex settings. The authors reveal the limitations of these metrics through examples where they fail to capture the generalization behavior accurately. Addressing this gap the paper introduces a new metric the primal gap designed to overcome the shortcomings of the existing measures. Substantial theoretical work is presented to establish generalization bounds for the primal gap in nonconvexconcave settings. The paper also contrasts the generalization behavior of two popular algorithms gradient descentascent (GDA) and gradient descentmax (GDMax) using the proposed metric. Additionally it addresses two open questions providing generalization bounds for primal risk and primaldual risk without relying on strong concavity assumptions. 2. Assessment:  Strengths: 1. Innovative Metric: The introduction of the primal gap as a metric for assessing generalization in minimax optimization is a significant contribution addressing a notable gap in the literature. 2. Comprehensive Theoretical Analysis: The paper rigorously derives generalization bounds for the new metric and demonstrates its superiority over existing metrics through detailed theoretical proofs and examples. 3. Practical Insights: By comparing GDA and GDMaxs generalization using the primal gap the paper offers valuable insights for practitioners in selecting appropriate algorithms for their tasks.  Weaknesses: 1. Empirical Validation: The paper primarily focuses on theoretical contributions lacking empirical experiments to validate the theoretical findings and the practical effectiveness of the primal gap metric. 2. Complexity and Accessibility: The heavy reliance on deep theoretical analysis may make the paper less accessible to practitioners or researchers outside the theoretical machine learning community. 3. Questions and Suggestions for Improvement:  Empirical Studies: Can the authors provide empirical evidence supporting the theoretical claims perhaps by conducting experiments on realworld datasets or in the context of generative adversarial networks (GANs)  Comparative Analysis: It would be beneficial to include a more indepth comparison of the primal gap metric with other potential candidate metrics that could serve similar purposes.  Scalability Discussion: How does the proposed metric scale with highdimensional data or in highly complex nonconvexconcave settings  Future Directions: The paper could benefit from a discussion on potential applications beyond GDA and GDMax exploring how the primal gap metric could influence the development of new algorithms. 4. Limitations and Societal Impact: The authors mention comparisons between algorithms and theoretical bounds but there is limited discussion on the practical implications particularly any potential negative societal impacts such as biases in algorithm performance across different application domains. A constructive suggestion would be to include a section discussing the limitations regarding the applicability of the primal gap metric across various domains and any potential biases it might introduce or overlook in evaluations. For instance investigating whether certain types of minimax problems are more sensitive to the choice of generalization metric could provide insights into more equitable algorithm design and deployment. Overall this paper makes a noteworthy theoretical advancement in stochastic minimax optimization by proposing a new metric for assessing generalization. However for a more complete evaluation of the metrics impact empirical validation and a broader discussion on limitations and societal implications would be valuable additions.", "rP9xfRSF4F": "Summary of the Paper and Its Contributions: The paper introduces a novel approach to the problem of Optimally Timed Intervention (OTI) in systems that risk critical failures. Unlike previous methods that rely on classification or regression models to predict the timing of such events this work treats OTI as a problem of minimizing the expected time until a critical event given a constraint on the probability of missing the event. This approach is innovative as it conceptualizes the problem through the lens of a hazard rate process allowing the employment of optimal stopping theory. The contributions are significant: the paper outlines a methodology to derive analytically determined intervention policies based on conditional hazard rate functions. Furthermore it adapts recurrent neural networks (RNNs) to model these processes dynamically proposing a dynamic deep recurrent survival analysis (DDRSA) architecture. The efficacy of the proposed method is demonstrated through experiments that show its superiority over popular existing intervention strategies. Assessment of Strengths: 1. Novelty: This work pioneers the framing of the OTI problem in a way that connects it with optimal stopping theory offering a fresh perspective over the heuristicdriven methods prevalent in contemporary research. 2. Analytical Rigor: The depth of mathematical formulation including proofs and corollaries ensures that the proposed intervention policies are not just empirically derived but are grounded in solid theoretical foundations. 3. Practical Application: The development of the DDRSA architecture and its demonstration on realworld datasets underscore the practical viability of the theory. This bridge between theoretical excellence and practical utility is a key strength. 4. Performance: The demonstration of the proposed methods outperforming static threshold policies across different datasets solidifies the papers contributions to the field. Weaknesses: 1. Complexity: While analytical rigor is a strength the complexity of the methodology could be a barrier to adoption especially in industrial settings where simplicity and interpretability are often preferred. 2. Generalizability: The paper\u2019s reliance on Markovian assumptions and RNNs raises questions about the applicability of the OTI policy across systems where these assumptions may not hold or where data is not amenable to RNN modeling. 3. Scalability: Given the detailed computation needed at every timestep and for several values of k the scalability of the proposed method in very large systems or in realtime scenarios remains unaddressed. Questions and Suggestions for the Authors: 1. Interpretability: Can the authors suggest ways to enhance the interpretability of the OTI policies derived through their method especially for stakeholders with limited technical knowledge 2. Generalizability: Have variations from the Markovian assumptions been explored and if so how robust are the proposed OTI policies in such scenarios 3. Scalability Concerns: It would be beneficial if the authors could discuss the computational demands of the DDRSA architecture in largescale systems and suggest potential optimizations or alternative approaches to mitigate scalability issues. Limitations and Societal Impact: The paper briefly touches upon the limitations inherent in the modeling assumptions and the practical applicability of the proposed method. However a more comprehensive discussion on the limitations especially concerning the complexity and computational demands of the method would be instructive. Furthermore the potential negative societal impacts such as the reliance on automated decisionmaking for critical interventions have not been addressed. Suggestions for improvement include:  Simplification: Exploring simplified versions of the architecture that maintain a balance between performance and interpretability.  Ethical Considerations: Adding a section discussing the ethical implications of automating critical interventions especially in scenarios where inaccuracies in prediction could lead to significant harm.  Bias and Fairness: Investigating whether the models could inadvertently learn biases from historical data leading to inequitable interventions and suggesting mitigation strategies.", "vQzDYi4dPwM": " Peer Review  1) Brief Summary This paper investigates the interpolation problem for monotone data sets using neural networks constrained to nonnegative weights referred to as monotone neural networks. The authors prove that any monotone dataset can be interpolated by a monotone network with threshold activation functions achieving a constant depth of 4 and proving a size bound of O(nd). This work advances the understanding of the expressiveness of monotone neural networks shows that depth matters significantly in their construction and identifies a superpolynomial size necessity for approximating certain monotone functions compared to general neural networks. This fills a knowledge gap in the previous understanding of how restricting networks to positive parameters affects their approximation capabilities.  2) Strengths and Weaknesses Strengths:  Significant Theoretical Contribution: The paper proves several critical results regarding the expressiveness and limitations of monotone neural networks pushing the envelope of current understanding.  Robust Mathematical Foundation: The proofs particularly for the universal approximation capability at a constant depth and the superpolynomial separation are rigorous and wellfounded.  Relevance: As monotone relationships are common in many domains like economics and healthcare the paper addresses a significant and relevant problem.  Innovative Approach: The leveraging of techniques from Boolean circuit complexity to provide lower bounds introduces an innovative approach to studying neural network properties. Weaknesses:  Limited Practical Applicability: Despite the strong theoretical contributions the paper doesnt offer much in terms of practical application. There\u2019s a gap between the theoretical findings and the usability of these results in realworld problems.  Complexity of Results: The results particularly the constructions for achieving certain bounds are quite complex. This may impede understanding and applicability by a broader audience.  Lack of Experimental Validation: Theoretical results are not complemented by empirical tests. Practical experiments showcasing the networks performance in real datasets would strengthen the papers impact.  3) Questions and Suggestions Questions:  Scalability of Constructions: How do the proposed constructions scale with very large datasets Is there a practical limit to the dataset size for which these monotone networks remain efficient  Impact of Activation Functions: Beyond threshold activations how do other common activation functions impact the expressiveness and efficiency of monotone networks Suggestions:  Empirical Evaluation: To solidify the practical relevance of this work it would be beneficial to include experiments with real and synthetic datasets comparing monotone networks to standard networks in terms of approximation accuracy and computational resources.  Extension to Other Activations: Exploring the impact of using different activation functions such as sigmoids or Leaky ReLU on the efficiency and expressiveness of monotone networks could provide valuable insights.  Consideration of Learning Dynamics: Discussing how learning algorithms like gradient descent behave when constrained to positive weights might offer practical guidance for implementing these networks.  4) Limitations and Societal Impact The authors do not address the practical applicability of their theoretical results significantly. While the paper pushes forward theoretical knowledge the disconnect between theoretical constructs and practical utility might limit the impact of the findings. Additionally theres no discussion on the potential negative societal impact including how these findings might affect fairness or bias in machine learning applications when monotonicity assumptions are imposed. Suggestions for Improvement:  Address Generalization: Discussing how these networks generalize to unseen data and comparing their performance to unconstrained networks might provide insights into their practical utility.  Consider the Societal Impact: Providing a reflection on how ensuring monotonicity might influence model fairness especially in critical applications like healthcare or finance would enrich the discussion. In summary this paper makes notable theoretical contributions to understanding monotone neural networks but could be improved by addressing its practical applicability and considering broader societal implications.", "lSfrwyww-FR": " Peer Review for \"Blackbox Attacks via Surrogate Ensemble Search (BASES)\"  1. Summary of the Paper This paper introduces BASES a novel method designed to conduct blackbox adversarial attacks with remarkable efficiency. Unlike existing methods that either fall under transferbased or querybased attacks BASES integrates the strengths of both approaches while significantly reducing the reliance on a large number of queries to achieve high success rates. The proposed method leverages a perturbation machine (PM) that operates over a set of surrogate models using a weighted loss function to generate perturbed images meant to fool a victim model. By employing a bilevel optimization strategy that tweaks the weights within the loss function based on minimal feedback from the victim model BASES achieves outstanding performance in both the computational efficiency and effectiveness of the attacks across a variety of image classifiers and even extends its applicability to object detection models.  2. Assessment of Strengths and Weaknesses Strengths:  The paper targets a highly relevant and challenging issue in the field of machine learning security presenting a significant advancement in the efficiency of conducting adversarial attacks.  The proposed BASES method is remarkably efficient requiring fewer queries than contemporary methods to achieve a high success rate. This efficiency makes it practical for realworld applications where query options are limited.  BASES demonstrates versatility and generalizability across multiple models and tasks including challenging realworld applications like attacking Google Cloud Vision API.  The paper provides a welldetailed experimental setup and comparison with stateoftheart methods showcasing the superiority of BASES in a clear and convincing manner. Weaknesses:  The paper does not extensively discuss the potential defenses against such efficient adversarial attacks which could be crucial for understanding the broader implications of their method.  While the paper briefly mentions the societal impacts and acknowledges the dualuse nature of adversarial attacks it could benefit from a deeper analysis of ethical considerations and countermeasures.  The computational cost analysis focuses on query efficiency but does not adequately address the total computational resources required including those necessary for training and maintaining the surrogate ensemble.  3. Questions and Suggestions for Authors  Question: Could the authors elaborate on the potential defensive mechanisms that could mitigate the risks posed by such efficient adversarial attacks  Suggestion: It would be beneficial to include a more comprehensive discussion on the ethical implications and potential safeguards against the misuse of this powerful adversarial attack methodology.  Suggestion: Considering the computational costs beyond query efficiency such as the resources needed for setting up and maintaining the surrogate ensemble could provide a more rounded analysis of the methods practicality.  Limitations and Societal Impact While the authors have outlined the limitations related to the need for a diverse ensemble and the computational expense associated with the perturbation generation a broader discourse on the ethical ramifications and counteractive measures would strengthen the paper. In terms of societal impact acknowledging the potential misuse of such efficient adversarial techniques is essential. However delineating more concrete steps towards developing defensive strategies and promoting responsible use could enhance the papers contribution to building robust and secure ML systems.  Constructive Suggestions for Improvement  Expanding the discussion on defense mechanisms could not only balance the narrative around adversarial attacks but also contribute valuable insights to the ongoing research on model robustness.  A dedicated section exploring ethical considerations potential misuse and suggesting guidelines for ethical research in adversarial machine learning would be highly beneficial.  Providing a more detailed analysis of the total computational costs including the resources required for managing surrogate ensembles would offer a clearer picture of the methods practical applicability and efficiency.", "lTKXh991Ayv": "Review of \"Investigating the Vulnerability of Spatiotemporal Traffic Forecasting Models under Adversarial Attacks\" 1. Summary: The paper presents an indepth exploration of the vulnerabilities present in machinelearned spatiotemporal traffic forecasting models when subjected to adversarial attacks. It introduces a novel adversarial spatiotemporal attack framework designed to exploit these vulnerabilities by selectively targeting timedependent victim nodes through an iterative gradientguided method and generating realvalued adversarial traffic states within a specified perturbation constraint. The framework is modelagnostic supporting various attack scenarios such as whitebox greybox and blackbox attacks. The paper also provides a theoretical analysis establishing the worstcase performance bounds of the proposed attack methods. It validates the frameworks effectiveness through comprehensive experiments conducted on two realworld traffic datasets demonstrating significant performance degradation in vulnerable forecasting models. Additionally it shows that the resilience of these models can be enhanced through adversarial training incorporated with the proposed attacks. 2. Assessment: Strengths:  The paper addresses a critical yet underexplored aspect of spatiotemporal traffic forecasting models\u2014 their vulnerability to adversarial attacks. Identifying and mitigating such vulnerabilities is paramount for the reliability of Intelligent Transportation Systems (ITS).  The methodology is grounded in a welljustified rationale for selecting victim nodes and generating adversarial traffic states distinguishing it from bruteforce or random attacks that lack precision and scalability.  The inclusion of theoretical bounds for the adversarial loss adds a valuable analytical dimension to the paper providing insights into the factors influencing the attacks success and guiding the design of more robust forecasting models.  Extensive experiments with realworld datasets confirm the practical relevance and applicability of the proposed attack framework offering a solid empirical foundation for the claims made. Weaknesses:  While the paper does include an evaluation of adversarial training as a defensive measure it could benefit from a more comprehensive exploration of defensive strategies. Given the potential implications of adversarial attacks on ITS a deeper analysis of how to protect against such attacks is warranted.  The societal impacts of adversarial attacks on traffic forecasting models and by extension on ITS are not sufficiently explored. While the technical aspects of the attacks are welladdressed a discussion on potential negative consequences (e.g. traffic congestion accidents) and ethical considerations would provide a more holistic view.  The papers methodology primarily focuses on a citywide scale for attacks. However the implications and effectiveness of localized attacks targeting specific areas within a city which could have different consequences are not discussed. 3. Questions and Suggestions for the Authors:  Could you explore additional defensive mechanisms beyond adversarial training to enhance the robustness of spatiotemporal traffic forecasting models For instance incorporating anomaly detection to identify unexpected shifts in traffic data patterns could be a viable line of defense.  A discussion on the ethical considerations of conducting such adversarial attacks even in a controlled experimental setting would be beneficial. How do you ensure that the knowledge gained from this research is used responsibly to improve ITS rather than exploit its vulnerabilities  It would be enlightening to see an analysis of the implications of localized adversarial attacks within specific urban areas or during hightraffic events. Understanding the varied impacts could guide more targeted defenses and forecasting model adjustments. 4. Limitations: The authors have made commendable efforts in addressing the limitations and potentially harmful applications of their work. However an explicit discussion on the ethical implications and potential misuse of their findings in realworld scenarios is lacking. The paper would benefit from recommendations on how to responsibly manage and disseminate knowledge of such vulnerabilities to prevent misuse. Furthermore exploring a broader range of defensive strategies would significantly strengthen the work ensuring it contributes not only to identifying vulnerabilities but also to fortifying the systems against them.", "oMhmv3hLOF2": " Peer Review  Summary of the Paper This paper presents an innovative approach for streaming radiance field reconstruction for dynamic scenes aimed at novel view synthesis. The authors develop a method that utilizes explicit grid representations and an incremental learning paradigm significantly increasing training efficiency and reducing storage overhead compared to existing methods. This approach dubbed StreamRF is underpinned by a narrowband tuning strategy for incremental model updating and a model differencebased compression to mitigate storage costs. The empirical results showcased in the paper demonstrate a remarkable 1000\u00d7 speedup in training compared to stateoftheart implicit methods along with substantial improvements in storage efficiency and competitive rendering quality.  Assessment of Strengths and Weaknesses Strengths: 1. Enhanced Efficiency: The proposed approach significantly improves the training speed without compromising on the rendering quality which is a notable breakthrough in dynamic scene rendering. 2. Storage Optimization: The model differencebased compression technique substantially reduces the storage requirements making the method more practical for realworld applications. 3. Incremental Learning Paradigm: The paper innovatively adopts an incremental approach to model dynamic scenes allowing for faster adaptation to new frames and efficient handling of streaming video data. 4. Comprehensive Evaluation: The experimental setup involving challenging video sequences and comparison with several baseline methods thoroughly validates the effectiveness of the proposed method. Weaknesses: 1. Method Complexity: The multicomponent nature of the method (explicit grid training narrowband tuning differencebased compression pilot model guidance) might introduce implementation complexity and potential integration challenges. 2. Generalization and Scalability: It is unclear how well the method generalizes to diverse dynamic scenes beyond the tested datasets or scales to longer video sequences without a loss in performance or efficiency gains. 3. Potential for Artifacts: Despite the reported improvements in rendering quality there is a concern about the potential for artifacts especially in handling highfrequency details or transparent objects as briefly mentioned in the limitations.  Questions and Suggestions for the Authors 1. Generalization Capability: Could the authors elaborate on the methods ability to generalize to a wider variety of dynamic scenes including outdoor environments or scenes with more complex dynamic elements 2. Handling Long Sequences: While the method achieves impressive speedups and storage efficiency is there an impact on memory consumption and potential performance degradation when processing exceptionally long video sequences 3. Artifact Mitigation: Given the mention of potential artifacts in the rendering images are there strategies or future research directions the authors consider exploring to further mitigate or address these concerns  Limitations and Constructive Suggestions The authors briefly touch upon the limitations related to the loss of highfrequency details and the handling of transparenttranslucent objects. While these acknowledgments are appreciated a more indepth discussion on these limitations and their implications for realworld deployments would enhance the paper. Additionally potential negative societal impacts such as privacy concerns or the misuse of synthetic visual content are not addressed. Constructive suggestions include:  Exploring hybrid explicitimplicit models to better capture highfrequency details and handle transparency more effectively.  Incorporating adaptive mechanisms that can tailor the processing based on scene complexity or specific features potentially improving the handling of long sequences and reducing artifacts.  Providing guidelines or ethical considerations for the use of this technology mitigating potential negative societal impacts. In summary the paper introduces a significant advancement in dynamic scene modeling with practical implications for VRAR applications. By addressing the highlighted concerns and suggestions the paper could further solidify its contributions and broad applicability.", "uzqUp0GjKDu": " Peer Review of \"Unsupervised Learning under Latent Label Shift (LLS)\"  1) Briefly summarize the paper and its contributions This paper introduces a novel method for unsupervised learning under the framework it terms as Latent Label Shift (LLS) which focuses on leveraging unlabeled data across multiple domains where only the label distributions vary while classconditional distributions remain invariant. The authors establish an isomorphism with topic modeling for finite input spaces and extend these concepts to continuous data by introducing a practical algorithm DiscriminateDiscretizeFactorizeAdjust (DDFA) which utilizes domaindiscriminative models to improve upon unsupervised classification methods. The theoretical underpinnings are substantiated through semisynthetic experiments demonstrating superior performance over leading unsupervised algorithms in identifying underlying classes especially in scenarios where traditional methods based on featurespace similarity falter.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Novel Approach: The paper presents a unique unsupervised learning framework leveraging distribution shifts across domains to uncover latent classes a significant departure from conventional methods based on similarity in feature space.  Theoretical Foundation: The theoretical analysis establishing isomorphism with topic modeling and proving identifiability in both discrete and continuous settings adds rigor and depth to the work.  Practical Algorithm: The DDFA algorithm is wellconceived blending domain discrimination with clustering and factorization techniques making the theoretical insights actionable in practice.  Empirical Validation: Extensive semisynthetic experiments underscore the methods capability to outperform existing unsupervised learning approaches significantly particularly highlighting its effectiveness in challenging scenarios misaligned with traditional assumptions of featurespace clustering. Weaknesses:  Complexity and Scalability: The DDFA framework involves multiple stages including domain discrimination data discretization and nonnegative matrix factorization which may translate to considerable computational complexity. The scalability of this approach to very large datasets or very highdimensional data remains unclear.  Generalization of Results: The experiments while extensive are semisynthetic and focus on image datasets. Realworld datasets across different domains and modalities might present unforeseen challenges not captured in this evaluation.  Parameter Sensitivity: The performance of the DDFA algorithm likely depends on the choice of hyperparameters (e.g. the number of clusters in the discretization stage) but the paper does not extensively discuss the sensitivity to these parameters or provide guidelines for their selection.  3) Please list up and carefully describe any questions and suggestions for the authors Questions:  How does the performance of the DDFA algorithm scale with the dimensionality of the data and the size of the dataset Are there practical limits to the applicability of this technique  Could the authors provide more insights into the selection of hyperparameters for the DDFA algorithm especially regarding the choice of the number of clusters and the impact of this choice on algorithm performance Suggestions:  Extended Evaluation: To bolster the generalizability of the findings it would be beneficial to test the DDFA algorithm on a broader range of realworld datasets including nonimage data to explore its applicability and limitations across different domains and modalities.  Parameter Selection Guidelines: Providing more comprehensive guidelines or a strategy for selecting hyperparameters possibly supported by additional experiments could greatly aid practitioners in applying this method to new problems.  Comparison with Latest Methods: Including comparisons with the newest unsupervised learning methods especially those that have been developed or popularized subsequent to the considered baselines could provide a clearer picture of the proposed methods standing in the current research landscape.  Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work If not please include constructive suggestions for improvement. The paper does not explicitly address potential limitations in scalability and generalizability nor does it discuss the sensitivity of the approach to hyperparameter choices. Moreover there is no discussion of the potential negative societal impacts such as the misidentification of classes leading to biased or erroneous conclusions in sensitive applications. The authors could enhance the paper by including a discussion of these issues offering guidelines for mitigating risks and ensuring responsible use of the methodology in various applications.", "wSVEd3Ta42m": "Review of \"Extending Distributional Reinforcement Learning for CVaR Optimization\" 1) Summary of the paper and its contributions This paper targets the challenge of learning a risksensitive policy that optimizes the Conditional ValueatRisk (CVaR) using distributional reinforcement learning (RL). The authors identify a significant issue in the existing application of distributional RL for CVaR optimization where the standard actionselection strategy based on the distributional Bellman optimality operator does not converge to the optimal CVaR policy. They propose a novel distributional Bellman operator and a corresponding actionselection strategy that enables convergence to an optimal policy tailored for the CVaR objective. They validate the approach on both synthetic and realworld data demonstrating that their method outperforms existing strategies in learning CVaRoptimized policies. The research builds upon and extends the current knowledge within the risksensitive reinforcement learning domain leveraging recent advances in deep RL and distributional RL to address a fundamental challenge in optimizing for CVaR. 2) Assessment of the strengths and weaknesses Strengths:  Theoretical Contribution: The paper addresses a critical gap in the literature providing a rigorous theoretical analysis of why existing strategies fail in the context of CVaR optimization and proposing a theoretically sound alternative solution.  Practical Applicability: By demonstrating efficacy on both synthetic and realworld datasets including a complex application in option trading the paper showcases the practical relevance and versatility of the proposed method.  Simplicity and Integration: The proposed approach is described as a simple extension to standard distributional RL algorithms which makes it accessible and potentially easy to integrate into existing frameworks. Weaknesses:  Convergence Analysis: While the paper provides theoretical justification for the proposed modifications it lacks a comprehensive theoretical analysis regarding the convergence properties of the new distributional Bellman operator in more general or complex settings.  Empirical Evaluation Scope: The experiments though insightful focus on specific domains (synthetic data and option trading). Additional domains particularly with different characteristics or higher complexity could further validate the approachs generalizability.  Exploration of Limitations: The discussion on limitations particularly on potential challenges in scaling to very highdimensional state spaces or dealing with extremely rare catastrophic events is somewhat limited. 3) Questions and Suggestions for the Authors  Convergence in Complex Domains: Can the authors provide more insight or theoretical analysis on how the proposed approach behaves in terms of convergence and stability in more complex domains or with highly nonstationary environments  Comparison with Risksensitive Policy Gradient Methods: The paper primarily compares the proposed method against other distributional RL approaches. Including a comparison with risksensitive policy gradient methods could provide a more comprehensive understanding of its relative performance.  Scalability and Efficiency: Could the authors discuss potential improvements or modifications to enhance the scalability and efficiency of the proposed approach especially in very highdimensional or continuous action spaces 4) Limitations and Societal Impact The authors have briefly touched upon the limitations of their approach primarily focusing on the empirical domains selected for validation. A more thorough discussion on the potential limitations regarding scalability adaptability to highly dynamic environments and sensitivity to hyperparameter choices would be beneficial. Additionally the paper does not delve into the potential negative societal impacts of their work. Given the application domains such as financial trading it is crucial to discuss the ethical considerations and potential for misuse such as exacerbating market volatility or unfair advantages in highfrequency trading. Suggestions for improvement include expanding the scope of empirical evaluations and incorporating mechanisms to ensure the responsible use of the technology in sensitive applications.", "slKVqAflN5": "The paper under review presents a significant advancement in the optimization of Lipschitz functions through two main contributions. Firstly it introduces a method that circumvents the limitations posed by the assumptions of directional differentiability and the need for a nonstandard subgradient oracle model as seen in the work of Zhang Lin Jegelka Sra and Jadbabaie [1]. By incorporating a minor stochastic perturbation in each iteration step the authors successfully adapt the algorithm to function efficiently with any Lipschitz function capable of evaluating values and gradients at points of differentiability. This key modification not only broadens the applicability of the method but also simplifies the previously complex proof techniques. Secondly the paper proposes a cuttingedge algorithm that significantly enhances the efficiency of minimizing Lipschitz functions in lowdimensional spaces achieving a rate of O(d\u03b5\u22123) for Lipschitz functions and O(d\u03b5\u22122) for weakly convex functions. Strengths: 1. Generality of Method: The first contribution of eliminating restrictive assumptions on the functions being minimized is a substantial improvement. This makes the algorithm versatile and applicable to a wide range of problems in modern machine learning where Lipschitz continuity is common but directional differentiability might not be. 2. Improved Efficiency in Low Dimensions: The second contribution wherein the authors have achieved better efficiency in low dimensions is particularly noteworthy. This could have significant practical implications for problems inherent in control theory and other fields where the number of variables is small. 3. Simplification of Proof Techniques: Simplifying the proof techniques besides being an intellectual advancement makes the method more accessible and understandable potentially leading to wider adoption and further improvements by the larger research community. Weaknesses: 1. Stochastic Nature and its Implications: The introduction of randomness to overcome the need for a nonstandard subgradient oracle introduces stochasticity into the model. This aspect might have implications on the consistency and predictability of the algorithms performance in practical applications. 2. Extension to Stochastic Settings Unclear: The authors acknowledge that their improvements and modifications do not extend readily to stochastic settings. This limitation could restrict the application of the improved algorithm in situations where stochasticity in data or models is inherent or preferable. 3. Potential for Societal Impact not Discussed: While the technical advancements are clear the paper does not delve into the broader societal impacts or ethical considerations of the developed algorithms. Given the wide applicability of these optimization methods including sectors with significant societal implications (e.g. AI control systems) a discussion on potential negative impacts would have been beneficial. Questions and Suggestions for the Authors: 1. Handling Stochasticity: Could the authors elaborate on potential strategies to mitigate any adverse effects stemming from the stochastic nature of the proposed modification Are there specific scenarios where this might pose significant challenges 2. Extension to Stochastic Models: Given the stated limitation of extending this work to stochastic settings do the authors have preliminary thoughts on how the current methodology could be adapted or evolved to cater to such scenarios 3. Societal Impact Considerations: Could the authors comment on any societal domains they foresee being significantly impacted by these advancements in optimization techniques Are there considerations or safeguards that researchers should keep in mind when applying these methods in sensitive areas Limitations and Societal Impact: The omission of a discussion on the potential negative societal impacts is a notable limitation of this paper. In contemporary research especially in fields closely tied to machine learning and optimization understanding and preemptively addressing possible adverse outcomes is crucial. The authors are encouraged to elaborate on the range of applications they envision for their algorithm and to consider both the direct and indirect consequences of its deployment in various domains particularly those with profound societal implications such as surveillance decisionmaking in justice systems and automated control in public infrastructure. Constructively the paper could benefit from a dedicated section discussing ethical considerations possible misuses biases or fairness concerns associated with the application of their optimization methods. Further including guidelines or recommendations for researchers and practitioners on responsible usage could enhance the papers contribution to the field responsibly.", "tq_J_MqB3UB": " Peer Review for \"Diverse Weight Averaging for Improved Generalization in OutofDistribution Tasks\" 1) Summary: The paper introduces Diverse Weight Averaging (DiWA) a novel weight averaging strategy aimed at enhancing outofdistribution (OOD) generalization for neural networks in computer vision tasks. DiWA focuses on increasing model diversity by averaging weights from models trained independently under different runs with each possessing slight variances in hyperparameters and training procedures. This strategy is grounded in a detailed theoretical analysis which includes a new decomposition of the expected error under WA into bias variance covariance and locality terms showcasing the importance of diversity under OOD conditions. Empirically DiWA demonstrates superior performance on the DomainBed benchmark without incurring inference overheads presenting a substantial advancement in the field. 2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Foundation: The paper provides a robust theoretical basis for DiWA showcasing how it outperforms traditional weight averaging methods under OOD conditions. The biasvariancecovariancelocality decomposition is particularly insightful offering a clear explanation for DiWAs effectiveness.  Empirical Validation: The experimental results are compelling especially the consistent improvement over existing stateoftheart methods across various realworld datasets. The fact that DiWA achieves this without additional inference cost is remarkable.  Practicality and Generality: DiWAs approach is practically implementable and can be broadly applied to neural networks designed for computer vision increasing its potential impact. Weaknesses:  Assumption Limitations: While the theoretical model provides a strong foundation it relies on certain assumptions (e.g. shared initialization and mild hyperparameter search spaces) that may limit its applicability in scenarios where these conditions cannot be met.  Exploration of Diversity Sources: The paper focuses on diversity from hyperparameter variance and independent training runs but does not extensively explore or discuss other potential sources of diversity (e.g. architectural or task diversity within the network).  Analysis on Correlation Shift: The paper acknowledges that DiWA does not address correlation shifts effectively. Although this limitation is recognized potential approaches or modifications to DiWA that could mitigate this issue are not thoroughly explored. 3) Questions and Suggestions for the Authors:  Expanding Diversity Sources: Could the authors discuss or speculate on the impact of incorporating additional sources of diversity beyond hyperparameters and independent runs For instance would architectural diversity or varying task objectives further boost DiWAs effectiveness  Addressing Correlation Shifts: Given the limitation with correlation shifts could there be potential extensions to DiWA or additional strategies that might be combined with DiWA to tackle this challenge  Robustness to Hyperparameter Choices: The paper shows that DiWA performs well under \"mild\" hyperparameter ranges. Could the authors provide more insights on how DiWAs performance might vary with broader or more aggressive hyperparameter tuning  Comparison with Other Ensemble Techniques: While DiWA is compared with traditional weight averaging and SharpnessAware Minimization how does it stack against more complex ensemble strategies such as boosting or bagging in the context of OOD generalization 4) Limitations and Potential Negative Societal Impact: The authors address the limitations regarding DiWAs performance under correlation shifts and its dependency on certain assumptions for its theoretical model. However there is little discussion on the potential negative societal impacts. While DiWA primarily aims at improving model performance in benign contexts increasing the complexity and opacity of model averaging techniques could exacerbate issues around model interpretability and trust especially in critical applications like healthcare or autonomous systems. It would be constructive for the authors to acknowledge these challenges and suggest paths for responsible adoption including transparency measures and interpretability enhancements.", "nQcc_muJyFB": " Review of \"Improving Feature Distillation with Projector Ensemble\"  Summary This paper explores the underinvestigated area of the projectors role in feature distillation within knowledge distillation frameworks. The authors argue that while previous works have focused on designing loss functions and selecting layers for feature distillation the impact of the feature projector between the student and the teacher networks has not been adequately addressed. They propose a novel method that uses an ensemble of projectors for feature distillation aiming to improve the student networks feature learning ability. The core idea is that adding a projector helps disentangle the learning tasks of feature extraction for classification and feature matching for distillation thereby reducing the student networks tendency to overfit the teachers feature distributions. The proposed method is simple and straightforward to implement and the experimental results validated on different datasets and teacherstudent pairs consistently show its superiority over stateoftheart methods in feature distillation.  Strengths 1. Novelty: This work addresses an overlooked aspect of knowledge distillation by focusing on the role of the projector in feature distillation. The idea of using a projector ensemble is innovative and contributes new insights into improving knowledge distillation performance. 2. Empirical Validation: The authors provide comprehensive experimental results that demonstrate the effectiveness and generalization capability of their proposed method across various datasets and network architectures. 3. Clarity and Reproducibility: The paper is wellwritten with clear explanations of the methodology and experimental setup. The provision of code enhances its reproducibility and accessibility for further research.  Weaknesses 1. Limited Theoretical Insight: While the empirical evidence supports the effectiveness of the proposed method the paper lacks a deeper theoretical analysis of why and how the projector ensemble leads to improved performance in knowledge distillation. 2. Evaluation Scope: The evaluation focuses on image classification tasks. Expanding the scope to include other tasks such as object detection or semantic segmentation could further validate the methods applicability and robustness. 3. Comparison with Recent Methods: Though the paper compares its method with several baselines and stateoftheart approaches it might benefit from including or discussing very recent related works to ensure the proposed methods competitive edge is accurately assessed.  Questions and Suggestions for the Authors 1. Theoretical Analysis: Can the authors provide more theoretical insight or hypotheses on why employing a projector ensemble can significantly improve feature distillation processes possibly including a discussion on the disentanglement of learning tasks 2. Extension to Other Tasks: Have the authors considered applying their proposed method to tasks beyond image classification If so can preliminary results or future directions be provided 3. Impact of Projector Ensemble Size: How does the size or number of projectors in the ensemble affect the distillation performance Is there an optimal range or diminishing returns beyond a certain point  Limitations and Societal Impact The authors have not extensively discussed the potential limitations and negative societal impacts of their work. The approach seems primarily applicable to deep learning models in computer vision tasks limited by computational resources due to the introduction of multiple projectors. This could imply increased energy consumption and carbon footprint during training a critical consideration in the current climate of sustainable AI practices. Constructive suggestions for improvement include addressing the methods scalability to tasks with constrained computational resources and discussing the environmental impact associated with increased computational demands. Exploring efficient projector ensemble configurations that balance performance improvements with computational efficiency could be a valuable future direction. Overall this paper presents a valuable contribution to knowledge distillation research by highlighting the significance of the projectors role in feature distillation. Its insights and findings offer promising directions for enhancing the efficiency and effectiveness of knowledge transfer between teacherstudent networks.", "rhdfTOiXBng": "Review of \"NATURALPROVER: Grounded Proof Generation via Language Modeling\" 1) Summary: The paper introduces NATURALPROVER a system aimed at generating mathematical proofs in natural mathematical language leveraging largescale language models. This approach focuses on two novel tasks: suggesting the next step in a mathematical proof and fully generating a proof grounded on acquired background knowledge from references such as theorems and definitions. The authors develop a methodology that includes incontext reference constraints and constrained decoding to enforce the correct application of these references within proofs. Their evaluation based on human judgment by universitylevel mathematics students demonstrates NATURALPROVERs ability to outperform a finetuned GPT3 model in proof suggestion quality and correct proof generation marking a step forward in assisting mathematics learning and reasoning with AI. 2) Strengths and Weaknesses: Strengths:  Novelty: The paper tackles an underexplored area of generating proofs in natural mathematical language using neural language models contributing new tasks a novel methodology and a dataset for further exploration.  Methodology: The approach of grounding proofs in reference theorems and definitions combined with constrained decoding to enforce reference usage is both innovative and effective showing significant improvements over baselines.  Empirical Evaluation: The thorough human evaluation by mathematics students provides strong evidence of the models capability to generate useful and correct proofs demonstrating practical value in educational contexts. Weaknesses:  Model Limitations: Despite improvements the model struggles with longer proofs and logical coherence limiting its applicability for more complex theorem proving.  Reference Dependency: The current success is partly dependent on provided references which may not always be available or easily determined in realworld scenarios.  Potential Negative Societal Impact: The paper does not discuss potential negative societal impacts such as reliance on AI for proof generation potentially devaluing human mathematical skills or the misuse of the system to generate incorrect proofs intentionally. 3) Questions and Suggestions for the Authors:  On Limitations: Can the authors discuss strategies for improving logical coherence in longer proofs and reducing dependency on provided references  Dataset and Benchmarking: Could the authors expand on how the NATURALPROOFS benchmark varies from existing ones in mathematical theorem proving and detail the criteria for theorem selection  Societal Impact: Could the paper include a discussion on potential negative societal impacts and mitigation strategies especially considering the educational context  Interactivity and Feedback Loops: The paper mentions a future direction involving humanintheloop configurations. Could the authors speculate on how feedback from proof correction by users might be incorporated into NATURALPROVER\u2019s training regime 4) Limitations and Potential Negative Societal Impact: The paper acknowledges limitations around the models performance on longer proofs and the need for provided references suggesting room for improvement in generalization and independence from userprovided inputs. Additionally theres a missed opportunity in discussing the potential negative societal impacts. For instance undue reliance on AI for learning mathematical proofs could impact the development of critical problemsolving skills. Furthermore incorrect or misleading proofs generated unintentionally due to model errors could lead to misunderstandings if used unverified in educational materials. Improvement Suggestions:  The authors could explore techniques for implicit knowledge integration reducing explicit dependency on provided references and making the system more robust in varied contexts.  An indepth analysis of error patterns in longer proofs might yield insights into targeted improvements for logical coherence and multistep reasoning.  The inclusion of a discussion on ethical considerations and potential safeguards against misuse could enrich the paper making it a comprehensive study with societal impact considerations.", "wsnMW0c_Au": "The paper in question investigates the algorithmic equivalence between nonconvex gradient descent (GD) and convex mirror descent (MD) a link that is built upon the foundation of reparameterization. This consideration aligns with the broader pursuit of understanding the mechanisms behind the success of gradient descent on nonconvex problems especially those encountered in deep learning. The authors extend previous work from continuous to discrete time proving a regret bound of \\(O(T23)\\) for nonconvex online gradient descent under specific geometric and smoothness conditions. This result answers an open question posed by Amid and Warmuth [4] and paves the way for exploring the generalized equivalences between these two fundamental optimization approaches. Besides establishing this theoretical underpinning the paper potentially influences nonconvex optimization strategies by suggesting a new method to analyze and approach these problems. Strengths: 1. Theoretical Contribution: The paper tackles an open problem and provides a solid theoretical contribution by proving an \\(O(T23)\\) regret bound for nonconvex online gradient descent under certain conditions. This finding contributes significantly to the understanding of algorithmic behavior in nonconvex settings. 2. Novelty in Approach: Employing the concept of reparameterization to establish equivalence between nonconvex gradient descent and convex mirror descent is both innovative and insightful. It opens up novel avenues for exploring optimization algorithms\u2019 properties and their potential interrelations. 3. Bridging Theoretical Gaps: This work viably bridges the existing gap between continuous and discretetime analyses contributing to a more comprehensive understanding of optimization algorithms across different settings. Weaknesses: 1. Assumption Limitations: The assumptions made (e.g. specific geometric and smoothness conditions) might limit the generalizability of the results. It would be beneficial if the authors discussed the extent to which these conditions are necessary or could be relaxed. 2. Practical Implications Unclear: While the theoretical findings are robust the paper could benefit from a clearer exposition of the practical implications. For instance discussing how these findings could influence the design of new algorithms or the optimization of deep learning models would be valuable. 3. Complexity and Accessibility: The papers high technicality may make it less accessible to a broader audience. A section summarizing the findings in laymans terms or providing more intuitive examples could enhance its accessibility. Questions and Suggestions for the Authors: 1. On Assumptions: Could you elaborate on whether the geometric and smoothness conditions are strictly necessary for your findings Are there scenarios where the regret bound holds under less stringent conditions 2. Practical Impact: How do you envision your findings impacting the design or analysis of optimization algorithms in practical machine learning or deep learning settings 3. Extensions and Future Directions: Are there potential extensions of your work that could address the limitations in generalizability or applicability to broader nonconvex optimization problems Limitations and Societal Impact: The authors have made commendable efforts in addressing the limitations associated with their assumptions. However a more detailed discussion on whether these limitations could potentially be overcome in future work would benefit the reader. Regarding societal impact while direct implications may not be evident understanding optimization algorithms behavior in nonconvex scenarios is pivotal in advancing machine learning and AI technologies. As such the authors could contemplate and discuss the broader societal implications particularly concerning the ethical use and possible misuse of improved optimization strategies in sensitive applications. Constructive suggestions for future research directions might include exploring the applicability of these theoretical findings in realworld datasets or scenarios potentially leading to more efficient and robust machine learning models.", "s7SukMH7ie9": "The paper investigates the relatively unexplored area of adversarial training (AT) with complementary labels (CLs) presenting it as a unique and practical scenario of adversarial learning under imperfect supervision. In standard settings AT aims to defend against imperceptible adversarial perturbations by enhancing model robustness through minmax optimization. Extending this concept the authors identify and address the novel challenge of employing AT in the presence of CLs\u2014labels indicating classes that samples do not belong to rather than their true classes. Their investigation reveals that straightforward application of AT in such settings results in failure due to intractable adversarial optimization and generation of lowquality adversarial examples. To tackle these issues the authors introduce a novel training strategy leveraging gradually informative attacks composed of two main components: Warmup Attack and PseudoLabel Attack. This approach is validated through extensive experiments across multiple benchmark datasets demonstrating its effectiveness compared to conventional methods. Strengths: 1. Novelty and Practicality: The exploration of AT under complementary labels introduces a significant extension to the traditional adversarial learning paradigm addressing practical constraints of data labeling. 2. Theoretical and Empirical Analysis: The paper provides a solid theoretical foundation for its approach backed by exhaustive empirical evidence. This dual analysis deepens the understanding of ATs behavior in nonstandard settings. 3. Effectiveness of Proposed Method: Through welldesigned experiments the authors clearly demonstrate the superiority of their approach over existing methods in enhancing model robustness against adversarial examples in the context of CLs. 4. Reproducibility: By making their code publicly available the authors facilitate further research and verification of their results enhancing the papers impact and utility. Weaknesses: 1. Generalization Across Different Data Domains: While the paper presents results across several datasets theres limited discussion on the methods applicability to various realworld scenarios particularly where CLs distribution might not follow the uniform assumption or in highly imbalanced datasets. 2. Analysis of Component Contributions: While the ablation study highlights the importance of Warmup and PseudoLabel Attacks deeper insights into the individual contributions and limitations of these components could enhance understanding and guide future refinements. 3. Consideration of Societal Impacts: The discussion on potential negative societal impacts especially considering imperfect supervision could propagate biases or errors is somewhat limited. An expanded analysis in this area could better prepare practitioners for responsible application. Questions and Suggestions for Authors: 1. Impact of NonUniform CL Distributions: Have the authors considered scenarios where CLs may not be uniformly distributed and if so how might this affect the proposed methods effectiveness 2. Model Transferability: Can the authors provide insight into how well models trained under their proposed method transfer across different tasks or domains particularly to those not covered in the experiments 3. Societal Impact Considerations: Could the authors expand on potential negative societal impacts particularly in relation to biases that imperfect supervision might introduce and suggest mitigation strategies Limitations and Improvements: While the authors make significant contributions the paper could be further strengthened by addressing the aforementioned weaknesses. Particularly expanding the discussion on the methods generalizability to nonuniform CL distributions and imbalanced datasets providing deeper analysis into the individual contributions of the proposed methods components and offering a more thorough examination of potential societal impacts would make the work even more comprehensive. Additionally suggestions for future research directions such as exploring alternative forms of imperfect supervision or integrating the proposed method with other adversarial defense mechanisms could provide valuable paths forward.", "kOIaB1hzaLe": " Peer Review of \"A Generalized Framework for LikelihoodtoEvidence Ratio Estimation in SimulationBased Inference\"  1) Summary of the Paper The paper presents a novel framework termed NREC for likelihoodtoevidence ratio estimation in simulationbased inference (SBI). The authors address the limitations of existing binary (NREA) and multiclass (NREB) classification approaches by proposing a multiclass framework that avoids the intrinsic bias present in NREB. The proposed method NREC is shown to recover both NREA and NREB under specific conditions offering a more reliable diagnostic tool for practitioners. The framework is evaluated across various training regimes and a bound on mutual information is suggested as a performance metric for SBI methods. The experimental analysis demonstrates the superiority of NREC over existing models in terms of diagnostics and performance.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper addresses a significant gap in SBI literature by proposing a novel framework that overcomes the bias issue of NREB providing a more reliable tool for practitioners.  Comprehensive Evaluation: The framework is thoroughly evaluated across different training regimes and the introduction of a mutual information bound as a performance metric is a notable advancement.  Practical Implications: The improvement in diagnostics reliability and recommendation for hyperparameters have direct practical implications potentially enhancing the performance of SBI in various scientific domains. Weaknesses:  Model Complexity: The increased complexity with the introduction of additional classes and the management of a gamma parameter might pose challenges in terms of computational resources and finetuning for optimal performance.  Generalization Concerns: While the paper presents an extensive evaluation the generalization of findings across a wider range of simulators and realworld scenarios remains to be fully addressed.  Societal Impact Discussion: Although the broader impact section is included a more detailed discussion on potential ethical implications misapplications or biases in model deployment across different domains would have been beneficial.  3) Questions and Suggestions for the Authors  Model Interpretability: Can the authors comment on the interpretability of NREC especially in comparison to NREA and NREB and possible impact on practitioners\u2019 ability to draw insights from diagnostics  Scalability: Given the complexity and additional parameters in NREC what strategies do the authors suggest for scaling the framework to handle larger datasets and more complex simulation models  Realworld Application: It would be enriching to see case studies or examples of NREC applied to realworld problems. How would the framework handle scenarios with highly imbalanced or noisy datasets  Generalization Study: Could the authors consider a more diverse set of simulation models including those from different scientific domains to validate the generalization ability of NREC  4) Limitations and Potential Negative Societal Impact The authors have made an effort to address limitations by thoroughly evaluating the model across different scenarios and mentioning the broader impact. However the discussion on limitations could be expanded to include potential issues related to the computational efficiency and scalability of the proposed framework. Furthermore the societal impact section could be more detailed reflecting on how biases in the simulation model or data could propagate through the framework and affect the inference results. Constructively the paper could benefit from a section on mitigation strategies for identified limitations and potential misuse ensuring that the frameworks application remains ethical and scientifically sound. Overall this paper makes a notable contribution to the field of SBI by introducing a promising framework that alleviates significant limitations of existing methods. With refinement and further exploration of suggested areas it has the potential to set a new standard for likelihoodtoevidence ratio estimation.", "vmjckXzRXmh": " Peer Review  1) Brief Summary of the Paper The paper investigates the phenomenon of linear transferability in contrastive representations within an unsupervised domain adaptation setting. It constructs a theoretical framework to demonstrate when and why contrastive learning representations can accurately predict classes across different domains (e.g. from photos to sketches) with only a linear classifier trained on the source domain. The key contribution lies in proving that linear transferability occurs when intraclass acrossdomain relationships are stronger than interclass acrossdomain relationships. The authors propose a variant of the linear probe termed Preconditioned Feature Averaging (PFA) which directly calculates class representations in the source domain and applies a preconditioner matrix for better adaptation to the target domain. Empirical validations on domain adaptation tasks support the theory showing that PFA outperforms standard linear probing techniques.  2) Assessment Strengths:  Theoretical Contribution: The paper fills a significant gap by providing theoretical insights into why contrastive learning can effectively handle domain adaptation tasks through the novel concept of linear transferability. This contributes to a deeper understanding of contrastive learning\u2019s potential beyond simple cluster formation.  Realistic Assumptions: Unlike previous works that rely on bounded density ratios between source and target domains or Gaussian distributions of data this paper operates under more realistic and weaker assumptions. This enhances the applicability of their findings to realworld scenarios.  Empirical Performance: The proposed PFA algorithm demonstrates superior domain adaptation performance on benchmark datasets confirming the practical value of the theoretical propositions. Weaknesses:  Limited Scope of Empirical Evaluation: While the paper provides empirical evidence to support its claims the evaluations are restricted to specific datasets and a direct comparison with a broader range of unsupervised domain adaptation methods is lacking. This limits the understanding of where PFA stands in relation to the stateoftheart.  Overemphasis on Linear Classifiers: The focus on linear classifiers might constrain the exploration of the full potential of contrastive representations. Nonlinear classifiers could further harness the expressiveness of these representations and an investigation into this aspect could provide a more comprehensive view.  3) Questions and Suggestions for the Authors  Broader Empirical Evaluation: Can the authors extend their empirical evaluation to include more diverse datasets and a comparison against a wider array of stateoftheart unsupervised domain adaptation methods  Investigation into Nonlinear Classifiers: Given the emphasis on linear classifiers exploring how nonlinear classifiers perform with contrastive representations could be an intriguing direction. Are there theoretical guarantees or empirical results that could shed light on this aspect  Impact of Preconditioner Matrix: The introduction of the preconditioner matrix is a novel aspect of the proposed algorithm. However the paper could benefit from a deeper analysis of its impact particularly in terms of how different choices of the matrix affect adaptation performance.  4) Limitations and Societal Impact The authors have addressed limitations to a notable extent particularly by acknowledging the empirical evaluations scope. However a discussion on the potential negative societal impacts is missing. For instance unsupervised domain adaptation could be used in surveillance systems raising privacy concerns. Suggestions for improvement include expanding the discussion sections to cover these aspects ensuring readers are aware of the broader implications of deploying such technologies.  Conclusion This paper makes significant contributions to understanding and improving contrastive learning for domain adaptation supported by both theoretical analyses and empirical validation. While it offers valuable insights further exploration in terms of empirical breadth and theoretical exploration with nonlinear models could enrich the findings.", "rApvGord7j": " Review of \"Optimizing Fairness in Machine Learning with Predictive Parity: Theory and Algorithm\"  Summary The paper under review presents a comprehensive study on the application of predictive parity as a fairness measure in machine learning. The authors start by highlighting the importance of considering fairness in machine learning algorithms pointing out the potential biases that may arise during the training process due to historical inequalities or underrepresentation of vulnerable groups. They emphasize the rapidly growing interest in designing algorithms that meet fairness constraints with minimal accuracy loss. The focus of this study is on predictive parity a sufficiencybased fairness measure that ensures equal probabilities of positive outcomes for different protected groups given a positive prediction. The paper makes several contributions including proving when fair Bayesoptimal classifiers under predictive parity are groupwise thresholding rules (GWTRs) exploring the conditions under which fair Bayesoptimal classifiers may lead to withingroup unfairness and proposing the FairBayesDPP algorithm as an effective way to achieve predictive parity while maintaining accuracy.  Assessment Strengths 1. Novelty and Relevance: The consideration of predictive parity fills a gap in the fairness in machine learning literature addressing a less studied but critically important aspect of fairness measures. 2. Theoretical Contributions: The establishment of conditions under which GWTRs can achieve predictive parity is a significant theoretical contribution expanding the understanding of fair Bayesoptimal classifiers. 3. Practical Impact: The development of the FairBayesDPP algorithm offers a practical tool for implementing fairness in machine learning models especially in highstakes applications. 4. Experimental Validation: The inclusion of experiments on both synthetic and empirical data provides a solid foundation for the methodology proposed demonstrating the effectiveness of FairBayesDPP in achieving predictive parity without substantial loss in accuracy.  Weaknesses 1. Comparison with Existing Fairness Measures: While the paper provides a strong case for predictive parity it lacks a detailed comparison with other fairness measures particularly in practical scenarios. This comparison could highlight the unique advantages or potential tradeoffs of predictive parity. 2. Generalizability of the FairBayesDPP Algorithm: The paper focuses on binary protected attributes and binary outcomes. Additional investigation is required to understand the algorithm\u2019s performance in more complex scenarios with multiclass outcomes or protected attributes. 3. Discussion on Societal Impact: While the authors briefly mention the potential negative societal impacts of unfair machine learning algorithms a more thorough exploration of how predictive parity specifically interacts with societal impacts would strengthen the paper.  Questions and Suggestions for the Authors 1. Comparative Analysis: Could the authors provide more insight into how predictive parity compares with other fairness measures such as demographic parity or equality of opportunity in terms of both theoretical properties and practical outcomes 2. Multiclass Extension: Is there potential for extending the FairBayesDPP algorithm to handle multiclass protected attributes or outcomes If so what modifications might be necessary 3. Societal Impact Exploration: It would be beneficial if the authors could elaborate on the potential negative societal impacts specific to the enforcement of predictive parity compared to other fairness measures. Are there scenarios where predictive parity may inadvertently perpetuate bias  Limitations and Suggestions The paper provides a comprehensive analysis of predictive parity in fair machine learning but could benefit from a deeper exploration of its comparison to other fairness criteria and its societal impacts. Addressing these limitations could provide a clearer understanding of the tradeoffs involved in selecting fairness measures and contribute to more informed decisionmaking in the application of fairness in machine learning.  Improvement Suggestion: Including case studies or expanded experiments contrasting predictive parity with other fairness measures on the same datasets could provide valuable insights into practical applications benefits and drawbacks.  Addressing Societal Impacts: A dedicated section discussing potential negative impacts safeguards and ethical considerations specific to predictive parity would be beneficial. This could include an exploration of scenarios where predictive parity might not be the optimal fairness measure due to specific domain requirements or societal contexts.", "ofRmFwBvvXh": " Review of \"Approximation Error Bounds and Classification with Deep Convolutional Neural Networks\"  1. Summary This paper presents a significant advancement in understanding the approximation capabilities of deep convolutional neural networks (CNNs) for Sobolevregular functions. The authors derive novel explicit nonasymptotic approximation error bounds that depend flexibly on the network depth and filter lengths for functions within Sobolev spaces including those embeddable into H\u00f6lder spaces. These bounds exhibit polynomial dependence on the ambient dimension improving upon the exponential dependence seen in prior studies. Furthermore an approximation result is established for functions supported on approximate lowerdimensional manifolds highlighting CNNs ability to mitigate the curse of dimensionality in such scenarios. Leveraging these results the paper sets forth nonasymptotic excess risk bounds for binary classification problems using CNNs with a series of convex surrogate losses evidencing how these methods can avoid the curse of dimensionality under certain data distributions.  2. Assessment Strengths:  Novelty and Theoretical Contribution: The derivation of nonasymptotic error bounds with explicit prefactors based on network parameters for Sobolevregular functions using CNNs is highly novel. The incorporation of functions embedded in H\u00f6lder spaces and supported on lowerdimensional manifolds further enriches this theoretical contribution.  Practical Implications: By applying the theoretical findings to classification using CNNs with various convex surrogate losses the paper provides a solid theoretical foundation that could explain CNNs effectiveness in realworld applications particularly in highdimensional space.  Broad Relevance: The comparison of CNNs approximation capabilities with existing works and the detailed discussion on related literature in approximation theory and classification methods enrich the papers relevance to both the machine learning theoretical community and practitioners. Weaknesses:  Binary Focus: The papers focus on binary classification while insightful leaves open questions regarding its implications for multiclass classification and regression scenarios which are common in practical applications.  Assumption Specificity: While the assumption of data being supported on an approximate lowerdimensional manifold is realistic it may not capture the full plethora of structures present in realworld datasets such as spatial invariance in image data.  Scalability and Complexity: The paper lacks discussion on the computational complexity and scalability of the proposed methods an important consideration for their practical deployment especially for very large datasets and network architectures.  3. Questions and Suggestions  Extension to Multiclass Classification and Regression: Could the authors provide insights or preliminary results on how their approximation results could be extended to multiclass classification and regression This could significantly broaden the papers impact.  Complexity Analysis: An analysis of the computational complexity and potential scalability challenges of applying the derived bounds in practice would be beneficial. Are there any techniques or considerations that could be suggested to mitigate computational demands  Further Discussion on RealWorld Data Structures: While the assumption regarding data support on lowerdimensional manifolds is a step forward incorporating discussions or considerations for other data structures (e.g. spatial invariance) could provide a more comprehensive framework for understanding CNNs success.  4. Limitations The authors have presented a thoughtful discussion on limitations particularly highlighting the binary focus of the application of their results. However expanding on additional limitations such as the practical computational aspects of implementing these bounds in realworld scenarios or addressing the diversity of data structures encountered in practice would enrich the paper. Moreover a discussion on the potential negative societal impacts especially considering biases that might be inherent in realworld datasets should be explored to ensure the responsible advancement of machine learning technologies.  Suggestions for Improvement 1. Expand Scope: Consider discussing or providing preliminary insights into extending the theoretical results to multiclass classification or other learning tasks. 2. Complexity and Scalability: Include a section discussing the computational complexity of implementing the derived bounds and potential strategies for dealing with largescale problems. 3. RealWorld Application: Provide examples or case studies demonstrating the application of the bounds in practical settings potentially addressing various data structures beyond lowdimensional manifolds. 4. Societal Impact: A careful analysis of the potential negative societal impacts including algorithmic biases that might be reinforced by the proposed methods would be beneficial.", "yewD_qbYifc": "The paper introduces a novel reinforcement learning (RL) approach Priority Convention Reinforcement Learning (PCRL) specifically designed for multiagent systems encountering largediscreteactionspace (LDAS) challenges. These challenges arise due to the combinatorial explosion of potential actions as the number of agents increases making traditional RL strategies inefficient. The key innovation of PCRL is the incorporation of agreedupon priority conventions that allow the system to break action selection symmetries and reduce the decision space without sacrificing optimality. These conventions include positionbased agent priority and prescribed action priority enabling a sequential microstep generation of action vectors that guarantee the consideration of the lexicographically smallest optimal action vector. The effectiveness of PCRL is demonstrated through experiments with a multiagent complete coverage planning application and a cooperative pong game showing improved performance over jointaction RL methods and heuristic approaches in terms of LDAS handling and optimality finding. Strengths: 1. Novelty: The introduction of priority conventions is a unique and effective strategy to tackle the LDAS problem in multiagent systems. It provides a direct approach that contrasts with the indirect methods prevalent in existing research which often compromise on solution quality. 2. Theoretical Basis: The paper provides solid theoretical underpinnings for the PCRL algorithm including proofs on optimality preservation and space complexity reduction. These foundations give credibility to the proposed method and its capacity to address LDAS challenges effectively. 3. Experimental Validation: The extensive experiments conducted provide strong empirical evidence of the PCRLs capabilities. The comparison with other RL methods and heuristic algorithms across different application scenarios clearly demonstrates its superior performance and broad applicability. Weaknesses: 1. Scalability Concerns: While the paper shows the effectiveness of PCRL in certain multiagent scenarios there is limited discussion on the scalability of the approach particularly in terms of computational requirements and the impact of increasing the number of agents beyond the experimented scales. 2. Generalizability: The focus on priority conventions as the solution might not generalize well to all types of multiagent environments especially those where agent priorities cannot be easily determined or are highly dynamic. 3. Implementation Details: The paper could benefit from more detailed explanations about the implementation of priority conventions within the PCRL framework particularly how these conventions are encoded and updated during learning. Questions and Suggestions for the Authors: 1. Scalability and Efficiency: Could the authors elaborate on the computational efficiency of PCRL when scaling to systems with a significantly larger number of agents Are there strategies in place to maintain or improve efficiency under such conditions 2. Adaptability to Dynamic Environments: How adaptable is PCRL to environments where agent priorities are not static or easily defined Would the method still be effective in scenarios where agent roles and priorities change over time 3. Comparative Analysis with Recent Approaches: Given the rapid advancements in RL and multiagent systems a comparison with more recent stateoftheart methods could further solidify the claims of superiority. Limitations and Societal Impact: The authors have made an effort to address the limitations of their work particularly in relation to scalability and generalizability. However a more thorough discussion on the potential computational limits and the methods to address them would be beneficial. Regarding societal impact while the paper claims neutrality a more detailed examination of potential negative implications especially in sensitive applications (e.g. military or surveillance) is recommended. Suggestions for mitigating any unintended consequences should also be provided.", "utahaTbcHdP": " Review of \"Neural Collapse under ClassImbalance: Exploring the Geometry of Deep Learning Models Beyond Zero Training Error\"  Summary This paper investigates a phenomenon known as Neural Collapse (NC) which occurs in the geometry of class embeddings and classifier weights in deep neural networks trained beyond zero training error. Traditionally observed with balanced datasets the authors seek to understand if this phenomenon can be extended to classimbalanced scenarios. They employ the unconstrainedfeatures model (UFM) and introduce a novel concept SimplexEncodedLabels Interpolation (SELI) to characterize neural collapse invariantly amidst class imbalances. The paper theoretically proves that embeddings and classifiers under class imbalances still follow a simplexencoded label matrix with their geometries determined by this matrixs singular value decomposition (SVD) factors. Furthermore extensive experiments on synthetic and real datasets validate the theory acknowledging that convergence to the SELI geometry deteriorates as class imbalance intensifies. The authors also highlight the pivotal role of ridge regularization in adjusting the geometry under imbalances opening new directions for research into firstorder method convergence rates and their preferred solutions under class imbalances.  Strengths 1. Theoretical Contribution: The extension of Neural Collapse to classimbalanced scenarios with the introduction of SELI adds significant theoretical depth to the understanding of deep learning geometry. The rigorous mathematical proofs provide a solid foundation for the proposed theories. 2. Experimental Validation: The mixture of synthetic and realworld datasets for experiments ensures that the theoretical findings are not just mathematically accurate but also practically relevant. The diverse experiments strengthen the papers claims about SELIs efficacy in capturing the neural collapse phenomenon in imbalanced settings. 3. Novel Characterization of Imbalances: By focusing on class imbalances a commonly encountered realworld problem the paper addresses an important gap in the literature. It provides insights into how imbalances affect the geometric structures learned by deep learning models which is crucial for designing better performing and more robust models.  Weaknesses 1. Convergence Rate Concerns: The paper notes that convergence to the SELI geometry worsens with increasing class imbalances. However it lacks a detailed analysis or potential solutions to mitigate this issue. Further exploration into how to maintain or improve convergence rates in highly imbalanced contexts would be beneficial. 2. Limited Exploration of Ridge Regularization: While the paper identifies ridge regularization as crucial in classimbalanced settings it offers limited insights into how different regularization strengths might impact model geometry and performance. More comprehensive studies on this aspect could provide useful guidelines for practitioners. 3. Potential Overfitting Risks: The focus on training models beyond zero training error and achieving a specific geometric configuration could increase the risk of overfitting especially in realworld imbalanced datasets. The paper does not discuss strategies to safeguard against overfitting while pursuing the SELI geometry.  Questions and Suggestions for the Authors 1. Handling Increased Imbalances: Can the authors suggest any specific strategies or adjustments to improve convergence to the SELI geometry as class imbalances become more severe Would incorporating additional regularization forms or leveraging data augmentation techniques help 2. Impact of Ridge Regularization: It would be valuable to have a more comprehensive analysis of how varying levels of ridge regularization impact the SELI geometry and overall model performance. Could the authors provide empirical or theoretical insights on optimal regularization settings for different degrees of imbalance 3. Balancing SELI Convergence and Overfitting: As models are pushed to achieve the SELI geometry there might be concerns regarding overfitting especially with realworld imbalanced datasets. Could the authors comment on best practices or safeguards to ensure models remain generalizable while striving for the SELI geometry  Limitations and Societal Impact The authors acknowledge the negative impact of deteriorating convergence with class imbalances but do not provide explicit solutions. Exploring mechanisms to combat this could enhance the papers practical applicability. Moreover a discussion on the potential societal impacts such as bias amplification in imbalanced datasets would add depth emphasizing the need for responsible model training and deployment practices. Constructive suggestions include more focus on regularization strategies and an exploration of alternative loss functions that might mitigate some identified issues enhancing the overall robustness and fairness of deep learning models in imbalanced settings.", "m16lH6XJsbb": "Peer Review for \"Spectrum Random Masking (SRM) for Generalization in ImageBased Reinforcement Learning\" 1) Brief Summary This paper introduces a novel approach to tackling the issue of generalization in imagebased reinforcement learning (RL) by shifting the focus from spatialbased data augmentation to frequencybased augmentation specifically through Spectrum Random Masking (SRM). The authors propose SRM as a method that randomly masks certain frequency bands of input images during training with the aim of forcing the RL agent to learn a more generalized and robust policy that performs well across various unseen environmental conditions. Extensive experiments on the DMControl Generalization Benchmark demonstrate that SRM significantly enhances the generalization capabilities of RL models outperforming stateoftheart methods. 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper\u2019s focus on frequency domain rather than spatial modifications is a significant departure from the norm in data augmentation for RL providing a fresh perspective on tackling generalization.  Comprehensive Evaluation: The authors conduct extensive experiments across different environments showcasing the broad applicability and effectiveness of SRM in enhancing model generalization.  PlugandPlay Method: SRM\u2019s design as a taskagnostic plugandplay augmentation method enhances its appeal as it can be easily integrated with existing RL models without requiring additional parameters or environmentspecific adjustments. Weaknesses:  Lack of Theoretical Insight: The paper provides limited theoretical explanation or intuition behind why frequency domain masking improves generalization which could strengthen the understanding and future development of similar strategies.  Potentially Limited Scope: The focus on imagebased observations might restrict the applicability of SRM as many RL tasks utilize state vectors or other forms of nonimage data.  Evaluation on More Complex Tasks Missing: While the DMControl Generalization Benchmark is a standard testing ground evaluation on more complex realworld tasks could further validate SRM\u2019s effectiveness in diverse applications. 3) Questions and Suggestions for the Authors  Theoretical Foundation: Could the authors provide more theoretical insights or hypotheses on why manipulating the frequency components of input images can lead to better generalization in RL models  Application to NonImage Observations: Is there a possibility to extend the concept of SRM to nonimagebased RL tasks possibly through analogous frequency domain techniques for state vectors  Evaluation on More Complex Environments: It would be valuable to see how SRM performs in more complex or realworld scenarios beyond the controlled settings of the DMControl suite. Could future work include such evaluations 4) Limitations and Societal Impact The authors have made an effort to address the limitations of their work primarily focusing on the methods applicability to imagebased observations. While the study acknowledges challenges in extending SRM to tasks with nonimage inputs constructive suggestions on overcoming these hurdles are somewhat lacking. Future work could explore analogous methods in the frequency domain for different types of data or hybrid approaches that combine spatial and frequencybased augmentations for comprehensive generalization improvement. Regarding potential negative societal impacts the paper does not delve deeply into this aspect. Its vital to consider how improved generalization in RL models especially in imagebased tasks could affect areas such as surveillance autonomous vehicles or other applications with significant ethical and privacy implications. A discussion on safeguarding against misuse and ensuring responsible application of such technologies would be a valuable addition. Conclusion \"Spectrum Random Masking (SRM) for Generalization in ImageBased Reinforcement Learning\" presents a promising direction for enhancing the robustness and generalization of RL models. By innovatively applying frequency domain techniques the paper breaks new ground in data augmentation strategies for RL. However further theoretical insights broader applications and considerations of ethical implications would strengthen the work.", "kgT6D7Z4Xv9": " Review of \"On the Role of Path Independence in Equilibrium Models for Upwards Generalization\"  Summary This paper investigates the phenomenon of upwards generalization\u2014specifically the ability of neural network architectures to generalize to more difficult problem instances than those encountered during training through the application of scalable testtime compute. The focus is on equilibrium models a class of architectures that determine outputs as fixed points of learned dynamical systems. The core thesis centers on the concept of path independence described as the tendency of a system to converge to the same steadystate behavior regardless of its initial conditions given sufficient computational resources. The authors introduce a metric for measuring path independence the Asymptotic Alignment (AA) score and demonstrate through various tasks that high AA scores correlate with strong upwards generalization capabilities. Furthermore they show that interventions designed to enhance path independence (e.g. through random initialization or dynamic compute budgets during training) improve model generalization on more challenging instances. Conversely penalizing path independence tends to degrade this ability.  Assessment Strengths: 1. Novelty: The paper introduces an innovative perspective on addressing upwards generalization through path independence in equilibrium models. This connection between path independence and generalization is both novel and compelling. 2. Methodology: The design of the Asymptotic Alignment score as a metric for path independence is robust with a thorough validation method that includes adversarial stress testing. Moreover the experimental setup covering a range of tasks and interventions is wellconstructed and comprehensive. 3. Clarity: The paper is wellstructured and clearly communicates complex ideas. The introduction of new concepts such as the AA score and the detailed explanation of experimental interventions are particularly wellarticulated. Weaknesses: 1. Generalization beyond Equilibrium Models: While the paper provides valuable insights into equilibrium models it remains somewhat unclear how broadly applicable these conclusions might be to other neural network architectures especially those not inherently designed for fixedpoint computation. 2. Potential for Bias in Metrics: The AA score while innovative is primarily validated within the scope of the presented experiments. Its effectiveness as a universal metric for path independence and as a predictor of upwards generalization across varied contexts and datasets could benefit from further validation. 3. Discussion on Practical Implications: The paper could enhance its impact by providing more detailed discussions on the practical implications of implementing path independence in realworld scenarios. This would help in understanding the operational complexity and potential computational costs versus performance benefits.  Questions and Suggestions 1. Broader Applicability: Can the authors elaborate on the potential applicability of the path independence principle to other types of neural networks particularly those not based on equilibrium models 2. Metric Universality: It would be beneficial to see further validation of the AA score across a broader range of datasets and potentially in conjunction with other metrics to assess path independence and its correlation with upwards generalization. 3. Practical Implementation Guidance: Additional insights into the practical implementation of the proposed interventions (e.g. randomized initialization and depth) in a broader range of applications would be valuable. Specifically guidelines on balancing computational costs with performance gains would aid in broader adoption.  Limitations and Improvement Suggestions The paper discusses the limitations related to the specificity of its findings to equilibrium models and acknowledges the potential for further validation of the AA score. However it could benefit from a deeper discussion on the scalability of implementing path independencepromoting interventions in practice particularly in terms of computational costs and the feasibility of such approaches in resourceconstrained environments. An exploration of potential negative societal impacts especially in contexts where computational resources are limited would also strengthen the paper. Suggestions for improvement include:  Further Empirical Validation: Conducting additional experiments across a wider array of architectures and tasks to explore the universality of the findings.  CostBenefit Analysis: Providing a detailed analysis of the computational costs versus performance benefits of implementing interventions designed to enhance path independence.  Societal Impact Consideration: Including a section that critically examines the potential broader societal impacts particularly focusing on equity in access to computational resources required for implementing such models.", "xpR25Tsem9C": " Review of \"HHVAEM: Hierarchical Variational Autoencoders for MixedType Incomplete Data with Hamiltonian Monte Carlo\"  Summary The paper introduces HHVAEM a novel deep hierarchical variational autoencoder (VAE) specifically designed to handle mixedtype and incomplete data efficiently using Hamiltonian Monte Carlo (HMC) for sampling. The HHVAEM model significantly improves upon the limitations of existing VAE approaches by incorporating multiple layers of latent variables and leveraging a Hamiltonian Monte Carlo method with automatic hyperparameter tuning. This methodology enables lower bias sampling from the true posterior distribution enhancing the models ability to impute missing data perform supervised learning with missing data and conduct informed feature acquisition. The paper presents a convincing set of experiments demonstrating HHVAEMs superior performance over existing baselines in tasks of missing data imputation supervised learning with missing features and active information acquisition.  Assessment Strengths: 1. Innovative Approach: The integration of HMC with a hierarchical VAE structure addresses key limitations in the field providing a significant methodological advancement. 2. Comprehensive Evaluation: The authors present a broad suite of experiments encompassing missing data imputation supervised learning and information acquisition tasks effectively showcasing the models capabilities across a range of scenarios. 3. Demonstration of Superiority: Through rigorous comparison with existing baselines HHVAEM is convincingly shown to outperform alternatives making a strong case for its efficacy. Weaknesses: 1. Computational Complexity: The paper acknowledges increased computational cost but does not provide a detailed analysis or comparison of this aspect with baseline methods. Given the iterative nature of HMC and the hierarchical model structure the computational requirements could be a significant concern for practical application on large datasets or in resourceconstrained environments. 2. Generalization to Different Data Types: While the model is evaluated on mixedtype data the specific challenges and performance on extreme cases of data heterogeneity (e.g. highly imbalanced or irregular data types) are not explored in depth. 3. Hyperparameter Sensitivity: The automatic tuning of HMC hyperparameters is a crucial feature however the sensitivity and robustness of the model to these parameters are not thoroughly investigated.  Questions and Suggestions for the Authors 1. Model Interpretability: Given the complex hierarchical structure and the Monte Carlo underpinnings could the authors elaborate on the interpretability of the model Are there mechanisms or visualizations implemented to understand the decisionmaking process especially concerning feature acquisition decisions 2. Extension to Larger Datasets: Could the authors discuss how HHVAEM might be scaled to handle very large datasets Are there specific optimizations or parallel processing techniques that could mitigate the computational overhead 3. Robustness to Hyperparameter Changes: It would be beneficial to see an analysis of the models performance sensitivity to the tuning of HMC hyperparameters. Additional insights into how the automatic tuning is achieved and its stability across different datasets would be valuable. 4. Further Exploration into Data Types: The paper provides a strong foundation but further investigation into extremely heterogeneous datasets or those with nonstandard data types could highlight additional strengths or limitations of the model.  Limitations  Societal Impact The paper briefly mentions the computational cost but could benefit from a more comprehensive discussion on limitations such as scalability to extremely large datasets and the potential for model interpretability. Additionally the societal impact especially concerning privacy concerns with incomplete data should be addressed. Could the techniques applied inadvertently reveal sensitive information through imputed data or feature acquisition strategies Suggestions for improvement include a detailed analysis of computational efficiency exploration of privacypreserving mechanisms and clearer communication on these aspects to ensure the responsible use of HHVAEM in sensitive applications.", "wZEfHUM5ri": " Peer Review on \"Learning 3D Geometry via CrossView Image Completion\"  1) Summary of the Paper and Its Contributions The paper introduces a novel selfsupervised pretraining method named Crossview Completion (CroCo) aimed at learning 3D geometry from unlabeled image pairs depicting the same scene from different viewpoints. Unlike conventional Masked Image Modeling (MIM) approaches that operate on a single image and focus on highlevel semantic tasks (e.g. image classification) CroCo leverages a pair of images for masked content prediction significantly improving performance on 3D vision and geometric downstream tasks such as depth estimation and optical flow. The proposed method involves encoding nonoverlapping patches from both images using a sharedweight Vision Transformer (ViT) then reconstructing the masked patches of the first image by considering visible patches and the second image facilitated through a transformerbased decoder using crossattention layers. The method is evaluated on a synthetic dataset derived from indoor scenes showing promising results in monocular 3D vision tasks and competitive performance on binocular tasks without taskspecific architecture modifications.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Relevance: The proposition of the crossview completion task is innovative and timely addressing the need for better 3D geometric understanding in vision models.  Comprehensive Evaluation: The paper presents extensive experiments demonstrating the superiority of CroCo over existing methods on a variety of 3D vision tasks underscoring its effectiveness and applicability.  Generic Architecture: Utilizing a generic architecture without taskspecific designs or sophisticated data augmentations for binocular tasks is a significant advantage for broader applications. Weaknesses:  Dataset Limitation: The pretraining exclusively on synthetic indoor scenes might limit the models generalizability to realworld scenarios and outdoor environments. Exploring extensions to more diverse datasets could provide insights into the models adaptability.  Highlevel Semantic Tasks: As noted CroCo falls short on highlevel semantic tasks compared to traditional MIM models which could be a drawback for applications requiring robust feature extraction across both geometric and semantic tasks.  Computational Efficiency: The impact of the proposed method on computational resources and training efficiency especially considering the use of transformer decoders and crossattention is not thoroughly discussed. Insights into computational costs compared to benefits would be valuable.  3) Questions and Suggestions for the Authors  Generalizability to Realworld Data: Have the authors considered evaluating the model on realworld image pairs perhaps through datasets obtained via StructurefromMotion or georeferencing techniques This could validate the models applicability beyond synthetic data.  Extension to Outdoor Scenes: It would be interesting to see how CroCo performs on outdoor scenes potentially offering insights into the models versatility and limitations in handling varied geometries and scales.  Combining with Semantic Tasks: Exploring how CroCo could be integrated with or complemented by training on semanticrich datasets like ImageNet might help overcome its limitations on highlevel semantic tasks. Any thoughts or preliminary experiments in this direction would be insightful.  Decoder Design Alternatives: The choice between CrossBlock and CatBlock decoders seems to hinge on a balance between model size and computational cost. Could the authors elaborate on how these considerations impact the decision in practical applications  4) Limitations and Societal Impact The paper touches briefly on the limitations concerning dataset choice and performance on highlevel semantic tasks. However a deeper discussion on the computational efficiency and scalability of the proposed method would strengthen the paper. Exploring the potential negative societal impacts especially considering biases that may arise from training on synthetic indoor scenes and the models performance in realworld applications would provide a more rounded view. Suggestions for improvement include:  Extending the evaluation to realworld datasets and outdoor scenes to prove the models generalizability.  Investigating ways to integrate CroCo with existing methods optimized for highlevel semantic tasks.  A thorough analysis of the computational resources required for training and inference offering pathways to efficiency improvements. Overall the paper presents a significant advancement in selfsupervised learning for 3D vision tasks opening new avenues for research while also highlighting areas for further investigation to fully harness its potential.", "ue4gP8ZKiWb": "1) Brief Summary: The paper introduces a Prompt Certified Machine Unlearning (PCMU) algorithm that significantly optimizes the process of machine unlearning by executing training and unlearning operations simultaneously in advance. This method primarily hinges on the amalgamation of randomized data smoothing and gradient quantization to provide efficient removal of data from trained models without extensive recomputation. By theoretically establishing the certified radius R for data changes and a certified budget for data removals the authors propose a robust framework for managing data deletion requests. This framework ensures that the unlearning models parameters and gradients remain unaffected within a predefined certified budget. Extensive experiments conducted on various datasets including FashionMNIST CIFAR10 and SVHN underscore PCMUs superiority over existing machine unlearning techniques both in terms of accuracy and efficiency in handling data removal requests. 2) Strengths and Weaknesses: Strengths:  The innovative approach to simultaneous training and unlearning significantly reduces computational costs associated with traditional methods which is a notable advantage when dealing with largescale data.  The application of randomized smoothing and gradient quantization for machine unlearning is novel and effectively addresses the inefficiencies of sequential unlearning requests.  Extensive empirical evaluation offers solid proof of the method\u2019s effectiveness over several benchmarks indicating its practical utility. Weaknesses:  The reliance on randomized smoothing and gradient quantization could introduce challenges in scenarios where data distributions are highly irregular or when the model requires extremely high levels of precision.  The theoretical framework while robust is complex and might present difficulties in understanding and implementation for practitioners not deeply versed in the underlying mathematics.  The empirical evaluation while comprehensive is somewhat narrow in scope focusing primarily on image classification tasks. Additional tests in varied contexts such as text or sequential data could further solidify the methods general applicability. 3) Questions and Suggestions:  Could the authors elaborate on how the PCMU algorithm performs in cases where data distributions are highly skewed or nonnormal Are there modifications to the approach that could accommodate such scenarios  A suggestion would be to explore the application of PCMU in contexts beyond image classification such as natural language processing (NLP) or timeseries analysis to evaluate its versatility.  How does the PCMU method scales in terms of computational resources as the size of the dataset increases It would be beneficial to include a scalability analysis comparing PCMU with baseline methods over increasingly large datasets.  The paper could benefit from a more detailed discussion on the tradeoffs between the certified budget of data removals and model performance. How sensitive is the model performance to changes in the certified radius and budget 4) Limitations and Potential Negative Societal Impact: The authors have provided a comprehensive theoretical basis for the PCMU method but may not have fully addressed potential limitations related to data diversity and distribution anomalies. Moreover the societal implications including privacy concerns and the potential for misuse in erasing data traces unethically are not discussed. Constructive suggestions for improvement include:  Providing a deeper analysis of the methods performance across a broader range of data types and distributions could help identify limitations and necessary adjustments.  It would be beneficial to include a section discussing ethical considerations and potential safeguards against misuse such as unauthorized data erasure or manipulation with malicious intent.  Developing guidelines or additional mechanisms to ensure that the unlearning process is conducted transparently and with accountability could mitigate some of the potential negative impacts on society.", "vRrFVHxFiXJ": "Review of \"chemCPA: Compositional Perturbation Autoencoder for Drug Perturbations in SingleCell Transcriptomics\" 1. Summary and Contributions: The paper presents \"chemCPA\" an innovative computational model designed to predict cellular responses to drug perturbations at the singlecell level focusing on compounds not previously observed in the dataset. By incorporating chemical structural knowledge of drugs and employing a transfer learning approach from bulk RNA highthroughput screening (HTS) datasets the model aims to improve generalization performance and thereby reduce the necessity for costly and extensive experimental screenings. The main contributions include the introduction of the chemCPA model an encoderdecoder architecture that leverages molecular representations for predicting singlecell responses to drug perturbations a transfer learning scheme to utilize information from bulk RNAseq HTS for better generalization and demonstration of chemCPAs superior performance in predicting unobserved drugcovariate combinations over existing methods. 2. Strengths and Weaknesses:  Strengths: a. Innovative Approach: The paper tackles the significant challenge of predicting cellular responses to unobserved drug perturbations which is crucial for drug discovery and development. b. Comprehensive Methodology: The use of a transfer learning scheme to leverage bulk RNAseq HTS datasets for improving the model\u2019s generalization capability is commendable. c. Empirical Validation: The model outperforms existing methods (scGen and CPA) on the task of predicting unobserved drugcovariate combinations providing a validation of its utility.  Weaknesses: a. Model Complexity: The complexity of the chemCPA model while enabling its functionality may pose challenges in interpretability and practical deployment. b. Dependence on Preexisting Data: While the transfer learning aspect is a strength it also implies a dependence on the availability and quality of preexisting bulk RNAseq datasets which could limit the applicability in scenarios where such datasets are not readily available or are of poor quality. c. Generalization Ability: While the paper reports an improved generalization ability its unclear how the model performs across a wider array of chemical structures and cellular contexts not represented in the training data. 3. Questions and Suggestions for the Authors:  Question on Model Interpretability: Can the authors comment on the interpretability of the chemCPA model especially in terms of understanding how specific drug features influence predicted cellular responses  Suggestion for Broader Validation: To solidify the models generalization claims it would be beneficial to test chemCPA on a more diverse set of drugs and cellular contexts outside of those represented in the training datasets.  Inquiry on Limitations Addressed: Have the authors considered any potential biases introduced through the transfer learning process especially biases inherent in the preexisting bulk RNAseq datasets 4. Limitations and Societal Impact: The authors mention the use of preexisting datasets for transfer learning but do not deeply discuss the potential limitations and biases these datasets may introduce. Additionally while chemCPA holds promise for accelerating drug discovery discussions on the potential negative societal impacts such as privacy concerns related to patient data and the possibility of model misuse are notably absent. A constructive suggestion would be to incorporate a section discussing these points offering guidance on mitigating biases and ensuring ethical use of chemCPA. Overall while the paper introduces a promising tool for drug discovery a more thorough exploration of its limitations and potential negative impacts would strengthen its contributions.", "lXUp6skJ7r": "Peer Review of \"Adversarial Style Augmentation for Domain Generalization in Semantic Segmentation\" 1) Summary: The paper introduces a novel method AdvStyle aimed at addressing the problem of domain generalization (DG) in semantic segmentation. This is particularly relevant for applications such as autonomous driving where models trained on synthetic data need to generalize to realworld domains (e.g. different weather or lighting conditions) that were not seen during training. The authors observed that the style of an image characterized by channelwise mean and standard deviation significantly impacts model performance across domains. Based on this observation they propose AdvStyle an adversarial style augmentation technique that dynamically generates hard styles during training to improve model robustness against unseen realworld styles. The approach updates the style feature of a training image through adversarial training and uses this to create adversarial images that help train a more robust model. Extensive experiments demonstrate AdvStyles effectiveness achieving stateoftheart performance on two synthetictoreal semantic segmentation datasets and showing promising results in domaingeneralized image classification. 2) Assessment of Strengths and Weaknesses: Strengths:  Originality and Impact: The idea of using adversarial training to manipulate image styles for domain generalization is innovative. This work could significantly impact various applications especially in cases where annotated realworld data is scarce or privacysensitive.  Methodological Rigor: The formulation of AdvStyle is well thought out and based on clear observations linking image style to domain generalization performance. The method is well integrated within the training process offering a dynamic and continuous way of generating challenging styles to enhance model robustness.  Experimental Validation: The authors conducted comprehensive experiments testing AdvStyle across different models backbones and normalization techniques. Moreover the methods applicability beyond semantic segmentation into image classification underscores its versatility. Weaknesses:  Broader Generalization Across Domains: While the paper demonstrates AdvStyles effectiveness in synthetictoreal settings it does not explore its efficacy across a wider range of domain shifts (e.g. across realworld datasets). This limitation could affect the methods applicability in broader contexts.  Computational Overhead: The adversarial training step introduces additional computational overhead. The paper does not provide details on the increase in training time or resource requirement which are crucial for practical implementation.  Limitations in Method Description: While the paper outlines the overall approach and provides pseudocode clearer explanations or visual aids detailing the image composition process during adversarial training could enhance reproducibility. 3) Questions and Suggestions for the Authors:  Broader Applicability: Can the authors discuss or speculate about the methods potential performance and applicability in situations involving realtoreal domain shifts  Computational Costs: Could the authors provide details on the computational overhead associated with AdvStyle For instance comparisons of training times with and without AdvStyle would be informative.  Extension to Other Tasks: Given the promising results in semantic segmentation and image classification does the team envision AdvStyles adaptation to other computer vision tasks such as object detection or instance segmentation 4) Limitations and Societal Impact: The authors did not explicitly discuss potential limitations regarding the scalability of the method to various domain shifts especially in realworld scenarios where style variations might be more subtle or complex. Moreover while the paper briefly mentions data privacy as a motivation a deeper exploration of AdvStyles implications for privacypreserving machine learning could enrich the discussion. Constructive suggestions include conducting experiments across diverse realworld datasets to explore broader applicability and including an analysis of the methods computational efficiency in the paper. In conclusion the paper presents a novel and impactful technique for improving domain generalization in semantic segmentation. Despite some limitations and areas requiring further exploration AdvStyle represents a significant advancement in the field warranting its further development and exploration.", "mUeMOdJ2IJp": "The paper addresses a significant challenge in statistics and machine learning: the recovery of linear subspaces in the presence of heterogeneity and irregular noise particularly within Federated Learning settings. It advances the field by developing a novel estimator for Principal Component Analysis (PCA) that efficiently handles nonspherical and userdependent noise using at least two data points from each user. This approach sidesteps the restrictive assumptions required by previous related works. The authors provide rigorous theoretical support for their estimator by proving upper bounds on estimation errors across varied scenarios and matching informationtheoretic lower bounds even in the case of spherical Gaussian noise. Additionally the paper extends its methodology to tackle linear regression problems demonstrating the estimators broader applicability.  Strengths: 1. Novelty and Practicality: This paper presents an innovative approach to subspace recovery that is highly relevant for federated learning contexts where data heterogeneity and privacy concerns are paramount. 2. Theoretical Rigor: The authors provide a comprehensive theoretical analysis including upper bounds on the estimation error and matching lower bounds that affirm the optimality of their approach. 3. Applicability Beyond PCA: By extending their methods to linear regression settings the authors demonstrate the versatility and broader applicability of their techniques. 4. Efficiency Consideration: The development of an efficiently computable estimator addresses practical concerns making the proposed solution viable for realworld applications.  Weaknesses: 1. Exclusivity of Two Data Points Requirement: The necessity for at least two data points from each user may limit applicability in extremely sparse data scenarios though this limitation is somewhat inherent to the problem space. 2. Potential for Overemphasis on Theoretical Guarantees: While the theoretical contributions are strong the paper could benefit from more empirical evaluations to substantiate its claims in realworld scenarios. 3. Assumptions Still Required: Despite aiming to reduce restrictive assumptions commonly seen in similar works the methods developed still rely on certain assumptions (e.g. subGaussian noise) which could affect generalizability.  Questions and Suggestions for Authors: 1. Empirical Validation: Could the authors provide empirical evidence or case studies demonstrating the efficacy of their proposed estimator in practical federated learning contexts 2. Comparative Analysis: How does the proposed method perform relative to the stateoftheart especially in terms of computational efficiency and estimation accuracy in the presence of highly irregular noise 3. Extension to Sparse Data Scenarios: Could future work explore adaptations of the proposed method to handle scenarios where only a single data point is available from some users possibly through imputation techniques or other strategies 4. Generalizability to Other Noise Models: While the paper addresses nonspherical and userdependent noise how might the proposed methods be adapted or extended to accommodate other complex noise structures  Limitations and Potential Negative Societal Impact: The paper acknowledges the limitations inherent to the requirement of at least two data points per user but does not fully explore potential negative societal impacts. Given the increasing emphasis on privacy in federated learning future iterations could benefit from a discussion on how the proposed methods align with privacypreserving paradigms and any potential unintended consequences in terms of data bias or privacy compromises. Constructive suggestions include exploring mechanisms to enhance privacy protection directly within the estimator or providing guidelines on balancing privacy considerations with estimator efficiency and accuracy. In conclusion this paper makes significant theoretical contributions to the literature on subspace recovery in federated learning settings offering new avenues for efficient and practical estimator design. Future work that addresses the outlined weaknesses and questions could further solidify the importance and applicability of these contributions.", "qk1qpCN-k6": "Review of \"Quantifying Overfitting and Information Efficiency in Linear Regression through the Information Bottleneck\" 1) Summary: The paper tackles the paradox of overfitting in large neural networks which despite their ability to achieve zero training loss often show good generalization performance. Through an informationtheoretic lens\u2014specifically the Information Bottleneck (IB) method\u2014the authors analyze overfitting by defining it as residual information: essentially the parts of a model that encode noise from the training data. They develop a framework to quantify the tradeoff between residual and relevant information in models offering a deep dive into linear regression problems. Their approach yields insights into the nonmonotonic relationship between overfitting and sample size the phenomena of double and multiple descents and the information efficiency of randomized least squares regression compared to an IBoptimized representation. This work extends beyond conventional overfitting measures (like generalization error) introducing a novel quantitative analytical toolset for understanding and assessing learning algorithms performance. 2) Assessment: Strengths:  The paper introduces a novel quantitative framework for the analysis of overfitting and information efficiency a significant advancement over traditional generalization error measures.  The usage of the Information Bottleneck method is well justified and creatively applied to linear regression providing a fresh perspective on a wellstudied problem.  The analytical results supported by random matrix theory are thorough and offer profound insights into the information complexity of linear maps and the effects of sample size on overfitting.  The discussion on the informationtheoretic analogs of double and multiple descent phenomena is particularly informative highlighting complex behaviors in model training that are often overlooked. Weaknesses:  The paper focuses exclusively on linear regression models limiting the general applicability of its findings. While this focus allows for indepth analysis it also leaves unanswered questions about the broader relevance of their framework to other types of machine learning models.  Some of the mathematical derivations and theoretical explanations are dense possibly making the paper less accessible to readers not already versed in information theory or the specific nuances of linear regression.  The practical implications of the findings are not thoroughly explored. For example the paper could benefit from a discussion on how this theoretical framework could be applied to improve actual learning algorithms or model selection in realworld scenarios. 3) Questions and Suggestions:  Could the authors elaborate on how their framework might be extended or adapted to analyze overfitting in nonlinear models or is it strictly applicable only to linear regression problems  It would be beneficial if the paper included more concrete examples or simulations that demonstrate the practical applications of their findings. Can the authors provide additional insights or case studies where their analytical framework directly informs the design or selection of learning algorithms  Considering the focus on linear regression are there specific types of data or learning tasks where the authors anticipate their results will be most impactful A discussion around potential domains or scenarios where their findings could be particularly valuable would enhance the papers practical relevance. 4) Limitations and Societal Impact: The paper does acknowledge its primary limitation by focusing on linear regression. Expanding this framework to more complex nonlinear models could significantly enhance its utility and impact. Regarding potential negative societal impacts the paper does not delve into this area. However it is generally understood that more efficient learning algorithms by reducing overfitting could lead to more accurate and reliable machine learning systems potentially benefiting various sectors like healthcare finance and autonomous systems. Constructively the authors could discuss possible misuses or unintended consequences of optimized algorithms especially if they inadvertently amplify biases present in the training data. Such a discussion would round out the paper and acknowledge the broader implications of their work.", "mq-8p5pUnEX": " Review of \"Leveraging Temporal Latent Bottlenecks for Efficient Sequence Processing with Transformers\"  Summary This paper introduces a novel architecture aimed at improving the efficiency and generalization of Transformers when processing sequential data. The key innovation is the division of the Transformers computation into two streams: a fast stream which processes input in fixedsize chunks using a Transformer architecture for immediate perception and a slow stream which uses a recurrent neural network to learn a compressed specialized representation of the sequence by aggregating information across chunks. This setup is designed to combine the expressive power of Transformers with a more efficient selective attention mechanism that improves sample efficiency and generalization by focusing on salient information. The authors evaluate their model on tasks in visual perception sequential decisionmaking and selfsupervised learning demonstrating improvements over several competitive baselines.  Assessment Strengths: 1. Theoretical Innovation: The idea of bifurcating the sequence processing into fast and slow streams is conceptually novel. It elegantly addresses the issue of computational inefficiency in Transformers by reducing the need to model all pairwise interactions in a sequence. 2. Practical Benefits: The models performance improvements\u2014in terms of sample efficiency and generalization across various tasks including image classification and sequential decisionmaking\u2014present a compelling case for its utility in realworld applications. 3. Insightful Analyses: The qualitative and quantitative analyses particularly the ablation studies and visualizations of the temporal latent bottleneck modules attention offer deep insights into why the model performs well and how it emphasizes relevant information over irrelevant details. Weaknesses: 1. Fixed Chunk Size: The architectures dependence on a fixed chunk size treated as a hyperparameter limits its flexibility and could hinder performance on tasks where the optimal granularity of input processing is dynamic or unknown. 2. Potential for Overfitting: While the model exhibits improved sample efficiency the dualstream setup especially with the recurrent nature of the slow stream might increase the risk of overfitting particularly in datasparse environments. This aspect is not thoroughly analyzed in the paper. 3. Complexity and Interpretability: The added complexity from managing two separate processing streams and their interactions could make the model less interpretable and harder to debug an issue not addressed by the authors.  Questions and Suggestions 1. Dynamic Chunking: Have the authors considered approaches for dynamically determining the chunk size based on the input data or task requirements This could potentially enhance the models adaptability and performance. 2. Overfitting Mitigation: Could the authors elaborate on any strategies or safeguards implemented or recommended for preventing overfitting given the recurrent slow streams potential to memorize rather than generalize 3. Interpretability Enhancements: Are there methods or tools the authors would suggest to improve the interpretability of the model especially concerning the interaction between the fast and slow streams  Limitations and Societal Impact The authors have not thoroughly addressed the potential negative societal impacts of their work such as the use of this technology in autonomous systems or surveillance applications where efficiency improvements could lead to enhanced capabilities for privacy invasion or autonomous decisionmaking with ethical implications. Moreover the discussion on limitations primarily focuses on technical aspects like the fixed chunk size and does not cover broader issues like fairness accountability and transparency which are critical when deploying AI systems in realworld scenarios. Suggestions for Improvement:  Broader Limitations Discussion: Expanding the limitations discussion to cover potential misuse and ethical considerations would provide a more holistic view of the works impact.  Dynamic Chunking Techniques: Research into adaptive chunking methods could make the model more versatile and robust across different tasks.  Fairness and Bias Analysis: Given the model\u2019s improved generalization capabilities an analysis of how it handles or potentially amplifies biases in the training data would be valuable.", "o4uFFg9_TpV": "Review of \"Visual Prompting via Image Inpainting\" 1. Summary of the Paper and Its Contributions This paper introduces an innovative concept termed \"visual prompting\" which adapts a pretrained inpainting model to perform new imagetoimage tasks without taskspecific finetuning or model modification. Inspired by the success of textual prompting in NLP the study explores the potential of using visual examples as prompts to guide the models output for various computer vision tasks. The approach utilizes largecapacity inpainting models trained on a curated dataset of 88k unlabeled images from academic papers. The key contributions of this work include the introduction and validation of the visual prompting framework the creation of a novel dataset conducive to learning gridlike structures and the demonstration that combining this dataset with more general image data enhances model performance. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The concept of visual prompting is novel and imaginative drawing a meaningful parallel to prompting mechanisms in NLP. It opens a new avenue for leveraging pretrained models in computer vision tasks.  Generalizability: The proposed method shows an impressive ability to generalize across a variety of imagetoimage tasks offering evidence of the potential versatility of the approach.  Dataset Creation: The authors have undertaken a significant effort in curating a new and unique dataset that is instrumental for the success of their proposed approach. Weaknesses:  Implementation Detail Clarity: While the paper provides an overview of the methodological aspects it lacks depth in certain technical descriptions which may hinder reproducibility. For instance more detailed insights into the specific choices in the inpainting algorithms and the criteria for dataset curation might be valuable.  Quantitative Benchmarking: The paper could benefit from more extensive quantitative comparisons with established benchmarks or competing methods across the tasks mentioned. Providing such benchmarks could significantly enhance the papers credibility and its contribution to the field.  Exploration of Limitations: The discussion on the limitations and potential failings of the visual prompting approach appears to be somewhat limited. A more thorough examination of scenarios where the method could falter or underperform would contribute to a more balanced and nuanced article. 3. Questions and Suggestions for the Authors  Further Clarification: Could the authors expand on the technical aspects of the inpainting models used especially how the models deal with the complexity and variety inherent in the visual prompts  Benchmarking Addition: It may be beneficial to include comparative performance metrics against established methods for the tasks mentioned. Has the model been tested against tasks where it didnt perform as expected and what were the insights from such evaluations  Limitations and Societal Impacts: Considering the application scope in what scenarios could the proposed approach potentially mislead or produce unintended results Discussing societal implications especially in sensitive areas employing AI would be valuable. 4. Limitations and Societal Impact The authors have ventured into addressing the limitations of their approach but could elaborate further especially on the models dependency on the curated datasets specificity and how this might limit its applicability in wider domains. On societal impacts while the paper touches upon positive applications a deeper reflection on potential misuse or ethical concerns in visual AI prompted tasks (such as deepfakes or misinformation) would provide a more comprehensive view. Suggestions for improvement could include strategies for mitigating risks associated with adversarial uses of the technology and exploring the broader implications of automated image generation and modification in societal contexts. Overall this paper presents a compelling exploration of visual prompting demonstrating significant potential for future research and application. Addressing the noted weaknesses and questions could further solidify its contributions and ensure broader applicability and understanding of its societal impacts.", "rYkGxHPnCIf": " Peer Review of \"NonEquilibrium Importance Sampling Based on Flow Dynamics\"  1) Brief Summary The paper develops a novel method termed NonEquilibrium Importance Sampling (NEIS) that addresses the computational challenges of estimating expectations and normalization constants for complex highdimensional distributions. NEIS employs a base distribution and propagates samples along the velocities of a flow field enabling variance reduction in estimations. The authors theoretically prove under specific conditions that the NEIS estimator can achieve zero variance and compare this approach with traditional importance sampling methods including those utilizing transport maps. The methods practical implementation is facilitated through deep learning optimizing the flows velocity field represented by a neural network to minimize variance. Numerical experiments demonstrate significant variance reduction in benchmark examples establishing NEISs superiority over the annealed importance sampling (AIS) and highlighting its potential efficacy in broader scientific and statistical inference applications.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Foundation: The papers theoretical contributions including the conditions under which the NEIS estimator achieves zero variance are significant. This provides a strong foundation for the NEIS approach lending credibility and inspiring confidence in its effectiveness.  Innovative Approach: NEIS introduces a creative solution to a wellknown problem in computational sciences combining flow dynamics with importance sampling in a manner that is both novel and potentially transformative.  Practical Implementation: By utilizing deep learning to optimize the velocity field the authors bridge the gap between theory and practice enabling the application of NEIS to realworld problems and making the methodology accessible to a wider audience.  Empirical Validation: The numerical experiments provide compelling evidence of NEISs capability to significantly reduce variance compared to traditional methods effectively demonstrating the methods practical value. Weaknesses:  Model Complexity: The complexity of the NEIS methodology might pose challenges for its adoption particularly for researchers or practitioners without a strong background in both statistical physics and deep learning.  Scalability: While the paper presents results in up to 10 dimensions the scalability of NEIS to higherdimensional or more complex distributions remains unclear potentially limiting its applicability.  Comparison with Existing Methods: The comparison largely focuses on AIS a broader comparison with other stateoftheart sampling methods could strengthen the argument for NEISs superiority.  3) Questions and Suggestions for Authors  Scalability and Performance in Higher Dimensions: Can the authors elaborate on NEISs performance in settings beyond 10 dimensions or provide insights into its scalability and computational efficiency in more complex scenarios  Comparison with Other Sampling Methods: Expanding on the comparison section to include more recent or advanced sampling methods could provide a clearer view of where NEIS stands in the landscape of importance sampling strategies.  Implementation Details: Providing more details on the neural network architecture training parameters and implementation choices might help readers replicate or build upon this work.  4) Limitations The authors have made commendable efforts to address the limitations of their work including a clear discussion on the challenges of model complexity and the necessity of deep learning for practical implementation. However a more explicit discussion on the limitations regarding scalability and a broader comparison with a wider range of existing methods would enhance the paper. Suggestions for Improvement:  Enhanced Scalability Analysis: A detailed examination or a case study on highdimensional or more complex realworld problems could offer valuable insights into NEISs scalability and broader applicability.  Explicit Discussion on Societal Impact: While the paper mentions the potential negative societal impact briefly a more thorough discussion especially considering the algorithms application in sensitive statistical inference tasks would be beneficial. This might encompass considerations on data privacy potential biases in model selection and misuse of computationally intensive methods.", "w0O3F4cTNfG": "The paper explores causal discovery in scenarios complicated by measurement error focusing on linear structural equation models (SEMs) and introducing innovative methods for identifying causal structures under these constraints. It proposes a novel conceptual bridge between models with measurement error (SEMME) and those containing unobserved parentless causes (SEMUR) founded on the utilization of a common mixing matrix. This connection facilitates the transferability of identifiability results between these model types. By adopting a twopart faithfulness assumption the paper delineates the extent to which causal structures can be identified presenting a methodology that encompasses both model identification and the learning of causal structures tested on synthetic datasets.  Strengths: 1. Novel Methodological Contribution: The introduction of a mapping between SEMME and SEMUR models based on their shared mixing matrix is both unique and significant. This innovative approach not only enhances the understanding of identifiability in such systems but also broadens the scope for applying causal discovery methods in complex scenarios. 2. Theoretical Depth and Rigor: The paper is theoretically robust providing a thorough mathematical foundation for its claims. The identifiability results under the twopart faithfulness assumption are particularly noteworthy offering clear conditions under which causal structures can be learned. 3. Practical Implications: By extending the theory to practical algorithms and validating these through simulations on synthetic data the paper demonstrates the realworld applicability of its findings. This bridges the gap between theoretical causal discovery and practical challenges encountered in areas like neuroscience and genomics.  Weaknesses: 1. Limited Evaluation on Synthetic Data: While the paper effectively demonstrates the efficacy of its proposed methods on synthetic data the absence of realworld data experiments may limit the understanding of the methods\u2019 performance in practical complex scenarios. 2. Assumptions\u2019 Generality: The twopart faithfulness assumption while foundational to the papers methodology may not always hold in realworld systems. This could restrict the applicability of the proposed approach in certain scenarios and additional discussion on the prevalence of these assumptions in practical settings would have been beneficial. 3. Explanation of Conceptual Mapping: The explanation and intuition behind the mapping between SEMME and SEMUR models could be improved. More detailed examples or visual illustrations of this mapping process would help in making the concepts more accessible to readers not deeply familiar with causal inference.  Questions and Suggestions: 1. RealWorld Applicability: Could the authors provide more insights into how their approach can be adapted or extended to handle nonlinear systems which are common in realworld datasets 2. Robustness to Violations of Assumptions: It would be beneficial for the paper to discuss the robustness of the proposed methods to potential violations of the faithfulness assumption. Are there any strategies or modifications to the algorithm that could mitigate the impact of such violations 3. Comparative Analysis: Including a comparative analysis with other stateoftheart causal discovery methods on both synthetic and realworld datasets would enhance the papers contribution by contextualizing its performance and applicability.  Limitations and Societal Impact: The paper acknowledges that its methodology relies on specific assumptions (e.g. the twopart faithfulness assumption) that might not hold in all realworld scenarios. While this limitation is addressed through the theoretical framework practical implications of these limitations need further exploration. In terms of societal impact while the paper does not directly discuss potential negative consequences it is imperative to consider the implications of inaccurately inferred causal relationships especially in sensitive fields such as healthcare and public policy. It would be advantageous for future work to delve into these aspects ensuring that causal discovery methods are applied responsibly.  Suggestions for Improvement:  Expanding the evaluation to include realworld datasets alongside synthetic data to demonstrate the methods effectiveness and robustness in practical applications.  Further exploration on the generality of the faithfulness assumption in realworld scenarios and potential strategies for dealing with violations of this assumption.  Enhancing the clarity and accessibility of the methodological explanations possibly through additional examples or visualizations to make the paper accessible to a broader audience. In summary the paper presents a significant step forward in the field of causal discovery under measurement error offering a robust theoretical foundation and promising preliminary results. Addressing the highlighted limitations and incorporating the suggested improvements could greatly enhance its contribution and applicability to realworld challenges.", "nZRTRevUO-": "Review of \"Local Latent Bayesian Optimization (LOLBO) for Structured Optimization via Deep Autoencoders\" 1. Summary The paper presents a novel approach to Bayesian optimization (BO) over the latent spaces of deep autoencoders (DAEs) aiming to address the optimization of structured discrete hardtoenumerate spaces such as molecules. The proposed method Local Latent Bayesian Optimization (LOLBO) integrates trust region concepts with latent space BO leveraging a semisupervised deep autoencoder model. This model jointly optimizes the latent representation and the surrogate model aligning the optimization process in the latent and input spaces more closely. The approach is demonstrated to significantly outperform stateoftheart methods over several realworld benchmarks achieving up to 22\u00d7 improvements in optimization performance. 2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: LOLBOs integration of trust regions with latent space optimization is a novel contribution to the field of Bayesian optimization especially in the context of structured search spaces.  Significant Performance Gains: The empirical results showcasing up to 22\u00d7 improvement over existing methods on realworld benchmarks are impressive emphasizing the potential of LOLBO in practical applications.  WellDesigned Methodology: The joint optimization of the surrogate model and the DAE alongside the adaptation of trust regions to latent space is methodologically sound. The semisupervised approach ensures that the optimization process benefits from both unlabeled and labeled data. Weaknesses:  Limited Discussion on the Choice of Trust Region Parameters: While the paper mentions the use of default parameters for trust region dynamics a deeper exploration of how the choice of these parameters might affect performance would be valuable.  Computational Cost Considerations: The paper briefly mentions computational considerations but lacks a detailed analysis of the computational costs involved in retraining the DAE and the implications for practical deployments.  Generalizability Concerns: While the paper provides compelling empirical evidence within the context of molecule design its not immediately clear how generalizable these results are to other types of structured optimization problems. 3. Questions and Suggestions for the Authors  Parameter Sensitivity: Can the authors provide more insights into how sensitive LOLBOs performance is to the choice of trust region parameters and the implications for tuning these parameters in different contexts  Computational Cost Analysis: A more detailed discussion on the computational costs of LOLBO particularly regarding the frequency of updating the DAE would be beneficial. How do these costs scale with the complexity of the search space  Generalizability: Could the authors elaborate on the potential applications of LOLBO outside molecule design Are there any limitations or considerations that need to be addressed when applying LOLBO to other domains  Qualitative Evaluation of Outputs: Considering the performance on molecule design tasks could the authors provide a qualitative assessment of the generated molecules It would be interesting to see if the optimized molecules not only score well on objective metrics but also make sense from a chemical standpoint. 4. Limitations and Societal Impact  The authors do not thoroughly address the potential negative societal impacts of their work. For molecular design the optimization of certain properties without considering toxicity or environmental impact could lead to undesirable outcomes. Additionally the paper could benefit from a discussion on the limitations regarding the applicability of LOLBO to a wider range of structured optimization problems. Constructive suggestions include incorporating explicit constraints or objectives related to safety ethics and environmental sustainability in the optimization process and a more thorough investigation into the factors influencing the generalizability of the method to other domains.", "omI5hgwgrsa": "The paper under consideration advances the understanding and development of algorithms for decentralized stochastic variational inequalities (VIs) a framework that encompasses a broad range of applied tasks including machine learning challenges through both theoretical bounds and novel algorithms. VIs themselves are a versatile class of problems including games minimization saddle point and equilibrium problems. This work particularly focuses on the decentralized setting for stochastic (sumtype) VIs across both fixed and timevarying network topologies a domain that has not been comprehensively explored before. The authors establish lower complexity bounds for both communication and local iterations in this setting and propose optimal algorithms that match these lower bounds. Notably the proposed algorithms are claimed to outperform existing solutions not only in the decentralized stochastic setting but also in the deterministic and nondistributed stochastic contexts which is confirmed through experimental validations.  Strengths 1. Theoretical Contributions: The presentation of the first lower bounds for the communication and local computation complexities of decentralized VIs in the stochastic case is a significant theoretical advancement. These bounds provide a foundational understanding of the limitations inherent in solving VIs in a decentralized manner. 2. Algorithmic Innovations: The development of four new algorithms tailored to both fixed and timevarying network topologies with proven optimality in terms of communication and local iteration complexities is a substantial contribution. This optimality combined with the algorithms\u2019 superior performance across various distributed settings sets a new benchmark in the field. 3. Experimental Validation: The authors undertake a comprehensive set of experiments to validate the effectiveness of their algorithms. This empirical evidence strengthens the papers claims about the algorithms performance and broader applicability in machine learning contexts.  Weaknesses 1. Assumption Clarity: While the paper outlines the assumptions underpinning their theoretical results a more detailed discussion on the practical implications of these assumptions could enhance the readers understanding. Specifically how common or restrictive are these assumptions in realworld decentralized computing environments 2. Algorithm Complexity: The description of the algorithms particularly their implementation specifics could benefit from a more detailed exposition. Given the claim of optimality and superiority over existing methods understanding the complexity and practical feasibility of implementation in typical decentralized environments would be beneficial. 3. Broad Applicability Discussion: While the paper convincingly demonstrates the algorithms effectiveness for specific types of variational inequalities a discussion on the limitations or challenges in extending this approach to more general or nonstandard VI forms could provide a more balanced view.  Questions and Suggestions for Authors 1. Assumption Justification: Can the authors provide further justification or discussion regarding the practicality of the assumptions made in their theoretical analysis E.g. the proximal friendliness of the function g in realworld scenarios. 2. Implementation Details: Could the authors expand on the implementation complexity of the optimal algorithms particularly in terms of computational resources and scalability in large decentralized networks 3. Extension to NonStandard VIs: How applicable are the proposed algorithms to variational inequalities that might not strictly adhere to the assumptions laid out Are there potential modifications that could extend their applicability 4. Comparison with NonOptimal Methods: Given the claim of optimality a comparison with nonoptimal but widely used methods could further establish the practical significance of the proposed algorithms. Is there such a comparison available especially in more complex or noisy network conditions  Limitations and Societal Impact The paper provides a solid foundation in understanding the limitations inherent in the problem domain itself through the establishment of lower bounds. However a discussion on the practical limitations of deploying these optimal algorithms especially in highly dynamic or resourceconstrained decentralized environments is missing. Additionally the societal impact particularly regarding data privacy and energy efficiency in decentralized settings is not addressed. These aspects are critical considering the increasing relevance of decentralized processing in sensitive applications. Constructive suggestions for improvement include:  Expanding the discussion on the practical challenges of implementing the proposed algorithms in realworld decentralized networks.  Exploring potential modifications or adaptations of the algorithms that could address more general or varied forms of variational inequalities.  Considering the inclusion of a detailed analysis of the algorithms impact on privacy and energy efficiency which are pivotal concerns in decentralized machine learning setups.", "q0XxMcbaZH9": " Review of \"Boosting QueryBased Instance Segmenters with Discriminative Query Embedding Learning\"  1. Summary This paper aims to enhance the performance of querybased instance segmentation methods by introducing a novel training framework focused on discriminative query embedding learning. The foundation of their approach rests on datasetlevel uniqueness and transformation equivariance of the relations between queries and instances. Unlike traditional methods that learn instance queries within individual scenes this work proposes learning discriminative query vectors capable of unique instance identification at the dataset level through interscene instance querying. Moreover it enforces equivariance in image (instance) representations and queries against geometric transformations to establish robust instancequery matching. The proposed method is extensively evaluated across four prominent querybased models demonstrating notable improvements on the COCO and LVISv1 datasets without necessitating architectural changes or additional computational burdens during deployment.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty in Training Framework: The shift to a crossscene query embedding learning and the emphasis on transformation equivariance introduce a fresh perspective to the field. These aspects enable more discriminative query learning and robustness addressing gaps in the current querybased instance segmentation techniques.  Empirical Validation: The methodological merits are wellsupported by comprehensive experiments across various datasets and model architectures showing consistent improvements. Such extensive validation underscores the frameworks adaptability and effectiveness.  No Architectural Changes Needed: A significant advantage of this work is its ability to be applied seamlessly to existing models without altering their architectures making it broadly applicable and easy to adopt.  Thorough Analysis: The paper provides an indepth analysis of component contributions design decisions and potential scalability through ablation studies demonstrating a thorough understanding of the proposed frameworks mechanisms. Weaknesses:  Lack of Theoretical Insights: While the empirical results are strong the paper falls short in providing theoretical underpinnings for why and how the specific components (e.g. datasetlevel uniqueness and transformation equivariance) contribute to the observed performance gains. A more detailed theoretical analysis could enhance the understanding and justification of the proposed method.  Potential Overfitting Concerns: Although not explicitly mentioned learning queries across the entire dataset might raise concerns about overfitting especially on datasets with limited diversity in instances. An exploration into how the method scales with dataset size and diversity and mechanisms to prevent overfitting would be valuable.  Societal Impacts and Ethics: The paper briefly mentions applications such as autonomous driving and augmented reality but does not elaborate on the potential negative societal impacts or ethical considerations of improved instance segmentation models. Discussing these aspects could ensure a more responsible advancement of the technology.  3. Questions and Suggestions for the Authors  Theoretical Background: Can the authors provide a more detailed theoretical rationale for the specific choices in the proposed method particularly on datasetlevel uniqueness and transformation equivariance  Overfitting Prevention: How does the proposed method ensure generalization especially in scenarios with diverse or limited datasets Are there any mechanisms or regularizations implemented to counter potential overfitting  Societal Impact Discussion: Could the authors elaborate on any potential negative societal impacts of their work such as privacy concerns in surveillance applications or biases in autonomous systems This would foster a more comprehensive understanding of the technologys implications.  4. Limitations and Societal Impact The authors have not adequately addressed the potential limitations related to overfitting and dataset diversity. Expanding on these aspects could improve the applicability and robustness of the proposed method. Moreover the discussion on societal impact is notably absent. Given the pervasive nature of instance segmentation across various applications evaluating and acknowledging potential negative implications such as privacy erosion or biased decisionmaking in autonomous systems is crucial. Suggestions for addressing these limitations include incorporating regularization techniques to mitigate overfitting and providing guidelines or considerations for ethical use cases of the technology. In summary this paper presents a promising advancement in querybased instance segmentation through an innovative training framework. Addressing the outlined weaknesses and expanding upon the suggested areas could further solidify its contributions and applicability.", "pUPFRSxfACD": " Summary The paper aims to address the challenge of learning invariant features from heterogeneous data without explicit environment partitions a common scenario in machine learning applications. The authors convincingly argue that without additional inductive biases or auxiliary information this task is fundamentally impossible. They propose a novel framework ZIN (auxiliary information Z for environmental INference) which jointly learns environment partition and invariant representations assisted by additional auxiliary variables. The authors establish both necessary and sufficient conditions under which their framework can provably identify invariant features across synthetic and realworld datasets. Their findings underscore the importance of making the role of inductive biases more explicit in future works on learning invariant models without provided environment partitions.  Strengths 1. Theoretical Contribution: The paper provides a strong theoretical foundation proving that without auxiliary information or inductive biases learning invariant features from heterogeneous data lacking explicit environment indexes is impossible. The sufficient and necessary conditions for the identifiability of invariant features significantly advance our understanding of when and how invariant learning can be achieved. 2. Practical Relevance: The proposed ZIN framework is highly relevant for practical applications where environment partitions are not available. The empirical validation across various datasets demonstrates ZINs effectiveness in improving outofdistribution generalization a critical issue in machine learning. 3. Accessibility and Replicability: By making their code available the authors encourage replication and further investigation. This transparency is beneficial for the advancement of the research community.  Weaknesses 1. Assumptions on Auxiliary Variables: While necessary for the frameworks success the dependency on auxiliary variables introduces additional assumptions about the data collection process. The conditions required for these variables may not always be met in practical scenarios limiting the frameworks applicability. 2. Lack of Nonlinear Feature Extractor Guarantees: The authors acknowledge the limitation of their theoretical guarantees to feature selection and linear feature learning scenarios. The lack of guarantees for general nonlinear feature extractors is a significant gap given the common use of complex models in practice. 3. Potential Negative Societal Impact Not Addressed: The paper does not discuss the potential negative societal impacts of their methodology. Given that invariant learning can be applied to sensitive areas (e.g. facial recognition predictive policing) reflecting on these impacts and providing guidelines for ethical use would have been valuable.  Questions and Suggestions 1. Extension to Nonlinear Models: Can the authors speculate on potential approaches to extend their theoretical guarantees to general nonlinear models This extension would significantly enhance the frameworks practical utility. 2. Selection of Auxiliary Variables: It would be helpful to have more detailed guidelines or criteria for selecting effective auxiliary variables particularly in complex or poorly understood domains. Could unsupervised learning or domain knowledge play a role here 3. Evaluation on Additional Datasets: While the paper provides comprehensive empirical evaluations further validation in domains with known causal structures but complex distributions could further attest to the frameworks robustness.  Limitations and Societal Impact The authors should consider providing a more explicit discussion on the limitations regarding the selection and assumption of auxiliary variables. Additionally a thoughtful reflection on the potential negative societal impacts of their methodology especially in sensitive application areas would enhance the papers completeness. Recommendations for responsible use and ethical considerations in applying the ZIN framework could mitigate unintended consequences.", "yI7i9yc3Upr": "Review of \"Controlling AutoRegressive Generation Models with NeurallyDecomposed Oracle (NADO)\" 1. Summary of Contributions: The paper introduces a novel framework NADO aiming to achieve controllable text generation by guiding a pretrained base language model using sequencelevel boolean oracle functions decomposed into tokenlevel guidance. This is accomplished without the need for additional auxiliary labeled data leveraging examples sampled from the base model itself. The framework employs posterior regularization to derive a closedform optimal solution for integrating tokenlevel guidance into the base model ensuring the generated text adheres to specific control factors (e.g. lexical constraints formality). Experiments on tasks such as lexically constrained generation and formalitycontrolled machine translation illustrate the frameworks efficacy in steering the base model towards desired outcomes while preserving generation quality. 2. Assessment of Strengths and Weaknesses: Strengths:  The framework provides a generalizable and flexible approach to controllable text generation applicable across different tasks and base models.  NADOs method of sampling from the base model for training negates the need for external labeled data making it an efficient solution.  The incorporation of theoretical analysis provides a solid foundation for understanding how the quality of the NADO approximation impacts control over the generated text.  Experimental results showcase impressive performance improvements over existing methods demonstrating NADOs ability to effectively enforce control constraints without sacrificing text quality. Weaknesses:  The paper could benefit from a more extensive exploration of the limitations and potential biases introduced by the sequencelevel oracle functions and how they might impact generation diversity or introduce unintended biases.  There is a lack of discussion regarding the computation complexity introduced by the tokenlevel decomposition and its scalability to larger models and datasets.  Although the paper touches upon the potential for societal impact it does not extensively discuss or evaluate the ethical implications or potential misuses of such controllable generation technologies. 3. Questions and Suggestions for the Authors:  Could the authors elaborate on potential strategies to mitigate risks associated with the misuse of controllable text generation technologies especially in sensitive applications  How does the computational complexity of the NADO framework compare to other controllable generation methods especially when scaling to larger models and datasets  The paper would benefit from a discussion regarding the choice of sequencelevel oracle functions their design and how they might bias the generation process toward specific narratives or perspectives.  It would be insightful to see an expansion of the experimental evaluation to include tests for generation diversity and robustness under varied and complex control scenarios. 4. Addressing Limitations and Societal Impact: The authors recognize the potential for negative societal impacts inherent in controllable text generation technologies but do not provide a detailed analysis or mitigation strategies. It would be beneficial for future iterations of this work to include:  A thorough discussion on the ethical considerations of text generation control including scenarios where it might be misused.  Proposals for safeguards or frameworks that could be implemented to prevent or minimize harmful applications of this technology.  Consideration of how diversity fairness and bias should be addressed when defining oracle functions and training the NADO model. Overall this paper presents a significant advancement in the field of controllable text generation. The suggested improvements and additional considerations could further strengthen the work ensuring its applicability and responsibility in broader contexts.", "mmzkqUKNVm": "Review of \"Improving Semantic Segmentation through Boundary Enhancement via Semantic Diffusion\" 1. Summary: The paper presents a novel approach to enhance boundary quality in semantic segmentation tasks by introducing a Semantic Diffusion Network (SDN). The cores of the method center around an operatorlevel strategy that formulates boundary feature enhancement as an anisotropic diffusion process which is approximated via a novel learnable module the SDN. This module integrates a semantic difference convolution operator with a feature fusion operation to emphasize interclass boundaries while being agnostic to intraclass texture edges. The efficiency and flexibility of SDN allow it to be easily incorporated into existing encoderdecoder segmentation frameworks demonstrating substantial improvements on challenging benchmarks like ADE20K and Cityscapes. 2. Strengths and Weaknesses: Strengths:  Novelty: The approach of addressing the intrinsic smoothing issue of convolutional operations through anisotropic diffusion at the operator level is innovative.  Performance Improvement: The paper shows a significant improvement in boundary quality with extensive experiments proving the efficacy across multiple baseline models and benchmarks.  Flexibility and Efficiency: SDNs design allows for straightforward integration into existing segmentation pipelines without considerable computational overhead which is practically beneficial. Weaknesses:  Generalization across Datasets: While improvements are demonstrated on ADE20K and Cityscapes the generalization capability of SDN across a wider variety of datasets especially those with less pronounced boundary regions is not discussed.  Analysis of Failure Cases: The paper lacks an indepth analysis of scenarios where SDN may not perform optimally such as complex overlapping objects or very fine structures.  Computational Cost Evaluation: Despite mentioning efficiency a detailed analysis comparing the computational costs and runtimes before and after integrating SDN into existing models would offer clearer insights. 3. Questions and Suggestions:  Generalization Capability: Could the authors provide insights or additional experiments on how SDN generalizes to other datasets particularly those with more subtle or complex boundaries  Failure Case Analysis: It would be helpful if the authors included a section discussing the limitations or failure cases of SDN providing examples where the method struggles and potential reasons.  Detailed Computational Cost Analysis: A comparative analysis of the computational cost and inference time with and without SDN integration in various baseline models would be valuable. This could help readers better understand the practical implications of deploying SDN in realworld applications.  Exploration of Fusion Techniques: The feature fusion module plays a critical role in integrating enhanced boundary features. Could alternative fusion strategies further improve the models performance 4. Limitations and Societal Impact: The paper briefly touches upon the limitations of their approach but could benefit from a more detailed discussion. A clearer delineation between the strengths and potential weaknesses or scenarios where SDN may not provide substantial improvements would be useful. Regarding societal impact while the paper does not directly address potential negative implications it is essential to consider how enhanced segmentation models could affect privacy or surveillance applications. Constructively the authors might explore settings to ensure their contributions foster positive outcomes such as medical imaging diagnostics where boundary delineation is critical.", "uzn0WLCfuC_": "Summary: The paper introduces Expected Improvement (EI) as a novel strategy for contextual bandits expanding its application beyond traditional Bayesian optimization. The authors propose two EIbased algorithms: LinEI for scenarios with linear reward functions and NeuralEI for more complex reward functions modeled by deep neural networks. They demonstrate nearoptimal regret performance for LinEI and establish regret bounds for NeuralEI correlating with the effective dimension of a neural tangent kernel matrix. Empirical evidence shows that both algorithms outperform existing methods particularly in highdimensional settings. Strengths: 1. Originality: The application of EI in contextual bandits is novel contributing a fresh perspective to the explorationexploitation tradeoff problem. 2. Theoretical Rigor: The paper provides a robust theoretical foundation for both algorithms including regret analysis and comparisons to existing methods like LinUCB and LinTS. 3. Empirical Validation: Extensive experiments demonstrate the algorithms superiority across various benchmark datasets reinforcing the theoretical claims. 4. Practicality: By showing improved performance particularly in highdimensional settings the paper addresses a critical challenge in contextual bandits increasing the algorithms applicability to realworld problems. Weaknesses: 1. Dependency on Parameters: The performance of the algorithms seems to heavily rely on the tuning of specific parameters (e.g. C0 \u03b2 for LinEI) which may limit their practical deployment without detailed guidance on setting these parameters. 2. Limitation in Reward Function Complexity: While the introduction of NeuralEI is an attempt to address more complex reward functions the reliance on deep neural networks might not be suitable for scenarios where model interpretability is crucial. 3. Computational Complexity: The paper does not extensively discuss the computational demands of the proposed algorithms which is critical for evaluating their feasibility in largescale applications. Questions and Suggestions for the Authors: 1. Parameter Sensitivity: Can the authors provide more insights or guidance on the selection and tuning of algorithm parameters such as C0 and \u03b2 for LinEI 2. Model Interpretability: Could the authors discuss the tradeoffs between model complexity and interpretability especially concerning NeuralEIs use of deep neural networks 3. Computational Demand: A more detailed analysis of the computational complexity and potential optimization strategies for both algorithms would be valuable. 4. Extension to Other Contexts: How applicable are the proposed EIbased algorithms to nonstationary environments or scenarios with dynamic rewards Limitations and Societal Impacts: The paper somewhat addresses the limitations by comparing the proposed algorithms against existing methods and discussing the theoretical foundations. However a more indepth analysis of the computational complexity and guidance on parameter tuning would be beneficial. Regarding societal impact the paper does not delve into potential negative consequences such as the misuse of sophisticated exploitation strategies in sensitive domains like healthcare or finance. Constructive suggestions include promoting transparency in algorithmic decisions especially when deploying NeuralEI in areas requiring interpretability and ethical considerations of automated decisionmaking.", "upuYKQiyxa_": "Review of \"Towards Robust Vision Transformers by Focusing on Foreground Features\" 1) Summary: The paper proposes an innovative approach to enhancing the robustness of Vision Transformer (ViT) models against distribution shifts by finetuning them to prioritize foreground objects over spurious background cues in their decisionmaking process. The finetuning procedure employs three specialized loss terms to: a) diminish the models relevancy assignment to background features b) ensure comprehensive foreground feature consideration and c) maintain high decision confidence. An impressive aspect of this method is its use of segmentation maps which can be obtained via an automated selfsupervised process eliminating the need for additional labeled data. The authors demonstrate significant improvements in robustness across various datasets especially in scenarios with domain shifts. 2) Assessment: Strengths:  Innovativeness: The paper addresses a crucial aspect of improving Vision Transformer models by reducing their reliance on background features which has been a persistent issue affecting robustness and generalization.  SelfSufficiency: The ability to generate necessary foreground masks using a selfsupervised variant of ViT model is particularly striking as it alleviates the need for manual annotation or additional labeled datasets.  Experimental Validation: The comprehensive battery of experiments including comparisons with baseline methods and across different datasets encountering domain shifts provides convincing evidence of the effectiveness of the proposed method.  General Applicability: The methodology is presented as a generic improvement applicable to virtually any image classifier not just ViTs broadening its potential impact. Weaknesses:  Potential for Overfitting to Foreground: While focusing on the foreground is beneficial for robustness to domain shifts there may be scenarios where context or background cues are crucial for correct classification. The paper does not address potential drawbacks of possibly neglecting useful background information.  Scalability of Finetuning Process: Although the method is presented as computationally feasible for large models detailed analysis on the computational overhead and scalability especially for extremely large datasets is missing.  Limited Discussion on Limitations: The discussion section briefly touches on variations in the effectiveness across classes but lacks a deeper exploration of scenarios where the method might underperform. 3) Questions and Suggestions for the Authors:  Impact of Foreground Focus on ContextReliant Categories: Could the authors provide insights or additional experiments on how the method performs on categories heavily reliant on context clues encoded in the background  Adaptability to Other Model Architectures: While the method demonstrates success with ViTs can the authors speculate on its adaptability and potential challenges when applying it to significantly different architectures such as CNNs or hybrid models  Mitigation Strategies for Potential Overemphasis on Foreground: In cases where the model might overly focus on foreground to the detriment of useful background cues do the authors propose any strategies or adjustments to the finetuning process to avoid this 4) Limitations and Societal Impacts: The authors have made a commendable effort in addressing limitations and showcasing the potential positive impact of their work in improving the robustness of vision models. However a more detailed discussion on scenarios where their method might introduce biases or overfit to foreground features leading to misclassification would enhance the paper. Suggestions for improvement include incorporating a more nuanced control mechanism that can dynamically adjust the balance between foreground and background information based on context or the specific classification task at hand. Additionally exploring the societal implications of deploying more robust classification models such as privacy concerns or misuse in surveillance could contribute to a more comprehensive understanding of the broader impacts. In conclusion the paper presents a promising approach towards enhancing the robustness of vision models by emphasizing foreground features. With further exploration of the outlined concerns and potential ethical implications the proposed method has the potential to significantly contribute to the development of more reliable and generalizable visionbased systems.", "q_AeTuxv02D": " Review  1) Brief Summary of the Paper and Its Contributions This paper presents a seminal theoretical study on graph Message Passing Neural Networks (gMPNNs) such as Graph Neural Networks (GNNs) for inductive outofdistribution (OOD) link prediction tasks. This context involves deploying the model on test graphs significantly larger than those seen during training. The authors rigorously demonstrate that conventional strategies relying on permutationequivariant node embeddings yield deteriorating performance as the test graph size increases converging towards randomness. A novel structural pairwise embedding approach is proposed equipped with a theoretically grounded gMPNN architecture designed for effective OOD link prediction. The paper provides nonasymptotic bounds evidencing the models convergence to desirable embeddings and empirically validates these findings using random graphs generated from stochastic block models (SBMs).  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Novelty: The paper stands as the first to develop a theoretical framework that critically addresses the challenge of OOD link prediction on larger test graphs. It fills a significant gap in the existing literature on GNNs and link prediction.  Strong Empirical Validation: The theoretical contributions are complemented with extensive empirical experiments which not only support the theoretical claims but also provide insights into the practical implications of the proposed method.  Clarity and Depth: The discussion deeply interrogates the limitations of current methodologies and convincingly argues for the proposed paradigm including rigorous proofs and clear exposition of complex concepts. Weaknesses:  Generalization to Realworld Graphs: The empirical assessment primarily utilizes synthetic graphs generated from stochastic block models. While these models are insightful the paper would benefit from an evaluation on realworld graph datasets to assert the practical applicability of the proposed method.  Comparison with Baselines: The authors acknowledge not focusing on realworld tasks or comparing with existing link prediction methods potentially limiting the immediate understanding of the practical superiority of the proposed approach.  3) Questions and Suggestions  Applicability to Nonstructural Node Features: How does the proposed method adapt to graphs where node features play a significant role in link prediction tasks such as citation networks or social networks with rich attribute information  Scalability and Computational Efficiency: Can the authors comment on the scalability of the proposed gMPNNs method especially in the context of very large graphs and suggest potential optimizations if any  Extension to Positional Embeddings: The paper leaves the study of OOD link prediction with positional embeddings for future work. It would be useful to discuss preliminary ideas on how the current methodology could be adapted for such embeddings.  4) Limitations The authors have identified the scope of their theoretical contributions and empirically validated their approach using synthetic datasets. They could further enhance the paper by providing experimental results on realworld datasets enhancing the generalizability and applicability of their findings. There is also a need for a more detailed discussion on how the proposed method deals with graph attributes or features that are not purely structural. For societal impact while the paper posits no direct adverse outcomes a broader discourse on the implications of enhanced link prediction capabilities in sensitive domains (e.g. privacy in social networks) could enrich the discourse.  Constructive Suggestions for Improvement  Experimental Validation: Augment the empirical evaluation with realworld datasets to bolster the practicality of the proposed methods.  Comparative Analysis: Include comparisons with stateoftheart link prediction methodologies to clearly position the proposed approach within the existing landscape.  Discussion on Node Features: Expand the discussion to include how node attributes beyond structural information could be effectively incorporated within the proposed framework for more comprehensive link prediction tasks.  Societal Impact Considerations: Broaden the discussion on potential societal impacts particularly focusing on privacy concerns and ethical considerations in the application of advanced link prediction methodologies.", "t4vTbQnhM8": " PeerReview for \"Assessing the Quality of Synthetic Data Generators using NonParametric Kernel Stein Discrepancies\"  1. Summary of the Paper The paper introduces a method for quality assessment of synthetic data generators particularly focusing on implicit deep generative models used in machine learning for tasks such as privacy data analysis and data augmentation. The core of this methodology is the nonparametric kernel Stein discrepancy (NPKSD) which leverages a nonparametric Stein operator estimated from synthetic data samples. This approach allows for assessing the goodnessoffit of the synthetic data to the original data distribution. The paper presents theoretical guarantees for NPKSD demonstrates its superiority over existing methods through experiments on synthetic and real datasets and provides an opensource implementation of their method.  2. Assessment of Strengths and Weaknesses Strengths:  Theoretical Contribution: The paper successfully addresses a critical gap in the quality assessment of implicit generative models by proposing a novel NPKSD approach accompanied by theoretical justifications and guarantees.  Practical Relevance: Given the increasing reliance on synthetic data for various machine learning tasks providing a robust method for quality assessment is highly relevant and beneficial for the community.  Experimental Validation: Comprehensive experiments on both synthetic and real datasets effectively showcase the methods power performance compared to existing approaches offering empirical evidence of its superiority. Weaknesses:  Complexity and Accessibility: The methodological complexity and the depth of theoretical discussions might make the paper less accessible to practitioners or researchers from adjacent fields not deeply familiar with kernel methods or Stein discrepancies.  Evaluation Scope: While the experiments affirm the methods efficacy the diversity of datasets particularly in realworld scenarios could be expanded to cover a more extensive range of data types and generative model architectures.  Discussion on Limitations and Societal Impacts: The paper could benefit from a more detailed discussion on the limitations of the proposed method and a broader contemplation on potential negative societal impacts particularly in misuse or overreliance scenarios.  3. Questions and Suggestions for the Authors  Regarding the Theoretical Framework: Could the authors provide more intuition behind the choice of kernel functions and summarize statistics in designing the NPKSD How sensitive is the proposed method to these choices  On Experimental Design: It would be beneficial to see comparisons with more baseline methods and across diverse datasets including more complex realworld scenarios. Could future work explore these aspects  Regarding Limitations: The paper briefly mentions the methods robustness against model misspecification but does not thoroughly explore limitations or failure modes. Could the authors elaborate on situations where NPKSD might underperform or be inapplicable  4. Limitations and Societal Impacts The authors have not fully addressed the papers limitations or the potential negative societal impacts. While the method represents a significant advance its potential misuse in creating misleadingly accurate synthetic datasets especially in sensitive fields like healthcare or finance warrants discussion. Suggestions for improvement include:  Broadening the Discussion on Limitations: A more detailed examination of scenarios where NPKSD could fail or provide less reliable assessments including highdimensional data complexities or extreme data imbalance would be valuable.  Societal Impacts: An explicit section discussing ethical considerations possible misuse and guidelines for responsible use could enhance the papers contribution to fostering a conscientious research environment. Overall the paper makes a commendable contribution to the field of synthetic data generation quality assessment. Addressing the highlighted queries and suggestions would further refine its impact and applicability.", "p3w4l4nf_Rr": "Summary of the Paper and Its Contributions: This paper focuses on the challenge of Nashregret minimization in congestion games a critical subject in game theory with extensive realworld applications such as internet routing traffic management and electrical grids. Specifically the authors study both centralized and decentralized algorithms designed to address the exponential action set problem intrinsic to congestion games. The innovation lies in proposing algorithms with sample complexities that depend only polynomially on the number of players and facilities explicitly detaching from the traditionally exponential dependence on the action set size. The paper introduces three main innovations: 1. A centralized algorithm for congestion games that leverages optimism in the face of uncertainty ensuring enough exploration without dependence on the action set size. 2. A decentralized algorithm employing the FrankWolfe method and Goptimal design to efficiently explore actions in a decentralized setup again showing polynomial sample complexity. 3. Extension of the problem to Markov congestion games and proposing a centralized algorithm that handles nonstationarity inherent in such games effectively. Assessment of Strengths and Weaknesses: Strengths:  Novelty and Theoretical Grounding: The algorithms introduced represent a significant step forward in addressing the inherent complexities of learning in congestion games both from centralized and decentralized approaches.  Broad Applicability: By tackling an issue with wide realworld implications the potential impact of this work is substantial.  Rigor in Analysis: The paper provides a thorough theoretical analysis of the proposed algorithms offering finitesample guarantees and demonstrating polynomial dependency on key parameters. Weaknesses:  Empirical Validation Missing: Despite promising theoretical contributions the paper lacks empirical results. Realworld datasets or simulations that showcase the algorithms performance could offer more holistic insights into their applicability and effectiveness.  Complexity Considerations: The paper while comprehensive in theoretical descriptions might miss addressing the computational complexity and practical deployment challenges of these algorithms in realworld scenarios. Questions and Suggestions for the Authors: 1. Empirical Evidence: Can the authors provide empirical evidence or case studies to demonstrate how these algorithms perform in practical realworld scenarios Including such data would immensely strengthen the papers contributions. 2. Algorithm Scalability: How do the proposed algorithms scale with the increase in the number of players and facilities It would be beneficial to include discussion or analysis on scalability issues. 3. Comparison with Existing Solutions: A more detailed comparison with existing algorithms particularly those that might not have polynomial sample complexity but offer other advantages could be insightful. How do the proposed solutions compare in terms of computation time and resources required 4. Potential for Decentralized Implementation: Given the papers emphasis on decentralized solutions the potential implications or strategies for achieving a sampleefficient decentralized algorithm that guarantees diminishing regret from each players perspective would be a valuable addition. Limitations and Societal Impact: The paper briefly mentions potential negative societal impacts however a more detailed discussion would strengthen the work. Particularly in congestion games modeling traffic management or electrical grids algorithmic decisions can have unforeseen consequences on accessibility fairness and environmental impact. Constructive Suggestions for Improvement:  The authors could enhance the paper by discussing the ethical and societal considerations in deploying such algorithms especially in public resource management scenarios.  An exploration of the potential biases introduced by algorithmic decisions and how they might affect different stakeholders unequally could be valuable. Including a section on fairness and mitigation strategies for potential negative impacts would make the contributions more robust.  The paper would benefit from a section dedicated to discussing practical deployment challenges and computational complexity in detail offering readers a clearer understanding of the algorithms realworld applicability.", "wQVjGP5NbP9": "Review of \"A Simple and Efficient Differentially Private Algorithm for Correlation Clustering\" 1. Summary The paper introduces a novel algorithm for solving the mindisagree version of the correlation clustering problem on unweighted complete graphs with differential privacy (DP) guarantees. This is a significant advancement in unsupervised learning particularly for clustering applications that require privacy preservation such as community detection duplicate detection and automated labeling among others. The algorithm achieves a constant multiplicative approximation and a nearly optimal O(n log2 n) additive approximation for fixed privacy parameters (\\xcf\\xb5 \u03b4) which presents a stronger additive error bound compared to previous works. 2. Assessment of Strengths and Weaknesses Strengths:  Advancement in Privacy Guarantees: The presented algorithm significantly improves upon the additive error margin in correlation clustering with DP approaching nearoptimality up to polylogarithmic factors. This contribution directly addresses a critical gap in prior literature where the additive error bounds were far from the theoretical lower bounds.  Theoretical and Practical Relevance: The problem tackled is of both theoretical interest and practical relevance. Correlation clustering is widely used across various domains and incorporating DP is crucial in numerous realworld applications where data privacy is a concern.  Rigorous Mathematical Foundation: The paper provides a solid theoretical basis for the algorithms effectiveness including proofs of privacy and approximation guarantees. The methods foundation on recent advancements in correlation clustering and differential privacy illustrates a deep understanding of the field.  Contribution to Future Research: Beyond presenting an efficient algorithm the paper lays groundwork for future research by identifying potential improvements and new directions such as exploring the weighted version of the problem addressing the quadratic dependence on 1\\xcf\\xb5 and enhancing the practical applicability of DP algorithms for correlation clustering. Weaknesses:  Practical Scalability Concerns: While theoretically sound the paper acknowledges that the additive approximation term remains problematic for practical scalability potentially limiting the algorithms realworld applicability. This issue is crucial and might hinder adoption in less dense datasets or applications requiring efficient computation on large graphs.  Lack of Experimental Evaluation: The paper does not provide empirical evidence of the algorithms performance on real or synthetic datasets. Experimental evaluation could offer insights into the practical efficacy scalability and how the theoretical bounds translate to realworld scenarios.  Complexity and Implementation Details: While the paper is thorough in its theoretical analysis it offers limited guidance on the practical implementation of the algorithm including computational complexity and resource requirements which are critical for assessing the feasibility of deploying such a solution in practice. 3. Questions and Suggestions for the Authors  Experimental Evaluation: Can the authors provide experimental results to demonstrate the algorithms practical performance on standard datasets It would be particularly interesting to see a comparison with both DP and nonDP correlation clustering algorithms.  Addressing Practical Scalability: Given the acknowledged limitations regarding practical scalability and the problematic additive approximation term what strategies do the authors suggest to mitigate these issues Are there specific modifications or approximations that could enhance realworld applicability  Implementation and Complexity: Could the authors discuss the computational complexity of their algorithm in more detail particularly with respect to the size of the input graph and the DP parameters (\\xcf\\xb5 \u03b4) Furthermore insights into the algorithms resource requirements (e.g. memory CPU) for implementing it in practice would be valuable. 4. Limitations and Societal Impact The authors have outlined the primary limitations regarding practical scalability and the high additive error for certain settings. These limitations could restrict the algorithms applicability in realworld scenarios especially for sparse datasets or applications requiring efficient processing of largescale graphs. Regarding societal impact while the focus on differential privacy is commendable and addresses crucial privacy concerns further discussion on potential misuse or unintended consequences of such technologies would provide a more balanced view. Suggestions for improvement include expanding on methods to enhance the algorithms practicality without significantly compromising privacy guarantees and providing more concrete recommendations for ethical considerations in the deployment of DP algorithms.", "kImIIKGqDFA": " Peer Review for \"Adaptive Gradient Variance Modulator (AGVM) for LargeBatch Training of Dense Visual Predictors\"  1) Briefly summarize the paper and its contributions This paper introduces a novel algorithm termed Adaptive Gradient Variance Modulator (AGVM) designed to effectively train dense visual predictors like object detection and segmentation models with very large batch sizes. The authors identify the problem of gradient variance misalignment across different modules within these complex architectures which hampers performance when trained with large batch sizes\u2014a challenge not adequately addressed by previous largebatch optimization techniques. The AGVM algorithm aligns gradient variances ensuring stable training and improved generalization across various architectures tasks and optimizers. Notably AGVM demonstrates remarkable efficiency enabling training regimes that significantly reduce training time without compromising accuracy as evidenced by experiments on COCO and ADE20K datasets.  2) Assessment of the papers strengths and weaknesses Strengths:  Originality: AGVM addresses the underexplored issue of gradient variance misalignment in largebatch training scenarios offering an innovative solution that is broadly applicable across different architectures and tasks.  Flexibility and Compatibility: The plugandplay nature of AGVM along with its compatibility with multiple optimizers and architectures including CNNs and Transformers represents a significant strength broadening its applicability.  Empirical Validation: Extensive experiments illustrating AGVMs superiority over existing methods provide strong empirical evidence of its effectiveness. The achievement of training dense visual predictors with batch sizes larger than 10K and reducing training time up to 20.9\u00d7 are particularly notable. Weaknesses:  Lack of InDepth Theoretical Analysis: While the paper provides a convergence analysis a more profound theoretical foundation explaining why AGVM works so effectively across different scenarios would have strengthened the paper.  Limited Discussion on Generalisation Beyond Visual Tasks: The paper focuses on dense visual predictors and does not explore or discuss the adaptability of AGVM to other domains of deep learning which could benefit from largebatch training.  Potential for Overfitting on Certain Tasks: While performance metrics are impressive theres a lack of discussion about overfitting risks when applying AGVM to smaller datasets or specific scenarios where model complexity is substantially higher than dataset complexity.  3) Questions and suggestions for the authors  Clarification on Computational Overhead: Could the authors provide more insights into the computational overhead introduced by AGVM especially in comparison to other largebatch training techniques  Generalization to Other Domains: Have the authors considered or tested AGVMs applicability to nonvisual prediction tasks If not this could be an interesting avenue for future work.  Hyperparameter Sensitivity Analysis: How sensitive is AGVMs performance to its hyperparameters (e.g. the momentum update coefficient \u03b1 and interval parameter \u03c4) Detailed sensitivity analysis could help practitioners better understand how to tune AGVM for different tasks.  4) Limitations: Addressing limitations and potential negative societal impact The authors have not fully discussed the limitations of AGVM particularly regarding its computational overhead and potential less efficacy in nonmodular architectures or tasks beyond dense visual predictions. To address this the authors could include a section exploring these limitations and suggesting pathways for future research to overcome them. Furthermore while the potential negative societal impact is briefly mentioned related to the speed training of fraud models a more thorough exploration of ethical implications including privacy and surveillance concerns amplified by faster training times could be beneficial. Constructive suggestions include developing guidelines for responsible use and exploring mechanisms to detect and mitigate misuse of the technology.  Conclusion Overall this paper presents a significant contribution to the field of largebatch training for dense visual predictors offering a novel solution to the gradient variance misalignment problem and showcasing impressive empirical results. While there are areas for further exploration and clarification the work sets a strong foundation for future research and application in largescale deep learning.", "mwIPkVDeFg": "Review of \u201cDimension Independent Communication Complexity Bounds for Distributed Optimization of Overparameterized Models\u201d 1) Summary: This paper addresses an important question in the field of decentralized optimization particularly in the context of training large machine learning models in a distributed manner. The core research problem explored is determining the minimum communication overhead required to train an overparameterized model across distributed nodes while maintaining nearzero training loss. The authors establish that the best possible communication complexity for a class of overparameterized models is \\xce\\xa9(N) notably independent of the problem dimension D. They further propose algorithms for specific overparameterized models achieving nearoptimal communication complexity through linear compression and adaptive quantization techniques. This work is significant as it provides dimensionindependent communication complexity bounds for distributed optimization representing a novel contribution to the literature. 2) Strengths and Weaknesses: Strengths:  Novelty: The paper presents pioneering results in achieving dimensionindependent communication complexity for overparameterized models in a distributed setting. This has significant implications for realworld applications especially in scenarios dealing with very large models and datasets.  Theoretical Depth: The authors provide rigorous theoretical analyses including lower bounds for communication complexity and detailed algorithmic strategies. These results are backed by comprehensive proofs contributing to the papers overall strength.  Practical Relevance: The proposed algorithms potentially reduce the communication overhead drastically which is a major bottleneck in distributed training of machine learning models. This has direct implications for reducing resource consumption and accelerating training times in distributed systems. Weaknesses:  Assumptions: Some of the assumptions such as the specific conditions required for the overparameterized neural network and the particular structure of the network (pyramidal topology) may limit the generalizability of the results. It would be beneficial to discuss the implications if these assumptions are relaxed or not met.  Empirical Evaluation: The paper provides a solid theoretical foundation but lacks empirical evidence to support the theoretical findings. Including experimental results on realworld datasets and comparing the performance of the proposed algorithms against stateoftheart methods could enhance the papers impact.  Discussion on Limitations and Societal Impact: The paper could benefit from a more detailed discussion on potential limitations and the broader societal implications of the proposed methods particularly in terms of any scenarios where the reduced communication complexity might introduce tradeoffs in accuracy or fairness. 3) Questions and Suggestions:  Empirical Validation: Can the authors provide empirical evidence to support the scalability and efficiency of the proposed algorithms Comparing them with existing methods on standard benchmarks could be insightful.  Impact of Assumptions: How would the results change if some of the restrictive assumptions are relaxed A discussion on the robustness of the proposed methods under varying conditions would be valuable.  Extension to Other Models: Are there possibilities to extend the proposed communicationefficient strategies to other types of models beyond the specific overparameterized models discussed Insight into this area could open new research directions.  Societal Impact Consideration: How do the authors anticipate their proposed methods impacting the broader societal context especially considering energy efficiency and the environmental impact of training large models Further elaboration on this could improve the papers relevance. 4) Limitations and Societal Impact: The authors have made significant theoretical contributions but have not adequately addressed the practical limitations regarding the assumptions made and the lack of empirical validation. Additionally the potential negative societal impacts such as biases that may be amplified through the models trained with reduced communication complexity are not discussed. To strengthen the paper the authors could:  Include a section discussing the limitations of their theoretical analysis and the conditions under which their proposed algorithms are most effective.  Provide insights into the practical applicability of their methods including potential challenges in implementation.  Discuss the broader societal implications especially considering energy consumption and environmental impact to provide a holistic view of their works significance. Overall this paper makes a valuable addition to the field of distributed optimization particularly in the context of overparameterized models with promising theoretical results that pave the way for future research and practical applications.", "v1bxRZJ9c8V": " Peer Review  1. Summary of the Paper This paper introduces a novel modeling approach for understanding the continuoustime dynamics of interacting objects leveraging the strength of Gaussian processes (GPs) and latent space representation to achieve this. The authors propose a model that disentangles the independent dynamics of each object and their interactions utilizing latent Gaussian process ordinary differential equations (GPODEs). The model represents each object as a node in a graph with interactions modeled through messages from neighboring nodes. This framework allows for the efficient inference of complex network relationships using modern variational sparse Gaussian process techniques. Empirically the authors demonstrate that their model outperforms traditional neural networkbased models in making reliable longterm predictions and can effectively handle missing information either dynamic or static. Additionally the model\u2019s capacity to decouple independent dynamics from interaction dynamics offers advantages in scenarios requiring extrapolation.  2. Assessment of the Paper Strengths:  The paper addresses a significant gap in current dynamical systems modeling by providing a sophisticated approach that bridges the gap between Gaussian process models and graph neural networks (GNNs).  The proposed model\u2019s ability to impart reliable uncertainty estimates significantly advances over current neural networkbased alternatives that fail to provide such measures.  It demonstrates a methodological improvement in handling missing information a common issue in realworld datasets enhancing the models applicability.  The empirical evaluation is thorough comparing the model with several baselines across different datasets and scenarios underpinning the model\u2019s strengths in various setups. Weaknesses:  Despite the methodological advancements the computational complexity especially for very largescale systems is not fully addressed. The scalability of the proposed variational sparse Gaussian process inference techniques in extremely complex or highdimensional interaction networks remains an open question.  While the paper demonstrates the efficacy of the proposed model through empirical evaluations a deeper theoretical analysis on the convergence properties stability of the solution and sensitivity to the hyperparameters and model configurations would strengthen the foundation of the proposed approach.  The realworld applicability of the model beyond the presented scenarios is discussed in limited detail. Case studies or applications in specific domains (e.g. autonomous driving robotics) could elucidate the practical impact and limitations of the model.  3. Questions and Suggestions for Authors  Question regarding scalability: How does the models performance and computational requirements scale with the complexity and size of the interaction networks Any insights or preliminary results on extremely large or complex systems would be valuable.  Theoretical underpinnings: Can the authors provide more detail on the theoretical analysis of the models properties such as stability and sensitivity analysis This would help in understanding the conditions under which the model is expected to perform well.  Extension to other domains: Have the authors considered extending the model to specific realworld applications and if so what modifications or considerations are necessary to adapt the model for such purposes  Suggestion for improvement: Incorporating a discussion on how domainspecific knowledge can be integrated into the model could be beneficial. For example defining interaction kernels based on physical laws relevant to the system being modeled could potentially improve the models predictive capabilities and interpretability.  4. Limitations and Broader Impact The authors have not fully explored the limitations regarding the scalability of their model to extremely large systems and the potential negative societal impact of their work. Specifically they should consider the following suggestions for improvement:  Scalability considerations: A deeper investigation into computational efficiency improvements perhaps through more advanced sparse approximation techniques or leveraging parallel computing frameworks could significantly enhance the model\u2019s scalability.  Societal impact discussion: A section dedicated to the potential negative societal impacts especially in critical applications like autonomous driving where erroneous longterm predictions could have severe consequences would be informative. Providing guidelines on responsible use and potential safeguards could mitigate some of these concerns. In conclusion the paper presents a significant advancement in modeling continuoustime dynamics of interacting objects with uncertainty awareness demonstrating notable potential for various applications. Addressing the proposed questions and suggestions could further elevate the works contributions and applicability.", "lAN7mytwrIy": " Review of \"ElasticMVS: SelfSupervised MultiView Stereopsis with Elastic Part Representation\"  1) Summary of the Paper This paper introduces a novel approach to selfsupervised multiview stereopsis (MVS) named ElasticMVS which significantly enhances the prediction of dense surface geometry from a set of images without requiring ground truth 3D data for training. The core innovation is the introduction of an elastic part representation that capitalizes on the inherent geometric correlations\u2014such as surface connectedness and occlusion boundaries\u2014implicitly inferred from images to guide pixelwise multiview correspondences more reliably especially in challenging textureless or reflectant areas. The method utilizes a contrastive learningbased strategy to train the pixelwise part representation and employs a partaware propagation and evaluation scheme to iteratively optimize depth estimates. Experiments demonstrate the superior performance of ElasticMVS over both supervised and selfsupervised counterparts in terms of reconstruction accuracy and completeness especially on largescale benchmark tasks.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The elastic part representation approach is novel and addresses a significant gap in the literature by effectively utilizing geometric correlations inferred from images for better MVS depth estimation.  Outstanding Performance: ElasticMVS outperforms existing methods on various benchmarks demonstrating superior reconstruction accuracy and completeness particularly in challenging largescale scenes.  SelfSupervised Learning: By leveraging a selfsupervised learning strategy the method overcomes the limitation of needing ground truth 3D data for training making it highly applicable in realworld scenarios where such data are scarce. Weaknesses:  Computational Complexity: While the paper touches upon memory efficiency and computational aspects it doesnt provide extensive details on the computational complexity or runtime efficiency especially when scaled to very large datasets or realtime applications.  Generalization to Diverse Environments: While the paper shows impressive results on the DTU and Tanks and Temples datasets it lacks detailed analysis of the method\u2019s generalization across diverse environments including highly dynamic scenes or those with extreme lighting conditions.  3) Questions and Suggestions for the Authors  Generalization Capability: Could the authors provide insights or additional experimental results on how ElasticMVS performs in more dynamically varied environments including those with significant lighting changes or moving objects  Comparative Analysis: A deeper comparative analysis with the closest competing methods might help understand the specific scenarios where ElasticMVS particularly excels or falls short.  Scalability and Efficiency: Given the method\u2019s computational needs can the authors elaborate on its scalability particularly in terms of processing time and memory consumption in realtime or largescale applications Suggestions for future work could include optimizing the architecture for improved computational efficiency.  4) Limitations and Societal Impacts The authors have made considerable efforts to address the limitations of current MVS methods by introducing a selfsupervised learning framework that does not rely on groundtruth 3D data. However the paper could benefit from a more explicit discussion about the computational requirements and potential challenges in deploying ElasticMVS in realtime or on lowpowered devices. Moreover theres limited discussion on the potential negative societal impacts of the technology. While the paper focuses on technical innovation and performance improvement future versions could benefit from addressing how the technology might be misused or raise privacy concerns especially as it pertains to surveillance or data security. Constructive suggestions include exploring lightweight versions of the model for increased accessibility and efficiency and setting ethical guidelines for the use of such advanced reconstruction technologies.", "tUH1Or4xblM": "The paper presents a novel approach to video object segmentation that focuses on the automatic discovery tracking and segmentation of multiple moving objects in videos while also competently handling their mutual occlusions. The paper introduces an ObjectCentric Layered Representation (OCLR) model which uniquely employs optical flow information through a transformer architecture to segment objects and infer their depth ordering. This approach allows for effective handling of complex scenarios such as occlusions and varying object movements. Key contributions include the development of a synthetic data generation pipeline for training the model thorough ablation studies confirming the models capabilities and demonstrating superior performance on standard benchmarks through the innovative use of synthetic training data alone. Furthermore the paper explores testtime adaptation with RGB information significantly enhancing model performance. Strengths: 1. The OCLR models innovative use of optical flow and transformers to handle multiobject segmentation and occlusions is a significant advancement. 2. The synthetic data generation pipeline addresses the challenge of requiring large manually annotated datasets. It promotes efficient training and contributes towards solving the Sim2Real generalization issue. 3. Comprehensive ablation studies clearly validate the models effectiveness in learning critical aspects such as object permanence and temporal shape consistency. 4. The work stands out for its stateoftheart performance on several benchmarks achieved without relying on manual annotations. The testtime adaptation further boosts performance showcasing the methods robustness and adaptability. Weaknesses: 1. The reliance on optical flow alone might limit the models ability to detect stationary objects over extended periods as acknowledged by the authors. 2. While the proposed solution significantly reduces the need for manual annotations the creation of a synthetic data pipeline and its optimization may present its own set of challenges. 3. The global layer ordering assumption might not be ideal for all scenarios especially those involving complex interactions between objects or changes in their relative positions. Questions and Suggestions for the Authors: 1. Could the authors elaborate on the potential extensions of their model to incorporate RGB information more deeply into the pretraining process 2. It would be beneficial to discuss the limitations and potential challenges in generating and optimizing synthetic data pipelines for training. 3. A further exploration of how the model might handle scenarios where global layer ordering is insufficient would be insightful. Is there ongoing work to address these more complex interactions Limitations and Societal Impact: The paper acknowledges the limitations related to the models dependence on optical flow and its performance with stationary objects. A suggestion for improvement would be a deeper integration of RGB data to complement optical flow possibly enhancing the models capability to handle stationary objects better. Another area for enhancement could involve refining the layered model to account for more complex object interactions. While the paper briefly mentions potential societal impacts through its contributions to video object segmentation a more detailed discussion on this aspect would be valuable. For example addressing concerns related to privacy and surveillance particularly when applied to realworld video data could provide a more comprehensive view of the technologys societal implications. The works focus on reducing manual annotation efforts is commendable potentially leading to resourceefficient advancements in video analysis technologies.", "pHdiaqgh_nf": "Review of \"Reconstructing Complex Image Stimuli from fMRI Signals with Multimodal Deep Learning\" 1. Summary: The paper presents a novel approach to reconstruct complex images from human brain signals measured through functional magnetic resonance imaging (fMRI). Unlike prior works that focus on simple shapes or single objects this study aims to decode images rich in semantics and akin to everyday visuals. The authors introduce a methodology that leverages an additional text modality addressing the challenge of data scarcity in fMRI datasets. By mapping fMRI signals to a prealigned visionlanguage latent space and utilizing a generative model conditioned on this space the paper demonstrates the reconstruction of photorealistic images that accurately capture ground truth content. The work contributes to our understanding of visual decoding in the brain highlights the importance of multimodal models in explaining brain functions and proposes an efficient solution to the challenge of data scarcity in fMRI image reconstruction. 2. Assessment: Strengths:  Novelty and Relevance: The paper addresses a significant challenge in neuroscience with an innovative approach. The use of a text modality and prealigned visionlanguage latent space is a creative solution to the data scarcity and semantic complexity of the task.  Technical Soundness: The methodology is welldesigned leveraging stateoftheart deep learning techniques and multimodal embeddings. The incremental approach to model training and the detailed evaluation of each component demonstrate technical rigour.  Quality of Results: The results including photorealistic image reconstructions and microstimulation studies are impressive. They not only show the potential of the proposed pipeline but also open new avenues for understanding brain functions. Weaknesses:  Generalizability: The paper focuses on data from a single subject which raises questions about the models applicability across different individuals. Further studies on a broader dataset are needed to validate the findings.  Interpretability: While the results are promising the interpretability of the generated images and the underlying model decisions could be enhanced. Understanding why certain reconstructions are more faithful than others would provide deeper insights into the decoding process.  Potential for Bias: The reliance on pretrained models and datasets introduces a risk of bias. The paper does not fully address how biases in these external resources could impact the reconstruction quality or the ethical implications thereof. 3. Questions and Suggestions:  How does the model perform across diverse subjects Future work should explore the variability in brain signal decoding across different individuals considering factors like age gender and neurological diversity.  Can interpretability be improved Techniques such as attention mechanisms or explainable AI methods could offer more insight into the models decisionmaking process enhancing the scientific understanding of brain signal decoding.  Addressing Potential Biases: The authors should investigate the impact of biases in the pretrained models and datasets on the reconstructions. Proposing methods to mitigate these biases would improve the reliability and ethical standing of the research. 4. Limitations and Societal Impact: The authors acknowledge the current limitations regarding generalizability and the potential for bias due to the use of pretrained models. However more detailed exploration and proactive measures to address these issues would strengthen the paper. Ethical considerations especially concerning privacy and the potential misuse of brain signal decoding technology require further discussion. Suggestions for improvement include conducting a more diverse and inclusive study implementing bias mitigation techniques and engaging in a thorough ethical review of the technologys applications. In conclusion this paper makes a significant contribution to the field of neuroscience and image reconstruction from fMRI signals. It proposes an innovative approach that paves the way for further research. Addressing the outlined questions and concerns will enhance its impact and applicability.", "ob8tk9Q_2tN": " Peer Review for \"MinAM: A Minimal Memory Anderson Mixing Method for Convex and Nonconvex Optimization\"  1. Summary of the Paper This paper introduces MinAM a novel variant of the Anderson Mixing (AM) method designed to address the significant memory requirements of AM when solving largescale optimization issues. By retaining only one recursively modified vector pair MinAM achieves the minimal memory footprint among AM methods which is crucial for applications on resourceconstrained devices. Through thorough theoretical analysis the authors establish the local convergence rate of MinAM for strongly convex quadratic optimization and extend it to generalize nonlinear optimization problems and stochastic optimization settings. The effectiveness of MinAM is demonstrated via experiments on logistic regression and neural network training problems showing its competitive performance with established quasiNewton methods and superior memory efficiency.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The MinAM method is a significant advancement in the field of optimization particularly addressing the computational and memory efficiency challenges in largescale problems. The minimal memory requirement is a critical improvement for applications in resourcelimited environments.  Theoretical Foundation: The authors provide a solid theoretical basis for MinAM showing its equivalence to fullmemory TypeI AM for strongly convex quadratic problems and establishing convergence properties for more general scenarios.  Practical Appeal: By combining theoretical rigor with practical experiments the paper demonstrates the realworld applicability and effectiveness of MinAM making it a valuable tool for researchers and practitioners in optimization. Weaknesses:  Limited Comparison in Experiments: While the experimental results are promising the comparison with a broader range of existing optimization methods especially the latest developments in the field could further validate the claimed advantages of MinAM.  Potential Overfitting to Specific Problem Sets: The chosen logistic regression and neural network training problems are standard however the performance of MinAM on a wider variety of problem types and datasets could be more thoroughly evaluated to ensure its generalizability.  Discussion on Parameter Sensitivity: The paper could benefit from a deeper discussion on the sensitivity of MinAMs performance to the choice of mixing parameters and how adaptive choices are made in practice offering more guidance to users of the method.  3. Questions and Suggestions for the Authors  Could the authors elaborate on how MinAMs performance vary with changes in the mixing parameter Any practical guidelines for parameter selection would be useful.  To further showcase MinAMs advantages consider including a comparison with more recent and related optimization methods particularly those designed for similar largescale problems.  A discussion on potential strategies to ensure the robustness of MinAM across a broader spectrum of optimization problems would be valuable. Are there specific problem characteristics where MinAM is expected to underperform  It would be interesting to see a more detailed analysis or case studies on the implementation of MinAM in realworld resourceconstrained environments.  4. Limitations and Constructive Suggestions for Improvement The authors have provided a robust theoretical analysis of MinAM\u2019s effectiveness and its memory efficiency. However the paper could be strengthened by addressing the variability in performance due to parameter settings and extending the experimental validation to cover a wider range of optimization problems and settings.  Extension to Additional Problems: Testing MinAM on a broader set of optimization challenges including more diverse nonconvex problems could better illustrate its versatility and limitations.  Parameter Sensitivity Analysis: Including a detailed sensitivity analysis of the mixing parameters would significantly benefit users of MinAM by providing practical insights into how parameter choices affect performance.  Potential Negative Societal Impacts: While not discussed any optimization method could have negative impacts depending on its use case (e.g. facilitating more effective adversarial attacks in machine learning). A brief discussion on ethical considerations and safeguards against misuse would contribute to a more rounded paper. In summary MinAM presents a significant advancement in minimizing memory requirements for optimization problems supported by solid theoretical and experimental work. Addressing the suggested areas could further enhance its contribution and applicability in the optimization domain.", "xjXN3wEvCGG": "The paper under review presents a novel approach for learning temporal task specifications from expert demonstrations specifically focusing on tasks represented by Deterministic Finite Automata (DFA). Learning such specifications is challenging due to the potentially infinite number of tasks unknown requirements for task memory and a discrete solution space. The authors introduce the Demonstration Informed Specification Search (DISS) an algorithmic family that alternates between hypothesizing labeled examples to address the demonstrations and sampling tasks consistent with these examples. This technique only requires blackbox access to a maximum entropy planner and a task sampler from labeled examples. The approach is verified through a concrete implementation for DFAs and demonstrates the ability to efficiently identify tasks with minimal expert demonstrations. Strengths: 1. Innovativeness: The proposed DISS framework addresses a significant gap in learning from demonstration (LfD) especially in identifying temporal task specifications without explicit task representation. 2. Problem Statement Clarity: The paper does a commendable job in laying out the challenges in learning temporal task specifications and situates DISS effectively within this context. 3. Theoretical Foundation: The method is wellgrounded in theory with a thorough discussion on the combination of maximum entropy planning and grammatical inference to tackle the problem. 4. Practical Relevance: Offered solution has practical relevance especially in domains like robotics and autonomous systems where task learning from demonstrations is paramount. 5. Empirical Evaluation: The experimental setup and comparison with baseline approaches effectively demonstrate DISSs efficacy. Weaknesses: 1. Complexity and Scalability: The paper does not sufficiently address the computational complexity of the DISS algorithm especially concerning the size of the state space and the number of demonstrations. 2. Generalizability: While the focus is on DFAs its unclear how DISS can be extended to more complex or different types of automata without significant modifications. 3. Implementation Details: While an opensource implementation is provided the paper could benefit from a deeper dive into practical implementation challenges and considerations. 4. Comparison with StateoftheArt: While comparisons are made with baseline approaches a broader comparison with stateoftheart methods in task and policy learning from demonstrations could strengthen the paper. Questions and Suggestions for Authors: 1. Complexity Analysis: Can the authors provide a complexity analysis of the DISS algorithm especially how it scales with the state space size and demonstration number 2. Generalizability and Limitations: How can DISS be generalized to handle more complex automata or specifications beyond DFAs What limitations might arise in such scenarios 3. Hyperparameter Sensitivity: The paper introduces several hyperparameters (e.g. temperature settings in simulated annealing). How sensitive is DISS to these parameters and what strategies are recommended for their selection 4. Realworld Applications: Could the authors provide insights or case studies on the application of DISS in realworld scenarios particularly in robotics or humanrobot interaction Limitations and Societal Impact: The paper acknowledges limitations in scope by focusing on DFAs and mentions the potential of extending the approach to more comprehensive automata types. However a more detailed discussion on the limitations especially related to computational resources would be beneficial. Concerning societal impact while the paper claims positive implications an elaboration on potential negative implications such as misuse in automating tasks without considering ethical or societal norms could provide a balanced view. Suggestions include incorporating ethical guidelines for use and a broader discussion on responsible applications of DISS in sensitive domains.", "mhQLcMjWw75": "Review for \"Federated Prototypewise Contrastive Learning (FedPCL) for Efficient Representation Fusion in Federated Learning\" 1) Summary: This paper introduces a novel approach within the Federated Learning (FL) paradigm termed Federated Prototypewise Contrastive Learning (FedPCL) to address the computational and communication challenges imposed by training largescale models. Instead of training from scratch FedPCL leverages multiple fixed pretrained models as backbones enabling lightweight efficient FL by fusing these models representations for personalized client models. The core innovation lies in the utilization of classwise prototypes as a means to share knowledge across clients with a focus on extracting clientspecific and classrelevant information effectively. The proposed approach not only reduces the computational and communication burden significantly but also showcases superior performance in knowledge integration and personalization over several baselines across various benchmark datasets. 2) Strengths and Weaknesses:  Strengths: 1. Innovation in Framework Design: The paper proposes a creative solution to the scalability and efficiency challenges of FL by incorporating pretrained models and classwise prototypes. This approach significantly deviates from conventional FL methodologies offering a fresh perspective on addressing these challenges. 2. Effective Knowledge Sharing: By sharing prototypes instead of parameters or raw data FedPCL effectively captures classrelevant information while preserving privacy and minimizing communication overhead. 3. Extensive Evaluation: The authors conduct comprehensive experiments that demonstrate the efficacy of FedPCL in fusing representations from pretrained backbones and in achieving better personalization. The comparison with stateoftheart FL algorithms across multiple settings and datasets provides a robust validation of the proposed method.  Weaknesses: 1. Limited Scope of Tasks: The current study primarily focuses on vision tasks. The applicability and performance of FedPCL in other domains such as natural language processing or timeseries analysis remain unexplored. 2. Potential Bias Towards Pretrained Models: The reliance on fixed pretrained models may introduce biases or limitations inherent in those models potentially affecting the universality and adaptability of the approach. 3. Lack of Detailed Discussion on Prototype Selection: While the usage of prototypes is a central element of FedPCL the paper could benefit from a more indepth discussion on the criteria for prototype selection and its impact on overall performance. 3) Questions and Suggestions: 1. On Generalization to Other Domains: How do the authors envision adapting FedPCL for domains other than vision considering different data modalities and tasks 2. Impact of Pretrained Model Selection: Could the authors elaborate on the sensitivity of FedPCLs performance to the choice of pretrained models and how to best approach this selection process 3. Prototype Selection and Evolution: How are prototypes initially selected and do they evolve during training Providing insights or algorithms for dynamic prototype selection could enhance understanding and practical application. 4. Suggestion for Future Work: It would be valuable to explore the integration of FedPCL with other privacypreserving techniques such as differential privacy to assess and further enhance the privacy guarantees. 5. Explainability Inquiry: Considering FedPCL\u2019s prototypebased methodology is there potential to leverage this for explainable AI in federated settings offering insights into model decisions at the client level 4) Limitations and Societal Impact: The authors mention the potential negative societal impact due to biases in pretrained models but do not delve deeply into mitigating strategies. An exploration of how different foundation models could introduce or perpetuate biases and specific measures to counteract such effects would strengthen the paper\u2019s considerations of societal impact. Furthermore expanding the discussion on the limitations of applying FedPCL to tasks beyond vision and potential challenges in prototype management as the system scales would offer a more rounded view of the framework\u2019s practical applicability and limitations. Suggestions for Improvement: 1. Augment the discussion on addressing biases from pretrained models and prototype selection to enhance the model\u2019s fairness and robustness. 2. Investigate and describe potential pathways for extending FedPCL\u2019s application to a wider range of tasks and data modalities. 3. Provide a more thorough examination or future work direction on integrating FedPCL with additional privacypreserving mechanisms.", "nzuuao_V-B_": "Summary and Contributions: The paper systematically reevaluates stateoftheart serverside input recovery attack methods in the context of federated learning (FL) specifically focusing on batch image reconstruction across a variety of models. The study introduces implicit and explicit model variations to explore the differential impact on the vulnerability against input recovery attacks. Implicit variations pertain to changes within the same architecture like BN modes and training epochs whereas explicit variations refer to structural changes in the model such as skip connections and channel size. The novel contribution of this work lies in proposing the angular Lipschitz constant of model gradients as a predictive measure of a models vulnerability to input recovery attacks. This measure leverages a theorem on the convergence of attackers\u2019 gradient matching optimization and is designed to be scaleinvariant providing a tool for developing clientside defense strategies especially in scenarios where global model information is limited (blackbox settings). Strengths: 1. Novelty: The introduction of the angular Lipschitz constant as a predictive measure for assessing vulnerability to input recovery attacks in FL represents a significant advancement in the field. 2. Comprehensive Analysis: By covering a broad spectrum of models and introducing both implicit and explicit model variations the paper provides a thorough examination of the factors influencing the effectiveness of input recovery attacks. 3. Practical Implications: The study directly addresses the pressing issue of privacy in FL offering a tool (angular Lipschitz constant) that can be utilized for enhancing clientside defenses against privacy breaches. 4. Empirical Validation: The results demonstrating the strong monotonic correlation between the proposed measure and both the loss drop during optimization and the quality of reconstructed images substantiate the theoretical claims made in the paper. Weaknesses: 1. Generalization of Results: The paper focuses mainly on image data and convolutional neural network (CNN) models. It would be beneficial to see if the findings generalize to other types of data and model architectures. 2. Evaluation of Defensive Strategies: While the paper proposes a measure for assessing vulnerability it stops short of evaluating or suggesting specific defensive strategies that leverage this measure. 3. Discussion on Societal Impact: The paper does not delve into the potential negative societal implications of both the vulnerability to input recovery attacks in FL and the proposed measures. Questions and Suggestions: 1. Expansion to Other Data Types: Have the authors considered extending their evaluation to include nonimage data and models other than CNNs If not this could be a valuable direction for future work. 2. Defensive Strategies: Could the authors suggest specific defensive mechanisms or modifications to FL protocols that could incorporate the angular Lipschitz constant to enhance privacy 3. Societal Impact: The authors are encouraged to include a discussion on the potential negative societal impacts particularly focusing on scenarios where the proposed measure might fail or be misused. Limitations and Societal Impact: The authors acknowledge the limitations regarding the focus on image data and CNN models indicating the need for future research to explore other data types and architectures. However they have not adequately addressed the potential negative societal impacts of their work. For constructive suggestions it would be beneficial to include a discussion on the broader implications of input recovery attacks in FL the effectiveness of proposed defenses in realworld scenarios and the ethical considerations surrounding the use of such measures.", "xT5rDp5VqKO": "Peer Review for the Submitted Manuscript 1) Brief Summary: The paper presents a fascinating investigation into the realm of neuromorphic signal processing specifically focusing on the concept of coincidence detection as found in Rosenblatts classic perceptron models. It challenges the contemporary reliance on deep learning for pattern recognition by demonstrating that neuromorphic methods when applied on spiking neural network hardware with neuromodulatory mechanisms can potentially outperform deep learning techniques in accuracy. Through a comparative analysis against a 26 layer convolutional neural network (ResNet26) in classifying pathogenic bacteria the authors have shown that their method not only achieves higher accuracy but also significantly reduces the computational resources and time needed for training. This work sheds light on the potential of revisiting and integrating classical neuromorphic methods with modern computing capabilities to address current computational challenges. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The paper takes a novel stance by comparing classical neuromorphic methods with stateoftheart deep learning providing fresh insights into the domain of pattern recognition.  Impressive Experimental Results: The superiority of the coincidence detection method over deep learning in terms of both performance and efficiency is welldocumented and supported by thorough experimental analysis.  Relevance and Timeliness: Given the increasing demand for efficient computational methods the paper\u2019s exploration of lowresource alternatives is both relevant and timely.  Clarity and Depth of Presentation: The paper is wellwritten with clear explanations of concepts and methodologies that are accessible to readers with varying levels of expertise. Weaknesses:  Limited Scope of Comparison: The paper primarily compares the proposed method with a specific deep learning model (ResNet26) on a single dataset. Expanding the comparison to include various deep learning models and datasets would provide more robust evidence of the methods effectiveness.  Approximation Limitation: The authors acknowledge using an approximation for coincidence detection which might limit the generalizability of their findings. A deeper exploration of the impact of this approximation would strengthen the paper.  Potential Negative Societal Impact: The discussion regarding the societal implications of the proposed method is absent. Given the sensitivity and potential severity of the papers context (identifying pathogenic bacteria) addressing any ethical or societal risks is crucial. 3) Questions and Suggestions for the Authors:  Could the authors elaborate on the choice of the logistic regression model and its implications for the robustness of their results  How does the performance of the proposed method change with variations in the dataset size and complexity  It would be beneficial if the authors could provide a more detailed comparison with other deep learning methods and on datasets from different domains to validate the generalizability of their findings.  Are there specific areas or applications where the authors envision their method would have the most impact considering its strengths and limitations 4) Limitations and Potential Negative Societal Impact: The paper acknowledges the use of an approximation in the implementation of coincidence detection which is a thoughtful inclusion. However there is a notable absence of discussion regarding the potential negative societal impacts especially in applications critical to public health. As the method involves classification of pathogenic bacteria incorrect classifications could have severe implications. It is crucial for future iterations of this work to discuss these aspects incorporating risk assessments and suggesting protocols to mitigate potential adverse effects. Furthermore expanding the limitations discussion to include the models scalability adaptability to different contexts and its behaviour under diverse and noisy datasets would provide a wellrounded insight into its practical applicability. Recommendations for Improvement:  Include a broader comparative analysis with multiple deep learning models and datasets.  Provide a more detailed examination of the approximations impact on the models performance.  Integrate a segment discussing the ethical considerations and potential societal risks associated with the method\u2019s application along with strategies for mitigation.  Explore and discuss the methods scalability and adaptability to various realworld scenarios.", "zbuq101sCNV": "Review: 1) Summary of the Paper: The paper introduces TANGO an endtoend neural architecture for textdriven photorealistic 3D style transfer. By leveraging the pretrained Contrastive LanguageImage Pretraining (CLIP) model TANGO stylizes a given 3D mesh according to a text prompt disentangling the task into learning spatially varying bidirectional reflectance distribution function (SVBRDF) local geometric variation and lighting conditions. The process is facilitated by a spherical Gaussians based differentiable renderer allowing for photorealistic rendering effects even on lowquality meshes. The system demonstrates superiority over existing methods like Text2Mesh showcasing an ability to produce more realistic renditions with no geometric selfintersections robust performance on sparse meshes and support for complex reflectance and lighting scenarios. 2) Assessment of Strengths and Weaknesses:  Strengths:  Innovative Approach: The disentanglement of the style transfer problem into SVBRDF local geometric variations and lighting conditions under the supervision of the CLIP model is novel and impactful leading to highquality photorealistic renderings.  Quality of Results: The paper shows through extensive experiments that TANGO can achieve high fidelity in stylization surpassing prior works in realism and consistency across various 3D geometries and stylizations.  Generalization: The ability to work effectively with lowquality meshes without requiring additional taskspecific training data is a significant advantage broadening the applicability of the approach.  Weaknesses:  Simplified Shading Model: The simplified approach towards shading particularly in handling indirect lighting effects might limit the realism that can be achieved in complex scenes. This simplification may affect the models ability to generalize across different lighting scenarios.  Computational Performance: While TANGO shows promising results the computational performance and the resources required (e.g. training on an Nvidia RTX 3090 GPU) might limit its accessibility and realtime applications. 3) Questions and Suggestions for Authors:  Model Complexity: Can the authors provide insights on the computational complexity of TANGO especially in comparison to existing methods like Text2Mesh Details on runtime for stylization tasks would be beneficial.  Indirect Lighting: Have the authors considered incorporating more sophisticated models for indirect lighting to address the noted limitation Suggestions on how this could potentially be integrated into the TANGO framework would be intriguing.  Societal Impact Discussion: The possibility of displacing junior artists is mentioned as a negative societal impact. Could the authors expand on potential measures to mitigate such impacts perhaps through reskilling initiatives or adapting the technology to assist rather than replace human artists 4) Limitations and Societal Impact: While TANGO advances the state of the art in textdriven 3D mesh stylization its limitations around the simplified shading model primarily regarding indirect lighting may hinder its utility for complex scenes requiring multiple light bounces. This limitation along with resourceintensive requirements should be addressed in future work to enhance accessibility and generalizability. The potential negative societal impact notably the risk to junior artists employment opportunities is acknowledged yet the paper could further explore ethical considerations and mitigation strategies. Constructively TANGO and alike technologies could be positioned as tools to augment creativity and efficiency in the arts and entertainment industries promoting synergies between artificial intelligence and human creativity.", "lJx2vng-KiC": " Peer Review  1) Brief Summary of the Paper This paper presents a comprehensive analysis of learning dynamics in deep linear networks focusing on the role of initial network weights and prior knowledge. By extending Fukumizu\u2019s matrix Riccati solution the authors derive exact analytical expressions for various aspects of network behavior over time including network output representational similarity and the neural tangent kernel. This work reveals that certain initializations can lead to fast learning dynamics without compromising the quality of learned representations. It also discusses the difference in learning dynamics between rich and lazy regimes and provides insights into continual learning reversal learning and the revision of structured knowledge. Overall this paper contributes to our understanding of how prior knowledge embedded in initial weights influences learning in deep neural networks.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Advancement: The paper significantly advances the theoretical understanding of the impact of initialization on learning dynamics in deep neural networks. The analytical solutions provided are a notable strength.  Broad Implications: By connecting theory with practice the results have wideranging implications for areas like continual learning and structured knowledge offering valuable insights for both.  Quality of Analysis: The mathematical rigor and the clarity in deriving the solutions contribute to the paper\u2019s quality. The consideration of various learning scenarios (rich and lazy regimes continual learning etc.) enriches the discussion.  Comprehensive Simulation: The simulation and analysis supported by the online availability of code enhance the reproducibility of the papers results and allow others to build upon this work. Weaknesses:  Limited to Deep Linear Networks: The generalization of these findings beyond deep linear networks to more complex nonlinear structures remains an open question. This limitation might restrict the practical implications of the results.  Assumption Constraints: The assumptions made in deriving the solutions such as whitened inputs and zerobalanced weights may not always hold in realworld scenarios potentially limiting the applicability of the findings.  Potential for Numerical Instability: While the authors mention numerical stability in deriving the solutions further details or a more thorough exploration of potential instabilities or limitations in the computational approach would be beneficial.  3) Questions and Suggestions for the Authors  Generalization to Nonlinear Networks: Could the authors elaborate on potential methods or approaches to extend these results to more complex nonlinear network models  Assumption Relaxation: Are there any initial steps or preliminary results on relaxing some of the assumptions (e.g. whitened inputs zerobalanced weights) to cover a broader range of realworld scenarios  Numerical Stability: Could the authors provide additional insights or detailed examples where numerical instability might arise and how they propose to mitigate such issues in future work  4) Limitations and Societal Impact The authors have made a commendable effort in discussing the limitations of their work specifically its applicability primarily to deep linear networks and the reliance on certain assumptions. To further this discussion exploring the extension of these results to more general nonlinear network architectures would be highly valuable as most practical applications of deep learning utilize such models. Regarding potential negative societal impacts the paper primarily focuses on theoretical underpinnings and does not delve into specific applications that might lead to negative societal implications. However as the research progresses towards practical applications it would be beneficial for future work to consider and address potential ethical considerations especially in sensitive areas such as privacy surveillance and autonomous decisionmaking systems where biased or rapid forgetting of crucial knowledge could have significant consequences. Constructively the paper could improve by providing a clearer roadmap or suggestions for overcoming the discussed limitations perhaps by outlining specific research directions that could explore the applicability of these findings in more complex network architectures or under less restrictive assumptions.  This review appreciates the papers contributions while suggesting areas for improvement and further exploration to enhance its impact and applicability in advancing the understanding of learning dynamics in deep neural networks.", "zKBbP3R86oc": " Review of \"SMOL: Learning Heterogeneous Precisions in Neural Networks for Efficient Deployment\"  1) Summary of the Paper and Its Contributions This paper addresses the challenge of training deep neural networks (DNNs) with quantized parameters and activations to reduce energy consumption and inference time pivotal for deploying models on resourceconstrained devices. The authors propose SMOL a novel method that enables the learning of heterogeneous precisions at the most granular level\u2014per weight and per activation\u2014within a network. This is achieved through a designed framework that approximates the intractability of optimizing discrete precisions by training perparameter noise magnitudes. The framework expands to include learning precisions for hidden state activations. The empirical results demonstrate that SMOL outperforms existing stateoftheart quantization techniques across various benchmarks including CNNs on CIFAR and ImageNet and GANs showcasing the utility and efficiency of learning diverse precision assignments.  2) Assessment of the Papers Strengths and Weaknesses Strengths:  The proposed SMOL approach introduces a principled and modular framework for learning heterogeneous precisions which is a significant step forward in optimizing DNNs for deployment on energyconstrained devices.  By incorporating precision learning directly into the training process rather than as a posttraining adjustment SMOL ensures better coherence between the training and inference phases potentially leading to performance gains.  The methodological enhancements allow for perparameter precision assignments offering a more refined and granular control over model quantization compared to perlayer approaches which is shown to be beneficial in the experimental results.  The paper is wellstructured providing a comprehensive contextual background clear explanations of the proposed method and thorough experimental setups that demonstrate the approachs effectiveness across different models and datasets. Weaknesses:  While the paper discusses the potential for energy efficiency and reduced computational costs there is limited concrete analysis on the actual hardware requirements and the feasibility of implementing such finegrained precision control in current hardware architectures.  The scalability of the approach to even larger models and datasets beyond those tested could benefit from further exploration especially as the demand for larger more complex models continues to grow.  The paper could benefit from a more extensive discussion on the broader implications of adopting heterogeneous precision approaches including potential challenges in standardization and interoperability across different hardware platforms.  3) Questions and Suggestions for the Authors  Could the authors provide more insights into the hardware requirements for implementing the finegrained precision control proposed in SMOL For instance what are the specific demands on memory architecture and computational units  It would be interesting to see how SMOL performs on larger more recent models beyond those tested such as larger versions of GPT or BERT. Could the authors comment on potential limitations or considerations when applying SMOL to such models  A discussion on how the proposed method could integrate with or complement other network compression techniques such as sparsityinduced pruning or lowrank factorization might add valuable context and insights for future research directions.  4) Addressing Limitations and Societal Impact The authors have made an earnest effort to address the limitations of their work by discussing potential hardware implementation challenges and providing an open avenue for future research on adapting SMOL to various architectures. However more detailed analysis on the robustness of models quantized with heterogeneous precisions especially in adversarial settings would strengthen the paper. Additionally while reducing energy consumption and computational costs are noble goals with positive environmental impacts a brief exploration into any tradeoffs such as increased training time or potential accuracy drops in certain scenarios would provide a more balanced view. Constructive suggestions include:  Expanding the experimental evaluation to include robustness tests against adversarial examples and model deployment on actual lowpower devices to showcase practical applicability.  Incorporating an analysis of training time overhead introduced by SMOL to provide readers with a comprehensive understanding of the methods costbenefit ratio. These constructive suggestions aim to fortify the papers contributions and ensure its relevance and applicability in advancing energyefficient deployed machine learning models.", "lQ--doSB2o": " Peer Review for \"Leveraging FeatureLevel Adversarial Attacks for Interpretability in Image Recognition Models\"  1) Briefly summarize the paper and its contributions This paper explores the paradigm of featurelevel adversarial attacks in the context of image recognition drawing inspiration from natural phenomena like the adversarial \u201ceyespots\u201d of the ringlet butterfly. In contrast to conventional pixellevel adversarial perturbations the authors propose a method to manipulate latent representations within image generators creating adversarial attacks that are not only interpretable by humans but also robust in nature. The main contributions include:  Demonstrating the usefulness of featurelevel attacks in studying deep network representations.  Introducing versatile and robust techniques for generating featurelevel adversarial examples capable of targeted universal disguised physicallyrealizable and blackbox attacks on ImageNet scale models.  Utilizing these adversarial examples as tools for network interpretability allowing for an understanding and exploitation of flawed featureclass associations through copypaste attacks.  2) Thorough assessment of the strengths and weaknesses of the paper Strengths:  Originality and Innovation: The paper introduces an innovative approach to generating adversarial examples shifting focus from pixellevel to featurelevel perturbations. This not only augments the leveraging of adversarial attacks for interpretability purposes but also enhances the practicality and robustness of these attacks.  Comprehensive Analysis: The authors provide a thorough examination of their methods through a variety of attacks demonstrating versatility and robustness across different scenarios. The methodological rigor in evaluating these attacks including physicallyrealizable and blackbox attacks is commendable.  Interpretability as a Diagnostic Tool: Leveraging adversarial examples for interpretability and uncovering spurious featureclass associations is a novel contribution. This not only broadens the scope of adversarial research but also offers practical means to diagnose and improve model reliability. Weaknesses:  Complexity and Scalability: The proposed methods while effective are complex and might be resourceintensive which can limit scalability and practical application especially for larger datasets beyond ImageNet.  Dependency on Generative Models: The quality and success of generated adversarial examples heavily rely on the underlying generative models. Any limitations in these models can directly impact the effectiveness of the proposed attacks.  Evaluation of Interpretability: While the paper demonstrates the utility of adversarial examples for interpretation the quantifiable measure of interpretability improvement especially compared to existing methods is not well articulated. It leaves the question of how much more interpretable these attacks are in practical terms.  3) Questions and suggestions for the authors  Scalability Concerns: How do the authors envisage tackling scalability issues especially when dealing with larger and more complex datasets or realworld applications beyond ImageNet  Comparison with Existing Interpretability Tools: Could the authors expand on how this method compares directly with existing interpretability and diagnostic tools in terms of effectiveness efficiency and practical applicability  Further Applications: The method has clear applications in image recognition models. Have the authors considered its applicability and potential adaptations in other domains such as natural language processing or audio signal processing  4) Limitations and Potential negative societal impact The authors discuss the dualuse nature of adversarial attacks emphasizing their diagnostic value over potential malicious use. However a more indepth discussion on mitigation strategies against malicious deployment of such featurelevel adversarial examples would enhance the paper. It\u2019s crucial to consider and propose safeguarding mechanisms especially as the paper introduces more potent and less detectable forms of adversarial examples. Constructively developing countermeasures or improvements in model robustness alongside adversarial example generation would be beneficial. Furthermore the paper could benefit from a broader discussion on ethical considerations and potential biases introduced by generative models used in crafting adversarial examples.", "um2BxfgkT2_": " Peer Review of \"Tokenized Graph Transformer for Graph Learning\"  1) Summary of the Paper and Its Contributions The paper introduces a novel approach for applying standard Transformers to graphs named Tokenized Graph Transformer (TokenGT) achieving competitive results on the PCQM4Mv2 graph dataset without requiring graphspecific modifications to the Transformer architecture. By treating nodes and edges as tokens and augmenting them with tokenwise embeddings including orthonormal node identifiers and trainable type identifiers the authors demonstrate that their model can approximate permutation equivariant linear operators on graphs leveraging the theoretical foundation that it can be as expressive as a secondorder Invariant Graph Network (2IGN) and more expressive than conventional messagepassing Graph Neural Networks (GNN). This approach simplifies graph processing using Transformers and offers promising results both theoretically and empirically presenting a significant step towards generalpurpose attentional architectures for diverse data modalities including graphs.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Foundation: The paper provides a solid theoretical basis for treating graphs with standard Transformers by demonstrating their capability to approximate equivariant linear operators which contributes significantly to the understanding of Transformer expressiveness in graph learning.  Performance: The empirical results showing that the proposed TokenGT outperforms all GNN baselines on the PCQM4Mv2 dataset and achieves competitive performance with other Transformer variants indicate the models efficacy and potential for broad applicability.  Simplicity and Generalizability: The methods simplicity avoiding graphspecific modifications represents a considerable advantage in terms of potential extensions to other tasks and data modalities. Weaknesses:  Computational Efficiency: Despite some efficiency gains through kernelization the quadratic computational cost related to the selfattention mechanism for large graphs remains a concern. The exploration of more efficient Transformer architectures could be valuable.  Limited Exploration of Node Identifiers: The empirical evaluation mainly focuses on orthonormal random features and Laplacian eigenvectors as node identifiers. Exploring other node identifier schemes might uncover further performance gains or insights into the models behavior.  Comparative Analysis: While the paper compares TokenGT against GNN baselines and Transformer variants with graphspecific modifications a more comprehensive comparison including newer efficient GNNs or Graph Transformers could provide a clearer picture of the models standing.  3) Questions and Suggestions for the Authors  Extension to Dynamic Graphs: Could the authors discuss the potential applicability or required modifications of TokenGT for dynamic graphs where the graph structure changes over time  Node Identifier Exploration: What are the expected effects of using more complex node identifier schemes possibly learned endtoend on the models performance and computational efficiency  Efficiency Improvements: Are there ongoing efforts or initial ideas on how to further reduce the computational overhead introduced by treating each node and edge as separate tokens especially for very large graphs  4) Limitations and Societal Impact The authors have addressed the computational efficiency limitation to some extent however further exploration into efficient attention mechanisms or sparse processing could make the model more accessible for larger graph datasets. The discussion on societal impact is limited. While the focus is on theoretical and empirical advancements considering and discussing potential negative impacts such as biases in decisionmaking applications or environmental costs of training large models could provide a more rounded view of the works implications. Additionally emphasizing interpretability and transparency especially in how the Transformer processes graph structure could increase the models utility and safety in realworld applications.  Suggestions for Improvement:  Investigate and benchmark against a broader range of node identifier schemes.  Explore the integration of more advanced efficient attention mechanisms or hybrid models that combine the strengths of TokenGT with sparser graph processing techniques.  Include a discussion on potential biases fairness considerations and environmental impacts to provide a comprehensive overview of the models societal implications.", "wTp4KgVIJ5": "Review of \"Stochastic Sampling Averaging Gradient Descent Ascent for Federated MinMax Learning\" 1) Summary: This paper addresses the challenge of high communication complexity in federated minmax learning an emerging yet crucial area within federated learning frameworks especially when handling noni.i.d. datasets and partial client participation. The authors introduce a novel algorithmic framework called Stochastic Sampling Averaging Gradient Descent Ascent (SAGDA) which significantly lowers the communication complexity through a combination of assembling stochastic gradient estimators from randomly sampled clients and adjusting learning rates on the server and client sides. The authors posit and empirically validate that SAGDA achieves a linear speedup concerning both the number of clients and local update steps rendering an O(\u22122) communication complexity. Furthermore they demonstrate that the conventional Federated Stochastic Gradient Descent Ascent (FSGDA) is a specific case of SAGDA thereby also achieving a substantially improved communication complexity of O(\u22122). 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty and Technical Depth: The proposed SAGDA framework is innovative and addresses a significant gap in the federated minmax learning domain. The technical depth including the thorough theoretical analysis and convergence proofs provides a strong foundation for the effectiveness of SAGDA.  Significant Reduction in Communication Complexity: The paper effectively tackles one of the most pressing issues in federated learning\u2014communication overhead\u2014by demonstrating both theoretically and empirically that SAGDA achieves substantially lower communication complexity compared to existing approaches.  Comprehensive Experimental Studies: The extensive experiments provide solid empirical evidence supporting the theoretical claims. The use of different datasets and comparison with stateoftheart methods significantly enhance the papers credibility. Weaknesses:  Assumptions: The effectiveness of SAGDA as outlined relies on certain assumptions (e.g. Lipschitz Smoothness Polyak\\xc5\\x81ojasiewicz Condition). While common in theoretical analyses the practical implications of these assumptions especially in highly heterogeneous realworld scenarios remain underdiscussed.  Limited Discussion on Implementation and Scalability: While the algorithm technically improves communication efficiency theres sparse discussion about its practical implementation complexities especially in highly diverse and largescale federated environments.  Potential Negative Societal Impacts Not Addressed: The paper lacks a discussion on potential negative societal impacts. For instance the efficiency and reliability of the proposed framework in privacysensitive applications remain unexplored. 3) Questions and Suggestions for Authors:  Implementation Details: Could the authors provide more insights into the practical challenges and computational overheads (if any) associated with implementing SAGDA especially concerning maintaining the control variates across communication rounds  Robustness Across Varied Scenarios: How robust is SAGDA to extremely noni.i.d. data distributions and highly dynamic participating clients This information could significantly enhance the applicability of the proposed method.  Impact on Privacy: Given that federated learning is often adopted for its privacypreserving characteristics how does the introduction of control variates and the specific sampling mechanism affect the data privacy of participating clients 4) Limitations and Constructive Suggestions: The authors have made a commendable attempt at addressing the communication complexity in federated minmax learning. However they could further strengthen the paper by:  Expanding the Discussion on Realworld Applicability: Delve deeper into realworld challenges that might affect SAGDAs performance including device failures network instability and extremely skewed data distributions.  Addressing Potential Negative Societal Impacts: It would be valuable to discuss any potential adversarial exploitation or privacy concerns that might arise from the proposed method. Suggesting mitigative strategies for the same could be beneficial.  Enhancing the Methodology Section: A more detailed explanation of the mechanism behind choosing learning rates and constructing control variates could aid in understanding and replicating the proposed method. In conclusion this paper presents a significant step forward in federated minmax learning. By addressing the concerns and suggestions mentioned above the authors can further solidify their contributions to the field.", "kS5KG3mpSY": "Review of \"Adaptive MultiStage Density Ratio Estimation for Learning Generative Models with Informative Latent Space Prior\" 1. Summary of the Paper: This paper introduces a novel methodology for learning energybased models (EBMs) in the latent space of generative models without the heavily computational burden of Markov Chain Monte Carlo (MCMC) sampling. The authors propose an adaptive multistage density ratio estimation technique that utilizes noise contrastive estimation (NCE) for efficiently learning the latent space prior. By breaking down the density ratio estimation into multiple stages and adaptively learning these stages the method is capable of significantly refining the expressiveness of the prior. This enables the generation of sharper and more informative priors compared to baseline methods. The proposed approach demonstrates superior performance in tasks such as image generation reconstruction and anomaly detection across several benchmarks. 2. Assessment of the Paper: Strengths:  Innovation: The paper presents a highly innovative approach to learning EBMs in latent space addressing the computational inefficiencies associated with traditional MCMC sampling methods.  Performance: The empirical results across multiple tasks and datasets underline the effectiveness of the proposed method particularly in generating highquality images and improving anomaly detection.  Clarity and Rigor: The methodology is clearly laid out and the paper follows a logical structure that is easy to follow. Extensive experiments along with detailed appendices provide a thorough examination of the proposed method\u2019s capabilities.  Potential Impact: The technique opens up new possibilities for the construction of more expressive priors in generative models potentially impacting various applications in generative modeling and deep learning. Weaknesses:  Complexity: The multistage approach while effective introduces additional complexity in terms of implementation and understanding the impacts of each stage on the final model performance.  Parameter Efficiency: Although noted by the authors the inefficiency in terms of increasing the number of parameters with each added density ratio estimator remains a concern for scalability and resource consumption.  Generalization to Other Domains: While impressive results are shown on image data it\u2019s less clear how well this method generalizes to other types of data such as text or graphs or to tasks beyond those evaluated. 3. Questions and Suggestions for the Authors:  Could the authors elucidate on the theoretical underpinning that guarantees the convergence of the multistage learning process How does this adaptivity contribute to the stability of model training  It would be beneficial if the authors could discuss the limitations of their method when applied to nonimage data types. Are there inherent attributes of image data that particularly benefit from their approach  To address the parameter inefficiency have the authors considered techniques such as parameter sharing across stages or the use of more parameterefficient architectures for the density ratio estimators  A detailed comparison with other stateoftheart methods in generative modeling that do not use EBMs as priors might provide a clearer picture of where the proposed method stands in the broader landscape.  It might be interesting to explore the interpretability of the learned latent space. Can the multistage adaptation provide insights into the learned representations in a way that might be beneficial for understanding model decisions or for further model improvements 4. Limitations and Societal Impact: The paper could benefit from a more detailed discussion on the limitations of the proposed method particularly regarding the scalability to very large datasets and models and the generalization to other domains beyond images. Additionally addressing any potential negative societal impacts such as the generation of deceptive imagery or deepfakes would be prudent. Suggestions for improving parameter efficiency and reducing computational resources would also contribute to making the method more accessible and sustainable. In conclusion the paper presents a significant advancement in the effective and efficient learning of EBMs within the latent space of generative models. Despite the acknowledged limitations the innovative approach and comprehensive evaluation provide a strong foundation for future work. With further exploration and refinement the proposed method has the potential to significantly impact the field of generative modeling.", "tbId-oAOZo": " Peer Review of \"QueryPose: Sparse EndtoEnd MultiPerson Pose Regression\"  1) Summary of the Paper The paper introduces QueryPose an innovative framework for multiperson pose estimation (MPPE) that leverages a sparse querybased approach to directly predict keypoint sequences from an input image. Traditional MPPE methods have relied on dense representations requiring nondifferentiable postprocessing steps which are cumbersome and parameterheavy. QueryPose addresses this by employing learnable partlevel and instancelevel queries enhanced through a Spatial Part Embedding Generation Module (SPEGM) and a Selective Iteration Module (SIM). These proposed modules allow for the encoding of spatial details and structural information necessary for precise keypoint localization thereby eliminating the need for complex postprocesses used in dense paradigms. The framework demonstrates stateoftheart performance on both MS COCO and CrowdPose datasets surpassing existing methods in efficiency and accuracy.  2) Assessment of Strengths and Weaknesses Strengths:  The introduction of the concept of sparse learnable spatialaware part queries is a significant departure from the conventional dense representations in MPPE offering both conceptual and practical advancements in pose estimation.  The two novel modules SPEGM and SIM strategically enhance the querys ability to encode spatial and structural features effectively addressing prior shortcomings in capturing local details necessary for accurate pose estimation.  Demonstrates superior performance on recognized benchmarks convincingly outperforming stateoftheart methods in terms of accuracy and inferencing efficiency as supported by extensive experimental results.  The papers methodology and proposed approach are wellmotivated clearly articulated and supported by a rigorous experimental setup showcasing the frameworks effectiveness and its potential application in realworld scenarios. Weaknesses:  The paper could benefit from a more thorough discussion on the limitations of the proposed method. While performance on standard benchmarks is impressive insights into scenarios where QueryPose might underperform (e.g. extremely dense crowds occlusions) or its adaptability to less structured environments could enhance the readers understanding.  The societal impact particularly concerning surveillance and privacy issues inherent in pose estimation technologies is not discussed. An exploration of potential negative impacts and mitigation strategies would provide a wellrounded perspective on the research.  While the paper includes a discussion on the efficiency benefits of the sparse paradigm a more detailed analysis comparing the computational resources required relative to dense methods (including training times and inference speeds on standardized hardware) would provide valuable insights into the practical deployment of QueryPose.  3) Questions and Suggestions for the Authors  Can you provide more details on how QueryPose performs in challenging scenarios such as heavy occlusions or extremely crowded scenes Are there specific limitations in its current configuration that would affect its effectiveness in such conditions  It would be valuable to include a discussion on the potential societal impacts of deploying QueryPose in realworld applications especially regarding privacy concerns. What measures can be taken to ensure ethical use  Considering the efficiency improvements highlighted a comparative analysis of computational resources required for QueryPose versus traditional dense methods would be beneficial. Can you elaborate on the models performance regarding training time memory consumption and inference speed  In future work do you envisage integrating additional modalities (e.g. depth information) with QueryPose to further enhance pose estimation accuracy or to expand its applicability to more complex environments  4) Limitations and Societal Impact The authors have made commendable efforts in presenting a novel approach to MPPE and demonstrating its efficacy however the paper does not sufficiently address the limitations of the proposed method or its potential negative societal impacts. For improvement the authors should consider elaborating on scenarios where QueryPose might struggle or detailing its adaptability to various environments. Furthermore a thorough examination of the ethical considerations surrounding the use of such technologies with proposals for mitigation strategies would provide a more comprehensive overview of the researchs implications.", "q4IG88RJiMv": " Review of \"Exploring fDivergence in ROC Curve Arc Length for Enhanced Binary Classification\"  1. Summary of the Paper This paper embarks on an innovative journey to delve into the relationship between the Receiver Operating Characteristic (ROC) curves arc length and fdivergences. The authors introduced a groundbreaking perspective by proving that the arc length of an optimally drawn ROC curve embodies an fdivergence. By leveraging this insight they formulated the arc length through a variational objective approach approximating it with positive and negative data samples. Their method achieved a nonparametric convergence rate showcasing its efficiency and reliability. Furthermore they extended their approach to evaluate the surface area between the optimal ROC curve and a diagonal proposing a novel classification process centered around maximizing the approximate lower bound of the maximal Area Under the Curve (AUC). Empirical evaluations on the CIFAR10 dataset substantiated the procedures efficacy particularly in handling imbalanced binary classification tasks.  2. Assessment of Strengths and Weaknesses Strengths:  Innovation: The paper presents a novel approach by establishing a connection between ROC curve geometry and fdivergences thereby enriching the theoretical underpinnings of binary classification metrics.  Theoretical Contribution: The mathematical rigor in proving the arc length as an estimator for the arctangent of the likelihood ratio and its fdivergence properties is commendable.  Practical Significance: The proposed twostep classification procedure addresses a significant challenge in ML showing promising AUC performance in imbalanced classification tasks which is highly relevant in realworld applications.  Experimental Validation: The use of CIFAR10 for empirical validation is appropriate showcasing the methods practical advantages over existing AUC maximization techniques. Weaknesses:  Generalization Concerns: The paper primarily focuses on CIFAR10 for experimental validation. Including additional datasets especially those from different domains could strengthen the generalizability of the findings.  Complexity Analysis: While the paper acknowledges computational complexity a more detailed analysis comparing the proposed methods computational overhead with existing approaches would be instructive.  Limitation Discussion: The paper could benefit from a deeper discussion on potential limitations especially regarding the assumptions made for the theoretical framework and their implications in diverse practical scenarios.  3. Questions and Suggestions for the Authors  Dataset Diversity: Could the authors provide insight on how their methodology might perform across a broader spectrum of datasets including those with varying degrees of imbalance and feature dimensionality  Robustness Analysis: How robust is the proposed classification procedure against outliers and noise in the data Any empirical or theoretical insights in this direction would be valuable.  Scalability Concerns: Given the methods reliance on variational objectives and nonparametric approaches how does the computational complexity scale with increasingly large datasets Would the performance advantages still hold  Future Directions: Can the authors speculate on potential extensions of their work perhaps towards multiclass classification scenarios or integrating their approach with deep learning models  4. Limitations and Societal Impact The authors have not thoroughly addressed the limitations and potential negative societal impacts of their work. While the methodological advancements are evident a reflection on scenarios where their approach might not perform as expected especially in critical applications (e.g. medical diagnosis judicial decisionmaking) is missing. Additionally the paper could benefit from discussing how biases in data especially in imbalanced datasets could impact the fairness and ethics of the predictive models built using their proposed method. Suggestions for Improvement:  Broaden the Discussion on Limitations: A more detailed exploration of the theoretical and practical limitations including scenarios where the proposed method may falter or produce biased outcomes would provide a more balanced view.  Consider Ethical and Societal Implications: Reflecting on how the biases inherent in imbalanced datasets could be exacerbated or mitigated by their approach could provide valuable insights into fostering more ethical and fair machine learning practices. Overall the paper is a significant contribution to the machine learning field offering novel insights and techniques for enhancing binary classification tasks. Addressing the areas for improvement could further solidify the works standing and applicability.", "v2es9YoukWO": " PeerReview for \"Super Kernel Flow Network (SKFlow) for Optical Flow Estimation with Enhanced Handling of Occlusions\"  1. Summary of the Paper The paper introduces the Super Kernel Flow Network (SKFlow) a novel Convolutional Neural Network (CNN) architecture designed to address the challenges of occlusion in optical flow estimation. The main contribution of this work is the introduction of super kernels which enable the model to have enlarged receptive fields. This feature is crucial for complementing absent matching information and recovering occluded motions which are significant issues in optical flow estimation. The proposed SKFlow architecture utilizes conical connections and hybrid depthwise convolutions to efficiently implement these super kernels. Extensive experiments demonstrate SKFlows superior performance on various benchmarks especially in occluded areas. Notably the SKFlow model without the need for pretrained backbones and with a modest increase in computation achieves topranking performance on the Sintel benchmark.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of super kernels to the domain of optical flow estimation and their efficient implementation through conical connections and hybrid depthwise convolutions are innovative and valuable advancements in the field of computer vision.  Performance: The proposed SKFlow achieves stateoftheart performance across multiple benchmarks demonstrating the effectiveness of the method especially in handling occluded areas.  Generalization: The SKFlow model shows strong crossdataset generalization capabilities which is a crucial attribute for realworld applicability.  Efficiency: The model strikes a good balance between accuracy and computational cost with less than an 8.42 increase in MACs showcasing its practicality for deployment in various applications. Weaknesses:  Complexity in Implementation: The specialized architecture may present a higher barrier to implementation and integration into existing frameworks compared to more straightforward models.  Limited Exploration of Limitations: The paper could have offered a more indepth discussion on the limitations of the SKFlow especially in scenarios beyond occlusion to provide a more rounded understanding of its capabilities.  Potential for Overfitting: With the architecture relying heavily on super kernels there could be a higher risk of overfitting especially when applied to small datasets or highly specific tasks not covered in the benchmarks.  3. Questions and Suggestions for the Authors  Could the authors elaborate on how the SKFlow model performs in dynamic environments with rapidly changing occlusions  How does the SKFlow models performance compare to traditional methods in optical flow estimation on datasets not covered in the benchmarks (if any such evaluation has been done)  It would be beneficial if the authors could discuss any potential strategies to mitigate the increased complexity and potential overfitting risks associated with the SKFlow model.  A deeper exploration into the limitations of their approach especially in contexts other than occlusions could provide a more balanced view of the models applicability.  4. Limitations and Potential Negative Societal Impact While the authors provide an excellent overview of the SKFlow models performance a more thorough discussion of its limitations and potential negative societal impacts could enhance the paper. Specifically the increased complexity and computation cost albeit modest may limit the models accessibility for researchers or practitioners with constrained resources. Furthermore as with any advancement in video processing technology theres potential misuse in surveillance and privacy invasion. The authors could address these aspects by suggesting ways to maintain a balance between performance and complexity and by advocating for ethical guidelines in deploying such technologies.  Constructive Suggestions for Improvement  Extend the discussion on the models limitations and potential mitigation strategies providing a clearer picture of its applicability and limits.  Address potential ethical concerns especially concerning privacy and surveillance by suggesting guidelines for responsible use.  Consider adding a comparative analysis with traditional optical flow estimation methods on additional datasets to underscore the model\u2019s advantages and potential drawbacks.  Explore strategies to reduce the models complexity without significantly impacting performance making the approach more accessible to a broader audience. Overall the paper presents a significant advancement in optical flow estimation offering a promising solution to the longstanding challenge of occlusions. With further exploration of its limitations and broader implications this work could set a new standard for optical flow estimation methods.", "vjKIKdXijK": "The paper presented introduces a novel approach for certifying the convexity of differentiable functions focusing on the implementation of a method that checks the positive semidefiniteness of Hessian matrices. This approach is extended to a specified class of functions prevalent in classical machine learning. The authors have developed an algorithm to analyze computational graphs of Hessians to certify convexity effectively. They demonstrate that this Hessianbased method is more robust in identifying convexity for a broader class of differentiable functions than the disciplined convex programming (DCP) approach a widely recognized method in the field. Strengths:  Innovative Methodology: The authors introduce an original method for certifying the convexity of functions by checking the positive semidefiniteness of their Hessian matrices through computational graphs. This approach represents a significant advancement in the field of optimization.  Comprehensive Comparison: A detailed comparison between the proposed Hessian approach and the established DCP methodology provides readers with a clear understanding of their relative capabilities showcasing the Hessian methods superiority in handling a broader class of functions.  Practical Relevance: The ability to certify convexity in a more extensive range of differentiable functions is crucial for optimization and machine learning indicating the papers significant implications for practical applications.  WellStructured: The paper is wellorganized with clear delineation of sections and a logical flow that guides readers through the introduction methodology comparison and conclusions effectively. Weaknesses:  Limited Scope: While the paper thoroughly analyses and demonstrates the advantages of the Hessian approach over DCP for differentiable functions it falls short of exploring nondifferentiable functions and the potential applications or limitations therein.  Technical Complexity: The heavy reliance on advanced mathematical concepts and the specific formal language used for describing the problem may pose a barrier to understanding for readers not already familiar with the field.  Implementation Details: Although the paper outlines the computational approach it lacks a detailed discussion on the practical implementation aspects such as computational efficiency and scalability which are crucial for realworld applications. Questions and Suggestions for the Authors: 1. NonDifferentiable Functions: How might the Hessian approach be adapted or extended to handle nondifferentiable functions if at all possible 2. Computational Efficiency: Can the authors provide insights into the computational efficiency of their algorithm especially in comparison to DCP implementations 3. Scalability: How does the approach scale with the complexity and size of the function being analyzed and are there optimizations or improvements that could be made to enhance scalability 4. Extension to Other Fields: Could this methodology be applied outside of machine learning such as in physics or finance where optimization problems are also prevalent Limitations and Societal Impact: The authors do not deeply delve into the limitations of their approach particularly in terms of computational resources required for larger or more complex functions. Addressing this aspect would offer a more comprehensive view of the practical applicability of their methodology. Additionally the potential negative societal impact of the work is not discussed. Although the direct societal implications may seem minimal a discussion on how misuse of such an optimization tool could lead to unintended consequences such as unfair algorithmic decisions in machine learning applications would be valuable. A suggestion for improvement would be to include a detailed analysis of these aspects to provide a holistic view of the works implications.", "pIYYJflkhZ": "Review of \"SoftPatch: A Memorybased Unsupervised Anomaly Detection Method with Patchlevel Denoising for Noisy Image Data\" 1) Summary of the Paper and its Contributions This paper delves into the realm of unsupervised anomaly detection (AD) particularly within the context of industrial image sensory anomaly detection under noisy data conditions\u2014a scenario that has been sparingly addressed in past research. The primary contribution is the development of a novel algorithm termed \"SoftPatch\" which introduces a memorybased unsupervised AD approach focusing on patchlevel denoising to handle labellevel noise in training datasets. By leveraging noise discriminators and a coreset memory bank SoftPatch differentiates itself from existing methods by efficiently denoising data at the patch level and applying a reweighting scheme to adjust the anomaly detection boundary thus enhancing noise robustness and mitigating overconfidence issues in anomaly detection coresets. The paper claims that SoftPatch surpasses stateoftheart (SOTA) AD methods on MVTecAD and BTAD benchmarks especially under noisy data conditions thereby setting a new baseline for future research in unsupervised AD in noisy settings. 2) Assessment of the Papers Strengths and Weaknesses Strengths:  Novel Approach: The introduction of patchlevel denoising in the context of noisy unsupervised anomaly detection is a notable innovation. This technique not only improves the utility of the algorithm under realistic noiseaffected circumstances but also maximizes the use of available data by exploiting the normal patches of noisy samples.  Comprehensive Evaluation: The paper provides an extensive evaluation across various noise scenes demonstrating SoftPatchs effectiveness in comparison with SOTA methods. This thorough examination especially its robustness against noise showcases the practical applicability of SoftPatch in industrial environments.  Significant Contributions: The proposal of an unsupervised AD method specifically designed for noisy data fills a crucial gap in the literature expanding the possibilities for realworld industrial quality inspection applications. Weaknesses:  Lack of Theoretical Backbone: The paper could have benefitted from a deeper theoretical analysis of why patchlevel denoising is effective including potential drawbacks or scenarios where it might not perform as expected.  Generalization Concerns: While the method shows superior performance on specific benchmarks the paper does not extensively discuss its generalizability across different types of datasets or anomalies that significantly diverge from those in industrial inspection.  Limited Discussion on Computational Resources: Given the mention of a memory bank and multiple noise discriminators the paper falls short in discussing the computational demands of SoftPatch which could be a deciding factor for its deployment in realworld applications. 3) Questions and Suggestions for the Authors  Could the authors elaborate on the computational efficiency of SoftPatch especially in comparison with the SOTA methods Understanding its performance concerning computational resources would be critical for practical applications.  The paper would benefit from a deeper theoretical discussion on the principles behind patchlevel denoisings effectiveness and potential limitations. Are there specific noise distributions or anomaly types where SoftPatch might underperform  It would be interesting to see how SoftPatch performs on datasets outside the industrial inspection domain to assess its generalizability better. Can the authors provide insights or preliminary results on such evaluations 4) Limitations and Societal Impact The authors adequately discuss the methods superiority in handling noisy data for anomaly detection in industrial contexts but do not fully explore the computational demands associated with their approach. Given that realworld deployments often operate under constrained resources a more detailed analysis of computational overheads and efficiencies would enhance the papers practical relevance. Moreover while the paper briefly touches upon the unsupervised ADs potential for rapid deployment across new production lines without data filtration it does not delve into the broader societal implications of deploying such technology. For instance the impact on employment in quality inspection roles and the ethical considerations of automating such tasks could have been discussed to provide a holistic view of the technologys societal impact. Suggestions for Improvement:  Include an explicit discussion and analysis of the computational efficiency and resource requirements of SoftPatch to enhance its practical applicability narrative.  Address the generalizability of SoftPatch across various domains and anomaly types to underscore its versatility and adaptability.  Provide a nuanced exploration of the potential societal impacts including the benefits and challenges of deploying unsupervised AD technologies in industrial settings to foster a comprehensive understanding of its realworld implications.", "zuL5OYIBgcV": " Peer Review of the Manuscript Entitled \"Building HighPerforming NonDeep Neural Networks with Parallel Substructures\" Summary of the Paper This manuscript presents a novel architectural paradigm for neural networks ParNet which significantly reduces network depth without compromising performance on standard benchmarks. The principle rests on using parallel subnetworks rather than traditional sequential layer stacking effectively curtailing the depth required for achieving high accuracy in visual recognition tasks. The authors demonstrate the efficacy of ParNet through impressive performance metrics on ImageNet CIFAR10 and CIFAR100 datasets alongside object detection on MSCOCO with substantial reductions in network depth (to a depth of 12). Additionally the paper explores various architectural and training optimization strategies to further enhance performance and provides insightful discussions on the practical advantages of such parallel architectures especially in the context of future hardware trends. Assessment of Strengths and Weaknesses Strengths: 1. Innovative Approach: The idea of reducing neural network depth while maintaining performance is innovative and addresses critical latency issues in safetycritical systems. This work potentially opens new avenues for research in efficient network design. 2. Strong Empirical Results: The presented results are compelling particularly the performance of ParNet on several benchmarks with significantly reduced depth. This is a strong testament to the potential of the proposed architecture. 3. WellRounded Evaluation: The paper does not only focus on the accuracy but also considers the practical implications of such designs including discussions on hardware limitations and the potential for parallelization adding depth to the research. Weaknesses: 1. Comparison with Current SOTA: While the paper provides comparisons with ResNet and RepVGG it lacks a broader comparison with the current stateoftheart models especially those specifically designed for efficiency such as EfficientNet or MobileNets. 2. Hardware Utilization Details: The manuscript discusses the advantages of ParNet in the context of future hardware trends but lacks detailed empirical evidence on how current hardware architectures can be fully leveraged or how the proposed architecture compares with others in terms of energy consumption and throughput. 3. Limitations on Scalability and Generalization: The paper briefly mentions the scalability of ParNet by increasing width resolution and the number of streams. However it lacks a detailed exploration of the upper bounds of this scalability. Also theres limited discussion on the generalization of ParNet to tasks beyond image classification and object detection. Questions and Suggestions for Authors 1. Broadening the Comparison Spectrum: Can the authors provide a comparison with more recent efficiencyoriented architectures to contextualize ParNets performance 2. Detailed Hardware Analysis: Could the authors present a more indepth analysis of how ParNet performs on different hardware architectures including comparisons in energy efficiency and processing speed 3. Exploration of Upper Bound Scalability: It would be beneficial if the authors could explore or discuss the potential limitations in scalability of ParNet particularly concerning increasing the number of streams or wider applications. 4. Generalization Across Tasks: Further insights into how ParNet could be adapted or its performance on a broader range of tasks including but not limited to semantic segmentation and natural language processing tasks would be valuable. Limitations and Societal Impact The manuscript provides an initial look into the limitations and potential negative societal impacts focusing on the current nonsuitability of nondeep networks in lowcompute settings and the potential disadvantage in speed compared to architectures like RepVGG on contemporary GPUs. Nonetheless a more thorough discussion on the environmental impact considering the full lifecycle of deploying such networks including training energy expenditures might enhance the paper. Additionally considering the potential for misuse in surveillance or military applications a broader ethical consideration could be merited. Suggestions for Improvement: 1. Expanding on the exploration of environmental impacts and efficient training methodologies could enrich the discourse on societal impacts. 2. A deeper dive into ethical considerations given the potential dual use of such technologies in sensitive applications could strengthen the manuscript. In conclusion this manuscript contributes valuable insights into the design of efficient and highperforming neural networks presenting ParNet as a promising architecture that challenges the conventional depthcentric paradigm. With further exploration and refinement ParNet holds the potential to influence future directions in neural network architecture design significantly.", "yP0vpghGoLF": "This paper presents a significant advancement in the field of optimization and variational inequalities through the lens of the Past Extragradient (PEG) method also known as the Optimistic Gradient method. By focusing on a novel analysis utilizing potential functions and a computeraided discovery process the authors achieve noteworthy contributions in proving the O(1N) lastiterate convergence rate for Lipschitz and monotone operators extending these results to both unconstrained and constrained cases without the necessity for Lipschitz continuity of the Jacobian. This work not only addresses the previously open question of lastiterate convergence for monotone variational inequalities but also presents a more general and applicable methodology for assessing convergence rates in optimization problems especially those relevant to machine learning applications. Strengths: 1. Novel Methodological Contributions: The introduction of a novel analysis through potential functions for proving convergence rates without assumptions on the Jacobians Lipschitz continuity is a significant methodological leap. This approach not only simplifies the analysis but broadens the applicability of the convergence results. 2. Extension to Constrained Cases: Extending the O(1N) lastiterate convergence rate to constrained cases fills a crucial gap in the literature and showcases the methods versatility. This is a pivotal advancement given that many realworld optimization problems come with natural constraints. 3. Insightful ComputerAided Proofs: The use of computeraided methods for discovering and verifying the proofs adds an innovative dimension to the mathematical derivation highlighting the potential of computational tools in theoretical investigations. Weaknesses: 1. Numerical vs. Analytical Gap: The paper notes that empirically obtained upper bounds on convergence rates are significantly tighter than those proven analytically. This discrepancy suggests that there might still be room for analytical refinement to closely match empirical observations. 2. Restrictive Stepsizes for Constrained Cases: The more limited range of step sizes for the constrained case compared to the unconstrained case might limit the methods practical effectiveness in certain scenarios. 3. Comparison with Existing Methods: While the paper thoroughly discusses the novelty and contributions of the PEG method a more detailed comparative analysis with other firstorder methods especially in terms of empirical performance on standard optimization problems could further validate the methods practical significance. Questions and Suggestions for the Authors: 1. Bridging the NumericalAnalytical Gap: Could further refinement of the analytical methods narrow the gap between the empirical and proven convergence rates Any insights or ongoing work in this direction would be invaluable. 2. Practical Impact and Recommendations: Given the papers theoretical nature can the authors provide some guidance or preliminary results on the practical impact of these findings particularly in machine learning applications like training GANs or robust optimization problems 3. Future Directions: The discussion mentions stochastic methods and nonmonotone cases as open questions. Are there any preliminary insights or hypotheses the authors can share about these future research directions Limitations and Societal Impact: The authors discuss the limitations in terms of the discrepancy between numerical and analytical results and the restrictive step sizes for constrained cases. While adequately addressed within the scope of their contributions clarifying these limitations further\u2014particularly the potential practical implications of restrictive step sizes\u2014would be beneficial. Regarding societal impact the paper primarily focuses on theoretical advancements without delving into specific applications. Highlighting potential negative impacts such as misuse in adversarial machine learning could provide a more balanced view of the technologys societal implications. Constructive suggestions include exploring adaptive step size methods to mitigate the limitations in the constrained case and conducting empirical studies to assess the practical applicability and any unforeseen negative outcomes of the proposed methods in realworld scenarios.", "nX-gReQ0OT": " Peer Review Report  1. Brief Summary of the Paper: This paper introduces a novel deeplearning architecture that significantly advances the accuracy of solving the Schr\u00f6dinger equation for variational ground state energies of atoms and molecules. By combining deep learning with Monte Carlo methods the authors claim to have achieved 4070 lower energy error and six times lower computational cost than prior approaches. Their architecture systematically incorporates physical prior knowledge but also uncovers that an excessive incorporation of this knowledge can paradoxically decrease accuracy. The paper systematically compares the performance of the proposed method against both classic and contemporary methods establishing a new benchmark in computational chemistry.  2. Assessment of Strengths and Weaknesses: Strengths:  The paper addresses a highly relevant and challenging problem in computational chemistry with significant potential impacts on materials science drug discovery and other fields.  The proposed method represents a significant leap in terms of computational efficiency and accuracy over existing approaches which could democratize highaccuracy computational chemistry calculations.  The methodical dissection of improvements through an ablation study provides clear insights into the contributions of various architectural and methodological enhancements.  The authors challenge the conventional wisdom regarding the incorporation of physical prior knowledge contributing to the ongoing discourse on balancing domain knowledge and learning capability in machine learning applications. Weaknesses:  While the paper makes a strong case for the improvements brought by the proposed method the limitations section could benefit from a more detailed discussion on the scalability to even larger molecules or systems beyond acknowledging current computational limitations.  The paper primarily focuses on energy calculations. An exploration of the models applicability to other properties of chemical interest (e.g. dipole moments reaction barriers) would broaden its impact.  Given the complexity and novelty of the architecture the paper would benefit from providing more intuition or simplified explanations for a broader audience enhancing its accessibility and potential adoption.  The societal impact section is notably absent. Given the increasing awareness of the ethical implications of AI and computational research even a brief consideration of potential negative impacts or misuse would be valuable.  3. Questions and Suggestions for the Authors:  Question: Could the authors clarify if there are specific types of molecules or chemical environments where their method might underperform or face limitations  Suggestion: Consider expanding the discussion around the models scalability and potential strategies to address computational barriers for larger systems.  Suggestion: To increase the utility and appeal of the work it might be beneficial to illustrate the application of the proposed method to properties beyond ground state energies if data are available.  Suggestion: A section reflecting on the broader societal implications including but not limited to energy consumption of computation accessibility of the technology and potential misuse would enrich the paper.  4. Limitations and Societal Impact: The authors have acknowledged computational limitations and the issue with scaling for larger molecular systems. However a deeper analysis of potential strategies to overcome these barriers or the implications of these limitations on practical applications is missing. Regarding societal impact there is a missed opportunity to discuss the dualuse nature of computational chemistry advancements environmental concerns of increased compute demands and the potential for accelerating materials discovery in renewable energy or pollution reduction. Encouraging responsible use and deployment of such technologies is crucial and the authors could add value by briefly discussing these aspects.  Conclusion This paper makes a significant contribution to the field of computational chemistry by presenting a highly accurate and computationally efficient method for solving the Schr\u00f6dinger equation. The thorough assessment and clear presentation of results set a new benchmark for future work. Addressing the mentioned considerations and expanding the discussion on broader implications could further enhance its significance and impact.", "p9zeOtKQXKs": " Peer Review of \"A Unified Framework for Metagradient Estimation in GradientBased MetaRL\"  1) Summary of the Paper The paper presents a comprehensive study on gradientbased MetaReinforcement Learning (GMRL) focusing on identifying and mitigating biases in metagradient estimations. By introducing a unified framework the authors systematically analyze two major sources of bias in GMRL: compositional bias and multistep Hessian estimation bias. The paper provides theoretical bounds for these biases and empirically validates their impacts through experiments on tabular Markov Decision Processes (MDPs) the Iterated Prisoners Dilemma and Atari games. Moreover the authors investigate mitigation strategies including offpolicy learning and lowbias Hessian estimators demonstrating their effectiveness in reducing metagradient bias and thus improving the overall performance of GMRL algorithms.  2) Assessment of Strengths and Weaknesses  Strengths: 1. Theoretical Depth: The paper offers a significant theoretical contribution by providing a formal analysis of biases inherent in metagradient estimations within GMRL frameworks. The detailed theoretical insights accompanied by bounds on compositional and multistep Hessian biases enhance the understanding of underlying challenges in GMRL. 2. Unified Framework: The proposed framework abstracts across various GMRL algorithms offering a generalized platform for analyzing metagradient biases. This unified approach not only consolidates existing knowledge but also facilitates comparative analysis and development of generalized solutions for bias mitigation. 3. Empirical Validation: The extensive empirical studies serve as a robust validation of the theoretical findings. Conducting experiments across diverse settings (tabular MDPs Iterated Prisoners Dilemma and Atari games) and demonstrating the effectiveness of mitigation strategies underscores the practical relevance of the work.  Weaknesses: 1. Limited Scope of Mitigation Strategies: While the mitigation strategies presented (offpolicy learning and lowbias Hessian estimator) are useful the exploration of potential solutions feels somewhat limited. Incorporating a wider array of biascorrection mechanisms could have provided more comprehensive insights into overcoming the identified challenges. 2. Complexity and Accessibility: The high theoretical complexity and dense presentation might limit the papers accessibility to a broader audience. Efforts to simplify explanations or provide more intuitive understandings of the theoretical results could enhance the papers impact. 3. Generalization beyond Specific Settings: Although the paper presents experiments in varied settings further investigation into how the biases and their mitigation strategies generalize across an even broader range of GMRL applications would be beneficial.  3) Questions and Suggestions for the Authors  Expansion of Mitigation Strategies: Could additional bias mitigation strategies beyond offpolicy learning and lowbias Hessian estimators be explored For instance investigating adaptive learning rates or metalearner architectures tailored for reduced bias in gradient estimation might offer novel insights.  Simplification for Broader Accessibility: The theoretical parts of the paper while comprehensive are highly complex. Including simplified summaries or graphical illustrations could help make the material more accessible to readers not deeply familiar with the technical details of GMRL.  Generalization to Other MetaRL Frameworks: How might the identified biases and their implications vary in other metaRL frameworks beyond GMRL Exploring this could broaden the paper\u2019s relevance and application.  4) Limitations and Societal Impact The authors have made commendable efforts to address the limitations pertaining to the biases in metagradient estimations. However a more diverse scope of mitigation strategies could further enrich the paper. On the societal impact the discussion is somewhat lacking. It would be beneficial to explicitly address potential negative impacts particularly where biased decisionmaking in RL could lead to fairness or ethical concerns in realworld applications. Constructively suggesting guidelines for the ethical use of GMRL especially in sensitive domains would be a valuable addition. Overall this paper makes substantial contributions to the field of metareinforcement learning by shining a light on significant biases in metagradient estimation and providing theoretically grounded methods for their mitigation. Further explorations into broader mitigation strategies and more extensive generalizations would enhance its value significantly.", "mCzMqeWSFJ": " Peer Review of the Paper \"ComENet: A Complete and Efficient Graph Neural Network for 3D Molecular Graph Learning\"  1) Brief Summary The paper introduces ComENet a novel message passing scheme and graph neural network (GNN) architecture designed for learning from 3D molecular graphs with high efficiency and completeness. The authors define the concept of geometric completeness necessary for differentiating any two distinct 3D graphs and propose a method that guarantees this completeness while being significantly more computationally efficient than existing solutions. By incorporating quantuminspired basis functions and a novel message passing mechanism within a 1hop neighborhood ComENet achieves global and local completeness in 3D information processing. The method is thoroughly evaluated on largescale molecular datasets (OC20 Molecule3D and QM9) demonstrating its capability to handle vast amounts of data with superior or comparable performance to existing methods while considerably reducing the computational costs.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Contribution: The formal definition of geometric completeness and the methodological innovations that guarantee it are significant contributions offering a solid theoretical foundation for future research in 3D graph learning.  Computational Efficiency: Demonstrating an order of magnitude improvement in computational efficiency over prior methods without sacrificing performance is a remarkable achievement addressing a critical bottleneck in 3D molecular graph learning.  QuantumInspired Features: The integration of quantuminspired basis functions is a creative solution that enhances the models ability to learn complex molecular properties showcasing the authors innovative approach.  Experimental Validation: The comprehensive set of experiments across multiple datasets and the comparison with a range of existing models establish the effectiveness and efficiency of ComENet convincingly. Weaknesses:  Model Expressiveness: While the computational efficiency is significantly improved the paper briefly mentions a reduction in model expressiveness due to the operational limitation within a 1hop neighborhood. This aspect could be elaborated on further to understand the tradeoffs involved better.  Generalization to NonMolecular Data: The focus is strictly on molecular graphs and while the results are impressive it remains unclear how well the proposed approach generalizes to other types of 3D graphs not discussed in the paper.  Limitation Discussion: While the paper touches upon the inherent difficulty in obtaining 3D information for molecular graphs a more detailed discussion on potential solutions or directions for overcoming this fundamental challenge would be valuable.  3) Questions and Suggestions for the Authors  Model Expressiveness: Can the authors elaborate on the extent to which model expressiveness is compromised by focusing on a 1hop neighborhood and any potential strategies to mitigate this limitation  Generalization: Could the authors provide insights or preliminary results on applying ComENet to other types of 3D graphs such as those found in computer vision tasks (e.g. point clouds)  Data Acquisition Challenge: Considering the difficulty in acquiring 3D information could the authors speculate on or suggest methodologies for generating or estimating 3D structural information from more readily available data  4) Limitations and Potential Negative Societal Impact The authors acknowledge the limitation related to the dependency on existing 3D information for molecular graphs and hint at future research directions to address this challenge. However a deeper exploration of methodologies for generating or approximating 3D data would make the paper more comprehensive and forwardlooking. In terms of societal impact while the paper primarily discusses positive applications like drug and material discovery a discussion on the potential misuse of such technologies (e.g. creating harmful substances) and safeguards or ethical considerations to prevent misuse would add an important dimension to the work. Constructive Suggestions for Improvement:  A more nuanced discussion on the tradeoffs between computational efficiency and model expressiveness could be beneficial.  Exploring potential applications or adaptations of the methodology to other domains of 3D graph data could enhance the papers relevance and applicability.  A section dedicated to addressing ethical considerations related to the use and potential misuse of the proposed technology would provide a more holistic view of the works societal impact. In summary the paper makes significant contributions to the field of 3D molecular graph learning setting a new benchmark in terms of efficiency and completeness. Addressing the outlined questions and suggestions could further strengthen the papers impact and applicability.", "xTYL1J6Xt-z": "PeerReview of \"Efficient Generation of HighQuality Risk Scores Using BeamSearch and Star Ray Search\" 1) Brief Summary of the Paper This paper introduces FasterRisk a novel approach designed to efficiently generate highquality risk scores which are a critical component in decisionmaking in healthcare finance and criminal justice sectors. The authors provide a comprehensive solution to the problem of generating sparse linear models with integer coefficients (risk scores) directly from data which is known to be a computationally challenging task. The paper proposes a threestep framework that combines a beamsearch algorithm for logistic regression a method to obtain diverse pools of almostoptimal solutions and a \"star ray\" search technique to efficiently find integervalued solutions. These contributions are aimed at addressing the computational inefficiencies and limitations of previous methods in producing highquality risk scores. 2) Assessment of Strengths and Weaknesses Strengths:  Innovation: The paper presents a unique combination of algorithms to solve an NPhard problem swiftly showcasing a significant improvement over existing methods both in terms of computational efficiency and quality of risk scores generated.  Practical Relevance: By addressing the limitations of current risk score generation techniques the paper presents a method that can be practically deployed in various highstakes domains providing professionals with a tool that is both fast and generates interpretable models.  Extensive Evaluation: The experimental section rigorously tests the proposed method against existing stateoftheart solutions convincingly demonstrating its superior performance in both solution quality and runtime. Weaknesses:  Potential Overfitting Concerns: While the paper convincingly demonstrates the effectiveness of the proposed methods there is limited discussion on the potential for overfitting especially given the methods ability to generate multiple highquality risk scores.  Lack of External Validation: The paper focuses on internal validation through crossvalidation. An external validation using independent datasets not involved in the model development stage could further strengthen the papers claims.  Accessibility and Interpretability: Despite emphasizing the interpretability of risk scores the paper could benefit from a more detailed discussion on how nonexpert users can navigate choosing from the pool of generated models and interpreting them in the context of their specific applications. 3) Questions and Suggestions for the Authors  Overfitting Mitigation: Could the authors elaborate on any measures or techniques used to prevent overfitting when generating multiple highquality risk scores  External Validation: Are there plans to perform external validation using independent datasets to further verify the generalizability of the proposed method  User Guide for Model Selection: Given the generation of a pool of models providing a detailed user guide or criteria for selecting the most appropriate model for specific applications could enhance the papers practical value. Can the authors comment on how they envision users making these selections  Comparison with NonLinear Models: The paper focuses on linear risk scores. Including a discussion or comparison with nonlinear models even if initially outside the scope could provide valuable insights into the tradeoffs between interpretability and predictive performance. 4) Limitations and Societal Impact The authors do not extensively discuss the limitations and potential negative societal impacts of their work. Given the application domains of risk scores it is crucial to address the ethical implications biases and fairness aspects of the generated models. The following are constructive suggestions for improvement:  Bias and Fairness Analysis: A subsection analyzing potential biases in the generated risk scores and their implications on fairness could be beneficial. Strategies for mitigating such biases should also be discussed.  Transparency on Limitations: More transparency on the limitations of the proposed method including scenarios where it may not perform as expected can help users better understand its applicability.  Ethical Considerations: A discussion on the ethical considerations of deploying automated risk scores in highstakes decisions along with guidelines for ethical usage can enhance the papers societal impact considerations. In conclusion the paper makes a significant contribution to the field of risk score generation presenting a method that stands out for its speed efficiency and potential for broad application. Addressing the suggestions mentioned could further solidify its contributions and applicability.", "peZSbfNnBp4": " Peer Review  1) Summary of the Paper The paper addresses the challenge of Domain Generalization (DG) wherein models trained on a given set of training domains often perform unpredictably on unseen test domains due to stochasticities in optimization processes such as initial seed selection. The authors propose a novel simple model averaging protocol that not only enhances DG capabilities by reducing this stochasticity effect but also establishes a more reliable correlation between indomain validation accuracy and outofdomain test accuracy. This is invaluable for dependable early stopping. Further the study innovates by suggesting the ensembling of moving average models (EoA) from independent runs showing a substantial performance improvement over traditional ensembling methods. The work is substantiated by theoretical discussions on the biasvariance tradeoff adapted to DG alongside extensive empirical evaluations using pretrained models on the DomainBed benchmark demonstrating notable performance gains.  2) Strengths and Weaknesses Strengths:  Theoretical Justification: The paper provides a solid theoretical foundation for the proposed method by adapting the biasvariance tradeoff to DG offering a convincing explanation for the observed performance improvements.  Hyperparameterfree Approach: By advocating for a simple moving average of model parameters that doesnt require complex hyperparameter tuning (unlike SWAD) the methodology becomes broadly applicable and userfriendly.  Empirical Validation: Comprehensive benchmarking across several datasets and comparison with existing stateoftheart methods provide strong empirical support for the proposed approach. Weaknesses:  Lack of Discussion on Computational Requirements: While the paper mentions computational efficiency in passing a detailed analysis comparing the computational overhead of traditional ensembling vs. EoA especially considering the storage of numerous model parameters is missing.  Limited Analysis on the Effect of Different Architectures: The paper predominantly focuses on ResNet and RegNet architectures an exploration of how the proposed methods perform across various other model architectures would be beneficial.  Potential for Overfitting: There is minimal discussion on whether the improved indomain validation accuracy and outofdomain test accuracy correlation might inadvertently lead to overfitting or reduced model robustness.  3) Questions and Suggestions for the Authors  On Computational Analysis: Could the authors provide a more detailed computational analysis comparing the proposed method against traditional ensembling in terms of both execution time and memory requirements  Extension to Other Architectures: Have the authors considered applying and evaluating their proposed approach across a more diverse set of model architectures If not it could potentially broaden the applicability of their findings.  Overfitting Concerns: The improved correlation between indomain and outdomain performance is notable. However does this tighter correlation risk making the model more susceptible to overfitting Could additional regularization techniques or mechanisms be useful here  4) Limitations and Societal Impact The authors briefly touch upon the limitations in terms of the scope of architectures experimented with and the computational analysis. Further exploration into the potential negative impacts such as the risk of overfitting or the potential biases introduced by relying more heavily on indomain validation accuracy for model selection would enrich the discussion. Constructively diversifying the array of architectures tested and incorporating an indepth computational overhead analysis would provide a more comprehensive understanding of the practicality of deploying such a method in realworld scenarios. Moreover addressing potential overfitting and introducing mitigation strategies would make the proposal more robust. The societal impacts of deploying more reliable and generalizable models are inherently positive especially in critical areas like healthcare autonomous driving and other domains where unpredictability can have dire consequences. However the authors should also consider discussing the ethical considerations related to model transparency and the potential for unintended bias when models are applied across diverse realworld domains not represented in the training data.", "maSvlkPHc-k": " Peer Review of \"Addressing Discrimination Risk in Graph Feature Imputation Algorithms\"  1. Summary of the Paper This paper investigates a significant issue in the domain of graph feature imputation: the amplification of discrimination against marginalized groups. The authors identify a phenomenon they term \"discrimination risk\" where nodes belonging to marginalized groups in human networks tend to have imputed feature values that further distinguish them from dominant groups thereby perpetuating unfairness when machine learning models are applied to these data. The paper introduces and formalizes \"mean aggregation imputation\" a common framework for graph feature imputation and proposes an algorithm designed to minimize discrimination risk while maintaining the accuracy of the imputation. The paper\u2019s contributions are both theoretical with formal proofs and characterization of the conditions under which discrimination risk is heightened and empirical with evaluations conducted on synthetic and realworld datasets concerning fairness and accuracy.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty and Relevance: The paper addresses a novel aspect of fairness in machine learning on graphstructured data focusing on the preprocessing stage of imputation. This focus is particularly relevant given the increasing use of graph neural networks in various applications.  Theoretical Depth: The formalization of the \"mean aggregation imputation\" framework and the subsequent theoretical analysis are comprehensive. The proofs provided showcase a robust mathematical foundation for the identification and mitigation of discrimination risk.  Practical Implications: The proposed algorithm has the potential to influence the development of fairer machine learning models with practical benefits for highstakes applications like job candidate screening and loan approval.  Empirical Evaluation: The diverse set of experiments on both synthetic and realworld datasets provides a thorough validation of the proposed approach concerning both fairness and accuracy. Weaknesses:  Generalization of Results to RealWorld Data: While the algorithm improved fairness without significant loss in reconstruction error on synthetic datasets its effectiveness on realworld datasets was not consistent. This discrepancy raises questions about the algorithms practical applicability and generalizability.  Limitations in Group Definition: The papers reliance on discrete and static group memberships could limit its applicability in scenarios where group identities are fluid overlapping or intersectional.  Lack of Comparisons to Existing Methods: Absence of a direct comparison with existing graph feature imputation methods or fairnessenhancing algorithms leaves a gap in understanding the relative performance of the proposed algorithm.  3. Questions and Suggestions for the Authors  Have the authors considered extending their framework to account for intersectional identities given the mentioned limitation regarding discrete and static group memberships  A comparison with baseline methods in the realm of fair machine learning or graph feature imputation could further bolster the papers contributions. Can the authors integrate such comparisons in future work  Could the authors comment on potential approaches to enhance the generalizability of their algorithm to realworld datasets where fairness improvements were not observed  It would be valuable if the authors could discuss the scalability of their proposed algorithm especially concerning large and complex graph structures.  4. Consideration of Limitations and Societal Impact While the authors acknowledge the limitations related to the assumptions of discrete and static group memberships a more detailed discussion on how these limitations might be overcome in future work would be beneficial. Furthermore considering the potential negative societal impact it would be constructive to explore the ethical considerations surrounding the application of this technology more comprehensively. Suggestions for improvement include expanding on the ethical implications of using such algorithms in sensitive domains and providing guidance or frameworks for responsibly deploying these technologies in practice.", "rG0jm74xtx": "Review of \"Time Series Forecasting with a Disentangled DiffusionVAE Model\" 1) Summary The paper introduces a generative model named D3VAE for time series forecasting that leverages a bidirectional variational autoencoder (BVAE) integrated with elements of diffusion denoise and disentanglement mechanisms. The proposed model aims to address the challenges posed by the limited and noisy nature of realworld time series data through a couple of innovations: a coupled diffusion probabilistic model that augments time series data to improve generalization capability without increasing uncertainty a multiscale denoising score matching to improve the accuracy of generated series and a strategy for disentangling the latent variables to enhance interpretability and prediction stability. Extensive experiments are conducted on both synthetic and realworld datasets demonstrating that the D3VAE model outperforms existing stateoftheart algorithms. 2) Assessment Strengths:  The paper addresses a significant problem in time series forecasting especially the difficulty posed by short and noisy data series with an innovative generative modeling approach that combines diffusion denoise and disentanglement.  Introduction of a coupled diffusion process and multiscale denoising score matching represents a novel integration of techniques for improving the accuracy and reliability of time series forecasts.  The thorough experimental evaluation on a broad set of datasets showcases the models effectiveness and general applicability across different domains. Weaknesses:  While the paper presents a significant theoretical contribution the intuitive explanation and practical implications of some of the proposed techniques (particularly the coupled diffusion process and denoising score matching mechanism) could be elaborated further for readers not deeply familiar with these concepts.  The discussion on the choice of hyperparameters (e.g. the variance schedule and diffusion steps) is somewhat limited which might make replication and application of the proposed methods challenging for practitioners.  The paper could benefit from a more detailed discussion on the computational complexity of the D3VAE model especially in comparison with the baseline models it outperforms. 3) Questions and Suggestions:  Can the authors provide more insights or intuitive explanations on how the coupled diffusion process contributes to the generality and accuracy of the forecasting beyond the mathematical definitions  It would be valuable to include a discussion on the sensitivity of the model to its hyperparameters and potentially provide guidelines or recommendations for their selection in practical applications.  A deeper exploration into the computational demands of the D3VAE model compared to other baseline models could strengthen the paper. This includes both training time and inference time as well as any potential scalability concerns. 4) Limitations and Societal Impact: The authors have addressed limitations to some extent particularly mentioning the need for judicious parameter selection to avoid potential biases resulting from the diffusion process. However more explicit discussion on the computational complexity and efficiency of the model would be beneficial. In terms of negative societal impact the paper briefly mentions the importance of ethical considerations in deploying the model which is appreciated. Yet it stops short of discussing specific scenarios or mechanisms by which the model could be misused or cause harm nor does it offer mitigation strategies beyond the general call for responsible use. Suggestions for improvement include expanding the limitations section to cover computational aspects and enriching the discussion on potential negative societal impacts with concrete examples or speculative scenarios alongside strategies to mitigate these risks.", "sGugMYr3Hdy": " Peer Review  1) Brief Summary of the Paper The paper investigates the adaptation of pedagogy and pragmatism concepts from cognitive science to facilitate learning from demonstrations in artificial agent setups. It leverages a Bayesian Goal Inference (BGI) model to reduce goal ambiguity and accelerate the learning process particularly in environments where the number of demonstrations is limited. The authors introduce an artificial teacherlearner framework where both entities utilize goalconditioned Reinforcement Learning (RL) policies. Pedagogical teachers generate less ambiguous demonstrations while pragmatic learners are designed to infer the teachers goals more accurately. The effectiveness of this approach is demonstrated in multigoal environments through a series of experiments showing significant improvements in learning efficiency and goal inference accuracy compared to traditional methods. The paper also discusses the implementation details including the environments used for testing (Draw two balls and Fetch blockstacking) the metrics for evaluation and the experimental results which underscore the advantages of incorporating pedagogy and pragmatism into the learning process.  2) Assessment of Strengths and Weaknesses Strengths:  Novel Conceptual Framework: The paper adeptly integrates theories of pedagogy and pragmatism from cognitive science into the field of machine learning especially learning from demonstrations which is innovative and potentially impactful for both research communities.  Comprehensive Experimental Analysis: The extensive experiments conducted not only validate the theoretical propositions but also provide deep insights into the mechanisms effectiveness in practice.  Significant Performance Improvements: The results convincingly show that BGIagents outperform traditional approaches especially in scenarios with few demonstrations which is crucial for practical applications where data may be scarce. Weaknesses:  Scalability Concerns: While the paper provides substantial evidence of the methods effectiveness in specific multigoal environments there is limited discussion on scalability to more complex or diverse scenarios which might affect broader applicability.  Lack of Comparative Analysis: The paper could benefit from direct comparisons with alternative goal inference methods or learning paradigms to contextualize its contributions within the broader landscape of learning from demonstrations.  Assumptions on Environment and Task Formalization: The reliance on specific types of environments (e.g. Fetch blockstacking) and assumptions about the nature of teacherlearner interactions may limit the generalizability of the findings.  3) Questions and Suggestions for the Authors  Scalability: Can the authors discuss potential scalability issues or limitations when applying their approach to more complex environments or to tasks with higherdimensional goal spaces  Comparison with Other Methods: Would it be possible to include a comparative analysis with other stateoftheart goal inference methods to further justify the superiority of the BGI approach  Generalizability: How might the proposed method be adapted or extended to accommodate more general teachinglearning scenarios beyond the structured teacherlearner setup described  Impact on Learning Efficiency: The practical implications of improved learning efficiency are significant. Could the authors provide more insights into how this might affect the computational resources required for training  Limitations and Societal Impact The paper touches upon the limitations of the approach with regards to its application scope primarily tested in controlled multigoal environments. Theres less attention to the potential scalability and adaptability of the method to wider more complex scenarios. Additionally while the potential negative societal impact of the work seems minimal it would be valuable for future iterations of this research to include a discussion on ethical considerations especially as these pedagogical approaches become more sophisticated and are potentially integrated into systems that interact with humans or influence humanadjacent environments. Suggestions for improvement include a deeper exploration of the limitations in diverse settings and a proactive examination of the ethical implications of deploying such learning systems in realworld applications.", "vdxOesWgbyN": "The paper under review investigates the vulnerability of Split Federated Learning (SFL) to Model Extraction (ME) attacks addressing a critical concern in the protection of model Intellectual Property (IP) in federated learning environments. The authors propose five distinct ME attacks targeting the serverside model in SFL setups by exploiting gradient information obtained from malicious client queries. The effectiveness of these attacks is empirically demonstrated on the CIFAR10 dataset using the VGG11 model architecture showing significant success rates even with increased complexity of the serverside model. Through this work the authors expose a significant flaw in SFL  previously considered resistant to such attacks  and present a valuable analysis of attack performance under various conditions alongside suggestions for mitigating such vulnerabilities.  Assessment of Strengths 1. Novelty and Relevance: The exploration of ME attacks specifically tailored for SFL setups is both novel and highly relevant in the context of growing concerns over data privacy and security in decentralized learning systems. This work significantly extends our understanding of the vulnerabilities inherent in split learning models. 2. Comprehensive Attack Design: The paper proposes a diverse set of attacks catering to different data availability scenarios showcasing a thorough understanding of potential threat models. This broad consideration of attack vectors greatly enhances the papers relevance to a wider range of realworld applications. 3. Empirical Analysis: The extensive experimental validation provides a solid foundation for the claims made in the paper. The use of a wellknown dataset and model architecture ensures the reproducibility of results and offers clear insights into the practical implications of the proposed attacks.  Assessment of Weaknesses 1. Lack of Theoretical Analysis: While the empirical results are compelling the paper lacks a strong theoretical foundation explaining why these attacks are successful. Including a detailed analysis of the gradients information content or any inherent model properties that facilitate such attacks could provide a deeper understanding of the underlying vulnerabilities. 2. Limited Scope of Architectures and Datasets: The investigation primarily focuses on VGG11 model architecture and the CIFAR10 dataset. Expanding the evaluation to include a wider range of models and datasets especially those with more complex or nonvisual data could improve the generalizability of the findings. 3. Defensive Measures: While the paper briefly mentions potential defense mechanisms against ME attacks in SFL environments it falls short of providing a systematic evaluation of these defenses. A more comprehensive exploration of countermeasures including their efficacy and potential tradeoffs would be highly beneficial.  Questions and Suggestions for Authors 1. Theoretical Insights: Can the authors provide any theoretical backing or intuitive explanation for the success of the proposed ME attacks Insights into why certain models or layers are more susceptible could be invaluable. 2. Wider Evaluation: Could the authors extend their evaluation to include different model architectures learning tasks or datasets This would help assess the robustness of their findings across various domains. 3. Evaluation of Defense Mechanisms: Are there any defense strategies that the authors have considered but not included in the paper An exposition of potential defenses and their preliminary evaluation could significantly enrich the paper.  Addressing Limitations and Societal Impact The authors have acknowledged the limitation of their work primarily its focus on a specific model architecture and dataset. Expanding the scope of the study as suggested could help mitigate this limitation. Regarding the potential negative societal impact while the paper exposes vulnerabilities in SFL that could be exploited maliciously it also lays the groundwork for developing more secure federated learning systems. Constructively the authors could emphasize the importance of ongoing research into safeguarding machine learning models against such attacks highlighting the dual nature of their findings as both a warning and a catalyst for advancement in model security.", "vaxPmiHE3S": "Peer Review for \"Scaling Recurrent Neural Networks with Eventbased Gated Recurrent Units (EGRUs)\" 1) Summary: This paper introduces an innovative approach to reduce the computational burden associated with training and inference in Recurrent Neural Networks (RNNs) by presenting a model termed Eventbased Gated Recurrent Unit (EGRU). The EGRU model achieves activitysparsity by performing computations only when input events trigger updates in the units leading to a significant reduction in the amount of computation at each time step. This model extends Gated Recurrent Units (GRU) to an eventbased activitysparse framework demonstrating sparsity in parameter updates during gradient descent thereby enhancing computational efficiency both during inference and training phases. The authors validate the effectiveness of EGRU through benchmark tasks including language modeling and gesture prediction showing competitive or superior performance to stateoftheart RNN models while benefiting from high levels of activitysparsity. 2) Strengths and Weaknesses: Strengths:  Innovation: EGRU is a novel and significant leap towards efficient RNN computation addressing the critical issue of computational scalability in sequence modeling tasks.  Energy Efficiency: The model\u2019s premise on eventbased updates and activitysparsity directly contribute to reducing energy usage which is crucial for deploying largescale neural networks on energyconstrained devices.  Experimental Validation: The paper includes a comprehensive set of experiments showcasing the robustness of EGRUs across different tasks indicating its practical applicability and potential to replace or augment existing GRULSTM based models. Weaknesses:  Complexity of Implementation: Transitioning from traditional GRULSTM to eventbased models like EGRU may pose significant implementation challenges likely requiring considerable adjustments to existing frameworks that may hinder its immediate adoption.  Underexplored Limitations: While the authors mention activitysparsity\u2019s beneficial impact on computation and memory the potential tradeoffs in terms of model convergence speed or sensitivity to hyperparameter settings are insufficiently explored.  Lack of Broader Impact Discussion: Although the paper briefly touches upon the potential societal benefits and risks a more thorough discussion on how scaling up RNNs with models like EGRU could influence largerscale AI applications would have been insightful. 3) Questions and Suggestions for the Authors:  Could the authors detail any specific challenges encountered when implementing EGRU within current machine learning frameworks such as PyTorch or TensorFlow Any insights on overcoming these would be beneficial for researchers aiming to adopt this model.  Regarding the robust performance across diverse tasks can the authors speculate on how EGRUs inherent characteristics might contribute to its adaptability For instance does the eventbased computation lend itself better to certain types of sequence data characteristics  It would be interesting to see a deeper analysis on the impact of the level of activitysparsity on model performance across tasks. Specifically exploring the tradeoffs between sparsity levels computational efficiency and prediction accuracy could offer valuable guidelines for applying EGRU in practice.  Lastly while the paper understandably focuses on computational efficiency a discussion on potential methodologies to further improve upon the current EGRU\u2019s performance (e.g. through architectural tweaks or novel regularization techniques) could provide a roadmap for future research in this direction. 4) Limitations and Societal Impact: The authors have made a commendable effort in addressing limitations and potential negative societal impacts of their work. However the discussion could be expanded to include a more nuanced exploration of how the models ability to enable larger more computationally efficient RNNs could influence both positive outcomes (e.g. advancements in natural language processing realtime translation) and negative consequences (e.g. enhanced capabilities for generating misleading information or deepfakes). As the field moves towards more powerful models considerations around ethical implications and mitigation strategies become increasingly important. Constructive suggestions include the incorporation of fairness and bias evaluation metrics as part of the model assessment process and exploring partnerships with neuromorphic hardware developers to ensure the responsible advancement and application of such technologies.", "oR5WIUtsXmx": "Peer Review for \"Investigating Implicit Regularization in CoordinateMLPs\" 1) Brief Summary: The paper challenges the longheld belief in the deep learning community that the implicit regularization mechanism mainly attributed to stochastic gradient descent (SGD) and overparameterization universally aids in the generalizability of deep neural networks (DNNs). Specifically it casts doubt on this phenomenon in the context of coordinatemultilayer perceptrons (coordinateMLPs) which are specially designed neural networks for encoding highfrequency signals in tasks like shape representation and novel view synthesis in computer vision. The authors utilize a Fourier analysis approach to demonstrate that coordinateMLPs do not naturally bias towards lower complexity solutions thus undermining the efficacy of implicit regularization in these networks. They reveal that increasing a networks capacity or using certain architectural modifications often leads to a suppression of lower frequencies needed for smooth interpolations between training samples directly impacting the models generalization performance. To address this issue the authors propose a novel regularization technique that ensures the preservation of both high and low frequencies fostering better generalization without necessitating architectural adjustments. Experimental results confirm the theoretical insights demonstrating improvements in signal encoding tasks across various setups. 2) Strengths and Weaknesses: Strengths:  Originality and Relevance: The paper tackles a timely and largely unexplored challenge in the understanding of implicit regularization within a specific subset of neural networks presenting a stark counterexample to the prevailing notions within the field. The focus on coordinateMLPs is particularly pertinent given their increasing application in highfidelity signal representation tasks.  Technical Depth: The Fourier analysis methodology used to dissect the behavior of coordinateMLPs is both rigorous and illuminative. The authors successfully translate complex theoretical insights into understandable observations which they then directly tackle with their regularization proposal.  Practical Impact: By proposing a simple regularization method that can seamlessly integrate into existing networks the paper offers immediate tangible benefits for practitioners working with coordinateMLPs potentially enhancing the performance of applications in computer vision and signal processing. Weaknesses:  Empirical Validation Limitations: While the experiments demonstrate the effectiveness of the proposed regularization method in several cases the paper might benefit from a broader set of benchmarks including comparisons with stateoftheart methods that aim to circumvent the described limitations of coordinateMLPs.  Lack of Theoretical Analysis for Deep Networks: The discussion predominantly revolves around shallow architectures for illustrative purposes and admits this as a limitation. Considering the pervasive use of deep architectures in practice extending the theoretical framework to cover these cases more comprehensively would be a critical next step. 3) Questions and Suggestions for the Authors:  Extending to Deeper Architectures: Can the authors discuss potential strategies or challenges in extending their Fourier analysis to deep coordinateMLPs Any preliminary insights or results in this direction would be valuable.  Comparison to Other Regularization Techniques: It would be insightful to see a detailed comparison of the proposed regularization technique against existing methods particularly those aiming at balancing spectral properties or enhancing generalization in neural networks.  Impact on StateoftheArt Performance: How does the introduction of the proposed regularization technique affect the stateoftheart performance in typical tasks where coordinateMLPs are employed Providing such benchmarks could significantly bolster the papers impact. 4) Limitations and Societal Impact: The authors recognize the constrained scope of their analysis to shallow networks and admit the manual tuning requirement of their proposed regularization as limitations. While they do not directly address the potential negative societal impacts the focus on improving the generalizability and reliability of neural networks contributes positively towards minimizing unintended biases and errors in AI applications thus indirectly fostering a responsible advancement in technology. Constructively it would be beneficial for the authors to deliberate more explicitly on the broader implications of their work particularly in the context of highstakes applications (e.g. medical imaging or autonomous navigation) where the fidelity of signal representation and interpolation can have more pronounced consequences.", "wfKbtSjHA6F": "Peer Review of the Paper: \"Investigating Sparse Networks for DataLimited Image Recognition\" 1) Summary of the Paper: The paper presented investigates the efficacy of sparse neural networks identified through iterative magnitude pruning (IMP) based on the lottery ticket hypothesis for image recognition tasks in datalimited scenarios. The authors empirically demonstrate that these \"winning tickets\" or sparse subnetworks perform significantly better than their original dense counterparts in lowdata regimes. By conducting a series of comprehensive experiments across various datasets and incorporating augmentation strategies and finetuning from selfsupervised learning models the authors found that the sparse models not only outperform dense networks by up to 16 but also exhibit robustness to synthetic noise and domain shifts particularly in longtailed classification scenarios. Additionally they discovered that these sparse networks retain denser initial layers which contribute to their superior generalization capability. 2) Assessment of Strengths and Weaknesses: Strengths: a. Novelty and Timeliness: The approach of leveraging sparsity in networks for datalimited image recognition is both novel and timely given the escalating need for efficient AI models in domains with scarce labeled data. b. Comprehensive Experimental Validation: The authors\u2019 extensive experimentation across different datasets including challenging cases of longtailed distributions and domain shifts provides solid evidence supporting their claims. c. Practical Relevance: The paper addresses a significant problem\u2014improving performance in datalimited regimes\u2014with a solution that is practical and potentially impactful for realworld applications especially in domains like healthcare. d. Accessibility of Artifacts: Making the code available enhances the reproducibility of the work and encourages further research in this direction. Weaknesses: a. Lack of Theoretical Underpinning: While the empirical results are compelling the paper lacks a deeper theoretical analysis of why sparse networks perform exceptionally well in datalimited scenarios. b. Generalization Beyond Image Recognition: The focus is predominantly on image recognition tasks. Its unclear how well these findings generalize to other types of tasks or data modalities such as text or audio. c. Comparison with StateoftheArt: The paper could benefit from a more detailed comparison with the current stateoftheart methods for datalimited learning including fewshot and zeroshot learning approaches. 3) Questions and Suggestions for the Authors: a. Theoretical Insights: Can the authors provide any theoretical insights or hypotheses regarding the observed effectiveness of sparse networks in datalimited scenarios Including a discussion on this could enrich the papers depth. b. Extension to Other Modalities: Have the authors considered extending their experiments to tasks beyond image recognition Insights into such an extension could make the paper more appealing to a broader audience. c. Hyperparameter Sensitivity: The paper could benefit from a discussion on the sensitivity of the approach to hyperparameters particularly the pruning rate and the number of pruning iterations. d. Impact of Data Augmentation: It would be interesting to see a more detailed analysis of how different augmentation strategies specifically contribute to the performance improvements of sparse networks. 4) Limitations and Societal Impact: The paper mentions the benefits of sparse networks in datalimited regimes and their robustness to distributional shifts but it doesn\u2019t thoroughly discuss the potential limitations. For instance the computational cost of finding these winning tickets and the potential for reduced performance on certain types of image classification tasks should be addressed. Moreover the societal impacts both positive (e.g. enabling efficient models in resourceconstrained environments) and negative (e.g. potential misuse in surveillance) deserve attention. Constructive suggestions include:  Incorporating a discussion on the computational overhead associated with the iterative pruning process and exploring efficiency improvements.  Assessing and discussing the ecological impact of training such models given the growing concern around the carbon footprint of machine learning.  Including a broader discussion on ethical considerations and the dualuse nature of technology emphasizing responsible use and deployment.", "muvlhVKvd4": "Review of \"On the Unified Convergence of Stochastic Optimization Methods\" 1) Summary: This paper presents a fundamental unified convergence theorem that can be applied to derive both expected and almost sure convergence results for a broad class of stochastic optimization methods. The theorem does not focus on specific algorithms but instead establishes several abstract conditions applicable to various problem structures and algorithms. The authors illustrate the utility of their theorem by applying it to several classical stochastic optimization methods including the stochastic gradient method (SGD) random reshuffling (RR) the stochastic proximal gradient method (proxSGD) and stochastic modelbased methods. For these applications the paper recovers known results under more general settings and establishes new convergence results for certain methods particularly in nonconvex optimization settings. This demonstrates the theorems capacity as a plugintype tool for convergence analysis across a wide array of stochastic optimization methods. 2) Assessment: Strengths:  The paper addresses a significant and challenging problem in the domain of stochastic optimization: establishing a unified framework to derive convergence results across various methods. This contributes to a deeper understanding of fundamental properties of these methods and their interrelations.  The unified theorem is elegantly designed and applicable to a broad spectrum of problem settings and algorithms including nonconvex problems which are notoriously difficult to analyze.  The presentation of new convergence results for proxSGD and stochastic modelbased methods in nonconvex settings is particularly valuable filling a gap in the existing literature.  The methodology for applying the unified theorem is wellstructured and clearly illustrated through concrete examples making the theoretical framework accessible to readers and applicable in practice. Weaknesses:  While the paper presents a novel theorem and demonstrates its application to several algorithms there is limited discussion on the practical implications of these theoretical results. Specifically the impact of these results on the design selection or implementation of stochastic optimization methods in realworld applications remains unclear.  The manuscript could benefit from a more detailed comparison with related works particularly in discussing the novelty and advantages of the proposed approach over existing methods for analyzing convergence.  The paper could further address the applicability of the unified convergence theorem to emerging algorithms or problem settings not covered in the applications section providing insight into the theorems potential scope and limitations. Questions and Suggestions for the Authors:  Could the authors elaborate on potential practical implications or applications of their convergence results How might these theoretical insights influence the design or choice of stochastic optimization methods in specific realworld scenarios  A more detailed comparison with related works would enhance the readers understanding of the contributions novelty. Specifically illustrating the differences in assumptions applicability or results between the proposed approach and existing methods would be helpful.  It would be beneficial to discuss the potential applicability of the unified convergence theorem to other emerging stochastic optimization algorithms or problem settings not already covered in the paper. Are there particular algorithms or settings where the authors expect their theorem might encounter limitations or require significant adaptation Limitations and Societal Impact:  The authors have not discussed the limitations of their work in detail. While the paper outlines a theoretical framework applicable to a wide class of algorithms acknowledging potential limitations or conditions under which the theorem may not apply would strengthen the manuscript.  The potential negative societal impact of the work is not addressed. While primarily theoretical any optimization technique could be employed in contexts with ethical considerations (e.g. surveillance algorithmic trading). Discussing the broader implications of enhancing stochastic optimization methods especially in sensitive applications would be a responsible addition. Improvement Suggestions:  Provide a section discussing the practical implications of the convergence results with examples of realworld applications where these insights could be particularly beneficial.  Expand the comparison with related works highlighting the distinct contributions and advantages of the proposed approach in a more detailed manner.  Explore the applicability of the unified theorem to other algorithms or settings discussing potential limitations and future research directions explicitly.  Address the broader societal implications of the work acknowledging areas where enhanced optimization methods could have both positive and negative impacts.", "u6GIDyHitzF": "Review of \"Batched Dueling Bandit Algorithm under the Condorcet Condition\" 1. Summary: This paper introduces a new approach to the Karmed dueling bandit problem specifically focusing on the \"batched\" setting where updates are made in batches rather than sequentially. The key question addressed is whether a limited number of adaptive rounds can match the asymptotic regret bounds of the bestknown sequential algorithms under the Condorcet condition which assumes the existence of a superior arm. The proposed algorithm denoted as C2B achieves O(K2 log2(K))  O(K log(T)) asymptotic regret in O(log(T)) batches closely approximating the optimal sequential bounds. Computational experiments using realworld datasets validate the nearequivalence in performance between the proposed batched approach and traditional sequential algorithms. 2. Assessment: Strengths:  Novelty: The paper tackles the less explored batched dueling bandit problem proposing a novel algorithm that nearly matches the performance of sequential algorithms while significantly reducing the adaptation frequency.  Theoretical Contribution: The provided regret bounds under the Condorcet condition are mathematically rigorous and constitute a significant theoretical advancement in understanding batched learning scenarios.  Practical Relevance: The authors validate their theoretical findings through extensive simulations on realworld datasets demonstrating the algorithms practical relevance and effectiveness.  Clarity and Depth: The paper is wellorganized methodically presenting the problem algorithm design theoretical analysis and experimental results making it accessible to readers with varying levels of expertise. Weaknesses:  Comparative Analysis: While the paper presents a comparison with other algorithms it primarily focuses on the asymptotic performance. A more detailed exploration of finitetime behavior especially in practical nonasymptotic scenarios would provide a more comprehensive comparison.  Algorithm Complexity: The paper could benefit from a more detailed discussion on the computational complexity of the C2B algorithm including time and space requirements especially compared to the sequential algorithms.  Broader Applicability: The focus is predominantly on the Condorcet condition. Including insights on how the proposed method could extend to scenarios where a Condorcet winner does not exist would enhance the papers relevance. 3. Questions and Suggestions:  FiniteTime Behavior: Can the authors provide more insight into the finitetime regret behavior of the C2B algorithm potentially through additional experimental analysis or theoretical bounds  Computational Complexity: It would be beneficial to include a detailed analysis of the computational complexity of the C2B algorithm. How does it scale with the number of arms (K) and the time horizon (T)  Extension to Other Conditions: Do the authors foresee any extensions of the C2B algorithm to scenarios where the Condorcet condition might not hold Any initial thoughts or theoretical considerations would be valuable.  Additional Datasets: While the current selection of datasets is commendable including datasets from domains other than web search ranking (where batch updates might be even more crucial) could further demonstrate the algorithm\u2019s versatility. 4. Limitations: The authors have made a commendable effort in acknowledging and addressing the limitations of their work. However the discussion on potential negative societal impacts especially in decisionmaking systems where the algorithm might be applied is missing. For instance how might biases in pairwise comparisons or dataset selection affect the algorithms fairness or transparency Including a brief discussion on these aspects along with suggestions for mitigation would make the paper more comprehensive. In conclusion this paper presents an important step forward in the batched dueling bandit problem offering theoretical advancements and practical validation. Incorporating the suggested improvements especially regarding comparative analysis algorithm complexity and broader applicability would enhance the papers contribution significantly.", "mT18WLu9J_": "Review for the paper titled \"Exploiting Data Poisoning to Amplify Membership Inference Attacks in Machine Learning Models\" 1. Summary: The paper presents a novel investigation into how data poisoning attacks traditionally recognized for causing model performance degradation and controlled misprediction can be repurposed to increase privacy risks particularly through amplifying the exposure risk in membership inference attacks. The authors introduce a dirtylabel poisoning attack for generic supervised learning and an optimizationbased cleanlabel attack tailored for transfer learning scenarios. These attacks are evaluated on multiple computer vision benchmarks demonstrating their effectiveness in increasing membership inference precision with minimal impact on overall model performance. Moreover the work assesses potential countermeasures to mitigate these privacy threats. 2. Assessment: Strengths:  The paper tackles an underexplored but critical intersection of data poisoning and privacy leakage in machine learning presenting a timely contribution given the prevalent use of inthewild data for training modern AI systems.  The differentiation between dirtylabel and cleanlabel attacks and their respective adaptations to supervised and transfer learning scenarios provide comprehensive insight into the attack landscape contributing valuable knowledge to both the privacy and security domains of machine learning.  Extensive empirical evaluations affirm the threats posed by the proposed attacks and the attempt to explore countermeasures shows a responsible approach toward ethical AI research. Weaknesses:  The papers reliance on the assumption that attackers have knowledge of the feature extractor in the cleanlabel attack scenario might limit the applicability of these findings in realworld settings where such information might not be accessible.  While the paper explores potential countermeasures there seems to be a lack of deep exploration into how proposed defenses like differential privacy and regularization specifically interact with the unique characteristics of the attacks presented. This gap leaves questions regarding the practical deployment of these countermeasures.  The discussion on negative societal impacts is somewhat limited. Given the severe implications of increased privacy risks through these attacks a more thorough exploration of potential misuses and broader societal ramifications would have been beneficial. 3. Questions and Suggestions:  Can the authors discuss any potential realworld scenarios where attackers might leverage the proposed cleanlabel poisoning attack without knowledge of the feature extractor Exploring this might provide insight into how grave the threat could be in a blackbox setting.  The countermeasures explored provide a starting point but can the authors speculate on or suggest more advanced defense mechanisms particularly those that might utilize anomaly detection or adversarial training specifically tailored to detect and mitigate the described poisoning attacks  It would be interesting to see a deeper exploration of the tradeoffs involved in implementing the suggested countermeasures specifically regarding model performance and user privacy possibly through additional ablation studies. 4. Limitations and Suggestions for Improvement:  The authors have made a commendable effort in addressing the limitations and potential negative societal impacts. However a more detailed exploration of the implications of these attacks in sensitive applications such as healthcare or finance where privacy concerns are paramount could further highlight the urgency of developing robust countermeasures.  Given the novelty and potential impact of the proposed attacks a broader discussion on ethical considerations and guidelines for researchers and practitioners in deploying machine learning models especially in privacysensitive domains would enhance the papers contribution. Conclusion: This paper provides an important contribution to understanding and mitigating privacy risks in machine learning particularly through the lens of data poisoning. While there are areas for further exploration particularly regarding the practical deployment of countermeasures and understanding realworld implications the work lays a solid foundation for future research at the intersection of data integrity privacy and machine learning security.", "vdh62914QR": "Review of \"Generalization Error Analysis for ZerothOrder Stochastic Search (ZoSS) Algorithms\" 1) Summary of the Paper: This paper provides a comprehensive generalization error analysis for ZerothOrder Stochastic Search (ZoSS) algorithms which are a class of derivativefree optimization techniques. These algorithms are crucial for blackbox learning situations where the gradient of the loss function is unavailable or difficult to compute. By employing stochastic differences of perturbed loss evaluations the ZoSS algorithm updates model parameters without requiring direct gradient information. The key contributions include establishing generalization error bounds for ZoSS algorithms under the assumption of Lipschitz and smooth loss functions. Notably the paper demonstrates that ZoSS can achieve generalization error bounds comparable to Stochastic Gradient Descent (SGD) with a slightly decreased learning rate independent of the models dimensionality number of evaluations and batch size under certain conditions. These results extend existing work on SGD covering both bounded and unbounded nonconvex loss scenarios and provide novel insights into fullbatch Gradient Descent (GD) generalization guarantees. 2) Assessment of Strengths:  Innovation: The paper introduces significant advancements in the understanding of generalization errors in zerothorder optimization a domain that has direct applications in many critical learning scenarios. The comparative analysis with SGD is particularly insightful.  Theoretical Depth: The rigorous stabilitybased analysis extends and refines existing methodologies to accommodate gradientfree settings. The derivation of generalization bounds which match those of SGD under specific parameters represents a valuable theoretical contribution.  Broad Applicability: By covering a wide range of loss constraints (boundedunbounded convexnonconvex) the paper ensures the relevance of its findings across multiple domains including adversarial learning federated learning and beyond.  Clarity and Structure: Despite the technical depth the authors have managed to present their findings in a clear and structured manner. The division of the paper into welldefined sections aids in navigating the complex analyses and their implications. 3) Questions and Suggestions for the Authors:  Experimental Verification: While the theoretical contributions are solid the paper lacks empirical validation of the theoretical findings. How do ZoSS algorithms perform in realworld scenarios compared to SGD especially in highdimensional settings Including a section on experimental results could significantly strengthen the paper.  Discussion on Learning Rate Choices: The paper mentions \"appropriate choices of a slightly decreased learning rate\" for achieving similar generalization bounds to SGD. It would be beneficial to provide a more detailed discussion or guidelines on how to select these learning rates practically.  Extension to NonSmooth Losses: The focus is mainly on Lipschitz and smooth loss functions. Considering the importance of nonsmooth optimization in machine learning could the authors comment on how their analysis might extend to nonsmooth settings  Additional Insights into Batch Size Independence: The finding that generalization error and learning rate remain independent of the batch size for bounded nonconvex losses is intriguing. Could the authors provide more intuitive explanation or insights into why this independence holds 4) Limitations and Potential Negative Societal Impact: The authors have made an attempt to address the limitations and potential negative societal impacts of their work. However this discussion could be expanded. In particular the potentially adversarial applications of ZoSS such as in crafting blackbox adversarial attacks raise ethical considerations that merit further discussion. Moreover while the theoretical analyses are comprehensive the lack of empirical evaluation may limit the immediate applicability of the findings to realworld problems. Constructive suggestions include:  Enhancing the Discussion on Ethical Use: Offer explicit guidelines or ethical considerations for using ZoSS in sensitive applications particularly in adversarial machine learning.  Addressing the Lack of Empirical Evidence: Encourage the inclusion of experimental results in future work to validate the theoretical claims and ensure their practical relevance.  Exploring Broader Impacts: A more detailed discussion on how ZoSS could impact various fields beyond machine learning positively or negatively would provide a more holistic view of the technologys societal implications.", "yZgxl3bgumu": "Review of \"Leveraging Graph Neural Networks for Simulating Soft Tissue Deformation in Neurosurgical Settings\" 1. Brief Summary: The paper introduces a new datadriven framework PhysGNN aimed at addressing the challenge of capturing intraoperative brain shift during neurosurgical procedures. This framework leverages graph neural networks (GNNs) to approximate the soft tissue deformation under prescribed forces an approach that promises to balance the tradeoff between accuracy and computational speed inherent in finite element method (FEM)based techniques. PhysGNN stands out by incorporating mesh structural information into its learning process which is a notable gap in existing datadriven models that simulate tissue deformation. The authors present empirical evidence to demonstrate PhysGNNs ability to accurately and efficiently predict soft tissue deformation positioning it as a competitive alternative to stateoftheart algorithms. 2. Assessment of Strengths and Weaknesses: Strengths:  Novelty: The integration of GNNs for simulating tissue deformation under prescribed forces is a novel approach. By leveraging mesh structural information PhysGNN addresses a significant limitation in existing machine learning models.  Accuracy and Efficiency: The paper provides a compelling empirical evaluation showing that PhysGNN achieves high accuracy in predicting tissue deformation with significantly reduced computational times compared to traditional FEM simulations.  Practical Relevance: The ability to accurately simulate soft tissue deformation has direct implications for improving the precision of neurosurgical interventions making this work highly relevant to current medical practices.  Reproducibility: By making their data and implementations publicly accessible the authors encourage reproducibility and further research setting a commendable benchmark in the field. Weaknesses:  Limited Comparison with Existing Methods: Although the paper compares PhysGNN with stateoftheart methods the comparisons appear to be primarily qualitative. A more rigorous quantitative comparison would strengthen the paper\u2019s claims.  Generalization to Other Settings: While the paper focuses on neurosurgery it does not discuss the applicability of PhysGNN to simulate deformations in other types of soft tissues or in different surgical contexts.  Addressing Variation Across Patients: The paper does not deeply explore how PhysGNN accounts for variability in tissue properties across different patients which could affect the models predictive accuracy in realworld scenarios. 3. Questions and Suggestions for the Authors:  Comparative Analysis: Could you provide a more detailed comparison possibly including quantitative metrics between PhysGNN and other existing machine learning models that have aimed to address similar problems  Generalization Capabilities: It would be beneficial if you could discuss or provide insights into how PhysGNN might be adapted or tested for other soft tissues or surgical settings beyond neurosurgery.  Patient Variability: How does PhysGNN handle variations in tissue properties among patients Incorporating adaptive or personalized adjustments based on individual patient data could be an area for further exploration.  RealWorld Applicability: Can you elaborate on any plans or ongoing efforts to validate PhysGNN\u2019s predictions with intraoperative data or in clinical settings 4. Limitations and Potential Negative Societal Impact: The authors have made a commendable effort in outlining the limitations related to comparisons with existing studies due to accessibility and computational power. They also mention the dependency of FEA results on MRI image segmentation quality and selected constitutive laws which adds to the realism of their discussion. However a critical analysis of the potential negative societal impacts of their work is missing. For example reliance on technically complex models like PhysGNN could inadvertently widen the gap between wellfunded technologically advanced medical institutions and those in lowerresource settings. To address this potential limitation the authors could explore ways to simplify the implementation or reduce the computational resources required for PhysGNN. Additionally discussing strategies for disseminating and training medical professionals in diverse settings could further enhance the papers contribution to equitable advancements in surgical practices.", "y5ziOXtKybL": " Peer Review of \"Consistency and Optimal Convergence of Bayesian Neural Networks with SpikeandSlab and Shrinkage Priors\"  1. Summary of the Paper This paper investigates the theoretical underpinnings of Bayesian neural networks (BNNs) with a focus on posterior consistency and the convergence rates of models applied to regression functions belonging to the Besov space. The authors specifically consider BNNs equipped with spikeandslab priors and shrinkage priors demonstrating that these networks can achieve near minimax optimal convergence rates without the need for prior knowledge of the regression functions smoothness. Through rigorous theoretical analysis the paper extends existing results related to BNNs and the approximation capabilities of neural networks with Rectified Linear Unit (ReLU) activation functions. The paper also discusses practical implications by suggesting a shrinkage prior that is computationally more feasible than the spikeandslab prior while achieving similar theoretical guarantees.  2. Assessment of the Paper Strengths:  The paper addresses an important question in the domain of Bayesian neural networks focusing on the consistency and convergence properties which are crucial for the practical deployment of these models.  The extension of existing theories to the Besov space and the demonstration that BNNs with specific priors can achieve near minimax optimal convergence rates without knowing the true functions smoothness represent significant theoretical advances.  The paper is wellstructured starting from a thorough introduction related works and preliminaries to the main results and practical considerations which facilitates understanding.  The authors introduce a computationally feasible shrinkage prior that maintains theoretical guarantees which is valuable for practical implementations of BNNs. Weaknesses:  While the theoretical results are compelling the paper presents limited empirical evidence to support the practical applicability of the proposed models. Additional experimental results especially comparisons with existing approaches on real datasets would strengthen the paper.  The discussion on the potential limitations and societal implications of the proposed methods is absent. Given the widespread application of neural networks acknowledging and addressing possible negative impacts is important.  The paper is highly technical which while necessary for conveying rigorous mathematical results could limit its accessibility to a broader audience.  3. Questions and Suggestions for the Authors  Question: How sensitive are the proposed BNN models to the choice of hyperparameters (e.g. network depth and width) in practical scenarios Do the theoretical properties hold under a broad range of configurations  Suggestion: It would be beneficial to include experimental validation on benchmark datasets and compare the performance of the proposed BNN models with both traditional neural networks and alternative Bayesian approaches. This would highlight the practical advantages of the proposed priors in realworld settings.  Suggestion: Consider discussing potential limitations of the proposed approach such as computational complexity and scalability to very large datasets. Providing guidelines or strategies to mitigate these challenges would be helpful for practitioners.  Question: Could the authors elaborate on how the proposed models might be adapted or extended to classification tasks considering the focus on regression in the current work  4. Limitations and Potential Negative Societal Impact The authors do not provide a discussion on the limitations or the potential negative societal impact of their work. It is suggested that the authors:  Briefly discuss the computational requirements and scalability of the proposed BNN models acknowledging any inherent limitations that might impact practical applicability.  Address potential pitfalls in the deployment of BNN models such as overreliance on predictive uncertainty which might lead to overconfident decisions in critical applications.  Consider the ethical implications of automating decisions based on predictions from BNNs especially in domains with significant societal impact (e.g. healthcare law enforcement). Constructive suggestions for improvement include providing strategies to enhance model transparency and interpretability as well as recommendations for responsible application of BNNs in sensitive areas.", "rcMG-hzYtR": " 1. Brief summary of the paper and its contributions The paper at hand introduces a novel approach to reinforcement learning particularly focusing on the optimization of policies for realworld application from simulation environments. This optimization addresses the issue of model misspecification due to discrepancies between simulation and realworld environments a common challenge in deploying deep reinforcement learning (DRL) for realworld control tasks. The primary contribution is the development of the MaxMin Twin Delayed Deep Deterministic Policy Gradient algorithm (M2TD3) an offpolicy actorcritic approach designed to optimize worstcase performance within a defined set of uncertainty parameters. By incorporating a technique of simultaneous gradient ascent descent the study claims to effectively tackle a maxmin optimization problem therefore guaranteeing improved realworld policy performance. The algorithm is evaluated on 19 tasks using the MuJoCo environment where it reportedly outperforms multiple baseline approaches in terms of worstcase performance.  2. Assessment of strengths and weaknesses Strengths:  Novelty: The introduction of M2TD3 is innovative addressing the significant challenge of model misspecification by optimizing worstcase performance a muchneeded approach in the field of reinforcement learning.  Practical Relevance: This study addresses a critical barrier to the realworld application of DRL providing a pathway towards more reliable deployment of learned policies in environments where direct interaction is costly or risky.  Robust Evaluation: The thorough assessment across 19 MuJoCo tasks and comparison with several baselines demonstrate the algorithm\u2019s effectiveness and robust performance.  Publicly Available Implementation: Providing the algorithm\u2019s implementation fosters transparency and allows for community verification experimentation and further development. Weaknesses:  Limited Discussion on Computational Overheads: While the method shows improved performance the paper lacks detailed discussion on the computational costs and practicality of employing M2TD3 especially in more complex realworld scenarios.  Unclear Limitation Discussion: The paper briefly mentions the potential suboptimal performance in scenarios with small uncertainty sets but doesn\u2019t thoroughly explore or provide solutions for such cases.  Potential for Bias in Evaluation: While the experiments are comprehensive the paper does not discuss whether the selection of tasks or baseline comparisons might inherently favor the proposed approach.  3. Questions and Suggestions for the Authors  Computational Efficiency Inquiry: Can the authors elaborate on the computational overhead introduced by the trilevel optimization and simultaneous gradient methods Are there recommended approaches for managing these costs in complex scenarios  Handling Small Uncertainty Sets: Could the authors discuss potential strategies or modifications to M2TD3 that might improve its performance or stability in scenarios with smaller uncertainty parameter sets  Bias Mitigation in Task Selection: How did the authors ensure that the selection of tasks and baseline algorithms for comparison did not introduce bias towards the proposed M2TD3 algorithm  4. Limitations and Suggestions for Improvement While the authors have made significant contributions the discussion on limitations and potential negative societal impacts is somewhat lacking. Suggestions for Improvement:  Broader Limitation Discussion: Expanding the discussion on the algorithm\u2019s limitations particularly in different or more complex realworld scenarios would be valuable. This includes detailing scenarios where M2TD3 may not be the optimal choice and providing guidance on mitigating these limitations.  Societal Impacts: The paper could benefit from a section discussing potential negative societal impacts especially considering reinforcement learning\u2019s growing application in critical domains. For example optimizing for worstcase performance may promote overly conservative behaviors in systems where bold or innovative strategies might be needed.  Enhanced Evaluation for RealWorld Application: Although simulations are necessary incorporating case studies or pilot tests in actual realworld environments could significantly strengthen the evidence for M2TD3\u2019s practical effectiveness and reliability.", "m6HNNpQO8dc": " Review of the Paper \"Introducing LogitSpace Probabilistic Boolean Operators as Activation Functions in Neural Networks\"  1) Brief Summary The paper presents an innovative approach to understanding and implementing activation functions in artificial neural networks (ANNs) through a novel lens: the probabilistic Boolean logic applied in logitspace. The key contributions include the derivation of logitspace equivalents for AND OR and XNOR operators assuming independence between probabilities. To circumvent the computational challenges posed by these exact operations the authors propose computationally efficient approximations (ANDAIL ORAIL XNORAIL) that simplify calculations to comparisons and additions making them practically implementable as activation functions. Theoretical underpinnings are provided alongside empirical validations across various tasks such as image classification transfer learning and abstract reasoning. These novel functions aim to redistribute computational effort into the neural level promising parameterefficient models while potentially necessitating fewer network layers.  2) Strengths and Weaknesses Strengths:  Innovation: The paper addresses the gap in activation function design by leveraging complex dendritic computations found in biological neurons thereby pushing the boundary of traditional neural network functionalities.  Theoretical Foundation: A strong theoretical base is established for the derivation of ANDAIL ORAIL and XNORAIL connecting symbolic AIs logical operations with ANNs instinctive processing.  Broad Evaluation: The methods are tested across a variety of tasks demonstrating the versatility and potential applicability of the proposed activation functions.  Efficiency: The proposed activation functions are designed to be computationally efficient addressing a critical consideration for the practical deployment of neural networks. Weaknesses:  Assumption of Independence: The functions assume independence between logits however this assumption might not always hold potentially limiting the applicability of the activation functions in complex realworld scenarios.  Evaluation Depth: While the paper tests the activations across several tasks the analysis on each task seems relatively shallow. A more detailed analysis possibly including error analysis and comparisons with a broader range of baseline activation functions could provide deeper insights.  Complexity Concerns: Despite the computational efficiencies introduced the conceptual and implementation complexity of these activation functions is higher than traditional functions like ReLU or sigmoid. This complexity could hinder widespread adoption without significant performance gains.  3) Questions and Suggestions for the Authors  Q1: How do the proposed activation functions perform in deeper networks with more complex architectures beyond the ones tested (e.g. Transformer models) Suggestion: It would be beneficial to explore and elaborate on the performance and implications of using ANDAIL ORAIL and XNORAIL in various modern deep learning architectures.  Q2: Could the authors provide more insights into the choice of tasks for evaluation For instance what was the rationale behind choosing these specific tasks and how do they demonstrate the unique strengths of the proposed activation functions Suggestion: Include a justification for the selection of tasks and ideally expand the evaluation to more tasks particularly those where current activation functions struggle.  Q3: The paper briefly mentions the potential negative societal impact of implementing these functions. Could this be expanded upon especially in the context of fairness accountability and transparency in AI Suggestion: It would be insightful to include a dedicated section discussing the broader implications of deploying these activation functions in realworld applications considering both technical and ethical aspects.  4) Limitations and Societal Impact The paper does well to advance the theory and practical application of activation functions in neural networks. However the limitations regarding the assumption of independence between logits and the potential complexity barriers to adoption could limit the immediate impact. On societal aspects while the authors briefly touch on the implications a deeper dive into the potential negative consequences such as biases or errors in decisionmaking processes caused by these new functions would be prudent. The integration of more complex neurons could hypothetically make models more \u201cblackbox\u201d raising concerns about interpretability and accountability in AIdriven decisions. Suggestions for Improvement:  Consider exploring mechanisms to relax the independence assumption or provide guidelines on scenarios where the proposed functions are most effective.  Deepen the evaluation on each task with comparative studies against more activation functions and detailed error analyses to better understand the conditions under which the proposed functions excel or fall short.  Elaborate on the computational complexity and practical implementation concerns to aid adoption.  Include a more comprehensive discussion on ethical considerations focusing on transparency interpretability and potential biases introduced by these new activation functions.", "luGXvawYWJ": " Review of \"Exploring Dataset Factorization through HaBa for Enhanced Dataset Distillation\"  1) Summary of the Paper This paper introduces a novel perspective and methodology for dataset distillation (DD) named HaBa (HallucinatorBasis) to address the limitations faced by traditional DD methods regarding information loss and overfitting due to the distilled datasets size and composition. The authors propose a dataset factorization strategy that decomposes a dataset into hallucination networks and bases enabling the reconstruction of image samples through flexible combinations thereby significantly improving data representation capability and data efficiency of distilled datasets. They introduce adversarial contrastive constraints to further enhance the diversity and informativeness of generated images. Extensive experimental results demonstrate HaBas superiority over stateoftheart methods in improving downstream task performances while reducing storage costs.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: HaBa introduces a unique factorization approach to DD addressing previous methods limitations by significantly increasing the informativeness and diversity of distilled datasets.  PlugandPlay Compatibility: As a versatile strategy applicable to existing DD baselines HaBa offers broad utility across different DD frameworks showcasing its adaptability.  Empirical Evidence: The comprehensive experiments across multiple benchmarks and settings not only validate HaBas effectiveness but also its superiority in enhancing downstream model training and generalization abilities with reduced parameter storage requirements.  Societal Impact Consideration: The method indirectly addresses potential privacy and copyright issues in publishing raw data by offering an efficient data compression scheme that limits the exposure of original data details. Weaknesses:  Increased Computational Complexity: While achieving performance improvements and storage efficiency the online pairwise combination process of HaBa introduces slightly higher time and GPU memory costs.  Dependence on Baseline Method Limitations: The performance of HaBa can be constrained by the inherent limitations of the baseline DD methods it builds upon potentially limiting gains in settings with large numbers of images.  3) Questions and Suggestions for the Authors 1. Regarding Dataset Diversity: How does HaBa ensure that the factorized components (hallucination networks and bases) maintain a balanced representation of all classes in the original dataset especially in highly imbalanced datasets 2. On Computational Efficiency: Could the authors detail the tradeoffs in computational efficiency versus performance gains provided by HaBa and any ways to mitigate the increased computational costs 3. Future Work Direction: Can HaBa be extended to other types of data beyond images such as timeseries or textual data for dataset distillation purposes Suggestions:  It would be beneficial to explore optimization strategies for hallucinator and basis generation processes to reduce the added computational overhead.  Investigating the impact of varying the number of hallucinators based on class distribution or interclass relationships might yield further performance improvements or efficiency gains.  Conducting user studies or realworld application tests could supplement the empirical evidence and showcase HaBas practical utility further.  4) Limitations and Societal Impact The authors have adequately addressed the potential limitations and societal impact of their work. However a deeper discussion on mitigating the slightly increased time and computational resources due to HaBas complex factorization processes could make the method more accessible to users with limited computational capabilities. Additionally while the paper touches upon the positive aspects of reducing privacy concerns through dataset compression explicitly addressing any potential misuse of hallucination networks in generating synthetic data that could be misleading or harmful would provide a more balanced view of the societal implications. Constructively the paper could benefit from exploring variational approaches to hallucinator and basis generation to reduce computational costs and examining safeguards against generating nonrepresentative or biased data through these factorized components.", "pZtdVOQuA3": "Review of \"An Importance Sampling Approach for Efficient Training and Rendering of Neural Radiance Fields\" 1) Summary This paper introduces an alternative rendering algorithm designed for neural radiance fields leveraging importance sampling to enhance efficiency in view synthesis. The traditional neural radiance field framework synthesizes novel views by densely sampling points along rays cast through pixels calculating radiance emitted at each point and then integrating these contributions. This process while effective is computationally expensive as it involves numerous evaluations of the underlying density and radiance fields. The authors propose a method that identifies nontransparent points on rays through importance sampling based on the distribution induced by the density field thereby reducing the number of necessary radiance evaluations. They also introduce a reparameterization of the sampling process enabling endtoend learning similar to the original approach but with fewer evaluations per ray. The method shows competitive model and rendering quality while significantly reducing computational costs. 2) Assessment Strengths:  The paper addresses a critical issue of computational efficiency in rendering neural radiance fields a topic of significant relevance given the growing interest in highfidelity 3D scene reconstruction and rendering.  The proposed importance sampling strategy is both intuitive and mathematically sound grounding the work in solid theoretical foundations.  The techniques compatibility with existing neural radiance field structures allowing it to serve as a \"dropin\" replacement for more traditional volume rendering algorithms suggests the potential for widespread applicability and ease of adoption.  The experimental evidence provided supports the claim of achieving comparable rendering quality to baseline methods while notably reducing computational and memory requirements. Weaknesses:  The paper could benefit from a broader set of experiments particularly comparing the proposed method against more recent and diverse baselines in rendering neural radiance fields.  While the method reduces computational costs the paper lacks a thorough discussion and analysis of any potential tradeoffs in rendering fidelity or limitations in scenarios with complex geometric or lighting conditions.  The discussion on potential negative societal impacts is quite superficial. Given the broad applicability of neural rendering technologies a deeper consideration of implications including misuse or ethical concerns would be beneficial. 3) Questions and Suggestions for the Authors:  Could the authors clarify if there are specific types of scenes or lighting conditions where the importance sampling method might struggle to maintain quality or efficiency  It would be valuable to see a comparison or analysis of how the proposed method performs relative to other stateoftheart approaches in terms of not just efficiency but also scalability and adaptability to different scene complexities.  The paper mentions potential applications in online retail for improved user experiences. Could the authors expand on this with examples or suggested pathways for integration Limitations and Societal Impact:  The authors mention the potential for reduced computational costs and environmental benefits which is commendable. However there seems to be a lack of discussion on the limitations particularly regarding the possible degradation of quality in complex scenes or under varied lighting conditions.  In terms of societal impact while the positive aspects of improved user experience in ecommerce are highlighted theres a missed opportunity to discuss broader implications. For example the democratization of highquality 3D content creation or concerns around the creation of photorealistic deepfakes.  A constructive suggestion would be to incorporate a deeper analysis of these aspects potentially including a section on ethical considerations and a more thorough discussion on limitations related to scene complexity lighting and material properties.", "wFymjzZEEkH": " Review of \"lpproj: A Personalized Federated Learning Approach with LowDimensional Projection and Infimal Convolution Regularization\"  1. Summary of the Paper The paper introduces a novel personalized Federated Learning (FL) algorithm termed lpproj designed to mitigate issues such as high communication costs susceptibility to adversarial attacks during training and unfair performance distribution across devices. By projecting local models into a shared lowdimensional random subspace and employing infimal convolution for regularization the authors aim to maintain a balance between local and reference models achieving personalization while addressing the mentioned challenges. They provide a theoretical convergence analysis for smooth objectives with square regularizers demonstrating the methods robustness and fairness particularly in linear problems. Empirical results showcase its superiority over existing stateoftheart (SOTA) methods in terms of communication efficiency attack resistance and performance equity across clients.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty and Theoretical Contribution: The paper presents a new perspective on tackling the classic challenges of personalized FL by introducing a lowdimensional projection and infimal convolution regularization. The theoretical foundation and convergence analysis add significant value.  Comprehensive Evaluation: Extensive experiments demonstrate the empirical advantages of lpproj over several benchmarks and SOTA methods concerning communication efficiency robustness to adversarial attacks and performance fairness.  Robustness and Fairness: The methods ability to provide robust performance under various adversarial models and its tendency to ensure more uniform accuracy across clients are particularly commendable. Weaknesses:  Complexity and Implementation Details: The paper could benefit from a clearer exposition of the algorithms complexity and more detailed guidance on replication such as pseudocode completeness and parameter selection insights.  Generalizability of Theoretical Results: While the paper provides a solid theoretical foundation within the scope of its assumptions it remains somewhat narrow. Expanding the theoretical analysis to encompass nonlinear models and other forms of regularizers could enhance the works applicability.  Discussion on Societal Impact: The authors briefly mention potential negative societal impacts but do not thoroughly explore this aspect. A more indepth discussion would have been beneficial especially considering the increasing concern over algorithmic fairness and the ethical aspects of ML.  3. Questions and Suggestions for the Authors  Algorithm Complexity: Could the authors elaborate on the computational complexity of lpproj especially in comparison to conventional FL and personalization methods This would help readers assess the practical feasibility of the approach.  Extension to NonLinear Models: The paper mainly discusses linear problems for theoretical analysis. Are there plans to extend this analysis to nonlinear models and if so what challenges do the authors anticipate  Impact of Hyperparameter Choices: The role of hyperparameters particularly \u03bb and the dimension of the projection subspace is crucial. Could the authors provide more insights or guidelines on how these are chosen or optimized in practice  Discussion on Potential Negative Societal Impacts: Given the algorithms potential to influence model fairness could the authors delve deeper into any unintended consequences or negative impacts especially those that might exacerbate existing inequalities  4. Limitations and Constructive Suggestions for Improvement While the paper tackles important challenges in personalized FL and offers promising solutions its theoretical scope is somewhat limited primarily to smoother objectives with square regularizers and linear problems. A broader theoretical foundation covering more general objective functions and regularization forms could make the contributions more impactful. Additionally a more thorough discussion on the potential negative societal impacts addressing concerns such as privacy security and algorithmic biases would be valuable. Implementing these suggestions could further strengthen the papers contributions and its applicability to a wider range of FL scenarios.", "stAKQ6vnFti": " Peer Review for \"Contrastive Learning with LowDimensional Reconstruction (CLLR)\"  1) Brief Summary The paper introduces a novel framework for contrastive learning (CL) named Contrastive Learning with LowDimensional Reconstruction (CLLR) which aims to address the issue of highdimensional feature embeddings causing sparse distributions and making it difficult to capture underlying similarities between instances. CLLR proposes the use of a regularized projection layer to reduce the dimensionality of feature embeddings while maintaining effective instance discrimination. This approach is supported by theoretical guarantees including a tighter error bound and is tested on realworld datasets across various domains showing improved performance over existing CL methods. The paper contributes a new perspective on reducing dimensionality in contrastive learning complete with theoretical analysis and empirical validation.  2) Assessment of Strengths and Weaknesses Strengths:  Originality: The approach to tackle the highdimensionality issue in CL by introducing a lowdimensional projection space is novel and addresses an important challenge in the field.  Theoretical Support: Providing theoretical guarantees for the proposed method adds strength to the claims of improved performance and reliability.  Empirical Validation: Extensive experiments across different domains (vision language and reinforcement learning) help validate the effectiveness of CLLR in practical applications. Weaknesses:  Complexity and Implementation Details: The paper while thorough might be too heavy on theory for practitioners to easily grasp and implement CLLR based on the provided descriptions and algorithms.  Generalization across Domains: Despite experiments across multiple domains the adaptation of CLLR to different existing CL models might require additional finetuning which is not fully explored.  Comparison with StateoftheArt Methods: The comparisons mainly focus on traditional CL approaches and a more direct comparison with recent advancements specifically addressing the dimensionality issue would strengthen the case for CLLR.  3) Questions and Suggestions for the Authors  Implementation Details: Can the authors provide more insights into the practical implementation of CLLR especially on how to choose the regularization parameters \u03b1 and \u03bb effectively in different contexts  Generalization and Adaptation: How does CLLR adapt to CL models that might have different characteristics such as those relying on specific types of data augmentation or network architectures  Comparison with Recent Advances: Given the rapid development in the field comparing CLLR with more recent methods that also address high dimensionality perhaps even noncontrastive approaches could provide a more comprehensive evaluation.  4) Addressing Limitations and Societal Impact The paper discusses theoretical aspects and empirical results but does not delve deeply into the limitations or the potential negative societal impact of the work. Considering the broad application of contrastive learning including sensitive areas like facial recognition it would be beneficial to include a discussion on:  Limitations: An elaboration on the scenarios where CLLR might not perform optimally its scalability and computational requirements.  Societal Impact: A reflection on ethical considerations especially privacy and bias implications of deploying CLLR in realworld scenarios.  Suggestions for Improvement:  Expand the discussion on the practical implementation and parameter tuning to aid reproducibility.  Compare CLLR with both traditional and cuttingedge methods that have tackled similar issues.  Include a section on limitations and societal impact to provide a balanced view of the technologys potential implications. In summary the paper introduces an innovative and theoretically backed approach to addressing a critical challenge in contrastive learning. With further clarifications and comparisons the work has the potential to significantly contribute to the field.", "uLhKRH-ovde": "The paper introduces a novel communicationefficient distributed stochastic local gradient clipping algorithm (CELGC) to improve the deep neural network training process in a distributed setting. It specifically addresses the application of gradient clipping mechanisms in distributed systems an area that was relatively unexplored focusing on how these can enhance training efficiency and effectiveness for nonconvex optimization problems especially in contexts with nonLipschitz continuous gradients and the need for reduced communication rounds. The contributions include a novel algorithm that achieves linear speedup and reduced communication by employing gradient clipping based on local gradients and periodic global weight averaging. Theoretical analysis yielding specific iteration and communication complexity bounds underpins these claims supported by experimental validation on benchmark datasets across different neural network architectures.  Strengths: 1. Novelty and Relevance: The paper addresses a significant gap in distributed deep learning regarding the efficient employment of gradient clipping to manage exploding gradients  a common challenge in training complex models like RNNs and LSTMs. 2. Theoretical Foundation: The development of a theoretical framework underpinning the CELGC algorithm including iteration and communication complexity analysis is comprehensive and solid. Truncated random variables novel treatment offers a substantial contribution to stochastic optimization theory. 3. Empirical Validation: Extensive experiments conducted across different architectures and datasets confirm the theorys practical applicability. The ability to achieve faster convergence with reduced communication underlines the algorithms practical utility in distributed settings.  Weaknesses: 1. Homogeneous Data Focus: The analysis primarily assumes homogeneous data distribution across the distributed nodes which might not always be realistic in realworld scenarios especially in federated learning contexts. 2. Comparison with StateoftheArt: While demonstrating substantial improvements over baseline approaches and naive parallel implementations it remains unclear how CELGC compares against the latest stateoftheart distributed training methods especially those explicitly designed for noni.i.d. data or other advanced federated learning algorithms. 3. Generalizability of Results: The experiments though extensive are confined to specific model architectures and datasets. Broader validation across more diverse settings would have strengthened the argument for the algorithms universal applicability.  Questions and Suggestions for the Authors: 1. Heterogeneous Data Handling: Could the authors elaborate on potential modifications or extensions of the CELGC algorithm to cater to heterogeneous data distributions  a common scenario in federated learning applications 2. Robustness Evaluation: It would be valuable if the authors can include a discussion or empirical analysis on the robustness of the CELGC algorithm concerning hyperparameter choices especially the clipping threshold and communication intervals. 3. Comparative Analysis: Expanding the comparative analysis to include recent advances in communicationefficient distributed learning possibly through ablation studies or additional benchmarks would offer a clearer positioning of the CELGC algorithms performance. 4. Impact of Skipped Communications: Further insight into the impact of varying the communication interval (I) across different tasks and architectures would be beneficial. Specifically understanding the tradeoffs between convergence rate communication overhead and final model accuracy would be useful for practitioners.  Limitations and Societal Impact: The paper does acknowledge its primary limitation regarding the assumption of homogeneous data distribution. Extending the theoretical framework to accommodate heterogeneous data distributions represents a crucial area for future work. Regarding potential negative societal impacts the paper does not delve into this aspect. However any improvements in distributed deep learning efficiency could indirectly contribute to increased computational resource demands and energy consumption raising environmental concerns. Constructively future iterations of this work could explore energy efficiency aspects of the proposed algorithm and its implications for sustainable AI practices.", "xONqm0NUJc": " Peer Review of \"Relational Proxies for FineGrained Visual Categorization\"  1) Summary of the Paper and its Contributions The paper introduces \"Relational Proxies\" a groundbreaking approach designed to tackle the issue of finegrained visual categorization (FGVC) by focusing on the relational information between local and global views of objects. The authors provide a formal framework to understand the challenge of finegrained distinguishability and propose necessary and sufficient conditions for a model to effectively learn and distinguish between finegrained categories. Utilizing theoretical findings the authors develop a model that calculates metric space embeddings for relationagnostic and relationaware components within an image utilizing class representative vectors named Relational Proxies. The approach achieves stateoftheart results on seven benchmark FGVC datasets further validated through an extensive suite of experiments and ablations.  2) Assessment of Strengths and Weaknesses Strengths:  Novel Theoretical Contributions: The paper excels in its theoretical depth providing a rigorous formalization of distinguishability in finegrained visual categorization and establishing a solid foundation for the development of the Relational Proxies approach.  Significant Performance Improvements: Remarkably the method surpasses existing works with impressive margins across multiple FGVC datasets highlighting the effectiveness of leveraging crossview relational information in finegrained settings.  Comprehensive Validation: Through a series of experiments including ablation and comparative studies the authors convincingly validate their theoretical propositions and demonstrate the superiority of their approach. Weaknesses:  Method Complexity and Computation: The complexity of the relationalproxy model particularly when handling multiple local views and calculating relational embeddings could pose challenges in terms of computation especially on larger or more complex images.  Generalization to New Categories: While the method shows impressive results on benchmark datasets its unclear how well it generalizes to new or unseen categories especially those with limited data or those that dont fit well into the kdistinguishability framework.  3) Questions and Suggestions for the Authors  Improvement on Generalization: Could the authors provide insights or further experiments on how the Relational Proxies approach performs on unseen categories or datasets with limited training examples  Computational Efficiency: Are there potential modifications or optimizations that could reduce the computational requirements without significantly compromising the models performance  Extension to Other Domains: Given the relational nature of the proxy model can this approach be extended to nonvisual tasks such as text or audio categorization where finegrained distinctions are also critical  4) Limitations and Societal Impacts While the authors have addressed the limitations associated with the generic cropping methodology for obtaining local views a more sophisticated method for identifying relevant object parts could enhance performance and efficiency. Regarding societal impacts the paper rightly mentions the potential for underlying dataset biases to influence model predictions. It would be beneficial for future work to explore mechanisms for detecting and correcting such biases to ensure fair and unbiased FGVC applications. Overall the paper presents a notable advance in finegrained visual categorization offering significant theoretical contributions and practical improvements over existing methods. Suggestions for further exploration could enrich the models applicability and efficiency positioning Relational Proxies as a foundational approach in the FGVC domain.", "nDemfqKHTpK": " Review of \"GraB: Gradient Balancing for Efficient Data Ordering in SGD\"  1) Summary This paper addresses the challenge of improving convergence rates in Stochastic Gradient Descent (SGD) by investigating efficient data ordering strategies. The authors propose a novel algorithm named Gradient Balancing (GraB) which aims to outperform Random Reshuffling (RR) by organizing training data using a \"herding\" methodology and balancing gradients to minimize computational and memory overhead. GraB is theoretically grounded and empirically validated to achieve faster convergence rates on smooth nonconvex objectives compared to RR with significantly reduced memory and computational requirements. The authors experimentally demonstrate GraBs performance on several benchmarks including MNIST CIFAR10 WikiText and GLUE showing its superiority over RR and other greedy ordering methods in both convergence speed and resource efficiency.  2) Assessment  Strengths:  Theoretical Contribution: The paper provides a significant theoretical contribution by closing the gap in understanding how ordered SGD can outpace RR in convergence by employing a gradient balancing method. The herding framework and the integration of discrepancy minimization theory for reducing memory usage introduce novel approaches to the problem.  Practical Relevance: GraB promises considerable improvements in computational efficiency and memory usage crucial for training large models on extensive datasets.  Experimental Validation: Extensive experiments across multiple benchmarks validate the theoretical advantages of GraB showcasing its potential for widespread adoption.  Weaknesses:  Limited Discussion on Realworld Applications: While the experiments cover benchmark datasets the paper could benefit from a discussion on the implications of GraB in realworld largescale machine learning tasks outside of academic benchmarks.  Assumptions and Applicability: The assumptions made for the theoretical analysis such as smoothness and the PL condition might limit GraBs applicability to all problem types. A discussion on how GraB would perform under less ideal conditions would be insightful.  Comparison with StateoftheArt: The paper positions GraB against RR and greedy ordering but does not provide a broad comparison with other stateoftheart data ordering or sampling techniques that could potentially outperform or complement GraB.  3) Questions and Suggestions for Authors  Adaptability and Tuning: How adaptable is GraB to different machine learning models and tasks Is there a need for parameter tuning specific to the task at hand and if so how does this process compare to tuning for RR  Integration with Other Techniques: Could GraB be integrated with other optimization techniques (e.g. adaptive learning rates momentum) to further enhance its performance If so what modifications if any would be required  Impact of Batch Size: The paper discusses ordering at the level of individual examples. It would be beneficial to explore how batch size affects GraBs performance and whether there are optimal batch sizes that maximize its benefits.  Robustness to NonIdeal Conditions: Could the authors provide insights or empirical results on how GraB performs under nonideal assumptions such as nonsmooth loss functions or when the PL condition is not met  4) Limitations and Societal Impact The authors have made an effort to address limitations by proposing an algorithm that reduces memory and computation overhead a significant barrier in training large models. However the paper could benefit from a discussion on the potential negative societal impacts especially if the technique disproportionately benefits those with access to more powerful computing resources potentially widening the gap between research environments with varying resource availability. Constructive suggestions include exploring ways to make GraB more accessible and effective for resourceconstrained settings possibly through further algorithmic optimizations or through integration with platforms designed for distributed computation. Overall this paper makes a substantive contribution to efficient SGD processing and opens new avenues for future research in optimized data ordering techniques.", "n7Rk_RDh90": "Peer Review of \"A Differentiable Framework for 3D Concept Grounding with Neural Descriptor Fields\" 1. Summary and Contributions: The paper introduces an innovative framework for performing 3D concept grounding by leveraging the continuous and differentiable nature of neural descriptor fields (NDFs). The authors propose a method where each 3D coordinate in a scene is assigned a highdimensional descriptor and concept grounding is executed through computing the similarity between these descriptors and language concept embeddings. This allows for both 3D semantic and instance segmentations to emerge from question and answer supervision. The main contributions include the introduction of neural descriptor fields for 3D concept learning and grounding the definition of neural operators for reasoning tasks such as filtering and counting and the demonstration of superior performance over existing models on the PARTNETREASONING dataset for both segmentation and visual reasoning tasks. Additionally the authors show that their framework can generalize to unseen shape categories and real scans. 2. Assessment of Strengths and Weaknesses: Strengths:  Originality: The approach of leveraging neural descriptor fields as a differentiable and continuous intermediary for concept grounding is novel and represents a significant advancement in the field of 3D visual reasoning.  Performance: The experimental results demonstrate a clear advantage over baseline models in both reasoning and segmentation tasks showcasing the efficacy of the proposed framework.  Generalization: The ability of the framework to generalize to unseen categories and realscanned images is highly beneficial for practical applications highlighting the versatility of the approach. Weaknesses:  Complexity and Scalability: The paper does not sufficiently address the computational complexity or scalability of the proposed method which could be critical for realworld applications involving larger datasets or more complex scenes.  Lack of Ablation Studies: The absence of ablation studies makes it challenging to understand the contribution of individual components of the framework to the overall performance.  Limited Discussion on Failure Cases: The paper could benefit from a deeper discussion on the limitations or failure cases of the model providing a more balanced view of its capabilities. 3. Questions and Suggestions for the Authors:  Computational Requirements: Can the authors provide insights into the computational requirements of their framework especially when scaling to larger or more complex scenes  Component Contribution: It would be beneficial to include ablation studies to quantify the impact of integral components such as the neural descriptor fields and the defined neural operators on the models performance.  Failure Case Analysis: Could the authors discuss any observed failure modes or limitations of their framework particularly in scenarios with complex occlusions or highly cluttered scenes 4. Limitations and Societal Impact: The paper briefly mentions the generalization capabilities in realworld scenarios as a limitation but does not extensively explore other potential limitations or negative societal impacts. It would be beneficial for the authors to consider the following suggestions for improvement:  Ethical Considerations: Delve deeper into the ethical implications of deploying this technology especially concerning privacy concerns in realworld environments.  Bias and Fairness: Address potential biases in the model especially those arising from the dataset or the training process and propose measures to mitigate such biases.  Detailed Limitation Discussion: Expand the discussion on limitations including potential challenges in deploying the technology in dynamic environments or its adaptability to diverse realworld scenarios. Overall the paper presents a novel and effective approach to 3D concept grounding and visual reasoning making significant contributions to the field. Addressing the outlined questions and suggestions could further strengthen the work making it more transparent and applicable to realworld scenarios.", "uFSrUpapQ5K": "Peer Review for the Paper on OffPolicy Evaluation with Deficient Support 1) Summary of the Paper This paper addresses the OffPolicy Evaluation (OPE) problem with a focus on scenarios exhibiting deficient support where there is a mismatch between the actions available in logged data and those considered by a new evaluation policy. In such cases traditional Inverse Propensity Scoring (IPS) estimators which rely on the full support assumption (each action under evaluation has a nonzero probability under the logged policy) can be severely biased. The authors propose two alternative estimators that leverage side information about actions to reliably estimate rewards for unsupported actions thereby offering a solution to the deficient support problem without necessitating the full support assumption. These estimators are the adapted \"PseudoInverse estimator\" and the novel \"Similarity estimator\". Theoretical analysis and experimental evaluations demonstrate the utility of these estimators in achieving unbiased estimates even with deficient support. 2) Assessment of Strengths and Weaknesses Strengths:  Innovation: The novel Similarity estimator and the adaptation of the PseudoInverse estimator for deficient support in OPE present a significant advancement in handling scenarios where traditional methods fall short.  Theoretical Foundation: The paper provides a strong theoretical framework proving that the proposed estimators are unbiased under relaxed assumptions that do not require full support. Finitesample concentration inequalities offer insights into the estimators behavior when assumptions are further relaxed.  Practical Relevance: By addressing deficient support the paper tackles a common realworld issue enhancing the applicability of OPE in various domains such as recommendation systems and healthcare.  Experimental Evaluation: The analysis using realworld data effectively demonstrates the superiority of the proposed methods over conventional strategies like IPS and regressionbased estimators in cases of deficient support. Weaknesses:  Assumption Clarity: While the paper mentions alternative assumptions for the unbiasedness of estimators it could provide a more detailed discussion on the practical feasibility and implications of these assumptions in realworld datasets.  Hyperparameter Selection: The paper introduces a hyperparameter selection method for the Similarity estimator but acknowledges its limitations. A more exhaustive exploration of hyperparameter impact and robust selection methods could strengthen the paper.  Comparison with StateoftheArt: The experimental section primarily compares the proposed methods against traditional IPS and regressionbased estimators. Including comparisons with other stateoftheart OPE methods tailored to handle deficient support could offer a more comprehensive evaluation. 3) Questions and Suggestions for the Authors  Clarification of Assumptions: Can the authors provide more insights into the realworld viability of the alternative assumptions mentioned for unbiasedness Are there typical scenarios where these assumptions may not hold  Hyperparameter Sensitivity: How sensitive are the proposed estimators to the choice of hyperparameters particularly the weighting function in the Similarity estimator Could the paper benefit from an expanded discussion on this aspect  LongTerm Impact: Could the authors speculate on the longterm implications of their work on the field of OPE especially concerning ethical considerations and potential for misuse in sensitive applications 4) Limitations and Societal Impact The paper has made commendable efforts in providing theoretical and experimental evidence for the effectiveness of the proposed estimators. However the discussion on limitations mainly focuses on technical aspects such as the assumption validity and hyperparameter selection. A broader reflection on potential limitations including data privacy concerns and the ethical use of side information could enrich the discourse. Moreover while the authors briefly mention the positive societal impact of improving OPE especially in highstakes domains a deeper exploration of negative implications such as reinforcing biases present in the side information would provide a more balanced view. Constructively the authors could consider developing guidelines for ethical and responsible use of their estimators particularly in domains with heightened ethical scrutiny.", "mXP-qQcYCBN": "Review of \"SelfSupervised Learning of Structured Representations in Images via AutoLink\" 1) Summary of Contributions: The paper introduces AutoLink a novel selfsupervised methodology for learning structured representations in the form of 2D keypoints linked by straight edges from unlabelled image collections. The key contributions are:  A framework to disentangle object structure from appearance without supervision by predicting keypoint locations and learning a graph that describes their connectivity.  The adoption of an inpaintingbased approach for training where the graph acts as a bottleneck for structural information and an edge map combines latent graph edge weights and keypoint locations in a soft differentiable manner.  AutoLink demonstrates superior performance on keypoint and pose estimation benchmarks compared to existing selfsupervised methods and showcases broad applicability across various datasets including humans animals and objects in uncontrolled backgrounds. 2) Strengths and Weaknesses:  Strengths:  The paper tackles a significant issue in learning structured object representations without the need for expensive annotations presenting a valuable contribution to the field of selfsupervised learning.  The proposed method is innovative leveraging a graph to capture object structure shared across instances which is a novel approach compared to prior works that focus on independent parts or require predefined topology.  The experimental results across multiple benchmarks and diverse datasets convincingly support the effectiveness and generalizability of the approach.  Weaknesses:  The methods reliance on the inpainting task to enforce structural learning might limit its effectiveness to cases where structural information is significantly influential in the inpainting outcome. This aspect could reduce its applicability in contexts where appearance cues dominate.  The paper could benefit from a deeper analysis of the methods limitations particularly in challenging scenarios such as extreme occlusions or highly cluttered backgrounds.  There is limited discussion on the computational efficiency of the method specifically concerning the scalability to larger graphs or higherresolution images. 3) Questions and Suggestions for the Authors:  How does AutoLink perform in scenarios with significant occlusions or objects with highly deformable parts Some insight into these challenging cases would be valuable.  Could the authors elaborate on the methods computational efficiency especially when dealing with highresolution images or a large number of keypoints  It would be beneficial if the authors could discuss potential extensions of AutoLink to 3D reconstruction tasks considering the structural information it learns. 4) Limitations and Societal Impact:  The authors mention the potential misuse of recovered keypoints and edges in deep fakes and surveillance applications as a risk. While they argue that the method does not improve upon supervised models in detail for humans a more thorough discussion on mitigative steps for these risks would strengthen the paper.  The paper does not fully explore the limitations in challenging scenarios such as extreme occlusions highly deformable objects or cluttered scenes. Highlighting these limitations and proposing potential future directions would provide a more balanced view.  As for suggestions integrating techniques that explicitly model occlusions or employing attention mechanisms might enhance the methods robustness in complex scenarios. Furthermore exploring the scalability of the approach to 3D tasks could significantly increase its impact. In summary \"SelfSupervised Learning of Structured Representations in Images via AutoLink\" presents a compelling method for unsupervised learning of object structures. While the approach is innovative and the results are promising addressing the outlined questions and considering the limitations further would make a stronger case for the methods applicability and robustness.", "xbhsFMxORxV": "Review of \"Hyphen: A DiscourseAware Hyperbolic Spectral CoAttention Network for Social Text Classification\" Summary: The paper introduces Hyphen a novel machine learning model that tackles social text classification tasks including fake news detection hate speech identification rumor verification and sarcasm recognition. By integrating public discourse signals derived from user comments with source posts through Abstract Meaning Representation (AMR) graphs the model employs hyperbolic geometry for its representation learning to effectively map the hierarchical structure of discourse. Further innovation is introduced with a Fourier coattention mechanism that enhances the models ability to discern the correlation between public discourse and the source post aiming to generalize social text classification tasks beyond the capacities of current stateoftheart models. Assessment: Strengths: 1. Innovative Approach: The usage of hyperbolic space for graph representation learning and the introduction of a Fourier coattention mechanism are novel contributions enhancing the models ability to handle the complexities of social text classification. 2. Comprehensive Experiments: The model is thoroughly evaluated across a broad spectrum of datasets and social text classification tasks demonstrating its versatility and stronger performance compared to existing methods which bolsters the claim of stateoftheart results. 3. Explainability: Hyphens capacity to provide explanations for its predictions through hyperbolic coattention weights offers a significant advance towards the interpretability of AI models in social text analysis a critical requirement for practical applications. 4. Public Availability of Code: By providing the code publicly the authors facilitate reproducibility and further experimentation by the research community. Weaknesses: 1. Model Complexity and Computational Efficiency: The paper lacks a detailed discussion on the computational cost and practical scalability of Hyphen especially considering the complexity introduced by hyperbolic space computations and Fourier transforms. 2. Assessment of Explainability: While the models potential to generate explanations is highlighted a deeper evaluation of the quality and utility of these explanations in realworld scenarios is needed. The comparison with just one baseline (dEFEND) in this context could be expanded. 3. Discussion on Negative Societal Impact: The paper briefly touches upon the potential negative societal impacts but could benefit from a more thorough examination especially considering the sensitivity of tasks like hate speech detection and fake news identification. 4. Limitations on Data Diversity: There is no explicit discussion on how the model performs across languages cultures or diverse social media platforms where user interaction dynamics might significantly differ. Questions and Suggestions for the Authors: 1. Computational Resources: Could the authors provide more insights into the computational resources required for training and deploying Hyphen especially in comparison with existing models 2. Model Generalization: How well does Hyphen generalize to languages other than English or to social media platforms with differing user interaction structures 3. Explainability Evaluation: Could further light be shed on the methodology used to evaluate the explainability of the model Are there any user studies planned to assess the practical utility of the generated explanations 4. Dataset Diversity: The paper could benefit from experiments on more diverse datasets possibly including nonEnglish datasets or less represented social media platforms to evaluate the models robustness and generalization capabilities further. Limitations and Societal Impact: The authors have somewhat addressed the limitations and potential negative societal impacts but more comprehensive suggestions for mitigating potential misuse of the technology in sensitive areas like hate speech detection and fake news identification are warranted. Constructively exploring safeguards against model exploitation and discussing ethical considerations in the broader application contexts would strengthen the paper. Additionally exploring and addressing the models performance on a diverse range of datasets including languages and cultures not represented in the current study would be an invaluable addition to ensure its applicability and mitigate biases.", "q-snd9xOG3b": "Review of the Paper on Canonical Noise Distributions (CNDs) for fDifferential Privacy 1) Summary of the Paper: This paper focuses on exploring and expanding the concept of Canonical Noise Distributions (CNDs) a crucial mechanism used to satisfy fDifferential Privacy (fDP) a generalization of differential privacy that optimizes the privacy budget. The contributions of the paper revolve around two primary areas: the construction of logconcave CNDs desired for their interpretability and analytical ease and the construction of multivariate CNDs crucial for handling scenarios involving the release of multiple outputs. Through theoretical analysis the paper establishes the conditions under which both logconcave and multivariate CNDs exist by linking their existence to properties of the tradeoff functions defined in fDP such as infinite divisibility and decomposability. The paper notably highlights the limitations existing within pure \u03b5DP revealing its incompatibility with both logconcave and multivariate CNDs unlike other forms of DP such as GaussianDP and LaplaceDP which accommodate such CND constructions. 2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper provides substantial theoretical insight into the mechanics of fDP and extends the understanding of how privacy guarantees can be optimized through CNDs. By establishing the conditions under which logconcave and multivariate CNDs can be constructed the paper significantly advances the state of knowledge in privacy mechanisms.  Relevance: With increasing concerns over data privacy the development of effective and efficient mechanisms to safeguard privacy while retaining data utility is highly relevant. The papers focus on optimizing privacy budgets through CNDs addresses a critical need in both academic and applied fields.  Clarity and Depth: The paper is wellstructured providing a clear exposition of complex concepts. The mathematical proofs although dense are rigorously presented and the implications of the findings are thoroughly discussed. Weaknesses:  Limited Applicability Discussions: While the theoretical importance is underscored the paper could benefit from deeper discussions on applicability including realworld scenarios where logconcave or multivariate CNDs could be specifically advantageous.  Comparative Analysis: A more detailed comparison with other privacy mechanisms especially in terms of efficiency implementation complexity and scalability would help in understanding the relative positioning of CNDs within the broader privacy protection toolkit.  Empirical Validation: The absence of empirical validation or simulations to complement the theoretical results is a missed opportunity. Practical demonstrations of the CND constructions along with their performance and impact on privacyutility tradeoffs would strengthen the papers significance. 3) Questions and Suggestions for the Authors:  Empirical Studies: Can the authors provide empirical studies or simulations showcasing the implementation and efficacy of the proposed CND constructions in practical use cases  Broader Applicability: How do the authors foresee the applicability of their findings in the evolving landscape of privacy regulations and norms Are there specific sectors (e.g. healthcare finance) where logconcave or multivariate CNDs would be particularly beneficial  Comparative Performance: It would be valuable if the authors could include a comparative analysis of the proposed CND mechanisms against existing privacypreserving mechanisms on metrics such as computational complexity privacyutility tradeoffs and scalability. 4) Limitations and Societal Impact: The authors have made an effort to explore the existence and construction of CNDs within fDP but the discussion on limitations and potential negative societal impacts appears to be somewhat lacking. Addressing questions around potential misuse especially in how tightly optimized privacy guarantees might be circumvented or exploited is essential. Furthermore while the theoretical foundation is strong the practical challenges of implementing these CNDs including computational overhead and the potential for decreased data utility warrant more detailed examination. Constructive suggestions include:  Enhancing Transparency: Providing guidelines or considerations for practitioners on selecting appropriate privacy parameters when implementing CNDs could aid in mitigating potential misuse.  Addressing Imbalance: Discussing how to balance privacy protection with data utility particularly in contexts where high utility is paramount could help in broadening the papers appeal and applicability. In conclusion this paper makes significant theoretical contributions to the field of differential privacy. However enriching the discussion with practical implications broader societal impacts and comparative analyses could immensely enhance its relevance and applicability.", "qfC1uDXfDJo": "1) Summary of the Paper and Contributions: The paper investigates the optimization problem connected to fitting twolayer ReLU neural networks to minimize squared loss particularly where labels are generated by a target network. The authors have dissected the problem by leveraging the inherent symmetry within the system devising novel analytical tools to study how overparameterization affects the loss landscape particularly in transforming spurious minima into saddles. They offer sharp estimates on the loss and Hessian spectrum at various minima proving significant changes brought about by increasing neuron counts. Specifically the paper discusses how additional neurons can modify the loss landscape making it more navigable for gradientbased optimization methods by reducing spurious minima. Analytical techniques combining algebraic geometry representation theory and symmetry breaking are introduced marking a significant contribution to understanding the dynamics of neural network training. The insights are solidified through rigorous proofs including applications of Cauchy\u2019s interlacing theorem to confirm the theoretical findings. 2) Assessment of Strengths and Weaknesses: Strengths:  The paper makes a significant theoretical contribution by providing a detailed mechanism of how overparameterization helps in navigating the loss landscape of neural network optimization a topic of considerable interest in deep learning theory.  Novel analytical tools and rigorous proofs offer a fresh perspective on the problem making strong theoretical contributions to the field.  The methodology is solid utilizing a combination of complex mathematical disciplines to approach the problem in a novel way. Weaknesses:  While the theoretical foundation is strong the paper lacks empirical validation of the proposed theories. Including simulations or realworld data experiments could make the findings more compelling.  The complexity of the mathematical techniques used may limit the accessibility of the work to a broader audience potentially hindering its impact outside strictly theoretical circles.  Given the focus on twolayer ReLU networks the generalizability of the results to more complex architectures or different activation functions remains unclear. 3) Questions and Suggestions for the Authors:  Can the authors provide empirical evidence or simulations to support the theoretical findings particularly demonstrating the transformation of spurious minima into saddles with increasing neurons  It would be beneficial if the authors could discuss the implications of their findings on more complex neural network architectures and other types of activation functions addressing the generalizability of their results.  Given the paper\u2019s complexity an introductory section or a primer on the key mathematical concepts (algebraic geometry representation theory) for a nonexpert audience could enhance its accessibility. 4) Addressing Limitations and Potential Negative Societal Impact: The authors have offered a clear acknowledgment of their works limitations particularly noting its theoretical nature and the specificity to twolayer ReLU networks. However a discussion on the potential negative societal impacts is absent. While pure theoretical research might seem far removed from direct societal impacts reflecting on possible misapplications or longterm consequences (e.g. contributing to more powerful AI systems that might be misused) could provide a more comprehensive view. To strengthen the paper the authors could:  Suggest pathways for empirical validation of their theoretical findings.  Consider potential broader impacts of their work on AI development including ethical considerations around the use of increasingly complex and opaque models in sensitive applications. Overall the paper is a substantial theoretical contribution to the understanding of neural network optimization with clear strengths in its novel analytical approach and detailed exploration of symmetrybreaking effects on the loss landscape. Addressing its limitations and providing a broader context for the implications of its findings could further enhance its value.", "uPyNR2yPoe": "The paper introduces ELIGN a novel multiagent selfsupervised intrinsic reward mechanism for decentralized training under partial observability inspired by selforganization principles observed in zoology particularly the expected behavior alignment between neighboring agents. This approach enables agents in a multiagent system to enhance collaborative behaviors through expectation alignment without the need for external rewards or centralized training. The authors demonstrate the effectiveness of ELIGN across various tasks within the multiagent particle environment and the Google Research football environment showing superior performance compared to sparse and curiositybased rewards especially in settings of partial observability and decentralized training. Strengths: 1. Innovative Approach: The paper presents a compelling paradigm shift from centralized training to a decentralized methodology inspired by natural phenomena which is both novel and imaginative. 2. Broad Applicability: ELIGN is taskagnostic making it applicable to a wide range of multiagent system scenarios both cooperative and competitive. 3. Strong Empirical Validation: Extensive experiments showcase the superiority of ELIGN over existing reward mechanisms including its scalability and its ability to enable effective coordination and generalization to new partners. 4. Theoretical Motivation: The link between the selforganization principles in zoology and the proposed solution provides a strong theoretical foundation for the work. Weaknesses: 1. Limited Environment Variety: The evaluation is confined to simulation environments. Realworld applicability and robustness across diverse realworld scenarios remain untested. 2. Potential for Negative Societal Impact: The paper does not fully discuss the potential negative societal impacts especially considering the deployment of such AI models in sensitive realworld applications. 3. Complexity in LargeScale Applications: While the paper states that ELIGN scales well the complexity and computational requirements for very largescale applications (hundreds or thousands of agents) are not thoroughly addressed. Questions and Suggestions for Authors: 1. RealWorld Applicability: Can the authors provide insights or preliminary results on how ELIGN performs in more complex or realworld scenarios beyond the tested simulation environments 2. InterAgent Communication: How does ELIGN handle scenarios where explicit communication between agents might significantly enhance coordination Is integrating a communication mechanism feasible or beneficial 3. Negative Societal Impact: Could the authors elaborate on potential negative societal impacts particularly in scenarios where misaligned incentives could lead to adversarial or harmful behaviors Limitations: While the authors have demonstrated the scalability of ELIGN in the tested environments the discussion on its computational efficiency and feasibility in realworld largescale applications is lacking. Additionally the potential negative societal impacts especially when deployed in sensitive realworld applications have not been adequately addressed. It would be insightful if the authors could discuss these aspects further perhaps suggesting frameworks or guidelines for mitigating potential negative impacts ensuring ethical use and maintaining transparency in decisionmaking processes in multiagent systems powered by ELIGN.", "r5rzV51GZx": "Review of \"Robust Collaborative Inference via Feature Purification\" 1. Summary: This paper proposes a novel framework CoPur designed to enhance the robustness of collaborative inference systems against adversarial attacks at inference time. In collaborative inference scenarios multiple agents (sensors or IoT devices) contribute to a collective prediction task by sending their embedded feature vectors to a Fusion Center (FC) which then performs prediction based on these features. In such setups the system is vulnerable to attacks where a fraction of these agents are compromised and send manipulated or incomplete data to the FC. The authors address this issue by introducing a certifiably robust collaborative inference method leveraging feature purification allowing the FC to recover accurate feature vectors despite the presence of adversarial corruptions or incomplete observations. The core innovation lies in exploiting the blocksparse nature of adversarial perturbations and the redundancy across embedded features that is assumed to lie on an underlying lowerdimensional manifold. Extensive experiments on ExtraSensory and NUSWIDE datasets demonstrate the superiority of CoPur over existing defense mechanisms against various adversarial attacks. 2. Assessment: Strengths:  Novelty and Theoretical Foundation: The paper introduces a new approach to enhancing the robustness of collaborative inference which is both innovative and grounded in rigorous theoretical analysis. The proposed nonlinear robust decomposition method and its ability to recover the true feature vector despite adversarial manipulations are significant contributions.  Practical Relevance: The problem tackled is highly relevant given the prevalence of collaborative inference in IoT and other distributed systems. The ability to defend against attacks with certifiable guarantees is a critical advancement.  Empirical Validation: The extensive experiments conducted on two datasets and against a variety of adversarial attack models including a novel untargeted distributed featureflipping attack provide solid evidence of the effectiveness of the proposed method. Weaknesses:  Assumption of Redundancy: The effectiveness of the proposed approach relies significantly on the assumption that the overall features lie on an underlying lowerdimensional manifold. This assumption may not hold in all realworld scenarios potentially limiting the applicability of the method.  Scalability Concerns: While the paper provides detailed theoretical and experimental validation of CoPur it does not extensively discuss the scalability of the approach particularly in contexts with a very large number of agents or highdimensional feature spaces.  Potential for Evading Attacks: While the paper shows robustness against a variety of attacks it remains to be seen how the framework would perform against more sophisticated adversarial strategies specifically designed to circumvent feature purification techniques. 3. Questions and Suggestions:  Can the authors elaborate on the potential impacts of the method in scenarios where the underlying manifold assumption does not hold Are there alternative strategies to handle such cases  It would be beneficial to discuss the computational complexity of CoPur particularly in comparison to existing defenses to better understand its scalability and practical deployment considerations.  Have the authors considered the case where attackers could have knowledge of the feature purification process and specifically design their attacks to bypass this defense mechanism Exploring this aspect could further validate the robustness of CoPur. 4. Limitations and Societal Impact: The authors have made an effort to address limitations particularly through theoretical analysis and empirical testing. However the assumption regarding the redundancy of features and lowerdimensional manifolds poses a potential limitation in diverse realworld applications. Regarding societal impact while the paper focuses on enhancing security in collaborative inference a discussion on potential negative implications such as enabling more powerful surveillance capabilities or misuse in adversarial contexts would have been appreciated. A suggestion for improvement could include a section discussing ethical considerations and potential countermeasures against misuse of the method proposed. Overall the paper represents a significant step forward in securing collaborative inference systems and provides a strong foundation for future research in the field.", "xvlaiSHgPrC": "Review of \"Optimal Concurrent Composition Theorems for Differential Privacy\" 1) Summary of the Paper: This paper investigates the concept of concurrent composition in the realm of differential privacy a crucial framework for safeguarding individual privacy amidst data analysis on sensitive datasets. Notably the authors extend the work of Vadhan and Wang (2021) who established an optimal concurrent composition theorem for pure differential privacy. The significant contribution of this research lies in its generalization of these results providing optimal parallel composition properties for various differential privacy definitions including approximate DP R\u00e9nyi DP and zeroconcentrated DP. The findings delineate that an adversary gains no strategic advantage through the interleaving of queries across independently operating mechanisms suggesting that differential privacy inherently supports interactive applications without compromising privacy guarantees. 2) Assessment of Strengths and Weaknesses: Strengths: a) Comprehensive Scope: The paper addresses a wide array of differential privacy notions facilitating a broader applicability of the results within the privacy research community and in practical scenarios. b) Solid Theoretical Foundation: The authors present a rigorous theoretical framework underpinning their results. This includes a novel characterization of R\u00e9nyi divergence and an innovative proof strategy for establishing concurrent composition theorems. c) Potential for Practical Impact: By demonstrating that concurrent compositions do not degrade privacy protections more than sequential compositions the work has significant implications for the design and deployment of privacypreserving interactive systems in realworld settings. d) Independence of Interactivity and Privacy: The demonstration that interactivity does not inherently degrade differential privacy is a powerful result emphasizing the robustness of differential privacy in interactive environments. Weaknesses: a) Assumption of Independence: The results rely on the independence of interactive mechanisms. Realworld scenarios might involve mechanisms with dependencies due to shared data or correlated operations which could limit the applicability of these theorems. b) Complexity of Theoretical Tools: Some of the proof techniques and theoretical constructions (e.g. the alternative characterization of R\u00e9nyi divergence) might be difficult for practitioners to apply directly. c) Lack of Empirical Evaluation: The paper is predominantly theoretical and lacks empirical studies or practical implementations that showcase the application of the concurrent composition theorems. 3) Questions and Suggestions for the Authors: a) Handling Dependencies: Can the authors discuss how their results might extend to scenarios where interactive mechanisms are not wholly independent or suggest approaches for handling such dependencies b) Practical Implementations: Could the authors provide examples or case studies of how their concurrent composition theorems can be implemented in realworld differential privacy scenarios possibly highlighting practical challenges c) Impact on Privacy Auditing: How do the authors envision their results impacting the auditing processes for differential privacy compliance especially in complex systems employing multiple interactive mechanisms d) Exploration of Negative Results: Are there scenarios under the concurrent composition setting where the differential privacy guarantees significantly degrade Discussing such cases could provide a more balanced view. 4) Limitations and Societal Impact: While the authors briefly note the absence of negative societal impacts of their work a deeper exploration of potential limitations and the broader implications would strengthen the paper. Specifically the implications of concurrent composition theorems on privacy regulations ethical considerations in deploying such technologies and the potential misuse of these theoretical advancements in surveillance or data exploitation contexts warrant further discussion. Constructively the authors could outline best practices or safeguards to mitigate any potential adverse impacts stemming from their findings. In conclusion this work marks a significant advancement in the understanding of differential privacy under concurrent compositions with broad theoretical and potential practical implications. Addressing the highlighted suggestions could further enhance the utility and impact of this research.", "yZcPRIZEwOG": "The paper presents a novel approach to solving the problem of policy optimization (PO) within Reinforcement Learning (RL) under Linear Temporal Logic (LTL) constraints. These constraints offer a richer language for task specification beyond simple scalar cost functions enabling the encoding of more complex objectives and behaviors that are desired or required in a given task. The authors introduce a modelbased method that leverages a generative model of the environment and a lower bound on transition probabilities to achieve both task satisfaction and cost optimality. This is achieved through a reduction to a reachability problem allowing for a rigorous sample complexity analysis of the algorithm developed. Empirical results are presented showing the methods strong performance in achieving task compliance with fewer samples than predicted by worstcase guarantees.  Strengths: 1. Innovative Approach: The paper proposes a groundbreaking framework for integrating LTL in RL significantly reducing the reliance on previous restrictive assumptions and offering stronger theoretical guarantees. 2. Rigorous Theoretical Analysis: The sample complexity analysis offers valuable insights into the efficiency of the proposed algorithm providing a strong theoretical foundation for the work. 3. Empirical Validation: Demonstrating the algorithms efficacy on both infinite and indefinite horizon problems with composite specifications adds practical value to the theoretical concepts discussed.  Weaknesses: 1. Assumptions: While less restrictive than prior work the assumptions made (such as access to a generative model and a known lower bound on transition probabilities) might still limit the applicability in realworld scenarios where such information might not be readily available. 2. Scalability and Generalization: There is limited discussion on the scalability of the proposed method to more complex environments or its generalization capabilities beyond the examples and settings tested. 3. Comparison With Modelfree Approaches: The paper mainly compares the proposed method against existing modelbased approaches. An exploration into how the algorithm stacks up against modelfree algorithms which might have different strengths and weaknesses is lacking.  Questions and Suggestions: 1. RealWorld Applicability: Can the authors comment on potential limitations or adjustments needed when applying their algorithm in more complex realworld environments 2. Scalability: Are there strategies that could be employed to scale the proposed method to environments with significantly larger state or action spaces 3. ModelFree Comparison: It would be insightful to see a comparison with stateoftheart modelfree methods especially in terms of sample efficiency and ability to handle complex LTL specifications.  Limitations and Societal Impact: The authors have provided a foundational analysis of the limitations inherent in their assumptions primarily around the availability of a generative model and known transition probabilities. They have however not fully explored the potential negative societal impacts especially in critical applications where failure to satisfy LTL specifications could have severe consequences. For improvement the paper could benefit from a discussion on:  Ensuring robustness and safety in applications with high stakes such as autonomous driving or healthcare.  Strategies for mitigating risks when the assumptions do not hold or are partially satisfied.  A broader consideration of the ethical and societal implications of deploying such systems including transparency accountability and fairness in decisionmaking processes driven by the optimized policies. In conclusion this paper makes a significant contribution to the intersection of LTL and RL providing a novel method that broadens our understanding of policy optimization under complex constraints. With further exploration and enhancement this work has the potential to impact a wide range of realworld applications that require nuanced and sophisticated decisionmaking capabilities.", "pluyPFTiTeJ": " Peer Review on \"Leveraging RateDistortion Theory for Efficient Domain Generalization\"  1) Summary of the Paper and Its Contributions This paper introduces a novel approach to domain generalization a critical challenge in machine learning where models trained on data from multiple domains must generalize well to unseen domains. Traditional methods focus on jointly minimizing empirical risk and a surrogate penalty designed to capture generalization capability often leading to suboptimal indistribution performance. This work presents an alternative optimization framework that minimizes the domain generalization penalty under the constraint of maintaining empirical risk optimization thereby ensuring no adverse effects on indistribution performance. The authors leverage ratedistortion theory to efficiently solve this optimization problem demonstrating the approach\u2019s effectiveness across various domain generalization methods and benchmarks. This paper represents a significant shift in domain generalization emphasizing preserving empirical risk while addressing the generalization gap.  2) Assessment of the Paper Strengths: a) Innovative Approach: The paper successfully introduces a novel optimization framework that safeguards against the degradation of indistribution performance a noticeable limitation in existing domain generalization methods. b) Theoretical Foundation: The adaptation of ratedistortion theory for solving the proposed optimization problem is wellarticulated providing a solid theoretical underpinning for the approach. c) Broad Applicability: The method is designed to be compatible with any penaltybased domain generalization method showcasing versatile applicability. d) Comprehensive Evaluation: Through extensive experiments on the WILDS and DomainBed benchmarks the paper demonstrates the method\u2019s efficacy across multiple domain generalization models and diverse datasets. Weaknesses: a) Computational Complexity: Despite its effectiveness the method introduces significant computational overhead as gradients need to be computed for each domain substantially increasing training times. b) Potential Scalability Issues: Given the computational complexity the practicality of scaling this approach to very large datasets and models or domains with realtime constraints is uncertain. c) Assumption on Empirical Risk Monotonicity: The method assumes an imperviousness to indistribution performance which may not always hold especially in cases where domain peculiarities crucially influence the empirical risk optimization landscape.  3) Questions and Suggestions a) Handling Computational Complexity: Can the authors propose or discuss approximation techniques that might help in reducing the computational overhead b) Scalability Concerns: It would be beneficial for the authors to explore the algorithm\u2019s scalability particularly in settings with numerous domains extremely large datasets or realtime constraints. c) Empirical Risk Assumption: Further elaboration on how the method behaves when the assumption about the nonimpedance on indistribution performance fails would add valuable insights. d) Broader Impact Discussion: Given the broad applicability of this method a comprehensive discussion on potential societal impacts including biases in domain generalization and ethical considerations surrounding automated decisionmaking in diverse domains would enhance the papers relevance and responsibility.  4) Limitations and Societal Impacts The authors have partially addressed the limitations related to computational complexity but could further explore approximation methods or algorithmic efficiencies to mitigate this issue. Additionally while the potential for negative societal impacts seems low given the method\u2019s nature it would be prudent for future work to consider the broader implications of domain generalization techniques including their use in sensitive or impactful areas where biases or erroneous generalizations could have significant consequences. Suggestions for improvement include a deeper dive into scalable strategies and a thorough investigation into ethical considerations surrounding domain generalization applications.", "q-tTkgjuiv5": "PeerReview Report 1. Summary of the Paper and its Contributions The paper addresses the problem of fairly allocating graphical resources represented by vertices in a graph where agents utilities are defined by the weight of the maximum matching in the induced subgraph. The authors focus on achieving maximin share (MMS) fairness and envyfreeness up to one item (EF1) fairness criteria under conditions of homogeneous and heterogeneous agent valuations. For homogeneous agents the paper designs polynomialtime algorithms that achieve constant approximation ratios for MMS fairness while also maintaining a significant fraction of the maximum possible social welfare under EF1 allocations. For heterogeneous agents the paper proves strong impossibility results for MMS fairness and presents polynomialtime algorithms that ensure a constant fraction of maximum social welfare for EF1 allocations under certain conditions (binaryweight and twoagent scenarios). 2. Assessment of Strengths and Weaknesses Strengths:  The paper tackles an important problem in the domain of resource allocation by considering graphical resources which has numerous realworld applications.  The approach to handle both homogeneous and heterogeneous agents provides a comprehensive understanding of the problems complexity.  Theoretical results are wellsupported with polynomialtime algorithms for homogeneous agents and specific conditions for heterogeneous agents which is impressive. Weaknesses:  The strong impossibility result for MMS fairness with heterogeneous agents might limit the applicability of the proposed solutions in diverse settings.  The paper assumes agents utilities solely depend on the weight of the maximum matching which might not encompass all realworld scenarios where other factors could influence utility.  The paper lacks empirical validation of the proposed algorithms which would strengthen the claims regarding their efficiency and practicality. 3. Questions and Suggestions for the Authors  Considering the works theoretical nature have the authors considered conducting simulations or realworld experiments to validate the efficiency and applicability of their proposed algorithms  How do the authors envisage extending their work to address the limitations imposed by the strong impossibility results for MMS fairness with heterogeneous agents  Since the utility definition is based on the weight of the maximum matching it might be restrictive for certain applications. Can the authors discuss potential modifications to their approach that would allow for a broader definition of utility  Given the papers focus on graphical resources can the authors provide more concrete examples of realworld scenarios where their findings could be directly applied 4. Limitations and Suggestions for Improvement The authors did not provide empirical evidence to support the practicality and efficiency of their algorithms. Including simulation results or a case study could significantly enhance the papers impact. Moreover addressing the impossibility of achieving MMS fairness for heterogeneous agents represents a critical limitation exploring approximate or heuristic approaches for such scenarios could be a valuable future direction. The paper overlooks discussing the potential negative societal impacts of their work particularly in cases where the algorithmic fairness criteria might not align with social or ethical fairness perceptions. Deeper exploration into these aspects including how to mitigate any unintended negative impacts would improve the papers depth and relevance. Conclusion This paper makes significant contributions to the field of fair allocation of graphical resources particularly in designing algorithms for MMS fairness for homogeneous agents and revealing the challenges associated with achieving MMS fairness for heterogeneous agents. While the limitations regarding the applicability in diverse settings and the lack of empirical validation present areas for improvement the theoretical advancements provided offer valuable insights into the efficient and fair allocation of graphical resources.", "xDaoT2zlJ0r": "Review of \"First IntegralPreserving Neural Differential Equations for Dynamical System Modeling\" 1) Summary of the Contributions This paper introduces First IntegralPreserving Neural Differential Equations (FINDE) a novel approach for modeling dynamical systems using neural networks. FINDE uniquely focuses on preserving first integrals invariant quantities of the system without requiring prior knowledge of the systems underlying structure. This is achieved by leveraging projection methods and discrete gradient methods allowing FINDE to maintain these invariants accurately across continuous and discrete time. Experimental results demonstrate FINDEs superior performance in predicting future states of dynamical systems and identifying quantities consistent with wellknown first integrals. 2) Assessment of Strengths and Weaknesses Strengths:  Innovation: The approach to preserve first integrals without prior structural knowledge of the dynamical system is novel and addresses a significant gap in the modeling of such systems.  Theoretical Justification: The paper provides solid theoretical underpinnings for FINDE including definitions the formulation of continuous and discrete variants and proofs of first integral preservation.  Experimental Results: The paper showcases comprehensive experiments across different datasets demonstrating the effectiveness of FINDE over traditional methods. The results were compelling showing improved longterm prediction accuracy and the discovery of invariant quantities.  Potential for Scientific Discovery: Beyond its immediate applications in dynamical system modeling FINDEs ability to identify unknown first integrals can facilitate new scientific discoveries and deepen our understanding of complex systems. Weaknesses:  Computational Cost: The approach appears to require significantly more computation especially for discrete FINDE. This aspect may limit its scalability to very large systems or realtime applications though the paper briefly discusses computational considerations.  Generalizability: While the paper presents results for a range of systems the broader applicability of FINDE to all types of dynamical systems particularly nonHamiltonian or systems with highly chaotic behavior remains to be fully explored.  Assumption Clarity: The paper assumes the presence of functionally independent first integrals in the target system. A more thorough discussion on the implications of this assumption and the conditions under which it holds would strengthen the paper. 3) Questions and Suggestions for the Authors  Handling of Noisy Data: The experiments assume relatively clean datasets. How does FINDE perform in the presence of significant noise and what modifications if any are suggested to handle noisy data  Scalability Considerations: Could the authors elaborate on strategies to mitigate the computational demands of FINDE especially for largescale dynamical systems Investigating approximation techniques or sparsity could be valuable.  Extension to Chaotic Systems: Are there plans to test FINDE on highly chaotic systems Insights on how FINDE handles or could be adapted for such systems would be intriguing. 4) Limitations and Societal Impact The paper briefly touches on computational complexity but could benefit from a more detailed treatment of other limitations such as scalability to very large systems and robustness to data quality issues. While the societal impact of this work seems broadly positive enabling more accurate modeling of complex systems a discussion on potential misuse or misinterpretation especially in critical applications like climate dynamics or epidemiological modeling could be an interesting addition. Suggestions for mitigating these risks and ensuring the responsible application of FINDE would enhance the paper. In summary this paper represents a significant advance in the modeling of dynamical systems through neural networks with promising applications and potential for uncovering new scientific insights. Addressing its computational demands and further exploring its limitations and generality will be crucial steps forward.", "xOqqlH_E5k0": "Review of \"Augmented Deep Unrolling Neural Network for Compressive Hyperspectral Imaging\" 1. Summary: The paper introduces an advanced deep unrolling neural network (DUN) tailored for snapshot compressive hyperspectral imaging (SCHI) a process critical in capturing detailed spectral information with potential applications in numerous fields. The authors propose a modification to the standard proximal gradient descent (PGD) mechanism with two novel modules designed for efficient gradient update and proximal mapping. The gradient update benefits from a memoryassistant descent module inspired by momentumbased heuristics enhancing convergence speed through past gradients memory. The proximal mapping adopts a subnetwork characterized by crossstage selfattention capable of harnessing intrinsic selfsimilarities within the hyperspectral image (HSI) across the spectral axis. This feature not only optimizes feature flow across the network but also emphasizes spectral geometry consistency ensuring the reconstruction algorithm prioritizes geometric aspects of spectral curves. Extensive evaluations across various datasets confirm the superiority of the proposed method over prevailing approaches. 2. Assessment:  Strengths:  The incorporation of memoryassistant modules for gradient updates represents a significant advancement potentially setting a new standard for future works in SCHI.  The crossstage selfattention mechanism introduces an innovative method for efficiently leveraging the inherent properties of HSIs such as their selfsimilarities across the spectral axis.  The proposed spectral geometry consistency loss is a novel and logical addition that ensures the preservation of critical spectral properties leading to more accurate and realistic reconstructions.  Extensive experimental validation and comparison with stateoftheart methods demonstrate the methods effectiveness and applicability to realworld scenarios.  Weaknesses:  The computational complexity of the proposed network especially with the inclusion of memoryassisted modules and crossstage selfattention is not thoroughly discussed. This could be a crucial aspect when considering realtime applications.  While the paper extensively validates the proposed method across different datasets the impact of the choice of datasets on the overall performance and generalizability of the method remains unclear.  The discussion on the potential limitations and negative societal impacts of the proposed work is notably absent from the main text making it difficult to fully assess the works broader implications. 3. Questions and Suggestions for the Authors:  Could the authors elaborate on the computational complexity of the proposed method and its feasibility for realtime or nearrealtime applications  It would be beneficial if the authors discussed how the choice of datasets influences the performance and generalizability of the proposed method. Can the authors provide insights on the methods performance in environments drastically different from those in the evaluated datasets  The authors are encouraged to discuss any potential limitations of their work more openly in the main text along with any negative societal impacts that the technology could potentially entail or exacerbate. 4. Limitations and Societal Impact:  The paper could benefit from a more thorough discussion on the potential limitations such as scenarios where the proposed method might underperform or be impractical. Furthermore a reflection on the computational demands of the method and potential remedies or optimizations could make the paper more rounded.  Regarding societal impact the authors should contemplate and articulate potential negative implications if any of their technology. For instance any misuse in surveillance contexts or concerns related to privacy and data security should be addressed. In conclusion the paper presents compelling contributions to the field of hyperspectral imaging with its novel approach to SCHI reconstruction. Nevertheless addressing the suggested refinements could significantly strengthen the work enhancing its relevance and applicability to a broader array of realworld scenarios.", "toR64fsPir": "The presented paper introduces a generative tensorbased latent space model (TLSM) to analyze multilayer networks with inherent community structures. The authors focus on embedding vertices into a lowdimensional space that reflects the community closeness while capturing layerspecific heterogeneities through a layereffect factor matrix. This model leverages tensor decomposition techniques to preserve original network structures and enable effective community detection and network estimation. The proposed model is complemented by an alternative updating scheme for parameter estimation and an indepth theoretical examination of asymptotic consistencies in multilayer network estimation and community detection. Extensive numerical experiments on both synthetic and realworld networks underscore the practical utility and theoretical underpinnings of TLSM. Strengths: 1. Innovative Approach: The models focus on capturing layer heterogeneities while preserving community structures in multilayer networks is a significant step forward from existing methodologies that often overlook this aspect. 2. Comprehensive Theoretical Analysis: The paper offers a robust theoretical framework establishing the asymptotic consistencies of the proposed model in network estimation and community detection which adds credibility and depth to the methodology. 3. Practical Utility: Through extensive experiments on synthetic and reallife multilayer networks the paper demonstrates the applicability and effectiveness of the proposed model across various contexts enhancing its appeal to both researchers and practitioners. Weaknesses: 1. Complexity and Computation Limitations: The models complexity particularly the tensor decomposition and alternative updating scheme may pose computational challenges for very large networks or those with numerous layers though this is partially mitigated by the proposed efficient algorithm. 2. Parameter Sensitivity: The models efficacy heavily relies on the selection and estimation of several parameters (e.g. sparsity factor sn and regularization parameter \u03bbn) which might demand substantial tuning efforts in practice to achieve optimal performance. Questions and Suggestions for the Authors: 1. Robustness Analysis: How robust is TLSM against variations in network density and layer heterogeneity Including a sensitivity analysis could provide insights into the models applicability across diverse network structures. 2. Extension to Directed Networks: The current model focuses on undirected and unweighted networks. Could the authors discuss potential adaptations or extensions of TLSM to accommodate directed networks or weighted connections 3. Comparison with Graph Neural Networks: Given the recent advancements in graph neural networks (GNNs) for network analysis comparing TLSM with GNNbased approaches could further elucidate TLSMs advantages or identify potential areas for integration or improvement. Limitations and Societal Impact: While the authors briefly mention the computational considerations of the model a more detailed discussion on computational efficiency and scalability especially in comparison to existing methods would be beneficial. Moreover the authors should consider discussing potential negative societal impacts especially in applications where community detection might lead to privacy concerns or biased outcomes. Suggestions for mitigating such impacts could significantly strengthen the paper. Overall this paper presents a noteworthy contribution to the field of network analysis by addressing the challenges of multilayer network structure preservation and community detection. With improvements in addressing its limitations and incorporating suggested analyses this work has the potential to set a new standard in the study of complex networks.", "rHnbVaqzXne": " Review of \"Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical Representation Learning\"  1) Summary: The paper introduces the Rotated Hyperbolic Wrapped Normal Distribution (RoWN) an innovative alteration of the Hyperbolic Wrapped Normal Distribution (HWN). The work begins by addressing the limitations of the HWN in representing hierarchical data particularly its inefficiency in capturing variations at the same hierarchical level. Through a geometric analysis the authors identify that the diagonal HWN fails to accurately model the local variation within the hierarchy due to inappropriate distribution of data points in the Poincar\u00e9 disk model. To overcome these limitations RoWN is proposed which effectively utilizes a rotated diagonal covariance matrix to align major axes with hierarchical variations thereby enhancing representation learning in hyperbolic space. The paper showcases the efficacy of RoWN through empirical evaluations on synthetic and realworld datasets including WordNet and Atari 2600 Breakout demonstrating superior performance over traditional HWN and Euclidean models.  2) Assessment: Strengths:  Innovative Approach: The introduction of RoWN is a significant advancement in the field of probabilistic modeling in hyperbolic spaces. It addresses a critical limitation in hierarchical data representation which is a notable contribution.  Comprehensive Analysis: The geometric analysis of HWN and the subsequent motivation for RoWN provide a solid theoretical foundation for the proposed method. This depth of analysis adds significant value to the work.  Empirical Validation: The thorough evaluation of RoWN across different datasets and comparison with baseline models demonstrate its effectiveness and generalizability. Weaknesses:  Limited Scope of Generalization: The focus is primarily on the hyperbolic space. While this is not inherently a weakness a discussion of possible extensions or adaptations to other Riemannian spaces could broaden the impact of the findings.  Optimization Challenges: Although the paper mentions the difficulty in optimizing full covariance HWN it does not provide substantial insights or solutions to this issue beyond the conjecture. More discussion or proposed directions for optimization techniques could enhance this aspect.  3) Questions and Suggestions:  Extension to Other Riemannian Spaces: Could the authors elaborate on potential strategies or challenges in generalizing RoWN to other Riemannian spaces Any preliminary thoughts or experiments in this direction would be valuable.  Optimization Techniques: Considering the identified challenges in optimizing the full covariance HWN are there specific optimization algorithms or methods the authors foresee as promising for addressing these issues Sharing any preliminary insights or related work could be beneficial.  4) Limitations and Societal Impact: The authors acknowledge the scope of their work is limited to hyperbolic space and hint at the need for exploring distributions in other Riemannian spaces. They also conjecture the optimization hardships of the full covariance HWN without delving into potential solutions. While these acknowledgments are appreciated further discussion on overcoming these limitations would enhance the paper. Additionally the societal impact particularly the ethical considerations in applying RoWN for hierarchical data modeling in sensitive areas (e.g. social networks knowledge graphs with cultural or political content) is not addressed. Considering the potential misuse or biases that could be amplified through hierarchical representations it would be prudent for future work to include careful consideration of these aspects.  Constructive Suggestions for Improvement:  Exploring and discussing potential extensions to other Riemannian spaces could make the work more impactful across a broader range of applications.  A deeper investigation into optimization challenges with practical solutions or pathways could fortify the works technical rigor.  Including a discussion on ethical considerations and potential negative societal impacts along with mitigation strategies would make the work more holistic and responsible. Overall the paper presents a significant contribution to probabilistic modeling in hyperbolic spaces with the introduction of RoWN. With further exploration and refinement in the highlighted areas this work has the potential to set a new direction in hierarchical data representation learning.", "qw3MZb1Juo": "Review of \"Federated NotTrue Distillation for Alleviating Forgetting in Federated Learning\" Summary and Contributions: The paper presents an inventive approach titled Federated NotTrue Distillation (FedNTD) to tackle the problem of forgetting in federated learning systems. By drawing an analogy to continual learning the authors identify that a significant challenge in federated learning is the forgetting of global knowledge especially when local models are trained on data distributions that do not well represent the global data distribution. To mitigate this issue FedNTD employs a distillation technique that preserves global knowledge across training rounds without true class labels from the local data. The key contributions of this paper are: (1) A systematic study highlighting the forgetting phenomenon in federated learning due to data heterogeneity (2) The proposal of the FedNTD algorithm that preserves global knowledge effectively without imposing additional communication costs or compromising data privacy and (3) Extensive experiments that validate the stateoftheart performance of FedNTD across various datasets and federated setups. Assessment of Strengths and Weaknesses: Strengths: 1. Rigorous analysis and novel approach: The paper systematically investigates the issue of forgetting in federated learning and provides a novel solution with FedNTD. The analogy to continual learning is insightful and successfully motivates the proposed methodology. 2. Comprehensive experimental validation: The authors conducted extensive experiments demonstrating that FedNTD not only addresses the forgetting problem but also achieves superior performance compared to existing federated learning methods. 3. Practical significance: FedNTD is particularly noteworthy because it does not require additional resources such as increased communication costs or auxiliary data making it a practical choice for realworld federated learning applications. Weaknesses: 1. Lack of theoretical underpinning: While the experimental results are promising the paper somewhat lacks a thorough theoretical analysis of why and how FedNTD effectively prevents forgetting in federated learning. 2. Specificity of the nottrue class approach: The methods reliance on distillation for nottrue classes might not be easily extendable to problems beyond classification such as regression or unsupervised learning tasks limiting its applicability. Questions and Suggestions for the Authors: 1. Theoretical foundation: Could the authors provide a more detailed theoretical analysis or intuitive explanation on why focusing on nottrue classes effectively mitigates the forgetting problem This would strengthen the papers contributions and help in understanding the methods applicability to different scenarios. 2. Applicability to other learning tasks: How would the authors envision adapting FedNTD to tasks other than classification such as regression or unsupervised learning Some discussion or direction on this front would broaden the impact of the work. 3. Data privacy implications: Given the method relies on distillation are there any potential data privacy implications or risks that might arise from using global knowledge in local model training It would be valuable to discuss any such concerns and how they might be addressed. Limitations and Societal Impact: The authors have made commendable efforts in addressing the limitations and potential negative societal impacts of their work. However the paper could further explore the implications of biases in the global model being potentially reinforced in local models due to the knowledge preservation technique. Additionally discussing potential countermeasures or considerations for ensuring that FedNTD does not exacerbate biases would be beneficial. Overall this paper presents a significant step forward in addressing a critical challenge in federated learning. With further theoretical insights and exploration of its applicability to a broader range of learning tasks the contributions of this work have the potential to influence future research in federated learning positively.", "oUigTwc7Cw5": " Peer Review  1) Summary of the Paper This paper applies the principles of efficient coding theory to understand the organization and functional diversity of retinal ganglion cells (RGCs) by modelling their response to natural video stimuli. The authors developed a computational model to explore how changes in retinal channel capacity\u2014simulated as the number of available RGCs\u2014impact the density and diversity of RGC mosaic organizations. They found that increasing the retinal channel capacity leads to the emergence of new RGC types each specializing in encoding different spatiotemporal frequency bands. Furthermore they observed a transition from alignment to antialignment in the organization of ON and OFF cell type mosaics with varying noise levels. The study contributes a unified theoretical framework that aligns well with observed biological evidence showing how the retinal system could optimize encoding efficiency under resource constraints.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Contribution: The paper provides a robust theoretical framework for understanding the diversity and organization of RGCs extending the applicability of efficient coding theory to temporal and spatial aspects of retinal processing.  Comprehensive Modeling Approach: By systematically varying the number of cells and analyzing the impact on cell types and mosaic structures the study offers indepth insights into how retinal encoding adjusts to different channel capacities.  Realworld Relevance: The use of natural videos for training the model adds practical relevance to the study ensuring the findings are applicable to realworld visual processing.  Alignment with Biological Evidence: The results correspond well with known biological characteristics of RGCs reinforcing the models validity. Weaknesses:  Model Assumptions: The assumptions made for linear filtering and separable filters may simplify complex biological processes too much risking overlooking nonlinear dynamics present in actual retinal processing.  Limited Exploration of Noise Impact: While the study examines the effects of input and output noise on mosaic organization it could be expanded to include other types of noise and their interplay in shaping RGC configurations.  Generalization Concerns: It remains unclear how well the findings generalize across species given variations in retinal structure and function.  3) Questions and Suggestions for Authors  Clarification on Model Assumptions: Could the authors elaborate on the choice of model assumptions particularly the rationale behind using linear filtering and separable filters and discuss any potential impacts these assumptions might have on the studys conclusions  Exploration of Nonlinear Dynamics: Have the authors considered extending their model to incorporate nonlinear dynamics of RGCs If not this might be a valuable direction for future research to capture more complex aspects of retinal processing.  Impact of Different Noise Types: The study focuses on input and output noise but considering the retinas noisy environment exploring other noise types (e.g. synaptic noise) could provide a more nuanced understanding of RGC organization.  4) Limitations and Societal Impact The authors have made a commendable effort to address the limitations inherent in their model particularly by discussing the simplifications involved. However further discussion on the potential limitations regarding the generalizability of the results across different species and the implications of model assumptions on the studys outcomes would strengthen the paper. Regarding societal impact the study is primarily theoretical and focused on explaining biological phenomena hence direct negative societal impacts seem minimal. However elucidating any potential applications of this research in improving artificial visual systems or treatments for visual impairments could enhance the papers relevance and highlight positive societal contributions. Constructive suggestions include extending the model to incorporate more complex biological dynamics and exploring the impact of various types of noise. Additionally discussing potential translational applications of the research could increase its impact.", "zfo2LqFEVY": " 1) Brief Summary of the Paper The paper introduces a novel Multimodal Grouping Network (MGN) aimed at enhancing audiovisual video parsing under a weaklysupervised setting which is critical for more accurately parsing videos into semantically meaningful temporal segments across different modalities (audio and visual). MGN addresses the limitations of previous approaches by introducing explicit multimodal grouping which aggregates unimodal features through unimodal grouping with learnable categorical embedding tokens and leverages crossmodal grouping for modalityaware predictions. Remarkably MGN not only achieves superior parsing performance on the LLP dataset when compared to stateoftheart methods but also significantly reduces the model size utilizing only 47.2 of the parameters of previous baseline models.  2) Assessment of the Papers Strengths and Weaknesses Strengths:  Innovative Approach: The introduction of explicit multimodal grouping to handle audio and visual features separately and in combination is a novel and effective strategy for improving parsing accuracy.  Superior Performance: Experimental results demonstrating MGNs superiority over existing stateoftheart methods across various metrics validate the authors claims and highlight the effectiveness of their approach.  Efficiency: The significant reduction in model size without compromising on performance is commendable offering practical advantages in terms of deployment and scaling. Weaknesses:  Audio Modality Performance: The performance gains for audio events are not as significant as for visual events suggesting an imbalance in the models efficacy across different modalities.  Limited Depth Analysis: The paper mentions performance degradation with an increase in transformer layers depth in grouping modules but does not explore this in detail. Understanding the limitations related to transformer depth could provide insights for future improvements.  Dependence on VideoLevel Annotations: While MGN addresses weaklysupervised learning excellently its heavy reliance on videolevel labels without segment or modalitylevel annotations might limit its application in scenarios where finergrained supervision is essential.  3) Questions and Suggestions for the Authors  Regarding Audio Performance: Can the authors elaborate on potential strategies to enhance the models performance specifically for audio events  Transformer Depth: Could the authors provide more insights into the issues encountered with increasing transformer depth in the grouping modules Any thoughts on how to mitigate these issues would be beneficial.  Application to Other Domains: Have the authors considered applying their MGN model to other forms of multimodal data beyond audiovisual content e.g. textimage data If so what adaptations if any would be necessary  Limitations and Societal Impact While the authors briefly mention the potential for the model to inherit biases from training data a more thorough discussion on limitations and negative societal impacts is warranted. For instance the paper could benefit from a section detailing strategies for mitigating data biases and ensuring the models equitable performance across diverse video content. Additionally the authors should consider elaborating on the limitations related to the models reliance on videolevel annotations and exploring semisupervised learning approaches as a potential future direction. Constructive suggestions would include implementing fairness and bias evaluation metrics during model validation and investing in datasets with diverse and balanced representations for training and testing.", "lfe1CdzuXBJ": " Review of \u201cGroup Meritocratic Fairness in Linear Contextual Bandits\u201d  1) Summary of the Paper The paper addresses the challenge of maximizing rewards in the linear contextual bandit problem while ensuring fairness across candidates from sensitive groups. The authors introduce the notion of Group Meritocratic Fairness (GMF) which focuses on selecting candidates based on the highest relative rank within their groups a method that accounts for the disparities in reward distributions due to societal biases or differences in job types. A greedy policy is developed to estimate this rank and make selection decisions demonstrating sublinear fair pseudoregret both theoretically and through simulations with the US census data. The policy uniquely does not require an exploration phase due to its fair selection mechanism.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper introduces a novel fairness metric (GMF) in the realm of linear contextual bandit problems filling a gap in addressing discrimination and bias in sequential decisionmaking.  Relevance: Given increasing concerns around AI and fairness the work is highly relevant to both academic research and practical applications like hiring and admissions.  Solid Experimental Design: Simulations and realworld experiments with US census data convincingly support the theoretical findings showcasing the practical viability of the approach.  Theoretical Contribution: The paper provides a detailed theoretical analysis including a sublinear fair pseudoregret bound for the proposed greedy policy which is an invaluable resource for further research in this area. Weaknesses:  Limited Scalability: The computational and memory costs associated with the FairGreedy algorithm increase linearly over time which may limit scalability to larger datasets or realtime applications.  Restricted Assumptions: The analysis relies on contexts of different groups being independent random variables and rewards being absolutely continuous which might not hold in all practical scenarios.  Lack of Comparison with Other Fairness Approaches: Although the paper provides a solid comparison with standard algorithms it lacks a detailed comparison with other fairnessfocused methods that could have enriched the analysis.  3) Questions and Suggestions for the Authors Questions:  Could the authors elaborate on the potential implications of the assumption of independent contexts between groups in realworld scenarios where such independence might not hold  How does the FairGreedy policys performance and computational demand scale with significantly larger datasets compared to those used in the experiments Suggestions:  Exploring Scalability Enhancements: Investigate and discuss strategies for reducing the computational and memory footprint of the FairGreedy algorithm to enhance its scalability.  Expanded Comparisons: Include comparisons with other fairnessoriented algorithms or policies to provide a broader context for the effectiveness and uniqueness of the GMF approach.  Relaxing Assumptions for Broader Applicability: Future work could look into relaxing some of the restrictive assumptions such as the independence of contexts across groups and the absolute continuity of rewards to make the approach applicable to a wider range of scenarios.  4) Limitations and Societal Impact The authors have made an effort to address the limitations related to the assumptions on the distribution and independence of context vectors. However further exploration into scaling the computational aspects for larger realworld applications is warranted. Regarding societal impact the paper takes an important step towards fair decisionmaking in hiring and other selection processes. Nonetheless the implementation of these strategies should be accompanied by ethical considerations to ensure that they do not unintentionally reinforce existing biases or inequalities. Suggestions for improvement include a thorough examination of the algorithms behavior in diverse realworld environments and incorporating feedback mechanisms to continually assess and adjust the fairness criteria based on societal changes and values.", "wmsw0bihpZF": " Review of \"Optimizing Data Collection for Machine Learning: A LearnOptimizeCollect Approach\"  1. Summary of the Paper This paper introduces a novel framework for addressing the challenge of data collection in machine learning termed as LearnOptimizeCollect (LOC). The authors argue that current approaches to data collection often based on rough estimates or extrapolations from neural scaling laws are either inefficient or risk significant undercollection potentially jeopardizing project success. The primary contribution of this work is the formulation of data collection as an optimal data collection problem letting designers specify performance targets costs and time horizons. The paper then presents LOC an optimization framework that integrates gradient descent for minimizing future data collection costs generalizing to multiple data types and sources. Through numerical experiments on various machine learning tasks the paper demonstrates that LOC can significantly reduce the likelihood of failing to meet performance targets and do so at lower cost compared to conventional estimationbased approaches.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper addresses a crucial yet underexplored problem in deploying effective machine learning systems\u2014how to optimize data collection. The proposed LOC framework is original and offers a clear improvement over existing methods.  Relevance: Given the increasing reliance on large datasets for training robust machine learning models the topic is highly relevant. It directly addresses cost and efficiency which are critical concerns in practical machine learning applications.  Theoretical and Practical Integration: The paper successfully bridges theoretical modeling with practical application. By applying the model to varied tasks like classification segmentation and object detection it demonstrates LOCs versatility and effectiveness.  Experimental Validation: The experiments are welldesigned and convincingly show the advantages of the LOC approach over traditional estimation methods. The inclusion of realworld task scenarios enhances the papers practical implications. Weaknesses:  Generalization of Results: While the experiments cover a range of tasks they are limited to the domain of computer vision. It would be beneficial to see the framework tested on different types of data such as text or audio to assess its broader applicability.  Complexity and Scalability: The paper does not thoroughly discuss the computational complexity of LOC particularly in scenarios involving very large datasets or complex multisource data collection problems. Its unclear how scalable this approach is especially under tight computational or time constraints.  Parameter Sensitivity: While the paper mentions robustness to cost and penalty parameters there is insufficient detail on how sensitive the LOC framework is to these and other hyperparameters. More substantive analysis on this aspect would strengthen the papers findings.  3. Questions and Suggestions for Improvement  Broader Testing: Can the authors extend the experimental validation to nonvision tasks to demonstrate LOCs effectiveness across different data modalities  Complexity Analysis: A more detailed discussion on the computational complexity of LOC including time and memory requirements would provide a clearer understanding of its scalability.  Parameter Sensitivity Analysis: It would be beneficial to include a more detailed sensitivity analysis regarding the LOCs parameters. Understanding how variations in these parameters affect performance could guide users in more effectively applying the framework.  Fairness and Privacy Considerations: The authors briefly mention the potential for incorporating privacy and fairness constraints. Expanding on how LOC could integrate such considerations perhaps even with preliminary models or suggestions would be a valuable addition.  4. Limitations and Societal Impact The paper briefly acknowledges the need for fairness and privacy in data collection but does not fully explore these dimensions. Given the potential for biased data collection to exacerbate existing inequalities a more thorough analysis of how LOC can be adapted to prevent such outcomes is warranted. On the positive side reducing unnecessary data collection can have beneficial environmental implications highlighting a significant positive societal impact of this work. Overall this paper presents a valuable contribution to the field of machine learning data collection offering a novel and practically useful framework for optimizing data collection efforts. Addressing the outlined questions and suggestions could further enhance its impact and applicability.", "zGvRdBW06F5": "Peer Review of \"Enabling Tiny OnDevice Training for Deep Learning on IoT Devices\" 1. Brief Summary This paper introduces an innovative algorithmsystem codesign framework tailored for tiny ondevice training of deep learning convolutional neural networks (CNNs) under the strict memory constraints (256KB) typical of IoT devices. This breakthrough is achieved through two main contributions: QuantizationAware Scaling (QAS) and Sparse Update methodologies supported by a purposebuilt training system the Tiny Training Engine (TTE). Together these advancements enable ondevice finetuning and lifelong learning without the need for data offloading thus preserving user privacy. 2. Assessment of Strengths and Weaknesses  Strengths: a. The paper addresses a critical and timely challenge in the IoT and machine learning domain focusing on the privacy and adaptability of AI models directly on devices with extremely limited memory resources. b. Introducing QAS the paper cleverly tackles the optimization barriers posed by low bitprecision in quantized graphs enabling effective training under tight memory constraints without compromising on model accuracy. c. Sparse Update represents a significant methodological innovation allowing for selective backpropagation based on the importance of layers and subtensors thus minimizing memory footprint while still achieving high model performance. d. The development of the Tiny Training Engine is a commendable systemlevel innovation that demonstrates practical application and significant memory savings compared to existing deep learning frameworks.  Weaknesses: a. The study primarily focuses on vision recognition tasks with convolutional neural networks limiting its immediate applicability to other domains or model architectures such as RNNs or Transformers. b. The discussion on societal impacts particularly the environmental considerations linked to electricity consumption during the development and testing phases is somewhat superficial and could benefit from deeper analysis and mitigation strategies. c. The demonstration is limited to a single hardware configuration (STM32F746 microcontroller) which may not fully represent the performance or constraints of other contemporary or future IoT devices. 3. Questions and Suggestions for the Authors a. Have you considered the scalability of your approach for more complex models or other forms of neural networks It would be beneficial to discuss potential barriers and expansion strategies. b. The study could benefit from an expanded evaluation involving multiple hardware configurations to better understand the generalizability of the proposed solutions across different microcontrollers and IoT devices. c. Could the authors provide more insights into the tradeoffs between model accuracy training time and memory usage This would be particularly helpful for practitioners aiming to implement the framework under different operational constraints. d. Given the importance of privacy protection highlighted in the study can the authors elaborate on potential vulnerabilities or considerations when implementing the ondevice training in realworld scenarios 4. Limitations and Suggestions for Improvement The authors have done a commendable job in addressing many technical limitations through their innovative approach. However the broader applicability beyond vision recognition tasks and the need for further exploration into more diverse model architectures and IoT platforms are notable limitations. Future work could explore these directions to enhance the frameworks utility and applicability further. Additionally while the paper briefly mentions the societal benefits of ondevice learning in terms of privacy a more thorough consideration of potential negative societal impacts and environmental considerations would be valuable. Specifically proposing strategies for minimizing the carbon footprint linked to the development and deployment of these models could enrich the papers discussion on its societal impacts.", "z9CkpUorPI": "Review of \"NeuroSchedule: A Graph Neural Networkbased Scheduling Method for HighLevel Synthesis\" 1. Summary: This paper introduces NeuroSchedule a pioneering approach leveraging Graph Neural Networks (GNNs) for scheduling in highlevel synthesis (HLS). HLS is crucial for transforming highlevel language specifications into circuitlevel implementations with scheduling being a crucial step for performance optimization. Traditional scheduling techniques either suffer from excessive runtime or suboptimal solution quality. NeuroSchedule addresses these issues by proposing a GNNbased model that can predict scheduling priorities efficiently offering a substantial improvement in both solution quality and runtime. The main contributions include the formulation of a learning problem for HLS scheduling a GNNbased learning framework and the adoption of pretraining models to enhance scalability across various scheduling scenarios. The experimental results demonstrate NeuroSchedules ability to achieve nearoptimal solutions with significant speedups and improved solution quality over stateoftheart methods. 2. Assessment:  Strengths: 1. Novelty: The paper introduces the first attempt at using GNNs for tackling the HLS scheduling problem presenting a novel and promising direction in EDA tools development. 2. Performance Improvement: NeuroSchedule substantially outperforms existing methods providing over a 50000\u00d7 improvement in runtime while also improving solution quality by 6.10 compared to the leading entropydirected method. 3. Comprehensive Evaluation: The method has been thoroughly evaluated against traditional and stateoftheart methods across benchmarks showing its effectiveness and efficiency. 4. Scalability: The adoption of pretraining models to handle various scheduling settings effectively addresses scalability a common concern in machine learning applications for EDA.  Weaknesses: 1. Lack of Theoretical Insight: While the empirical results are promising the paper lacks a theoretical analysis of why GNNs are particularly suited for the scheduling problem in HLS. 2. Generalization Concerns: The paper does not fully address the generalization capability of NeuroSchedule across different hardware architectures or more complex scheduling scenarios beyond what is tested. 3. Impact of Pretraining: The paper briefly mentions the use of pretraining but lacks indepth discussions or thorough evaluations of its impact and the tradeoffs involved. 4. Computational Overheads: While runtime improvements are highlighted theres little discussion on the computational cost or overhead introduced by training the GNN models. 3. Questions and Suggestions for the Authors:  Can the authors provide theoretical insights or intuitions on why GNNs are ideally suited for the scheduling problem in HLS  It would be beneficial to discuss the generalization capability of NeuroSchedule to other forms of hardware architectures or more complex design specifications.  More details on the impact of pretraining on the models performance and scalability would be desirable. Specifically the paper could benefit from a discussion on how pretraining affects the adaptability of NeuroSchedule to various scheduling configurations.  A discussion on the computational overheads involved in training and deploying NeuroSchedule compared to its runtime improvements would provide a more balanced view of its practical applicability. 4. Limitations and Societal Impact: The authors have not fully discussed the limitations or potential negative societal impacts of their work. While the paper focuses on the technical advancements considerations regarding the computational resources required for training the models potential biases in the training data and the environmental impact of deploying such AIbased solutions are notably absent. To improve the authors could:  Analyze the environmental impact of training GNN models for HLS scheduling considering the energy consumption and sustainability aspects.  Discuss the fairness and bias implications of automated scheduling especially how training data and model assumptions might influence the outcomes.  Propose strategies to mitigate potential negative impacts including more efficient training algorithms bias detection and correction mechanisms and transparency in model decisions. Overall NeuroSchedule represents a significant advancement in HLS offering potential efficiency improvements in FPGA programming and broader applications. However a more detailed exploration of its theoretical foundations limitations generalization capabilities and societal impacts would enhance the understanding and applicability of this promising approach.", "w_jvWzNXd6n": " 1. Summary of the Paper and its Contributions The paper introduces the SBMTransformer an innovative model designed to overcome the computational inefficiencies inherent in the traditional Transformer architecture by implementing a mixedmembership Stochastic Block Model (SBM). This model allows each attention head to adaptively determine the sparsity of its attention mechanism based on input data effectively varying the computational cost from linear to quadratic as needed. The SBMTransformer is demonstrated to universally approximate arbitrary functions with empirical evidence of its superior performance on the LRA and GLUE benchmarks compared to other efficient Transformer variants and the original dense attention Transformer. The authors provide open access to their implementation for further exploration and replication of results.  2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The usage of a mixedmembership Stochastic Block Model for dynamic attention sparsity is novel offering a fresh perspective on addressing computational inefficiencies in Transformers.  Empirical Validation: The models capabilities are thoroughly validated across diverse benchmarks demonstrating its universal applicability and superior performance.  Theoretical Grounding: The authors provide a theoretical framework to support the universal approximability of the SBMTransformer strengthening the credibility of their model. Weaknesses:  GPU Optimization: The paper identifies the challenge of sparse tensor operations being less optimized for GPU computation as a limitation. While acknowledged this might significantly hinder the practical deployment of SBMTransformers especially in resourceintensive applications.  Complex Implementation: The innovative nature of the SBM attention mechanism could potentially lead to complexities in understanding implementing or extending the model limiting its accessibility to a wider audience.  Generalization Potential: Despite strong empirical results the specific conditions or data characteristics under which SBMTransformer outperforms traditional and other efficient Transformers require clearer elucidation.  3. Questions and Suggestions for the Authors  Extension to Other Architectures: Can the SBM mechanism be adapted or extended to other neural network architectures beyond Transformers especially in nonsequential data processing  Impact of Cluster Number (k): How sensitive is the models performance to the number of clusters (k) used in the SBM Some guidelines or a sensitivity analysis could provide better insights for practitioners.  Handling GPU Optimization: Are there ongoing efforts or potential collaborations to optimize sparse tensor operations on GPUs Any insight into this could significantly enhance the usability of SBMTransformers.  4. Limitations and Societal Impact The authors have adequately addressed the primary limitations of their work particularly highlighting the current inefficiencies in GPU computation regarding unstructured sparsity. This acknowledgment is crucial as it sets a clear direction for future work in computational optimization that will maximize the potential of SBMTransformers. However a further discussion on the implications of these computational demands in terms of energy consumption and environmental impact would have been beneficial. On the societal impact front while the authors do not foresee immediate negative impacts the broader implications of increasingly efficient Transformers in automating tasks potentially affecting employment in certain sectors could have been considered. Suggestions for future iterations of this work could include exploring energyefficient computing methods for handling sparse operations and a more indepth analysis of the societal implications of widespread adoption of such efficient AI models.", "xs9Sia9J_O": "Review of the Paper \"Mitigating Error Accumulation in IGM Decomposition through Imitation Learning\" 1. Summary: The paper presents a novel approach to address the problem of error accumulation in Individual Global Max (IGM) decomposition in cooperative multiagent reinforcement learning (MARL) specifically within the framework of centralized training and decentralized execution (CTDE). The authors identify a critical issue in IGMbased methods which is the lossy decomposition from jointaction value to individualaction values and the subsequent error accumulation during the training process. To mitigate these problems the paper introduces a training paradigm named IGMDA that combines IGM decomposition with an imitation learning strategy specifically using the DAgger algorithm. Theoretical proofs and empirical evaluations on the StarCraft MultiAgent Challenge (SMAC) benchmark demonstrate that IGMDA significantly improves performance by avoiding error accumulation especially in scenarios with limited perceptual capabilities (zero vision range). 2. Assessment:  Strengths: 1. Novelty and Theoretical Contribution: The paper addresses a less explored issue in IGM decomposition and introduces a theoretically sound solution. The use of imitation learning to avoid error accumulation in hypernetworkbased methods is novel and could be influential for future research in MARL. 2. Empirical Validation: Comprehensive experiments on the SMAC benchmark with different vision ranges and difficulty levels robustly validate the effectiveness of the proposed method. 3. Theoretical Soundness: The provision of theoretical proofs for propositions relating to error accumulation and the mitigation thereof through IGMDA adds a layer of rigor to the work.  Weaknesses: 1. Generalization of Results: While the paper showcases the effectiveness of IGMDA in the SMAC benchmark the generalization of these results to other domains or benchmarks is not thoroughly discussed. 2. Comparison with Baselines: The selection of baselines is adequate but the impact of the proposed method compared to other forms of error mitigation or decomposition methods is not fully explored. 3. Complexity Analysis: There is limited discussion on the computational complexity and scalability of IGMDA which is crucial for practical implementation in largescale MARL problems. 3. Questions and Suggestions:  Generalization: How well does IGMDA generalize to other multiagent problems outside the SMAC benchmark It would be beneficial to see evaluations on different benchmarks or realworld scenarios to assess the methods versatility.  Comparison with other Error Mitigation Strategies: Are there alternative strategies to mitigate error accumulation in MARL A comparative analysis with such methods could help highlight the unique benefits of IGMDA.  Scalability and Complexity: The paper could be improved by a detailed analysis of the computational complexity of IGMDA especially in comparison with traditional IGM methods. Suggestions to optimize or reduce its computational footprint could also be valuable.  Hyperparameter Sensitivity: The paper could provide insights into the sensitivity of IGMDAs performance to its hyperparameters particularly those related to the DAgger algorithm. 4. Limitations and Societal Impact: The authors have addressed the limitations regarding the focus on hypernetworkbased methods and the usage of a single imitation learning technique. Expanding the work to include other types of value decomposition methods and advanced imitation learning techniques is suggested as future work. Regarding societal impact the paper does not discuss potential negative implications. However as with any AI research its crucial to consider ethical use guidelines especially in applications where MARL can be implemented such as autonomous systems. Suggestions for improvement include a more explicit discussion on the ethical considerations and potential misuse of the technology in sensitive applications. Conclusion: The paper makes a significant contribution to the field of MARL by addressing the problem of error accumulation in IGM decomposition. Despite some limitations the proposal of IGMDA is both novel and theoretically sound with substantial empirical evidence supporting its efficacy. Future work addressing the outlined questions and suggestions could further strengthen the impact and applicability of this research.", "qYc8VnmUwbv": " 1) Brief Summary of the Paper and its Contributions: This paper addresses the computationalstatistical gap in sequential and batch statistical learning particularly focusing on the smoothed sequential learning setting. The authors developed a computationally efficient algorithm that achieves statistically optimal log(T\u03b5) regret for several classes of realizable smoothed online classification problems (like Kwise linear classification and piecewise affine regression) overcoming the exponential gap present in prior works between optimal regret and efficiently achievable regret. Noteworthy is the novel characterization of the geometry of the disagreement region as well as the introduction of technical tools of independent interest including a general anticoncentration bound for the determinant of matrix averages.  2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: The paper introduces innovative techniques that diverge from standard disagreementbased analyses offering new insights into achieving optimal regret in smoothed sequential learning. The computational efficiency of the algorithm coupled with its optimal regret performance across various settings represents a significant advancement.  Broad Applicability: The algorithm is tested across multiple settings demonstrating its versatility in handling affine thresholds nonlinear features Kclass affine classification and piecewise affine regression among others. This wide applicability enhances the impact of the findings.  Theoretical Contributions: Beyond solving the problem at hand the paper contributes new technical tools and a novel characterization of the disagreement regions geometry which can be of independent interest for future research in online learning and related fields. Weaknesses:  Assumption Specificity: While the algorithm performs well under the smoothed sequential learning setting the reliance on specific assumptions about the adversarys behavior and the nature of the contexts (e.g. \u03b5smoothness) may limit applicability in more adversarial or unpredictable environments.  Complexity of Implementation: The descriptions suggest that although the algorithm is computationally efficient the underlying mechanics including computations involving the John ellipsoid and various geometric and probabilistic tools might pose practical implementation challenges for those not deeply familiar with the fields.  Empirical Validation: The paper primarily focuses on theoretical contributions and lacks comprehensive empirical validation across realworld datasets or comparison to other stateoftheart algorithms in similar settings.  3) Questions and Suggestions for the Authors:  Clarification on Assumptions: Can the authors provide more intuition or discussion on the practical implications of the \u03b5smoothness assumption and its prevalence in realworld data settings  Implementation Details: It would be beneficial for reproducibility and practical applicability if the authors could include pseudocode or more detailed implementation guidance for the algorithm particularly regarding the computation of the John ellipsoid.  Empirical Evaluation: Are there plans to empirically evaluate the proposed algorithms performance on realworld datasets Such validation would significantly strengthen the impact and applicability of the findings.  4) Limitations and Societal Impact: The paper could provide a more explicit discussion of the limitations particularly concerning the assumptions made and the potential challenges in applying the algorithm to less structured or more adversarial environments than those considered. Additionally the potential societal impact both positive and negative could be more thoroughly explored. For instance while the algorithm can significantly advance the fields of online learning and inform more efficient decisionmaking processes its misuse in sensitive applications (like surveillance or predictive policing) warrants cautious consideration. The authors might consider discussing these aspects and suggesting guidelines for ethical use.", "sFQJ0IOkHF": " Peer Review of \"DivBO: A Diversityaware Framework for Ensemble Learning in AutoML Systems\"  1) Summary of the Paper The paper introduces DivBO a novel diversityaware framework designed to solve the Combined Algorithm Selection and Hyperparameters optimization (CASH) problems specifically within the context of Automated Machine Learning (AutoML) systems focusing on ensemble learning. The primary innovation of DivBO lies in its incorporation of diversity search into the CASH problem addressing the inefficiency of conventional methods that prioritize performance over diversity among base learners. DivBO utilizes a diversity surrogate model to predict the pairwise diversity of configurations and employs a weighted acquisition function in a Bayesian optimization framework to optimize both performance and diversity. The method is tested across 15 public datasets demonstrating superior performance against 10 contemporary methods including stateoftheart ensemble learning strategies for CASH problems.  2) Assessment of Strengths and Weaknesses Strengths:  Innovation: DivBO creatively addresses the limitation of extant AutoML systems by integrating diversity explicitly into the model selection and hyperparameter optimization processes. This is a notable advancement for enhancing ensemble learning performance in CASH problems.  Empirical Validation: The methodology is rigorously validated on a diverse set of public datasets and it consistently outperforms both traditional AutoML systems and the most recent baselines in ensemble learning. The extensive empirical results add credibility to the claims made.  Practical Impact: By improving the diversity within the ensemble learners DivBO potentially enhances the robustness and generalizability of AutoML systems which is crucial for realworld applications across various domains. Weaknesses:  Complexity and Computational Requirements: The addition of a diversity search could potentially increase the computational complexity and resource requirements although the paper argues that the time complexity is manageable. This aspect could be a barrier for practical adoption in resourceconstrained environments.  Scope of Diversity: The paper focuses on diversity in terms of the Euclidean distance of probabilistic predictions which is a specific aspect of diversity. There could be other dimensions of diversity that are not captured by this metric limiting the scope of the approach.  Lack of Theoretical Underpinning: While the empirical results are strong the paper does not provide a deep theoretical exploration of why diversityenhancing techniques improve ensemble performance in AutoML. A theoretical analysis could offer insights into the conditions under which DivBO would be most effective.  3) Questions and Suggestions for the Authors  Question on Diversity Metric: Could the authors elaborate on the choice of using Euclidean distance as the diversity metric Are there other metrics that were considered or tested and how do they impact the performance of DivBO  Computational Resource Analysis: It would be informative to include a more detailed analysis of the computational resources required for DivBO especially in comparison to other methods. Understanding the tradeoffs between computational cost and performance gain is crucial for practical applications.  Extension to Regression Problems: The authors mention the potential applicability of DivBO to regression problems with a different diversity function. Could they provide preliminary results or insights on how DivBO might perform in such scenarios  Potential for Theoretical Analysis: Do the authors plan to explore a theoretical framework to explain the observed empirical benefits of integrating diversity into ensemble learning for CASH problems Such analysis would be valuable for understanding the fundamental principles underlying the approach.  4) Limitations and Constructive Suggestions The paper provides limited discussion on the potential negative societal impacts of the work. While AutoML has numerous beneficial applications its automation and optimization capabilities could inadvertently reinforce existing biases in datasets or lead to overreliance on automated decisionmaking systems without adequate human oversight. It would be beneficial for the authors to acknowledge these concerns and discuss possible safeguards or ethical considerations that should be taken into account when deploying AutoML solutions like DivBO in realworld settings.  Improvement Suggestions:  Consider exploring alternative diversity metrics and their implications on ensemble learning within the AutoML context to broaden the applicability and robustness of DivBO.  Provide a deeper analysis of the computational efficiency of DivBO potentially including comparisons with other stateoftheart methods to highlight its practical viability.  Engage in theoretical exploration to support the empirical findings offering insights into why and under what conditions diversity optimization enhances AutoML ensemble performance.", "nxw9_ny7_H": "Peer Review for \"AugNet: Embedding Data Augmentation Invariances into Neural Networks\" 1. Summary of Contributions: The paper presents a novel methodology termed AugNet for learning data invariances directly from training data thereby circumventing the complexities of bilevel optimization inherent in Automatic Data Augmentation (ADA). The authors introduce differentiable data augmentation layers within the neural network architecture which can adapt to any form of differentiable augmentations across diverse data types beyond standard computer vision tasks. AugNet is characterized by its inbuilt ability to discover and encode data invariances leading to a significant simplification and acceleration of the training process compared to existing ADA techniques. Empirical results across various datasets including computer vision and sleep stage classification demonstrate that AugNet achieves comparable if not superior performance while being more efficient to train. Furthermore the authors showcase that AugNets performance is not constrained by the models capacity thus overcoming a limitation observed in traditional data augmentation methods. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovativeness: The introduction of learnable data augmentation layers that can be directly integrated into the network architecture is an innovative approach that efficiently addresses the learning of invariances from training data.  Versatility and Generality: The methods applicability beyond computer vision to include tasks like sleep stage classification using EEG signals demonstrates significant versatility and the potential for broad adoption across different domains.  Performance: The empirical evaluation presents strong evidence that AugNet can achieve or exceed the performance of stateoftheart ADA methods with substantially less computational overhead which is an impressive feat. Weaknesses:  Potential for Overfitting: While the paper discusses the methodologys strengths extensively there seems to be a lack of discussion around the potential for overfitting especially in situations where the training data may not be sufficiently representative of the underlying data distribution.  Computational Complexity at Inference: Although AugNet simplifies the training process the increased computational complexity at inference time due to the necessity of multiple forward passes (as mentioned in the limitations section) could be a significant downside for realtime applications.  Limited Discussion on Differentiable Augmentations: The reliance on differentiable augmentations is a core part of AugNets design. However the discussion on potential limitations or challenges in finding or designing these augmentations for specific tasks is somewhat lacking. 3. Questions and Suggestions for the Authors:  Question: How does AugNet perform under scenarios of limited training data and what strategies could potentially mitigate any degradation in performance  Suggestion: It would be beneficial to include a more indepth analysis of the risk of overfitting with AugNet perhaps through additional experimental validations with varying sizes of training data or through theoretical insights.  Question: Could the authors elaborate on the possibilities and challenges of extending AugNet to nondifferentiable augmentations possibly through approximation techniques  Suggestion: To address the increased computational requirement at inference exploring and discussing potential optimizations or strategies to balance the tradeoff between augmentation complexity and inference speed would be valuable. 4. Limitations and Societal Impact: The authors acknowledge the limitations regarding the computational complexity at inference time due to the need for multiple forward passes when a large number of augmentations are employed. A constructive suggestion would be to explore adaptive mechanisms that dynamically adjust the number of augmentations based on the complexity of the input or the confidence of the prediction potentially reducing the computational burden without significantly compromising accuracy. On the societal impact front while the paper successfully addresses the technical limitations and potential efficiencies introduced by AugNet there is a minimal discussion on the broader implications of deploying such technologies especially in sensitive areas such as healthcare or security. It would be beneficial to include a thoughtful analysis of potential negative impacts such as biases in augmented data or the unintended consequences of more widely deploying highly efficient but complex machine learning models.", "lkrnoLxX1Do": " Peer Review for \"SelfSupervised Image Restoration of Blurry and Noisy Photographs Using Complementary Images\"  1) Brief Summary of the Paper The paper introduces a novel approach to image restoration under lowlight conditions a common challenge in photography where balancing noise and blur becomes essential. Traditional methods either enhance brightness resulting in high ISO noise or extend exposure causing blur due to camera motion. This study proposes leveraging both shortexposure noisy images and longexposure blurry images to improve restoration quality. By utilizing the complementarity of the two types of images the authors develop a selfsupervised learning model SelfIR which uses shortexposure images for deblurring and identifies sharp regions in longexposure images for denoising. The method tested on both synthetic and realworld datasets demonstrates superiority over existing stateoftheart techniques in selfsupervised denoising and supervised deblurring and denoising methods. The code is shared publicly for further research.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The dual use of blurry and noisy images to mutually aid in the tasks of denoising and deblurring is both novel and technically interesting presenting a practical solution to a common problem in lowlight photography.  Strong Experimental Validation: Extensive experiments using both synthetic and realworld datasets showcase the methods effectiveness outperforming current stateoftheart approaches.  Publicly Available Implementation: By providing their code the authors encourage reproducibility and further research aligning with open science principles. Weaknesses:  Limited Discussion on Limitations: The paper could benefit from a deeper discussion on the limitations of the approach especially considering different types of noise not covered in the current dataset or potential failure cases.  Lack of Comparative Analysis: Although the paper mentions surpassing stateoftheart methods a more detailed comparison or breakdown of what contributes to the performance gain could provide clearer insights.  Potential for Societal Impacts: The societal implications particularly how enhanced lowlight image restoration could be misused are not explored.  3) Questions and Suggestions for the Authors  Question: How does SelfIR handle extremely high ISO levels where noise could significantly affect the images visual quality beyond just graininess  Suggestion: Consider including a more diverse range of lowlight conditions in your datasets incorporating various environmental and artificial lighting conditions to test the models robustness.  Suggestion: A deeper exploration into the models limitations would be valuable highlighting scenarios where SelfIR may not perform optimally and suggesting possible directions for future improvements.  4) Evaluation of Limitations and Societal Impact The authors do not thoroughly address the limitations within the scope of their research. A detailed discussion regarding potential failure cases or the models applicability across different ranges of ISO levels and lighting conditions would enhance the papers credibility. Furthermore while the positive applications of this research are clear it is important for the authors to consider and discuss negative societal impacts such as privacy concerns arising from the potential misuse of enhanced lowlight surveillance imagery. Constructive Suggestions for Improvement:  Expand the Discussion section to include limitations related to different noise models and practical deployment scenarios.  Provide a comparative analysis of why SelfIR outperforms others in certain conditions potentially identifying areas for further enhancement.  Incorporate a section on potential negative societal impacts offering guidelines or considerations for ethical use of the technology.", "v6CqBssIwYw": " Peer Review for \"InstanceBased Uncertainty Estimation for GradientBoosted Regression Trees (IBUG)\"  1) Summary of the Paper The paper introduces a novel method named InstanceBased Uncertainty estimation for Gradientboosted regression trees (IBUG) designed to enhance GBRT models by estimating probabilistic predictions. Recognizing the limitation of GBRTs in estimating uncertainty IBUG employs a knearest neighbor approach using the tree ensemble kernel to compute a nonparametric distribution around predictions. The method\u2019s performance was validated on 22 benchmark regression datasets demonstrating comparable or superior results to stateoftheart methods both in probabilistic and point predictions. IBUG stands out by facilitating model flexibility accommodating various base GBRT models and directly modeling the output distribution with any parametric or nonparametric distribution.  2) Strengths and Weaknesses of the Paper Strengths:  Innovation and Applicability: IBUG addresses a significant gap in GBRT applications by providing uncertainty estimates which is a valuable addition to both forecasting and explainable AI (XAI) fields.  Flexibility and Compatibility: The model\u2019s ability to work with any GBRT base model and its capacity to utilize different distributions for modeling output make it a versatile tool for practitioners.  Empirical Validation: The extensive experimentation across 22 datasets and comparison with current stateoftheart methods effectively validate IBUGs performance.  Open Source Contribution: Releasing the source code enhances reproducibility and allows for wider adoption and further development of the method. Weaknesses:  Computational Efficiency: The method suffers from relatively slow prediction times primarily due to its reliance on knearest training instances which could hinder its application in timesensitive or largescale scenarios.  Potential Overfitting: The reliance on the knearest neighbor approach while beneficial for capturing uncertainty may lead to overfitting in certain datasets where the training instances are not representative of the wider distribution.  3) Questions and Suggestions for the Authors  Scalability Concerns: Could the authors elaborate on potential approaches to mitigate the computational costs especially in scenarios with very large datasets  K Selection Process: How sensitive is IBUG\u2019s performance to the choice of k Could the paper provide insights or guidelines on choosing an optimal k for various datasets  Comparison with Deep Learning Models: Given the mention of deep learning models success in unstructured data a comparison with deep models capable of uncertainty estimation in tabular data would enrich the discussion.  Discussion on Overfitting: Could the authors discuss any measures or experiments conducted to evaluate the risk of overfitting with IBUG given its use of training instances for prediction  4) Limitations and Societal Impact The paper briefly touches on the computational inefficiency of IBUG in comparison to other methods but does not thoroughly discuss potential workarounds or the impact of this limitation in practical scenarios. Additionally while the paper assumes that providing uncertainty estimates is inherently beneficial a more nuanced discussion on potential negative societal impacts such as misuse of probabilistic predictions or overreliance on uncertain estimates in critical applications would be valuable. Suggestions for Improvement:  Providing a detailed analysis of scenarios where IBUG might not be the best fit due to its computational overhead could help practitioners make informed decisions.  Exploring and discussing hybrid approaches that could potentially enhance prediction efficiency without significantly compromising on the models accuracy and uncertainty estimation capabilities.  A section dedicated to the responsible use of uncertainty estimates especially in highstakes domains could add value by encouraging ethical application of the method.", "xqYGGRt7kM": " Peer Review of \"Exploration of Streaming Algorithms for Stochastic MultiArmed Bandits\"  1. Brief Summary This paper delves into streaming algorithms for stochastic MultiArmed Bandits (MABs) with a focus on pure exploration to identify the best arm among a sequentially arriving set of n arms. It builds upon Assadi and Wang [STOC 2020] extending their work under the streaming model to achieve instancesensitive sample complexity an approach echoed by classical nonstreaming algorithms. The authors offer significant insights into the complexity of pure exploration in streaming MABs establishing lower bounds that demonstrate the inherent difficulty in obtaining instancesensitive sample complexity with limited memory. They argue that any streaming algorithm in this context necessitates either a higher sample complexity than the instancesensitive benchmark or substantial memory capacity. The paper counters these negative results with an algorithm that operates under stringent conditions but achieves instanceoptimal sample complexity with just a single arm memory.  2. Assessment Strengths  Contribution to Theory and Practice: The paper significantly advances the theoretical understanding of streaming MABs by introducing the arm trapping lemma and its corresponding complexity results which have broader implications beyond the immediate problem. This contributes to the foundational theory of MABs and potentially influences practical applications needing efficient data stream management.  Innovative Algorithm: The proposed algorithm for achieving instancesensitive sample complexity with a singlearm memory addresses a critical challenge in streaming MABs providing a potentially impactful tool for applications dealing with enormous datasets.  Rigorous Analysis: The paper is grounded in thorough mathematical and theoretical analysis offering proofs and formulations that solidify its findings and claims. The lower bound proofs in particular provide valuable insights into the problems complexity. Weaknesses  Assumption Restrictiveness: The algorithms effectiveness under very strong assumptions (knowledge of \\xe2\\x88\\x86[2] random order stream and known INSTcomplexity) could limit its applicability in realworld scenarios where such information might not be readily available or may be computationally impractical to assume beforehand.  Lack of Empirical Validation: The paper does not include empirical studies or experimental validation of the proposed algorithm. Practical performance insights comparisons with existing algorithms and impacts of the assumptions on real datasets could significantly strengthen the paper\u2019s implications for practical applications.  Complexity of Practicality: While the theoretical contributions are significant the paper does not thoroughly discuss the practical implementation challenges or the computational overhead introduced by the algorithm especially in realtime systems or those with stringent resource constraints.  3. Questions and Suggestions for the Authors  Empirical Validation: Could the authors provide some empirical evidence or simulation results of the proposed algorithm\u2019s performance in practical realworld datasets especially in comparison to existing algorithms  Assumption Relaxation: Is there a way to relax some of the stringent assumptions particularly the knowledge of \\xe2\\x88\\x86[2] and INSTcomplexity to make the algorithm more widely applicable  Practical Implementation: It would be beneficial to discuss the algorithms computational complexity and the practical implications of maintaining a singlearm memory in real streaming environments. How does it scale with the size of n or under varying stream velocities  Impact of Assumption Violation: What is the expected performance drop or potential failure modes of the algorithm if one or more of the assumptions do not hold (e.g. if the stream is not in random order)  4. Limitations and Societal Impact The paper addresses potential limitations by noting the assumptions required for the algorithm to achieve its theoretical bounds. However a more indepth discussion on the feasibility of meeting these assumptions in practice and the algorithms robustness under assumption violations would be beneficial. Furthermore the societal impact particularly in areas like online advertising finance or healthcare where strategic exploration can significantly influence outcomes remains unexplored. Constructive suggestions for improvement include:  Providing guidelines or strategies for approximating or achieving the necessary conditions for the algorithm in practical scenarios.  Discussing the ethical implications and fairness considerations in applications where strategic decisionmaking based on MABs can have significant societal impacts. In summary while the paper makes a noteworthy theoretical contribution to the field of streaming MABs addressing the practicality concerns and providing empirical validations could greatly enhance its impact and applicability.", "sr0289wAUa": "Summary and Contributions The paper introduces Adaptive Skill Prior for Reinforcement Learning (ASPiRe) a novel approach that accelerates reinforcement learning by leveraging multiple specialized skill priors derived from separate labeled offline datasets. ASPiRe\u2019s key innovation lies in its ability to dynamically combine these priors through an Adaptive Weight Module (AWM) which infers optimal weight assignments based on the task at hand allowing for both sequential and concurrent skill integration. This mechanism significantly improves learning efficiency and task success rates in environments with sparse rewards. The paper evaluates the efficacy of ASPiRe against competitive baselines across various complex tasks demonstrating its superior performance in utilizing and combining multiple behavior priors for accelerated reinforcement learning. Strengths 1. Innovative Approach: ASPiRe addresses the limitations of single skill prior approaches by introducing a flexible framework that can combine multiple specialized skill priors adaptively a significant advancement in reinforcement learning methodologies. 2. Comprehensive Evaluation: The experiments conducted are thorough and convincingly demonstrate ASPiRe\u2019s superiority in enhancing the learning process in challenging environments showcasing clear improvements over existing baselines. 3. Potential for Wide Application: The approach\u2019s ability to dynamically combine skills from distinct domains suggests wide applicability across various tasks potentially benefiting a broad range of reinforcement learning applications. Weaknesses 1. Practicality of Skill Prior Generation: The necessity for separated and labeled datasets to extract specialized skill priors may limit ASPiRes practical application in scenarios where such datasets are unavailable or hard to generate. 2. Complexity of Implementation: While ASPiRe shows promising results the complexity involved in implementing such a system with multiple skill priors and adaptive weighting might pose challenges in terms of computational resources and realworld applicability. 3. Lack of Discussion on Hyperparameter Sensitivity: The paper does not address how sensitive ASPiRe\u2019s performance is to its hyperparameters particularly in the context of the Adaptive Weight Module which could impact its robustness and adaptability across different tasks. Questions and Suggestions for Authors 1. Skill Prior Generalization: How well do you expect ASPiRe to perform in tasks radically different from those in the trained datasets A discussion on the generalizability and transferability of skill priors would be valuable. 2. Handling Unlabeled Data: Could the authors elaborate on potential strategies for extending ASPiRe to work with unlabeled datasets and automating the extraction of distinct primitive skill priors 3. Impact of Hyperparameters: It would be beneficial to include an analysis of the sensitivity of ASPiRe\u2019s outcomes to its various hyperparameters particularly the weights in the AWM and the regularization terms. Limitations and Societal Impact The paper marginally touches upon the practical challenges and limitations of requiring labeled datasets to extract skill priors. An indepth discussion on strategies to overcome this limitation such as unsupervised learning techniques for skill extraction would enhance the papers value. Furthermore the potential negative societal impacts of deploying reinforcement learning models that rely on offline datasets such as propagating biases present in the training data are not addressed. Constructive suggestions include incorporating fairness criteria in the skill prior extraction process and developing mechanisms to audit and rectify biases in learned policies.", "tZUOiVGO6jN": "Review of \"JOADER: Efficient Data Preparation for Parallel DNN Training\" 1. Summary of the Paper: The paper presents JOADER a novel system designed to optimize the data loading process for parallel deep neural network (DNN) training sessions that utilize overlapping datasets. The authors address the inefficiencies found in current data preparation practices which often lead to redundant IO and CPU usage due to independent data loading and preprocessing across parallel jobs. By introducing a dependent sampling algorithm (DSA) alongside a domainspecific cache policy and underpinning these with a novel tree data structure JOADER achieves improved training speeds (up to 200 for ResNet18) without compromising accuracy. The system accommodates flexible training scenarios\u2014varied job start times different dataset overlaps and disparate training speeds\u2014overcoming limitations of prior systems that require synchronous job starts and uniform dataset usage. 2. Assessment: Strengths:  The proposed DSA effectively increases sampling locality while preserving the randomness needed for unbiased training a notable improvement over existing techniques.  JOADER intelligently addresses the practical constraint of variable dataset overlaps start times and training speeds among parallel jobs delivering a flexible solution applicable to a wider array of deep learning tasks.  The novel tree data structure not only efficiently implements DSA but also significantly reduces the time complexity of data loading demonstrating solid algorithmic innovation.  Comprehensive experimental evaluation provides convincing evidence that JOADER achieves significant performance improvements in both synchronous and asynchronous training scenarios. Weaknesses:  The paper does not thoroughly discuss the scalability of JOADER especially when dealing with extremely large numbers of parallel jobs or datasets significantly larger than those tested.  While the paper briefly mentions the potential negative impact of its approach on tasks with highly correlated workloads the discussion lacks depth and does not provide clear guidelines or solutions for those scenarios.  The comparison with existing methods like CoorDL is valuable yet the paper might benefit from a broader comparison including more diverse approaches to data loading and caching in deep learning contexts. 3. Questions and Suggestions for the Authors:  Can the authors elaborate on the scalability limits of JOADER particularly in contexts involving largerscale datasets or significantly more parallel jobs than those tested  Could further insights be provided into the impact of JOADER on tasks with correlated workloads Are there modifications or complementary strategies that could mitigate the reduced performance in these cases  It would be helpful if the authors could discuss the potential integration of JOADER with emerging dataintensive deep learning paradigms such as federated learning or continual learning frameworks.  A more detailed discussion on the potential for tuning the domainspecific cache policy based on specific characteristics of the datasets or training jobs could add valuable depth to the paper. 4. Limitations and Societal Impact: The paper acknowledges limitations regarding JOADERs performance with correlated workloads but does not thoroughly explore this aspect. Given the increasing complexity and diversity of machine learning tasks understanding these limitations is crucial for practitioners. Additionally the discussion on potential negative societal impacts such as increased energy consumption due to potential inefficiencies in less optimal scenarios is notably absent. As a suggestion the authors could enhance the robustness of their work by providing a deeper analysis of these limitations and proposing strategies for mitigation. Identifying scenarios where JOADER may lead to inferior outcomes and suggesting alternative approaches would significantly strengthen the paper.", "wN1CBFFx7JF": " Peer Review of \"Consistency of Prediction Error for Deep Neural Networks on Time Series Data\"  Summary of the Paper This paper investigates the theoretical properties of deep feedforward neural networks equipped with ReLU activation functions for modeling time series data. It fills a gap in the current literature where most theoretical developments assume independent observations. The authors provide nonasymptotic bounds for prediction errors accommodating temporally dependent observations through mixingtype conditions. Their theoretical results are expanded upon with numerical simulations and an application to a macroeconomic data set. Through rigorous analysis the paper contributes to understanding how neural networks can be effective in forecasting tasks across various domains despite the temporal dependencies characteristic of time series data.  Assessment of Strengths and Weaknesses Strengths: 1. Theoretical Contribution: Establishing nonasymptotic bounds for prediction errors in the context of time series data is significant. These results extend the understanding of neural network performance to scenarios with temporal dependence. 2. Practical Implications: By demonstrating the theory through simulations and a realworld application the paper showcases the practical utility of the theoretical developments enhancing the relevance of the findings. 3. Breadth of Applicability: The application of the theory across a diverse set of fields including finance biotechnology and macroeconomics underpins the versatility of the proposed approach. Weaknesses: 1. Restrictive Assumptions: The reliance on mixingtype conditions and assuming boundedness for certain parameters may limit the generalizability of the results. These conditions might not hold in all practical time series scenarios. 2. Focus on ReLU Activation Function: Although ReLU is popular the theoretical analysis is constrained to networks using this specific activation function. Expanding the scope to include other activation functions could make the findings more robust and widely applicable. 3. Complexity Analysis: The paper lacks a detailed discussion on the computational complexity of implementing the proposed neural network models especially in highdimensional settings where performance might be a critical concern.  Questions and Suggestions for Authors 1. Generalization to Other Activation Functions: Can the theoretical results be generalized to networks with other types of activation functions such as sigmoid or tanh This would broaden the applicability of the findings. 2. Mitigating Restrictive Assumptions: Are there potential methodological adaptations that could relax the mixingtype conditions or the boundedness assumptions while maintaining prediction error consistency 3. Complexity and Scalability: Could the authors provide insights or empirical evidence on the computational complexity and scalability of the proposed approach particularly for highdimensional time series data 4. Extensions to NonStationary Series: Most realworld time series data exhibit nonstationarity. How might the proposed model or theoretical analysis be adapted to account for such characteristics  Limitations and Potential Negative Societal Impact The authors have primarily focused on establishing theoretical properties under specific conditions without thoroughly examining the limitations or potential negative societal impacts of their work. Given the extensive application of neural networks in critical domains any misestimation or model failure could have farreaching consequences. To improve the authors could discuss: 1. The implications of model failure especially in sensitive fields like health sciences and finance where inaccurate forecasts could lead to significant negative outcomes. 2. Strategies for mitigating the risks associated with the application of their model particularly in scenarios where the assumptions might not hold. 3. The potential for data privacy concerns when applying such models to personal or sensitive data offering suggestions for ensuring data protection. In sum while the paper makes valuable theoretical contributions to the field of deep learning for time series analysis addressing these additional considerations could greatly enhance its practical significance and societal relevance.", "m67FNFdgLO9": " Peer Review of the Paper: \"Dense Interspecies Face Embedding (DIFE)\"  1) Summary of the Paper The paper introduces an innovative approach named Dense Interspecies Face Embedding (DIFE) aimed at understanding and extracting common features among faces of various animal species including humans. Utilizing multiteacher knowledge distillation from Continuous Surface Embedding (CSE) and StyleGAN2 the researchers present a method that compensates for the scarcity of annotated animal face data and addresses the challenges of ambiguous interspecies connections and significant shape and style variances among different species faces. Leveraging synthesized pseudopair images and introducing a semantic matching loss the paper achieves substantial advances over previous methods on tasks such as interspecies facial keypoint transfer face image manipulation and dense keypoint transfer. The innovations and applications detailed in this work represent a significant contribution to the field of computer vision particularly in the understanding and manipulation of both human and animal face images.  2) Thorough Assessment of Strengths and Weaknesses Strengths:  Innovation: The approach of utilizing multiteacher knowledge distillation from StyleGAN2 and CSE for the task of interspecies face embedding is novel and exhibits high potential for various applications including ecological research and media production.  Methodological Rigor: The methodologies for overcoming the lack of animal face data ambiguous interspecies connections and shapestyle variances are well thought out and robustly implemented. The use of pseudopair images synthesis and semantic matching loss demonstrates a deep understanding of the problem and an effective strategy for addressing it.  Quantitative Evaluation: The comprehensive evaluation against several datasets and benchmarks comparison with baselines and various applications showcase the methods effectiveness and superiority over existing methods. Weaknesses:  Generalization and Scalability: While the paper demonstrates impressive results within controlled experimental settings it mentions challenges related to scaling up to multiple animal species beyond the three domains focused on in the study. The generalization to outofdomain data and species with significantly different characteristics remains a concern.  Dependence on Pretrained Models: The efficacy of DIFE heavily relies on the performance and quality of pretrained CSE and StyleGAN2 models. This dependency could pose limitations in scenarios where these models underperform or are not readily available for certain species.  Addressing Failure Cases: The paper briefly mentions typical failure cases such as occlusion or dark illumination but does not provide a comprehensive strategy for overcoming these issues. This aspect could significantly impact the utility and applicability of DIFE in realworld conditions.  3) Questions and Suggestions for the Authors  Generalization to Unknown Species: What strategies do you propose for extending DIFEs applicability to unknown animal species or significantly outofdomain data where pretrained StyleGAN2 models are not available  Handling Failure Cases: Could you elaborate on potential methodologies or enhancements to DIFE that could address failure cases particularly in scenarios of face occlusion and varying illumination conditions  Societal Impact and Ethical Considerations: While touching upon potential negative societal impacts briefly a more thorough discussion on the ethical use of this technology particularly in the context of privacy concerns and the potential for misuse in creating misleading or deceptive media would be beneficial.  4) Limitations and Suggestions for Improvement The authors have made an effort to address limitations related to the scope of animal species covered and the dependency on pretrained models. However the paper would benefit from a more extensive discussion on the potential societal impacts of the technology considering both positive applications and the risks of misuse. Suggestions for future work could include exploring unsupervised or semisupervised learning techniques to reduce dependency on pretrained models and investigating more robust methodologies to handle common failure cases such as occlusion. Additionally developing guidelines and ethical frameworks for the application of such technologies could address potential concerns regarding privacy and deception. Overall the paper makes a significant contribution to the field of computer vision and sets a strong foundation for future research in interspecies face understanding.", "m97Cdr9IOZJ": "Summarization of the Paper: The paper discusses the advancement of Invertible Neural Networks (INNs) through the development and analysis of Coupling Flows (CFlows) and their parametric variant ParaCFlows. These models are proven to universally approximate any diffeomorphism which is a differentiable and invertible function under the condition of adding certain singlecoordinate transforms. This theoretical breakthrough is validated in the context of contextual Bayesian optimization showcasing ParaCFlows superior performance over existing neural surrogates particularly in handling optimization tasks with high efficiency and accurate gradient approximations. The experiments demonstrate the practical implications of these findings in enhancing the performance of machine learning models in various optimization scenarios. Assessment of Strengths and Weaknesses: Strengths: 1. Theoretical Contribution: The paper significantly advances the theoretical understanding of the universality of CFlows and ParaCFlows for approximating diffeomorphisms and parametric diffeomorphisms respectively. It does so with rigorous proofs and establishes conditions under which these approximations hold contributing substantially to the field of generative modeling and optimization. 2. Practical Implications: By applying ParaCFlows to contextual Bayesian optimization tasks the authors successfully demonstrate the models practical utility and superiority over other surrogates. This includes more efficient optimization and accurate gradientbased searches offering significant advantages for realworld applications. 3. Open Source Code: Providing access to the implementation code fosters reproducibility and allows for broader experimentation and validation by the research community enhancing the potential impact of this work. Weaknesses: 1. Dimension Augmentation SideEffects: The paper acknowledges that dimension augmentation a key feature for achieving universality complicates the utilization of ParaCFlows in generative modeling tasks due to the challenge in fast logdeterminant estimation. This limitation may restrict the models applicability or efficiency in some scenarios. 2. Model Complexity and Training: While the paper shows the effective application of ParaCFlows it does not deeply discuss the computational costs or the intricacies involved in training such models. Considering the complex architecture an analysis on training difficulty resource consumption and potential optimization strategies would have been beneficial. 3. Lack of Comparative Analysis: Despite demonstrating the superiority of ParaCFlows in certain tasks the paper lacks a comprehensive comparative analysis with stateoftheart methods across a broader range of tasks. Such analysis could help in better understanding the competitive advantages or possible shortcomings of the proposed models. Questions and Suggestions for the Authors: 1. Dimension Augmentation TradeOff: Can the authors elaborate on potential strategies to mitigate the negative impact of dimension augmentation on the logdeterminant estimation in generative tasks Are there alternative approaches to achieve universality without resorting to dimension expansion 2. Training Complexity: It would be valuable to gain insights into the training dynamics of ParaCFlows. How does the training complexity compare to existing models and are there specific challenges or strategies that have proven effective in training such models 3. Extended Comparative Analysis: Could the authors extend their experimental evaluation to include a broader comparison with other stateoftheart models in generative tasks and beyond optimization This could help highlight the unique strengths or reveal additional contexts where ParaCFlows excel. Limitations and Improvements: While the paper makes substantial theoretical and practical contributions addressing its dimensionality augmentation drawback is crucial. Exploring alternative methods to achieve universality without sacrificing efficiency in other tasks would be an important step forward. Additionally a more comprehensive analysis of the models training complexity and broader comparative studies could further solidify the position of ParaCFlows within the machine learning and optimization landscape. These improvements could enhance the papers contribution and broaden the applicability of the proposed models.", "mhe2C2VWwCW": "Review for \"Predictive Querying in Neural Autoregressive Sequence Models\"  1. Summary This paper introduces a framework for efficiently answering a multitude of predictive queries in the context of autoregressive neural models like recurrent neural networks (RNNs) and transformers. Queries of interest include conditional probabilities about future events in a sequence given its history such as the likelihood of a certain event occurring before another or within a set number of steps. The key contributions are: the systematic representation of complex predictive queries through elementary building blocks the development of new estimation methods (beam search importance sampling and a hybrid approach) and the evaluation of these methods across four different datasets showing improved efficiency and accuracy over naive forward simulation.  2. Assessment  Strengths:  Innovation: The proposed framework represents a significant step forward in querying neural sequence models. The methodologies for reducing complex queries to simpler manageable elements and the corresponding query estimation methods are both novel and impactful.  Applicability: The approach is tested extensively across different domains demonstrating its general applicability. This versatility opens up new possibilities for realtime predictive querying in a variety of fields from natural language processing to genomics and beyond.  Performance: The experimental results convincingly show that the proposed methods outperform the naive baseline with the hybrid approach in particular offering significant improvements in efficiency and accuracy. This highlights the practical value of the proposed approach.  Code Availability: By providing the code the authors ensure reproducibility and facilitate further research in this area.  Weaknesses:  Dataset Limitations: While the selected datasets cover a breadth of applications they are relatively small in scale and scope. The impact of the proposed methods on largescale realworld applications remains to be fully validated.  Complexity of Implementation: The complexity of implementing the proposed methods especially the hybrid approach could be a barrier for some practitioners. The paper could benefit from providing more detailed guidance or considerations for those looking to apply these methods in different contexts.  Limited Discussion on Model Calibration: There is a brief mention of the model accuracy not being the focus but the implications of model calibration and accuracy on the proposed querying method are not deeply explored. Given the reliance on model predictions this could be a crucial aspect affecting the utility of the proposed methods.  3. Questions and Suggestions for the Authors:  Can the authors elaborate on the potential for scaling the proposed methods to larger and more complex sequences possibly involving continuous time models  It would be beneficial if the authors could provide more insights into the impact of model miscalibration on querying accuracy. Are there any recommended strategies to mitigate this  Concerning the hybrid method could the authors discuss the computational tradeoffs involved especially in comparison to solely using beam search or importance sampling This could help practitioners in making implementation decisions.  The paper would benefit from a discussion on the possibility of integrating this querying framework with online learning models where the sequence model updates over time. How would this impact the querying frameworks efficiency and accuracy  4. Limitations and Societal Impact The authors have partially addressed the limitations by acknowledging the constrained scope of the datasets and the focus on modelbased estimates rather than the datagenerating process. However the impact of model inaccuracy especially miscalibration on querying results needs more attention. Additionally the potential computational complexity of implementing the proposed methods particularly in realtime applications could be a limiting factor not fully discussed. The potential negative societal impacts highlighted by the authors are valid and appreciated. It is crucial to consider how predictive querying could be misused or lead to biased decisions affecting individuals especially in sensitive applications like education or healthcare. Constructively the authors could further discuss measures to mitigate such risks such as fairnessaware modeling techniques or transparency in model predictions to users affected by predictive decisions. Overall this paper presents an important advancement in the methodology for predictive querying in neural sequence models with promising applications and a solid experimental evaluation. Further exploration and discussion on the highlighted points could enhance its contributions and applicability.", "rO6UExXrFzz": "Review of the Paper on Controlling the Capacity of Composed Function Classes with Gaussian Noise 1. Summary The paper investigates the problem of controlling the capacity of composed function classes which is crucial for designing complex learning algorithms. It focuses on the composition of two classes of functions F and H and examines how the capacity of the composed class H\\xe2\\x88\\x98F can be controlled. The authors find that the capacity measured by uniform covering numbers becomes prohibitively large or even unbounded especially when dealing with infinite domain sets. To address this issue the paper proposes the novel approach of adding a small amount of Gaussian noise to the output of F before composition. This method effectively controls the capacity of H\\xe2\\x88\\x98F. To support their approach the authors define new notions of uniform covering numbers for random functions with respect to total variation and Wasserstein distances and demonstrate their results through experiments on the MNIST dataset using multilayer sigmoid neural networks. 2. Assessment Strengths:  Novelty: The approach of adding Gaussian noise to control the capacity of composed function classes is innovative and addresses a significant problem in the design of complex learning functions.  Rigor: The paper provides a strong theoretical foundation for their method defining new metrics and leveraging them to prove the effectiveness of their approach.  Empirical Validation: Preliminary results on the MNIST dataset validate the theoretical findings demonstrating the practical applicability of the proposed method. Weaknesses:  Limited Empirical Evaluation: While preliminary results are encouraging a broader set of experiments covering more datasets and network architectures would strengthen the papers claims.  Potential for Broader Impact Discussion: The paper could benefit from a more detailed discussion on potential applications beyond multilayer sigmoid neural networks and the implications for various domains.  Clarity on Limitations: The identification and discussion of the methods limitations especially in contexts other than the ones considered (e.g. ReLU networks) could be more thorough. 3. Questions and Suggestions for the Authors  Broader Applications: Could the authors elaborate on potential applications of their method in domains outside of the ones examined such as convolutional neural networks or recurrent neural networks  Impact of Noise Levels: How do varying levels of Gaussian noise impact the performance and capacity control especially in more complex or deeper networks Further experiments in this direction could provide valuable insights.  Handling of Limitations: Are there specific strategies the authors would suggest to mitigate the limitations identified such as extending the approach to ReLU networks Further elaboration on potential future research directions to address these limitations would be beneficial. 4. Limitations and Potential Negative Societal Impact The discussion on limitations primarily focuses on the bounded nature of the activation functions considered and acknowledges the need for further research to extend the results to other types of networks (e.g. ReLU). However the paper could further discuss the potential computational implications of adding noise especially in largescale applications and any negative impacts this might have on resource efficiency. Additionally a discussion on the potential negative societal impacts of the methodology though seemingly minimal would make the exploration of ethical considerations more comprehensive. Constructive Suggestions for Improvement:  Expanding the empirical validation section with more datasets and varied architectures would significantly bolster the papers claims.  A section discussing the broader implications of the method on various fields and potential negative societal impacts would provide a more wellrounded view of the research.  More explicit strategies for addressing the identified limitations and potential directions for future research would guide ongoing and future work in this area. Overall the paper presents a promising approach to a challenging problem in machine learning backed by rigorous theoretical analysis and preliminary empirical evidence. Expanding the empirical validation and further exploring the methods implications and limitations would make this work even more impactful.", "zGPeowwxWb": " Peer Review  1) Briefly Summarize the Paper and Its Contributions The paper introduces a novel approach to generative modeling by combining denoising diffusion implicit models (DDIMs) with deep equilibrium (DEQ) models. This fusion aims to address the limitations of traditional diffusion models which require long sampling chains by proposing a model that can execute these sampling steps in parallel thereby reducing the time needed for highfidelity image generation and model inversion tasks. The authors term this new model formulation as DEQDDIM. The paper demonstrates the effectiveness of DEQDDIM through experiments on CIFAR10 CelebA and LSUN datasets showing that it can achieve comparable or better image quality with significantly less sampling time and faster model inversion compared to sequential sampling processes of traditional DDIM.  2) Thorough Assessment of the Strengths and Weaknesses Strengths:  Innovative Approach: The paper presents a creative solution to a wellknown limitation of diffusion models by leveraging the DEQ framework showcasing a significant improvement in efficiency.  Empirical Validation: The authors provide extensive experimental results that validate the efficacy of their proposed method demonstrating its advantages over traditional diffusion models in both single image generation and model inversion contexts.  Generalization: They explore how their approach can be extended to a broader family of diffusion models including stochastic variants showcasing the methods versatility. Weaknesses:  Computational Complexity: While the approach allows for parallel execution the increased computational requirements per step due to the simultaneous solving of equilibrium points for all timesteps might limit the applicability in resourceconstrained environments.  Model Stability: The paper briefly touches upon the potential instability issues during training due to the illconditioning of Jacobian in DEQ models but does not deeply investigate or propose specific solutions to this challenge.  Societal Impact Discussion Lacking: Although the paper mentions potential negative impacts in a generic manner a more detailed discussion on specific adverse effects especially regarding ethical considerations in image generation and manipulation would have been beneficial.  3) Questions and Suggestions for the Authors  Question: Could the authors elaborate on how the DEQDDIM model behaves in terms of interpretability of the latent space compared to traditional DDIMs  Suggestion: It may be useful to conduct a deeper analysis of the stability issues mentioned and explore techniques to mitigate these during training potentially enhancing model robustness.  Suggestion: Considering the advancements made in this paper a comparative study on the energy consumption of DEQDDIM against conventional models could offer insights into its environmental impact.  Limitations and Societal Impact The authors have pointed out the computational intensity of their model as a limitation and briefly mentioned the potential for misuse in image manipulation. However a more comprehensive discussion on how to mitigate these negative impacts would enrich the paper. For example proposing guidelines for ethical use and exploring mechanisms to trace generated images back to the model could help address some societal concerns. Moreover an assessment of energy consumption and carbon footprint given the increased computational requirements could provide a clearer picture of the environmental impact.", "z64kN1h1-rR": " Peer Review for \"Leveraging Deep Ensembles for Pessimistic QValue Estimation in Offline RL\"  1. Summary of the Paper The paper presents a comprehensive study on the utilization of ensembles of Qfunctions as a mechanism for generating pessimistic value estimates in offline reinforcement learning (RL). This study critiques the conventional approach of using shared pessimistic targets across ensemble members which the authors argue paradoxically leads to optimistic value estimates. To address this issue the paper introduces the Model Standarddeviation Gradients (MSG) algorithm which capitalizes on independently training Qfunction ensembles without shared targets. Through extensive theoretical analysis and experimentation on D4RL and RL Unplugged benchmarks the authors demonstrate that MSG significantly outperforms stateoftheart methods in challenging domains reaffirms the necessity of using independently trained Qfunctions and investigates the efficacy of efficient ensemble approximations developed for supervised learning within the realm of offline RL.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty and Theoretical Contribution: The identification of a critical flaw in the use of shared pessimistic targets in ensemblebased algorithms is compelling. The theoretical analyses providing evidence of this flaw are thorough and wellarticulated.  Practical Relevance: The MSG algorithm addresses a real and significant issue in offline RL offering a practical solution that has shown to yield superior results in benchmark tests.  Empirical Validation: The extensive experiments provide strong empirical evidence supporting the superiority of MSG over existing approaches. The use of both D4RL and RL Unplugged benchmarks enhances the generalizability of the findings. Weaknesses:  Computational Costs: Despite the papers attempt to address the computational costs associated with using separate networks for each ensemble member this remains a significant challenge especially for complex models or largerscale problems.  Limited Discussion on Efficient Ensemble Approximations: While the paper explores the potential of using efficient ensemble approximations the discussion feels somewhat cursory and the comparative analysis could benefit from a more detailed exploration of why these approximations underperform in offline RL settings.  Potential Overfitting Risks: There is a lack of discussion about the potential for overfitting with the MSG algorithm especially given its reliance on deep ensembles and the tradeoff between trust and pessimism.  3. Questions and Suggestions for the Authors 1. On Generalization to Online RL: Given the focus on offline RL how applicable is the MSG algorithm to online RL settings Experimentation or discussion on this aspect could broaden the impact of the research. 2. Impact of Ensemble Size: It would be beneficial to explore the relationship between ensemble size and the performance of MSG in more depth. Specifically how does increasing the ensemble size affect computational costs and is there an optimal size that balances performance with efficiency 3. Alternative Solutions to Computational Cost: Could the authors elaborate on potential solutions to the identified computational bottlenecks perhaps exploring more innovative ways to approximate ensembles without significant loss in performance 4. Consideration of Overfitting: How do the authors propose to mitigate the risk of overfitting with MSG given its dependence on deep ensembles Including regularization techniques or other mitigation strategies in the discussion could enhance the robustness of the proposed approach.  4. Limitations and Societal Impact The paper does acknowledge the computational costs associated with using separate networks for each ensemble member but does not thoroughly discuss potential negative societal impacts. Given the broader applicability of RL an exploration of how MSGs performance and computational efficiency might translate into different realworld scenarios would be invaluable. Potential negative impacts such as increased energy consumption or biases in decisionmaking processes stemming from the reliance on offline datasets should be addressed. Suggestions for mitigating such impacts could include developing more efficient ensemble methods tailored for RL or incorporating fairness and sustainability considerations into the algorithms design and application. In conclusion the paper makes a significant contribution to the field of offline RL by addressing a hitherto overlooked issue with shared pessimistic targets in ensemblebased algorithms. While there are areas that require further exploration particularly regarding computational costs and the less effective performance of efficient ensemble approximations the introduction of the MSG algorithm represents a meaningful step forward. Future work expanding on these findings and addressing the mentioned limitations could have a profound impact on the efficiency and applicability of uncertainty estimation methods in RL.", "r__gfIasEdN": " Review of \"Overcoming Distribution Shift in Paraphrase Identification with Generalized Autoregressive ParaphraseIdentification (GAP)\"  1. Summary The paper targets the challenge of distribution shift in paraphrase identification within Natural Language Processing (NLP). The authors highlight that negative example pairs often introduce biases affecting the models generalization capability when facing outofdistribution test pairs. To mitigate this they propose training separate models for positive and negative pairs and developing a perplexitybased metric to determine the weight of the negative model during inference. Their approach termed Generalized Autoregressive ParaphraseIdentification (GAP) alongside an extension GAPX that includes an extra discriminative model shows significant improvement in outofdistribution performance over current stateoftheart transformers.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of separate modeling for positive and negative pairs along with the perplexitybased metric for adjusting inference reliance represents a novel approach to handling bias in paraphrase identification tasks.  Empirical Results: The authors present strong empirical evidence that their method outperforms existing approaches with notable improvements in macro F1 and accuracy across various outofdistribution scenarios.  Theoretical Rationale: The Bayesian approach to model the distribution of sentences and adjust their dependence on positive or negative pairs is welljustified and theoretically sound.  Practicality: Making the code publicly available enhances the papers contribution by allowing others to reproduce and build upon the proposed method. Weaknesses:  Model Complexity: The introduction of separate models for positive and negative pairs alongside an additional discriminative model increases the model complexity which may lead to computational inefficiencies especially in terms of inference time.  Generalizability Concerns: While the proposed method shows improvement in outofdistribution scenarios it is unclear how the method would perform on paraphrase identification tasks across languages or more diverse datasets beyond those evaluated.  3. Questions and Suggestions for the Authors  Handling of Highly Imbalanced Data: Could the authors elaborate on how their method performs in scenarios where there is a significant imbalance between positive and negative pairs in the dataset Enhancing the method to automatically adjust for such imbalances could improve its applicability.  Comparison with Other Debiasing Techniques: Would incorporating contemporary debiasing techniques (e.g. adversarial training) into either the GAP or GAPX framework result in further performance improvements A comparative analysis could be beneficial.  Impact of Model Complexity: It would be useful for the authors to discuss the tradeoffs between model complexity and performance improvements in more detail possibly including computational cost analyses.  Extension to Multilingual and Diverse Datasets: Encouraging the authors to test their methodology on multilingual datasets or various domains could highlight the adaptability and robustness of their approach to different contexts.  4. Limitations and Societal Impact The authors have not thoroughly discussed the limitations related to the computational efficiency and potential negative societal impacts of their work. While the approach demonstrates improved performance the increased model complexity and computational requirements might limit its use in resourceconstrained environments. Additionally the paper does not address potential biases that may arise from the datasets themselves which could perpetuate or amplify societal biases. Suggestions for Improvement:  Computational Efficiency: Offering guidelines on optimizing computational resources or proposing a lighter version of the model could widen its application.  Addressing Dataset Biases: Incorporating mechanisms to identify and mitigate biases inherent in the training datasets would be valuable. This could involve techniques for dataset auditing and bias correction during the model training phase. In summary this paper presents an innovative method for improving outofdistribution generalization in paraphrase identification tasks. While its strengths are notable addressing its computational complexity and extending its evaluation to more varied contexts could enhance its contribution further.", "vkhYWVtfcSQ": "Review of \u201cEnergybased Surprise Minimization in MultiAgent Reinforcement Learning\u201d 1) Summary of the Paper This paper presents a novel approach to MultiAgent Reinforcement Learning (MARL) focusing on the minimization of surprise  a measure of uncertainty faced by agents due to sudden environmental changes or fastpaced dynamics. The authors propose utilizing an EnergyBased Model (EBM) to estimate and minimize surprise across agents in a multiagent system thus ensuring better collaboration and convergence towards less surprising and more predictable states. The core of their method involves a temporal EBM theoretically aligned with minimizing conjugate entropy which not only offers a perspective of convergence but also provides a practical way to control for surprise in complex agent interactions. The paper validates its theoretical insights through empirical studies on multiagent tasks demanding significant collaboration amidst dynamic environments showcasing the effectiveness of the proposed EMIX (Energybased MIXer) framework. 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Conceptual Approach: The integration of surprise minimization with energybased models in MARL is both novel and insightful. By focusing on the freeenergy principle for convergence the paper carves out new theoretical and practical paths for handling fastpaced dynamics in multiagent settings.  Theoretical Grounding: The paper provides a solid theoretical foundation for the EMIX framework including clear assumptions detailed derivations and theorems that validate the proposed approachs efficacy.  Comprehensive Empirical Validation: The authors undertake extensive empirical evaluation particularly using StarCraft II micromanagement scenarios which are known for their complexity and the need for nuanced coordination and strategy among agents. The comparison with existing methods (e.g. QMIX VDN COMA IQL SMiRLQMIX) and the resulting improvements in success rates strongly support the effectiveness of the EMIX framework. Weaknesses:  Limited Discussion on Limitations and Negative Impacts: While the paper extensively discusses the strengths and implications of introducing an energybased surprise minimization approach it falls short in thoroughly addressing the potential limitations or negative societal impacts of their work.  Scalability Concerns: While the authors hint at scalability challenges particularly in highdimensional action spaces there is insufficient detail on how the proposed EMIX framework would scale to very large multiagent systems or extremely complex environments beyond those tested.  Generalization to Other Domains: The focus on StarCraft II micromanagement scenarios while valuable leaves open questions regarding how well the proposed approach generalizes to other domains or realworld multiagent problems that may not align as closely with the studys environments. 3) Questions and Suggestions for the Authors  Extension to Other Domains: Can the authors provide insights or preliminary results on applying the EMIX framework to other multiagent domains especially those with different dynamics or reward structures from StarCraft II scenarios  Addressing Scalability: It would be beneficial if the authors could discuss potential strategies or modifications to the EMIX framework that could address scalability concerns particularly in environments with a larger number of agents or higher dimensionality in action spaces.  Exploration of Negative Impacts: Could the authors elaborate on any considerations given to the potential negative societal impacts of their work especially in applications where surprise minimization might lead to unintended consequences 4) Limitations and Suggestions for Improvement While the paper makes significant contributions to the field of MARL its discussion on limitations and potential negative societal impacts is notably scant. The authors should consider providing a more detailed examination of potential downsides ethical considerations and unintended consequences of implementing their proposed method especially in sensitive or critical applications. Moreover addressing the scalability and generalization concerns with concrete future work directions or potential solutions would strengthen the papers impact and applicability to a broader range of multiagent systems and environments.", "xILbvAsHEV": " Review of \"Efficient \\xce\\xa6Hedge Algorithm for Minimizing ExtensiveForm Trigger Regret\"  Summary This paper addresses the computational challenges associated with translating solutions from NormalForm Games (NFGs) to ExtensiveForm Games (EFGs) due to the exponential increase in game size. The authors propose an efficient implementation of the \\xce\\xa6Hedge algorithm designed for minimizing extensiveform trigger regret in EFGs under both full and bandit feedback settings. The approach shows that \\xce\\xa6Hedge through the use of dilated regularizers is equivalent to standard Online Mirror Descent (OMD) algorithms for EFGs. This equivalence enables the design of a novel class of OMD algorithms and an improved algorithm called Balanced EFCEOMD boasting a sharper trigger regret bound under bandit feedback. The papers contributions are pivotal in computational game theory offering practical algorithms for learning EFGs with computational efficiency.  Assessment of Strengths 1. Novelty and Technical Rigor: The paper presents a commendable technical contribution by establishing the equivalence between the \\xce\\xa6Hedge algorithm and OMD algorithms with dilated regularizers which is a novel insight. The introduction of Balanced EFCEOMD and its theoretical underpinnings represent a significant advance in the field. 2. Empirical Evaluation: Although primarily theoretical the work\u2019s relevance is supported by rigorous proofs notably achieving a match with the informationtheoretic lower bound in terms of regret bounds under bandit feedback. 3. Practical Relevance: The ability to efficiently implement algorithms for EFGs has direct implications for realworld applications including cybersecurity auctions and strategic multiagent interactions making this work highly relevant.  Weaknesses 1. Lack of Empirical Validation: The paper is predominantly theoretical with a notable absence of empirical results. Including experiments to demonstrate the practical effectiveness of the proposed algorithms in real or synthetic EFG scenarios would strengthen the paper. 2. Accessibility and Readability: The dense mathematical exposition might be challenging for readers not deeply versed in theoretical aspects of game theory and machine learning. Summarizing the key theoretical results more accessibly could broaden the papers appeal. 3. Comparison with StateoftheArt: While the paper benchmarks the theoretical improvements of Balanced EFCEOMD over existing approaches a direct empirical comparison to stateoftheart algorithms under various settings would further validate its advantages.  Questions and Suggestions 1. Empirical Evaluation: Can the authors provide empirical evaluations comparing the performance of Balanced EFCEOMD with existing algorithms in realworld EFGs 2. Extension to Other Equilibria: It would be interesting to see if the techniques introduced could be extended to compute other types of equilibria in EFGs or applied to different game settings (e.g. stochastic games). 3. Implementation Details: More insights into the practical implementation challenges of these algorithms if any and how they compare in terms of computational resources to existing solutions would be valuable.  Limitations and Societal Impact The authors have not explicitly discussed the limitations and potential negative societal impacts of their work. While the theoretical nature of this study might inherently limit direct negative consequences considering and discussing potential misuses (e.g. in manipulating markets or exploiting systems through strategic gameplaying) is crucial. A section outlining potential ethical considerations misuse cases and recommendations for mitigating such risks would enhance the paper.  Suggestions for Improvement  Incorporating empirical experiments to strengthen the papers findings and its practical relevance.  Expanding the discussions on the limitations and potential societal impact to address ethical considerations.  Enhancing the accessibility of the mathematical content to reach a broader audience.", "vWUmBjin_-o": "Review of \"Symmetry Regularization for Learning Equivariant Representations\" 1. Summary of Contributions: This paper presents a novel method for learning equivariant representations in deep learning models which is crucial for improving sample efficiency and generalization in AI applications. The authors propose a simple but effective approach that leverages group invariants as symmetry regularizations in the embedding space ensuring equivariance through other means and avoiding the need to know the possibly nonlinear group action in the input space. The method provides a way to enforce equivariance to a chosen transformation group by maintaining the invariance of specific group actions in the embedding space. Moreover the paper explores the use of this approach for learning disentangled representations demonstrating the effectiveness of the proposed method through qualitative and quantitative results in tasks like world modeling and reinforcement learning. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Concept: The idea of using symmetry regularization (SymReg) based on group invariants for learning equivariant representations is conceptually innovative. It addresses the limitations of previous approaches that require specialized network architectures or knowledge of linear group actions.  General Applicability: Unlike many equivariant models the proposed method does not constrain the network architecture making it broadly applicable across different domains and data types.  Empirical Validation: The extensive experiments conducted including world modeling and reinforcement learning tasks provide strong empirical evidence for the methods efficacy in learning effective representations. Weaknesses:  Theoretical Depth: While the empirical results are promising the paper could benefit from a deeper theoretical analysis of why and how the method outperforms other approaches especially in the context of nonlinear group actions.  Disentanglement Quality: The approach shows potential for learning disentangled representations but the evaluation of disentanglement could be more rigorous. Metrics for quantifying disentanglement and comparing it with stateoftheart methods could enhance the papers impact.  Broader Applicability and Limitations: While the method is shown to be effective in specific applications its effectiveness in more diverse settings or with different types of data (beyond the tested environments) is not fully explored. 3. Questions and Suggestions for Improvement:  Theoretical Insights: Can the authors provide more theoretical insights or proofs to further justify the proposed approach especially concerning the handling of nonlinear group actions  Disentanglement Evaluation: More rigorous evaluation methods for disentanglement should be considered. Can the authors compare their approach using established disentanglement metrics against known baselines  Exploration of Limitations: The paper could benefit from a deeper discussion of the methodologys limitations such as scenarios where the approach may not perform as expected. Are there any particular data types or transformations where SymReg might fail or be less effective  Potential Negative Societal Impact: While not discussed understanding the potential misuse or negative impacts of such models especially in sensitive applications is essential. The authors could reflect on this aspect and suggest mitigation strategies. 4. Addressing Limitations and Societal Impact: The paper lacks a detailed exploration of the limitations and potential negative societal impacts of the proposed method. To improve this the authors should explicitly discuss scenarios where their approach might not be suitable or where caution is warranted especially in applications with ethical or privacy concerns. Offering suggestions to mitigate these concerns would demonstrate a commitment to responsible AI development. Including a section on potential misuse and strategies to prevent it would further strengthen the paper. In summary this paper makes significant contributions to the field of equivariant representation learning by introducing a novel symmetry regularization approach. Despite its strengths considering the above suggestions could enhance the papers theoretical depth methodological rigor and consideration of broader implications and limitations.", "oiztwzmM9l": " Summary The paper proposes two novel algorithms SampleThenOptimize Batch Neural Thompson Sampling (STOBNTS) and STOBNTSLinear that address limitations in Bayesian optimization (BO) with neural networks (NNs) as surrogate models specifically their inefficiency in problems with categorical highdimensional or image inputs. The proposed algorithms avoid the computationally expensive inversion of a large parameter matrix required by existing methods and support batch evaluations while maintaining theoretical guarantees about their performance. Through both theoretical analysis and empirical experiments including applications in automated machine learning (AutoML) and reinforcement learning (RL) the authors demonstrate that their algorithms offer competitive if not superior performances compared to existing BO and neural bandit methods.  Strengths  Innovation: The introduction of STOBNTS and STOBNTSLinear represents a significant methodological advance in the domain of BO particularly in addressing the challenges posed by the high computational demand of inverting a large parameter matrix and the need for batch evaluations.  Theoretical Rigor: The paper provides a thorough theoretical analysis of the proposed algorithms deriving upper bounds on cumulative regret and establishing their asymptotic noregret properties under certain conditions.  Practical Relevance: The empirical studies showcase the algorithms\u2019 effectiveness in realworld scenarios including AutoML and RL tasks presenting a strong case for their applicability and utility in solving practical optimization problems.  Weaknesses  Limitations in Theoretical Analysis for Finitewidth NNs: While the paper offers substantial theoretical insights the analysis on finitewidth NNs is limited to STOBNTSLinear. The absence of similar analyses for STOBNTS might constrain a full understanding of its theoretical underpinnings in realworld applications where infinitewidth NN assumptions dont hold.  Potential OverSpecialization: The remarkable performance of the proposed algorithms might be attributed in part to the specific choice of surrogate models and the optimization settings considered. It would be beneficial to analyze how these algorithms perform in a broader range of applications and settings including different NN architectures or problems requiring nuanced explorationexploitation balances.  Questions and Suggestions 1. Extension to Finitewidth NNs: Could the authors elaborate on potential approaches or modifications required to extend the theoretical analysis to STOBNTS with finitewidth NNs 2. Impact of NN Architecture: The paper mentions optimal choices for NN architecture (depth L and width m) based on empirical results. It would be insightful to understand how these choices were arrived at and whether there are theoretical considerations driving these choices. 3. Comparison with Other Surrogate Models: The focus on NNs as surrogate models is justified but it would be instructive to compare or discuss the performance and practicality of using other types of surrogate models especially in contexts where NNs may not be the most efficient. 4. Societal Implications: The discussion on potential negative societal impacts is appreciated. Further elaboration on strategies to mitigate these impacts such as energyefficient computing methodologies or optimizing algorithmic efficiency would strengthen the narrative.  Limitations and Constructive Suggestions The limited theoretical analysis for finitewidth NNs is a notable shortcoming. Future works could explore extending the analysis to STOBNTS and investigating adaptive strategies for NN architecture selection based on problem characteristics. The paper only briefly touches on potential negative societal impacts related to increased electricity consumption due to deep learning methods. A more indepth exploration of environmental impacts and sustainability aspects coupled with potential mitigation strategies would not only address these concerns but also cater to the growing emphasis on green AI. In conclusion this work makes significant contributions to Bayesian optimization especially in contexts challenging for traditional GPbased methods. Addressing the outlined suggestions and questions would undoubtedly enhance its impact and applicability.", "xp5VOBxTxZ": "Review for \"Understanding the Bias of Gradient Descent on Normalized Neural Nets towards Sharpness Reduction\" 1. Summary of the Paper This paper explores the fascinating interplay between normalization layers weight decay (WD) and gradient descent (GD) dynamics in neural networks particularly focusing on how these elements encourage a bias towards minimizing the sharpness of the loss landscape. The notion of spherical sharpness is introduced to capture this phenomenon robustly given the scaleinvariance induced by normalization layers. Through rigorous mathematical analysis and empirical validation (e.g. on CIFAR10 with VGG11 and ResNet20) the authors propose a framework elucidating how under the influence of normalization and WD GD is inherently biased towards regions in the parameter space that exhibit lower spherical sharpness thereby suggesting a theoretical underpinning for the observed generalization benefits. The work extends to a broader class of optimization dynamics suggesting this phenomenon is not restricted to the specific setup traditionally considered. 2. Assessment of Strengths and Weaknesses Strengths: a. Originality: The paper presents a novel insight into the implicit bias of GD when applied to scaleinvariant neural networks enriched with normalization layers and WD. The introduction of spherical sharpness and the rigorous definition of the Edge of Stability (EoS) regime are particularly noteworthy. b. Rigor: The mathematical analysis provided is thorough combining insights from classical optimization theory with specific nuances introduced by normalization and WD in a neural network context. The reliance on welldefined mathematical constructs (such as the Polyak\\xc5\\x81ojasiewicz condition) and the Leveraging of previous works to construct a novel analytical framework are commendable. c. Empirical Validation: The empirical studies including controlled experiments and ablation studies effectively support the theoretical claims enhancing the papers credibility. Weaknesses: a. Accessibility: The dense mathematical exposition might limit the papers accessibility to a broader audience. Simplifying the presentation or providing more intuitive explanations could help bridge this gap. b. Generalization beyond the Assumptions: While the paper provides a compelling case under its defined assumptions it leaves open questions regarding the applicability of the findings beyond scaleinvariant losses or outside the domain of fullbatch GD. c. Consideration of Practical Implications: The practical implications of the sharpnessreduction bias particularly in relation to learning rates regularization strategies and network architecture choices are not deeply explored. Insights into how practitioners might leverage these findings to improve training outcomes would be valuable. 3. Questions and Suggestions for the Authors a. Clarification on the Applicability to Other Optimizers: Given the focus on GD how do the authors envision the results extending to stochastic gradient descent or more sophisticated optimizers like Adam or RMSprop b. Exploration of NonScaleInvariant Cases: Could the authors speculate on or provide preliminary insights into how their analysis might extend to cases where the loss is not strictly scaleinvariant c. Impact on Network Architecture Design: Could the findings inform specific design choices in network architecture particularly concerning normalization layer placements or alternatives to weight decay for regularization 4. Limitations and Societal Impact The paper does not fully address the limitations associated with the scale invariance requirement and its generalization to other settings (e.g. stochastic gradients realworld applications beyond CIFAR10). Future work exploring these dimensions could significantly enhance the findings applicability. Regarding societal impact while direct negative implications seem limited the paper misses an opportunity to discuss broader implications of optimization bias such as potential impacts on model interpretability or fairness. Suggestions for improvement could include a discussion section on ethical considerations and a more holistic reflection on how bias in optimization strategies might translate into biases in model predictions especially in sensitive applications. Overall this paper makes a significant theoretical contribution to our understanding of gradient descent dynamics in normalized neural networks enriched with insightful empirical studies. Addressing the outlined questions and expanding the discussion on limitations and broader implications could further strengthen the work.", "z9cpLkoSNNh": "PeerReview on \"Framework for Continual Learning through Feature Extraction with DPGrad and Obstructions for Nonlinear Features\" 1) Summary of the Paper The paper presents a novel framework for continual learning through the lens of feature extraction focusing on managing a models ability to learn from a sequence of environments without catastrophic forgetting. The authors propose distinct approaches for linear and nonlinear feature mappings. For linear feature mappings they introduce an efficient gradientbased algorithm called DPGrad which avoids catastrophic forgetting and maintains good accuracy across changing environments. For nonlinear feature mappings they demonstrate that achieving similar guarantees is fundamentally impossible detailing an impossibility result supported by a constructed instance for which no proper continual learner could succeed. 2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Foundation: The paper successfully formalizes the continual learning problem through feature extraction filling a significant gap in theoretical research within this field. The clear definitions and rigorous proofs for both linear and nonlinear settings are comprehensive and contribute substantially to the understanding of continual learning.  Practical Algorithm (DPGrad): The introduction of DPGrad for the linear case is a notable strength. By drawing parallels to existing projectionbased algorithms while offering a novel theoretical analysis it not only addresses catastrophic forgetting efficiently but also promotes memory and sample efficiency showcasing the algorithms practical viability through simulations.  Comprehensive Analysis: Theoretical results a) demonstrating the feasibility of a continual learning algorithm in the linear case and b) proving the impossibility of such an algorithm in the nonlinear case present a wellrounded perspective on the limits and potentials of continual learning strategies. Weaknesses:  Limited Scope for Nonlinear Features: While the impossibility result for nonlinear features is valuable it leaves a gap in the exploration of potential albeit imperfect approaches that might mitigate catastrophic forgetting in nonlinear scenarios. Further it primarily focuses on a theoretical standpoint without exploring practical workarounds or approximate solutions.  Lack of Extensive Empirical Validation: Though the paper presents simulations to demonstrate DPGrads effectiveness comprehensive empirical studies particularly on realworld datasets and comparing more variants of continual learning approaches would have strengthened the findings.  Potential for Broader Impact Discussion: Discussion on the societal impacts or ethical considerations of the proposed framework especially in applicationcentric domains like robotics or personal data analytics is minimal. 3) Questions and Suggestions for the Authors  Exploration of Imperfect Solutions: Given the impossibility result for nonlinear features could the authors speculate on or propose any imperfect or heuristicbased approaches that may partially overcome the challenges in nonlinear settings  Empirical Validation Suggestions: Could additional experiments especially on wellknown continual learning benchmarks (other than the briefly mentioned Rotated and Permuted MNIST in the appendix) offer a broader view of DPGrads practical performance against other stateoftheart methods  Impact Clarification: The paper could benefit from a section discussing potential negative societal impacts or ethical considerations particularly focusing on scenarios where catastrophic forgetting might have significant consequences. 4) Limitations and Suggestions for Improvement While the authors have started to address the limitations by mentioning the setups constraints for nonlinear feature mappings and the exploratory nature of the empirical studies a more detailed discussion on these aspects could enrich the paper. It would be beneficial for the authors to explicitly address:  The possible generalizations of DPGrad to nonlinear contexts and the expected challenges therein.  The limitations imposed by the assumptions (e.g. known environment identity distribution assumptions) and how they might affect the algorithms applicability in more complex realworld scenarios.  Additionally expanding the discussion to include potential negative societal impacts and exploring preventive measures or safeguards could provide a wellrounded view of the proposed continual learning frameworks broader implications.", "kjR8GiwqCK": " Peer Review of \"Indexed Minimum Empirical Divergence for Reinforcement Learning (IMEDRL) in Ergodic Markov Decision Problems\"  1) Summary of the Paper This paper introduces IMEDRL an algorithm designed for reinforcement learning (RL) within the context of discrete undiscounted infinitehorizon Markov Decision Problems (MDPs) operating under the average reward criterion. The authors focus on regret minimization. They aim to approach the problem with a strategy that minimizes regret relative to an optimal policy even when the learner lacks prior knowledge of the MDPs rewards and transitions. To achieve this they leverage and extend the Indexed Minimum Empirical Divergence (IMED) bandit algorithm. They demonstrate IMEDRLs theoretical asymptotic optimality by comparing its regret upper bound with known problemdependent lower bounds assuming ergodic MDPs. Empirical evaluations on classical tabular MDP setups further underline IMEDRLs practical competitiveness.  2) Strengths and Weaknesses Strengths:  Theoretical Contribution: The paper successfully addresses a gap in the current reinforcement learning literature by presenting an algorithm (IMEDRL) which is provably asymptotically optimal within the context it is designed for i.e. ergodic MDPs under the average reward criterion.  Practical Relevance: By providing numerical studies the authors clearly showcase the algorithms realworld applicability and efficiency compared to existing stateoftheart algorithms. This practical insight is crucial for algorithm adoption and further study.  WellFramed within Existing Literature: The paper does an excellent job situating IMEDRL amidst existing strategies highlighting its novel contributions and improvements over potential limitations of other approaches. Weaknesses:  Limited Discussion on Generalizability: While IMEDRL shows promising results for ergodic MDPs the discussion on its limitations and potential applicability or adaptability to nonergodic MDPs is somewhat sparse. Greater exploration into this area could enhance the robustness of the algorithms utility.  Potential Complexity in Implementation: Despite discussions on computational complexity theres a hint that the practicality of IMEDRL especially regarding its computational overhead (e.g. running value iteration at each step) might be a barrier against its adoption in more complex or larger statespace scenarios without significant computational resources.  3) Questions and Suggestions for the Authors  Generalizability: Can the authors discuss any preliminary insights or theoretical underpinnings that might suggest a pathway for adapting IMEDRL to work with nonergodic MDPs or extend its applicability to more complex RL scenarios such as those involving function approximation methods  Computational Efficiency: While IMEDRL shows efficiency in numerical experiments its computational overhead might be a concern for larger problems. Could there be strategies or modifications to IMEDRL that could reduce its computational demand without significantly compromising its optimality  Impact of Assumptions: The paper makes several assumptions such as lighttailed semibounded rewards and ergodicity. How sensitive is IMEDRLs performance to these assumptions and what could be the practical implications of these assumptions not holding  4) Limitations and Societal Impacts The paper acknowledges key assumptions (lighttailed semibounded rewards and ergodicity) which might limit the generality and applicability of IMEDRL across all potential MDP scenarios. A clearer articulation of the implication of these limitations in practical settings would be beneficial. While the potential for negative societal impacts appears minimal directly from the algorithms application considerations of how reinforcement learning strategies can be applied ethically in decisionmaking systems that impact society should be encouraged. Further discussion on safeguarding against misuse and ensuring fair unbiased outcomes in applications of IMEDRL could strengthen the papers contribution to responsible AI development.  Improvement Suggestions:  Extending discussions on adaptability towards nonergodic MDPs and addressing computational scalability could significantly enhance the papers contribution.  A deeper exploration of the sensitivities and implications of the stated assumptions for IMEDRLs application would provide valuable insights for practitioners considering its implementation.  Incorporating discussions on ethical considerations and the responsible application of IMEDRL would align with emerging norms for AIML research responsibly. Conclusion: This paper makes a significant theoretical and practical contribution to the field of reinforcement learning by proposing IMEDRL. Despite certain limitations and areas requiring deeper exploration the work presents a solid foundation for future research and application particularly in solving discrete undiscounted infinitehorizon MDPs within an ergodic framework.", "oWx_9VJgyV7": "Review of \"ShapeAware Neural 3D Keypoint Field (SNAKE): Joint Surface Reconstruction and 3D Keypoint Detection\" 1) Summary: This paper presents a novel unsupervised method dubbed SNAKE (shapeaware neural 3D keypoint field) for detecting 3D keypoints from point clouds through the joint task of surface reconstruction and keypoint detection. Unlike existing methods that either focus on detecting keypoints based on salient features or learn to predict transformationinvariant keypoints SNAKE simultaneously predicts implicit shape indicators and keypoint saliency by taking 3D coordinates as input. This is achieved through a network that incorporates a shape implicit decoder alongside a keypoint saliency decoder. The method is evaluated across various benchmarks including ModelNet40 KeypointNet SMPL meshes 3DMatch and Redwood datasets demonstrating stateoftheart performance in semantic consistency repeatability and geometric registration tasks. 2) Strengths and Weaknesses:  Strengths:  Innovative Approach: The paper introduces an innovative approach to 3D keypoint detection by leveraging the mutual relationship between shape reconstruction and keypoint detection which is relatively unexplored in the literature.  Unsupervised Learning: Utilizing an unsupervised learning paradigm broadens the applicability of the method to scenarios where labeled data is scarce or unavailable.  Comprehensive Evaluation: The method is thoroughly evaluated across a variety of datasets and challenges demonstrating its versatility and robustness across different scenarios.  Weaknesses:  Computational Efficiency: The optimization process for explicit keypoint extraction during inference may significantly limit the methods applicability in realtime or resourceconstrained environments.  Generalization to Complex Shapes: While the method shows impressive results on the provided datasets it is unclear how well it would perform on highly complex or irregular 3D shapes not wellrepresented in the training datasets.  Lack of Comparative Baseline: While the paper compares SNAKE against a variety of methods the inclusion of more recent or closely related stateoftheart methods in the evaluation would provide a clearer context for SNAKEs performance advantages. 3) Questions and Suggestions:  Generalization Capabilities: Can the authors elaborate on how SNAKE would perform on highly complex shapes or densely packed environments which might present significant challenges in distinguishing between keypoints and background  RealTime Application Feasibility: Given the computational intensity mentioned are there potential optimizations or simplifications that could be applied to make the method more feasible for realtime applications  Affect of Different Point Cloud Densities: How does the performance of SNAKE vary with the density of the input point clouds especially given that realworld data can often be noisy or irregular  Extension to Other Domains: Could the authors speculate on how the method might be extended or adapted for use in other domains such as medical imaging or robotics where 3D keypoint detection is crucial 4) Limitations and Societal Impact: The authors acknowledge the computational cost associated with their method primarily during the inference stage due to the need for optimization. More work is necessary to address this limitation to broaden the methods applicability especially in scenarios requiring realtime performance. Regarding societal impact while the methods potential use in autonomous robotics is highlighted there is an ethical constraint to consider especially in terms of reliability and safety. To mitigate potential negative impacts future iterations of SNAKE could focus on improving reliability in diverse and unpredictable environments reducing the likelihood of incorrect decisionmaking that could lead to harm.", "scfOjwTtZ8S": "Review of \"Exploit questionAnswering Grounding for effective Exploration in languageconditioned Reinforcement learning (EAGER)\" 1) Summary of the Paper This paper proposes a novel technique named EAGER (Exploit questionAnswering Grounding for effective Exploration in languageconditioned Reinforcement learning) to improve sample efficiency in reinforcement learning (RL) tasks that involve long horizon and sparse rewards. The main innovation is the introduction of an automated reward shaping method that leverages languages abstraction and generalization properties. By generating and answering questions based on the initial language goal EAGER guides the learning agent towards achieving auxiliary objectives thus directing its exploration efficiently. These auxiliary objectives arise from questions generated by masking words in the goal instruction with the agent rewarded based on its confidence in the answers derived from its own trajectory. This process requires minimal expert intervention as it principally utilizes preexisting datasets of demonstrated behaviors for pretraining the Question Answering (QA) module. The paper validates the EAGER framework through experiments on the BabyAI platform demonstrating its robustness and efficiency compared to stateoftheart methods like ELLA and RIDE. 2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of a QGQA framework to reinforcement learning for automated reward shaping is innovative offering a new path towards minimizing the need for expert intervention in setting auxiliary objectives.  Effectiveness: The experimental results showcase significant improvements in sample efficiency underscoring the methods potential to address the challenge of sparse rewards and long horizons in RL.  Generality: The EAGER framework is designed to be less dependent on expert knowledge making it more widely applicable across different tasks and environments than previous approaches. Weaknesses:  Dependence on Pretrained QA System: Even though EAGER aims to reduce expert involvement its performance is heavily reliant on the pretraining of the QA system which could limit its applicability in environments where such pretrained models or comparable quality data are not available.  Complexity of Implementation: The mechanism of generating and answering questions while innovative introduces additional layers of complexity to the RL setup potentially making it challenging for practical implementation and scaling.  Evaluation Scope: The evaluation is limited to the BabyAI platform which while informative may not fully demonstrate the frameworks effectiveness in more complex or realistic environments. 3) Questions and Suggestions for the Authors  Scalability: It would be beneficial to understand how EAGER scales with the complexity of tasks and environments. Can the authors provide insights or additional experiments on more complex scenarios beyond BabyAI  Comparison with Other Methods: The paper makes a significant comparison with ELLA and RIDE. How does EAGER perform relative to other intrinsic motivationbased approaches not mentioned in the paper especially in nonlanguage conditioned settings  Impact of Question Complexity: How does the complexity of generated questions affect the learning process Are there diminishing returns for more complex questions or is there an optimal level of complexity that aids learning efficiency 4) Limitations and Suggestions The paper acknowledged its limitations particularly the reliance on a pretrained QA system and the evaluation scope limited to the BabyAI platform. To address these points future work could explore strategies for reducing dependence on preexisting datasets by investigating online learning or reinforcement learning techniques for the QA module. Furthermore testing the EAGER framework in more diverse and complex environments would significantly strengthen the case for its applicability and robustness. Lastly considering the potential implications of automated questiongeneration mechanisms particularly in contexts where misinformation or harmful biases could be propagated including a discussion on ethical considerations and safeguards would be prudent.", "lKFOwaYNQlb": "Review of \"Distributed InfluenceAugmented Local Simulators for Scalable Reinforcement Learning in Large MultiAgent Systems\" 1) Summary: This paper extends the InfluenceBased Abstraction (IBA) framework to MultiAgent Reinforcement Learning (MARL) to address computational challenges in training large multiagent systems. The primary contribution is the introduction of distributed InfluenceAugmented Local Simulators (DIALS) that factorize large networked systems into multiple subregions allowing for parallelized training of agents. This approach is designed to mitigate the negative effects of simultaneous learning and nonstationarity through a mechanism that monitors influence distributions among different local simulators. Importantly the authors argue that frequent retraining of Approximate Influence Predictors (AIPs) is unnecessary and potentially beneficial as it can lead to a more stable learning environment. Empirical results from two domains traffic control and warehouse commissioning demonstrate the efficacy of DIALS in reducing training times and improving scalability and performance compared to training on a global simulator. 2) Assessment: Strengths:  The paper addresses a significant and challenging problem in MARL related to the scalability and efficiency of training in large complex systems.  The novel use of distributed InfluenceAugmented Local Simulators (DIALS) for parallel agent training is both innovative and practical offering clear computational advantages.  The approach is wellgrounded in the IBA framework and the extension to MARL is logically and thoroughly argued through both theoretical analysis and empirical validation.  Empirical results from two different and complex domains demonstrate the effectiveness and versatility of the proposed method showcasing considerable reductions in training time and improvements in agent performance. Weaknesses:  While the authors discuss the mitigation of nonstationarity through their approach a deeper exploration of how DIALS compares explicitly to other methods targeting nonstationarity in MARL would strengthen the paper.  The paper posits that not frequently retraining AIPs can be beneficial based on empirical observations. However a more systematic exploration or theoretical analysis of when and why this is the case would be valuable.  The requirement for highfidelity local simulators and domain knowledge to identify influence sources might limit the applicability of the approach to certain types of environments or necessitate significant preparatory work.  Memory demand as acknowledged by the authors could become a significant limitation in even larger scale applications and strategies to manage or mitigate this issue are not discussed in depth. 3) Questions and Suggestions:  Could the authors elaborate on potential strategies or methodologies to identify influence sources in less structured or understood environments where domain knowledge might not readily point to these  It would be beneficial to see a comparative analysis with other stateoftheart approaches specifically designed to handle nonstationarity in MARL. How does DIALS perform in such benchmarks  Considering the memory demand issue highlighted are there any envisioned approaches or ongoing work to optimize memory usage without compromising the performance benefits offered by DIALS  A deeper theoretical exploration of the conditions under which AIPs do not require frequent retraining would be valuable. Additionally could there be an adaptive mechanism to determine the retraining frequency based on observed system dynamics 4) Limitations and Societal Impact: The authors have made a good effort to discuss the potential limitations of their work notably the requirement for domain knowledge and highfidelity local simulators and the memory demands of the DIALS approach. However a more thorough discussion on the potential negative societal impacts especially considering the application domains (e.g. traffic control logistics) and the implications of deploying MARL strategies in realworld systems would be beneficial. It would be valuable to include considerations on the robustness and safety of the deployed policies particularly in scenarios where imperfect or biased influence predictions could lead to suboptimal or harmful decisionmaking. Suggestions for future work could include exploring these ethical and safety considerations in greater depth perhaps through the incorporation of safety constraints or ethical guidelines within the DIALS framework.", "mTra5BIUyRV": " Review of \"Designing Fair Ranking Algorithms Under Attribute Noise Models\"  1) Summary This paper tackles the fair ranking problem which seeks to order a set of items to maximize utility while adhering to certain group fairness constraints. The innovation of this work lies in addressing the challenges introduced by inaccuracies in sociallysalient (including protected) attributes which can severely compromise the fairness guarantees of conventional fairranking algorithms. The authors introduce a model considering such attributes are perturbed randomly and independently. They present a fairranking framework that relaxes the fairness constraints based on probabilistic information about these inaccuracies offering provable guarantees on both fairness and utility. This framework is adaptable to multiple overlapping groups and a broad class of fairness constraints and it is evaluated both theoretically and empirically against existing approaches.  2) Strengths and Weaknesses Strengths:  Novelty: The paper addresses a significant yet underexplored aspect of fair ranking algorithms by incorporating inaccuracies in attributes directly into the model. This adds a layer of realism and applicability.  Theoretical Depth: The authors provide rigorous theoretical analysis including provable guarantees on the framework\u2019s performance and the informationtheoretical impossibility of significantly outperforming these guarantees.  Empirical Validation: Through empirical studies on synthetic and realworld datasets the framework is shown to outperform baselines in fairness metrics without sacrificing utility lending credence to the theoretical findings.  Generalizability: The framework\u2019s applicability to multiple overlapping groups and various fairness constraints enhances its utility in diverse realworld settings. Weaknesses:  Assumptions on Errors: The model assumes errors in sociallysalient attributes are random and independent which might not always hold in practice. Correlated or systematic errors could degrade the frameworks effectiveness.  Limited Scope on Fairness Notions: While covering a wide array of fairness constraints the framework does not directly address qualitative aspects of representation which can be crucial for holistic fairness assessments.  Potential Computational Efficiency Concerns: Given the extensive theoretical guarantees and the polynomialtime algorithm for approximate solutions the practical computational efficiency especially on large datasets is not thoroughly discussed.  3) Questions and Suggestions for the Authors  Error Model Generalization: Could the authors discuss or propose extensions to the framework that might accommodate correlated or systematic errors in attributes further increasing its applicability  Qualitative Fairness Notions: Are there potential adaptations of the framework that could incorporate or address qualitative aspects of fairness such as perceptual bias or stereotype reinforcement  Computational Efficiency: Could the paper benefit from a deeper exploration of the framework\u2019s computational efficiency possibly including comparisons in computational resource usage against the baselines over large datasets  Broader Societal Impact Discussion: While the paper tangentially mentions negative societal impacts a more detailed discussion on potential misuse ethical considerations or guidelines for responsible use could enhance its contributions.  4) Limitations and Improvement Suggestions The paper makes significant contributions to the fair ranking literature especially concerning the handling of inaccuracies in sociallysalient attributes. However the assumption that errors are random and independent might limit its effectiveness in scenarios where this is not the case. Extending the model to account for nonindependent or correlated errors could be a valuable direction for future research. Furthermore while the paper focuses on theoretical development and empirical validation discussing the computational efficiency in more detail could provide a more comprehensive assessment of the framework\u2019s practical applicability. In summary the paper provides a novel and theoretically sound approach to designing fair ranking algorithms that are robust to inaccuracies in attribute data. Despite the discussed limitations its contributions to fair ranking literature are clear providing a solid foundation for future enhancements and research in this area.", "snUOkDdJypm": "Peer Review of \"LastIterate Convergence Rate in Constrained Smooth Monotone Games\" 1) Summary: This paper investigates the lastiterate convergence rate of the extragradient (EG) algorithm and the optimistic gradient (OG) algorithm within multiplayer games especially focusing on constrained smooth monotone games where players action sets are convex. Prior work primarily focused on unconstrained settings. This research introduces the notion of tangent residual as a measure of proximity to a Nash equilibrium and utilizes it to achieve O(1\\xe2\\x88\\x9aT) lastiterate convergence rates for both algorithms which align with existing lower bounds. These results represent significant progress by answering an open question in the field and broadening the understanding of lastiterate convergence in multiplayer game settings. 2) Assessment: Strengths:  Innovative Approach: The introduction of the tangent residual as a method for measuring the proximity to a Nash equilibrium is both novel and insightful. It effectively circumvents the limitations of previous studies by providing a viable metric in constrained settings a notable advancement.  Rigorous Analysis: The authors provide a comprehensive analysis of both the EG and OG algorithms lastiterate convergence rates. The use of computeraided proofs based on sumofsquares (SOS) programming adds rigor to their methodological framework.  Addressing an Open Question: By extending the analysis to constrained settings and meeting the O(1\\xe2\\x88\\x9aT) convergence rate the paper answers a significant open question in the field marking an impressive contribution to the literature on learning in games.  Clear Presentation: Despite the complexity of the content the paper is wellorganized and articulates its findings and methodology in an accessible manner. Weaknesses:  Potential for Broader Implications: While the paper effectively addresses its stated objectives it could further discuss the implications of its findings beyond the immediate scope. For instance an exploration of how tangent residuals might influence other domains or contribute to algorithmic improvements beyond those studied could enhance its impact.  Experimental Validation Lacking: The paper primarily focuses on theoretical contributions without empirical validation. Including simulations or realworld data to demonstrate the practical applicability of their findings especially the effectiveness of the tangent residual in different settings could strengthen their argument.  Assumption Clarity: While the paper makes a series of assumptions (e.g. smoothness convexity) a more detailed discussion regarding the necessity and implications of these assumptions including any potential biases or limitations they introduce could provide a more nuanced understanding of the results. 3) Questions and Suggestions for Authors: Questions:  Could you elaborate on potential strategies to empirically validate the practical applicability of the tangent residual concept in realworld game settings  What are the implications of adopting the tangent residual for other learning algorithms not covered in this study Suggestions:  Consider exploring the implications of your findings beyond multiplayer games possibly by suggesting potential applications in other domains or for other types of learning algorithms.  An extension of the work to include empirical validation or simulations demonstrating the effectiveness and applicability of the tangent residual in varied settings could further solidify your contributions. 4) Limitations and Ethical Considerations: The authors have done a commendable job in laying out the assumptions underlying their work yet a more thorough discussion on the limitations of these assumptions would be beneficial. Particularly the paper could benefit from acknowledging any implications these limitations may have on the extrapolation of their findings to broader contexts or different types of games. Regarding societal impact while the paper is theoretical in nature incorporating a discussion on how advancements in understanding Nash equilibria and convergence rates could influence decisionmaking processes in economic social or political frameworks would be insightful. Suggestions for improvement include expanding the analysis to cover potential negative impacts such as the misuse of such algorithms in manipulating market dynamics or in scenarios where strategic decisionmaking could lead to socially undesirable outcomes. In conclusion this paper marks a significant step forward in understanding lastiterate convergence in constrained smooth monotone games introducing innovative concepts and methodologies that open new avenues for future research. Further exploration of its implications both theoretically and practically could enhance its contribution to the field.", "zqQKGaNI4lp": "Summary of the Paper and Its Contributions This paper addresses a previously unexplored area within the theory of overparametrized learning specifically focusing on establishing generalization guarantees in the interpolating regime for ensemble classification methods. The authors develop an ensemble classification method that not only interpolates the training data but also achieves consistency across a broad class of data distributions particularly for data distributed on a Riemannian manifold. The key innovation introduced is the manifoldHilbert kernel which extends the Hilbert kernel to data on Riemannian manifolds. This kernel is demonstrated to support weakly consistent kernel smoothing regression and classification a significant theoretical advance. Specifically for data on the sphere the authors show that the manifoldHilbert kernel equates to a weighted random partition kernel realized through an infinite ensemble of partitionbased classifiers. This work therefore presents the first demonstration of an interpolating ensemble method maintaining consistency for a wide range of distributions in arbitrary dimensions. Assessment of the Papers Strengths and Weaknesses Strengths: 1. Novelty and Theoretical Contribution: The introduction of the manifoldHilbert kernel and its application in developing a novel ensemble classification method in the interpolating regime is a noteworthy theoretical contribution. This work fills a significant gap in the literature around ensemble methods and their generalization guarantees. 2. Broad Applicability: The methods applicability to a wide class of data distributions especially on Riemannian manifolds is a substantial strength. This broadens the potential application areas including those where data naturally belong to a manifold structure. 3. Technical Rigor: The paper is technically solid offering rigorous proofs and a detailed theoretical framework that extends existing work on kernel smoothing regression and classification to manifold settings. Weaknesses: 1. Complexity and Accessibility: The heavy reliance on advanced mathematical concepts may limit the papers accessibility to a broader audience. Simplifying the exposition or providing more intuitive explanations could help in this regard. 2. Empirical Validation: The paper primarily focuses on the theoretical development of the manifoldHilbert kernel and its properties. Incorporating empirical results demonstrating the practical effectiveness of the proposed method would significantly strengthen the paper. 3. Comparison with Existing Methods: While the paper situates its contribution within the landscape of overparametrized learning and ensemble methods a more direct comparison (theoretical or empirical) with existing methods could better highlight the advantages and potential limitations of the proposed approach. Questions and Suggestions for the Authors 1. Empirical Validation: Could the authors provide empirical evidence supporting the theoretical advantages of the proposed ensemble classification method Specifically how does it perform relative to existing methods on benchmark datasets 2. Simplification for Accessibility: Are there ways to simplify the exposition or use more intuitive explanations to make the paper more accessible to readers not deeply familiar with Riemannian geometry or the finer points of kernel methods 3. Broader Applicability: Could the authors discuss the applicability of their method beyond the specific case of the sphere What challenges or modifications would apply to other Riemannian manifolds of practical interest Limitations and Societal Impact The paper does an adequate job addressing the theoretical limitations particularly noting the lack of rate of convergence analysis for the ensemble methods and the fact that the base classifiers in the ensemble are dataindependent. However it could improve by discussing potential negative societal impacts especially in cases where the method might be applied in sensitive areas (e.g. facial recognition predictive policing). A constructively critical discussion on mitigating possible biases or misuse in such applications would be beneficial. Constructive suggestions for improvement include encouraging the authors to explore rate of convergence for their method and to consider datadependent versions of the ensemble classifiers which might be more reflective of practical use cases. Additionally a section discussing ethical considerations and potential safeguards against misuse in sensitive applications would make a timely and relevant contribution to the discussion on responsible AI research.", "ofwkaIWFqqv": "Review of the Paper: \"GRASP: GoaldRiven Actorcritic retroSynthetic Planning Framework\" 1. Summary of the paper and its contributions: The paper introduces a novel framework for retrosynthetic planning dubbed GRASP (GoaldRiven Actorcritic retroSynthetic Planning) aiming to enhance the sequential decisionmaking process in synthetic chemistry particularly drug discovery. This process involves finding synthetic pathways for a target molecule through iterative decisions on feasible reactions. Unlike existing strategies that mainly focus on predicting feasible reactions without considering the sequential nature and diversity of the planning or the specific goals of chemists such as using affordable building block materials GRASP implements a reinforcement learning (RL) based approach. It innovates by learning from both successful and failed experiences emphasizing goaldriven retrosynthesis that meets chemists specific requirements. The paper demonstrates GRASPs superiority over stateoftheart methods in terms of search efficiency and quality through experiments on the Pistachio dataset and a dataset designed by chemists. 2. Thorough assessment of strengths and weaknesses: Strengths:  Innovative Approach: GRASP\u2019s integration of goaldriven planning with an actorcritic RL framework represents a significant leap forward in the field of retrosynthetic planning addressing the limitations of existing methods around sequential decisionmaking and goal alignment with chemists needs.  Comprehensive Training Strategy: The framework benefits from a rich training regime that includes learning from both positive and negative experiences goaldriven relabeling and selfimitation learning allowing it to navigate vast reaction spaces effectively.  Performance Advantage: Demonstrated improvements in search efficiency and pathway quality backed by extensive experiments and user studies validate GRASP\u2019s practical effectiveness and its potential to significantly impact the drug discovery process.  Realworld Application: Deployment in a realworld scenario with demonstrated benefits in reducing labor costs and enhancing pathway design efficiency emphasizes GRASPs practical value. Weaknesses:  Complexity and Computational Demands: The actorcritic structure combined with MonteCarlo Tree Search might introduce significant computational complexity potentially limiting scalability or requiring substantial computational resources.  Dependence on Datasets: While the results are promising performance heavily relies on the datasets used for training. The generalization to other datasets or to scenarios with sparse or less curated data is not extensively discussed.  Limited Discussion on Failure Modes: The paper could benefit from a more thorough examination of scenarios where GRASP might underperform such as dealing with extremely rare reactions or molecules and how such challenges could be mitigated. 3. Questions and suggestions for the authors:  Generalization Ability: Can the authors provide insights on GRASP\u2019s adaptability to different chemical domains or less common molecular structures not wellrepresented in the training data  Computational Efficiency: Are there ways to optimize GRASP\u2019s computational demands without significantly compromising its performance perhaps through model pruning or more efficient search strategies  Enhancement of Training Data Diversity: Could incorporating synthetic pathways designed by chemists from diverse backgrounds improve the model\u2019s learning especially in handling rare or complex molecules  Addressing Bias: How does GRASP mitigate the risk of bias towards more frequent reactions or synthetic pathways inherent in the training data and what strategies are in place to ensure a balanced exploration of the chemical space 4. Limitations and potential negative societal impact: The authors have made an effort to address the limitations and potential negative impacts of their work especially emphasizing the need for expert oversight in novel or complex cases. However further discussion on the computational demands and the potential environmental impact of extensive training procedures would be beneficial. Suggestions for improvements include exploring more energyefficient training algorithms and incorporating mechanisms to ensure the models ethical use preventing its application in creating harmful substances. Additionally expanding on the generalizability of the model to various chemical domains and the steps taken to mitigate biases in the model would strengthen the paper.", "v_0F4IZJZw": "The paper examines the challenges of mitigating toxicity in pretrained language models (LMs) using domainadaptive training methods. It specifically focuses on three dimensions: the training corpus model size and parameter efficiency. The authors propose a novel approach SelfGeneration Enabled domainAdaptive Training (SGEAT) which utilizes selfgenerated nontoxic datasets for domain adaptation. This method surpasses existing baselines in reducing toxicity without significantly compromising model quality as evaluated by both automatic metrics and human judgments. The study offers an extensive exploration across model sizes from 126M to 530B parameters highlighting that large models require more effort to detoxify. It also compares parameterefficient training methods showing that adapter layers offer a superior balance of detoxification and model quality compared to full model adaptation and prefixtuning. Strengths: 1. Innovative Approach to Detoxification: The use of selfgenerated data for domainadaptive training is novel and shows superior data efficiency in detoxification. 2. Comprehensive Analysis: The paper thoroughly investigates detoxification across a wide range of model sizes and training strategies providing valuable insights into the scalability and effectiveness of different methods. 3. Parameter Efficiency: The exploration of adapter layers introduces a practical solution for detoxifying large language models without the need to retrain the entire model which is computationally advantageous. Weaknesses: 1. Bias and Fairness Concerns: While the paper briefly touches upon potential biases against marginalized groups the discussion is somewhat limited. It would benefit from a more detailed analysis of how detoxification methods might inadvertently amplify biases. 2. Generalization of Results: The detoxification approaches are evaluated primarily on English language datasets. The effectiveness and applicability of the proposed methods to other languages or diverse linguistic contexts remain unclear. 3. RealWorld Deployment Considerations: While the paper presents impressive results in a controlled experimental setting it lacks a discussion on the practical challenges and considerations for deploying these detoxified models in realworld applications. Questions and Suggestions for the Authors: 1. Bias Mitigation Strategies: Could the authors elaborate on potential strategies or adjustments to SGEAT that could address the amplification of bias against certain demographic groups 2. Extension to Multilingual Models: Do the authors anticipate any specific challenges or considerations when applying their domainadaptive training techniques to multilingual or nonEnglish LMs 3. Deployment Implications: It would be beneficial if the authors could discuss potential implications for realworld applications. How might the tradeoff between detoxification and model quality impact practical deployments and are there specific use cases where their approach would be particularly beneficial or problematic Limitations and Societal Impact: The paper acknowledges potential biases introduced by hate speech detectors like Perspective API which could lead to filtering out content related to minority groups. Additionally pretrained models may already contain biases which could affect selfgenerated data. While the authors recognize these issues a more comprehensive analysis and mitigation strategy would strengthen the work. In terms of societal impact theres a need for careful consideration of how these detoxified models are integrated into applications to ensure that they do not perpetuate or exacerbate existing inequalities. Suggestions for improvement include incorporating more diverse and inclusive data sources and deploying more nuanced evaluation metrics that can better capture the subtleties of language use across different communities.", "y--ZUTfbNB": " Peer Review for \"Fast Algorithms for Distance Matrices\"  1) Brief Summary of the Paper and its Contributions: This paper presents innovative approaches to computing matrixvector products for distance matrices which represent all pairwise distances between points in a dataset based on a given distance function. The authors focus on designing algorithms tailored to exploit the geometric structure inherent in distance matrices to achieve faster computation times for fundamental linear algebraic operations. Key contributions include:  The presentation of efficient algorithms for matrixvector product computations for various distance matrices significantly improving upon existing methods in terms of speed and scalability.  A detailed evaluation of upper and lower bounds for these computations with a particular focus on the `1 metric demonstrating algorithmic separations based on metric specifications.  The introduction of algorithms for generating distance matrices more swiftly than previously possible including an approximation method for the `2 distance matrix that outperforms the constraints of the JohnsonLindenstrauss lemma.  Numerous applications of these algorithmic improvements in computational linear algebra including eigenvalue computations lowrank approximations and fast matrix multiplication are also highlighted.  2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: The paper addresses a crucial computational challenge by leveraging the structure of distance matrices for algorithmic gains a step forward in algorithmic linear algebra and its applications in dataintensive domains.  Broad Impact: The proposed methods have wide applicability across various fields that rely on distance computations such as machine learning biology and signal processing potentially influencing a broad range of applications.  Theoretical and Practical Insight: The combination of theoretical bounds with empirical evaluations showcases both the analytical rigor and the practical validity of the proposed algorithms.  Detailed Analysis: The examination of both upper and lower bounds provides a thorough understanding of the problem space and the effectiveness of the solutions. Weaknesses:  Limited Consideration of Alternative Approaches: While the paper effectively argues the superiority of its proposed methods it lacks a comprehensive comparison with other contemporary solutions outside of matrixvector product computations such as emerging machine learning models or graphbased approaches that might offer competitive or complementary benefits.  Complexity and Accessibility: The high technical complexity of the methodologies and their explanations may limit the papers accessibility to a broader audience or practitioners not deeply versed in algorithmic linear algebra.  3) Questions and Suggestions for the Authors:  Clarification on Practical Implementations: Can the authors provide more details on the practical challenges of implementing these algorithms particularly concerning memory requirements and parallelization opportunities  Expansion on Lower Bounds Discussion: The discussion on lower bounds especially in relation to the Strong Exponential Time Hypothesis (SETH) is intriguing. Can the authors elaborate on the implications of these bounds for future work in the field  Comparative Analysis: It would be beneficial to include a more detailed comparison with stateoftheart methods especially in empirical evaluations to highlight the advantages and potential tradeoffs more clearly.  Broader Applicability: The paper focuses on specific distance metrics (`1 and `2). Expanding the discussion to include other metrics or dimensionalities could underscore the versatility of the proposed algorithms.  4) Limitations and Suggestions for Improvement: The authors do an admirable job discussing the theoretical limits of their algorithms and their potential societal impact. However a more explicit discussion on the limitations regarding the scalability and computational resources required for larger datasets would enrich the paper. Similarly exploring the algorithms behavior in nonEuclidean spaces or under different data distributions could offer further insights into their applicability. Constructive Suggestions:  Incorporate a broader set of experiments with diverse datasets to validate the algorithms performance across different scenarios.  Discuss the feasibility of adapting these algorithms for other commonly used distance functions or in the context of highdimensional data where distance matrix computations become even more challenging.  Explore the ethical implications of fast algorithmic computations in sensitive applications such as surveillance or personal data analysis and suggest guidelines for responsible use. In summary this paper makes significant contributions to the field of algorithmic research related to distance matrices with potential widereaching implications. Addressing the mentioned areas could further strengthen the papers impact and broaden its applicability.", "wjqr6aqkLUV": " Peer Review on \"Condensation in Neural Networks with Small Initialization: Understanding the Role of Multiplicity\"  1) Summary of the Paper This paper investigates the phenomenon of weight vector condensation in the initial training stages of Rectified Linear Unit (ReLU) neural networks with small initialization. The authors demonstrate both empirically and theoretically that the maximum number of condensed orientations of input weights in hidden neurons is directly related to the multiplicity of the activation function employed in the network. Specifically they find that this number is twice the multiplicity of the activation function. The study extends previous findings by considering multilayer fully connected neural networks and various common activation functions with multiplicity one as well as examining layers with onedimensional input and arbitrary multiplicity. Through extensive experiments and theoretical analysis the authors contribute to the understanding of how small initialization affects neural network training and its implications for network capacity and generalization.  2) Assessment of Strengths and Weaknesses Strengths:  Originality and Relevance: The paper addresses an important and relatively underexplored aspect of neural network initialization and training dynamics contributing valuable insights into the role of activation function multiplicity in weight vector condensation.  Comprehensive Analysis: The combination of empirical experiments and theoretical analysis to investigate the phenomenon across different activation functions and network architectures strengthens the papers conclusions.  Practical Implications: Understanding how small initialization leads to condensed orientations and its effect on network capacity and generalization has practical implications for designing and training neural networks especially overparameterized models. Weaknesses:  Generalizability of Results: While the paper provides insightful analysis for fully connected networks and common activation functions the extension of these results to other network types (e.g. convolutional neural networks) and more complex or less common activation functions is not discussed.  Depth of Theoretical Analysis: The theoretical analysis while insightful could benefit from a more detailed mathematical grounding providing clearer derivations and proofs of the presented propositions.  Potential Overemphasis on Specific Scenarios: The focus on fully connected networks and specific types of activation functions might limit the broader applicability of the findings. Exploring a wider variety of architectures and activation functions could enhance the papers impact.  3) Questions and Suggestions for the Authors  Extendibility: Could the authors discuss how their findings might extend to other network architectures such as convolutional or recurrent neural networks  Robustness of Findings: How robust are the observed phenomena to variations in other hyperparameters (such as learning rate batch size) beyond the small initialization  Additional Activation Functions: It would be beneficial if the authors could explore or discuss the implications of their findings for less common activation functions or activation functions with higher multiplicities.  Clarity on Theoretical Proofs: Providing more detailed proofs or a clearer theoretical framework could help in understanding the underlying mechanisms of the observed condensation phenomenon.  4) Limitations and Potential Negative Societal Impact The authors have addressed the limitations of their work primarily in terms of the scope of neural network architectures and activation functions considered. Expanding the analysis to include a wider array of architectures and activation functions could provide a more comprehensive understanding of the condensation phenomenon. Regarding potential negative societal impacts the paper does not directly touch upon these aspects. However in general improving the generalization performance of neural networks can have broad implications including the acceleration of AI development in sensitive applications. While this work contributes positively towards understanding neural network behavior future research should remain mindful of ensuring ethical use and avoidance of harm in applications developed on the basis of these insights. To improve the paper the authors could include a short discussion on potential extensions of their work and any broader ethical considerations related to the practical application of their findings.", "sOVNpUEgKMp": "Given the extensive and detailed nature of the input content this review will strive to provide a comprehensive evaluation following the requested format:  1. Brief Summary of the Paper and Its Contributions The paper presents an innovative approach to addressing the crossdistribution generalization issue inherent in deep models for vehicle routing problems (VRPs) through an Adaptive MultiDistribution Knowledge Distillation (AMDKD) scheme. The primary contributions are:  The introduction of knowledge distillation in neural combinatorial optimization to improve crossdistribution generalization for solving VRPs marking a significant conceptual advancement in the field.  The development of the AMDKD scheme which distills diverse knowledge from multiple teacher models trained on different exemplar distributions into a single generalist lightweight student model. This model manages to absorb hardtomaster knowledge from varied distributions effectively.  The application of AMDKD to two deep models demonstrating its generality and efficiency in inference while achieving competitive results on a wide range of VRP instances including unseen distributions and benchmark datasets.  2. Assessment of Paper Strengths and Weaknesses  Strengths:  Innovation in Approach: The papers novel application of knowledge distillation to VRPs represents a significant leap forward in addressing the generalization issue making it a considerable contribution to the field.  Comprehensive Experiments: The extensive experimental results comparing the AMDKD scheme against baseline deep models across multiple distributions and benchmark datasets strongly support the proposed methods efficacy.  Generic and Efficient Solution: AMDKDs applicability to different neural models and its efficiency in inference achieved by generating a lightweight but generalist student model is highly advantageous offering broad applicability and practical benefits.  Weaknesses:  Limited Analysis on Exemplar Distributions: The choice of exemplar distributions is crucial for the success of the AMDKD scheme. However there is limited discussion on how these distributions were selected and the impact of different selections on the generality and performance of the student model.  Potential Scalability Concerns: While the paper demonstrates the effectiveness of AMDKD on certain VRP instances there is scant discussion on its scalability particularly for larger problem sizes that are common in realworld applications.  Addressing Limitations and Societal Impact: The limitation discussion primarily focuses on the methods potential for not significantly boosting performance across all unseen distributions. However it lacks a broader discussion on potential negative societal impacts such as increased resource consumption or biases towards certain distributions.  3. Questions and Suggestions for the Authors  Could the authors elaborate on how the exemplar distributions were chosen and whether different combinations of distributions were evaluated for their impact on the student models performance  Are there any insights into how AMDKD might scale to larger VRP instances or more complex VRP variants not covered in the current experiments  It would be beneficial if the authors could discuss any potential negative societal impacts of their work such as the environmental implications of increased computational resource use for training and inference or the risk of model bias towards certain types of VRP instances.  Limitations and Potential Negative Societal Impact The papers limitation section discusses the potential for limited boosts in performance across some unseen distributions and outlines future research directions. However a comprehensive exploration of potential negative societal impacts such as environmental considerations related to computational demands or biases in model applicability to realworld problems is lacking. Constructive suggestions for improvement include:  Conducting a more indepth analysis of potential biases introduced by the choice of exemplar distributions and exploring strategies to mitigate such biases.  Evaluating the environmental impact of the proposed method considering the computational resources required for training and inference and suggesting ways to improve computational efficiency.  Expanding the discussion on how AMDKD can be adapted or improved to address larger more complex VRP instances ensuring its broad applicability and utility in solving realworld problems.", "xLnfzQYSIue": "Review of \"On the Asymptotic \u03b2Optimality of Top Two Algorithms for Bounded MultiArmed Bandits\" 1) Summary: The paper expands the realm of Top Two algorithms previously confined to Gaussian arms with known variances into more generalized settings including nonparametric families of distributions with bounded support. Top Two algorithms leveraging a leader and a challenger selection mechanism are analyzed for their effectiveness in the best arm identification scenario under fixed confidence. The authors introduce modifications to the selection process of the leader and the challenger that do not require sampling thus making the algorithms applicable to a wider array of distributions. Through theoretical analyses they demonstrate the asymptotic \u03b2optimality for these algorithms in contexts of bounded distributions. The paper culminates with experiments on realworld crop management data showcasing the practical superiority of the proposed methodologies over existing baselines. 2) Strengths and Weaknesses: Strengths:  Generalization to NonParametric Settings: A significant contribution is the extension of Top Two algorithms to handle a broader class of distributions making this approach more versatile and applicable to various practical scenarios.  Theoretical Rigor: The paper provides a comprehensive theoretical framework and a proof of asymptotic \u03b2optimality for the introduced algorithms under bounded distributions. This solid theoretical backing is commendable.  Empirical Validation: Experiments with realworld agricultural data and synthetic Bernoulli instances substantiate the algorithms effectiveness and practical utility. Weaknesses:  Computational Concerns: While the paper discusses computational aspects there is insufficient detail on the computational complexity and scalability of the algorithms particularly for largescale applications.  Lack of Nonasymptotic Analysis: The analysis focuses on asymptotic guarantees. Nonasymptotic performance insights which are crucial for understanding practical applicability are missing.  Assumption on Distinct Means: The assumption that \\xe2\\x88\\x86min(F)  0 (distinct means) might limit the generalization of results to cases where arms have very similar or equal means which often occurs in practice. 3) Questions and Suggestions:  Computational Complexity: Could the authors provide more details on the computational complexity of the proposed algorithms especially in comparison with existing solutions Understanding their computational demands is crucial for assessing their suitability for largescale problems.  Nonasymptotic Performance: It would be beneficial to complement the asymptotic analysis with nonasymptotic performance bounds. Could the authors comment on the possibility of deriving such bounds and their expected impact on the theoretical understanding of these algorithms  Extension to Equal Means: Is there a way to relax the assumption on distinct means (i.e. \\xe2\\x88\\x86min(F)  0) Providing insights or tentative approaches for dealing with scenarios involving equal means could significantly enhance the papers contribution. 4) Limitations and Societal Impact: The paper does not delve into the potential limitations regarding the assumptions made particularly the distinct means assumption which might not hold in all practical scenarios. Moreover the societal impact particularly any negative implications of applying these algorithms in critical decisionmaking processes is not addressed. Given the application in agriculture considerations around fairness equitable resource distribution and environmental impact would be pertinent to discuss. Constructive Suggestions for Improvement:  Broadening the Analysis: Incorporating nonasymptotic performance analyses could provide a more rounded understanding of the algorithms practical efficacy.  Further Computational Analysis: A detailed comparison of computational complexities and scalability discussions could enhance the papers applicative value.  Societal Impact Discussion: Including a section on the potential societal impact discussing both positive and negative aspects would make the paper more comprehensive and responsible. In conclusion the paper makes significant strides in generalizing Top Two algorithms for a broad array of distributions supported by robust theoretical analyses and empirical validations. Addressing the outlined limitations could further augment its contributions to the multiarmed bandit literature.", "sL7XH6-V21e": "The paper under review investigates the theoretical aspects of Deep Forest (DF) an advanced ensemble learning algorithm that extends Random Forest (RF) by incorporating multilayer feature transformations through prediction concatenation (PreConc). The primary contributions of the study include the first consistency analysis of PreConc in deep forest proving universal consistency under appropriate conditions and demonstrating that a twolayer deep forest outperforms shallow layers regarding convergence rates under certain settings. This work fills a crucial gap in understanding DFs theoretical underpinnings providing a cornerstone for further exploration and innovation in treebased ensemble learning methods. Strengths: 1. Theoretical Contributions: The paper provides significant theoretical insights into why deep forest architectures can outperform traditional random forests especially highlighting the role of the PreConc operation in enhancing feature transformation and demonstrating universal consistency. 2. Empirical Validation: The authors conduct a series of simulation experiments that not only support their theoretical findings but also give practical insights into how and why deep forests can be more effective than their random forest counterparts under specific conditions. 3. Novelty: This work is pioneering in its theoretical examination of the deep forests feature transformation efficacy offering a new perspective on understanding ensemble learning architectures. Weaknesses: 1. Assumptions: The paper relies on two major assumptions that may not always hold in practical scenarios. The conditions laid out in these assumptions could limit the applicability of the results to a broader spectrum of datasets and learning tasks. 2. Complexity and Accessibility: The mathematical rigor while a strength for theoretical robustness may pose a challenge in terms of accessibility and practical applicability for a wider audience including practitioners. 3. Scope of Experiments: Although the simulation studies validate the theoretical claims the paper does not include experiments on realworld datasets. As a result the practical utility of the findings in reallife applications remains somewhat speculative. Questions and Suggestions for the Authors: 1. Realworld Applications: Can the authors provide insights or preliminary results on the application of their theoretical findings in realworld scenarios possibly including datasets from domains like medicine finance or computer vision 2. Assumptions Relaxation: It would be beneficial if the authors discussed the possibility of relaxing the assumptions made in this study. Are there any ongoing efforts or future directions to explore the validity of their findings under less restrictive conditions 3. Impact of Hyperparameters: The role of hyperparameters in the construction of deep forests such as the number of trees per layer and the depth of trees is crucial. Could the authors expand on the sensitivity of their results to these hyperparameters and suggest practical guidelines for their selection 4. Comparative Analysis: A broader comparison with other ensemble methods especially gradient boosting machines and deep learningbased approaches could further contextualize the advantages and limitations of deep forests. Limitations and Suggestions: The authors have made commendable efforts to address the limitations inherent in their theoretical framework however the realworld applicability of their findings could be enhanced by including experiments on diverse datasets and exploring the implications of their assumptions in practical scenarios. Additionally a discussion on the potential negative societal impacts especially in scenarios where model interpretability and fairness are critical would strengthen the paper. Constructively future work could focus on extending the theoretical analysis to more complex deep forest structures and exploring adaptive mechanisms for hyperparameter tuning to enhance the practical utility of deep forests in a wider array of applications.", "u4dXcUEsN7B": "Review of \"Exploiting Example Influence on StabilityPlasticity Dilemma in Continual Learning Using MetaSP Algorithm\" 1. Summary: This paper presents a novel approach named MetaSP to address the StabilityPlasticity (SP) dilemma in continual learning (CL) by exploiting the differential influence of training examples on stability and plasticity. Inspired by the Influence Function (IF) the authors propose a metalearning algorithm that computes example influence without the computational burden of Hessian inversion. This is achieved through pseudo updates and gradient computations from examplelevel perturbations. The authors then introduce a DualObjective Optimization (DOO) problem to fuse stability and plasticity influences aiming for SP Pareto optimality. The fused influence informs model updates and rehearsal memory optimizations demonstrating significant performance improvements over stateoftheart methods on task and classincremental benchmark CL datasets. 2. Assessment: Strengths:  The paper addresses a crucial problem in continual learning \u2013 the balance between retaining previous knowledge and incorporating new information efficiently (the SP dilemma).  The proposed MetaSP algorithm is innovative leveraging examplelevel perturbations and metalearning to compute example influence in a computationally efficient manner.  The dualobjective optimization approach for fusing influences on stability and plasticity is wellthoughtout providing a compelling solution to achieve a balance between the two.  Empirical results are strong showing significant improvements over existing stateoftheart methods across various benchmarks which is a testament to the effectiveness of the proposed approach. Weaknesses:  While the paper presents a clever method to avoid the computational drawbacks of IF by using metalearning it lacks a deeper theoretical discussion on the implications of approximating IF in this manner.  The paper could benefit from a more extensive discussion on the scalability of MetaSP especially in scenarios with a large number of tasks or very complex task structures.  The limitation section briefly mentions concerns regarding privacy efficiency for online learning and performance under small memory sizes but lacks detailed explorations or potential solutions for these issues. Questions and Suggestions for the Authors:  Could the authors elaborate on the theoretical foundation of approximating IF using metalearning Specifically are there any tradeoffs in terms of the accuracy of influence estimation  The paper could benefit from an expanded discussion on potential strategies to overcome the identified limitations especially regarding privacy concerns and the algorithms performance in online CL scenarios.  Could the authors explore the impact of various memory sizes more deeply It would be interesting to see how MetaSP performs under extremely constrained memory conditions and whether any modifications can help mitigate performance drops. 3. Limitations: The limitations mentioned by the authors including concerns related to privacy the efficiency of the method in online learning contexts and its performance under restricted memory scenarios are acknowledged but not extensively explored. Constructively it would be beneficial if future work could address these limitations more thoroughly perhaps by introducing adaptive mechanisms to optimize memory usage based on available resources or developing privacypreserving versions of MetaSP. Additionally strategies to reduce computational costs for online learning settings should be considered to enhance the practical applicability of the method. Overall the paper represents a significant step forward in addressing the SP dilemma in continual learning through a novel and effective algorithmic approach. However addressing the theoretical and practical limitations outlined could further strengthen the works impact and applicability in a broader range of CL scenarios.", "rBCvMG-JsPd": " PeerReview of \"FewShot Learning with ParameterEfficient FineTuning and Incontext Learning Comparisons\"  1) Summary: This paper presents a comprehensive study comparing fewshot incontext learning (ICL) and parameterefficient finetuning (PEFT) methods for adapting pretrained language models (LMs) to perform new tasks with limited labeled data. The authors introduce a new PEFT method called (IA)\u00b3 that scales activations with learned vectors demonstrating stronger performance with a minimal increase in parameters. A specific model configuration named \"TFew\" based on the T0 model and incorporating (IA)\u00b3 is proposed which significantly outperforms existing benchmarks including achieving superhuman performance on the RAFT benchmark. The paper also discusses the computational memory and storage differences between ICL and PEFT methods advocating for PEFT and specifically TFew as more efficient alternatives for fewshot learning tasks.  2) Assessment: Strengths:  Innovative Approach: The introduction of (IA)\u00b3 as a PEFT method represents a significant advancement in making fewshot learning more efficient and effective.  Comprehensive Comparison: The paper thoroughly compares ICL and PEFT providing clear insights into the advantages and limitations of each approach.  Practical Relevance: The TFew models superior performance on the RAFT benchmark underscores its practical applicability for realworld tasks.  Resource Efficiency: Detailed analysis of computational costs demonstrates the efficiency of TFew over ICL highlighting its suitability for largescale applications. Weaknesses:  Limited Scope of Tasks: While impressive the papers focus on classification tasks potentially limits the generalizability of the findings. A broader evaluation including generative tasks would enhance the contribution.  Potential Overfitting to Benchmarks: The exceptional performance on benchmarks raises questions about overfitting and the adaptability of TFew to truly unseen tasks not aligned with benchmark characteristics.  Insufficient Discussion on Societal Implications: The paper briefly mentions the importance of addressing negative societal impacts but lacks a thorough analysis or guidelines for responsible usage of TFew.  3) Questions and Suggestions for Authors:  Can the authors elaborate on the potential limitations of TFew when applied to generative tasks and how they plan to address these in future work  It would be beneficial to include a broader discussion on the ethical use of TFew especially in contexts where biased or harmful outputs could have significant consequences.  The paper could greatly benefit from a more detailed examination of the potential for overfitting to benchmarks and strategies to ensure TFews robustness in diverse realworld applications.  4) Limitations and Societal Impact: While the paper provides a commendable analysis of computational and memory costs it falls short of fully addressing the limitations related to task diversity and the potential for overfitting to benchmarks. Additionally the discussion on negative societal impacts is noticeably brief. As PEFT methods including TFew become more powerful and widely used a comprehensive framework for assessing and mitigating ethical risks is crucial. Constructive suggestions for improvement include:  Expanding the evaluation to include a wider array of tasks with a focus on generative and openended challenges.  Incorporating mechanisms to prevent or mitigate bias and ensure that the models outputs are aligned with ethical guidelines including transparency in model training and decisionmaking processes.  Establishing an ongoing evaluation process that includes diverse stakeholders to continually assess the impact of TFew and similar technologies on society. Overall while the paper makes significant contributions to the field of fewshot learning attention to these areas would enhance its comprehensiveness and relevance.", "p6hArCtwLAU": "Paper Summary: This paper presents TreeMoCo a novel selfsupervised learning framework designed to encode and learn representations of neuron tree morphology without requiring annotated data. The framework is built upon a TreeLSTM network for encoding neuron morphologies and employs contrastive learning for feature learning. TreeMoCo is tested on 2403 highquality 3D neuron reconstructions from mouse brains sourced from three different public datasets. The results demonstrate TreeMoCo\u2019s effectiveness in classifying main and subtypes of brain cell types marking the first attempt to apply contrastive learning to neuron tree morphology representation. The availability of the code is also a significant contribution presenting opportunities for further research and development in this area. Strengths: 1. Innovative Approach: The application of a selfsupervised learning framework particularly contrastive learning to neuron morphology is novel and addresses the gap in quantitative analysis of neuron trees. 2. Comprehensive Testing: The utilization of three distinct public datasets for testing ensures that the findings are robust across different data sources enhancing the reliability of the proposed method. 3. Potential for Impact: Given the importance of neuron morphology in various neuroscience research areas TreeMoCo presents a potentially impactful tool for advancing understanding and analysis in this field. Weaknesses: 1. Limitation on Data Type: The study focuses solely on dendritic arbors with axons left for future work. Including axons in the analysis could provide a more comprehensive understanding of neuron morphology. 2. Sensitivity to Data Quality: TreeMoCo shows variability in performance depending on the completeness of the neuron reconstructions across different datasets. This could limit its applicability in cases where only incomplete reconstructions are available. 3. Oscillation in Performance: The paper mentions performance oscillation in downstream tasks after long training epochs on smaller datasets which might indicate the need for optimization in the training process or early stopping criteria. Questions and Suggestions for the Authors: 1. Extension to Axons: Have the authors considered extending TreeMoCo\u2019s application to include axon morphology If so what challenges do they anticipate and how might they address them 2. Addressing Data Quality Sensitivity: Could the authors elaborate on potential strategies to make TreeMoCo more robust to the variability in reconstruction completeness of neuron datasets 3. Early Stopping Criterion: The paper notes the absence of an effective early stopping criterion. Could the authors suggest any approaches or criteria that could be investigated to mitigate the observed performance oscillation in smaller datasets Limitations and Societal Impact: The authors have discussed limitations including the framework\u2019s sensitivity to global changes due to the nature of TreeLSTM and its oscillation in performance on incomplete datasets or smaller dataset sizes. These should be addressed in future iterations to enhance TreeMoCo\u2019s robustness and applicability. Regarding potential negative societal impact the paper does not discuss this aspect. However as TreeMoCo advances the understanding of neuron morphology it is important to consider ethical implications particularly in how this knowledge might be applied in neuroscience. For example enhancing privacy measures for data sourcing and ensuring the responsible use of the technology in research could be important considerations. Suggestions for improvement include further investigation into network architecture or augmentation strategies to counteract sensitivity to data completeness and exploring ethical guidelines for the use and application of TreeMoCo in research.", "znbTxnBPlx": " Peer Review for \"A Novel Training Method for Stabilized Supralinear Networks (SSNs) for Biological and Machine Learning Computations\"  1) Briefly summarize the paper and its contributions This paper presents an innovative approach to bridge the gap between the biological realism and computational performance of neural networks by developing a new method for training stabilized supralinear networks (SSNs). The authors provide a solution to the challenge of dynamical instabilities in SSNs which are a canonical model for cortical neural circuits and exhibit properties that closely mimic those of biological neural networks. Through a novel training method that involves the dynamicsneutral addition of neurons the authors successfully train SSNs to perform MNIST classification and demonstrate that their approach enables SSNs to tackle more complex tasks such as amortized Markov chain Monte Carlobased inference under a Gaussian scale mixture model. These contributions validate the approach and open new avenues for training biologically realistic neural networks on a broader range of tasks.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Novelty: The paper presents a unique method for training SSNs that significantly advances the capability of these networks to learn and perform complex computations. This approach addresses the critical challenge of dynamical instabilities in SSNs a longstanding hurdle in neural network research.  Biological Relevance: By adhering more closely to biological constraints than existing artificial neural networks the trained SSNs offer a promising pathway for studying the computational mechanisms of the brain at a higher resolution.  Applicability: The methods success in training SSNs for challenging tasks including a previously unattainable task highlights its potential applicability to a wide range of neural networkbased computations and cognitive tasks across different cortical areas.  Reproducibility and Clarity: The paper is wellorganized and the methods are described in detail facilitating reproducibility. The inclusion of algorithm development experimentation and a detailed theoretical motivation further strengthen the paper. Weaknesses:  Generalization: While the paper demonstrates the method\u2019s effectiveness on specific tasks it remains uncertain how well this approach generalizes across other types of neural networks and a broader spectrum of computational challenges.  Evaluation: The comparison with other models particularly in terms of biological realism and computational performance could be expanded. A more comprehensive evaluation might include a wider range of benchmarks and tasks to firmly establish the methods relative standing.  Computational Efficiency: The paper mentions the computational expense associated with the network growth method but does not provide extensive analysis or solutions to mitigate this issue. Further exploration of the scalability and efficiency of the method would be beneficial.  3) Please list up and carefully describe any questions and suggestions for the authors  Generalization to Other Models: Can the authors discuss the potential applicability of their training method to other neural network architectures especially those not explicitly modeled after cortical circuits  Efficiency Improvements: Are there any strategies that could be employed to enhance the computational efficiency of the training method thus making it more scalable and applicable to larger networks  Broader Benchmarking: Could the authors extend their evaluation to include additional benchmarks or comparison with other biologically inspired neural network training methods This would help contextualize the contributions and understand the methods place within the broader field.  Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work If not please include constructive suggestions for improvement The paper discusses the computational expense associated with the proposed training method but does not explicitly address broader limitations including the potential for overfitting the methods scalability to very large networks and the generalization to tasks beyond those demonstrated. Additionally potential negative societal impacts such as the application of these biologically realistic models in creating more sophisticated AI systems with unforeseen consequences are not discussed. It would be constructive for the authors to:  Explore strategies to overcome or mitigate the mentioned computational issues.  Provide a detailed discussion on how the method could be adapted or evolved to maintain stability in even larger networks or across a more diverse array of tasks.  Include a section discussing ethical considerations and potential negative societal impacts offering suggestions for responsible research and application of the technology.", "zLVLB-OncUY": " Review of \"Contrastive Optimal Positive Generation via Latent Transformation for SelfSupervised Visual Representation Learning\"  1) Summary of Contributions This paper introduces an innovative approach named Contrastive Optimal Positive Generation (COPGen) for generating instancespecific optimal positives for contrastive learning in a selfsupervised setting. The method leverages the semantic consistency and computational advantages of latent space transformations using welltrained generative models. COPGen proposes an instancespecific navigator in the latent space that aims to minimize mutual information between a generated positive pair while maintaining semantic consistency. This approach theoretically ensures the generation of optimal positives that preserve semantic information while discarding nuisance details. Empirically the paper presents extensive experiments where COPGen outperforms both latent transformation methods and realimagebased methods in selfsupervised contrastive learning tasks.  2) Strengths and Weaknesses Strengths:  Theoretical Foundation: The paper provides a solid theoretical analysis supporting the utility of COPGen in generating optimal positive pairs enhancing the understandability and credibility of their approach.  Innovative Use of Generative Models: Leveraging latent space transformations through pretrained generative models for generating instancespecific positives is a novel and impactful contribution to the domain of selfsupervised learning.  Empirical Validation: Extensive experiments and comparisons validate the effectiveness of COPGen across various benchmarks indicating its practical benefits and the potential for broad applications. Weaknesses:  Dependence on Pretrained Generative Models: The performance of COPGen is inherently tied to the quality of the underlying generative models. The paper could benefit from a deeper exploration of the implications of this dependence.  Computational Complexity: While the paper briefly mentions computational overhead a more detailed analysis of computational costs and efficiency compared to baseline methods would further strengthen the paper.  Generalizability: Although the paper suggests that COPGen outperforms others in selfsupervised settings its effectiveness in diverse or more complex scenarios (e.g. tasks with highly irregular or abstract semantics) remains less explored.  3) Questions and Suggestions for the Authors  Robustness to Generative Model Quality: Can the authors provide insights into how variations in the quality of pretrained generative models impact the effectiveness of COPGen  Computational Overhead Analysis: Could the authors elaborate on the computational resources required for COPGen especially in the context of largescale datasets  Generalizability Concerns: How well does COPGen perform in tasks with highly abstract or nonobjectcentric semantics Are there limitations to the types of semantics the current model can effectively capture  4) Limitations and Potential Negative Societal Impact The paper acknowledges that the efficacy of COPGen hinges on the quality of pretrained generative models which could limit its applicability in scenarios where highquality generative models are not available. However the discussion on potential negative societal impacts is minimal. The reliance on generative models raises concerns regarding data privacy and bias propagation. Suggestions for Improvement:  Exploring Bias and Fairness: Conduct additional experiments or discussions on how biases inherent in the pretrained models might affect the representations learned through COPGen and propose mitigation strategies.  Enhanced Generative Model Training: Investigate methods for training generative models that reduce dependence on data space augmentations and explore more diverse and abstract semantics to broaden COPGens applicability. Overall this paper presents a valuable contribution to selfsupervised visual representation learning proposing a novel methodology with strong theoretical backing and significant empirical results. Addressing the highlighted questions and potential improvements could make it an even more robust and widely applicable tool in the field.", "tadPkBL2gHa": " Peer Review  1) Briefly summarize the paper and its contributions The paper addresses a critical challenge in academic recruitment settings such as faculty hiring and PhD admissions where committees must make recruitment decisions under uncertainty about candidates likelihood of accepting offers. Traditionally methods have involved sequential offer making under a strict budget constraint. The authors propose a new model where offers are extended in a single batch with a soft constraint on the target number of positions that is they aim to maximize the expected quality of recruits accounting for potential penalties from deviating from the target hire number. They provide theoretical underpinnings for their model showing that certain greedy algorithms can be optimal under specific conditions and expand this with fully polynomialtime approximation schemes and constantfactor approximations based on the penalty functions structure. Their empirical evaluations support the theoretical findings showing the practical relevance of their proposed methods over traditional sequential models.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  The paper addresses a practical and relevant problem in academic recruitment providing a novel perspective by treating the target number of positions as a soft constraint.  It bridges a gap between theory and practice by formalizing intuitive recruitment strategies into algorithmic solutions providing a stronger foundation for decisionmaking in these settings.  Theoretical contributions are significant offering guarantees for the proposed algorithms performance and demonstrating the conditions under which they are effective.  Empirical evaluation adds credibility to the theoretical results showing that the algorithms perform well across different scenarios particularly highlighting their adaptability to negative correlations between candidate quality and acceptance probability. Weaknesses:  While the problem setting is clearly defined and motivated the assumption of knowing the value and probability of acceptance for each candidate might not always be realistic in actual hiring scenarios.  The empirical evaluation although supportive of the theoretical claims could benefit from realworld data to better assess the models practical applicability and robustness.  The discussion on potential societal impacts is brief and somewhat speculative. A more indepth analysis of the ethical considerations and potential downstream effects on the academic job market would strengthen the paper.  3) Please list up and carefully describe any questions and suggestions for the authors Questions:  How do the authors envision their models being implemented in actual faculty hiring or PhD admission processes especially in terms of estimating the value and probability of acceptance for each candidate  Can the authors provide more insight into how their algorithms might perform in the presence of highly competitive candidates who are likely to receive multiple offers Suggestions:  Consider a broader set of empirical evaluations possibly incorporating case studies or simulations based on more realistic nuanced datasets to validate the models further.  Extend the discussion on the limitations and societal impacts particularly considering the potential for algorithmic bias and how it might influence diversity within recruitment processes.  Explore modifications to the algorithms that could accommodate realtime feedback as often hiring committees might receive some indication of a candidates leanings before making further offers.  Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work If not please include constructive suggestions for improvement. The authors briefly touch upon the limitations and potential negative societal impacts such as the potential inefficiency of the academic job market and the speculative broader pool of candidates receiving offers if their methodology were adopted. However this discussion could be more indepth. Specifically exploring the ethical considerations concerning data privacy algorithmic bias and fairness would be crucial given that these algorithms could significantly influence individuals career opportunities. Additionally the paper could benefit from a discussion on the potential for their approach to exacerbate existing disparities in academic recruitment if not carefully implemented with recommendations on safeguards or considerations to mitigate such risks.", "vhKaBdOOobB": " Peer Review for \"DFC Attention and GhostNetV2: Enhancing Mobile Networks with Decoupled Fully Connected Attention\"  1. Summary This paper introduces a novel hardwarefriendly attention mechanism named DFC (Decoupled Fully Connected) attention aimed at improving the performance of lightweight convolutional neural networks (CNNs) for mobile applications. The key innovation is the decomposition of a fully connected (FC) layer into horizontal and vertical operations to efficiently capture longrange dependencies across pixels. Leveraging this attention mechanism the authors propose an upgraded version of GhostNet dubbed GhostNetV2 which shows superior performance on benchmarks like ImageNet and MS COCO over its predecessor and other existing lightweight architectures with a minimal increase in computational cost.  2. Assessment Strengths:  Innovation: The DFC attention mechanism is a novel approach addressing the longrange dependency limitation in lightweight CNN models. Its deploymentefficient architecture preserving computational simplicity while capturing global information is particularly commendable.  Performance: The paper demonstrates robust empirical results where GhostNetV2 significantly outperforms existing models in terms of accuracy with comparable computational costs. These results are supported by extensive experiments across multiple datasets and tasks (image classification object detection and semantic segmentation).  Relevance: Given the increasing demand for efficient AI models in mobile and edge computing areas this work is highly relevant and timely. It provides a practical solution to deploy advanced AI algorithms on resourceconstrained devices without compromising performance. Weaknesses:  Generalization: While the paper reports improvements across various tasks it remains to be seen how the DFC attention and the GhostNetV2 architecture generalize to other domains beyond image processing such as audio or text.  Indepth Analysis: The paper could benefit from a more detailed analysis of why DFC attention leads to performance improvements. Specifically insights into the attention maps and their role in feature enhancement could provide a deeper understanding of the mechanism\u2019s efficiency.  Hardware Dependency: The efficiency of the proposed approach is discussed predominantly in the context of ARMbased mobile processors. An examination of its performance and efficiency on different hardware architectures would provide a clearer picture of its adaptability and limitations.  3. Questions and Suggestions for the Authors  Generalization Capability: Can the authors provide additional insights or experiments on the potential applicability of DFC attention in other domains such as NLP or speech recognition  Attention Mechanism Analysis: It would be beneficial to include a detailed analysis or visualization of the attention maps generated by DFC attention comparing them with conventional attention mechanisms to understand why the proposed method is more effective.  Hardware Adaptability: Are there specific hardware features on ARM devices that the DFC attention leverages for efficiency How might its performance vary on different computing platforms such as GPUs or custom AI accelerators  4. Limitations and Societal Impact The authors have outlined the computational efficiency and performance benefits of the proposed GhostNetV2 architecture. However there is limited discussion on the limitations regarding the adaptability of DFC attention across different hardware or its impact on power consumption which are critical for mobile deployments. Additionally the broader societal implications such as the potential for increased surveillance capabilities or the environmental impact of deploying more powerful AI models in everyday devices are not addressed. Constructive suggestions include:  Exploring Power Efficiency: Future work should include an analysis of power consumption relative to performance gains as this is crucial for mobile applications.  Adaptability Study: A comparative analysis of the proposed model\u2019s performance on a variety of hardware platforms would be valuable to understand its limitations and potential optimizations.  Societal Considerations: An explicit acknowledgment of potential negative implications with suggestions for mitigating risks associated with deployment would make the paper more comprehensive. This work marks a significant advancement in the field of lightweight neural networks offering promising avenues for future research and practical applications in mobile and edge computing.", "sZAbXH4ezvg": " Review of \"Expanding Effective Range and Multiscale Modeling in Implicit Graph Neural Networks\"  Summary This paper targets two principal weaknesses in the current implicit Graph Neural Networks (GNNs): their limited effective range for capturing longrange dependencies and their inability to model multiscale information on graphs. By analyzing the inherent limitations in the expressiveness of these networks due to their fixedpoint aggregation scheme the authors introduce MultiScale Graph Neural Network with Implicit layers (MGNNI). MGNNI incorporates multiple graph scales into the model in order to capture longrange and multiscale graph features effectively. Through theoretical analyses and comprehensive experiments involving synthetic and realworld datasets the paper highlights MGNNIs superior capability to address the identified limitations demonstrating its improved performance over existing methods on node and graph classification tasks.  Assessment Strengths: 1. Theoretical Contribution: An insightful theoretical analysis reveals the intrinsic limitations of implicit GNNs in terms of their effective range. This luminary work adds significantly to our understanding of GNN architectures and sets a strong foundation for future improvements. 2. Practical Impact: The introduction of MGNNI is a notable practical contribution addressing critical weaknesses of previous models. Expanding the effective range and enabling multiscale modeling could open new doors for GNN applications in various fields from social network analysis to bioinformatics. 3. Empirical Validation: Extensive experiments using both synthetic and realworld datasets convincingly demonstrate the advantages of MGNNI over existing implicit GNNs and some explicit GNN models. Such thorough validation bolsters confidence in the proposed models utility. Weaknesses: 1. Complexity and Efficiency: While the paper briefly discusses the computational complexity a more detailed analysis of the timespace tradeoffs and computational efficiency especially in comparison to explicit GNNs would be beneficial for a comprehensive understanding. 2. Generalizability of MGNNI: The paper showcases the advantageous use of MGNNI in specific settings (e.g. node and graph classification tasks). However further discussion on its applicability and performance across a broader range of problems and datasets would strengthen the case for its versatility. 3. Analysis on Realworld Impact: Though the model\u2019s academic potentials are wellarticulated insights into its realworld implications practical deployability and scalability particularly in dynamic largescale graph settings remain somewhat explored.  Questions and Suggestions 1. Scalability and Efficiency: Can the authors provide more insights into the scalability of MGNNI particularly in largescale graph scenarios Suggestions for optimization or adaptations for such contexts would be highly valuable. 2. Impact of Hyperparameters: The role of the multiscale hyperparameter \\(m\\) and the damping factor \\(\\lambda\\) in the performance of MGNNI have been crucial. A deeper exploration into their sensitivity and strategies for effective selection could provide readers with practical guidelines for deploying MGNNI. 3. Comparison with StateoftheArt: While MGNNI outperforms implicit GNNs and some explicit models how does it stand against the latest stateoftheart GNNs especially those which also propose to tackle longrange dependencies and multiscale information  Limitations and Societal Impact The paper momentarily touches upon the limitations related to approximation errors in iterative methods which is appreciative. For a more robust discussion emphasizing potential limitations in diverse practical scenarios (e.g. graphs with evolving structures) would provide a rounded perspective. Regarding societal impact although GNNs hold transformative potentials in various domains a brief discussion on potential negative implications such as privacy concerns in social network analysis would be responsible. Finally suggestions for mitigating any such unintended consequences would enhance the papers consideration of ethical implications.", "tYAS1Rpys5": " Peerreview of \"Simulationguided Beam Search (SGBS) for Combinatorial Optimization\"  1) Summary of the Paper The paper presents a novel approach Simulationguided Beam Search (SGBS) aimed at enhancing the performance of neural approaches in solving combinatorial optimization (CO) problems specifically focusing on the Traveling Salesperson Problem (TSP) Capacitated Vehicle Routing Problem (CVRP) and the Flexible Flow Shop Problem (FFSP). SGBS employs a fixedwidth tree search informed by both neural networkderived policies and rollouts to identify and expand promising candidate solutions. This method is further combined with Efficient Active Search (EAS) to improve both the quality of solutions found and the policy used during search. The authors provide empirical evidence showing that SGBS particularly when used in conjunction with EAS outperforms existing neural and traditional approaches in solving the mentioned CO problems thereby significantly reducing the optimality gap.  2) Assessment of the Strengths and Weaknesses  Strengths: 1. Originality: The proposal of SGBS is innovative effectively combining the strengths of MonteCarlo Tree Search (MCTS) and beam search with neural approaches to address the limitations of existing methods in solving CO problems. 2. Performance: The results demonstrate not only the viability of SGBS in achieving superior solution quality compared to stateoftheart neural methods but also its competitiveness with traditional heuristics particularly when hybridized with EAS. 3. Broad Applicability: The method is applied to three distinct CO problems showcasing its generalizability and potential applicability to a wide range of CO tasks beyond those tested. 4. Practicality: By providing a method that adjusts to various computation time budgets and requiring minimal modifications to existing neural CO approaches the authors offer a readily deployable solution for practitioners.  Weaknesses: 1. Implementation Complexity for Realworld Applications: While the paper highlights the efficacy of SGBS  EAS the practical complexities of integrating these methods into existing systems for realworld applications remain unaddressed. 2. Computational Resource Requirements: The experiments were conducted on highend GPUs thus the computational cost and feasibility of deploying SGBS in environments with limited resources are unclear. 3. Lack of Comprehensive Comparison: The paper could benefit from a more exhaustive comparison with a broader range of both neural and nonneural baselines to further validate the superiority of SGBS. 4. Hyperparameter Sensitivity Analysis: There is an insufficient examination of the sensitivity of SGBSs performance to its hyperparameters which could pose challenges in tuning the approach for optimal performance across diverse problem sets.  3) Questions and Suggestions for the Authors 1. Can the authors provide insights into the computational efficiency of SGBS when deployed on median or lowtier computational resources 2. It would be beneficial if the authors could discuss the robustness of SGBS to variations in problem instance distributions and the approachs adaptability to unseen or dynamically changing problem sets. 3. Clarification on the process and criteria for choosing the beam width (\u03b2) and expansion factor (\u03b3) for each CO problem type would enhance the reproducibility and application of the proposed method. 4. A deeper exploration into the tradeoffs between solution quality and computational time particularly in comparison to existing methods would be valuable for practitioners prioritizing one aspect over another.  4) Limitations and Suggestions for Improvement The paper does not adequately discuss the potential limitations and societal impacts of deploying SGBS especially in realworld scenarios where computational resources may be constrained. Additionally the absence of a detailed sensitivity analysis of the methods hyperparameters could hinder its applicability across varied CO problem types.  Suggestions for Improvement: 1. Detailed Resource Requirement Analysis: Providing a comprehensive analysis of the computational resources required for SGBS across different hardware setups would greatly benefit practitioners working in constrained environments. 2. Hyperparameter Sensitivity Study: Conducting a sensitivity study to understand the impact of various hyperparameters on the performance of SGBS would aid in the methods broader applicability and tuning. 3. Extension to Other CO Problems: Demonstrating the effectiveness of SGBS on a wider range of CO problems could further validate the methods generalizability and encourage adoption. 4. Discussion on Societal Impacts: An indepth conversation regarding the potential societal implications particularly in operational and environmental contexts would enrich the discourse around the deployment of SGBS. In summary while the paper makes significant contributions to the field of CO addressing the noted limitations and considering the provided suggestions would further enhance the utility and impact of the proposed method.", "qqHMvHbfu6": "Review of \"Analyzing the Dynamics of Lewis Signaling Games for Language Emergence\" 1) Summary: This paper tackles the issue of language emergence within the scope of Lewis signaling games utilizing a deep reinforcement learning framework to simulate the communication protocol development between agents. Through analytical and empirical examinations the authors dissect the standard objective in Lewis games into two distinct losses: an information loss and a coadaptation loss. This decomposition is pivotal in understanding how overfitting within these components may thwart the emergence of structured communication. The authors experiments demonstrate that controlling for coadaptation overfitting encourages the development of languages with better generalization and compositionality. This work is innovative in its approach to dissect and manipulate the learning dynamics of Lewis signaling games providing new insights into the conditions under which a more structured and generalizable language can emerge. 2) Assessment: Strengths:  The paper is wellstructured providing a comprehensive background on Lewis signaling games and the importance of understanding language emergence dynamics. The decomposition of the games standard objective into information loss and coadaptation loss is a novel and significant contribution.  The empirical methodology is robust employing probing mechanisms and controlled experiments to investigate the effects of adjusting the coadaptation loss. The use of various listener training regimes (Continuous Partial Early stopping listener) adds depth to the analysis.  The findings offer a compelling argument that tackling coadaptation overfitting can lead to languages that are more structured and generalize better challenging some of the existing notions in the field. Weaknesses:  While the paper discusses the societal implications of language emergence models briefly it could benefit from a more detailed exploration of potential negative impacts specifically in the context of reinforcement learning biases or misuse.  The discussion on limitations primarily focuses on the methodological aspect such as computational costs. It would be strengthened by including considerations on the models scalability to more complex scenarios or its applicability to realworld communication systems.  The paper could provide clearer guidelines for future work especially in terms of exploring alternative loss functions regularization methods or game setups that might further enhance language structure and generalization. 3) Questions and Suggestions for Authors:  Could the authors elaborate on the potential societal impacts of their findings Specifically how might biases in emergent languages developed through Lewis signaling games be identified and mitigated  The paper benefits from a comparison between the impact of regularization on the speaker versus the listener. Could the authors speculate on or suggest methods to control overfitting that are applicable to the speaker potentially enhancing language emergence further  Given the intriguing results seen by manipulating the coadaptation loss could the authors discuss additional game dynamics or loss components that might warrant future investigation For instance exploring asymmetric information scenarios or incorporating external environmental pressures. 4) Limitations and Societal Impact: The paper makes a concerted effort to address limitations concerning computational demands and the adaptability of their approach across various training regimes. However it could benefit from a deeper dialogue on the theoretical limits of their model particularly in scaling to complex realworld communicative tasks. In terms of societal impact while the paper briefly mentions the importance of understanding biases in emergent languages it falls short of a thorough evaluation. Constructive recommendations could include a more detailed examination of how emergent languages could perpetuate or mitigate existing inequities in communication systems. Expanding on this the authors might explore strategies to ensure that the development of artificial languages through such models aligns with ethical considerations and promotes inclusivity.", "pD5Pl5hen_g": "The paper presents a novel algorithm to address the smooth and stronglyconvexstronglyconcave minimax optimization problem. The authors argue that current stateoftheart methods do not meet the established lower bound \\xce\\xa9(\\xe2\\x88\\x9a\u03bax\u03bay log 1\u03b5) for the number of gradient evaluations required to find an \u03b5accurate solution. To address this gap they introduce an algorithm achieving a gradient evaluation complexity of O(\\xe2\\x88\\x9a\u03bax\u03bay log 1\u03b5) thereby matching the lower bound. This algorithm is developed in three stages: a reformulation of the original problem application of a variant of the proximal point algorithm and an optimal algorithm for operator norm reduction in monotone inclusions.  Strengths: 1. Theoretical Contribution: The paper addresses a significant gap in the current literature by providing an algorithm that matches the theoretical lower bound for gradient evaluation complexity in the context of minimax optimization. This is a substantial theoretical advance. 2. Methodological Rigor: The development of the algorithm is methodologically sound with clear logical steps that draw on established mathematical principles and techniques. 3. Applicability: Given the wide application of minimax optimization in fields like machine learning the contributions of this paper have the potential for broad impact enhancing the efficiency of adversarial training reinforcement learning and other areas.  Weaknesses: 1. Experimental Validation: The paper lacks experimental results that demonstrate the practical performance of the proposed algorithm especially in comparison to existing stateoftheart methods. 2. Discussion on Assumptions: While the paper formally states the assumptions for the problem setting there is sparse discussion on the practical implications of these assumptions or how restrictive they might be in realworld applications. 3. Accessibility to a Wider Audience: The heavy reliance on technical language and advanced mathematical formalisms might make the paper less accessible to readers not wellversed in optimization theory.  Questions and Suggestions: 1. Experimental Evidence: Could the authors provide empirical evidence supporting the theoretical advancements with practical performance metrics in real or simulated datasets 2. Assumptions Analysis: A more detailed discussion on the implications and necessity of the initial assumptions (Assumptions 14) would be beneficial. Are there practical scenarios where these assumptions might not hold and how would that affect the applicability of the algorithm 3. Comparison with RECAPP: How does the proposed algorithm compare to RECAPP especially in terms of practical deployment and sensitivity to problem parameters  Limitations and Societal Impact: The paper does not discuss the limitations or potential negative societal impacts of the work. Considering the applications in machine learning it would be valuable to discuss the possibility of misuse in adversarial contexts such as deepfakes or security breaches. Constructively the authors could explore safeguards that could be implemented within their algorithm to mitigate such risks.  Improvement Suggestions: 1. Experimental Section: Including an experimental section that validates the theoretical improvements with practical applications would greatly enhance the paper. 2. Discussion on Limitations: A dedicated section discussing any limitations of the proposed method including potential bottlenecks in largescale applications or specific scenario limitations would provide a more balanced view of the work. 3. Societal Impact: Adding a discussion on the ethical considerations and potential for misuse of minimax optimization techniques in adversarial machine learning with proposed guidelines or recommendations for responsible use would be valuable.", "msFfpucKMf": "Review of \"Training Robust Options via Adversarial Reinforcement Learning\" 1. Summary of the Paper This paper introduces an innovative approach to compositional reinforcement learning by proposing a framework and two algorithms designed to train subtask policies robustly. These policies can be combined to perform any sequence of tasks optimally in a worstcase scenario rather than aiming for averagecase performance. The core idea relies on formulating the training process as a twoagent zerosum game where an adversary selects the sequence of subtasks challenging the current policy to adapt to the worstcase task sequence. The paper offers two reinforcement learning (RL) algorithms to tackle this problem: one that adapts existing multiagent RL methods to this new context and another that utilizes asynchrony to train subtask policies in parallel enhancing training efficiency. The effectiveness of these algorithms is demonstrated through experiments in two multitask environments with continuous states and actions where they outperform stateoftheart baselines. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The gametheoretic formulation of the problem and the introduction of adversarial elements into compositional reinforcement learning are highly innovative. This represents a significant conceptual leap in the design of RL algorithms that are robust to worstcase task sequences.  Practical Relevance: By focusing on worstcase scenarios the paper directly addresses the challenges of deploying RL in realworld environments where averagecase performance may not be sufficient for safetycritical applications.  Parallel Training: The asynchronous algorithm introduced for parallel training of subtask policies is a notable technical contribution that could significantly reduce the computational cost of learning in complex environments.  Empirical Evaluation: The comprehensive experiments demonstrate the proposed methods effectiveness and superiority over baselines in relevant task environments lending strong empirical support to the theoretical claims. Weaknesses:  Fixed Set of Subtasks: The approach assumes a predefined set of subtasks which may limit its applicability to environments where the decomposition into subtasks is not clear or where dynamic task generation is necessary.  Computational Complexity: While the asynchronous algorithm aims to reduce computational complexity the overall computational demands of the framework particularly for highdimensional or complex environments are not extensively discussed.  Scalability: Limited evaluation on two specific task environments leaves open questions about the scalability of the proposed methods to more varied or complex multitask environments. 3. Questions and Suggestions for the Authors  Task Decomposition: How does the proposed method perform in scenarios where task decomposition is not straightforward or needs to dynamically adapt Could the authors discuss potential extensions of their framework to accommodate more flexible task decompositions  Comparative Analysis: The paper could benefit from a more detailed comparative analysis with other adversarial training methods in reinforcement learning. How do the proposed algorithms compare to these methods in similar environments  Scalability and Generalization: Can the authors provide more insights into the scalability of their approach to environments with a larger number of subtasks or more complex tasks sequences Additionally an analysis on the generalization capabilities of the trained policies across diverse environments would be valuable.  Computational Resources: Providing details on the computational resources used for the experiments (e.g. number of CPUsGPUs training time) would help assess the practical feasibility of the proposed algorithms. 4. Limitations and Societal Impact While the paper touches upon potential negative societal impacts in general terms a more indepth analysis focusing on specific scenarios where the proposed methods could be misapplied would be beneficial. For instance the use in autonomous weapons or surveillance systems without appropriate safeguards could pose ethical concerns. Additionally the discussion on limitations could be expanded to address issues related to biases in the training data and how they might affect the fairness and transparency of the learned policies. Suggestions for future work could include exploring mechanisms to ensure ethical use and prevent misuse of the technology developed.", "vy7B8z0-4D": "Peer Review for \"A Novel Method for InterSubject Brain Alignment Using Optimal Transport and Fused Unbalanced Gromov Wasserstein (FUGW)\" 1) Briefly summarize the paper and its contributions This paper introduces a new method Fused Unbalanced Gromov Wasserstein (FUGW) to address the challenge of intersubject brain alignment in neuroscience. The traditional approach of coregistering individual brain data to a common template loses individualspecific details due to the coarse alignment process. FUGW advances this by leveraging Optimal Transport to align cortical surfaces based on the similarity of functional signals while also accounting for variations in the size of functional areas across individuals. This method is shown to significantly improve betweensubject correlation of activity for independent functional data leading to more precise grouplevel mappings without relying on hard anatomical constraints. The authors demonstrate the efficacy of FUGW through several experiments including alignment of brain data on individual anatomies and the computation of functional barycenters for groups of individuals. 2) Strengths and weaknesses of the paper Strengths:  The paper addresses a significant challenge in neuroscience related to intersubject variability in brain structure and function which is a barrier to accurate grouplevel analyses.  FUGW represents a substantial technical advancement by combining the strengths of Optimal Transport with the flexibility to account for variance in functional area sizes.  The method is wellvalidated through rigorous experiments including comparisons with traditional methods demonstrating clear improvements in alignment accuracy and functional mapping precision.  The opensource availability of the implementation is commendable facilitating replication and future research. Weaknesses:  Despite impressive technical contributions the paper could better contextualize its practical implications for neuroscience research particularly how it impacts our understanding of brain function or disorders.  The comparison with existing methods while thorough predominantly focuses on quantitative gains in correlation. More nuanced discussions on qualitative differences in resulting alignments and their neuroscientific significance would enhance the paper.  The authors briefly mention potential applications to aligning brains across species but do not provide empirical evidence in this direction. Exploring this aspect even preliminarily would significantly broaden the papers impact.  The limitations section could be expanded to more explicitly address computational requirements and scalability especially when handling large datasets like the Human Connectome Project. 3) Questions and suggestions for the authors  Could the authors provide more details on the computational complexity of FUGW and its scalability particularly when dealing with large datasets  A more indepth discussion on how FUGW could impact the interpretation of functional neuroimaging data in clinical settings or disorder characterization would be valuable. Are there specific conditions or scenarios where FUGWs contributions would be most significant  Exploring the application of FUGW to interspecies brain alignment even if on a preliminary basis would be extremely intriguing. Could the authors comment on any initial findings or planned investigations in this area  The paper would benefit from more detailed discussions on potential challenges or limitations in integrating FUGW into existing neuroimaging analysis pipelines. Are there specific preprocessing steps or data quality requirements that are critical for its successful application 4) Limitations and potential negative societal impact The authors touch on the limitations related to the application of FUGW to interspecies comparisons but could elaborate more on computational limitations and concerns related to overfitting or misinterpretation of aligned signals particularly when functional signals are subtle or complex. Addressing potential risks of erroneous conclusions in disease characterization or misunderstanding of functional brain architecture is crucial. Suggestions for improvement include a more robust discussion on the validation steps needed to ensure accurate interpretation of aligned data and recommendations for safeguarding against overinterpretation of results. Highlighting these points would not only strengthen the paper but also ensure responsible advancement in the field.", "x_HUcWi1aF1": "Peer Review: 1) Brief Summary: This paper presents an innovative approach to offline multiagent reinforcement learning (MARL) introducing a strategywise concentration principle to address the exponential dependency problem associated with the number of players in the joint action space. For twoplayer zerosum Markov games the authors propose a computationally efficient algorithm leveraging the convexity of the strategywise bonus to improve sample complexity related to the number of actions. Additionally for offline multiagent generalsum Markov games a novel algorithm is presented showcasing a sample complexity that scales linearly with the action size of each player and the number of players thus effectively mitigating the curse of multiagents. The paper also explores incorporating a prespecified strategy class into their algorithms leading to an adaptive sample complexity bound that depends logarithmically on the covering number of the strategy class. This work contributes by theoretically grounding offline MARL and proposing methodologies to overcome significant challenges in scalability and complexity. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty and Theoretical Contributions: The introduction of the strategywise concentration principle and its application to both zerosum and generalsum MARL settings is a significant theoretical advancement. It addresses a critical challenge in offline MARL reducing the dependency on the size of the joint action space which is known as the curse of multiagents.  Improved Sample Complexity: The paper presents algorithms that not only improve on existing methods in terms of sample complexity but also demonstrate how these complexities can be further reduced through the incorporation of prespecified strategy classes.  Broad Applicability: This work has the potential for wideranging applications from gaming to autonomous systems where offline MARL methods are preferable due to constraints on data acquisition. Weaknesses:  Computational Efficiency: While the paper addresses computational efficiency particularly for twoplayer zerosum games it acknowledges computational challenges associated with generalsum games. The paper suggests that future work could explore more computationally efficient algorithms indicating that the current solutions might not be readily applicable to largescale problems.  Empirical Validation: The paper primarily focuses on theoretical contributions and does not include empirical studies to validate the proposed algorithms. Including empirical results could strengthen the arguments and demonstrate the practical effectiveness of the algorithms.  Discussion on Societal Impact: While the paper briefly mentions potential negative societal impacts in its limitations section these discussions are not deeply explored. Given the broad applicability of MARL a more thorough analysis of potential negative repercussions including misuse and ethical considerations would be beneficial. 3) Questions and Suggestions for the Authors:  Empirical Evaluation: Can the authors provide empirical results or case studies to demonstrate the practical effectiveness of the proposed algorithms in realworld scenarios  Computational Efficiency in Generalsum Games: Are there specific strategies or approximations that could be explored to improve the computational efficiency of the algorithms for generalsum games  Societal Impact Discussion: Could the authors expand on the potential negative societal impacts of their work Are there safeguards or ethical guidelines that could be recommended to mitigate these risks  Extension to Function Approximation: The paper concludes with the suggestion for future work on MARL with function approximation. Could the authors speculate on how the strategywise concentration principle might be adapted or extended to settings with function approximation 4) Limitations: The authors have made a commendable effort in addressing the limitations of their work especially by pointing out the computational efficiency challenges in multiplayer generalsum Markov games and the absence of empirical validation. However the discussion on potential negative societal impacts is somewhat lacking. Given the increasing scrutiny on the ethical use of AI and machine learning technologies it would be beneficial for the authors to provide a more thorough examination of these aspects perhaps suggesting frameworks or principles for ethical deployment. Additionally incorporating empirical evaluations to illustrate the performance and applicability of the proposed algorithms in realworld scenarios could significantly strengthen the paper.", "rnJzy8JnaX": "Review of \"Leveraging LowResolution Frames for Efficient Video Recognition via CrossResolution Knowledge Distillation\" 1) Summary of the Paper: This paper presents an empirical investigation into the use of lowresolution frames for efficient video recognition. Previous work in the field primarily focuses on compact network design or reducing temporal redundancy to enhance efficiency. However the potential of leveraging lowresolution frames despite its intuitive benefit in reducing computational overhead has been relatively underexplored due to concerns over accuracy loss. The authors argue that the degradation in recognition performance at lower resolutions is not mainly due to the loss of information through downsampling but rather the mismatch between the network architecture and the input scale. They propose a method termed CrossResolution Knowledge Distillation (ResKD) to bridge this gap whereby a highresolution teacher network guides the training of a lowresolution student network. The methodology is validated across various architectures and datasets showing notable improvements in efficiency and accuracy over stateoftheart competitors without considerable complexity additions. 2) Assessment of Strengths and Weaknesses: Strengths:  The paper tackles an important but underexplored problem in video recognition opening up a new avenue for research in efficient video processing.  The empirical analysis provided is thorough examining the root causes of performance degradation at lower resolutions and disproving common assumptions regarding information loss.  The proposed ResKD method is both simple and innovative effectively leveraging the existent knowledge distillation framework but adapting it to a novel application in crossresolution learning.  The extensive experimental validation demonstrates the methods efficacy across multiple datasets and stateoftheart architectures underscoring its generalizability and scalability. Weaknesses:  While the paper discusses the theoretical motivations behind why ResKD works it could benefit from a deeper exploration into the theoretical underpinnings of crossresolution knowledge transfer.  The potential impact of ResKD on varying content types within videos (e.g. text fastmoving objects) is not explicitly discussed which could be critical for understanding its applicability to realworld tasks.  The societal impacts both positive and negative of deploying highly efficient but potentially less accurate video recognition models are not addressed. 3) Questions and Suggestions for the Authors:  Could the authors elaborate on any theoretical foundations that might explain the observed effectiveness of ResKD in bridging the networkinput scale mismatch  It would be interesting to see an analysis of how ResKD performs on different types of video content such as those with fine details or rapid movements to assess its broader applicability.  A discussion on the potential for model size and complexity reduction using ResKD beyond just efficiency improvements could provide additional insights into its benefits.  Considering the broader implications of more efficient video recognition models (e.g. surveillance) could the authors discuss any ethical considerations or possible adverse societal impacts 4) Limitations and Potential Societal Impact: The authors do not extensively address the potential negative societal impacts of their work. While efficiency improvements in video recognition can lead to beneficial applications like better content accessibility or energy savings they might also enable pervasive surveillance and privacy intrusions when deployed at scale. Constructive suggestions for improvement include:  Incorporating a section dedicated to discussing the dualuse nature of video recognition advancements highlighting measures to mitigate potential misuse.  Proposing guidelines or frameworks for responsible deployment of efficient video recognition technologies especially in sensitive contexts. Overall this paper represents a significant step forward in the pursuit of efficient video recognition solutions emphasizing the untapped potential of lowresolution frames when coupled with thoughtful networkinput scale alignment.", "zofwPmKL-DO": " Peer Review  1) Briefly summarize the paper and its contributions This paper presents quantum algorithms for logconcave sampling and estimating the normalizing constants of logconcave distributions which have wide applications in machine learning statistics physics and more. The authors develop quantum versions of underdamped Langevin diffusion and the Metropolisadjusted Langevin algorithm (MALA) demonstrating quantum speedups for these classical sampling techniques. They achieve matching query complexities with classical counterparts while using zerothorder oracles instead of firstorder and in the case of estimating normalizing constants they accomplish a quadratic speedup in precision. Furthermore they prove a nearoptimal quantum lower bound for normalizing constant estimation highlighting the efficiency of their quantum algorithms.  2) Thorough assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The paper introduces novel quantum algorithms that significantly improve upon classical approaches to logconcave sampling and normalizing constant estimation particularly highlighting the efficiency of using zerothorder oracles over firstorder.  Comprehensive Analysis: Detailed and rigorous proof work demonstrates the quantum speedup matching and surpassing classical query complexities and establishing theoretical lower bounds offering a strong theoretical foundation for the proposed algorithms.  Wide Applicability: Given the broad range of applications for logconcave sampling from machine learning to statistical mechanics the algorithms presented here have significant potential for impact across various fields.  Detailed Comparison: The authors provide meticulous comparisons between their quantum algorithms and existing classical methods clearly delineating the improvements offered by their approach. Weaknesses:  Practical Implementation Concerns: The paper primarily focuses on theoretical advancements and does not address the practical implementation of these quantum algorithms especially considering the current state of quantum computing hardware.  Assumption Specificity: The algorithms rely on specific conditions (e.g. Lsmoothness and \u00b5strong convexity of the function f) which might limit their applicability to a narrower range of realworld scenarios than suggested.  Quantum Hardware Requirements: There is a lack of discussion about the quantum resources (e.g. number of qubits coherence times) required to realize these algorithms which is vital for assessing their nearterm feasibility.  3) Questions and suggestions for the authors  Hardware Feasibility: Could the authors elaborate on the practical feasibility of implementing the proposed quantum algorithms with current or nearfuture quantum computing technology  Application Examples: Providing specific examples or case studies where these quantum algorithms could be applied would enhance the papers impact by illustrating their practical utility.  Robustness Analysis: How sensitive are the algorithms performances to quantum noise and errors An analysis considering noisy intermediatescale quantum (NISQ) technology would be beneficial.  Extension to Nonlogconcave Distributions: Is it possible to extend the approach to nonlogconcave distributions and if so what would be the anticipated challenges and potential solutions  4) Limitations and societal impacts The authors briefly discuss potential longterm applications and the lack of immediate negative societal impacts of their work. However a more thorough analysis of potential negative implications especially in terms of privacy security and potential misuse in sensitive applications (e.g. surveillance decisionmaking algorithms with societal biases) could strengthen the discussion. Constructive suggestions for improvement include:  Limitations: A more detailed discussion on the limitations regarding the assumptions made for the theoretical analysis and the potential barriers to practical implementation.  Societal Impacts: A broader contemplation of both the positive and negative societal impacts including ethical considerations and recommendations for responsible development and deployment of such quantum algorithms in the future. In summary while the paper presents significant theoretical advancements in quantum algorithms for logconcave sampling and estimating normalizing constants bolstering the analysis with discussions on practical feasibility broader application scenarios and a careful consideration of societal impacts would enhance its contribution.", "yipUuqxveCy": "Peer Review: 1. Summary of the Paper: The paper introduces a novel approach to offline multiagent reinforcement learning (offline MARL) by transforming it into a sequence modeling problem. The proposed method leverages the Transformer architecture for centralized training with a teacher policy that accesses all agents observations actions and rewards. This policy identifies and recombines beneficial behaviors in the dataset. Subsequently individual student policies are trained through policy distillation incorporating both the teachers features and the structural relations among multiple agent features. The method surpasses stateoftheart offline MARL baselines in various tasks demonstrating improvements in performance convergence rate sample efficiency and robustness to demonstration quality. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovation: The papers approach to reinterpret offline MARL as a sequence modeling problem is innovative and original. Utilizing a Transformerbased teacher policy for centralized training and then distilling knowledge into decentralized student policies is a clever strategy.  Performance: The empirical results show that the method outperforms existing offline MARL approaches across a range of benchmarks which is a significant achievement.  Detail in Methodology: The authors provide a comprehensive description of their approach including algorithms and detailed experimental setup which adds to the papers credibility.  Addressing a Significant Challenge: By enabling offline MARL without further online data collection the paper addresses a major hurdle in applying MARL to realworld scenarios especially in cases where online data collection is impractical or risky. Weaknesses:  Scalability Concerns: While the paper briefly discusses scalability the reliance on a centralized decision transformer could limit the methods effectiveness as the number of agents increases significantly. More thorough investigation and solutions to scalability would enhance the papers impact.  Limited Discussion on Societal Impact: Although the paper mentions potential negative applications such as surveillance it does not thoroughly discuss the broader societal implications of offline MARL technologies. A more detailed analysis in this area would be valuable.  Dependency on Quality of Offline Data: The performance of the proposed method is somewhat dependent on the quality of the offline dataset as noted in the experiment with poor demonstrations. Strategies to mitigate this dependency could improve the robustness of the approach. 3. Questions and Suggestions for the Authors:  On Scalability: Could you elaborate on potential strategies or modifications to the current framework that might address the scalability issue as the number of agents increases  On Societal Impact: It would be beneficial to include a deeper discussion on the ethical considerations and potential negative impacts of applying offline MARL in realworld scenarios. How can these risks be mitigated  On Data Dependency: Are there any methodologies or enhancements you foresee that could make the proposed method less reliant on the quality of the offline dataset Exploring techniques to augment or synthesize highquality demonstrations might be an avenue worth investigating. 4. Limitations and Improvement Suggestions: The paper could benefit from a more thorough discussion on its limitations particularly regarding scalability and the reliance on data quality. Addressing these concerns with potential solutions or future research directions would provide a more balanced view. Additionally expanding the conversation around the societal impacts and ethical considerations of offline MARL would underline the significance of responsible research in this area. Possible improvements could include designing more scalable architectures investigating methods to enhance or synthesize offline datasets and developing ethical guidelines for the application of MARL technologies.", "lhLEGeBC-ru": "Review of the paper titled: \"On Polynomial Time Guarantees for the BurerMonteiro Method in Smoothed Analysis Setting\" 1) Summary of the paper and its contributions: This paper addresses the utilization of the BurerMonteiro method in solving largescale semidefinite programs (SDPs) within polynomial time under a smoothed analysis framework. The authors investigate the efficacy of the BurerMonteiro approach when the number of variables in the nonconvex program represented by \\(p\\) slightly exceeds the square root of twice the number of constraints \\(m\\). Key contributions include proving that this method can achieve any desired accuracy in polynomial time given minor perturbations to the input thereby approaching the BarvinokPataki bound as the fixed constant \\(\\eta\\) tends to zero. The papers novelty lies in leveraging sophisticated tools from real algebraic geometry to approximate the prevalence of critical points and employing these to establish concentration bounds. Substantially the paper fosters the understanding of how the BurerMonteiro method can be practically applied to SDPs surpassing previous limitations regarding polynomial time guarantees and feasibility concerns. 2) Thorough assessment of strengths and weaknesses: Strengths:  The paper pioneers in providing polynomial time guarantees for solving SDPs using the BurerMonteiro method under the smoothed analysis setting. This significantly advances our understanding and applicability of the method in practical circumstances.  Utilizing real algebraic geometry for deriving concentration bounds is a creative approach that not only enhances the papers technical depth but also opens avenues for future research in this direction.  The comprehensive analysis including the establishment of a 2stage procedure for solving SDPs and polynomial time guarantees for both stages contributes to a robust theoretical framework.  The experimental section adequately supports theoretical claims offering empirical evidence of the method\u2019s efficacy across different problem settings. Weaknesses:  The paper assumes certain smoothness and compactness in its main results which might limit the applicability of the findings to more general or adversarial settings of SDPs.  While the experiments validate the methods performance a broader exploration of varying problem scales and additional comparisons with other existing methods might have offered deeper insights into its practical advantages and limitations.  The paper\u2019s contributions while novel are presented in a highly technical manner that could potentially deter a wider audience including practitioners from fully grasping the implications of the findings. 3) Questions and suggestions for the authors:  Could the authors discuss the potential impact of their findings on more general potentially nonsmooth settings of SDPs It would be insightful to understand the limitations and possibilities of extending the current analysis beyond the assumption of smoothness and compactness.  In relation to the experimental evaluation it would be beneficial if the authors could include a comparison with stateoftheart SDP solvers or algorithms that do not rely on the BurerMonteiro method. This could provide a clearer picture of where the proposed method stands in the broader SDP solver landscape.  A discussion on the practical implications of the choice of the parameter \\(p\\) and how it might affect the convergence and computational complexity in realworld applications would be valuable. Additionally insights into selecting or optimizing \\(p\\) in practice would enhance the papers applicability. Limitations and Potential negative societal impact: The authors have presented a foundational study with no immediate direct societal impacts discussed. However given the broad applicability of SDPs across various domains including those with significant societal impacts such as finance engineering and machine learning it is imperative to consider the broader implications. It would be constructive if future work could reflect on how advancements in solving SDPs might influence decisionmaking in these fields especially considering the ethical and societal ramifications of automated decision systems. Improvement could be made by including a discussion on potential misuse or ethical considerations inherent in the optimization problems solvable by the proposed method.", "sADLRl2STMe": "The paper under review investigates the interplay between explicit penalties and adaptive gradient optimizers in the context of matrix completion tasks with a focus on deep linear networks (DLNNs) and their implicit regularization properties. The authors propose a novel explicit penalty based on the ratio of the nuclear norm to the Frobenius norm of a matrix aiming to replicate the effect of implicit regularization observed in DLNNs. The study explores the impact of this penalty on the learning dynamics and generalization performance of singlelayer networks when used in conjunction with specific optimizer settings notably Adam. Their findings suggest that this approach can produce lowrank solutions with generalization performance comparable to or even exceeding that of deeper networks challenging the prevailing belief in the necessity of network depth for effective learning in such tasks.  Strengths: 1. Novelty and Relevance: The paper addresses a highly relevant question in the field of deep learning focusing on the understudied area of the explicit regularizations role alongside optimizer inductive biases. The proposed penalty is both novel and rooted in extensive prior work offering a fresh avenue for research on regularization techniques. 2. Empirical Rigor: The experiments are comprehensive and methodologically sound covering a broad range of scenarios to thoroughly evaluate the proposed penaltys effectiveness across different depths and optimizer settings. This includes a detailed comparison with unpenalized models models trained with different optimizers and varying network depths providing a robust empirical foundation for the conclusions. 3. Insights into Optimization Dynamics: The paper provides valuable insights into the learning dynamics under different regularization and optimization conditions particularly highlighting the role of depth and the interaction between explicit penalties and optimizer biases. This contributes to a deeper understanding of the factors influencing model generalization and optimization trajectories in DLNNs.  Weaknesses: 1. Theoretical Underpinning: While the empirical results are strong the paper could benefit from a more rigorous theoretical analysis of the proposed penalty. Specifically a clearer explanation of why and how this penalty interacts with adaptive gradient optimizers like Adam to produce the observed effects would strengthen the contribution. 2. Generalizability to Nonlinear Networks: The focus on DLNNs limits the broader applicability of the findings. While the authors mention this as a potential area for future work some initial investigation or discussion on the implications for nonlinear networks would have been valuable. 3. Comparison with Existing Regularization Methods: Although the paper provides a comparative analysis of the proposed penalty against unregularized models and briefly mentions other regularization techniques a more detailed comparison with existing explicit regularization methods could provide a clearer understanding of the novelty and utility of the proposed approach.  Questions and Suggestions for the Authors: 1. Theoretical Clarification: Can the authors provide additional theoretical intuition or analysis regarding the specific choice of the penalty ratio and its expected interactions with adaptive optimizers 2. Extension to Nonlinear Networks: Have the authors considered extending their analysis to nonlinear networks If so preliminary results or hypotheses about how the proposed penalty might perform in such settings would be insightful. 3. Comparison with Other Regularization Techniques: Could the authors expand on the comparison between their proposed penalty and existing regularization methods such as weight decay or other normbased penalties to highlight the unique contributions and benefits of their approach  Limitations and Societal Impact: The paper acknowledges the focus on DLNNs and suggests expanding to nonlinear networks in future work which is a fair limitation. However theres limited discussion on the potential negative societal impacts of the findings. For instance improved efficiency in learning could potentially lead to more pervasive use of deep learning models raising concerns around energy consumption privacy and ethical use of AI. The paper could benefit from a brief discussion on these aspects along with suggestions for mitigating potential adverse effects such as emphasizing the importance of responsible AI practices and the development of efficient computing techniques. Overall this paper makes a significant contribution to the understanding of explicit regularization and its interaction with optimizer inductive biases in the context of matrix completion tasks. With additional theoretical insights and broader exploration of its applicability this work could pave the way for novel approaches to designing more efficient and effective regularization strategies in deep learning.", "kvtVrzQPvgb": " Review of \"A New Procedure to Aggregate Multitask Performances in NLP Systems\"  Summary The paper presents a novel approach to aggregate performance scores from multiple tasks in Natural Language Processing (NLP) systems. The traditional meanaggregation method has limitations especially when dealing with metrics on different scales or unbounded metrics. The authors propose a ranking aggregation procedure based on the Kemeny consensus from social choice theory. This method uses rankings induced by each tasks performance to produce a final system ordering that is both theoretically grounded and empirically robust. The paper demonstrates through both synthetic and realworld examples (over 270k scores across benchmarks like GLUE XTREM and SEVAL) that this new approach leads to different conclusions about stateoftheart systems compared to the meanaggregation method with evidence of higher reliability and robustness. They also release their code and data to encourage adoption of their evaluation method.  Assessment Strengths: 1. Theoretical Foundation: Employing Kemeny consensus for aggregation is a strong theoretical choice catering to the nuances of multitask evaluation in NLP. 2. Comprehensive Evaluation: The extensive set of experiments both synthetic and real thoroughly test the proposed methods claims demonstrating its effectiveness over traditional approaches. 3. Practicality: The release of code and data is a commendable decision that supports transparency and facilitates reproducibility. Weaknesses: 1. Complexity and Efficiency: The paper does not delve into the computational complexity of the proposed ranking procedure especially important given the NPhard nature of the Kemeny consensus. 2. Discussion on Task Weights: While the method treats tasks equally in practical scenarios some tasks might be more important than others. The paper lacks discussion on handling weighted aggregation which might be critical for certain applications. 3. Validation on Other Domains: Although the methodology has potential across domains its validation is limited to NLP. Insights into its applicability and performance in fields like computer vision or audio processing would have been valuable.  Questions and Suggestions 1. Computational Efficiency: Can the authors comment on the computational efficiency of their proposed method particularly in comparison to traditional meanaggregation 2. Handling Task Importance: How might one incorporate the notion of task importance or weights into the proposed ranking aggregation framework 3. Domain Generalization: What steps if any are needed to adapt this ranking procedure for other domains (e.g. computer vision) The authors are encouraged to discuss potential challenges and solutions.  Limitations and Societal Impact While the paper provides a convincing argument for the proposed methods effectiveness in NLP benchmarks it lacks a detailed analysis of its computational efficiency. In applications where quick benchmarking is crucial this could be a potential hurdle. Additionally the equal weighting of tasks in the aggregation process might not suit all applications and guidance on incorporating task significance could improve the methods utility. The paper does not directly address the potential negative societal impacts of its work. However it is crucial to recognize that benchmark performance while valuable is not the sole measure of a systems utility or its potential impact on society. Future works could benefit from a discussion on how performance rankings intersect with ethical considerations such as fairness and inclusivity in NLP systems. Suggestions for Improvement: 1. Provide a detailed analysis of the computational requirements of the proposed method and strategies to scale it efficiently. 2. Include discussion and potentially preliminary methods for incorporating task weights or importance into the ranking aggregation to allow for more nuanced evaluations. 3. Expand on the applicability of the method outside NLP to understand its limitations and necessary adaptations fully. Overall this paper makes significant contributions to the evaluation of NLP systems in a multitask setting offering a robust alternative to meanaggregation with strong theoretical backing and empirical support.", "pyLFJ9TBZw": "Review of \"Generative Multitask Learning for Robustness to Target Shift in Causal Machine Learning\" 1) Summary: The paper introduces Generative Multitask Learning (GMTL) a novel methodology to enhance robustness to target shift by accounting for causal relationships in multitask learning environments. GMTL adapts the inference objective within multitask learning by conditioning on all targets to mitigate the influence of targetcausing confounders  variables that impact the targets but not the input. This adaptation allows GMTL to reduce the spurious dependencies between input and targets likely improving outofdistribution (OOD) generalization. The authors conduct experiments on the Attributes of People and Taskonomy datasets demonstrating the effectiveness of GMTL across various multitask learning methods. Results indicate improved robustness to target shift when compared to conventional multitask learning approaches. 2) Strengths and Weaknesses:  Strengths:  Novelty: The proposal of adjusting the inference objective to enhance robustness to target shift in multitask learning settings is innovative.  Methodological Rigor: The paper thoroughly explains the theoretical underpinnings of GMTL and its basis in causal inference which is wellsupported by empirical results.  Practicality: The fact that GMTL can be incorporated with existing multitask methods without requiring additional training is highly practical for realworld applications.  Empirical Validation: Comprehensive experiments across two distinct datasets and various multitask learning methods strengthen the evidence for GMTLs efficacy.  Weaknesses:  Limitations and Societal Impact Discussion: The paper does not thoroughly discuss potential limitations or the broader societal implications of applying GMTL in different contexts.  Hyperparameter \u03b1 Tuning: Although introduced as a crucial component for balancing between ID and OOD performance the paper provides limited insight into practical guidelines for choosing \u03b1 beyond a heuristic approach.  Scalability Concerns: While experiments demonstrate the effectiveness of GMTL on two datasets there is a lack of discussion on scalability and performance on vastly larger multitask learning problems or datasets with a higher degree of complexity. 3) Questions and Suggestions for Authors:  Limitations Discussion: Can the authors elaborate on potential limitations of GMTL particularly regarding scalability to largescale multitask learning problems  Societal Impact: Could the authors discuss any broader societal implications of improving robustness in machine learning through causal inference Are there specific areas where this approach might have a significant impact or unintended consequences  Practical Application: Are there practical scenarios or applications where GMTL has been tested or could be especially beneficial Insights on this could increase the papers relevance to practitioners.  Hyperparameter \u03b1: Could the authors provide more detailed guidance on how to choose the hyperparameter \u03b1 in different scenarios possibly supported by additional empirical evidence 4) Limitations and Societal Impact: The authors do not fully address the limitations related to the scalability of GMTL and its application across a broader spectrum of multitask learning problems. Moreover the discussion on the potential negative societal impact is missing. Understanding the implications of mitigating targetcausing confounders is vital especially in sensitive applications like healthcare or criminal justice where robustness and fairness are critical. Suggestions for Improvement:  Expand on Limitations: A section dedicated to addressing potential scalability issues and how GMTL performs with an increasing number of tasks or in more complex scenarios would be beneficial.  Broader Societal Implications: Include a thoughtful discussion on the societal impacts of applying GMTL in various domains considering both positive outcomes and potential risks.  Enhance Guidance on \u03b1 Selection: Provide a more comprehensive framework or empirical analysis for selecting the hyperparameter \u03b1 to aid practitioners in applying GMTL effectively.", "pkfpkWU536D": " Peer Review for \"Neural Shape Deformation Priors\"  1. Summary of Contributions The authors present a novel approach Neural Shape Deformation Priors (NSDP) for the task of shape manipulation which predicts mesh deformations of nonrigid objects using userdefined handle movements. Unlike previous methods that rely on optimization tasks guided by handcrafted regularizers NSDP leverages transformerbased networks to model deformation behavior based on the geometric properties of shapes. This is achieved by predicting a continuous deformation field from largescale datasets containing diverse nonrigid deformations. The key contributions include (i) a transformerbased architecture for learning local deformation fields based on geometric properties and user input (ii) a canonicalization process for handling source meshes in arbitrary poses and (iii) demonstrating superior performance over classical and recent methods on the DeformingThing4D dataset especially in challenging deformation scenarios and generalization to unseen deformations.  2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The use of transformerbased networks to learn deformation priors based on geometric properties represents a significant advance over traditional optimization and recent neural networkbased methods.  Versatility: The methods ability to handle source meshes in arbitrary poses through a canonicalization process is a valuable feature allowing for a wider range of applications.  Generalizability: NSDP demonstrates strong generalizability to new deformations and unseen identities as shown in the experimental comparisons and ablation studies. Weaknesses:  Dataset Limitations: The primary dataset DeformingThing4D while effective for demonstration purposes may not fully capture the spectrum of realworld deformations. The authors acknowledge this but more discussion on addressing this limitation in future work would be beneficial.  User Handle Restrictions: The training depends on handles selected from candidate parts limiting user interaction. Expanding handle selection could improve usability and applicability.  Potential for Misuse: While briefly mentioned the societal impact section could benefit from a more detailed exploration of potential negative impacts including mitigation strategies.  3. Questions and Suggestions for the Authors 1. Expanding Handle Placement: Can you discuss potential methods for allowing users to freely place handles anywhere on the mesh during training This could significantly enhance the interactive editing capabilities of NSDP. 2. Dealing with Extreme Deformations: Given the dataset limitations how well does NSDP handle extreme deformations or highly irregular shapes not represented in the training data 3. Potential Biases: Could there be inherent biases in the training datasets that affect the models generalization capabilities If so how might these be identified and mitigated  4. Limitations and Societal Impact While the authors acknowledge the limitations of the DeformingThing4D dataset and the handle selection process further discussion on strategies to extend the model\u2019s generalization capabilities to a broader set of realworld applications would be valuable. Additionally a more thorough consideration of the potential negative societal impacts including the generation of fraudulent or offensive content is needed. Concrete suggestions for improvement could include developing more diverse and challenging datasets and implementing safeguards or usage guidelines to mitigate misuse. In summary Neural Shape Deformation Priors represents a significant step forward in the domain of 3D shape manipulation offering promising directions for future research and application in creative industries. However addressing the identified limitations and potential negative impacts will be crucial for realizing its full potential.", "lxdWr1jN8-h": "The manuscript titled \"Exploring Symmetries in Robotic Path Planning with Steerable CNNs\" presents a novel approach to leverage group symmetries for improving data efficiency and generalization performance of endtoend differentiable algorithms in 2D robotic path planning tasks navigator and manipulation. The authors propose a paradigm named Symmetric Planning (SymPlan) built on the insights from Value Iteration Networks (VINs) and geometric deep learning to harness convolutional operations intrinsic ability to incorporate symmetry directly into planning algorithms. Implementing the idea through steerable convolution networks the work demonstrates significant improvements over nonequivariant counterparts in terms of training efficiency and generalizational results on navigation and manipulation tasks.  Strengths: Innovative Framework: The manuscript delivers an original perspective on integrating group symmetries in path planning through steerable CNNs filling a gap in current literature where symmetry exploitation has been limited and less direct. Solid Theoretical Insights: The authors provide a comprehensive theoretical foundation connecting value iteration in path planning to steerable CNNs and demonstrate that value iteration inherently possesses equivariance to symmetries found in 2D planning problems. Impressive Performance Gains: The empirical results showcase notable enhancements in data efficiency and generalization substantiated with thorough experiments across different tasks and settings. Broad Relevance: The proposed approach is beneficial to the wider field of robotic planning reinforcement learning and any domain where symmetry can serve as an important inductive bias to improve learning algorithms performance and efficiency.  Weaknesses: Generalization beyond 2D Path Planning: While the paper provides valuable insights within the context of 2D path planning the discussion on extending the proposed framework to higherdimensional spaces or different planning problems (beyond navigating and manipulating in 2D environments) is somewhat limited. Computational Overhead Concerns: While the method shows significant improvements in training efficiency and generalization there is little discussion on the computational overhead introduced by steerable CNNs which may be relevant for practical deployment scenarios especially in resourceconstrained settings. Assumption Clarity: The assumptions made for adopting steerable CNNs and their specific configurations (e.g. choosing specific representations for the steerable feature fields) could be detailed more extensively to understand their impact on the frameworks applicability and performance.  Questions and Suggestions: 1. Extension to Higher Dimensions: Could the authors elaborate on potential modifications required to extend SymPlan to higherdimensional or more complex path planning problems beyond the 2D grid environment 2. Computational Efficiency: It would be beneficial if the authors could discuss the computational costs associated with steerable CNNs compared to nonequivariant counterparts considering both training time and inference. 3. Representation Choices: The decision criteria for choosing between trivial regular and quotient representations for different components of the SymPlan framework appear implicit. Could the authors provide insights or guidelines on making these choices  Limitations and Societal Impact: The paper somewhat addresses its limitations regarding the scope of direct applicability (primarily to 2D path planning problems). However a more thorough discussion on computational overhead and generalization to nongrid higherdimensional spaces would strengthen this section. Regarding societal impact while the work has promising implications for robotic automation and efficiency which could lead to productivity increases and safety enhancements a discussion on the potential for displacement in employment or ethical considerations in autonomous decisionmaking applications would be appropriate.  Constructive Suggestions for Improvement:  A deeper analysis on extending the framework to nongrid environments or 3D space including theoretical and practical challenges.  Adding a comprehensive comparison in terms of computational costs possibly with benchmarks on computational resources required.  Elaborating on the choice of representations in steerable CNNs potentially with a case study highlighting how different choices affect performance. In summary this manuscript presents a compelling and innovative approach to leveraging symmetries in robotic path planning supported by robust theoretical insights and empirical evidence. Addressing the discussed questions and suggestions could further strengthen the papers contributions and broader relevance.", "tvDRmAxGIjw": "The paper presents a novel approach to posttraining quantization (PTQ) of PreTrained Language Models (PLMs) specifically focusing on overcoming the challenges associated with QuantizationAware Training (QAT). The authors introduce Modulewise Reconstruction Error Minimization (MREM) a method that partitions a PLM into multiple modules and minimizes reconstruction error for each module independently. They also propose a parallel training strategy to further accelerate the process without the need for endtoend backpropagation over the full dataset. The approach significantly reduces training time memory overhead and data requirements while closely matching the performance of QAT methods as demonstrated empirically on the GLUE and SQuAD benchmarks. Strengths: 1. Novel Approach: The paper introduces an innovative approach to PTQ that addresses significant limitations of existing QAT methods such as high computational and memory cost and the need for full data access. 2. Efficiency and Performance: The parallel training strategy coupled with modulewise optimization significantly accelerates the quantization process. At the same time it maintains performance levels close to those achieved with QAT making a compelling case for practical applications. 3. Empirical Validation: The authors thorough empirical study involving experiments on wellknown benchmarks like GLUE and SQuAD provides strong evidence of the effectiveness and efficiency of the proposed method. Weaknesses: 1. Module Dependency: The paper could explore in more detail the impact of modulewise partitioning on the overall performance especially concerning different types of architectures or deeper models. The dependency between modules and its effect on error propagation are not fully explored. 2. Generalizability: While the results are promising the experiments focus on BERTbased models. It remains unclear how the approach generalizes to other PLMs or neural network architectures beyond Transformers. 3. Comparison with Stateoftheart: The paper compares its method primarily against QAT and conventional PTQ approaches. A broader comparison including the latest advancements in efficient PLM quantization could provide a clearer picture of the methods standing. Questions and Suggestions for the Authors: 1. Module Configuration: How does the method perform with different configurations (e.g. number and size) of modules It would be beneficial to include insights or guidelines on optimal module configuration for various model architectures. 2. Error Propagation: Could the authors elaborate on the mechanism of error propagation between modules in the parallel training scheme Suggestions on mitigating adverse effects if any could be valuable. 3. Generality and Adaptability: Can the authors discuss the methods applicability to other model architectures or tasks beyond language understanding For instance would similar strategies apply to models like GPT or Transformerbased models in vision tasks 4. Hardware Efficiency: The paper demonstrates significant reductions in training time and memory usage. However insights into the methods performance across different hardware setups (GPUs TPUs) and potential optimizations could further highlight its practical benefits. Limitations and Societal Impact: The paper briefly touches upon the limitations of the approach mainly focusing on the scope of experiments. A more indepth discussion on potential limitations such as scalability to extremely large models or adaptability to rapidly evolving model architectures would be beneficial. Regarding potential negative societal impacts the paper does not delve deeply into this area. Given the significant efficiency improvements one potential concern could be the accelerated deployment of powerful PLMs in applications with ethical or privacy implications. Suggestions for responsible deployment and use of quantized models could enhance the papers consideration of societal impacts.", "u_7qyNFwkP8": "The paper presents a comprehensive study on the query complexity of achieving fair cake cutting focusing on the standard RobertsonWebb (RW) query model. It investigates three fairness notions\u2014envyfree perfect and equitable allocations\u2014and establishes lower and upper bounds for computing such allocations with a minimal number of cuts. Notably the paper offers tight lower bounds for contiguous envyfree allocations among three players and for perfect and equitable allocations between two players. Additionally it explores the realm of approximate fairness (\u03b5fairness) and significantly extends the understanding of the complexity involved in achieving \u03b5fair allocations. Furthermore the paper makes a novel contribution by formalizing moving knife procedures within the RW model demonstrating that many known procedures can be efficiently simulated with small error. Strengths: 1. Theoretical Depth and Rigour: The paper provides strong theoretical contributions by establishing new lower and upper bounds for the query complexity associated with achieving various fairness notions in cake cutting. The approach is mathematically rigorous and the proofs are detailed contributing to a better understanding of the problems complexity. 2. Broadening the Framework: By formalizing moving knife procedures within the RW model the paper broadens the theoretical framework for studying cakecutting problems. This allows for a more comprehensive analysis that bridges continuous and discrete models enhancing the versatility of the model for different applications. 3. Practical Implications: By exploring \u03b5fair allocations the paper addresses the practical challenges of achieving exact fairness which is often infeasible. This has significant implications for realworld applications where fair division is required but perfect fairness is unattainable. Weaknesses: 1. Assumption on Valuation Functions: The paper assumes that valuation functions are induced by nonnegative integrable functions over the cake. While this covers a wide range of scenarios it might not encompass all realworld applications particularly those involving more complex preferences. 2. Complexity for Larger Number of Players: Although the paper provides insightful results for three players and some bounds for a larger number of players it leaves open the question of the exact complexity for four or more players in certain fairness notions. This indicates that there might be room for further exploration and possibly tighter bounds. Questions and Suggestions for the Authors: 1. Extensions to More Players: Can the techniques introduced in the paper be extended to provide tighter bounds for the query complexity involving more than three players especially for envyfree or perfect allocations It would be beneficial to understand whether the complexity increases significantly with each additional player. 2. Handling More Complex Valuations: How might the model adapt to more complex valuation functions such as those involving lexicographic preferences or those that are not integrable Expanding the model to accommodate these cases could increase its applicability. 3. Experimental Validation: While the theoretical analysis is comprehensive have there been any empirical tests or simulations to validate the proposed bounds and procedures Doing so could strengthen the papers claims and provide insight into the practical performance of the algorithms. Limitations and Societal Impact: The authors have made strides in addressing the limitations inherent to the RW model and approximate fairness concepts. However further clarification on the assumptions regarding valuation functions and their implications for realworld scenarios would be valuable. While the paper does not delve deeply into potential negative societal impacts it is important to consider the fairness notions applicability to sensitive areas such as resource allocation in humanitarian contexts where algorithmic fairness might have profound ethical and societal repercussions. For improvement the authors could explore broader classes of valuation functions and investigate the algorithms performance in diverse practical scenarios to ensure the findings robustness and applicability.", "w4X7GLThiuJ": " Peer Review  1) Brief Summary This paper explores the domain of constrained twoplayer bilinear zerosum games focusing on scenarios where strategies are confined within given bounds such as those represented by the probability simplex. The authors introduce the Alternating Mirror Descent (AMD) algorithm as a strategic move for players presenting it as a variant of the mirror descent algorithm tailored for constrained optimization. Through the lens of convex optimization and employing a modified energy function they establish an O(K(23)) bound on the average regret following K iterations with this algorithm signifying an improvement over the traditional simultaneous mirror descent approach which exhibits an O(K(12)) average regret bound.  2) Assessment of Strengths and Weaknesses Strengths:  Innovation: The paper presents a novel contribution to the field of game theory and optimization by extending the application of the mirror descent algorithm to a constrained setting particularly for bilinear zerosum games. The AMD algorithms introduction is notable for its potential to enhance understanding and performance in constrained strategy spaces.  Theoretical Depth: The authors provide a rigorous theoretical underpinning for their claims. The use of a modified energy function to establish regret bounds and the interpretation of AMD as an alternating discretization of skewgradient flow in the dual space are both profound and contribute to the theoretical landscape of the domain.  Practical Relevance: The study touches upon a range of applications from economics and game theory to emerging fields like machine learning suggesting broad relevance and potential impact. Weaknesses:  Empirical Validation: While the theoretical contributions are significant the paper lacks empirical evidence to corroborate the theoretical findings. Including comparative analyses with existing algorithms across several constrained optimization scenarios could greatly enhance the papers validity and relevance.  Complexity Analysis: Besides the average regret bounds the paper could be improved by providing a detailed analysis of the computational complexity of the AMD algorithm. This would offer insights into its practical applicability in largescale problem settings.  Discussion on Parameter Selection: The role and impact of the step size parameter (\u03b7) in the performance of the AMD algorithm are not deeply explored. Further discussions or guidelines on choosing this parameter could aid in the practical implementation of the algorithm.  3) Questions and Suggestions  Empirical Evidence: Can the authors provide empirical results or simulations that demonstrate the algorithms performance in contrast to traditional methods particularly in realworld scenarios or benchmark datasets  Complexity Analysis: It would be beneficial if the authors could include a complexity analysis of the AMD algorithm comparing it with both the unconstrained and simultaneous mirror descent methods.  Parameter Sensitivity: How sensitive is the algorithms performance to the step size parameter (\u03b7) Providing guidelines or a sensitivity analysis on choosing \\xce\\xb7 could be significantly helpful.  Extensions to MultiPlayer Settings: The current focus is on twoplayer games. Could the authors discuss potential extensions of the AMD algorithm to multiplayer scenarios Identifying challenges and possible solutions would be insightful.  4) Limitations and Societal Impact The authors did not explicitly address the limitations and potential negative societal impacts of their work. While theoretical advancements in optimization and game theory can have widespread benefits its also important to consider scenarios where these algorithms could be misused or have unintended consequences especially in sensitive areas like automated decisionmaking systems or AIdriven financial strategies. Constructive Suggestions for Improvement:  The authors should include a section discussing potential limitations of their approach such as scenarios where the algorithm might underperform or be inapplicable.  It would be prudent to reflect on the broader societal implications of their work. Acknowledging areas where caution is warranted or where additional safeguards might be necessary can provide a more balanced perspective on the research. Overall this paper contributes valuable insights to the understanding and optimization of constrained twoplayer bilinear zerosum games with significant theoretical advancements and potential for wideranging applications.", "tPiE70y40cv": " Peer Review for the Paper Titled \"Relational Coreset via Aggregation Tree\"  1) Briefly Summarize the Paper and its Contributions The paper addresses the challenge of constructing coresets for relational data to alleviate computational complexity in machine learning tasks. Traditionally coreset construction assumes data is stored in a single matrix which doesnt hold for relational datasets spread across multiple tables. The authors propose a novel method named \"aggregation tree with pseudocube\" which builds a coreset from relational tables without the need to materialize a massive design matrix fully. Their method is showcased to support a variety of machine learning tasks including clustering logistic regression and SVM. Key contributions include:  A novel approach to generating coresets directly from relational data circumventing the need to join tables into a single matrix.  A demonstration of the methods applicability to various empirical risk minimization problems in machine learning under certain conditions.  Lower complexity coresets that rely on the datas intrinsic dimensions rather than its explicit dimensional representation.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The proposed method addresses a realworld challenge in handling relational databases for machine learning presenting a significant advance over the traditional coreset construction paradigms.  General Applicability: The ability of the proposed method to support various machine learning tasks enhances its utility across a broad spectrum of applications.  Efficiency: The approachs reliance on datas intrinsic dimensions for coreset size determination potentially offers significant computational savings especially for highdimensional datasets.  Wellfounded Theoretical Basis: The paper provides a robust theoretical framework for the proposed method including proofs of efficiency and efficacy. Weaknesses:  Implementation Complexity: While the theoretical framework is sound the practical implementation and optimization of the aggregation tree and pseudocube structure may pose significant challenges particularly in highly fragmented data environments.  Empirical Evaluation: The experimental evaluation although comprehensive focuses predominantly on runtime and optimization quality. A more diverse set of metrics including scalability and robustness against varying data characteristics could better highlight the methods effectiveness.  Comparison with the StateoftheArt: The paper could benefit from a more thorough comparison with existing methods particularly those designed for relational data outside the coreset paradigm to contextualize its improvements more effectively.  3) Questions and Suggestions for the Authors 1. Implementation Details: Can the authors provide more insights into the practical implementation challenges of the proposed method especially regarding the construction and maintenance of the aggregation tree in dynamic datasets 2. Scalability: How does the proposed method scale with the number of relational tables and the overall size of the database Are there any limitations in this respect 3. Comparison with Alternatives: It would be helpful to include a comparison with other data summarization techniques designed for relational databases and discuss the advantages the proposed method holds over them. 4. Robustness and Adaptability: How robust is the proposed method against changes in data distribution over time Can the coreset be efficiently updated without full reconstruction  Limitations and Societal Impact The paper could enhance its discussion on potential limitations such as the expected performance in highly dynamic databases or databases with complex intertable relationships. Additionally addressing potential negative societal impacts\u2014like biases resulting from coreset summarization in sensitive applications\u2014would provide a more comprehensive view of the methods applicability and ethical considerations. Suggestions for Improvement:  Expand the experimental evaluation to include more varied datasets especially those with complex relational structures to demonstrate the methods adaptability and robustness.  Include a section discussing ethical considerations particularly in how data summarization might affect decisionmaking processes in critical applications. Overall the paper presents a promising approach to coreset construction for relational data with the potential for broad applicability in machine learning tasks. Addressing the highlighted questions and suggestions could further strengthen the papers contributions and impact.", "ylila4AYSpV": " Peer Review for \"Designing Truthful Auction Mechanisms for Rich Ads\"  1) Summary of the Paper This paper tackles the intricate problem of designing truthful auction mechanisms for modern internet ad auctions where ads can have various formats and may require different space allocations. The authors embark on this journey by identifying and addressing a multiparameter mechanism design problem  a problem typically characterized by computational intractability when aiming for efficient allocation through conventional methods such as VickreyClarkeGroves (VCG) auctions. The primary contributions of this research include:  The adoption of a \"Myersonian\" approach to study allocation rules that are monotone both in terms of the bid and the set of rich ads proposing a framework for truthful auctions.  The development of a novel simple greedy and monotone allocation rule that guarantees a 3approximation to the optimal social welfare.  An exploration of the performance of this monotone allocation rule when paired with the Generalized Second Price (GSP) payment rule providing bounds on the Price of Anarchy (PoA) under various assumptions and scenarios.  An empirical evaluation of the proposed algorithms on realworld data demonstrating the practical viability and efficiency of the method.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Technical Contribution: The paper addresses an open problem with significant practical implications in internet advertising providing novel theoretical insights and algorithms for designing truthful auctions in a multidimensional setting.  Theoretical Rigor: The authors offer a comprehensive theoretical analysis proving the monotonicity of the proposed allocation rule and its approximation bounds to the optimal social welfare. The formal proofs for PoA bounds underpin the robustness of the proposed mechanisms.  Practical Relevance: The empirical evaluation employs realworld data showing that the proposed mechanism performs competitively in practice compared to the VCG mechanism but with significantly less computational overhead. Weaknesses:  Empirical Validation: While the paper presents an empirical study the evaluation primarily focuses on the mechanism\u2019s efficiency and does not delve deeply into its strategic properties or potential vulnerabilities in realworld scenarios.  Consideration of Ad Quality: The paper seems to primarily focus on the allocation and payment rules without thoroughly investigating the impact of ad quality and user experience on the auction outcomes.  Generality of Results: The specific focus on the \"rich ads\" problem may limit the general applicability of the findings to other types of auctions or mechanism design problems beyond internet advertising.  3) Questions and Suggestions for the Authors  Ad Quality: Can the authors elaborate on how ad quality factors into their proposed auction mechanism This is crucial for sustaining user engagement and maintaining the overall health of the advertising ecosystem.  Robustness: Given the potential for strategic manipulation in auctions could the authors discuss how robust their proposed mechanism is against various forms of collusion or strategic reporting by the advertisers  Extension to Other Domains: How might the principles and algorithms proposed in this paper be adapted or extended to other auction settings or mechanism design problems beyond the context of internet advertising  4) Limitations and Societal Impact The authors have made a commendable effort in addressing the computational challenges of designing truthful auctions for rich ads. However the paper could benefit from a more thorough discussion on limitations particularly regarding the assumptions made (e.g. the nooverbidding assumption) and their impact on the realworld applicability of the proposed mechanisms. Regarding potential negative societal impacts while the paper primarily focuses on the technical and theoretical aspects of the auction mechanisms it would be beneficial to include a discussion on how these mechanisms could influence the diversity of advertisements presented to users and the implications for smaller advertisers competing against larger entities with more resources. Suggestions for Improvement:  Discussing Limitations More Thoroughly: Elaborate on the assumptions underlying the proposed mechanisms and their potential implications on the viability and fairness of the auction outcomes in practice.  Considering Societal Impact: Provide insights into how the proposed auction mechanisms could impact the advertising landscape including implications for ad diversity and competition among advertisers of varying sizes. In summary this paper makes significant strides in addressing a complex problem in internet advertising. By refining its empirical evaluation and expanding the discussion on limitations and societal impacts the paper could offer valuable contributions to both the academic community and the industry.", "pZsAwqUgnAs": " Peer Review for \"Understanding the Efficiency and Generalization of Stochastic Gradient Descent in Highdimensional Spaces\"  1. Summary This paper provides an indepth exploration of the dynamics of multipass Stochastic Gradient Descent (SGD) in the specific context of highdimensional convex quadratic problems. The authors present a novel theoretical framework by demonstrating an asymptotic equivalence between the iterations of SGD and a specific Stochastic Differential Equation (SDE) dubbed homogenized SGD (HSGD). Through this equivalence they derive exact predictions for both the learning trajectory and the risk profile of SGD unveiling a mechanism they refer to as \"implicit conditioning\" that elucidates SGDs efficiency over Gradient Descent (GD) and fullbatch methods. Interestingly the paper also identifies settings where SGD underperforms relative to fullbatch momentum gradient descent adding nuance to our understanding of optimization efficiency in highdimensional spaces. Further it offers a critical insight into the generalization capabilities of SGD concluding that the inherent noise within SGD does not facilitate better generalization contrary to popular belief. The paper is methodologically rigorous making significant theoretical contributions by leveraging stochastic calculus and integral equations to offer a fresh perspective on the performance metrics of SGD.  2. Assessment  Strengths:  Theoretical Rigor: The paper is theoretically sound presenting a cohesive framework backed by rigorous mathematical proofs. The establishment of an equivalence between multipass SGD and HSGD and its characterization through a Volterra integral equation are significant contributions.  Novel Insights on SGD: The derivation of explicit formulas for learning and risk trajectories is a notable achievement. The work provides critical insights particularly the concept of implicit conditioning and the clarification that SGDs noise does not inherently improve generalization.  Practical Implications: By quantifying the conditions under which SGD outperforms fullbatch methods through the Implicit Conditioning Ratio (ICR) the authors offer valuable guidelines for practitioners in machine learning optimization tasks.  Weaknesses:  Limited Scope: The focus on convex quadratic problems may limit the general applicability of the findings to a broader range of machine learning challenges particularly those involving nonconvex optimization problems.  Empirical Validation: The paper primarily focuses on theoretical analyses with a lack of empirical validation to support the theoretical predictions. Including experiments that corroborate the theoretical insights could significantly enhance the papers impact.  Complexity of Analysis: While the mathematical rigor is a strength the complexity of the analysis might make the paper less accessible to a broader audience. Simplifying explanations or providing more intuitive discussions could help bridge this gap.  3. Questions and Suggestions for the Authors  Generalization to NonConvex Problems: Given the limited scope to convex quadratic problems can the authors speculate on how their findings might extend to nonconvex optimization scenarios  Empirical Validation: Can the authors provide empirical evidence even if preliminary that supports the theoretical results Specifically experiments demonstrating the ICRs predictive power in practical settings would be valuable.  Clarification on Noise and Generalization: The conclusion that SGD noise negatively impacts generalization is intriguing but counterintuitive to popular beliefs. Can the authors discuss possible reasons why empirical practices might suggest otherwise Might there be specific conditions under which noise could somehow aid generalization despite the theoretical findings  4. Limitations and Societal Impact The authors have provided a thorough discussion on the limitations particularly emphasizing the scope of applicability to convex quadratic problems. However they might consider extending the discussion to include the potential implications of their findings on computational efficiency and energy consumption in largescale machine learning tasks. Given the substantial energy footprint of modern machine learning insights that inform more efficient optimization strategies could have meaningful societal impacts. Further the paper lacks a direct discussion on the potential for negative societal impacts which while not immediately apparent due to the theoretical nature of the work could be beneficial to consider in the broader context of optimizations role in machine learning.  Suggestions for Improvement:  Empirical Support: Including empirical evidence that supports the theoretical insights would greatly enhance the papers credibility and impact.  Broader Context Discussion: Expanding the discussion to consider how these insights might inform optimization strategies in nonconvex problems and their potential societal impacts could broaden the papers appeal and applicability. In summary this paper makes substantial theoretical contributions to our understanding of SGD in highdimensional spaces. With enhancements in empirical validation and broader context discussions its findings could significantly impact both the academic field and practical optimization strategies in machine learning.", "nrOLtfeiIdh": "Review of \"RECOURSENET: Enhancing Prediction Accuracy through Recourse in Adverse Environments\"  1. Summary of the Paper This paper introduces RECOURSENET a novel framework aimed at improving machine learning predictions in unfavorable environments by recommending alterations in the environment settings for data recapture. The core idea revolves around the creation of a mechanism that can suggest changes in environmental conditions (e.g. camera settings for image captures) to generate new instances that are more conducive to accurate predictions by classifiers. The proposed RECOURSENET comprises three main components: a classifier optimized under recourse a recourse trigger for deciding when to recommend environmental changes and a recourse recommender network for suggesting the specific changes to be made. The paper emphasizes the challenge of optimizing these components without direct access to the underlying physical processes that generate new instances postrecourse and proposes a threelevel training strategy to address this. Experiments conducted on both synthetic and realworld datasets demonstrate the effectiveness of this approach.  2. Assessment of Strengths and Weaknesses  Strengths:  Novelty: The paper presents a unique approach to improving machine learning models predictions by intervening at the level of the environment rather than attempting modifications on the instances themselves. This strategy is notably innovative and opens up new avenues for enhancing prediction accuracy in adverse conditions.  Theoretical Foundation: The theoretical characterization of when recourse will prove beneficial backed by propositions and proofs provides a solid foundation for the proposed framework grounding it in rigorous analysis.  Comprehensive Experiments: The authors have carried out extensive experiments using both synthetic and realworld datasets to evaluate the performance of RECOURSENET. The detailed experimentation process and the positive results obtained substantiate the papers claims about the efficacy of their proposed method.  Weaknesses:  Assumptions and Practicality: The paper assumes a fixed labeled dataset and a limited number of environmental settings available during training which might not always be practically achievable especially in dynamically changing environments.  Complexity and Scalability: The proposed threelevel training strategy while innovative appears complex and computationally intensive. Concerns about the scalability of this approach to very large datasets or highly complex environments are not addressed.  Generalizability Across Domains: While the paper presents results on a diverse set of datasets including image and speech data the general applicability of RECOURSENET across all possible domains remains somewhat unclear. Specific domains with very subtle environmental influences might not benefit as significantly from this approach.  3. Questions and Suggestions for the Authors  Model Complexity: Could the authors comment on the computational complexity of RECOURSENET especially concerning the threelevel training procedure and suggest ways to manage or reduce complexity for largescale applications  Impact of Varying Environmental Settings: How sensitive is RECOURSENET to the number and type of environmental settings available during training Additional insights into how different settings influence the models performance would be valuable.  Extension to Continuous Environment Spaces: The paper primarily focuses on discrete or minimally varied environment spaces. Could the authors elaborate on potential approaches or modifications to RECOURSENET that would accommodate continuous environment variables  4. Limitations and Societal Impact The authors have partially addressed the limitations of their work particularly in terms of assumptions about dataset availability and environmental setting variability. However a more thorough discussion on the scalability of the model and its generalizability across various domains would strengthen the paper. Furthermore the potential negative societal impacts especially regarding privacy and ethical concerns in user interaction for recapturing data under recommended settings are not discussed. Constructively the paper could benefit from:  A section discussing ethical implications particularly in sensitive applications like healthcare where patient interactions are involved.  Exploration of lightweight versions of RECOURSENET that maintain effectiveness while reducing computational load making the approach more accessible and scalable.  Suggestions for mitigating risks associated with incorrect recourse recommendations which could lead to further data collection efforts increased costs or potential risks in critical applications. In summary the paper presents a novel and theoretically grounded approach to improving machine learning predictions through environmental interventions. With some clarifications and expansions on the outlined limitations and potential societal impacts the work would make a significant contribution to the field.", "l5UNyaHqFdO": " Peer Review of \"On the Convergence of Adam and Beyond\"  1) Brief Summary of the Paper This paper addresses the perplexing issue of why the Adam optimization algorithm despite theoretical divergence warnings highlighted by Reddi et al. (2018) continues to perform well in practical applications particularly in the field of deep learning. The authors propose that this discrepancy arises from a fundamental difference in the order of how problems and hyperparameters are chosen in theory versus practice. They rigorously prove that under certain conditions specifically when the secondorder momentum parameter \u03b22 is large and the firstorder momentum parameter \u03b21 satisfies \\(0 \\leq \\beta1  \\sqrt\\beta2  1\\) Adam can indeed converge to the neighborhood of critical points without any modifications to its original formulation. Importantly their analysis does not rely on assumptions of bounded gradients or bounded secondorder momentum which are common in other works. The paper also identifies a phase transition from divergence to convergence with the adjustment of \u03b22 providing practical guidance on tuning Adams hyperparameters.  2) Assessment of Strengths and Weaknesses Strengths: a) Theoretical Contribution: The paper is the first to show that vanilla Adam can converge under a broader range of hyperparameters without any modifications or constraints on the gradient norms. This significantly advances our understanding of Adam and provides a theoretical backing for its empirical success. b) Relevance: Given the widespread usage of Adam in various deep learning tasks these findings are highly relevant and valuable to researchers and practitioners alike. c) Thorough Analysis: The authors provide a detailed and rigorous mathematical analysis supplemented with experiments that demonstrate the phenomena under study. This combination of theoretical and empirical evidence strengthens the papers contributions. Weaknesses: a) Assumption Clarity: While the paper makes less restrictive assumptions than previous works it could benefit from a clearer discussion on the implications and realism of these assumptions in practice particularly concerning the \"strong growth condition\". b) Experimental Details: The paper presents experimental results supporting the theoretical findings but it lacks a detailed examination of the divergence behavior in practice. A deeper exploration of cases where Adam fails could provide more insights. c) Complexity for Practitioners: The practical implications and tuning recommendations are valuable but the complexity of the theoretical analysis may make it challenging for practitioners to derive actionable insights without a thorough study.  3) Questions and Suggestions for Authors a) Clarification on Assumptions: Could you provide more intuition on the \"strong growth condition\" and its prevalence in practical optimization problems encountered in deep learning b) Expanded Experimental Analysis: Can you extend the experimental analysis to cover more diverse architectures and datasets particularly to examine the divergence behavior of Adam in more detail c) Simplification for Practical Guidance: It would be beneficial if the paper could include a simplified summary or a practical guide derived from the findings that practitioners can use without delving into the complex mathematical details.  4) Evaluation of Limitations and Societal Impact The authors successfully address several limitations of prior works particularly by removing the need for bounded gradient assumptions. However the paper could further discuss the potential for misuse or misinterpretation of its findings especially in critical applications where optimization failure can have significant consequences. Additionally encouraging clearer communication of the theoretical findings for nonexpert audiences could maximize the papers positive impact. Suggestions for Improvement: a) Broader Discussion on Limitations: A more detailed discussion on the scenarios where the assumptions may not hold in practical applications would be beneficial. b) Societal Impact: While the paper is focused on theoretical aspects a brief discussion on the potential societal implications of more efficient and reliable training of deep learning models could add value. In summary this paper makes notable contributions to our understanding of Adams convergence properties offering valuable theoretical insights complemented by practical tuning recommendations. Addressing the mentioned weaknesses could further strengthen the paper.", "p-56bnzZhQ7": "Paper Summary: The paper presents a novel study on the computational complexity of learning halfspaces in the presence of Massart noise within the PAC (Probably Approximately Correct) learning model. A halfspace or linear threshold function is a central concept in machine learning representing a linear classifier. The Massart noise model which allows for variation in misclassification rates based on input presents a more realistic and challenging scenario compared to uniform noise models. The authors introduce the first computational hardness result for this problem. They establish that under the assumption of subexponentialtime hardness of the Learning with Errors (LWE) problem no polynomialtime algorithm can achieve a 01 error smaller than a certain bound even when the optimal error (OPT) is very small. This result essentially suggests that previously known efficient algorithms for learning halfspaces with Massart noise are nearoptimal under this assumption. The paper leverages techniques from the Statistical Query (SQ) model and introduces a twostep reduction process from the standard LWE problem to demonstrate the computational hardness of learning halfspaces with Massart noise. Strengths: 1. Theoretical Significance: The paper addresses a longstanding open problem in the domain of computational learning theory by establishing the first computational hardness result for learning halfspaces with Massart noise. The significance of this contribution cannot be overstated as it directly impacts our understanding of the limits of efficient learning algorithms in realistic noise settings. 2. Rigor: The methodology and proofs provided in the paper are mathematically rigorous and detailed. The authors carefully define all terms provide a highlevel overview of their approach and meticulously prove their main theorem. The reduction from the LWE problem to the learning problem of interest is particularly noteworthy for its elegance and creativity. 3. Clarity in Presentation: Despite the complexity of the subject matter the paper is wellstructured and written with clarity. The introduction effectively sets the stage for the problem domain and its importance and subsequent sections are logically organized to build upon each other. Weaknesses: 1. Assumption Dependence: The main result relies on the subexponentialtime hardness of the LWE problem. While this is a widely believed assumption the firm conclusions drawn in the paper depend on this external conjecture. It would be beneficial if future work could explore alternative assumptions or somehow reduce the dependency on the hardness of LWE. 2. Practical Implications: While the paper excels in theoretical contributions it does not discuss the practical implications of its results. Understanding how these findings could impact the development of learning algorithms in noisy environments would be of interest to practitioners and theoreticians alike. 3. Limited Discussion on Limitations and Societal Impact: The paper does not adequately address potential limitations or the negative societal impacts of its findings. Given the relevance of machine learning algorithms in numerous applications discussing the implications of computational hardness results on fairness transparency and accountability in automated decisionmaking would be beneficial. Questions and Suggestions for the Authors: 1. Alternative Assumptions: Can the authors speculate on weaker assumptions than the subexponentialtime hardness of LWE that could still yield meaningful hardness results for learning with Massart noise 2. Extension to Other Concept Classes: Given the results for halfspaces do the authors anticipate similar hardness results for other concept classes under Massart noise Could your techniques be adapted to prove such results 3. Practical Relevance: Could the authors expand on the practical implications of their findings How should practitioners interpret these results in the context of developing and deploying learning algorithms in noisy environments Limitations and Suggestions for Improvement: While the paper provides a significant theoretical contribution it lacks a discussion of practical implications and potential limitations. An exploration of how these hardness results might influence the design and deployment of learning algorithms in realworld settings would enhance the paper. Additionally a discussion on the societal impacts ethical considerations and potential biases influenced by the computational hardness of learning under Massart noise would provide a comprehensive understanding of the works broader context.", "r70ZpWKiCW": "Review for \"Gentle Teaching Assistant for SemiSupervised Semantic Segmentation (GTASeg)\" 1) Summary: The paper introduces a novel framework Gentle Teaching Assistant (GTASeg) aimed at enhancing performance in semisupervised semantic segmentation by effectively utilizing both labeled and unlabeled data. The core innovation lies in the introduction of a teaching assistant network that learns from pseudo labels generated by the teacher network and selectively transfers beneficial feature representation knowledge to a student model via Exponential Moving Average (EMA). The framework aims to mitigate the deleterious effects of incorrect pseudo labels on the mask predictor while improving the feature extractors learning with targeted representation knowledge. Extensive experimental validations on benchmark datasets (PASCAL VOC 2012 and Cityscapes) show that GTASeg outperforms existing methods significantly establishing new stateoftheart results. 2) Assessment: Strengths:  The paper addresses a wellknown limitation in semisupervised learning \u2014 the negative impact of incorrect pseudo labels with an innovative solution that disentangles the influence of pseudo labels on different components of the model.  Extensive experiments alongside comparisons with previous methods validate the effectiveness of GTASeg with significant improvements shown across different datasets and under varying conditions of labeled data availability.  The authors provide a comprehensive set of analytical experiments to understand the contribution of individual components within GTASeg which demonstrates the thoughtful design and robustness of the proposed framework. Weaknesses:  The methodology while novel introduces additional complexity in terms of an extra network (the gentle teaching assistant) into the learning process which might lead to increased computational and memory overhead during training.  The paper while thorough in its experimental validation could benefit from a deeper exploration of the theoretical underpinnings of why separating the feature extractor and mask predictors learning from pseudo labels is as effective as demonstrated.  Theres limited discussion on the implications of applying GTASeg in realworld scenarios beyond benchmark dataset performance. Particularly insights into its scalability and adaptability to various domainspecific applications would add value. 3) Questions and Suggestions:  Could the authors elaborate on potential strategies to reduce the computational overhead introduced by the GTA without significantly sacrificing the performance gains  It would be helpful if the authors could discuss any observed or potential limitations or challenges in applying GTASeg to other domains of semisupervised learning beyond semantic segmentation.  For further clarity can the authors provide more insights into the choice of the exponential moving average (EMA) for knowledge transfer Additionally exploring alternative mechanisms for such transmission may yield interesting comparative insights. 4) Limitations and Societal Impact:  The authors have not explicitly discussed the limitations related to the scalability of the proposed method concerning computational resources which is crucial for the applicability of GTASeg in resourceconstrained scenarios.  While the paper is primarily technical an exploration of the potential negative societal impacts such as the misuse of semisupervised segmentation in surveillance would make for a more rounded discussion.  Constructive suggestions for improvement would include a detailed analysis of the tradeoffs between performance gains and computational costs as well as strategies for model simplification that might make GTASeg more accessible for broader applications. In conclusion the presented paper makes a significant contribution to the field of semisupervised semantic segmentation by proposing a novel and effective framework. However addressing the raised questions and suggestions could further enhance the clarity applicability and understanding of the proposed GTASeg framework.", "lNokkSaUbfV": " Peer Review for \"Masked Decision Prediction for Reinforcement Learning\"  1) Brief Summary of the Paper This paper introduces Masked Decision Prediction (MaskDP) a new selfsupervised pretraining method for reinforcement learning (RL) and behavioral cloning (BC). By employing a masked autoencoder (MAE) approach to stateaction trajectories MaskDP can randomly mask state and action tokens and learns to reconstruct the missing information. This approach requires the model to infer maskedout states and actions facilitating the learning of dynamics. A key feature of MaskDP is its ability to adapt to multiple downstream tasks including goalreaching and offline RL demonstrating high performance and dataefficient finetuning capabilities. The method outperforms several baselines showing strong zeroshot transfer to new BC tasks and promising scaling behavior regarding model size.  2) Assessment of Strengths and Weaknesses Strengths: 1. Innovative Approach: The introduction of MaskDP as a selfsupervised pretraining method for RL is novel merging ideas from successes in NLP and vision with RL. 2. Generalization and ZeroShot Learning: Demonstrating zeroshot learning capabilities for multiple downstream tasks is a significant achievement potentially reducing the need for extensive labeled datasets in RL. 3. Scalability: The scalability of MaskDP evidenced by its performance improvements with model size increments is commendable and hints at its applicability to largescale RL problems. 4. Comprehensive Experiments: The empirical studies are thorough covering various scenarios under goalreaching skill prompting and offline RL settings providing solid evidence for the method\u2019s effectiveness. Weaknesses: 1. Dependence on Data Quality: While MaskDP shows promising results its performance is likely dependent on the quality of pretraining data. The distinction between the nearexpert and mixedquality data performance could indicate limitations in realworld scenarios with less ideal data. 2. Computational Overhead: The preprocessing of data with random masking and the subsequent need for deep networks for reconstruction could introduce significant computational overhead making it less practical for resourceconstrained environments. 3. Limited Task Diversity in Evaluation: The evaluation is somewhat restricted to goalreaching and skill execution tasks. The inclusion of a broader array of tasks could better illustrate the method\u2019s versatility. 4. Potential Overfitting to Masked Training Paradigm: There is a risk that the model might overfit to the training paradigm of predicting masked tokens which could potentially degrade performance in highly dynamic or unpredictable environments.  3) Questions and Suggestions for the Authors  Question 1: Could the authors elaborate on how MaskDP would perform in environments with high entropy where the prediction of masked states and actions becomes significantly harder  Question 2: Is there a theoretical underpinning for the choice of mask ratios How do these ratios impact the learning in deeply stochastic environments Suggestions: 1. Experiment Diversity: Including a wider array of RL tasks especially those involving complex dynamics and interactions could offer a more comprehensive validation of MaskDPs effectiveness. 2. Analysis of Computational Efficiency: A deeper analysis of the method\u2019s computational demands possibly comparing it with existing RL pretraining methods would provide valuable insights into its practical applicability. 3. Further Investigation on Mask Ratio Optimization: Exploring adaptive or dynamic masking strategies might improve learning efficiency and task generalization further.  4) Limitations and Potential Societal Impact The authors have identified that while MaskDP demonstrates promising results in controlled experimental settings its capabilities in handling realworld pixelbased inputs remain to be evaluated. Extending the method to cope with more diverse and less structured data types is a crucial step forward. There is also a mention of the potential societal impacts related to biases in largescale data acknowledging the need for careful data curation. However there could have been a more nuanced discussion regarding the implications of deploying such advanced RL systems in realworld scenarios considering ethical privacy and safety concerns. Suggestions for Improvement: 1. Explicit Discussion on Limitations: A more detailed discussion on computational viability and potential challenges in extending MaskDP to handle realworld complexity would be beneficial. 2. Addressing Societal Impact: Expanding on the potential societal impacts especially concerning automating critical decisionmaking processes in sensitive areas such as healthcare or security could provide a more rounded perspective on the implications of this research. In summary this paper presents a significant step forward in selfsupervised learning for RL demonstrating promising results across several tasks. While there are areas for improvement and extension the contributions provide a strong foundation for future research in scalable and generalizable RL algorithms.", "rZalM6vZ2J": " Draft Peer Review  1) Briefly summarize the paper and its contributions The paper presents a novel differential privacy (DP) algorithm for Principal Component Analysis (PCA) titled DPPCA which addresses two significant limitations of existing DPPCA methods: their superlinear sample complexity in dimensions (d) and their nonvanishing error in the face of small randomness in data. The proposed DPPCA is a singlepass algorithm that employs minibatch SGD along with a private mean estimation technique that minimally disturbs the original data while ensuring privacy by adapting to the geometry of minibatch gradients. The paper demonstrates that for subGaussian data DPPCA achieves nearly optimal statistical error rates with a sample complexity that grows only quasilinearly with dimension thereby significantly improving upon the existing approaches. Moreover it establishes a lower bound underscoring the necessity of subGaussian assumptions for optimal error rates in DPPCA.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Theoretical Advancements: The paper makes significant theoretical contributions by introducing an algorithm that not only improves sample complexity but also reduces error rates compared to existing DPPCA methods.  Relevance: Addressing differential privacy in PCA is highly relevant given the widespread application of PCA in data analysis and machine learning pipelines particularly those involving sensitive information.  Methodological Rigor: The proposed method is based on solid theoretical foundations employing techniques such as private mean estimation adeptly to enhance privacy while maintaining utility.  Empirical Evaluation: The theoretical claims are wellsupported by rigorous mathematical analysis though the absence of empirical evaluation is noted. Weaknesses:  Lack of Empirical Validation: One notable omission is the lack of empirical studies to validate the theoretical claims made by the paper.  Assumption Specificity: The methods efficacy is proven under somewhat restrictive assumptions (subGaussian) which raises questions about its applicability to broader data distributions.  Complexity of Implementation: The algorithms complexity particularly in terms of determining the learning rate and batch size for practical implementations might limit its accessibility to practitioners.  3) Please list up and carefully describe any questions and suggestions for the authors Questions:  Can the authors comment on the practical applicability of DPPCA to nonsubGaussian data distributions and whether there are potential modifications to accommodate such cases  Could the authors provide insights into the computational efficiency of DPPCA especially when compared to existing DPPCA methods  Are there plans to empirically validate the algorithms performance on realworld datasets which could help in understanding its practical implications Suggestions:  Empirical Evaluation: Incorporating empirical studies using realworld datasets can significantly strengthen the papers claims and help in understanding the algorithms practical utility.  Discussion on Implementational Details: Providing more guidance on choosing hyperparameters such as the learning rate and batch size in practical settings would be beneficial for potential users of DPPCA.  Broader Applicability: Exploring extensions or modifications of DPPCA that could handle wider data distributions beyond subGaussian could enhance the methods applicability.  4) Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work If not please include constructive suggestions for improvement. The authors have discussed the limitations of their work particularly concerning the subGaussian assumption necessary for the optimal error rate and the challenges in practical implementation due to the complexity of the algorithm. However a discussion of the potential negative societal impacts such as misuse of differential privacy to obscure data analysis in sensitive contexts is missing. It would be beneficial for the authors to include a section on ethical considerations and potential negative impacts alongside possible safeguards against misuse. Conclusion: This paper presents a significant advancement in differentially private PCA addressing key challenges in sample complexity and error rates. While the theoretical contributions are notable the paper could be enhanced by empirical validation and a more indepth discussion of practical implementation considerations and ethical implications.", "lUyAaz-iA4u": "The paper under review introduces and analyzes new variants of stochastic gradient descent (SGD) with stochastic Polyak stepsize (SPS) aimed at addressing two main drawbacks identified in previous work by Loizou et al. (2022) when applied in nonoverparameterized regimes. Specifically the original SPS requires prior knowledge of optimal minibatch losses which may not be available without the interpolation condition and it only assures convergence to a neighborhood of the solution. To overcome these limitations the authors propose modifications to SPS: a simple variant using lower bounds on function values instead of exact optimal values to tackle the first issue and a more sophisticated modified SPS named DecSPS designed to ensure convergence to the exact minimizer without prior knowledge of problem parameters thereby addressing the second issue. The paper provides theoretical proofs of convergence under specific conditions and validates the proposed methods through experimental testing on binary classification tasks. Strengths: 1. Theoretical Contribution: The manuscript significantly advances the theoretical understanding of Polyak stepsizes in the stochastic setting particularly for nonoverparameterized regimes. The introduction of DecSPS is particularly noteworthy as it represents the first stochastic adaptive optimization method that guarantees convergence to the exact solution without restrictive assumptions like bounded iteratesgradients. 2. Practical Value: By relaxing the need for a priori knowledge of optimal minibatch losses the proposed variants substantially enhance the practical utility of SPS in a wider array of scenarios including regularized objectives and noninterpolated settings. This broader applicability coupled with the competitive performance of the new methods has the potential to impact the application of SGD in machine learning significantly. Weaknesses: 1. Experimental Verification: While the experimental results support the theoretical findings the scope of these experiments is somewhat limited. For instance the evaluation primarily focuses on binary classification tasks. Expanding the experimental evaluation to include a broader range of tasks including deep learning challenges could further validate the effectiveness and versatility of the proposed methods. 2. Comparison with Existing Methods: The paper could benefit from a more extensive comparison with existing adaptive methods beyond Adam AdaGrad and AMSgrad. For example discussing where the proposed methods stand in relation to more recent advances in optimization algorithms could provide additional context and clarity on the novel contributions of this work. Questions and Suggestions for the Authors: 1. Extension to NonConvex Problems: The paper focuses on convex optimization problems. It would be interesting to know the authors\u2019 views or preliminary insights on how the proposed DecSPS might be adapted or extended to nonconvex optimization problems which are prevalent in deep learning applications. 2. Robustness to Hyperparameter Settings: The authors suggest default values for the hyperparameters in DecSPS. Could they provide further discussion or sensitivity analysis on how the performance of DecSPS is affected by variations in these hyperparameters Additionally is there any heuristic or rule of thumb that could aid practitioners in tuning these parameters for specific problems 3. Impact of Batch Size: The authors mention the effect of nonuniform sampling and minibatching as potential extensions. Preliminary thoughts or theoretical insights on how differing batch sizes might influence the behavior of the proposed SPS variants would be valuable. Limitations and Societal Impact: The authors have made commendable efforts to address the limitations of the original SPS and have put forward a rigorously tested solution in the form of DecSPS. However the discussion on potential negative societal impacts is missing. While this work is primarily theoretical and its immediate societal implications might not be apparent a reflection on how optimization algorithms can sometimes perpetuate bias in data or lead to overfitting would enrich the discourse. Additionally the authors could consider discussing how their work might contribute to more energyefficient AI systems due to potentially reduced training times which has environmental implications. Suggestions for improvement include incorporating a broader range of experiments and further discussing the implications of this work in the broader context of ethical AI and sustainable computing.", "wO53HILzu65": " PeerReview of the Submitted Paper Title: RecZilla: MetaLearning Approach to Recommender System Algorithm Selection  1) Brief Summary and Contributions The paper presents a substantial contribution to the field of recommender systems specifically in the automation of algorithm and hyperparameter selection for new datasets. The authors conducted an exhaustive largescale study involving 24 algorithms 100 sets of hyperparameters across 85 datasets and 315 performance metrics. From their analysis its clear that both algorithm performance and optimal hyperparameters are heavily dependent on the dataset and the performance metric in question. To address the challenge of selecting the most appropriate algorithm and hyperparameters for new unseen datasets the authors introduced RecZilla a metalearning based approach that leverages dataset metafeatures to predict the best recommender system configuration. The paper is enriched by making their code pretrained models and raw experimental results publicly available encouraging reproducibility and further experimentation by practitioners in the field.  2) Assessment of Strengths and Weaknesses Strengths:  Comprehensive Analysis: The paper stands out for its breadth in analyzing various algorithms across a wide range of datasets and metrics offering an unprecedented scale in recommender system studies.  Practical Utility: RecZilla has significant implications for both academic research and commercial applications in recommendation systems where selecting an optimal algorithm is crucial.  Open Source Contributions: By releasing the codebase pretrained models and raw experimental results the authors foster a culture of openness and facilitate rapid advances in the field. Weaknesses:  Dataset Diversity: While impressive in number the datasets employed might still not cover the full spectrum of realworld scenarios particularly those with highly specific or niche recommendation requirements.  Neural Network Approaches: The paper somewhat underrepresents neural networkbased approaches due to computational constraints. Given the rapid development in this area more emphasis on these methods could make RecZilla even more comprehensive.  Evaluation on RealWorld Performance: Although the paper presents an extensive evaluation it primarily focuses on offline experiments. The realworld effectiveness of RecZilla especially in terms of user satisfaction and business metrics remains to be validated.  3) Questions and Suggestions for the Authors  Future Work on Neural Networks: Could future versions of RecZilla include an expanded focus on neural networkbased recommender systems given their growing prevalence in the industry  Impact of Dataset Specificity: How does RecZilla handle highly specific datasets which may have unique characteristics not represented in the training metadatasets  Adaptation Over Time: Recommender systems are often dynamic with user preferences and item catalogs changing over time. How might RecZilla adapt to these changes or does it require retraining  4) Limitations and Potential Negative Societal Impact The authors have made an effort to address limitations in their study particularly concerning the scope of datasets and methods covered. However the potential societal impacts warrant further discussion. Recommendations algorithms can have profound effects on individuals and societies influencing purchasing behaviors information consumption and even sociopolitical views. While RecZilla aims to optimize performance metrics ethical considerations and biases in recommendations should be paramount. Constructive suggestions include:  Bias and Fairness Evaluation: Future iterations of RecZilla could benefit from incorporating bias detection and fairness assessments into the metalearning model ensuring that recommendations serve diverse user groups equitably.  Ethical Guidelines: The authors could provide guidelines or best practices for practitioners using RecZilla emphasizing the importance of ethical considerations in recommender system deployment. In conclusion this paper provides valuable insights and tools for the recommender systems community. By addressing the limitations and considering the broader implications of their work the authors can enhance the value and applicability of RecZilla.", "lC5-Ty_0FiN": "Review of \"Exploring Redundancy in Deep Neural Networks: A Mechanism for Benign Overfitting\" 1. Summary of the Paper: The paper investigates a counterintuitive phenomenon observed in deep neural networks (DNNs)  benign overfitting where adding more parameters to a DNN that perfectly fits its training data improves its generalization performance rather than impairing it. The authors explore the behavior of the last hidden layer representations in wide convolutional neural networks (CNNs) proposing that when the network width exceeds a certain threshold and is trained with regularization to achieve zero training error the neurons in the last hidden layer form \"clones\" \u2013 groups carrying identical information but with statistically independent noise. This redundancy they argue is a possible mechanism enabling benign overfitting as the network effectively averages over an increasing number of clones improving accuracy. The study spans different architectures and datasets providing a comprehensive analysis through both experimentation and analytical methods. 2. Assessment of the Papers Strengths and Weaknesses: Strengths:  Novel Insight: This work provides a fresh perspective on the phenomenon of benign overfitting in DNNs challenging the traditional understanding of the biasvariance tradeoff in machine learning.  Comprehensive Analysis: The paper combines empirical evidence from a range of DNN architectures and datasets alongside analytical methods to underpin its findings regarding the emergence of redundant representations and their role in improving generalization performance.  Practical Implications: The findings have important implications for the design and training of neural networks particularly in encouraging the use of regularization and training beyond the interpolation threshold for better generalization.  Reproducibility: The inclusion of code and detailed training setups enhances the papers reproducibility facilitating further research in this area. Weaknesses:  Theoretical Foundation: While the empirical results are compelling the paper could further strengthen its theoretical foundation. A deeper dive into why and how redundancy contributes to benign overfitting beyond empirical observations would add value.  Limited Discussion on Limitations and Societal Impact: The discussion on potential negative societal impacts is brief and could be expanded. Similarly while limitations are acknowledged a more detailed exploration of scenarios where the observed mechanism might not hold would be beneficial.  Generalizability to Other Architectures: While the study includes a variety of architectures the focus is on convolutional networks. It would be beneficial to see if similar behaviors are observed in other types of DNNs such as recurrent neural networks. 3. Questions and Suggestions for the Authors:  Theoretical Depth: Could the authors provide a more detailed theoretical explanation or model that predicts the formation of clones and their relationship with benign overfitting  Impact of Different Regularization Techniques: The paper focuses on weight decay as a form of regularization. How might other regularization techniques impact the formation of redundant representations and the benign overfitting phenomenon  Consideration of Other Network Types: Have the authors considered extending their analysis to other types of neural networks Results from such an exploration would enhance the papers generality.  Societal Impact and Ethical Considerations: The paper briefly mentions potential societal impacts. Could the authors expand on specific scenarios or applications where these findings could have negative consequences Additionally discussing the ethical considerations of deliberately designing overfitted yet generalizable models would provide a more rounded view. Limitations and Societal Impact: The paper provides a groundbreaking perspective on benign overfitting through redundant representations but stops short of an indepth theoretical explanation. The authors also briefly touch on potential negative societal impacts without thorough exploration. To improve the authors could delve deeper into the theoretical underpinnings of their observations possibly through mathematical modeling or simulations. Additionally a more thorough discussion of the ethical implications and potential biases that could be exacerbated by models designed to exploit benign overfitting would make the paper more comprehensive. Its crucial for future work to consider the broader implications of these findings not only in terms of model performance but also in terms of fairness transparency and societal impact.", "pn5trhFskOt": "The paper introduces a novel approach to audiovisual source localization (AVSL) an important task in multimedia analysis that seeks to identify the spatial location of sound sources within visual frames. Traditionally collecting precise groundtruth annotations for training AVSL models is costly propelling interest in weaklysupervised methods that leverage the natural cooccurrence of audio and visual signals. The paper identifies two significant limitations in current evaluation protocols: the reliance on early stopping which implicitly requires fully annotated datasets and the assumption that sound sources are always present which leads to a neglect of false positive detection.  Contributions:  Extended Evaluation Protocol: The authors enhance existing evaluation benchmarks to include negative samples where no visible sound sources are present. This more realistic setup is complemented by performance metrics that balance localization accuracy against recall addressing the critical issue of false positive detection.  SLAVC Method: A new approach named Simultaneous Localization and AudioVisual Correspondence (SLAVC) is proposed. It incorporates visual dropout and momentum encoders as regularization strategies to combat model overfitting and employs a dual mechanism to enhance localization accuracy while reducing false positives.  Assessment of Strengths and Weaknesses: Strengths:  Comprehensive Evaluation: By extending wellknown benchmarks and proposing a nuanced evaluation methodology the paper sets a new standard for assessing AVSL models encouraging future work to address both overfitting and the ability to handle negatives.  Innovative Approach: The SLAVC technique with its emphasis on regularization and explicit audiovisual correspondence represents a significant step forward in developing robust AVSL models. It is particularly commendable that these improvements are achieved without the need for early stopping enabling models to fully leverage large datasets.  Practical Impact: Offering code and pretrained models enhances the papers practical utility facilitating replication and further innovation by the research community. Weaknesses:  Generalization Concerns: While the method shows notable improvements on the selected datasets it remains unclear how well the approach generalizes across diverse unseen datasets particularly in different noise conditions or with sound sources not represented in the training data.  Limited Discussion on Societal Impact: The paper briefly mentions the potential for negative societal impacts but does not delve into specific scenarios or mitigation strategies an area increasingly important for multimedia research.  Questions and Suggestions for Authors: 1. Generalization Ability: Can the authors provide insights or preliminary results on the generalization of SLAVC to datasets not included in the study especially in unconstrained environments 2. Computational Efficiency: Given the additional components introduced by SLAVC what is the expected impact on computational requirements during training and inference Are there strategies to balance performance improvements with computational cost 3. Societal Impact: Could the authors expand on potential negative societal impacts such as privacy concerns or misuse of the technology and suggest guidelines or technical measures to mitigate these risks  Addressing Limitations and Societal Impact: The authors should consider conducting crossdataset validation to better understand the generalizability of their method. Additionally a more thorough analysis of computational requirements would help in assessing the practical applicability of SLAVC in resourceconstrained scenarios. Regarding societal impact articulating specific scenarios where AVSL could be misused and proposing countermeasures or ethical guidelines would enhance the papers responsibility and relevance.", "u5oLvX8x4wH": " Summary This paper addresses the study of improving the lastiterate convergence of momentumbased Stochastic Gradient Descent (mSGD) through relaxed step size conditions. Traditionally ensuring lastiterate convergence of SGD and mSGD requires the step size \\( \\epsilonn \\) to decay following certain restrictive criteria which could lead to slow convergence particularly during the early stages of iterations. The authors propose a more lenient decay requirement as \\( \\sumn1\\infty \\epsilonn2\\eta0  \\infty \\) (where \\( 0 \\leq \\eta0  12 \\)) enabling the use of larger step sizes early on for accelerating convergence without sacrificing the asymptotic decay of the gradient norm for a class of nonconvex loss functions. They also establish the viability of a constant step size \\( \\epsilonn \\equiv \\epsilon  0 \\) in ensuring convergence without the need for loss functions to exhibit strong convexity. Theoretical underpinnings are complemented with experiments illustrating these findings.  Assessment Strengths: 1. Theoretical Advancement: The paper extends existing theories on the convergence of mSGD by proving lastiterate convergence under more realistic and less restrictive conditions on step sizes. This contribution fills a significant gap in stochastic optimization literature. 2. Practical Relevance: By showing that larger step sizes can be employed early on without compromising convergence the study offers a pragmatic approach to accelerating mSGD in practical machine learning applications particularly valuable in deep learning contexts. 3. Solid Experimental Validation: The experiments conducted both on regression and classification tasks effectively demonstrate the benefits of the proposed step size adjustments on convergence rates thereby providing empirical validation for the theoretical results. Weaknesses: 1. Limited Exploration of Step Size Impact in Diverse Contexts: While the paper provides a solid theoretical basis for the proposed step size rule the empirical evidence might be further strengthened by exploring a broader range of machine learning problems and comparing more comprehensively with existing strategies. 2. Potential Overemphasis on LastIterate Convergence: The focus on lastiterate convergence while detailed and theoretically important might somewhat overshadow other aspects of convergence behavior that are equally relevant in practice such as convergence rate to first significant improvement or behavior under varying conditions of noise.  Questions and Suggestions for Authors 1. Exploration of Step Size Strategies in Diverse Contexts: Could the authors comment on or include findings related to how the proposed step size strategy performs across a broader range of machine learning models and datasets beyond the presented regression and classification examples 2. Comparison with Adaptive Step Size Methods: It would be interesting to see a direct comparison between the proposed step size method and other adaptive step size methods like Adam. Could future work possibly include such comparative analyses 3. Impact on Training Time: The improved convergence rate in the early stages could potentially reduce overall training time. Do the authors have any quantitative data on this aspect comparing traditional and proposed step size adjustments  Limitations and Societal Impact The authors have made strides in addressing the limitations inherent in traditional step size conditions for mSGD however the paper could benefit from a more explicit discussion of potential limitations or scenarios where the proposed approach may not offer significant advantages. On the societal impact front while direct negative impacts seem unlikely a nuanced discussion on how accelerated convergence might affect computational resource usage and energy consumption in largescale machine learning applications could add value. Given the increasing concern around the environmental impact of computing suggestions for mitigating potential increases in resource use would be welcome.", "sipwrPCrIS": " 1) Brief Summary of the Paper and its Contributions The paper introduces a novel theoretical framework to analyze feature learning in infinitewidth neural networks trained with gradient flow specifically focusing on the interpolation between the lazy and rich regimes of neural network training. The authors develop a selfconsistent dynamical mean field theory (DMFT) that extends the understanding of neural networks beyond the static approximation of the Neural Tangent Kernel (NTK) into a regime where feature learning can significantly alter the internal representation of data. This effort builds atop the pioneering work of Yang  Hu by using an equivalent parameterization with an additional scalar parameter to control the transition between the lazy and rich training regimes. Key contributions include:  A path integral formulation of gradient flow dynamics for infinitewidth networks leading to a set of selfconsistent saddle point equations for dynamical kernel order parameters.  Introduction of a polynomialtime numerical procedure to solve these equations allowing predictions of network training performances across various widths depths and training dynamics.  Demonstrations through experiments that the theory provides accurate descriptions in more realistic settings such as training convolutional neural networks (CNNs) on the CIFAR classification task.  2) Assessment of the Strengths and Weaknesses Strengths:  The theoretical advancement of incorporating a scalar parameter for navigating between lazy and rich training regimes provides a substantial improvement in understanding the dynamics of infinitewidth networks. This innovation allows the bridge of theoretical analyses with practical empirical observations in deep learning.  The provision of a computationally feasible numerical procedure to solve the developed theoretical framework is a significant contribution making the theory accessible for practical experimentation and further investigation.  The experimental validation especially in the context of CNNs on a realistic dataset like CIFAR showcases the practical relevance and applicability of the developed theory strengthening the papers contributions to the deep learning community. Weaknesses:  While the paper provides a robust theoretical framework and numerical methods the accessibility of the concepts to a broader audience might be challenging due to the depth of mathematical formalism.  The computational requirements despite being less than the exponential complexity of exact solutions remain substantial for largescale problems potentially limiting the immediate adoption of these methods for very large datasets or extremely deep networks.  The assumption of infinitewidth might still pose limitations in terms of direct applicability to finite practicalsized networks although the paper makes strides in addressing this through empirical validation.  3) Questions and Suggestions for the Authors  How does the performance of the proposed theory and numerical methods compare with existing approximation schemes (static NTK gradient independence assumption etc.) in network architectures not covered in the experiments such as recurrent neural networks or transformers  The paper briefly mentions the potential of the theory to explore variations in hidden layer widths and initialization variances. Could the authors elaborate on how these variations might influence the dynamics of feature learning and whether the current framework could accommodate such explorations without significant modifications  One possible direction for future work could involve extending the framework to cover not just the training dynamics but also aspects of model generalization and robustness. Have the authors considered how their current theoretical developments could be leveraged toward these ends or do they foresee major hurdles in such extensions  Limitations and Societal Impact The authors do recognize the limitations of their work especially regarding the assumption of infinitewidth which might not directly translate to finitesized networks practically used. While they address this through empirical validation further exploration into bridging this gap theoretically would enhance the works relevance. Additionally addressing computational scalability for larger datasets explicitly would be beneficial. Regarding potential negative societal impacts the paper does not delve deeply into this area. Given the theoretical nature of the work direct societal impacts might be minimal in the short term. However as the theory potentially leads to more efficient and powerful neural networks considerations around misuse bias and ethical use of such advanced models should be acknowledged. Suggestions for improvement include incorporating discussions on ethical considerations in applying these theoretical advancements and exploring the implications of more powerful neural networks on privacy security and societal biases.", "yKDKNzjHg8N": "Review of \"Dissecting Example Hardness through SecondSplit Forgetting Time (SSFT)\" 1. Summary of the Paper: This paper investigates the dynamics of neural networks during training particularly focusing on how different examples are learned and forgotten. The authors propose a novel metric SecondSplit Forgetting Time (SSFT) to track when an example from the original training set (first split) is no longer correctly classified after the network is finetuned on a new randomly held out data partition (second split). The main contribution of the paper is showing how SSFT effectively identifies examples that are mislabeled while other metrics struggle to distinguish between different types of hard examples such as those belonging to rare or complex subpopulations. The authors conduct experiments across various datasets and modalities to validate SSFTs utility in identifying mislabeled samples improving model generalization and identifying machine learning models failure modes. Theoretical analysis on overparameterized linear models is provided to offer insights into the observed phenomena. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Metric: SSFT is a significant contribution addressing the limitations of existing metrics by effectively distinguishing between mislabeled examples and those that are merely rare or complex.  Broad Evaluation: The paper features a comprehensive set of experiments across different datasets and settings including image and language modalities which thoroughly demonstrates SSFTs effectiveness and general applicability.  Practical Applications: The practical applications described such as identifying mislabeled samples and failure modes of models are highly relevant and add to the papers impact.  Theoretical Analysis: The inclusion of a theoretical framework to explain the empirical observations lends depth to the study and provides a solid foundation for SSFTs utility. Weaknesses:  Limitations on Learning Rate Consideration: The paper mentions that SSFTs performance is depending on the choice of learning rate for the secondsplit training but does not explore this dependency in depth or propose solutions to mitigate it.  Lack of Comparison with StateoftheArt: While the paper compares SSFT with other hardness metrics it does not benchmark against the latest stateoftheart methods for identifying mislabeled examples or improving generalization in noisy data settings.  Underexplored Aspects of Complex Examples: The paper could further delve into identifying and treating complex examples which unlike mislabeled or rare examples remain a challenging aspect for model training and generalization. 3. Questions and Suggestions for the Authors:  Question about Learning Rate Dependency: Can you elaborate on how SSFTs effectiveness varies with different learning rates for secondsplit training Are there guidelines for choosing an optimal learning rate  Comparison with StateoftheArt Methods: It would be beneficial to see a direct comparison of SSFT with current stateoftheart methods for identifying mislabeled examples and enhancing model generalization. Could such comparisons be included  Treatment of Complex Examples: SSFT identifies mislabeled and distinguishes rare examples well but offers less on handling complex examples. Could the authors suggest approaches or modifications to SSFT that might help in better identifying and treating complex examples 4. Limitations and Societal Impact: The authors have addressed the limitations of their work particularly regarding the dependency on the learning rate for secondsplit training. However they could further explore automated or adaptive methods to select appropriate learning rates to ensure SSFTs robustness across different settings. The potential negative societal impact is not explicitly addressed. Given the application in identifying mislabeled data and improving model generalization it\u2019s crucial to discuss the ethical implications especially in sensitive applications. Constructive suggestions include a deeper ethical analysis of using SSFT in realworld scenarios and guidelines to mitigate any potential negative impacts such as biases in the identification of mislabeled samples. In summary this paper introduces SSFT a promising new metric for understanding example hardness in neural network training with significant potential for practical applications and theoretical insights.", "ypXcTtbBsnZ": "Review of the Paper \"UniCLIP: A Unified Framework for Contrastive Language\u2013Image Pretraining\" 1. Summary: The paper introduces UniCLIP a novel framework aimed at improving the efficiency and performance of vision\u2013language pretraining models by integrating the contrastive loss for both intradomain (image\u2013image text\u2013text) and interdomain (image\u2013text) pairs into a single unified space. The framework addresses key challenges in this integration process through three innovative components: augmentationaware feature embedding MPNCE loss and domaindependent similarity measures. These elements collectively enable UniCLIP to handle discrepancies in contrastive loss among different modalities and domains effectively. The paper provides extensive experimental results that demonstrate UniCLIPs superiority over existing vision\u2013language pretraining methods across multiple downstream tasks validating each components contribution to enhancing overall performance. 2. Assessment: Strengths:  Innovative Approach: UniCLIP\u2019s methodology of unifying contrastive objectives in a single embedding space is a significant innovation addressing a gap in previous models that treated intradomain and interdomain contrastive losses separately. This innovation potentially paves the way for more efficient and powerful vision\u2013language pretraining models.  Comprehensive Evaluation: The paper thoroughly evaluates UniCLIP across a variety of downstream tasks showing notable improvements over existing methods. This comprehensive evaluation strengthens the case for UniCLIPs effectiveness.  Detailed Component Analysis: A key strength is the detailed examination of each component within the UniCLIP framework. The ablation studies provide clear evidence of how each component contributes to the overall performance offering valuable insights into their roles in the frameworks design. Weaknesses:  Computational Efficiency Concerns: While the paper extensively discusses performance improvements there is somewhat limited discussion on the computational efficiency of UniCLIP. Considering the additional computation from handling multipositive pairs and domaindependent measures it would be beneficial to have a more thorough analysis of computational costs and potential overheads.  Generalization to Other Modalities: Although the paper mentions the potential for UniCLIP to extend to other multimodal datasets there are no experiments conducted beyond visionlanguage tasks. It leaves some uncertainty about the frameworks adaptability and performance in contexts beyond the ones tested. 3. Questions and Suggestions:  Extended Analysis on Computational Efficiency: Could the authors provide insights or benchmarks on the computational efficiency of UniCLIP compared to existing methods especially regarding training time and resource requirements  Further Exploration Beyond VisionLanguage: It would be highly beneficial to see experiments or case studies applying UniCLIP to other types of multimodal datasets (e.g. audiotext videotext) to validate its modalityagnostic claim and understand potential limitations.  Discussion on Scaling: The scalability of UniCLIP particularly in dealing with even larger datasets and complex multimodal tasks was not extensively covered. Can the authors comment on how UniCLIP might perform and be optimized for scaling in more demanding scenarios 4. Limitations and Societal Impacts: The paper briefly touches upon the potential generalizability of UniCLIP to other modalities but does not explicitly discuss the limitations or the negative societal impacts. Given the increasing scrutiny around largescale pretrained models especially concerning biases and ethical use it would be constructive to include a section dedicated to discussing these aspects. Suggestions for improvement might include incorporating fairness and bias evaluation metrics into the performance assessment or proposing methods to mitigate potential negative biases in the pretraining process. Engaging with these considerations would not only enhance the papers depth but also contribute to responsible AI research practices. In conclusion the paper presents a valuable contribution to the field of visionlanguage pretraining with its introduction of UniCLIP. Addressing the noted gaps and suggestions particularly around computational efficiency generalization beyond visionlanguage tasks and societal impacts could further strengthen the work.", "rQAJmrLmGC6": "Review of \"Twostream Universal Referring localization Network (TURN) for Zeroresource Sounding Object Localization\" 1. Summary The paper introduces the Twostream Universal Referring localization Network (TURN) a novel model aimed at the task of sounding object localization without relying on training data from the targeted field. This is achieved by extracting knowledge from image grounding datasets and transferring it to the task at hand via wellaligned audiotext corpora addressing the zeroresource challenge. The model comprises two distinct streams: the localization stream which harnesses referring object localization knowledge and the alignment stream which learns a universal semantic space for text and audio data. Additionally an adaptive sampling strategy is presented to enhance data domain overlap identification thereby improving model performance. The efficacy of TURN is demonstrated through extensive experiments on various benchmarks achieving competitive results despite its zeroresource nature. 2. Assessment  Strengths: 1. Innovation: The paper addresses the sounding object localization task from a novel zeroresource perspective utilizing crossmodal knowledge transfer which is a significant and original contribution to the field. 2. Comprehensive Methodology: The intricate design of TURN incorporating elements like a twostream architecture adaptive sampling and a crossreconstruction mechanism illustrates a wellthoughtout approach to tackle the challenges inherent in zeroresource sounding object localization. 3. Solid Experimental Validation: The authors provide extensive experimental evidence to support the effectiveness of their method outperforming stateoftheart models on multiple benchmarks despite the zeroresource setting.  Weaknesses: 1. Complexity and Scalability: The models complexity especially with the dualstream architecture and the need for prealigned audiotext data might impede scalability and practical utility. 2. Generalization Concerns: It remains unclear how well TURN can generalize across diverse and unstructured realworld datasets beyond the evaluated benchmarks. 3. Potential for Bias: The reliance on preexisting datasets for transferring knowledge might introduce biases particularly if those datasets are not representative of a broader range of sounds or visual scenes. 3. Questions and Suggestions:  Could the authors elaborate on how TURN might be adapted or extended to handle more dynamic unstructured realworld scenarios where clean prealigned audiotext pairs might not be readily available  It would be beneficial to see a deeper analysis of the models performance on datasets with a high degree of domain variance to evaluate its generalization capability further.  A discussion on the potential biases introduced by utilizing preexisting datasets for knowledge transfer would be valuable how might one mitigate such biases 4. Limitations and Societal Impact: The authors have made commendable efforts to address the limitations inherent in the zeroresource approach such as domain and modality gaps. However more emphasis could be placed on the scalability of the model and its applicability in diverse realworld scenarios. Additionally while TURN demonstrates the feasibility of zeroresource sounding object localization the potential societal impact including privacy concerns and the perpetuation of biases from source datasets could be further discussed. Constructive suggestions include exploring unsupervised or semisupervised extensions of TURN for broader applicability and conducting bias and fairness audits on model predictions.", "n7XbkHOwKn6": " Review of \"CogVideo: A Largescale Pretrained Transformer for TexttoVideo Generation\"  Summary This paper introduces CogVideo a 9.4 billion parameter transformer model designed for texttovideo generation. The model is purportedly the first opensource largescale pretrained model in this area. It significantly advances the field by utilizing a pretrained texttoimage model CogView2 thereby avoiding expensive training from scratch. A novel multiframerate hierarchical training strategy is employed to improve alignment between text and video clips resulting in markedly enhanced generation accuracy especially for complex movements. The authors support their claims with both machine and human evaluations demonstrating CogVideos superior performance compared to existing models.  Assessment Strengths: 1. Innovative Approach: Using a pretrained texttoimage model as a foundation for texttovideo generation is a novel and resourceful approach. This leverages existing knowledge drastically reducing the need for extensive datasets and computational resources. 2. Multiframerate Hierarchical Training: The proposed training methodology effectively addresses the challenge of aligning text with video clips by considering frame rate variability. This significantly enhances the generation quality particularly for videos that involve complex actions. 3. Comprehensive Evaluation: Evaluations are thorough encompassing both quantitative metrics and qualitative human judgment. This robust assessment demonstrates CogVideos advancement over prior models. Weaknesses: 1. Potential for Bias and Misuse: While the authors mention the potential for misinformation they do not deeply explore the societal implications of generating realistic videos from text. More discussion and possible safeguards against the misuse of such a tool for generating deepfake content are necessary. 2. Technical Limitations Not Fully Addressed: The paper briefly mentions limitations related to input sequence length due to GPU memory constraints but does not provide detail on how these limitations affect the models performance or potential workarounds. 3. Lack of Open Source Code and Data: At the time of the submission the authors did not make the code or training data openly available. This limits reproducibility and scrutiny of the results presented.  Questions and Suggestions for the Authors 1. Bias and Ethical Safeguards: Could the authors elaborate on any builtin safeguards in CogVideo to prevent its misuse especially considering the potential for creating deepfakes Are there any mechanisms to detect or watermark generated videos 2. Handling of Technical Limitations: Can the authors provide more insights into how the input sequence length limitation affects the models applicability Are there strategies in place or in development to overcome these constraints 3. Reproducibility and Open Source Code: When do the authors plan to release the code and training datasets Moreover could additional details on the models architecture and training be provided to facilitate independent replication of the studys findings  Limitations and Societal Impact The paper mentions potential negative societal impacts specifically the risk of misinformation through the generation of realistic videos. However the discussion on this topic is brief and lacks depth. Given the increasing concerns around deepfakes and their implications for privacy security and democracy its crucial that the authors expand on the ethical considerations of their work. Recommendations include the development of detection tools the incorporation of digital watermarks in generated videos and engagement with policy discussions on the regulation of synthetic media. In conclusion CogVideo represents a significant advancement in texttovideo generation showcasing impressive performance improvements. However for this work to be fully embraced by the broader research community and to ensure its responsible use more attention should be given to the ethical implications technical limitations and reproducibility of the results presented.", "yam42JWePu": "Review of \"LOUPE: A FineGrained Semantically Aligned VisionLanguage PreTraining Framework\" 1) Summary This paper presents LOUPE a novel visionlanguage pretraining framework that models finegrained semantic alignment between visual regions and textual phrases using gametheoretic interactions. The authors introduce an uncertaintyaware neural Shapley interaction learning module to efficiently compute the gametheoretic interactions addressing computational challenges. LOUPEs effectiveness is validated through stateoftheart performance on imagetext retrieval benchmarks and the successful transfer to object detection and visual grounding tasks in a zeroshot manner without requiring objectlevel human annotations or finetuning. The work provides an alternative approach to learning finegrained semantics from largescale raw imagetext pairs expanding the horizons for visionlanguage tasks. 2) Assessment Strengths:  Innovative Approach: The application of gametheoretic interactions to model finegrained semantic alignment is novel in the context of visionlanguage pretraining. It opens up a new avenue for understanding and leveraging the intricate relationships between visual and textual elements.  Efficiency: The uncertaintyaware neural Shapley interaction learning module significantly reduces computational costs while maintaining estimation accuracy. This addresses a critical challenge in leveraging Shapley values making the proposed framework more practical for largescale pretraining tasks.  Performance: LOUPE achieves new benchmarks on a variety of visionlanguage tasks demonstrating its effectiveness. The zeroshot transferability to tasks like object detection and visual grounding further underscores the models capacity to understand and generalize finegrained visual semantics. Weaknesses:  Dependency on OfftheShelf Tools: The reliance on existing constituency parsers for phrase extraction could introduce biases or inaccuracies potentially limiting the models understanding of semantic alignments.  Potential for Noisy Training Data: Since the model is pretrained on webcollected data there is a risk of incorporating noisy or mismatched imagetext pairs. This could affect the robustness and reliability of the learned representations. 3) Questions and Suggestions  On Constituency Parsers: Have the authors considered the impact of parser inaccuracies on the performance and robustness of LOUPE It would be valuable to understand how parser errors propagate through the model and potentially develop strategies to mitigate their impact.  Handling Noisy Data: Can the authors elaborate on the measures taken to ensure the quality of training data Moreover could unsupervised or semisupervised techniques be integrated into LOUPE to inherently filter or correct noisy alignments during training  Extension to Other Modalities: Given the frameworks success in visionlanguage tasks does LOUPE have the potential to be extended to other modalities (e.g. audio) for multimodal alignment learning Insights or preliminary results in this direction would be intriguing. 4) Limitations and Societal Impact The paper acknowledges the reliance on offtheshelf constituency parsers and the challenge of training on potentially noisy web data. These limitations are adequately addressed but further discussion on mitigating strategies would enhance the paper. Regarding societal impact while the acknowledgement of potential misuse (e.g. privacy surveillance) is appreciated a deeper exploration of ethical considerations and safeguards is encouraged to ensure responsible use and deployment of such powerful models. Constructive Suggestions for Improvement:  Exploring advanced or custom constituency parsing techniques could improve phrase extraction accuracy enhancing the models ability to learn finegrained semantic alignments.  Incorporating mechanisms for data quality assessment and noise reduction directly into the training process could improve the robustness of LOUPE against noisy webcollected data.  Expanding the discussion on ethical considerations and potential safeguards to prevent misuse would strengthen the papers consideration of societal impacts. In summary LOUPE represents a significant advancement in visionlanguage pretraining with its novel approach and strong performance across multiple tasks. Addressing the outlined questions and suggestions could further solidify the papers contributions and potential impact.", "wxWTyJtiJZ": " Peer Review of \"Modeling and Optimizing Product Ranking under MultiplePurchases with Budget Constraints\"  1) Summary of the Paper This paper addresses the problem of product ranking for online retailers aiming to maximize revenue focusing on realism by assuming consumers can make multiple purchases based on their random attention span and budget constraints. It introduces a novel consumer choice model that allows for multiple purchases and proposes the MultiplePurchasewithBudget Upper Confidence Bound (MPBUCB) algorithms to optimize product ranking in both noncontextual and contextual settings. The algorithms are designed to learn consumer behaviors over time and maximize revenue with theoretical regret bounds proven. Experiments on synthetic and semisynthetic datasets demonstrate the effectiveness of the approach with source code made available for replication and further research.  2) Assessment of Strengths and Weaknesses  Strengths: 1. Novelty: The consumer choice model introduced is more aligned with realworld scenarios than the assumptions in previous studies. It realistically allows for multiple purchases and incorporates attention span and budget constraints. 2. Theoretical Contribution: The paper provides a rigorous theoretical framework for the MPBUCB algorithms including regret analysis which is crucial for understanding the efficiency and practicality of the proposed approach. 3. Practical Impact: By focusing on revenue maximization under realistic consumer choice models this work has direct implications for online retailers potentially offering them a more effective productranking algorithm to boost sales. 4. Replicability: The provision of source code enhances the papers credibility and allows for the validation of results and further experimentation by other researchers.  Weaknesses: 1. Complexity of Implementation: The introduction of random attention spans and purchase budgets while realistic adds complexity to the model which might hinder its ease of implementation in practical online retail settings without significant computational resources. 2. Assumption on Independence: The assumption that consumer interests in products are independent might not hold in all scenarios especially when complementary or substitutable goods are involved. 3. Limited Exploration of Consumer Diversity: The paper could expand its exploration into how diverse consumer segments with vastly different behaviors might affect the models effectiveness.  3) Questions and Suggestions for the Authors  Could the authors discuss the potential scalability of their algorithms in more detail considering the computational complexities involved  Have the authors considered extending their model to account for the interdependence between products such as substitutable and complementary goods which might affect purchasing behavior  It would be beneficial if the authors could address how the model and algorithms might adapt or be customized for different types of online retail platforms e.g. highend luxury goods versus everyday consumer products.  A suggestion would be to include a more diverse set of semisynthetic or realworld datasets to test the robustness of the proposed algorithms across different retail contexts.  4) Limitations and Societal Impact The authors have addressed the limitations related to the assumptions made for modeling consumer behavior but could further elaborate on the limitations regarding data diversity and realworld applicability. Additionally there is a missed opportunity to discuss the potential negative societal impacts such as privacy concerns with data collection and the possible reinforcement of consumerism. Constructively the authors could explore incorporating ethical considerations into the model such as privacypreserving algorithms and promoting sustainable purchasing practices. In conclusion this paper makes significant contributions to the field of online retailing and consumer choice modeling presenting a novel and more realistic approach to product ranking for revenue maximization. While there are some areas for improvement and expansion the work lays a solid foundation for future research in this domain.", "v7SFDrS44Cf": "Review of \"FLEXSUBNET: Flexible Neural Modeling of Submodular and \u03b1Submodular Functions for Data Selection and Summarization\" 1. Summary: This paper introduces FLEXSUBNET a flexible neural model for both monotone and nonmonotone submodular as well as \u03b1submodular functions with applications in data selection and summarization tasks. The authors propose a novel method for estimating these functions using (set value) pairs and (perimeterset highvaluesubset) pairs. They achieve this by applying a concave function on modular functions in a recursive manner using a differentiable quadrature procedure. Moreover they introduce a novel permutation adversarial training method for efficient and differentiable subset selection. Extensive experiments on synthetic and real datasets demonstrate the superiority of FLEXSUBNET over several baselines in terms of accuracy and efficiency. 2. Assessment: Strengths:  The paper addresses a significant problem in the field of data selection and summarization by providing a novel approach to model both monotone and nonmonotone submodular functions as well as \u03b1submodular functions.  The introduction of a differentiable quadrature procedure for learning highly expressive neural models of concave functions is innovative and may be of independent interest beyond the scope of this work.  The proposal of a permutation adversarial training method adds a novel dimension to the training of such models potentially opening up new avenues in subset selection methods.  Extensive experiments with both synthetic and real datasets not only validate the effectiveness of FLEXSUBNET but also underscore its superiority over existing baselines. Weaknesses:  Despite the novelty the complexity of the proposed models and the training procedure might present a steep learning curve and computational challenges in practical applications.  The paper could benefit from a deeper theoretical analysis concerning the convergence properties of the proposed training method and the scalability of the approach to larger datasets and more complex submodular functions.  Although the paper discusses the application of FLEXSUBNET in various data selection tasks it lacks a broader exploration of potential applications especially considering the models capability to handle nonmonotone submodular functions.  The societal impact discussion is rather brief. Given the broad applicability of submodular functions a more thorough consideration of potential negative impacts such as privacy implications would be valuable. 3. Questions and Suggestions for the Authors:  Can the authors provide further insights into the underlying theoretical guarantees if any associated with the permutation adversarial training method particularly concerning convergence  It would be beneficial if the authors could elaborate on the computational demands of FLEXSUBNET especially in comparison to the baselines. Are there specific scenarios where the proposed method might not be feasible due to its computational complexity  Considering the adaptability of FLEXSUBNET to both monotone and nonmonotone functions are there specific domains or applications beyond data summarization where the authors envisage the model could have a significant impact  On the broader societal impacts could the authors speculate on any potential misuse or ethical considerations that might arise from the deployment of such powerful data selection and summarization tools 4. Limitations and Societal Impact:  The authors somewhat address the limitations particularly noting the complexity of the model. However a more comprehensive discussion regarding scalability and computational efficiency would enhance the paper.  The societal impact section could be expanded to provide a detailed analysis of how the technology could be used responsibly considering its potential misuse in sensitive applications such as surveillance or in manipulating information through biased summarization. Suggestions for mitigating such risks including transparency mechanisms and ethical guidelines for deployment would be constructive additions.", "vDeh2yxTvuh": "Review of \"A Comparative Analysis of FlatMinima Optimizers in Deep Learning\" 1. Summary of the Paper The paper provides a comprehensive comparison of two popular flatminima optimizer approaches: Stochastic Weight Averaging (SWA) and SharpnessAware Minimization (SAM) which are aimed at improving the generalization of neural networks by finding flatter minima. Through extensive benchmarking across different domains (including computer vision natural language processing and graph representation learning) and analysis of the loss landscapes the paper reveals several insights into the characteristics and performance of these optimizers. Their study discovers that while both optimizers can outperform traditional methods in some contexts their effectiveness varies significantly across different tasks and architectures. 2. Assessment of Strengths and Weaknesses Strengths: a. Thorough Benchmarking: The extensive set of experiments across diverse domains provides a strong empirical foundation for the paper\u2019s conclusions making it a valuable resource for both practitioners and researchers. b. Depth of Analysis: The investigation into the loss landscapes and the detailed examination of how and why these optimizers impact model performance offer deep insights into their mechanisms of action. c. Practical Guidance: The findings lead to actionable recommendations for selecting optimizers based on task and architecture which is highly beneficial for practitioners. Weaknesses: a. Limited Theoretical Insights: While the empirical results are robust the paper could benefit from a more detailed theoretical discussion on why certain patterns are observed across tasks and models. b. Optimizer Hyperparameters: The study underlines the sensitivity of both SWA and SAM to their respective hyperparameters but does not explore the process of tuning these in depth. This omission might leave practitioners wishing for more guidance. c. Reproducibility Concerns: Though the paper commits to full reproducibility including sharing code and model checkpoints the complexity and diversity of experiments might make it challenging for others to exactly replicate the study. 3. Questions and Suggestions for the Authors a. Theoretical Underpinning: Can the authors provide more theoretical insights or hypotheses on the observed performance variations of SWA and SAM across different tasks b. Hyperparameter Tuning: It would be beneficial to include more details on the hyperparameter selection process for both optimizers. Are there any systematic approaches or heuristic rules that could guide practitioners c. Future Directions: Given the nuanced performance of these optimizers what are the authors thoughts on the next steps in developing or identifying optimization strategies that combine the strengths of SWA and SAM d. Limitations on Generalization: The paper showcases instances where flat minima optimizers failed to outperform baseline. Could the authors speculate on the characteristics of tasks or models where flatminima might not hold the promised benefits 4. Limitations and Impact The authors have made a commendable effort to address the limitations of their study and suggest future work directions. However the discussion on the potential negative societal impact is notably missing. As these optimization methods are further refined and possibly deployed in realworld applications understanding their limitations and misapplications is crucial. For instance overreliance on such optimizers without proper understanding or precaution could potentially lead to models that are brittle in the face of adversarial attacks or unexpected inputs. The authors are encouraged to reflect on these aspects and provide a brief analysis of the potential negative societal impacts particularly in highstakes applications. In Conclusion: This paper provides a significant contribution to the field of neural network optimization by systematically comparing the performance of SWA and SAM across a broad set of tasks. While it shines in its empirical depth further theoretical insights detailed guidance on hyperparameter tuning and a discussion on societal impacts would enhance its value.", "pF5aR69c9c": " Peer Review for \"MemoryConstrained Policy Optimization for Reinforcement Learning\"  1) Brief Summary The paper presents MemoryConstrained Policy Optimization (MCPO) a new approach in policy gradient reinforcement learning aimed at enhancing optimization by employing a dual trustregion mechanism. Unlike traditional methods that only consider the immediate previous policy for setting the trust region MCPO innovates by introducing a virtual policy which is a dynamic amalgamation of multiple past policies. This virtual policy determined by neural networkweighted memory of past policies is used alongside the previously accepted policy to form dual trust regions. Through this method MCPO aims to avoid the pitfalls of suboptimal policy updates and local optima that other models may succumb to due to reliance on a single trust region. The efficacy of MCPO is demonstrated across various environments and challenges including robotic locomotion navigation with sparse rewards and Atari games showing competitive and often superior performance to current stateoftheart methods.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The introduction of a dynamic virtual policy as a secondary trust region derived from a comprehensive memory of past policies is a novel contribution. This allows for more robust optimizations that arent as easily hindered by the shortcomings of the most recent policy.  Extensive Evaluation: The authors thoroughly evaluate MCPO across a variety of tasks and environments displaying its versatility and effectiveness relative to other contemporary methods.  Detailed Theoretical Framework: The paper provides a solid theoretical underpinning for MCPO including discussions on the importance of dual trust regions and the mechanism for dynamically building the virtual policy. Weaknesses:  Complexity and Overhead: The introduction of additional components such as the policy memory mechanism and dual trust regions adds to the complexity of the optimization process. This may lead to increased computational overhead.  Limited Exploration of Hyperparameter Sensitivity: While the paper tests a range of environments theres limited discussion on how sensitive MCPOs performance is to its hyperparameters such as the sizes of the policy memory (N) and the beta coefficients (\u03b2).  Potential for Overfitting to Past Policies: The reliance on past policies to form the virtual policy particularly in dynamic and unpredictable environments could potentially lead to overfitting if not managed correctly.  3) Questions and Suggestions for Authors  Hyperparameter Sensitivity: Could the authors provide more insights into how MCPOs performance is influenced by its key hyperparameters particularly in more complex environments  Comparison with Baseline Methods: It would be beneficial if the authors could include comparisons with a broader set of baseline methods especially those that might employ similar memory mechanisms or dual constraint strategies.  Exploration vs. Exploitation Tradeoff: How does MCPO balance the exploration vs. exploitation dilemma particularly given its reliance on past policies for guiding updates  Scalability: Could the authors discuss the scalability of MCPO to even more complex environments or those with higher dimensional stateaction spaces  4) Limitations and Potential Negative Societal Impact The authors have somewhat addressed the limitations by acknowledging the introduction of new components and hyperparameters which might complicate the optimization process. However they could elaborate more on potential challenges in scaling and the impact of hyperparameter sensitivity on the robustness of MCPO. Regarding societal impact the paper briefly mentions the potential for misuse in developing harmful RL agents. It would be constructive to suggest frameworks or guidelines to mitigate such risks ensuring the responsible use of MCPO and similar technologies in reinforcement learning applications. Further a more detailed discussion on the safeguarding measures for unethical uses and consideration of safety during exploration could enhance the papers approach to addressing potential negative societal impacts.", "wfel7CjOYk": "Review of the Paper: \"AllInOne Neural Composition for ResourceAdaptive Federated Learning\" 1. Brief Summary This paper presents a novel approach to Federated Learning (FL) named AllInOne Neural Composition to address the challenge of system heterogeneity \u2013 the variation in computational resources among clients participating in FL. Traditional FL systems assume uniform processing capacity which does not hold in realworld scenarios. The proposed method facilitates the construction of complexityadjustable models by employing a unified neural basis for soft and learnable model construction. This allows for efficient model training across devices of varying computational capacity without compromising the models access to the full range of knowledge across clients. The authors demonstrate the effectiveness of the technique through extensive experiments across both system and data heterogeneity settings showing significant performance improvements over existing FL strategies. 2. Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The paper addresses a crucial but often overlooked aspect of FL \u2013 system heterogeneity. The innovative AllInOne Neural Composition method is a significant departure from traditional approaches focusing on pruning presenting a new paradigm for constructing complexityflexible models.  Comprehensive Evaluation: The experiments are extensive and welldesigned covering a range of scenarios including system and data heterogeneity. The comparison with stateoftheart heterogeneityaware methods HeteroFL and FjORD adds value by demonstrating the superiority of the proposed approach.  Practical Relevance: By tackling system heterogeneity the work has high practical relevance. It makes FL more accessible and practical for realworld applications where devices with varying computational capacities are the norm. Weaknesses:  Complexity and Overhead: The paper does not adequately address the potential computational overhead and complexity introduced by the AllInOne Neural Composition especially in terms of training time and the requirement for orthogonal regularization.  Generalization to NonImage NonNLP Tasks: While the method shows impressive results on image classification and nextcharacter prediction tasks its applicability and performance on other types of tasks (e.g. regression tasks reinforcement learning) remain unclear.  Lack of Discussion on Scalability: The scalability of the proposed approach in extremely large federated networks is not discussed. The effectiveness and efficiency of the method in scenarios with thousands of devices participating in the FL process would be valuable to consider. 3. Questions and Suggestions for Authors  Generalization and Applicability: Can the authors provide insights or preliminary results on the applicability of their method to nonimage classification and nonNLP tasks such as regression tasks common in IoT applications  Scalability Considerations: It would be beneficial if the authors could discuss the scalability of their approach in larger FL setups. Are there any anticipated bottlenecks or limitations in scaling up the number of devices  Comparison with StateoftheArt: While the comparison with HeteroFL and FjORD is insightful including comparisons with recent advances in knowledge distillation or other neural composition methods could provide a fuller picture of the proposed methods positioning in the field. 4. Limitations and Societal Impacts The authors have not fully discussed the limitations related to the potential increase in computational overhead especially during the training phase. Additionally the discussion on the negative societal impact is missing. It would be advisable for the authors to consider scenarios where their method could potentially exacerbate biases due to uneven data distributions or where the reduced model complexity for lowercapacity devices could lead to lower performance potentially disadvantaging certain user groups. Suggestions for Improvement:  A more detailed analysis of the computational overhead and strategies to mitigate it would strengthen the paper.  The authors should consider analyzing or discussing the ethical implications and potential negative societal impacts of their work including strategies to mitigate bias and ensure fairness across devices of varying capacities. Overall the paper presents a significant contribution to FL addressing critical challenges of system heterogeneity. With improvements addressing the highlighted weaknesses and limitations this work has the potential to influence future research and applications in federated learning.", "n3lr7GdcbyD": " Peer Review on \"Advancements in the MonteiroSvaiter Acceleration Framework for Convex Optimization\"  1. Summary of the Paper This paper introduces significant advancements within the MonteiroSvaiter (MS) acceleration framework primarily focusing on convex optimization challenges. By refining the MS acceleration process the authors develop a variant that circumvents the requirement for solving an expensive implicit equation iteratively. The paper demonstrates how this refinement can improve the complexity of convex optimization especially for functions with Lipschitz continuous pth derivatives by a logarithmic factor. The authors propose an adaptive MonteiroSvaiterNewton (OaMSN) oracle and its firstorder counterpart (OaMSNfo) which enable more efficient optimization without preknowledge of problem parameters. In practical terms their methods show promising results on logistic regression problems outperforming several existing secondorder acceleration methods albeit with some underperformance against Newtons method in certain cases.  2. Strengths and Weaknesses Strengths:  Novelty: The paper addresses a previously open question in optimization theory presenting a unique approach to removing bisection from the MS framework. This is a substantial theoretical contribution.  Practical Implications: The introduction of the adaptive OaMSN oracle and its firstorder implementation (OaMSNfo) offers practical tools for optimizing convex functions particularly enhancing the efficiency and applicability of secondorder methods.  Theoretical Rigor: Comprehensive mathematical proofs underpin the theoretical advancements presented ensuring the robustness and reliability of the results.  Empirical Validation: The authors provide extensive empirical evidence to support their claims including comparisons with established methods demonstrating the practical value and efficiency of their proposed methods. Weaknesses:  Underperformance in Certain Scenarios: While the method outperforms previous acceleration schemes it underperforms against Newtons method for logistic regression tasks. This limitation may restrict its applicability in problems where Newtons method is already efficient.  Scalability Concerns: Despite improvements the firstorder oracle still relies on exact gradient and Hessianvector products which might limit scalability to very large datasets. Further research into stochastic gradient estimates could help mitigate this issue.  3. Questions and Suggestions for the Authors  Clarification on Newtons Method Performance: It would be beneficial if the authors could provide more insight or speculation on why Newtons method outperforms the proposed acceleration framework in logistic regression problems.  Future Directions: The paper mentions the potential for scalability improvements through stochastic gradient estimates. Could the authors elaborate on how they envision these improvements being implemented and their impact on the frameworks performance  Parameter Sensitivity: How sensitive is the performance of the proposed methods to the choice of parameters \u03b1 and \\xce\\x9b Would a more detailed parameter sensitivity analysis provide further insights into optimizing the methods performance across different problem settings  4. Limitations and Societal Impact The authors briefly discuss the limitations of their work including its practical performance in certain scenarios and scalability concerns. A more thorough examination of these limitations would strengthen the paper. For example exploring the frameworks performance in a wider range of optimization problems could provide a more comprehensive understanding of its applicability and limitations. Regarding potential negative societal impacts the paper does not delve into this aspect. While the papers focus is on a theoretical advancement in optimization it is important to consider the broader implications of optimization technologies especially in terms of privacy security and potential misuse in decisionmaking processes that may affect individuals and communities. Including a discussion on these topics even if speculative would be beneficial for a holistic understanding of the works impact. Suggestions for Improvement:  To address the underperformance against Newtons method further research into adapting the momentum mechanisms or exploring hybrid approaches might yield improvements.  Investigating and proposing solutions to enhance scalability possibly through stochastic methods would make the framework more versatile and applicable to a broader set of optimization tasks.  Incorporating a discussion of potential negative societal impacts along with guidelines for ethical and responsible use of the proposed optimization methods would be a valuable addition to the paper.", "pBz3h8VibKY": " Peer Review on \"Extending InteractionGrounded Learning for ActionInclusive Feedback Vectors\"  1) Summary of the Paper This paper tackles the challenge presented in InteractionGrounded Learning (IGL) when the feedback vector encompasses the action itself making prior approaches ineffective for application areas like BrainComputer Interfaces (BCI) and HumanComputer Interfaces (HCI). The authors introduce a novel methodology called actioninclusive IGL (AIIGL) that integrates action into the feedback while maintaining effective policy optimization. Leveraging contrastive learning for latent reward grounding and a symmetry breaking process for semantic meaning identification the paper provides theoretical guarantees for learning nearoptimal policies under a relaxed context conditional independence assumption. The approach is empirically validated using largescale supervised datasets from OpenML and a simulated BCI experiment showcasing its superior effectiveness compared to previous methods.  2) Assessment of Strengths and Weaknesses Strengths: a. Theoretical Contribution: The paper successfully challenges and extends the current limitations of IGL. Through rigorous theoretical analysis it provides a clear framework for incorporating action information within feedback vectors and guarantees for successful policy learning. b. Practical Relevance: By addressing a realworld issue in BCI and HCI applications the paper significantly contributes to the practical application of interactiongrounded learning showing promise for advancement in assistive technologies. c. Empirical Validation: The extensive experiments conducted using OpenML datasets and a simulated BCI experiment not only validate the proposed method but also demonstrate its adaptability and efficiency across different domains. Weaknesses: a. Complexity and Accessibility: The heavy theoretical focus and complex mathematical formulations may render the paper less accessible to practitioners or researchers from adjacent fields who could benefit from the proposed method. b. Binary Focus: The papers focus on binary latent rewards may limit its applicability to scenarios where rewards are multiclass or continuous potentially narrowing the broader impact. c. Lack of RealWorld Testing: While simulated environments provide a controlled setup for validation the absence of realworld experiments particularly in BCI contexts may raise questions about practical implementation challenges and effectiveness.  3) Questions and Suggestions for the Authors a. Extending Beyond Binary Rewards: Could the authors discuss potential extensions of their AIIGL approach to accommodate multiclass or continuous reward scenarios This could broaden the applicability of the method. b. Practical Implementation: It would be beneficial if the authors could provide insights or case studies on implementing AIIGL in realworld BCI or HCI applications addressing potential operational challenges. c. Interdisciplinary Accessibility: Given the technical depth what additional resources or explanations could be provided to make the paper more accessible to a wider audience including practitioners in neuroscience or HCI design  4) Limitations and Societal Impact While the authors briefly address potential negative impacts related to performance instability a more thorough discussion on limitations and the ethical considerations of deploying AIIGL in sensitive applications would be valuable. For instance discussing how misinterpretations of brain signals in a BCI context could lead to incorrect actions and the measures to mitigate such risks. Additionally exploring the potential for data privacy concerns especially when dealing with sensitive neurological data and proposing safeguarding mechanisms would strengthen the paper.  Conclusion This paper presents a significant step forward in the domain of InteractionGrounded Learning successfully tackling the challenge of actioninclusive feedback vectors. While it stands out for its theoretical rigor and empirical validation enhancing its accessibility and practical applicability considerations could amplify its impact across a wider spectrum of disciplines and applications.", "odOQU9PYrkD": "Review for \"Structuring Uncertainty in Stochastic Segmentation Networks\" 1. Summary of the Paper: This paper addresses limitations in contemporary image segmentation techniques that typically do not account for the inherent noise and ambiguity in data or the disagreements among experts regarding the correct segmentation. To tackle these challenges the authors propose an innovative method within the domain of Stochastic Segmentation Networks (SSNs) that dissects overall predicted uncertainty into finer factorspecific uncertainty components. This approach leverages a lowrank Gaussian distribution for the logits in the SSNs network head and interprets it as a factor model. By manipulating these factorspecific components the paper introduces a method for finegrained sample control enhancing interpretability and adjustment capabilities in segmentation tasks. The methodology is further enriched with the introduction of flow probabilities for quantifying deviations and visualizing uncertainties. Through extensive experiments across different datasets the paper demonstrates the effectiveness of utilizing rotated factors for finetuned control in image segmentation. 2. Assessment of Strengths and Weaknesses:  Strengths: a. Originality: The concept of breaking down the overall uncertainty into factorspecific components and using these for finegrained control is novel and presents a significant advancement over traditional deterministic or probabilistic segmentation techniques. b. Interpretability: By structuring the uncertainty into meaningful components this method offers improved interpretability making it easier for practitioners to understand and manipulate the uncertainty in segmentation tasks. c. Practical Relevance: The experimental validation covers a broad range of applications from medical imaging to land classification and traffic scene analysis indicating the methods wide applicability. d. Comprehensive Evaluation: A thorough experimental setup along with the introduction of the rotation criteria based on factorspecific flow probabilities provides a strong case for the methods effectiveness.  Weaknesses: a. Computational Complexity: The method appears to be computationally intensive especially in the computation of rotations based on flow probabilities. This could limit its scalability to very large datasets or realtime applications. b. Dependence on SSN Performance: The methods effectiveness is inherently tied to the base performance of the Stochastic Segmentation Networks. Any limitation in SSNs ability to predict aleatoric uncertainty could impact the overall results. c. Lack of Comparison with NonSSN Methods: While the paper focuses on SSNbased methods a comparison with advanced nonSSN probabilistic segmentation techniques could provide a more rounded view of its relative performance. 3. Questions and Suggestions for the Authors:  Could you elaborate on the computational complexity of the proposed method especially regarding the optimization of rotations and suggest potential ways to mitigate it  It would be beneficial if you could compare the proposed methods performance with nonSSN probabilistic segmentation techniques to showcase its advantages or potential shortcomings.  Can the method be adapted or extended to other types of neural network architectures beyond SSNs especially those used in other domains of computer vision  A detailed analysis of the methods scalability to higher resolution images or larger datasets could provide insight into its practical applicability in realworld scenarios. 4. Limitations and Societal Impact: The authors have not thoroughly addressed the potential limitations related to the computational demands and scalability of their method. Suggestions for improvements include exploring optimization techniques or parallel processing to enhance computational efficiency. Additionally the societal impact particularly in sensitive applications such as medical imaging should be considered. Incorrect or overly simplified segmentations could lead to misinterpretations. Hence a deeper exploration of the ethical implications in highstakes applications is advised. Conclusively this paper introduces a promising and innovative approach for handling uncertainty in image segmentation tasks offering both enhanced interpretability and practical control mechanisms. With attention to the cited weaknesses and considerations for broader impacts the proposed method has the potential to significantly advance the field of image segmentation.", "yJEUDfzsTX7": "Review of \"RiskSensitive Reinforcement Learning with Optimistic Regret Bounds\" 1. Summary The paper presents pioneering work in the field of risksensitive reinforcement learning especially focusing on safetycritical applications like healthcare and robotics. It diverges from the conventional expected reward objective emphasizing the need to account for tail outcomes. Specifically it offers theoretical advancements by proving the first regret bounds for a broad class of risksensitive objectives that include the CVaR (Conditional ValueatRisk) among others. Utilizing a novel optimistic Markov Decision Process (MDP) construction and characterization of the CVaR objective this work provides both theoretical insights and algorithmic strategies to minimize regret under risksensitive objectives in episodic settings. 2. Assessment Strengths: a. Theoretical Novelty and Rigor: The paper addresses a significant gap in the literature by providing the first regret proof for the popular and critical CVaR objective contributing a solid theoretical advancement in risksensitive reinforcement learning. b. Generalization of the RiskSensitive Objectives: The authors consider a broad spectrum of risk measures beyond CVaR showcasing the applicability of their approach across various risksensitive reinforcement learning scenarios. c. Practical Significance: Given the application areas of robotics and healthcare where risk consideration is paramount this work has substantial potential for societal impact by making reinforcement learning algorithms safer and more reliable. Weaknesses: a. Complexity: The theoretical groundwork while robust is complex and could limit the accessibility of the concepts to practitioners or researchers without a strong background in advanced mathematics. b. Empirical Validation: The experimental section primarily focuses on a simple frozen lake environment which might not fully capture the complexities of realworld applications in healthcare or robotics. Additional experiments in more complex settings could enhance the papers impact. 3. Questions and Suggestions a. Scalability: How does the proposed approach scale with the complexity of the MDP (e.g. state space action space) Some discussion or analysis on computational complexity would be valuable. b. Implementation Details: Are there any specific challenges or recommendations for practitioners looking to implement the proposed optimistic MDP construction in more complex environments c. Comparison with Other Risk Measures: The paper mainly compares the proposed risksensitive objectives with entropic risk measures. It might be insightful to compare with other risk measures or reinforcement learning strategies that consider risk. d. RealWorld Applications: Could the authors discuss potential applications of their work in specific healthcare or robotics scenarios 4. Limitations and Societal Impact The authors have made an effort to address the limitations by providing theoretical and empirical evidence supporting their approach. However theres limited discussion of the potential negative societal impacts especially if the proposed methods are incorrectly implemented or if the risksensitive objectives do not align well with realworld scenarios. It would be beneficial for the authors to: a. Risk of Misinterpretation: Briefly discuss how misinterpretation of the risk measures could lead to undesired behaviors in safetycritical applications. b. Deployment Guidelines: Provide some guidelines or considerations for deploying these algorithms in realworld settings to mitigate potential negative impacts. c. Extensions to Complex Environments: Suggest pathways or considerations for extending this work to more complex realworld environments that have inherent uncertainties and risks not captured in simpler models. In conclusion this paper makes a significant contribution to the field of risksensitive reinforcement learning by providing muchneeded theoretical and algorithmic insights. Addressing the points mentioned could further strengthen the papers contributions and its applicability to realworld safetycritical applications.", "prKLyXwzIW": "Review of the Paper on Robustified GMM Estimators 1) Summary This paper introduces a robustified Generalized Method of Moments (GMM) estimator designed to mitigate the adverse effects caused by outliers in the data particularly in statistical and econometric inferential problems. The novel estimator is capable of tolerating a constant fraction of adversarially corrupted samples while providing an `2 recovery guarantee. This advancement is made possible by leveraging recent developments in algorithmic robust statistics and extending them to the more complex setting of GMM. Additionally the paper demonstrates the application of this new algorithm to instrumental variables linear regression with heterogeneous treatment effects showing significant improvements in tolerance to data corruption over traditional methods. 2) Assessment of Strengths and Weaknesses Strengths:  The paper addresses a critical gap in robust statistics by extending the resilience of estimators to the GMM framework which is widely used across different fields including econometrics and causal inference.  A theoretical contribution of offering `2 recovery guarantees under the presence of a constant fraction of corrupted data samples is significant as it enhances the reliability of statistical inference under adverse conditions.  The paper\u2019s methodology is wellgrounded in recent literature on algorithmic robust statistics ensuring that the approach is both innovative and built upon solid theoretical foundations.  Experimental results particularly in the application to instrumental variables linear regression showcase the practical effectiveness of the proposed estimator evidencing a tangible improvement over existing approaches in terms of corruption tolerance. Weaknesses:  While the paper provides experimental evidence of the estimators performance in specific contexts there is a limited exploration of its behavior across a broader range of scenarios which may affect the generalizability of the results.  The computational complexity of the proposed robustified GMM estimator is not thoroughly discussed. Given the potential for increased computational demands due to the robustification process insights into the algorithm\u2019s scalability and efficiency would be valuable.  The paper could benefit from a deeper theoretical comparison with existing robust estimators beyond baseline methods particularly in illustrating the tradeoffs involved in achieving robustness against corrupted data samples.  While the paper briefly discusses potential limitations and societal impacts a more comprehensive analysis in this regard would strengthen the paper especially considering the increasing reliance on automated decision systems in sensitive applications. 3) Questions and Suggestions for the Authors  Could the authors provide more insights into the computational complexity and scalability of the proposed estimator especially in comparison to traditional GMM estimators  It would be beneficial to see a broader range of experimental scenarios including different types of data corruption and varying distributional assumptions to better understand the robustified estimator\u2019s generalizability.  A deeper theoretical comparison with other robust estimators in the literature could highlight the unique strengths and potential drawbacks of the proposed method more explicitly.  Expanding on the discussion of limitations and societal impacts particularly considering the deployment of such robust estimators in critical applications (e.g. healthcare finance) could contribute to a broader understanding of the algorithms practical implications. 4) Limitations and Societal Impacts The authors have made an initial attempt to address the limitations and potential negative societal impacts of their work. However this discussion could be expanded to offer a more nuanced view of the risks associated with deploying robustified estimators in automated decisionmaking systems. Specifically while improving the tolerance to corrupted data is generally desirable it may also inadvertently increase the resistance of such systems to oversight and correction by obscuring the effects of biased or tampered data inputs. Constructively the authors could explore mechanisms for ensuring that increased robustness does not compromise the transparency and accountability of automated decision systems potentially through the integration of diagnostic tools or interpretable machine learning techniques.", "u6p_NvZ23qt": "This paper presents a significant advancement in the understanding of generalization in overparameterized settings which are commonly encountered in modern machine learning. The authors introduce a novel generalization bound that uses the Moreau envelope of continuous loss functions to control the test error through Rademacher complexity and training error. They demonstrate the versatility of this approach by applying it to linear regression with square loss maxmargin classification through squared hinge loss and more broadly to any Lipschitz and smooth nonnegative losses. Additionally they establish nonasymptotic sharpness for empirical risk minimizers and extend their generalization theory beyond the confines of proportional scaling and model misspecification contributing to a deeper understanding of minimalnorm interpolation and its consequences on model generalization.  Strengths 1. Theoretical Rigor and Novelty: The paper introduces a notably rigorous and novel theoretical framework. It builds upon and extends the localized uniform convergence technique introducing a nonasymptotic generalization bound that is broadly applicable across different losses and settings surpassing the limitations of existing theories significantly. 2. Broad Applicability: The authors successfully apply their theory across various settings demonstrating its versatility. From recovering the optimistic rate for linear regression under square loss to addressing more complex scenarios involving model misspecification the paper effectively showcases the broad applicability of their theoretical advancements. 3. Practical Insights: Beyond theoretical contributions the paper provides essential insights into the behavior of overparameterized models. For instance it sheds light on the phenomenon of benign overfitting offering conditions under which consistency can be achieved despite overparameterization. 4. Comprehensive Analysis: The paper is thorough in its approach including a detailed discussion of related work careful problem formulation and clear articulation of the theoretical contributions. The inclusion of numerical simulations adds value by validating the theorys applicability in practical scenarios.  Weaknesses 1. Assumptions Limitations: While the paper addresses the limitations of the Gaussian data assumption this still represents a significant constraint on the theorys applicability. The reliance on Gaussian assumptions may limit the generalization of the findings to more complex realworld datasets where such conditions are not met. 2. Complexity of Framework: The presented theoretical framework while powerful is highly complex. This complexity might pose challenges to practitioners in understanding and applying these insights to practical problems. Simplifying the framework or providing more intuitive guidelines for application could help make the contributions more accessible. 3. Empirical Validation: Although numerical simulations are provided the paper could benefit from more extensive empirical validation. Specifically applying the theory to larger more diverse datasets or realworld scenarios could significantly strengthen the evidence for its practical applicability.  Questions and Suggestions for the Authors 1. Beyond Gaussian Assumptions: Can the authors provide more insight into how the generalization bound might be adapted or extended beyond the Gaussian assumption Any ongoing work or preliminary findings in this direction would be highly valuable. 2. Framework Complexity: Could the authors suggest ways to simplify the theoretical framework or offer more intuitive guidelines for practitioners This could help bridge the gap between theoretical insights and practical application. 3. Additional Empirical Validation: Are there plans to validate the theoretical findings on a broader range of datasets especially those that do not necessarily meet the Gaussian data assumption Such validation would be compelling evidence of the theory\u2019s practical relevance.  Limitations and Improvements The authors have done an admirable job of acknowledging the limitations of their work particularly regarding the Gaussian data assumption. A future direction could involve extending the theory to a wider range of distributions potentially using universality arguments to broaden the applicability of the results. Additionally providing simplified versions or distilled insights from the theoretical framework could be a valuable contribution to the field making the findings more accessible to a broader audience of practitioners.", "x2WTG5bV977": " Review of \"Clarifying the Role of Task Diversity in FewShot Learning: A Comparative Study of MAML and Transfer Learning\"  1) Brief Overview The paper investigates the impact of task diversity in fewshot learning benchmarks and proposes a method to quantify it using a novel metric named the diversity coefficient. A comparative analysis of ModelAgnostic MetaLearning (MAML) and transfer learning under strictly controlled conditions reveals that task diversity influences the performance equivalence between these methods. The findings suggest that in scenarios of low task diversity MAML and transfer learning achieve similar performance levels. The paper contributes to the field by proposing a metric for task diversity demonstrating the conditions under which MAML and transfer learning perform equivalently and prompting a reevaluation of fewshot learning benchmarks.  2) Assessment Strengths:  The introduction of the diversity coefficient as a metric for quantifying task diversity in fewshot learning benchmarks is innovative and provides a tangible method for future studies in the field.  The paper meticulously controls the experimental conditions (architecture optimizer convergence) to ensure a fair comparison between MAML and transfer learning methods which strengthens the studys validity.  The analysis of model size variations offers valuable insights into how these factors relate to the performance of metalearning and transfer learning algorithms under different diversity conditions.  The inclusion of both synthetic and realworld data experiments enriches the study and provides a comprehensive understanding of the phenomena under investigation. Weaknesses:  The paper focuses primarily on fewshot learning benchmarks with low diversity and the implications for benchmarks with high task diversity are not thoroughly explored.  Although the study compares MAML and transfer learning extensively the exploration of other metalearning algorithms is limited which could have provided a broader perspective on the effects of task diversity.  The societal impact assessment particularly any potential negative implications of the work is missing. This omission is crucial given the growing importance of addressing ethical considerations in AI research.  3) Questions and Suggestions  Question: The paper establishes the similarity in performance between MAML and transfer learning in lowdiversity tasks. Is there any insight on the performance comparison in highdiversity tasks  Suggestion: Future work could extend the investigation to include additional metalearning algorithms and consider their performance under both low and high task diversity conditions. This would provide a richer understanding of the dynamics at play.  Suggestion: Given the novel nature of the diversity coefficient providing more detailed guidelines or frameworks on its application in creating or evaluating fewshot learning benchmarks could be beneficial for future researchers.  4) Limitations and Societal Impact The authors did not discuss the papers limitations nor its potential negative societal impact. Given the increasing scrutiny on the fairness and bias of AI algorithms overlooking these aspects may leave critical ethical considerations unaddressed. Constructive suggestions for improvement:  Limitations: A dedicated section discussing the limitations of the study such as its focus on specific fewshot learning benchmarks and the generalizability of the diversity coefficient would provide a more balanced perspective.  Societal Impact: The authors should consider adding a discussion on the potential negative implications of their findings especially if the methods studied could exacerbate biases in lowdiversity tasks. Suggestions on mitigating any identified risks could contribute to a more responsible deployment of these technologies. Overall the paper presents significant insights into the role of task diversity in fewshot learning and proposes a valuable metric for its quantification. Addressing the identified weaknesses and incorporating the suggested enhancements could further solidify the contribution of this work to the field.", "nLKkHwYP4Au": " PeerReview:  1) Briefly Summarize the Paper and its Contributions: The paper introduces CAGroup3D a novel twostage fully sparse convolutional 3D object detection framework designed for processing unordered sparse and irregular point clouds in challenging indoor scenes. The contributions of the work are manifold:  A classaware 3D proposal generation strategy that enhances semantic consistency within groups and acknowledges object shape diversity across different categories by utilizing a local group strategy on voxel surfaces with consistent semantic predictions.  The development of an efficient RoIConv pooling module that aggregates finegrained spatial information directly from the backbone network enabling better geometric feature encoding for 3D proposals and refining box predictions.  The proposed method demonstrates superior performance on ScanNet V2 and SUN RGBD datasets outperforming stateoftheart models with significant margins in terms of mean average precision (mAP) metrics.  2) Assessment of Strengths and Weaknesses:  Strengths:  Novelty and Technical Soundness: The introduction of classaware proposal generation and RoIConv pooling modules are novel contributions that significantly improve 3D object detection accuracy in dense point clouds.  Performance: The method achieves stateoftheart results on two challenging datasets which is a testament to the framework\u2019s efficacy in handling complex indoor scenes.  Efficiency: The fully sparse convolution approach is computationally efficient and reduces memory footprint making it suitable for largescale point cloud processing.  Weaknesses:  Generalization to Outdoor Scenes: The paper focuses predominantly on indoor scene datasets. While indoor scene complexity is welladdressed it is unclear how the proposed method would perform in outdoor settings with potentially different object size distributions and occlusion patterns.  IntraCategory Variance Handling: The paper mentions that intracategory variances pose challenges due to point cloud incompleteness and scale variance within classes. While acknowledging this as a limitation the paper does not provide a concrete solution or an exploration into how significantly this might affect the model\u2019s performance.  3) Questions and Suggestions for the Authors:  Outdoor Scene Performance: Could the authors provide insights or preliminary results on how CAGroup3D performs in outdoor scenes particularly in autonomous driving scenarios where 3D object detection is pivotal  IntraCategory Variance: Are there plans to address intracategory variances more explicitly in future work Suggestions include adaptive or hierarchical clustering mechanisms that can dynamically adjust to intracategory size and shape variances.  Limitations on Training Time: Given the efficiency improvements were there any notable effects on training times compared to existing models It might be beneficial for readers to understand the tradeoffs between computational efficiency and training duration.  4) Limitations and Suggestions for Improvement: The authors have made commendable efforts to address limitations pertaining to computational efficiency and semantic segmentation inaccuracies. However the treatment of potential negative societal impacts seems to be missing. While the technology itself is neutral its applications particularly in surveillance within indoor spaces could raise privacy concerns. It would be beneficial to include a discussion on ethical considerations and potential safeguards against misuse. Furthermore the acknowledgment of intracategory variance as an open problem is appreciated and future work could explore adaptive learning mechanisms that cater more expressly to these variances within object categories potentially enhancing the models robustness and general applicability across diverse scenarios.  Conclusion: CAGroup3D represents a significant advancement in fully convolutional 3D object detection addressing critical challenges in dense point cloud processing within indoor scenes. Its novel contributions performance gains and computational efficiency set a new standard in the field. Addressing its limitations and the posed questions could further elevate its standing and applicability to broader use cases including outdoor environments.", "wu1Za9dY1GY": "Review of \"Exploring the Connection Between Graph Neural Networks and Dynamic Programming Through Category Theory and Abstract Algebra\" 1) Summary This paper investigates the theoretical underpinning of the relationship between graph neural networks (GNNs) and dynamic programming (DP) by employing concepts from category theory and abstract algebra. The authors aim to formalize and quantify the observed alignment between GNNs and the DP strategy underlying many polynomialtime algorithms. Through a categorical analysis the manuscript identifies a foundational connection that allows for the reinterpretation of GNNs in terms of DP operations specifically focusing on the processing of edgecentric tasks. The paper contributes to the field by:  Proposing a novel perspective on the algorithmic alignment of GNNs with DP supported by category theory and abstract algebra.  Demonstrating the utility of this connection through improved GNN architectures for edgecentric algorithmic tasks.  Empirical evaluation on the CLRS algorithmic reasoning benchmark showing the proposed models\u2019 effectiveness over traditional GNN approaches. 2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Novelty: The paper presents an original approach by connecting GNNs and DP using category theory which is a significant departure from the prevalent empirical methods used to enhance GNN architectures.  Applicability: The insights from this theoretical exploration lead directly to the development of more aligned GNN architectures which empirically prove to be effective thus bridging the gap between theory and practical applications.  Comprehensive Evaluation: The authors perform extensive experiments to validate their theoretical constructions. The use of the CLRS benchmark adds credibility because it includes a variety of algorithmic tasks allowing the benefits of the proposed models to be observed across different contexts. Weaknesses:  Accessibility: The heavy reliance on category theory and abstract algebra may limit the accessibility of this work to a broader audience. Readers without a prior background in these areas might find it challenging to grasp the fundamental contributions and implications of the research.  Generalization beyond EdgeCentric Tasks: While the paper convincingly demonstrates improvements for edgecentric tasks it remains less clear how these findings generalize to other types of algorithmic reasoning tasks.  Discussion on Limitations and Societal Impact: The paper could benefit from a more detailed discussion on the limitations of the proposed approach particularly regarding the scalability of the models and their applicability to realworld datasets. Furthermore an exploration of the potential negative societal impacts given the increasing importance of ethical considerations in AI research is notably absent. 3) Questions and Suggestions for the Authors  Could you elaborate on the potential for generalizing the proposed approach to nonedgecentric tasks Are there specific adjustments or considerations that would need to be made to apply your findings in these contexts  A deeper discussion on the limitations of your approach would be beneficial especially concerning scalability and computation efficiency. How do the proposed models fare in terms of computational resources compared to standard GNNs  Given the complexity of the theoretical framework adding a more intuitive explanation or visualization of the core concepts (e.g. polynomial spans and integral transforms) could enhance the papers accessibility.  In light of increasing scrutiny on the societal impacts of AI technologies it would be prudent to discuss any potential negative impacts of your work. For instance could there be instances where enhanced algorithmic reasoning capabilities might be misused and if so what safeguards could be recommended 4) Limitations and Societal Impact The authors do not thoroughly address the limitations of their approach or its potential negative societal impact. To strengthen the paper the authors should consider including a section discussing these aspects. Specifically they could explore the scalability of the proposed models to larger or more complex graph structures and the computational demands relative to standard GNN approaches. Moreover considering the evolving landscape of AI ethics its crucial to assess how advancements in algorithmic reasoning might be leveraged irresponsibly and to suggest mitigating strategies. Such an exploration not only rounds out the paper but also aligns with the broader scientific communitys push towards responsible AI research. In conclusion the paper represents a significant theoretical advance with practical implications for GNN architecture design. Addressing the highlighted questions and suggestions could further elevate its contributions and relevance to the broader AI and machine learning communities.", "ymAsTHhrnGm": " Peer Review of \"Learning Follower Utility in Sequential Games with Bounded Rationality\"  1. Summary of the Paper This paper investigates a novel problem within inverse game theory focused on sequential games specifically Stackelberg games. The authors address the challenge of inferring a games underlying parameters based on observed equilibrium behaviors of agents under the label \"inverse game theory.\" Unlike previous works that assume perfectly rational followers this study adopts the quantal response (QR) model which accounts for bounded rationality a more realistic representation of agent behavior. The main contribution is the demonstration that bounded rationality leads to statistically and computationally more tractable learning of the follower\u2019s utility parameters in Stackelberg games. The authors develop an algorithm capable of efficiently learning these parameters under certain conditions related to the number of leader strategies and the sample complexity. They validate their theoretical findings with systematic empirical experiments showing the robustness of their approach even when extending beyond the strict QR model.  2. Assessment of Strengths and Weaknesses Strengths:  Originality: The paper addresses an underexplored area by focusing on learning in inverse game theory with bounded rationality. This approach provides a realistic depiction of agent behavior offering practical insights for realworld applications.  Technical Rigor: The authors present rigorous theoretical analysis including proofs and propositions to support their claims on the efficiency and effectiveness of learning follower utility parameters in Stackelberg games.  Empirical Validation: The systematic experiments conducted on synthesized games to validate the theoretical results add empirical strength to the study. It ensures the findings are not only theoretically sound but also practically applicable. Weaknesses:  Assumption Transparency: While the paper adopts the QR model for bounded rationality it could further clarify the limitations or specific conditions under which this model is assumed to accurately reflect realworld agent behaviors.  Comparison with Existing Methods: The paper could benefit from a more detailed comparison with existing methods in inverse game theory particularly in demonstrating the advantages of the proposed approach in terms of computational complexity and learning efficiency.  Broader Societal Impact Discussion: The discussion on the potential negative societal impacts of leveraging bounded rationality in learning game parameters is somewhat limited. A more thorough examination of ethical considerations especially in security or economic contexts would be valuable.  3. Questions and Suggestions for the Authors  Model Generalization: Could the authors elaborate on how their approach could be generalized to other types of games or different models of bounded rationality beyond the QR model  Algorithm Performance: It would be helpful to include a performance comparison of the proposed learning algorithm against conventional methods that assume perfect rationality. How do the results compare in terms of accuracy and computational efficiency  Impact of Bounded Rationality Level: The paper discusses varying levels of bounded rationality (parameter ). Can the authors provide more insights on how different levels of bounded rationality impact the learning efficiency and outcome accuracy Can there be a too high or too low level of bounded rationality where the method becomes ineffective  4. Limitations and Potential Negative Societal Impact The authors touch upon the potential for broader applications of their findings including online shopping platforms policymaking and security domains. However they could expand on the discussion around the limitations of their approach especially regarding the assumptions made about the bounded rationality models applicability across different scenarios and agent behaviors. Furthermore there is a need for a more structured discussion on the ethical implications and potential negative societal impacts of applying such learning algorithms particularly in sensitive areas like security and economics. Suggestions for improvement include:  Expanding the discussion on the limitations of the QR model in representing bounded rationality and how this might affect the generalizability of the findings.  Providing guidelines or considerations for responsibly applying the insights from this study in realworld scenarios considering the potential for misuse or unintended consequences.  Encouraging interdisciplinary collaboration to explore the ethical economic and societal implications of leveraging bounded rationality in decisionmaking processes. Overall the paper presents a significant step forward in the study of inverse game theory by incorporating more realistic models of agent behavior. However addressing the mentioned weaknesses could further strengthen its contributions and applicability.", "o-mxIWAY1T8": "Review of \"Semantic Probabilistic Layers for StructuredOutput Prediction in Neural Networks\" 1. Summary: The paper introduces the Semantic Probabilistic Layer (SPL) a novel predictive layer designed for structuredoutput prediction (SOP) tasks capable of integrating into any neural network architecture. The SPL ensures that predictions adhere to prespecified symbolic constraints enhancing model consistency without sacrificing modularity or training efficiency. This advancement is supported by leveraging probabilistic circuits to model intricate label correlations and enforce logical constraints. The authors demonstrate SPLs superiority over stateoftheart neurosymbolic approaches across several SOP benchmarks including hierarchical multilabel classification pathfinding and preference learning achieving higher accuracy and perfect constraint satisfaction. A notable contribution is the ability of SPL to satisfy six delineated desiderata crucial for flexible and reliable prediction in realworld SOP tasks which previous methods have struggled to achieve simultaneously. 2. Strengths and Weaknesses:  Strengths: a. Innovativeness: The SPL approach is novel applying probabilistic circuits in a way that addresses longstanding challenges in SOP namely consistent adherence to logical constraints and endtoend learnability. b. Broad Applicability: SPL is designed as a \"dropin\" layer for existing neural networks which significantly enhances its applicability across various tasks and domains without necessitating architecturespecific adjustments. c. Empirical Validation: The paper presents comprehensive experiments demonstrating SPLs effectiveness over alternative methods across diverse SOP tasks providing a strong empirical foundation for its claims.  Weaknesses: a. Computational Efficiency Unclear: While the authors assert SPLs efficiency and mention its linear computational time relative to the predictor and the constraint representation size detailed analysis or comparison in terms of actual runtime or computational resources was not provided. b. Limited Exploration of Limitations: The paper could benefit from a more thorough discussion on the limitations of SPL particularly in scenarios with extremely large or complex constraint spaces or where constraints evolve dynamically. c. Potential for Overfitting: SPLs ability to tightly integrate complex logical constraints might increase the risk of overfitting to training data especially in tasks with noisy labels or inaccurately defined constraints. This aspect is not adequately explored. 3. Questions and Suggestions:  Questions: a. How does SPL perform in scenarios where symbolic constraints are only partially known or need to be inferred from data b. Can SPL be extended to work with continuous outputs or more complex output structures beyond binary or categorical labels  Suggestions: a. It would be beneficial to include a more detailed analysis of SPLs computational efficiency comparing it directly with baseline approaches in terms of runtime and resource consumption. b. Exploring regularization techniques or constraint relaxation methods could enhance SPLs robustness to overfitting and its applicability in less structured problem settings. c. Investigating SPLs integration with dynamic or learningbased constraint formulation methods could extend its usefulness in evolving task environments. 4. Limitations and Societal Impact: The authors have not thoroughly addressed SPLs limitations particularly regarding computational efficiency and potential challenges in highly complex or dynamic constraint settings. While the societal impact of ensuring model predictions adhere to logical constraints is broadly positive especially in safetycritical applications there is a missed opportunity to discuss ethical considerations or negative impacts such as the potential misuse in enforcing biased constraints. Constructive suggestions include exploring adaptive constraint learning and explicitly discussing ethical use cases and guidelines for SPLs application in sensitive domains.", "zTQdHSQUQWc": " Peer Review on \"A Frequency Improved Legendre Memory Model for LongTerm Time Series Forecasting\"  1) Summary of the Paper The authors propose a novel deep learning architecture the Frequency improved Legendre Memory model (FiLM) designed for longterm forecasting of time series data. Addressing the limitation of existing deep learning models in preserving historical information while minimizing overfitting to noise FiLM employs Legendre Polynomials for historical data approximation integrates Fourier projection to filter noise and utilizes lowrank approximation to enhance computational efficiency. The paper demonstrates through extensive experiments that FiLM significantly outperforms stateoftheart models in both multivariate and univariate forecasting tasks across various benchmark datasets evidencing substantial improvements in accuracy and computational efficiency.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The integration of Legendre Polynomials Fourier projection and lowrank approximation for feature extraction and noise reduction is both novel and theoretically wellmotivated addressing key challenges in longterm time series forecasting.  Empirical Validation: The comprehensive evaluation across multiple domains and benchmark datasets along with a comparison against stateoftheart models strongly supports the effectiveness of the FiLM model.  Generalizability: The proposed model and its components particularly the redesign of the Legendre Projection Unit (LPU) and Frequency Enhanced Layers (FEL) are designed to be adaptable to other deep learning modules enhancing the models applicability to a wide range of forecasting tasks.  Code Availability: Providing the source code is beneficial for reproducibility and facilitates further research and development by the community. Weaknesses:  Model Complexity and Interpretability: While the paper focuses on performance and computational efficiency there is less discussion around the complexity and interpretability of the model which are crucial for practical deployments and further developments.  Detailed Analysis on Noise Reduction: The mechanism of noise reduction through Fourier projection and lowrank approximation is wellfounded yet the paper could benefit from a deeper analysis or visualization of how noise is specifically identified and mitigated in various scenarios.  Lack of Discussion on Failure Cases: Insight into scenarios where FiLM underperforms or is less effective compared to other models would provide a more balanced view and inform future improvements.  3) Questions and Suggestions for the Authors  Could the authors expand on the impact of different types of noise on the models forecasting accuracy For instance how does FiLM handle highfrequency noise compared to outliers or abrupt changes in the time series  A deeper analysis comparing the models performance on datasets with distinct characteristics (e.g. different noise levels varying degrees of nonlinearity and periodicity) could highlight the models adaptability and limitations more clearly.  It would be beneficial to include case studies or indepth analyses of specific realworld applications where FiLM has been deployed discussing practical challenges and adjustments made to optimize performance.  To enhance the models utility further consider exploring and discussing strategies for model interpretability allowing users to understand the decisionmaking process especially in critical applications like financial forecasting or medical diagnosis.  4) Limitations and Societal Impact The authors have not thoroughly discussed the limitations and potential negative societal impacts of their work. Addressing model complexity and ensuring transparency and interpretability are crucial for preventing misuse or misinterpretation of model predictions in sensitive applications. A constructive suggestion would be to explore methods for simplifying the model without significantly compromising performance and to develop guidelines for responsible use especially considering the models applicability to various domains with different risk profiles. Adding a section discussing ethical considerations and potential biases in time series data along with strategies to mitigate these issues would contribute to the responsible advancement of forecasting technology.", "oOte_397Q4P": " Peer Review for \"Sparse Structure Search for Delta Tuning (S3Delta) in PreTrained Models\" 1) Summary of the Paper This paper introduces Sparse Structure Search for Delta Tuning (S3Delta) an approach aimed at optimizing the finetuning process of large pretrained models (PTMs) to reduce computational and storage burdens. Typically adaptation of PTMs requires finetuning all parameters which is resourceintensive. Delta Tuning (DT) methods which finetune only a small fraction of parameters have emerged as a solution. S3Delta advances this concept by automating the search for the most effective sparse structure of DT modules within PTMs. It leverages differentiable structure search with bilevel optimization and introduces a shifted global sigmoid method to control parameter sparsity explicitly. The approach is validated through extensive experiments showcasing its ability to maintain finetuning performance with drastically fewer trainable parameters compared to both manually designed structures and traditional fullmodel finetuning. 2) Assessment of Strengths and Weaknesses Strengths:  Innovation in Structure Search: S3Delta innovatively automates the search for DT module placement within PTMs addressing the limitations of manual designation that often result in suboptimal performance.  Significant Parameter Efficiency: The approach achieves impressive parameter efficiency demonstrating finetuning performance close to full finetuning with only a tiny fraction (as low as 0.01) of trainable parameters.  Transferability and Explainability: S3Deltas searched structures display high transferability across tasks and offer insights for future DT method design contributing valuable understandings toward parameterefficient tuning strategies.  Open Source Contribution: By making the code publicly available the authors encourage reproducibility and further experimentation in the research community. Weaknesses:  Complexity in Implementation: The methods reliance on differentiable structure search and bilevel optimization might pose challenges in terms of implementation complexity and computational overhead during the search phase.  Limited Evaluation on Model Varieties: The experiments predominantly focus on the T5large model. Evaluations including a broader range of PTMs could present a more comprehensive understanding of S3Deltas applicability.  Potential Overfitting Risk in Search Process: The search phase might be prone to overfitting especially given the high dimensionality of the search space. Although split validation aims to mitigate this additional regularization strategies could be beneficial. 3) Questions and Suggestions for the Authors  Evaluation on Diverse PTMs: How does S3Delta perform across different architectures beyond T5 Exploring this could underline the methods adaptability.  Comparison with Similar NAS Approaches: How does S3Delta compare with other Neural Architecture Search (NAS) strategies specifically designed for PTMs A direct comparison might highlight its unique benefits.  Insights on Failure Cases: Are there specific scenarios or tasks where S3Delta underperforms Analyzing such cases could guide future improvements.  Further Optimization of Search Efficiency: Could incorporating more advanced NAS techniques like early stopping or surrogate model employment reduce the search phases computational demands 4) Limitations and Societal Impact The authors have addressed the limitations related to the design complexity and the potential for overfitting in the search process. However an explicit discussion on the computational costs of the search phase and strategies to mitigate these would strengthen the paper. Additionally while the societal impact of efficient model tuning is generally positive\u2014reducing the environmental footprint of largescale AI models\u2014the paper could benefit from a discussion on ethical considerations in deploying optimized PTMs particularly in sensitive applications. Suggestions for Improvement:  Incorporate discussions on the broader applicability of S3Delta to various PTM architectures and the potential computational efficiencies in the search process.  Explore the ethical dimensions of deploying highly efficient yet potentially opaque machine learning models in critical domains.", "pgBpQYss2ba": "This paper tackles a central issue in online learning and decision making particularly in environments characterized by either dynamic or adversarial elements. Through a detailed exploration the authors extend the concept of the DecisionEstimation Coefficient (DEC) originally introduced in a stochastic context to adversarial settings. They succeed in establishing that a variant of the DEC applied to the convex hull of the model class under consideration is both necessary and sufficient for achieving low regret in adversarial decision making. This insight not only bridges existing gaps in the understanding of adversarial and stochastic settings but also provides a unified framework to approach a variety of online learning problems from bandits to more complex reinforcement learning scenarios with adversarial dynamics. Strengths: 1. Novel Theoretical Contributions: The paper introduces new upper and lower bounds for adversarial decision making directly linking them to the convexified DecisionEstimation Coefficient. This provides a significant theoretical advancement in understanding adversarial learning environments. 2. Bridging Stochastic and Adversarial Settings: By showing the DECs relevance in both stochastic and adversarial frameworks the authors offer a valuable perspective that unifies these often separately treated settings enhancing the understanding of their fundamental principles. 3. Methodological Rigor: The methodology employed is robust involving a comprehensive exploration and crossevaluation of different complexity measures such as the Information Ratio and ExplorationbyOptimization objectives establishing their interconnections in the process. Weaknesses: 1. Practical Applicability: Despite the theoretical advancements the papers direct implications for practical applications remain somewhat unclear. The algorithms presented particularly their computational efficiency in large decision space contexts are not thoroughly addressed which may limit practical implementations. 2. Accessibility and Complexity: The density of technical details and the level of abstraction might make the paper challenging for a broader audience. Simplifying the presentation or providing more intuitive explanations could enhance accessibility. Questions and Suggestions for the Authors: 1. Practical Implications: Could the authors elaborate on the practical applications of their theoretical findings particularly in realworld settings where adversarial dynamics are common Is there a pathway to simplifying the algorithms for more efficient practical implementation 2. Computational Efficiency: The paper mentions that improving computational efficiency for large decision spaces remains an open question. Could the authors speculate on potential methods or approaches that might address this challenge 3. Accessibility Enhancements: Are there possibilities to include more intuitive examples or graphical illustrations to help explain the core concepts and results making the paper more accessible to readers not deeply familiar with the technical background Limitations: The limitations regarding computational efficiency and the practical applicability of the proposed theoretical framework are acknowledged but not deeply explored. Additionally the paper while extensive in its theoretical exploration could better address the potential negative societal impacts especially in scenarios where adversarial dynamics are exploited maliciously. Constructive suggestions include developing a discussion section on these societal implications proposing guidelines to mitigate negative impacts and exploring the feasibility of incorporating ethical considerations into the adversarial learning algorithms design.", "lCGDKJGHoUv": "Peer Review of the Paper Titled \"Exploring the Stability and Generalization of Stochastic Gradient Descent\" 1) Summary of the Paper and Its Contributions: This paper embarks on an indepth examination of Stochastic Gradient Descent (SGD) in the context of stochastic convex optimization (SCO) particularly focusing on its stability and generalization capabilities. Traditionally SGD is analyzed under the lens of minimizing population risk but this work exposes instances where SGD despite minimizing population risk fails to minimize empirical risk hence exhibiting a constant generalization gap. This phenomenon is termed as benign underfitting challenging both conventional and interpolationbased views on generalization. The authors rigorously prove that withoutreplacement SGD (onepass) exhibits a lack of algorithmic stability and cannot be explained by uniform convergence or other known generalization bounds separating it from withreplacement SGD which does not exhibit this phenomenon. The paper extends its analysis to finitesum convex optimization and provides significant improvements in upper and lower bounds for the multiepoch regime of withoutreplacement SGD contributing to the understanding of SGDs behavior under various settings. 2) Assessment of Strengths and Weaknesses: Strengths:  Novel Insights into SGD: The paper breaks new ground by identifying and rigorously demonstrating the phenomenon of benign underfitting in SGD offering a profound shift in understanding SGDs generalization capabilities beyond conventional wisdom.  Theoretical Rigor: The authors present a solid mathematical framework accompanied by strong theoretical proofs ensuring that the results are not only significant but also reliably validated.  Comprehensive Analysis: By covering both onepass and multiepoch regimes with and without replacement SGD within finitesum convex optimization problems the paper provides a holistic understanding of SGDs dynamics across various scenarios.  Practical Implications: The findings have immediate implications for the design and analysis of optimization algorithms in machine learning paving the way for developing better techniques that acknowledge the limitations of SGD. Weaknesses:  Complexity and Accessibility: The technical depth while a strength also makes the paper challenging to digest for readers not deeply versed in optimization theory potentially limiting its accessibility to a broader audience.  Experimental Validation Lacking: While the theoretical contributions are significant the absence of empirical experiments to showcase the practical impact of benign underfitting on realworld datasets is a missed opportunity.  Limited Discussion on Broader Implications: The paper could benefit from a more extended discussion on the implications of these findings on existing learning algorithms and future directions for algorithm development under this new understanding of SGDs behavior. 3) Questions and Suggestions for the Authors:  Experimental Validation: Could the authors supplement the theoretical findings with empirical evidence demonstrating benign underfitting on standard datasets and comparing onepass vs. withreplacement SGD  Impact on Deep Learning: Given the widespread use of SGD in training deep neural networks how do the findings translate to nonconvex optimization scenarios typical in deep learning Could benign underfitting still play a role  Algorithmic Implications: In light of these results what are the potential strategies to mitigate the adverse effects of benign underfitting Could novel optimization algorithms or modifications to SGD circumvent these issues 4) Limitations and Societal Impact: The paper thoroughly discusses its theoretical limitations including the complexity and specific conditions under which their results hold. However a more detailed discussion on the practical limitations and the potential negative societal impact especially in applications of machine learning sensitive to optimization performance would enhance the work. For instance exploring whether benign underfitting could lead to suboptimal decisions in critical systems (e.g. healthcare or autonomous vehicles) where SGD is employed would be beneficial. Constructive suggestions could include advocating for more rigorous testing of machine learning models in sensitive applications or developing optimization algorithms that ensure both empirical risk minimization and stability. In conclusion this paper makes a substantial contribution to the field of optimization in machine learning particularly in the understanding of SGD. Its theoretical advancements set the stage for future work aiming to develop algorithms that account for the nuances of generalization beyond traditional bounds.", "p4xLHcTLRwh": "Title: Breaking LatticeBased Cryptography with SALSA: A TransformerBased Approach 1) Summary: The paper introduces SALSA (Secretrecovery Attacks on LWE via Seq2Seq models with Attention) a novel technique leveraging transformer models to attack latticebased cryptographic schemes grounded in the Learning With Errors (LWE) problem. By training transformers to perform modular arithmetic and then combining these models with cryptanalysis strategies the authors demonstrate the feasibility of attacking LWEbased cryptosystems especially those using smalltomid size instances with sparse binary secrets. The authors experiments show that transformers can indeed learn to predict the outcomes of modular arithmetic operations and consequently distinguish LWE instances from random instances leading to successful secret recovery in tested scenarios. 2) Assessment: Strengths:  The paper addresses a highly relevant topic in the context of postquantum cryptography and the ongoing search for quantumresistant algorithms.  The novel use of transformers for cryptanalysis particularly in the domain of modular arithmetic and LWEbased cryptosystems represents an innovative approach that opens new avenues for research.  The experimental setup and the detailed description of the methodology (data generation model training secret recovery algorithms) provide a clear understanding of how SALSA operates.  Providing the source code via GitHub enhances the papers reproducibility and allows for independent validation and further development by the community. Weaknesses:  The scalability of the approach to realworld LWE parameters (larger n and q values) and more complex secret distributions remains an open question. The paper primarily demonstrates success with smaller sparse binary secrets.  The sample efficiency and computational demands of training transformer models for this application might pose practical challenges especially when considering the constraints of realworld attack scenarios.  There is limited discussion on potential countermeasures or how cryptosystem designers might adapt to mitigate the risk posed by SALSA and similar MLbased attacks.  While the paper touches on the implications of applying machine learning to cryptanalysis a deeper examination of the ethical considerations and potential negative impacts of this research area would strengthen the discourse. 3) Questions and Suggestions for the Authors:  Could the authors elaborate on potential strategies to scale SALSA for attacking larger dimension LWE instances and elaborate on anticipated challenges  It would be beneficial if the authors provide more insight into the computational resources required for the experiments particularly as parameters scale to better assess the practical feasibility of such attacks.  Exploring potential defenses or adaptations of LWEbased schemes to resist SALSA or similar MLbased cryptanalysis techniques could significantly contribute to the discourse around secure cryptographic design. Can the authors propose preliminary thoughts on this  A discussion centered on ethical considerations including responsible disclosure and the potential misuse of ML in cryptanalysis would be valuable. How do the authors envision balancing the advancement of ML applications in cryptography with the need for security and privacy 4) Limitations:  The authors partially address the limitations of their approach notably its current applicability to smaller LWE instances and sparse binary secrets. However a more detailed discussion regarding the computational cost potential for scalability and implications for practical security would provide a clearer scope of the techniques limitations.  The potential negative societal impacts particularly in the context of undermining the security of emerging quantumresistant cryptographic standards are not thoroughly explored. Constructive suggestions for future research could include developing defensive strategies against MLbased cryptanalysis and fostering collaboration between the ML and cryptographic communities to anticipate and mitigate such risks.", "qHGCH75usg": "Review of \"BYOLExplore: A SelfSupervised Method for CuriosityDriven Exploration\" Summary: The paper introduces BYOLExplore an innovative approach to curiositydriven exploration in reinforcement learning (RL) particularly focusing on visually complex and partially observable environments. Instead of relying on multiple distinct objectives for learning world dynamics and exploration policies BYOLExplore optimizes a singular prediction loss in the latent space. Through selfsupervised learning it builds a world model and a curiositydriven policy concurrently. This work demonstrates the algorithms effectiveness on DMHARD8 a set of challenging 3D exploration tasks and on the ten hardest exploration games in Atari outperforming existing curiositydriven methods and achieving superhuman performance in Atari games all while maintaining a simpler design. Strengths: 1. Simplicity and Generality: One of the papers main strengths is the elegance and generality of the BYOLExplore approach. It simplifies the training process by applying a single prediction loss for both learning the world model and driving exploration. Such a unified approach is both innovative and efficient. 2. Superior Performance: The paper convincingly demonstrates that BYOLExplore significantly outperforms existing methods in challenging benchmarks such as DMHARD8 and the ten hardest Atari exploration games. Achieving superhuman performance in the latter with a less complex architecture is particularly noteworthy. 3. Comprehensive Evaluation: The authors thorough evaluate BYOLExplore across a wide range of environments including both 2D and 3D tasks as well as fully and partially observable settings. This extensive evaluation underscores the algorithms versatility and effectiveness. Weaknesses: 1. Handling Environmental Stochasticity: While the paper discusses BYOLExplores capability to filter out \"TVnoise\" and its nonoptimization for environments with stochastic elements like sticky actions this area seems underexplored. More insights into how BYOLExplore could be adapted or extended to better handle stochastic environments would be valuable including any limitations encountered. 2. Complexity and Scalability: Despite its simpler design compared to other stateoftheart methods the paper does not fully delve into the computational resources needed for BYOLExplore especially when dealing with highdimensional environments. Further discussion on its scalability and potential computational bottlenecks would enhance the paper. 3. Comparison with HumanLevel Exploration: While achieving superhuman performance is impressive the paper does not discuss how BYOLExplores exploration strategies compare qualitatively with human exploration tactics particularly in complex environments. Such an analysis could provide deeper insights into the algorithms behavior. Questions and Suggestions: 1. Can the authors provide more details on how BYOLExplore would perform in environments with higher levels of stochasticity and what modifications if any would be necessary for such settings 2. Given the performance improvements demonstrated what are the potential scalability challenges with BYOLExplore especially concerning memory requirements and computational time as the complexity of the environments increases 3. It would be interesting to see a deeper qualitative comparison of exploration strategies between BYOLExplore and human players particularly in the DMHARD8 tasks. Are there distinct exploration patterns that the algorithm learns which differ fundamentally from human strategies Limitations and Societal Impact: The authors touch upon the limitation related to environments with stochastic elements but do not extensively discuss the scalability issues or the algorithms environmental adaptability. Expanding on these limitations could provide a more balanced view of BYOLExplores applicability. Regarding potential negative societal impacts the paper does not delve into this aspect. While it might be less directly relevant for a method focused on exploration in artificial environments considering any indirect implications such as reinforcing biases present in training data or the environmental impact of increased computational demands would be prudent. Constructively the authors could explore strategies to optimize computational efficiency further and investigate the models performance in more diverse settings to ensure broader applicability and minimize unintended biases.", "xz-2eyIh7u": "Review for the Paper: \"Towards Robust MultiAgent Learning: The Impact of Adversarial Agents on Collaborative Bandit Problems\" 1) Brief Summary: This paper addresses the critical challenge of secure learning in multiagent systems vulnerable to adversarial attacks particularly in the context of stochastic bandit problems. By introducing a model with a central server and multiple collaborating agents where a fraction of these agents might act adversarially the study aims to understand the extent to which collaboration is beneficial despite potential sabotage. The authors propose a robust phased elimination algorithm and extend their approach to generalized linear bandit models and contextual bandits contributing significantly to filling the knowledge gap in robust multiagent sequential decisionmaking. Main results include proving tight nearoptimal regret bounds for both linear and nonlinear observation models under adversarial conditions underscoring the potential for beneficial collaboration even in the presence of adversaries. 2) Strengths and Weaknesses: Strengths:  Novelty and Technical Depth: The paper makes novel contributions by designing algorithms to counter adversarial actions in multiagent bandit problems extending these ideas to generalized linear and contextual bandit settings. The technical analyses including tight regret bounds and the fundamental limits of collaboration benefits in the face of adversaries are rigorous and innovative.  Practical Relevance: The study tackles a highly relevant problem in the domain of secure and distributed learning systems such as federated learning where adversarial presence is a realistic concern. The proposed methodologies are not only theoretically sound but also show practical viability through simulation results.  Comprehensive Approach: By covering multiple models (linear generalized linear and contextual) and providing a thorough analysis for each the paper offers a comprehensive framework for robust multiagent learning that can be applicable across various scenarios and domains. Weaknesses:  Extension to NonLinear Observations: While the extension of the proposed algorithm to generalized linear models is promising the authors mention that this results in an additional \\xe2\\x88\\x9ad factor in the regret bound which they label as an area for future work. This indicates a potential limitation in the current approach when dealing with highly nonlinear observation models.  RealWorld Validation: Despite providing simulation results the paper does not present realworld experiments. Incorporating realworld datasets and scenarios could significantly strengthen the papers claims about the practical effectiveness of the proposed algorithms.  Assumptions and Generalizability: The assumptions such as Gaussian noise and adversarial model might limit the generalizability of the results. Discussions on the robustness of the proposed algorithms under varied and possibly more complex adversarial strategies would be beneficial. 3) Questions and Suggestions for the Authors:  Extension to More Complex Models: Could the authors elaborate on potential approaches or modifications needed to extend their algorithms to handle highly nonlinear or even nonparametric observation models which are common in practical applications  Impact of Communication Efficiency: The paper briefly mentions the communication efficiency of the algorithms. It would be insightful to understand the tradeoffs between communication overhead and algorithmic robustness or regret bounds especially in scenarios with limited bandwidth or high communication costs.  Robustness Against Sophisticated Adversaries: The adversarial model assumes a fixed fraction of agents acting arbitrarily. How robust are the proposed algorithms against more sophisticated adversaries possibly employing strategy adaptation over time or targeting specific vulnerabilities in the learning process 4) Limitations and Societal Impact The paper thoroughly explores the limitations related to extending the analysis to more nonlinear observation models and hints at future work to address these areas. However discussions on limitations regarding the adversarial models assumptions would enhance the paper. On societal impact while the paper addresses an essential aspect of learning security it could further elaborate on ethical considerations especially concerning how these algorithms could inadvertently become robust against benign interventions aimed at correcting biased or unfair learning processes. Suggestions for improvements include a broader discussion on potential adversarial strategies adaptation mechanisms for the proposed algorithms and ethical guidelines for deploying such robust learning systems. Overall the paper presents significant advancements in understanding and enhancing the robustness of multiagent systems against adversarial actions. With further development and realworld validation the proposed methodologies could notably contribute to more secure distributed learning frameworks.", "qtZac7A3-F": "Review of \"Leveraging Discrete Adversarial Training for Robust Vision Models\" 1) Summary: This paper proposes Discrete Adversarial Training (DAT) a novel approach aimed at improving both the robustness and generalization capabilities of vision models simultaneously. Unlike traditional Adversarial Training (AT) which often compromises standard performance to enhance adversarial robustness DAT introduces a method leveraging the symbolic and discrete input space characteristic of Natural Language Processing (NLP) tasks to vision tasks. By converting image data into discrete textlike inputs using a Vector Quantized Generative Adversarial Network (VQGAN) DAT applies symbolic adversarial perturbations to these \"visual words\" thereby enhancing visual representation learning across various tasks without degrading clean accuracy. Crucially DAT achieves stateoftheart performance in robust image classification as demonstrated on ImageNetC and StylizedImageNet benchmarks. 2) Assessment: Strengths:  Innovativeness: The paper introduces a fresh perspective on adversarial training by incorporating discrete representation learning from NLP into vision tasks. This crossdomain application is both novel and creatively tackles the issue of standard performance degradation commonly associated with AT.  Performance Improvement: DAT significantly outperforms existing methods in robust image classification tasks setting new benchmarks on ImageNetC and StylizedImageNet. These results demonstrate the practical effectiveness and potential of the proposed method for enhancing robustness and generalization in vision models.  Versatility: The authors successfully apply DAT across multiple vision tasks (image classification object detection selfsupervised learning) architectures (CNNs ViTs) and robustness scenarios (lpbound attacks unrestricted semantic attacks) showcasing the methods broad applicability. Weaknesses:  Increased Training Cost: As with most forms of adversarial training DAT incurs a higher computational and time cost due to the additional steps needed for generating discrete adversarial examples. This might limit its applicability in resourceconstrained scenarios.  Assumption of Ideal Discretizer: The effectiveness of DAT hinges on the performance of the VQGAN model used for discretizing images. The paper assumes an ideal discretizer with minimal information loss which may not always be viable in practical scenarios.  Lack of Theoretical Explanation: While the empirical results are strong the paper lacks a deeper theoretical foundation explaining why DAT improves both robustness and generalization. A more rigorous exploration of the underlying principles could strengthen the papers contributions. 3) Questions and Suggestions for the Authors:  Training Cost Analysis: Could the authors provide a more detailed analysis comparing the training costs (in terms of time and computational resources) between DAT and traditional AT Suggestions on mitigating increased costs would also be valuable.  Impact of Codebook Size: How does the size of the visual words codebook (i.e. the granularity of discretization) affect the performance Exploring this could provide insights into the tradeoffs between discretization granularity and model performance.  Theoretical Insights: Any theoretical insights or hypotheses on why DAT is effective across different tasks and models would be appreciated. Specifically understanding the interaction between discrete input space and model generalization could be enlightening. 4) Limitations and Suggestions for Improvement: The authors should address the limitation regarding the increased training time and resource requirements inherent to DAT. Providing strategies for efficiency improvements or adaptations that reduce computational overhead without significantly compromising performance would be beneficial. Furthermore exploring the limitations imposed by the ideal discretizer assumption and proposing methods to alleviate potential issues arising from lessthanideal discretization processes could enhance the practical applicability of DAT. Finally deepening the theoretical foundation underlying DATs effectiveness would not only solidify the current contributions but also guide future research in adversarial training and discrete representation learning in vision tasks. Overall the paper is compelling and presents a significant advancement in adversarial training for vision models with promising results and broad applicability. Addressing the provided suggestions could further strengthen the impact and applicability of DAT in realworld scenarios.", "xbgtFOO9J5D": " Summary of the Paper The paper addresses the fairness and diversity considerations in ranking algorithms and the rank aggregation problem focusing on ensuring that candidates from various groups receive fair representation in final rankings. The authors introduce algorithms for finding the closest fair ranking to a given ranking under the Kendall tau and Ulam metrics emphasizing strong fairness or proportional fairness where each groups representation in the topk positions conforms to specified parameters. To tackle the rank aggregation problem from a fairness standpoint a novel metaalgorithm is presented that applies under any generalized mean objective with any fairness criteria yielding 3approximation algorithms for center and median problems under both metrics. The achievement of a (3\u03b5)approximation algorithm for the Ulam metric under strong fairness highlights a significant advancement.  Assessment of Strengths and Weaknesses Strengths: 1. Novelty and Impact: The paper introduces the first exact algorithm for closest fair ranking under the Kendall tau metric and a polynomialtime exact algorithm for the Ulam metric when groups are constant filling a gap in the literature on fairness in ranking and rank aggregation. 2. Broad Applicability: Providing a metaalgorithm for fair rank aggregation that is applicable across different metrics and fairness constraints extends the works significance to various applications from academic admissions to search engines. 3. Solid Theoretical Foundation: The mathematical rigor demonstrated through proofs and the formulation of algorithms strengthens the credibility of the proposed solutions. 4. Practical Value: The derivation of 3approximation algorithms for rank aggregation problems and achieving better than 3factor approximation for the Ulam metric underlines the practical applicability of the papers contributions. Weaknesses: 1. Complexity and Accessibility: The complexity of the algorithms and the density of mathematical proofs may make the paper less accessible to practitioners outside theoretical computer science. 2. Empirical Validation: The paper could benefit from an empirical evaluation of the proposed algorithms demonstrating their effectiveness and efficiency on realworld datasets. 3. Consideration of Overlapping Groups: While the paper tackles fairness for distinct groups it acknowledges but does not address the scenario where individuals can belong to multiple overlapping groups leaving room for further research.  Questions and Suggestions for Authors 1. Empirical Evaluation: Can the authors provide empirical evidence of their algorithms performance on realworld datasets It would be valuable to see comparisons with existing methods in terms of fairness outcomes and computational efficiency. 2. Extension to Overlapping Groups: Have the authors considered extending their algorithms to handle overlapping group memberships If so could they share preliminary insights or potential challenges in adapting their current approach 3. Scalability Concerns: How do the proposed algorithms scale with the increase in the number of candidates and groups Providing complexity analysis or scalability insights would be beneficial. 4. Impact of Fairness Constraints on Outcome Quality: How do the fairness constraints impact the quality of the ranking outcomes compared to unfair counterparts Understanding the tradeoffs involved could offer a more nuanced view of the algorithms practical implications.  Limitations and Suggestions for Improvement The paper successfully identifies and addresses a gap in fair ranking and rank aggregation research. However it could further enhance its impact by addressing its limitations:  Empirical Validation: Incorporating empirical evaluations with realworld data would significantly strengthen the papers contributions by demonstrating practical viability and comparative advantages over existing methods.  Handling Overlapping Groups: Exploring algorithms that accommodate elements belonging to multiple groups would address a practical limitation in applying fairness constraints in complex reallife scenarios.  User Accessibility: Providing software implementations of the proposed algorithms could increase accessibility to a broader audience including practitioners interested in applying these fairnessaware approaches in diverse settings. By addressing these aspects the paper can expand its contributions and applicability in promoting fairness in ranking and rank aggregation problems across varied domains.", "qq84D17BPu": "Peer Review Report 1. Brief Summary: This paper presents a novel approach to understanding and modeling the learning dynamics of deep neural networks (DNNs) through differential equations. By addressing the discretization error between gradient flow (GF) and gradient descent (GD) the authors develop an \"Equation of Motion\" (EoM) that more accurately represents the continuous dynamic process of GD in DNNs. They introduce a counter term to the gradient in GF effectively bridging the gap between the continuous and discrete descriptions. Their contributions include a precise derivation of the discretization error and its leading order the application of the EoM to scaleinvariant and translationinvariant layers and extensive empirical validation of their theoretical findings. 2. Assessment of Strengths and Weaknesses: Strengths:  Originality: The idea of addressing the gap between continuous and discrete optimization processes in DNNs by introducing a counter term is novel and insightful. This significantly advances our understanding of the learning dynamics in neural networks.  Theoretical Contribution: The paper provides a rigorous mathematical framework to describe the learning dynamics of GD accurately. The introduction of the EoM and the detailed analysis of discretization error represent substantial theoretical contributions.  Practical Relevance: By applying the EoM to specific cases such as scaleinvariant and translationinvariant layers the authors demonstrate the practical relevance of their theoretical findings. This increases the potential impact of their work on future research in neural network optimization.  Empirical Validation: The comprehensive experimental results strongly support the theoretical analysis. This validation strengthens the papers claims and its contributions to the deep learning community. Weaknesses:  Scope of Application: The paper focuses solely on the dynamics of GD and GF excluding stochasticity acceleration methods and adaptive optimizers. While the authors mention that their framework could potentially be extended to these aspects the current scope might limit the immediate applicability of their findings.  Complexity for Practical Implementation: The derivation and implementation of the counter term especially when considering higherorder terms appear to be complex. This might pose challenges for practitioners or researchers who wish to apply the EoM in various contexts. 3. Questions and Suggestions for Authors:  Could the authors provide more insight into how their EoM might be extended to include stochastic gradient descent (SGD) and other optimization techniques Any preliminary analysis or thoughts on this would be valuable.  The practical implementation of the counter term especially for higherorder discretization errors is likely to be complex. Could the authors discuss potential strategies to simplify the implementation or provide software tools to facilitate this  The paper extensively validates the theoretical findings with empirical results which is commendable. However could the authors explore the sensitivity of their model to different network architectures and datasets This would further establish the robustness and generalizability of their approach.  Given the potential complexity and computational requirements of applying the EoM with higherorder counter terms are there any practical guidelines the authors could provide on balancing accuracy and computational efficiency 4. Limitations and Societal Impact: The authors acknowledge the limitation of focusing solely on GD and GF without covering other optimization techniques. Extending the work to include SGD acceleration methods and adaptive optimizers is suggested as future work. Addressing this limitation could significantly broaden the applicability and impact of their research. Concerning societal impact while the paper focuses on theoretical and technical aspects it falls short of directly addressing potential negative societal impacts. Given the deep integration of neural networks in various applications it would be beneficial for the authors to consider and discuss any ethical implications or unjust biases their approach might inadvertently perpetuate or amplify especially in the context of automated decisionmaking systems. Suggestions for Improvement:  Explore the extension of the EoM to include SGD and other optimization techniques and provide preliminary thoughts or frameworks for these extensions.  Provide guidelines or tools for practitioners to simplify the implementation of the counter term especially for higherorder terms to increase the accessibility and applicability of the EoM.  Investigate the models sensitivity to different network architectures and datasets to verify its robustness and generalizability.  Consider discussing the ethical implications and potential for bias in models optimized using the EoM especially in applications with significant societal impact.", "xqyDqMojMfC": "Review of \"Stochastic Optimization for Constrained Nonconvex Problems with Markovian Data\" 1) Summary: This paper tackles the problem of stochastic approximation for constrained nonconvex optimization when dealing with Markovian data particularly focusing on instances where the transition kernel of the Markov chain is dependent on the state. Such issues are prevalent in machine learning applications like strategic classification and reinforcement learning. The authors provide an indepth analysis of both projectionbased and projectionfree algorithms tailored for these problems. Specifically they establish nonasymptotic bounds on the oracle complexity for achieving \u03b5stationary solutions demonstrating that the required number of calls to the stochastic firstorder oracle is of the order O(1\u03b5) for both algorithm types. For the projectionfree algorithm they further show that the number of calls to the linear minimization oracle is also of the order O(1\u03b5). The paper substantiates its theoretical findings with empirical results on the application of its proposed method on a strategic classification problem using neural networks. 2) Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper fills a critical gap in understanding the oracle complexity of stochastic approximation algorithms for Markovian data in constrained nonconvex optimization a topic not sufficiently covered in existing literature.  Practical Relevance: By providing a projectionfree algorithm and analyzing its performance the authors address a significant computational challenge in the application of these methods in practice.  Empirical Validation: The inclusion of empirical evaluation particularly the strategic classification problem with neural networks offers valuable evidence of the practical viability and potential applicability of the algorithms. Weaknesses:  Limited Scope of Experiments: The empirical evaluation mainly focuses on one application (strategic classification with neural networks) potentially limiting insights into the algorithms performance across a broader range of applications and data characteristics.  Assumption Clarity: While the paper relies on several assumptions to establish its results such as drift conditions and Lipschitz continuity the implications and necessity of these assumptions are not thoroughly discussed.  Broader Impact Discussion: The discussion on the potential societal impacts both positive and negative of applying these optimization techniques in machine learning applications especially in sensitive areas like strategic classification and reinforcement learning is lacking. 3) Questions and Suggestions for the Authors:  Broader Applicability: Can the authors provide insights into how their algorithms might perform under different conditions such as nonMarkovian data or other forms of dependency structures  Assumption Justification: More elaborate justification or discussion could be helpful to understand the necessity and impact of major assumptions used in this study particularly those related to the Markov chain and objective function properties.  Experimental Diversity: Considering the paper\u2019s contributions toward constrained nonconvex optimization with Markovian data would it be possible to include more diverse empirical evaluations possibly exploring other machine learning problems or different data dependency structures to better illustrate the algorithms performance and applicability scope  Societal Impact Analysis: Would the authors be willing to expand on the potential societal implications of their proposed algorithms especially considering the critical areas of application such as reinforcement learning and strategic classification 4) Limitations and Societal Impact: While the paper successfully addresses foundational theoretical perspectives and presents empirical evidence supporting its algorithms it falls short in providing a broad examination of the algorithms performance across diverse scenarios and conditions. Moreover the paper does not adequately discuss the assumptions implications on the algorithms applicability and robustness. Societal impacts particularly the ethical considerations in applications like strategic classification where classification decisions could have significant implications are scarcely addressed. Constructive suggestions include an expanded experimental section covering more varied conditions and a dedicated discussion on the ethical implications and societal impacts of deploying such optimization algorithms in realworld applications.", "wKf5dRSartn": "1) Brief Summary: The paper explores the realm of machine teaching focusing specifically on reducing the teaching complexity involved in conveying concepts to learners. It introduces a novel batch teaching model named STDmin which aims to minimize teaching complexity while also incorporating a notion of \"importity\" of examples. This model defies the standard GMcollusionfreeness by fulfilling three newly proposed desirable properties for teaching models. It establishes an upper boundary of the teaching complexity parameter STDmin by both the recursive teaching dimension (RTD) and the VapnikChervonenkis dimension (VCD) marking a significant advancement in the field by providing a complexity parameter for batch teaching that is linearly upperbounded by VCD. 2) Strengths and Weaknesses: Strengths:  Innovative Approach: The paper breaks new ground by proposing a batch teaching model STDmin which is not bound by the conventional GMcollusionfreeness yet meets essential postulates for reasonable teaching complexity measures.  Solid Theoretical Foundation: It delivers a thorough examination and insightful analysis aligning teaching complexity parameters with wellunderstood computational learning theory aspects notably RTD and VCD.  Practical Implications: The examination of STDmin against established parameters like VCD provides significant insights potentially influencing the design and analysis of future teaching and learning algorithms. Weaknesses:  Limited Application Discussion: While the paper touches on potential applications of machine teaching it could benefit from a deeper exploration of practical scenarios where the STDmin model could be particularly advantageous or its specific advantages over existing models.  Complexity and Accessibility: The high level of technical detail and complexity might make the paper less accessible to readers not deeply familiar with the theoretical underpinnings of machine teaching and computational learning theory. 3) Questions and Suggestions:  Empirical Evaluation: How does STDmin perform in practical teaching settings or simulations compared to other models It would be beneficial to see empirical evidence supporting the theoretical advantages of the proposed model.  Impact of Postulates: How critical are the three desirable properties introduced for teaching models in practical applications of machine teaching An exploration or discussion on this could provide clarity on their practical implications.  Extensions to Other Learning Paradigms: Could the concepts introduced in this paper be adapted or extended to other learning paradigms such as reinforcement learning or unsupervised learning scenarios 4) Limitations and Societal Impact: The paper provides a theoretical foundation without delving into potential negative societal impacts. While the focus on enhancing machine teaching is laudable future iterations should consider:  Bias and Fairness: How might biases in selected teaching examples influenced by the STDmin model propagate or amplify biases in machine learning models  Accessibility and Inclusivity: Consider discussing how the proposed model affects the accessibility of machine teaching tools for educators and learners with diverse backgrounds and capabilities. Improvements could include a section discussing these limitations and potential mitigations ensuring the broader implications of the work are considered.", "qqIrESv4f_L": "Review of \"INSPNet: Implicit Neural Signal Processing Network for INRs\" 1. Summary: The paper introduces INSPNet a novel approach to directly modify Implicit Neural Representations (INRs) without the necessity for explicit decoding. INRs encode continuous multimedia data using MLPs and have applications across various computer vision tasks. However directly editing INRs has been challenging due to their latent parameter representation. To address this the paper presents a method that operates on INRs via highorder differential operators exploiting the analytically computable spatial gradients and the translation invariance of neural networks. A key insight is that any continuous convolution filter can be approximated by a linear combination of these differential operators. The paper extends this framework to INSPConvNet a convolutional neural network that operates directly on INRs demonstrating its applicability in both lowlevel image processing tasks and highlevel tasks like image classification. 2. Assessment: Strengths:  The paper tackles a significant and nontrivial problem in the realm of implicit neural representations offering an innovative solution for directly modifying INRs without decoding.  The theoretical foundation is sound with rigorous proofs showing that any continuous convolution filter can be approximated by a linear combination of highorder differential operators.  Experimental results across various tasks including edge detection image denoising and image classification demonstrate the effectiveness and versatility of both INSPNet and INSPConvNet.  The work has a clear potential impact on fields that utilize implicit representations providing a platform for further research and applications that require direct manipulation of INRs. Weaknesses:  While the paper provides theoretical and experimental validation of the approach there is limited discussion on the computational efficiency and scalability of employing highorder derivatives especially for INSPConvNet.  The paper could benefit from a broader comparison with other methods especially those that attempt to directly manipulate INRs to clearly establish the advantages of INSPNet.  The limitations section briefly mentions the challenges with highorder derivatives computation but does not offer potential solutions or directions for mitigating these issues. 3. Questions and Suggestions for the Authors:  Could you elaborate on the computational overhead introduced by calculating highorder derivatives and any potential strategies to improve efficiency  A deeper discussion on the scalability of INSPConvNet especially when applying it to larger datasets or more complex tasks would be valuable. Can the current framework handle such scalability or are there inherent limitations  Have the authors considered any hybrid approaches that might combine the direct manipulation capabilities of INSPNet with traditional decodingandprocessing methods to balance between the expressiveness and computational efficiency 4. Limitations: The authors acknowledge the limitations related to the theoretical guarantees which only cover the expressiveness of convolution through infinite sequence approximation and practical challenges regarding memory efficiency and numerical stability when computing highorder derivatives. These limitations are critical especially for practical applications requiring scalability and efficiency. One suggestion is to explore approximation techniques that can reduce the computational burden of highorder derivatives or to investigate alternative formulations that could offer a tradeoff between expressiveness and computational demand. Additionally further exploration of the potential negative societal impacts particularly in contexts where these methods could be used to manipulate multimedia content in misleading ways should be considered and discussed.", "vfCd1Vt8BGq": " Review of \"Deriving Information Theoretic Generalization Bounds for Deep Learning via LeaveOneOut Conditional Mutual Information\"  1) Summary: This paper introduces an innovative approach to deriving generalization bounds for supervised learning algorithms particularly deep learning models through a new measure named leaveoneout conditional mutual information (looCMI). Unlike existing Conditional Mutual Information (CMI) bounds that often present computational and interpretability challenges in practical deep learning scenarios looCMI bounds offer an easier and more computationally efficient evaluation method. The technique asks a counterfactual question related to removing a single sample from the training set to infer the degree of memorization present in the model thus providing insights into its generalization capabilities. The paper showcases the applicability and nonvacuity of these bounds through empirical validation on image classification tasks establishing a strong connection between the proposed bounds and several known generalization theories and empirical measures.  2) Assessment: Strengths:  Innovativeness: The concept of using leaveoneout conditional mutual information as a metric to derive generalization bounds is novel and presents a shift from traditional methods. It addresses significant limitations seen in earlier approaches that relied on bounds often too loose or computationally intensive to compute for practical deep learning scenarios.  Empirical Validation: The authors substantiate their theoretical claims through rigorous empirical experiments on deep neural networks finetuned for standard image classification tasks. The fact that these bounds are nonvacuous in such scenarios adds immense value to the research providing a practical tool for assessing deep learning models.  Connection to Other Theories: The paper does an excellent job of relating looCMI bounds to various known entities in machine learning such as optimization algorithm stability and loss landscape geometry. This not only validates the proposed method against established theories but also improves our understanding of why certain models generalize better. Weaknesses:  Generality and Applicability: While the paper makes significant strides in applying looCMI bounds to image classification tasks using deep neural networks it remains to be seen how applicable and effective this method is across a broader spectrum of tasks and models. The paper could benefit from a discussion on the potential limitations or challenges in extending this approach to other domains.  Computational Efficiency Discussion: Although it is mentioned that looCMI bounds are more computationally efficient than previous CMI bounds a detailed discussion or comparative analysis on the computational resources required (e.g. time memory) would offer more clarity on the practical feasibility of implementing this method especially for very large datasets or models.  3) Questions and Suggestions:  Extension to Other Domains: Can the authors provide insights or preliminary results on how looCMI might perform in domains other than image classification like natural language processing or reinforcement learning Further exploration in this area could significantly enhance the papers impact.  Comparison with Other Methods: A more detailed comparison with existing generalization bounds including practical aspects like computational demands and scalability would be beneficial. How do looCMI bounds compare to other recent advancements in terms of tightness and computational efficiency  Sensitivity Analysis: A sensitivity analysis regarding the choice of hyperparameters (e.g. the noise variance in synthetic randomization) and its effect on the bounds tightness and the models generalization error could offer valuable insights for practitioners looking to apply this framework.  4) Limitations and Societal Impact: The authors adequately address the technical limitations and computational aspects of their work. However a discussion on the broader implications including any potential negative societal impacts is conspicuously absent. For instance while improving generalization is generally seen as beneficial optimizing for generalization without considering fairness or privacy implications might inadvertently magnify biases present in the data or lead to overgeneralization at the expense of minority groups. Suggestions for Improvement: 1. Expanding Applicability: Future work could focus on extending the applicability of looCMI bounds across diverse learning scenarios beyond supervised image classification to understand its limitations and strengths in various contexts. 2. Addressing Societal Impact: A section discussing the potential societal impacts of optimizing solely for generalization based on the proposed metric would be valuable. Specifically the authors should consider how their work aligns with broader ethical considerations in AI such as fairness transparency and privacy. In summary this paper presents a promising direction for deriving practical generalization bounds in machine learning particularly for deep learning models. Expanding its scope and addressing the broader implications of its findings could further solidify its contributions to the field.", "uvE-fQHA4t_": "Review for \"Exploring Generalization in Deep Learning via GradientNorm Based PACBayesian Bounds\" 1) Summary: This paper introduces a novel approach to understanding the generalization abilities of deep learning models by focusing on the gradient norm of the loss function as opposed to traditional methods that rely on uniform boundedness or Lipschitz continuity assumptions. The authors propose a new generalization bound within the PACBayesian framework by leveraging logSobolev inequalities (LSI) which inherently takes into account the models complexity via the norm of the gradient of the loss function. This approach leads to a bound for multiclass classification with linear models and deep neural networks under onaverage bounded loss and gradient norm assumptions. The paper demonstrates through theoretical analysis and empirical evaluation that these new bounds can provide tighter generalization performance compared to existing methods thereby closing the gap between theoretical understanding and practical reality of deep learning. 2) Assessment: Strengths:  The paper addresses a significant gap in the PACBayesian theory by proposing a generalization bound that does not rely on stringent assumptions of boundedness or Lipschitz continuity which are often unrealistic for stateoftheart deep learning models.  The use of logSobolev inequalities is welljustified and represents a meaningful advance in the methodology for deriving generalization bounds incorporating model complexity in a more natural and direct way than previous approaches.  The empirical evaluations are thorough and demonstrate the practical relevance of the proposed bounds. By showing tighter bounds across various architectures when compared to baseline methods the paper provides strong evidence for the utility of their approach in understanding realworld neural networks.  The paper is wellwritten and effectively communicates the technical contributions and the potential impact of this work. Weaknesses:  While the theoretical contributions are significant the practical implications of these bounds remain somewhat unclear. Tighter bounds are demonstrated empirically but the paper does not sufficiently discuss how these findings could influence the design or optimization of deep learning models.  The assumptions required for the application of the proposed bounds (onaverage bounded loss and gradient norm) may still not encompass all types of deep learning architectures or loss functions used in practice. There is a need for further discussion on the limitations of these assumptions.  The analysis primarily focuses on classification tasks. It would be beneficial to understand how the proposed method applies to other types of learning tasks such as regression or unsupervised learning. 3) Questions and Suggestions:  How might the insights gained from the proposed generalization bounds inform changes to network architecture training procedures or regularization methods to improve generalization in practice  Can the authors comment on the scalability of their approach to very large networks and datasets Any potential computational concerns or limitations should be addressed.  It would be valuable to include a discussion on how the proposed bounds might interact with other known factors affecting generalization such as dataset complexity or training duration.  Consider extending the analysis to regression or other tasks beyond classification to demonstrate the versatility of the approach. 4) Limitations and Societal Impact:  The authors have not thoroughly discussed the limitations related to the applicability of their bounds across different network architectures and task domains. Expanding on these limitations would provide a clearer picture of the bounds\u2019 utility.  The paper lacks a discussion on the potential negative societal impacts. While theoretical in nature its essential to consider how improved understanding of generalization could influence the development of AI systems with societal implications.  As a suggestion for improvement the authors could explore the implications of their findings on ethical considerations in AI such as bias and fairness and whether tighter generalization bounds could contribute to more trustworthy AI systems. Overall this paper makes significant theoretical contributions to our understanding of generalization in deep learning and presents promising empirical results that substantiate its claims. Future work should aim to bridge the gap between these theoretical advances and their practical applications in designing and training more generalizeable deep learning models.", "yfrDD_rmD5": " Peer Review  1) Summary of the Paper This paper introduces a novel technique to identify influential training examples in largescale natural language processing (NLP) models addressing the computational challenge and inefficacy of existing data influence methods which primarily focus on the models last layer of weights. The authors argue that the \"cancellation effect\" observed in lastlayerfocused influence computation methods diminishes the discriminative power of influence scores proposing an alternative approach that computes influence based on the word embedding layer dubbed TracInWE. This technique leverages gradients rather than embeddings to capture both lowlevel and highlevel semantic information allowing for the identification of influential examples at both the overall training input and word levels. The methodology demonstrates significant performance improvements over existing methods across different language classification tasks while also introducing a wordlevel decomposition of influence for enhanced interpretability.  2) Assessment of the Paper Strengths:  Novel Approach: The paper convincingly argues the limitations of lastlayerfocused influence computation and introduces an innovative solution by targeting the word embedding layer effectively circumventing the \"cancellation effect.\"  Comprehensive Evaluation: Through extensive experiments on various language classification tasks and models the authors demonstrate the superior efficacy of TracInWE over traditional influence methods bolstering the paper\u2019s claims with empirical evidence.  Interpretability: By affording wordlevel decomposition of influence the proposed method enhances the interpretability of model predictions a significant advancement for the field of explainable AI. Weaknesses:  Computational Feasibility Concerns: While TracInWE is shown to be more computationally efficient than other methods applied at the embedding layer the paper could benefit from a more detailed discussion on its scalability particularly for extremely large models and datasets.  Generalizability to NonNLP Tasks: The focus is primarily on NLP tasks. It remains unclear how the proposed method can be adapted or whether it would be effective for nonNLP deep learning applications.  3) Questions and Suggestions for the Authors  Scalability Discussion: Can the authors provide further insight into the computational requirements of TracInWE when applied to stateoftheart models like GPT3 or BERTLarge especially in terms of memory and runtime  Extension to NonNLP Tasks: Could the authors speculate on the potential modifications required to apply TracInWE to other domains such as computer vision or audio processing Are there inherent limitations to such adaptations  Sensitivity Analysis: It would be beneficial to see a sensitivity analysis regarding the \"overlap words\" parameters impact on TracInWEs performance. How does varying this parameter affect the methods accuracy and computation time  Further Validation: Considering the methods reliance on word overlap and gradient similarity how does TracInWE perform in languages with rich morphology or nonconcatenative morphology where word overlap might not fully capture semantic similarities  4) Limitations and Societal Impact The authors have outlined a method that improves the interpretability and efficacy of influence computation in NLP which is a commendable achievement. However they could address potential limitations regarding the methods application to languages with different morphological characteristics and its scalability to very large datasets and models. Additionally while the paper touches upon the societal impact of enhancing model interpretability further discussion on how TracInWE might mitigate or exacerbate issues such as bias in training data would provide a more rounded view of its potential societal implications. Constructive suggestions include offering guidelines for responsibly deploying TracInWE in diverse linguistic settings and exploring algorithmic adaptations to ensure scalable application to massive models without significant environmental costs.", "t0VbBTw-o8": " Peer Review of \"MessageInterception Smoothing for Certifiable Robustness in Graph Neural Networks\"  1. Summary The paper introduces a novel approach aimed at enhancing the adversarial robustness of Graph Neural Networks (GNNs) through a mechanism referred to as messageinterception smoothing. This method intercepts adversarial messages by randomly deleting edges andor masking node features in the graph addressing the issue with existing randomized smoothing techniques that provide overly pessimistic certificates by treating GNN models as blackboxes. The authors propose graybox certificates that leverage the architecturespecific messagepassing principle of GNNs allowing for the certification of robustness against stronger adversaries capable of manipulating multiple node features. The methods effectiveness is demonstrated through experiments on various datasets and models where it outperforms existing certificates in terms of certifiable robustness. Additionally graph sparsification is applied as a means to further improve the certifiable robustness while maintaining high accuracy.  2. Assessment of Strengths and Weaknesses  Strengths: 1. The idea of messageinterception smoothing is innovative addressing the limitations of existing blackbox and whitebox certificates for GNNs by proposing a flexible modelagnostic approach applicable to a wide range of messagepassing GNNs. 2. The paper provides a thorough theoretical foundation including a detailed explanation of the messageinterception smoothing technique its integration with the messagepassing framework and the derivation of provable graybox robustness certificates. This is complemented by proof sketches and discussions on the theoretical implications. 3. There is a significant empirical evaluation demonstrating the superiority of the proposed certificates over existing methods across multiple datasets and GNN architectures. The experiments are welldesigned covering a range of conditions and including a consideration of practical aspects such as computational efficiency and the impact of graph sparsification. 4. The paper effectively bridges the gap between theory and practice offering insights into how the proposed approach could influence the development of future messagepassing architectures training techniques and graph preprocessing techniques to enhance GNN robustness against adversarial attacks.  Weaknesses: 1. The discussion on limitations and societal impact is somewhat lacking. Although the authors briefly mention the limitations associated with the methods effectiveness across different graph structures and its robustness against hypothetical stronger adversaries there is little exploration of the broader societal implications particularly in contexts where GNNs are deployed for critical applications. 2. The methods adaptability to dynamic or evolving graphs is not addressed. GNNs are often applied in scenarios where the graph structure or node features change over time. The resilience of messageinterception smoothing in such contexts remains an open question. 3. Potential efficiency issues in largescale graphs are not fully explored. While the authors note the computational efficiency of their method compared to existing techniques the scalability of messageinterception smoothing to very large graph structures which are common in realworld applications could be further elaborated.  Questions and Suggestions for the Authors: 1. Can you elaborate on how the proposed method could be adapted or extended to handle dynamic graphs where the structure and node features evolve over time 2. Have you considered the potential for adversaries to anticipate the application of messageinterception smoothing and adapt their attack strategies accordingly How might this impact the robustness guarantees of your certificates 3. The experimental evaluation primarily focuses on certifiable robustness and computational efficiency. Could you provide more insights into the impact of messageinterception smoothing on model interpretability and explainability especially in complex graph structures 4. Considering the mentioned limitations do you have plans to explore the integration of your method with other robust training techniques or preprocessing strategies that could address these challenges  3. Limitations and Societal Impact The authors mention limitations related to the methods effectiveness across different graph structures and the theoretical bounds on certifiable robustness. However a more comprehensive discussion on the limitations particularly concerning the methods scalability and adaptability to dynamically changing graphs would strengthen the paper. Additionally a detailed examination of the potential negative societal impacts especially in sensitive applications is warranted along with suggestions on how to mitigate these risks.  Suggestions for Improvement:  Incorporate a section discussing potential countermeasures adversaries could employ in response to the deployment of messageinterception smoothing and how the method could be adapted to maintain robustness.  Extend the experimental evaluation to include largerscale graphs and dynamically changing graph structures to demonstrate the methods scalability and adaptability.  Provide a more thorough analysis of potential negative societal impacts offering guidelines for ethical deployment and suggestions for future work to address these concerns.", "sj9l1JCrAk6": "PeerReview of \"Federated Submodel Averaging for Improved Optimization in Recommender Systems and NLP\" 1. Summary of the Paper The manuscript presents an indepth study aimed at addressing optimization issues in federated learning (FL) specifically within recommender systems and natural language processing (NLP) contexts where data features demonstrate significant heat dispersion  a differentiation between hot and cold features based on client involvement. The key contribution is the introduction of a novel algorithm Federated Submodel Averaging (FedSubAvg) which adjusts the aggregation process of model updates by incorporating the feature heat metric thereby ensuring equitable updating speeds across model parameters. The authors theoretically demonstrate that FedSubAvg provides a preconditioning effect that facilitates more efficient global optimization and prove its convergence properties. Substantial empirical evaluations across various datasets (including MovieLens Sentiment140 Amazon and an industrial dataset from Alibaba) showcase the superior performance of FedSubAvg over existing FL algorithms like FedAvg and its variants in terms of convergence speed and training loss. 2. Assessment of Strengths and Weaknesses  Strengths: 1. Novelty: The concept of feature heat dispersion and its impact on federated learning optimization is a significant contribution. The FedSubAvg algorithm is both novel and relevant addressing a practical limitation of existing FL algorithms. 2. Theoretical Foundation: The paper provides a rigorous theoretical analysis of FedSubAvg proving its benefits over conventional approaches through preconditioning and demonstrating its convergence behavior. 3. Empirical Evaluation: Comprehensive experiments across multiple datasets and comparison with several baselines highlight the practical effectiveness of FedSubAvg.  Weaknesses: 1. Assumption Transparency: The assumptions required for the theoretical analysis particularly around the boundedness of the Hessian and gradient norms may not always hold in practical scenarios. More discussion on the implications of these assumptions and their realworld validity would be beneficial. 2. Scalability and Hardware Constraints: While the algorithm achieves better optimization performance there is limited discussion on its computational and memory overhead especially relevant in crossdevice FL settings with resourceconstrained clients. 3. Diversity of Applications: The focus is primarily on recommender systems and NLP. Exploring the algorithms applicability and effectiveness in other domains would broaden its appeal and utility. 3. Questions and Suggestions for the Authors  On Theoretical Assumptions: Could you provide more insight into the practicality of the theoretical assumptions made particularly regarding the bounded Hessian How robust is FedSubAvg to violations of these assumptions  Scalability Considerations: It would be valuable to include a discussion or analysis related to the computational and memory demands of FedSubAvg especially considering the diversity of devices that participate in federated learning.  Broadening Application Scopes: Have the authors considered testing FedSubAvg in domains beyond recommender systems and NLP If so sharing preliminary insights or plans would be enlightening.  Privacy Concerns: Given that FedSubAvg introduces a novel aggregation mechanism how does this impact the privacy guarantees of federated learning if at all Any additional measures needed 4. Authors Treatment of Limitations and Societal Impact The manuscript does not explicitly discuss the limitations or potential negative societal impacts of the proposed method. Given the increasing scrutiny of machine learning technologies acknowledging these aspects is crucial. Here are a few constructive suggestions:  Discuss Algorithm Limitations: A section dedicated to discussing FedSubAvgs limitations including scenarios where it may not perform as expected or its scalability and resource demands would be beneficial for readers.  Societal Impact Considerations: While FL inherently aims to preserve privacy by design the specific adaptations introduced by FedSubAvg should be evaluated for any unintended consequences such as potential biases in optimization due to feature heat dispersion. Addressing or at least acknowledging these concerns would strengthen the paper. In conclusion this work represents a significant step forward in federated learning particularly for applications with notable feature heat dispersion. Addressing the identified weaknesses and considering the suggested improvements could further enhance this valuable contribution to the field.", "mxnxRw8jiru": " Peer Review for \"Enhancing the Utility of Differentially Private Data Generation via Discriminative Sample Optimization\"  1. Summary of the Paper: The paper introduces a novel approach to differentially private data generation focused on optimizing a small set of samples that are representative of the original data distribution unlike traditional methods that attempt to train deep generative models. The key innovation lies in leveraging discriminative information from downstream tasks to guide the generation process thereby significantly improving the utility of synthetic samples for specific analytical purposes such as classification. This method addresses the challenge of generating highdimensional data under differential privacy constraints while maintaining practical utility for downstream tasks. The authors extensively validate their approach against stateoftheart methods in terms of sample utility memory and computation efficiency and generalization across various datasets and neural network architectures.  2. Assessment of the Paper: Strengths:  Novelty in Approach: The paper presents a unique perspective on differentially private data generation by focusing directly on optimizing informative samples rather than training complex deep generative models. This shift in focus could set a new direction for research in privacypreserving data generation.  Significant Improvement in Utility: The experimental results demonstrate a substantial improvement in task utility notably in classification accuracy over existing stateoftheart methods. This improvement is consistent across various privacy budgets datasets and neural network architectures highlighting the methods robustness and versatility.  Efficiency: Aside from improving sample utility the proposed method also offers benefits in terms of reduced memory and computation requirements for downstream analysis. This dual advantage makes it highly appealing for practical applications where resources are limited.  Generalization Capability: The paper convincingly shows that even when the synthetic samples are optimized for a specific architecture they still provide superior utility compared to baselines on other architectural setups. This generalizability is crucial for wide applicability. Weaknesses:  Limited Scope on Visual Quality: While the paper focuses on improving task utility there is an acknowledged tradeoff with the visual quality of generated samples. The impact of this tradeoff on applications where visual fidelity is critical remains unclear.  Scalability Concerns: The scalability of the approach to datasets with a large number of classes or requiring diverse samples to capture the data distribution is not thoroughly explored. Future work will need to address these scalability challenges.  Generality vs. Expressiveness: The paper rightly discusses the tradeoff between the generality of deep generative models and the taskspecific utility of the proposed approach. However the implications of this tradeoff for broader applications beyond the considered tasks could benefit from a more indepth discussion.  3. Questions and Suggestions for the Authors:  Visual Quality Improvement: Could the authors explore mechanisms to balance the tradeoff between sample utility and visual quality particularly for applications where both are critical  Scalability Exploration: Future work could benefit from exploring the scalability of the proposed method to more complex datasets and settings. Can the authors suggest potential strategies or modifications to tackle this challenge  Broader Applicability: While the focus on classification tasks is understandable expanding the methodology to other types of analyses (e.g. regression or unsupervised learning tasks) could enhance the papers impact. Are there ongoing efforts in this direction  4. Limitations and Societal Impact: The authors have addressed the limitations regarding visual quality and scalability appropriately offering a fair assessment of their methods scope. As for societal impact the paper emphasizes the potential positive implications of improving data utility for privacypreserving data generation which is commendable. However a more thorough discussion on potential misuse or unintended consequences especially in sensitive applications would strengthen the paper. Suggestions for future work include developing guidelines for responsibly deploying this technology and exploring safeguards against potential misuse. Overall this paper makes a significant contribution to the field of differentially private data generation offering a fresh perspective that balances task utility with privacy concerns.", "w-Aq4vmnTOP": "Review of Paper: Complete Characterization of Sample Complexity for NonInteractive Local Differential Privacy 1) Summary of the Paper: This paper undertakes a comprehensive study of the sample complexity necessary for learning and refutation tasks under the framework of noninteractive local differential privacy (LDP). The key contributions include a complete characterization of the sample complexity of agnostic Probably Approximately Correct (PAC) learning within noninteractive LDP protocols through the introduction of an approximate \\(L2\\) norm of a concept matrix associated with a given concept class. This characterization parallels the sample complexity for refutation tasks effectively illustrating an equivalence between agnostic learning and refutation under the constraints of noninteractive LDP. Additionally the paper extends its insights to the realm of realizable settings offering an analysis of realizable refutation and learning achieving this through a newly defined efficiently computable quantity that merges aspects of the \\(L2\\) norm and margin complexity. 2) Strengths and Weaknesses:  Strengths: 1. Theoretical Rigor: The paper presents a robust theoretical framework providing a novel characterization of sample complexity in the context of LDP. The mathematical proofs and derivations are detailed and appear to be thorough. 2. Practical Relevance: Addressing both learning and refutation in LDP settings is highly relevant given the increasing emphasis on privacypreserving machine learning. The results have potentially significant implications for designing efficient private learning algorithms. 3. Technical Novelty: The introduction of an approximate \\(L2\\) norm for measuring sample complexity and the efficient computable quantity \\(\\eta(C \\alpha)\\) for realizable settings are novel contributions that advance the understanding of LDP protocols.  Weaknesses: 1. Assumption Clarity: The paper assumes the reader has a strong foundational understanding of LDP PAC learning and refutation concepts without providing sufficient introductory explanations. This could limit its accessibility to a broader audience. 2. Empirical Validation Missing: Despite the papers strong theoretical contributions it lacks empirical evidence to demonstrate the practical effectiveness of the proposed theoretical results. 3. Discussion on Computational Complexity: While the paper excellently addresses sample complexity it offers limited insight into the computational complexity of the proposed learning and refutation protocols. 3) Questions and Suggestions:  Questions: 1. Could the authors provide more intuitive explanations or illustrations to make the core concepts more accessible to readers not thoroughly familiar with LDP or the specifics of learning in such a setting 2. Is there a potential extension of this work that could include empirical validations with realworld datasets or synthetic datasets designed to mimic practical scenarios  Suggestions: 1. It would enhance the paper to include a section or discussion on the implications of these findings on computational complexity and resource requirements for implementing the proposed methods in practical applications. 2. A comparative analysis with existing methods or theoretical frameworks dealing with LDP settings could further highlight the contributions and potential advantages of the proposed approach. 4) Limitations and Potential Negative Societal Impact: While the authors have significantly contributed to understanding the sample complexity in noninteractive LDP settings they have not explicitly discussed potential limitations or the negative societal impacts of their work. For improvement:  Address Data Bias: Any discussion on machine learning and differential privacy should consider the potential for data biases to be amplified through learning algorithms. The authors could discuss if their theoretical results hold under biased data distributions or suggest methods to mitigate such biases.  Potential Misuse: The paper should acknowledge the potential misuse of LDP mechanisms to obscure discriminatory practices under the guise of privacy protection. Suggestions for safeguards or ethical guidelines for the application of these theoretical insights could enhance the papers societal impact discussion. In conclusion this paper makes substantial theoretical contributions to understanding sample complexity in LDP settings. Addressing the above questions and suggestions could further strengthen the paper making it an even more valuable resource for both theoretical and applied researchers in the field of privacypreserving machine learning.", "tQRoZ9nRgM": "PeerReview Report: Title: Complexity and Achievable Performance in Sequential Matching under Uncertainty: A Study on Large Centralized Platforms 1) Summary of the Paper and Its Contributions This paper addresses the complex problem of sequentially matching jobs to workers on large centralized platforms where the skill proficiencies of workers are unknown. The authors formulate a model incorporating a unique treatment of the choice overload phenomenon representing a challenging scenario under incomplete information and imperfect learning. In response to the dynamic and uncertain environment of the gig economy they develop a novel rateoptimal learning algorithm which they showcase to perform robustly without prior knowledge of worker distribution or mean payoffs for jobworker pairs. Through rigorous theoretical analysis they establish lower bounds on the regret any matching algorithm might incur and prove the optimality of their proposed policy in terms of minimizing the expected cumulative regret. Specific contributions include tight informationtheoretic lower bounds on regret for the considered sequential matching problem and a novel algorithm with finitetime instancedependent expected regret logs n after n rounds establishing a secondorder dependence on workertype distributions. 2) Assessment of Strengths and Weaknesses Strengths:  Originality and Significance: The problem tackled is both practically relevant and theoretically challenging. The paper contributes significantly to dynamic matching under uncertainty especially in the gig economy context where understanding and optimizing such systems can lead to enhanced operational efficiencies.  Theoretical Rigor: The theoretical analysis including the establishment of lower bounds and the design of a novel learning algorithm is thorough. The proofs and mathematical treatment appear to be sound with clear definitions and logical flow.  Practical Relevance: By addressing choice overload and distributional shifts without prior distribution knowledge the model and algorithm have strong implications for realworld platforms. Weaknesses:  Empirical Validation: The paper focuses heavily on theoretical guarantees without providing empirical results. Including simulations or realworld data analyses could strengthen the work by highlighting the practical efficacy of the proposed solutions.  Assumption Clarity: While the assumption of finite jobtypes and discretization of skill levels is deemed reasonable the paper could benefit from a more detailed discussion on the implications of these and other assumptions ensuring that they dont overly restrict applicability. 3) Questions and Suggestions for the Authors  Empirical Results: Can the authors provide simulations or case studies demonstrating the algorithms performance in real or realistically simulated environments This would help validate the theoretical findings and illustrate the algorithms practical impact.  Assumption Justification: A more comprehensive justification for key assumptions (e.g. finite jobtypes known Kj a priori) and their relevance in realworld settings would be valuable. Are there circumstances under which these assumptions might not hold and how would that affect the applicability of the proposed solutions  Comparison with Existing Methods: It would be beneficial to include a comparison of the proposed algorithm with existing matching algorithms particularly those designed for similar settings. How does the proposed algorithm perform in terms of computational complexity and scalability 4) Limitations and Societal Impact The authors have made an effort to address the limitations inherent in their assumptions but could expand on the potential broader societal impacts of their work. Specifically considering the role of large centralized platforms in the gig economy its essential to discuss how optimizing such platforms might affect worker autonomy job satisfaction and market dynamics. Additionally while the paper states its contributions clearly elaborating on any ethical considerations or potential negative impacts (e.g. reinforcing existing inequalities or biases in job allocation) could provide a more nuanced view. Constructive Suggestions for Improvement:  Expand the discussion on the practical applicability and limitations of the model and algorithm considering realworld complexities and ethical considerations.  Strongly consider incorporating empirical validation to complement the theoretical analysis enhancing the papers appeal to a broader audience including practitioners. Overall the paper makes substantial theoretical contributions to the field and with some enhancements particularly concerning empirical evidence and discussions on assumptions and societal implications it could significantly impact both academia and industry.", "pk1C2qQ3nEQ": "Review of \"Balanced Entropy Acquisition for Improved Data Efficiency in Active Learning\" 1. Summary of the Paper This paper introduces a novel approach to enhance data efficiency in active learning paradigms particularly within machine learning models constrained by budgetary labeling resources. Central to its contribution is the design and implementation of a new uncertainty measure dubbed Balanced Entropy Acquisition (BalEntAcq). The measure is innovatively poised to balance the uncertainty involved in the softmax probabilities of model predictions against the actual label variability. This balancing act is technically realized through approximating marginal distributions via the Beta distribution allowing the authors to articulate the BalEntAcq as a fractional relationship between modified entropy and the marginalized joint entropy. This conceptualization facilitates a closedform representation of the BalEntAcq thereby enabling efficient parallelization. Empirical studies presented in the paper utilizing standard datasets like MNIST CIFAR100 SVHN and TinyImageNet underscore BalEntAcqs superior performance over established linearly scalable active learning methodologies including the likes of PowerBALD. 2. Strengths and Weaknesses Strengths:  Novelty: The introduction of BalEntAcq as a standalone measure for data selection underlines a significant stride in tackling the inherent redundancy plaguing current active learning mechanisms.  Analytical Rigor: The employment of Beta distribution for approximating marginal distributions and the subsequent derivation of a closedform expression for BalEntAcq reflect a wellgrounded analytical rigor.  Empirical Validation: The comprehensive experimentation undertaken across multiple datasets not only validates the efficacy of BalEntAcq but also firmly positions it as a preferred method over existing techniques like PowerBALD.  Practicality: Given that BalEntAcq can operate without relational computations across data points its adaptability and ease of implementation in realworld scenarios are notably enhanced. Weaknesses:  Scope of Applicability: The papers focus primarily rests on MCdropoutbased Bayesian neural networks. Although the authors hint at a broader applicability landscape for BalEntAcq explicit demonstrations within the text remain confined potentially narrowing perceived utility.  Theoretical Underpinnings regarding Beta Approximation: While innovative the transition from Beta approximation to BalEntAcq could benefit from a more detailed discussion particularly concerning the assumptions entailed and their implications.  Comparison with StateoftheArt: The comparison spectrum despite being comprehensive misses out on juxtaposing BalEntAcq against nonlinearly scalable methods that could present a different set of challenges and insights. 3. Questions and Suggestions for Authors  Broader Applicability: Could you provide additional insights or preliminary results on how BalEntAcq performs or might be adapted to other forms of Bayesian networks beyond the MCdropout frameworks  Theoretical Clarity: A deeper dive into the theoretical justification behind choosing Beta distribution for approximation and its implications on the robustness of BalEntAcq could enrich the papers narrative.  Comparative Analysis Enhancement: Consider extending the empirical comparison to include emerging or nonlinearly scalable active learning strategies thereby offering readers a broader spectrum of contextual understanding. 4. Limitations and Societal Impact The authors have candidly addressed the limitations particularly emphasizing the current applicability scope to dropoutbased Bayesian networks. An extension into broader Bayesian frameworks appears promising yet requires further exploration. The discussion on potential negative societal impacts is notably absent. While active learning methodologies inherently possess a neutral stance the prioritization and selection biases introduced by new measures like BalEntAcq could inadvertently skew data representation leading to biased models. A proactive deliberation on mitigating such biases along with strategic suggestions for diverse and equitable data selection can significantly enhance the works accountability and societal impact.", "owZdBnUiw2": "Peer Review of \"Ample and Focal Network (AFNet) for Efficient Video Recognition\" 1. Summary of the Paper: This paper introduces an innovative approach to video recognition termed Ample and Focal Network (AFNet) aimed at reducing the computational cost of action recognition in videos while enhancing performance. Traditional methods sample a limited number of frames to represent a video which often results in information loss and limited recognition performance due to the computational burden associated with processing a large number of frames. AFNet addresses this by incorporating a twobranch strategy: an ample branch that processes all input frames at reduced computational costs and a focal branch that concentrates on salient frames determined by the ample branch through a lightweight navigation module. This configuration not only allows for the processing of more frames at a lower computational cost but also dynamically focuses on the most relevant frames thus enhancing recognition accuracy. The paper validates AFNets efficacy and efficiency across five video recognition datasets showcasing superior performance when compared to existing stateoftheart methods. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The twobranch strategy comprising an ample branch and a focal branch guided by a navigation module is a novel contribution that effectively balances the computational cost and recognition performance.  Comprehensive Evaluation: The authors conduct extensive experiments across five datasets demonstrating the models versatility and superior performance over existing methods.  EndtoEnd Trainability: The design of the navigation module facilitates easy integration into existing CNN structures allowing for endtoend training without additional computational overhead typically associated with policy networks. Weaknesses:  Generalization Concerns: While the paper presents impressive results the adaptability of AFNet to scenarios with extreme variances in action dynamics and frame importance (e.g. videos with subtle actions or rapid transitions) requires further exploration.  Hardware Utilization Efficiency: The paper does not discuss AFNets efficiency in terms of hardware utilization which is crucial for deployment in realworld applications especially on devices with limited computational resources.  Limited Discussion on Failure Cases: The paper lacks an indepth discussion on scenarios where AFNet might not perform optimally which is important for understanding the models limitations and areas for improvement. 3. Questions and Suggestions for the Authors:  Could the authors elaborate on how AFNet would perform in scenarios involving subtle actions or quick complex activity transitions where frame saliency might be harder to determine  It would be beneficial if the paper included a section discussing the model\u2019s efficiency in terms of hardware utilization particularly for realworld applications on edge devices.  The addition of an analysis focusing on failure cases or less optimal scenarios for AFNet would provide a more comprehensive understanding of the models limitations guiding future improvements.  Considering the significance of temporal dynamics in video recognition how does AFNet compare to methods explicitly designed for temporal modeling such as those employing LSTM or GRU architectures 4. Limitations and Constructive Suggestions:  The discussion on the potential negative societal impact of the technology seems missing. It is crucial to consider and address these aspects given the broad application scenarios of video recognition technology including surveillance and privacy concerns. The authors are encouraged to include a section discussing these issues and how they might be mitigated.  To strengthen the paper further I suggest including more detailed analysis regarding the generalization capability of AFNet specifically in handling diverse video content with varying action dynamics and frame saliency. This could involve additional experiments or theoretical discussions.  Exploring and discussing the models performance and efficiency regarding hardware utilization would be highly valuable especially when considering deployment in realworld applications with computational constraints.", "kpSAfnHSgXR": " 1) Brief Summary The paper presents a novel approach to modeling directed graphs especially addressing the challenge of representing graphs that exhibit both cycles and transitivity. It proposes binary code box embeddings (BCBOX) which use learned binary codes to selectively intersect subsets of graphs thus allowing for the representation of arbitrary directed graphs including cyclic ones. The paper introduces two variants of binary code box embeddings: global binary codes (GBCBOX) for a union over intersections and pervertex binary codes (VBCBOX) for increased flexibility. Through theoretical analysis and empirical evaluation the paper demonstrates the models capability to preserve transitivity bias and model cycles outperforming existing methods on graph reconstruction and link prediction tasks across various graphs.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The concept of using binary code for box embeddings presents an innovative way to enhance representational power for graph modeling particularly in tackling the cycle representation challenge.  Theoretical Foundation: The paper provides a solid theoretical foundation proving that BCBOX models can indeed represent any directed graph a significant leap over previous limitations.  Empirical Validation: Strong empirical results demonstrate the models superiority in graph reconstruction and link prediction tasks showcasing practical benefits.  Open Source: By making their code available the authors encourage reproducibility and further exploration. Weaknesses:  Complexity and Scalability: The introduction of binary codes particularly in the VBCBOX variant significantly increases model complexity and parameter count which may raise concerns about scalability and computation efficiency.  Limitation in Transitivity Representation: The paper mentions potential weakening in the inductive bias of asymmetric transitivity due to the modeling approach for cycles. This tradeoff might impact the modeling of certain graphs where transitivity is a critical feature.  Experimental Scope: While the paper provides a diverse set of experiments the exploration of realworld applications and datasets particularly at a larger scale remains limited.  3) Questions and Suggestions for the Authors  Transitivity Bias: Can the authors elaborate on the extent to which the transitivity bias is affected in practical scenarios Are there specific types or structures of directed graphs where this tradeoff becomes particularly problematic  Scalability and Efficiency: Could the authors discuss potential strategies to improve the scalability and computational efficiency of VBCBOX models considering the increased parameter space  Realworld Applications: It would be valuable to see the application of BCBOX models in realworld scenarios such as social network analysis or knowledge graphs to understand their practical implications better.  Comparison with StateoftheArt: How do BCBOX models compare with the latest graph neural network (GNN) approaches especially those designed to handle directed graphs and cycles  4) Limitations and Societal Impact The authors have addressed the limitations related to representational capability and complexity. However more discussion is needed on the computational efficiency and scalability of the proposed models especially for very large graphs. Regarding societal impact the paper does not elaborate on potential negative aspects. Constructively the authors could explore safety and ethical considerations when applying their models to sensitive or critical domains such as predictive policing or social network analysis where inaccurate or biased graph representations could have significant consequences.", "r-6Z1SJbCpv": "Review of \"OPTFORMER: Towards Universal Hyperparameter Optimization with Transformers\" 1) Summary of the Paper This paper introduces the OPTFORMER a novel hyperparameter optimization (HPO) framework that utilizes a Transformerbased model to learn from a diverse array of tuning experiments. This approach represents a significant departure from traditional methods that require a fixed set of hyperparameters across tasks allowing the OPTFORMER to leverage a broader range of data including the extensive Google Vizier database. The papers primary contributions are the formulation of a metalearning HPO framework demonstrating the ability of the OPTFORMER to learn the behaviors of 7 different HPO algorithms and showcasing its effectiveness in making accurate and wellcalibrated function predictions. The authors argue that the OPTFORMER not only competes with traditional Gaussian Processes in terms of prediction accuracy but also offers a more robust prior distribution for hyperparameter response functions thus paving the way for its application as a general HPO optimizer. 2) Strengths and Weaknesses Strengths:  Novelty: The paper addresses a significant limitation in the field of HPO by introducing a method that can learn from experiments with varying hyperparameters. This represents a substantial advancement over traditional methods that require a uniform set of hyperparameters.  Data Use and Experiments: Leveraging one of the world\u2019s largest HPO datasets Google Vizier the paper sets a new standard for empirical evaluation in HPO research. The extensive experiments demonstrate the versatility and the effectiveness of the OPTFORMER across a broad spectrum of tasks.  Practical Implications: The methodology described could substantially reduce the computational resources required for hyperparameter tuning tasks especially for organizations that engage in largescale machine learning deployments. Weaknesses:  Complexity and Accessibility: The complexity of the methodology might make it inaccessible to practitioners outside of large research organizations. The paper could benefit from a more detailed discussion on the practical implementation and potential challenges in adopting this method.  Model Scalability: While the paper discusses the proficiency of the OPTFORMER in dealing with large datasets it does not thoroughly address potential scalability issues or the computational resources required for training and inference.  Evaluation Depth: Although the experimental evaluation is extensive it primarily focuses on performance metrics like predictive likelihood and calibration error. An indepth analysis comparing the computation times resource utilization and efficiency improvements over traditional methods would provide a more comprehensive evaluation of the OPTFORMERs practical benefits. 3) Questions and Suggestions for the Authors  Model Interpretability: How interpretable are the decisions made by the OPTFORMER especially in comparison to Gaussian Processes or other traditional methods Integrating some form of interpretability or explanation for the suggested hyperparameters could significantly increase the trust and adoption of your model in practical settings.  Impact of Data Heterogeneity: How does the diversity in the datasets especially in terms of task complexity and domain specificity affect the OPTFORMERs learning and performance Discussing the models sensitivity to the quality and variance of the input data might provide valuable insights into its applicability across different settings.  Extension to Other Forms of Optimization: Have the authors considered extending the approach to tackle other forms of optimization problems beyond HPO Given the models ability to learn from diverse datasets its applicability to broader optimization tasks would be an exciting avenue to explore. 4) Limitations and Societal Impact The authors have made an effort to address the limitations within their work such as the handling of hyperparameters with dynamic constraints and the challenge posed by the fixed maximum sequence length of Transformers. However a critical limitation that needs more discussion is the potential barrier to accessibility due to the models complexity and resource requirements. Small and medium enterprises might find it challenging to utilize such an advanced system for their HPO needs thus widening the technological gap between them and larger organizations. Regarding societal impact while the paper does not explicitly tackle negative societal impacts the democratization of efficient HPO methods like the OPTFORMER could lead to wider adoption of machine learning models potentially exacerbating issues related to model bias and privacy concerns. Suggesting guidelines or mechanisms to mitigate such risks when employing the OPTFORMER in diverse applications would be a valuable addition to the work. Overall the paper presents a significant leap forward in the domain of HPO and opens up numerous possibilities for future research and applications. With some enhancements in addressing the outlined concerns this work has the potential to become a cornerstone in the field of automated machine learning.", "yNPsd3oG_s": "PeerReview of \"A Novel Training Method for Mitigating Trojan Attacks in Deep Neural Networks\" 1) Summary of the Paper: The paper addresses a critical issue in deep neural networks (DNNs) the vulnerability to backdoor or Trojan attacks. These attacks can be injected by poisoning the training dataset or can naturally occur due to particular patterns in benign data. The authors identify that the primary cause of such vulnerabilities lies in the linearity of decision regions created by piecewise linear activation functions in DNNs. They develop a novel training methodology named NONE (NONLinEarity) aiming to mitigate both injected and natural Trojans by enforcing nonlinearity in decisionmaking regions of the network. The approach identifies and resets compromised neurons and removes biased or poisoning samples during the training process. Extensive experiments across multiple datasets and attack scenarios showcase the methods superiority over existing stateoftheart (SOTA) defenses demonstrating significant reductions in attack success rates (ASR) without substantially diminishing benign accuracy (BA). 2) Assessment of Strengths:  Innovative and Rigorous Analysis: The paper provides a thorough and novel analysis of the cause of DNN Trojans establishing a direct link between the linearity of decision regions and the susceptibility to Trojan attacks. This foundational insight is a significant contribution to the field.  Comprehensive Experimental Evaluation: The contribution includes extensive experiments on diverse datasets and against various types of attacks thoroughly demonstrating the effectiveness of the proposed method in reducing ASR while maintaining high BA.  Practical Significance: By addressing both injected and natural Trojans the proposed method tackles a broader spectrum of threats compared to existing solutions. This has considerable implications for the practical security of DNNs in various applications.  Reproducibility and Open Science: Making the code publicly available is commendable promoting transparency reproducibility and further research in the area. 3) Assessment of Weaknesses:  Lack of Discussion on Computational Overhead: While the paper discusses the effectiveness of NONE in mitigating Trojan attacks it does not explicitly address the computational overhead introduced by the novel training regime. Given the additional steps of identifying compromised neurons and reseting them clarification on the impact on training time and computational resources would be beneficial.  Limited Scope of Testing Environments: Although the experiments cover a range of datasets and attack scenarios the testing seems to concentrate on image classification tasks. Expanding the evaluation to other domains like natural language processing or other computer vision tasks would provide a more comprehensive understanding of the methods applicability.  Potential for Adaptive Attacks: The paper argues that the proposed method can mitigate known Trojan attacks but does not extensively discuss the resilience of NONE against potential adaptive attacks designed to circumvent the nonlinearity enforcement. 4) Questions and Suggestions:  Computational Overhead: Can the authors provide insights regarding the computational cost of implementing NONE compared to traditional training methods This information will help in evaluating the practicality of implementing NONE in resourceconstrained environments.  Scope of Applicability: Given the focus on image classification tasks do the authors anticipate any challenges in applying NONE to other domains such as natural language processing or reinforcement learning  Adaptive Attacks: How might NONE be adapted or evolved to remain effective against potential adaptive Trojan attacks that may arise specifically to counteract its mechanism 5) Limitations and Suggestions for Improvement: The authors should address the papers limitations regarding the computational overhead of the NONE training method more clearly. Providing empirical data on the additional computational costs would strengthen the paper. Additionally expanding the scope of evaluations to include other domains beyond image classification and thoroughly discussing the resilience against adaptive attacks would round out the papers contributions. Offering insights into these areas could significantly enhance the works robustness and applicability. 6) On Addressing Limitations and Potential Negative Societal Impact: The paper effectively outlines its contributions but could benefit from a more detailed discussion on potential negative societal impacts such as privacy concerns arising from the nature of data needed for training and testing robustness against attacks. Furthermore while acknowledging the works primary focus on mitigating Trojans in image classification expanding the discussion to include potential limitations in generalizability across different machine learning domains and models would be advantageous. Suggestions include proposing guidelines for adapting NONE to various machine learning tasks and initiating collaborations for crossdisciplinary research to tailor and test the methodology in broader contexts.", "ocViyp73pFO": " Review of \"Disentangling the Effect of Single Interventions from Concurrent Interventions: A Causal Inference Approach\"  Summary The paper addresses a significant challenge in causal inference: disentangling the effects of single interventions from those of concurrently applied interventions particularly in the presence of unobserved confounders. The authors prove that identifying the effect of single interventions in general SCMs is impossible without restrictions but demonstrate it is feasible under nonlinear continuous structural causal models with additive multivariate Gaussian noise. They propose an algorithm to learn the parameters of such causal models by leveraging both observational and sets of intervention data. The method\u2019s effectiveness is validated on both synthetic and realworld datasets.  Assessment Strengths: 1. Theoretical Contribution: The paper provides important theoretical insights by showing that under certain conditions (specifically within certain classes of nonlinear continuous SCMs with additive noise) single intervention effects can be identified even with unobserved confounders. This extends our understanding of what is possible within causal inference under complex interventions. 2. Practical Methodology: The algorithm proposed for disentangling single interventions from concurrent interventions is pragmatic meshing well with the theoretical identifiability proofs. This bridges a crucial gap between theory and application offering a concrete tool for practitioners. 3. Robust Validation: The empirical validation including both synthetic and realworld data tests is thorough and demonstrates the applicability of the proposed methods across different scenarios. The validation process is clear allowing for repeatability and further investigation by other researchers. Weaknesses: 1. Assumption Limitations: The papers restrictions to nonlinear continuous SCMs and additive multivariate Gaussian noise might limit the applicability in scenarios where these conditions do not hold. While these assumptions are common in the causal inference literature the universality of the proposed solutions might be constrained. 2. Potential for Broader Impact Discussion: Although the paper touches on the application areas such as medicine computational advertising and dietetics a more detailed discussion on the broader societal impacts both positive and negative would enhance the paper\u2019s relevance. For instance the implications of misidentifying causal effects in highstakes domains could have been explored in more depth.  Questions and Suggestions for Authors 1. Extended Applicability: Can the authors comment on the potential or planned extensions of their methodology to other types of noise or causal model structures This might offer insight into future directions and the potential universality of the approach. 2. Scalability Considerations: Does the proposed method scale efficiently with the dimensionality of data or the complexity of intervention sets It would be beneficial if the authors could discuss any computational constraints and possible optimizations. 3. Robustness to Model Misspecification: How sensitive is the proposed methodology to misspecifications in the SCM or incorrect assumptions about the noise distribution Some discussion or sensitivity analysis on this front would be valuable.  Limitations and Potential Negative Societal Impact The authors have laid the groundwork for addressing limitations related to the assumptions of their causal models and noise types. However a more profound discussion on the implications of these limitations in practical scenarios would strengthen the paper. The potential negative societal impact seems not thoroughly discussed. In particular the methodologys application in sensitive areas like medicine or public policy requires careful consideration of the consequences of incorrect causal inferences. It would be beneficial for the authors to elucidate on safeguards or ethical considerations that practitioners should take into account when applying their method to ensure responsible use.  Suggestions for Improvement 1. Exploration of More General Conditions: The authors could investigate broader classes of SCMs or noise distributions to enhance the applicability of their contributions. 2. Impact and Ethical Considerations: Devoting a section to discussing the broader impact including potential negative consequences and ethical considerations of applying their method in sensitive domains would provide a more holistic view of their work. 3. Additional Empirical Studies: Including case studies or additional empirical validation in more complex realworld settings might further substantiate the robustness and versatility of the proposed method.", "qf12cWVSksq": "Review of the Inception Transformer (iFormer) Paper 1) Summary: The paper presents a novel architecture named Inception Transformer (iFormer) designed to effectively learn comprehensive features in visual data by integrating both high and lowfrequency information. The core contribution is the Inception mixer which merges the capabilities of convolutionmaxpooling and selfattention to respectively handle high and lowfrequency information through a channel splitting mechanism. This approach not only addresses the Transformers challenge in capturing highfrequency details but also maintains its strength in modeling longrange dependencies. Further innovation is seen in the frequency ramp structure which adapts the processing emphasis across layers to balance the tradeoff between high and lowfrequency components dynamically. The iFormer demonstrates superior performance on various vision tasks including image classification object detection and semantic segmentation outperforming stateoftheart models like DeiTS and SwinB under certain settings. 2) Assessment: Strengths:  The paper tackles a wellidentified limitation of current Transformer models in processing visual data specifically their inefficiency in capturing highfrequency information such as edges and textures.  The proposed iFormer architecture is innovative blending the strengths of CNNs for local information processing with the global contextual understanding facilitated by Transformers. The inclusion of a channel splitting mechanism for parallel processing of frequency components is particularly commendable.  The frequency ramp structure is a thoughtful addition ensuring the model dynamically adjusts its focus on high versus lowfrequency information based on the hierarchical layer context mimicking natural visual perception systems.  Extensive experiments and comparisons with other models are provided showing the iFormers superior performance on several benchmarks and tasks which solidifies the papers contributions. Weaknesses:  The methodology section particularly the explanation of the Inception mixer and frequency ramp structure could benefit from further detail or a stepbystep walkthrough to enhance reproducibility and understanding.  The paper does not extensively discuss the computational efficiency of iFormer in comparison to its peers especially given its superior performance. Insights into its computational demands potential for scalability and comparison in terms of inference times would have been valuable.  While the performance improvements are impressive some of the gains especially in largescale settings are modest. Discussion on the tradeoffs between performance gains and increased complexity or computational resources required is sparse. 3) Questions and Suggestions:  Could the authors provide more insight or quantitative analysis on the computational efficiency of iFormer especially in comparison with models it outperforms  The paper would benefit from a deeper exploration of the Inception mixers design choices. For instance why were specific operations like maxpooling and depthwise convolution preferred Including ablation studies on these choices could enrich the discussion.  Considering the significance of the frequency ramp structure it would be intriguing to investigate its optimization or automatic determination possibly through metalearning or a neural architecture search strategy.  Future work could explore the implementation of iFormer in other domains beyond visual tasks such as audio signal processing or timeseries analysis to evaluate its generalizability and efficiency. Limitations and Societal Impact: While the paper acknowledges the computational constraints limiting experimentation with larger datasets and the manual tuning required for the channel ratio in the frequency ramp structure it falls short of thoroughly discussing potential negative impacts such as environmental costs due to higher computational demands or ethical concerns in applications. To mitigate these issues and enhance the model\u2019s practical applicability the authors could explore strategies for optimizing computational efficiency and propose more automated ways to determine model hyperparameters. Additionally a brief discussion on ethical considerations especially in sensitive applications of computer vision technologies would be valuable to guide responsible use and future developments.", "q2nJyb3cvR9": "This paper introduces a novel reinforcement learning (RL) algorithm that uses single seed randomization (SSR) for exploration. The method applies randomized value functions differentiating itself by injecting a single random seed per episode and employing a Bernsteintype magnitude of noise to guide exploration. This approach significantly improves upon the existing regret bounds in episodic timeinhomogeneous Markov Decision Processes (MDPs) achieving a nearoptimal \\xc3\\x95(H\\xe2\\x88\\x9aSAT) regret which aligns with the theoretical lower bound up to logarithmic factors. Notably this is the first instance where randomized exploration matches the lower bound achieved previously by optimistic algorithms. The paper also introduces a new clipping strategy for maintaining both optimism and pessimism within the value function estimation and a novel recursive formula for controlling the absolute estimation error contributing to the theoretical analysis of RL algorithms based on randomized exploration. Strengths: 1. Theoretical Contribution: This paper closes a significant gap in the literature by demonstrating that randomized exploration can achieve nearoptimal regret bounds in episodic timeinhomogeneous MDPs matching the theoretical lower bound for the first time. 2. Novel Methodology: The introduction of a single random seed per episode and a new clipping strategy are innovative. They serve as the foundational contributions enabling the algorithm to maintain a controlled explorationexploitation balance contributing to its theoretical performance guarantee. 3. Solid Analysis: The paper provides a thorough theoretical analysis including a novel recursive formula for estimation error analysis which is a significant contribution to understanding the dynamics of randomized exploration in RL. Weaknesses: 1. Practical Applicability: While the theoretical advancements are clear the paper lacks a detailed discussion on the practical applicability of the SSR algorithm especially in nontabular or more complex environments where function approximation is required. 2. Empirical Evaluation: The paper primarily focuses on theoretical contributions without providing comprehensive empirical evaluations. An extensive set of experiments comparing SSR with existing methods across different environments could strengthen the papers claims. 3. Algorithmic Complexity and Implementation Details: There is a lack of discussion around the computational complexity and the practical implementation details of the SSR algorithm which might limit its accessibility and application in practice. Questions and Suggestions for the Authors: 1. Extension to Function Approximation: Can the authors discuss potential extensions or necessary modifications of the SSR algorithm to make it applicable to settings with function approximation 2. Empirical Performance: It would be beneficial to include more empirical results particularly in complex environments to validate the algorithms performance and its practical viability compared to other stateoftheart algorithms. 3. Comparison with Optimistic Algorithms: Given the papers theoretical contributions a deeper comparative analysis with optimistic algorithms could offer insights into the conditions under which SSR particularly excels or falls short. Limitations and Societal Impact: The authors have not discussed the potential limitations and the negative societal impacts of their work. While theoretical research in RL generally has a lower immediate societal impact compared to applied research it is crucial to consider the broader implications of enhanced exploration techniques especially in sensitive applications like autonomous vehicles or healthcare. Constructive Suggestions for Improvement: 1. The paper could benefit from a clearer presentation of the practical implications of the SSR algorithm including potential applications and the algorithms performance in more realistic settings. 2. A detailed discussion on the computational aspects of the SSR algorithm including its complexity and scalability to highdimensional problems would enhance the paper. 3. Addressing the societal impact especially the ethical considerations around the deployment of advanced RL algorithms in realworld applications could provide a more rounded perspective on the works implications. In conclusion this paper makes significant theoretical contributions to the field of reinforcement learning by demonstrating that randomized exploration can achieve nearoptimal regret bounds. Further research and discussions on practical applications empirical validations and ethical considerations would enhance its impact and applicability.", "mhp4wLwiAI-": "Summary of the Paper: This paper addresses the challenges in training deep Graph Convolutional Networks (GCNs) which include oversmoothing information squashing vanishing gradients and overfitting that lead to substandard performance. Unlike previous works that mostly focused on architectural and regularization tweaks to deal with oversmoothing and oversquashing this study introduces a novel perspective by examining how healthy gradient flow can enhance the trainability and performance of deep GCNs. The contributions are threefold: they propose a new topologyaware isometric initialization based on the principle of isometry that incorporates graph topology they suggest a method for gradientguided dynamic rewiring of the networks with skip connections based on the gradient flow within each layer and they provide extensive experimental evidence demonstrating the effectiveness of these methods in improving the performance of deep GCNs to stateoftheart levels across multiple datasets. Assessment of Strengths and Weaknesses: Strengths: 1. Novel Perspective: The papers focus on the dynamics of gradient flow as a central aspect of enhancing trainability and performance is innovative and distinct from the dominant focus on architectural and regularization methods in existing literature. 2. Theoretical Foundation: The derivation of a topologyaware isometric initialization scheme provides a theoretically grounded approach to improving gradient flow a crucial factor seldom addressed in the context of GCNs. 3. Practical Efficacy: The dynamic rewiring method is a practical and potentially impactful approach enabling the model to adaptively adjust its structure during training for optimal performance. The extensive experimentation across various datasets solidifies the practical value of the proposed methods. Weaknesses: 1. Complexity and Comprehensibility: The methods introduced especially the topologyaware isometric initialization may present a steep learning curve and could be difficult to comprehend without a strong background in the area. 2. Generalization Concerns: While the paper presents extensive experimental results the generalization of the proposed methods to other types of graphstructured data or different neural network architectures remains unexplored. 3. Computational Efficiency Not Discussed: The impact of the proposed methods on computational efficiency including training time and resource requirements is not discussed which is crucial for their adoption in largerscale problems. Questions and Suggestions for the Authors: 1. Generalization to Other Domains: Can the authors provide insights or preliminary results on how their proposed methods might be adapted or generalized to other types of graphstructured data not covered in the experiments 2. Computational Efficiency: It would be beneficial to include a discussion on the computational efficiency of the proposed methods particularly how the gradientguided dynamic rewiring method affects the computational complexity during training. 3. Comparison with NonGCN Models: How do the proposed methods compare with stateoftheart models that are not based on GCNs This comparison can help highlight the specific advantages of improving GCNs using the proposed methods. Limitations and Potential Negative Societal Impact: The authors have addressed some limitations such as the focus on vanillaGCNs but further discussion on the computational efficiency and generalizability of the proposed methods could enhance the paper. Regarding potential negative societal impacts while the paper focuses on theoretical and methodological advancements in GCNs any technology capable of analyzing graphstructured data poses inherent risks related to privacy and misuse of personal data especially in sensitive applications like social network analysis. A brief discussion on these aspects could raise awareness and encourage responsible use of the technology. Suggestions for improving these areas could include exploring lightweight versions of the proposed methods for better computational efficiency and conducting experiments on a wider range of datasets to test generalizability.", "pHd0v8W30O": "Review of \"Latentvariable AdvantageWeighted Policy Optimization for Offline RL on Heterogeneous Datasets\" 1. Summary of the Paper: The paper introduces a novel method named Latentvariable AdvantageWeighted Policy Optimization (LAPO) for addressing the challenge of learning policies from heterogeneous offline datasets in reinforcement learning (RL) especially applicable in continuous control robotic applications. The key innovation lies in leveraging a latentvariable generative model to represent highadvantage stateaction pairs enabling the algorithm to adhere better to data distributions conducive to the task while maximizing rewards. The empirical results demonstrate LAPOs superiority outperforming existing offline RL methods by significant margins across various simulated tasks in terms of average performance. 2. Assessment of Strengths and Weaknesses:  Strengths:  Innovative Approach: The use of latentvariable models for effectively navigating the challenges presented by heterogeneous data is highly innovative. It represents a significant step forward in generating highadvantage stateaction samples conducive to task achievement.  Empirical Validation: The comprehensive set of experiments across a wide array of tasks including locomotion navigation and manipulation robustly demonstrates LAPOs effectiveness with substantial improvements over existing methods.  Addressing a Practical Challenge: The method aptly addresses the realworld challenge of learning from heterogeneous datasets a scenario frequently encountered in robotic applications making it highly relevant and impactful.  Weaknesses:  Lack of Theoretical Insights: The paper provides an extensive empirical evaluation but lacks a deeper theoretical analysis of why the proposed method outperforms existing techniques significantly.  Application Scope: While the empirical results are compelling insights into the method\u2019s applicability or performance in realworld robotic systems are missing. Understanding the impact of the method in practical scenarios beyond simulations would provide a fuller picture.  Complexity and Implementation Details: The paper could benefit from a more detailed discussion on the computational complexity of LAPO and practical implementation considerations. 3. Questions and Suggestions for Authors:  Could the authors provide more insights or theoretical backing that explains the significant performance gains of LAPO over existing methods  It would be beneficial if the authors can discuss the applicability of LAPO in realworld scenarios especially in complex robotic applications. Are there specific types of tasks or environments where LAPO might not perform as expected  A clearer exposition on the computational demands and potential scalability issues of LAPO along with suggestions for mitigating them would enhance the paper.  Considering the importance of the parameter settings (e.g. the choice of \u03bb and \u03b2) can the authors discuss the sensitivity of the performance of LAPO to these hyperparameters 4. Limitations and Societal Impact: The authors have not fully addressed the potential limitations and the negative societal impacts of their work. While the method shows promise in offline RL settings understanding its limitations in realworld applications and the ethical considerations when deployed in autonomous systems is crucial. For improvement the authors could:  Discuss the potential risks and ethical considerations when deploying LAPO in reallife robotic systems including scenarios where the method might fail or act unpredictably.  Elaborate on the limitations of LAPO when applied to realworld robots particularly addressing any scalability or adaptability concerns in dynamic environments.  Consider exploring the environmental impact of deploying such algorithms in terms of energy consumption and the carbon footprint associated with the computational resources required. Overall the paper represents a significant advancement in offline RL specifically for learning from heterogeneous datasets with promising implications for continuous control robotic applications. Addressing the highlighted questions and suggestions could further strengthen the contribution and applicability of LAPO.", "ouXTjiP0ffV": " Peer Review of the Paper \"Neural Correspondence Prior for 3D Shape Matching\"  1. Brief Summary The paper introduces Neural Correspondence Prior (NCP) a novel unsupervised paradigm for computing correspondences between 3D shapes including challenging cases like sparse point clouds and nonisometric meshes. The authors propose a twostage algorithm: an initial unsupervised training to obtain noisy matches followed by using these matches to train a network for improved accuracy. They demonstrate that networks can act as an effective prior in shape correspondence tasks improving over both traditional unsupervised techniques and supervised methods that rely on labeled data. Their experiments showcase the methods data efficiency fast processing and stateoftheart performance.  2. Assessment of Strengths and Weaknesses  Strengths: 1. Novelty and Originality: The approach of leveraging neural networks as a prior for 3D shape matching without the need for supervised training is innovative and takes a unique stance in the field. 2. Unsupervised Methodology: The ability to train and refine correspondences in an unsupervised manner especially for nonisometric and sparse point cloud matching addresses significant limitations in existing methods that require extensive labeled datasets. 3. Quality of Results: The method outperforms stateoftheart (SOTA) methods in multiple challenging scenarios demonstrating its effectiveness and versatility. 4. Generality of the Approach: The paper presents a method that is not tightly bound to specific network architectures or loss functions suggesting the wider applicability of the underlying principles.  Weaknesses: 1. Assumption on Initial Maps: While the method is robust its success partly relies on the initial maps obtained in the first stage having some meaningful structure. The performance can degrade significantly if these initial maps are nearly random or of very low quality. 2. Computational Efficiency Not Quantitatively Discussed: Despite mentioning fast processing as a strength the paper lacks a detailed comparison of computational requirements (e.g. training time inference time) relative to other methods. 3. Limited Discussion on Limitations and Societal Impact: The paper does not dedicate substantial discussion to the algorithms limitations beyond the assumption on initial map quality or potential negative societal impacts.  3. Questions and Suggestions for the Authors  Questions: 1. Robustness to Extremely Sparse Data: How does the performance of NCP change with increasingly sparse point clouds and is there a lower bound of density below which the method fails to generate reliable correspondences 2. Scalability: What are the computational resources required for larger shape collections Is there a practical limit to the number of shapes that can be effectively matched using this method  Suggestions: 1. Quantitative Analysis of Computational Efficiency: It would strengthen the paper to include detailed benchmarks of computational requirements especially in comparison to other methods. This information is crucial for assessing the practical applicability of NCP. 2. Further Exploration of Limitations: A more indepth discussion of the methods limitations and potential strategies to overcome them would be valuable. For instance potential approaches to improve results when starting with lowquality initial maps could be explored. 3. Discussion on Societal Impact: Adding a section on the potential societal impact including the risk of misuse and ethical considerations related to 3D data would make the paper more comprehensive.  4. Limitations and Potential Negative Societal Impact The paper briefly mentions the limitation regarding the dependence on the structure of initial maps but could benefit from an expanded discussion of limitations including potential failure modes or scalability issues. Furthermore there is no discussion on the potential negative societal impact. For example highaccuracy 3D shape matching could have implications for surveillance or privacy breaches if applied to sensitive datasets without appropriate safeguards. As a constructive suggestion the authors could include guidelines for ethical use and address how their work could be applied responsibly emphasizing data privacy and user consent where applicable.", "wlrYnGZ37Wv": "Brief Summary and Contributions: The paper introduces \"Sequencer\" a novel architecture for computer vision that challenges the prevailing dominance of Vision Transformers (ViT) and convolutional neural networks (CNNs) by proposing an attentionfree LSTMbased model. The primary innovation of Sequencer is the use of bidirectional LSTMs (BiLSTMs) to model longrange dependencies within images a task traditionally managed by selfattention mechanisms in ViTs. The authors further enhance Sequencers capability by introducing a twodimensional BiLSTM (BiLSTM2D) which processes vertical and horizontal image sequences in parallel to improve model efficiency and performance. Experimental results on the ImageNet1K dataset show that Sequencer particularly Sequencer2DL achieves competitive top1 accuracy while demonstrating robust resolution adaptability and effective parameter usage. The paper also testifies to Sequencers transferability and efficiency in semantic segmentation and other vision tasks establishing it as a compelling alternative to current stateoftheart architectures in computer vision. Assessment of Strengths and Weaknesses: Strengths: 1. Novelty of Approach: The use of LSTMs particularly in a twodimensional format to process spatial information in images is a significant departure from the current trends focused on selfattention and convolutional operations. This approach offers a fresh perspective on handling longrange dependencies and spatial relationships in images. 2. Competitive Performance: The paper demonstrates that Sequencer achieves competitive or superior performance on the ImageNet1K dataset compared to current stateoftheart models which is impressive given its unconventional architecture. 3. Resolution Adaptability: Sequencers robust performance across different resolutions without the need for finetuning or architectural adjustments addresses a critical limitation of many current models enhancing its practical applicability. 4. Efficiency in HighResolution Contexts: The paper provides evidence that Sequencer is more memoryefficient and has better throughput at higher resolutions compared to transformerbased models indicating its suitability for highresolution image processing. Weaknesses: 1. Throughput Concerns at Standard Resolution: Despite its advantages at high resolution Sequencer exhibits lower throughput at standard resolutions compared to transformer and convolutional models. This limitation might affect its adoption for applications where realtime processing is critical. 2. Limited Exploration of Internal Mechanisms: The paper could benefit from a deeper exploration of how Sequencers LSTMbased processing differs fundamentally from selfattention or convolutional processing in capturing image features and the implications of these differences for various tasks. 3. Comparative Analysis: While the paper presents competitive performance metrics a more detailed comparison including qualitative assessments and feature visualization against traditional CNNs and ViTs could provide clearer insights into Sequencers advantages and potential use cases. Questions and Suggestions for the Authors: 1. Model Interpretability: Could the authors share insights or additional results on how Sequencer interprets images differently from ViTs or CNNs perhaps via feature visualization or layer activation analysis 2. Throughput Optimization: Are there any strategies or modifications under consideration that could improve Sequencers throughput at standard resolutions without significantly compromising performance or efficiency 3. Extension to Other Tasks: Given Sequencers robust performance in standard image classification and semantic segmentation how well does it adapt to other complex tasks such as object detection or video processing Limitations and Societal Impact: The authors have addressed several limitations such as throughput at standard resolutions and the exploration of internal mechanisms. Still they could further assess the potential biases introduced by training primarily on ImageNet1K and explore the models behavior across diverse datasets to ensure generalizability. Regarding societal impact while the paper outlines potential efficiency gains an analysis of the environmental impact of training Sequencer considering its computational requirements compared to traditional models would be beneficial. Constructive Suggestions for Improvement: 1. Throughput Enhancement: Investigate and propose specific strategies or architectural tweaks that could improve Sequencers processing speed at standard resolutions making it more competitive with existing models in speedsensitive applications. 2. Broader Dataset Evaluation: Consider testing Sequencer on a more diverse set of datasets including those with higher variability or from different domains to assess its generalizability and performance stability. 3. Environmental Impact Analysis: Provide an analysis of Sequencers training and inference energy requirements compared to conventional models to offer insights into its environmental sustainability.", "vfR3gtIFd8Y": "Reviewer 1:  1) Briefly summarize the paper and its contributions: The paper introduces a novel method for forward variable selection in KarhunenLo\u00e8ve (KL) decomposed Gaussian Processes (GPs) particularly for the Bayesian Smoothing Spline ANOVA kernel (BSSANOVA). This is achieved via an ordered nature of basis functions in the KL expansion coupled with fast Gibbs sampling in a fully Bayesian approach. The method aims to effectively manage the high dimensionality challenge that KL decompositions inherently face. It offers competitive accuracies training and inference times for tabular datasets with low feature set dimensionality making it particularly useful for dynamic systems modeling. The approach is validated through comparisons with existing methods like random forest ResNet and other GP methods across SIR toy problems and the Cascaded Tanks benchmark dataset. 2) Thorough assessment of strengths and weaknesses: Strengths:  The paper addresses the crucial problem of high dimensionality in KL decomposed GPs presenting a forward variable selection method that leverages the ordered basis functions effectively.  By incorporating fast Gibbs sampling the proposed method significantly reduces computation time without sacrificing accuracy making it highly efficient for dynamic systems modeling.  The comparative analysis demonstrates the method\u2019s superior performance in static prediction and dynamic systems modeling over conventional methods like RF ResNet and existing GP approaches.  The inclusion of both a toy SIR problem and a realworld Cascaded Tanks benchmark dataset provides comprehensive validation of the method\u2019s applicability and effectiveness. Weaknesses:  The paper focuses primarily on tabular datasets with low feature set dimensionality which might limit its applicability to more complex datasets commonly encountered in other data science domains.  While the method is shown to outperform several existing models the comparisons mostly focus on prediction accuracies and inference times with less emphasis on the interpretability of the model.  The discussion on the methods limitations and potential negative societal impacts is minimal which could be critical for its application in sensitive areas. 3) Questions and suggestions for the authors:  Could the authors elaborate on how the proposed method could be adapted or extended to handle datasets with higher dimensionality or more complex feature interactions  It would be beneficial if the authors could provide more insight into the models interpretability particularly how the variable selection process could influence the understanding of underlying data patterns.  A more detailed discussion on the methods limitations and potential for negative societal impacts if any would offer a more balanced view of its applicability and areas where caution is warranted. 4) Limitations and potential negative societal impact: The paper briefly mentions the investigation of other benchmark dynamic systems and highdimensional feature spaces as future work indicating a limitation in its current form. Additionally while the paper does not address potential negative societal impacts directly it is essential to consider how modeling inaccuracies or biases in dynamic system predictions can have significant consequences especially in healthcare or environmental monitoring applications. It would be prudent for the authors to include a section discussing these aspects focusing on how to mitigate potential risks and ensure the responsible use of the proposed method. ", "vNrSXIFJ9wz": "Peer Review on \"Participant Selection in Vertically Federated Learning Through Mutual Information Estimation\" 1) Summary of the Paper The paper presents a novel approach to address two significant challenges in Vertical Federated Learning (VFL): scalability with increasing participants and the diminishing return on model quality with additional but not equally valuable participants. Inspired by these issues the authors propose a strategy for selecting a subset of participants that maximizes the mutual information (MI) between features and labels thereby preserving data privacy and computational efficiency. They introduce VFMINE a Vertically Federated Mutual Information Estimator leveraging Fagins algorithm for optimizing data movement and computation. Building atop VFMINE they present VFPS a group testingbased framework for participant selection. The proposed methods are demonstrated to significantly enhance the speed of participant selection and maintain or even improve the models quality with fewer participants. Extensive evaluations underline the effectiveness and efficiency of the approach compared to traditional fullparticipation training and other baseline selection methods. 2) Strengths and Weaknesses Assessment Strengths:  Originality: The paper tackles the rarely addressed problem of participant selection in VFL introducing the novel VFMINE and VFPS frameworks. The application of Fagins algorithm and the group testing approach in the context of federated learning are particularly innovative.  Technical Rigor: The authors provide a detailed formulation of the participant selection problem and a thoroughly developed solution through VFMINE and VFPS. Security concerns are rigorously analyzed ensuring the proposed methods adhere to privacypreserving principles inherent to federated learning.  Empirical Validation: Comprehensive experiments showcase the frameworks efficiency and their impact on model quality with various datasets and models. Comparisons to baselines and bruteforce methods offer a strong validation of the proposed approach. Weaknesses:  Generalization Concerns: While the paper provides extensive experimental results the exploration of the methods performance in diverse realworld settings especially with dynamic data distributions and nonstationary environments common in federated learning is limited.  Complexity and Scalability Analysis: The paper could benefit from a deeper discussion on the scalability of VFPS especially in cases with a very high number of participants. Computational overhead and its impact on practical deployments remain somewhat underexplored.  Societal Implications: While the authors briefly mention the potential negative societal impacts a more thorough examination of issues such as bias propagation through participant selection or exclusion is missing. 3) Questions and Suggestions for Authors  Could the authors expand on how VFMINE and VFPS would adapt to dynamically changing federated environments where participant data distributions evolve over time  Given the acceleration and efficiency improvements with VFPS is there a tradeoff in terms of model robustness or vulnerability to adversarial attacks considering fewer participants contribute to the training process  A deeper exploration into the scalability constraints of the proposed methods especially concerning communication overhead and computational demands in larger federated networks would enrich the discussion.  It would be valuable to include considerations or strategies for mitigating potential biases or inequalities that might arise from the participant selection process. 4) Limitations and Suggestions for Improvement The limitation section thoughtfully acknowledges areas for future research such as the need for continuous adaptation in multistage training scenarios and the generalization of the MI estimator. To enhance the paper further:  Incorporate a section that explicitly discusses mitigating strategies for potential bias and fairness issues induced by participant selection.  Explore and discuss the applicability of the proposed methods across different federated learning architectures (beyond VFL) and their potential adaptations.  Detailed analysis comparing the computational complexity and communication requirements of VFPS against existing methods would provide clearer insights into its scalability and practical deployment considerations. Overall this paper contributes significantly to the federated learning field by presenting an innovative approach to participant selection in VFL. With minor expansions on the discussed limitations and additional considerations regarding societal impacts and method scalability this work has the potential to set a foundation for future research in the area.", "pBJe5yu41Pq": "The paper presents an innovative approach to modeling and representing aleatoric uncertainty in Bayesian neural networks (BNNs) particularly in the context of classification problems. Unlike the traditional Gaussian observation models used in regression that allow for explicit representation of aleatoric uncertainty (inherent data randomness) through a noise variance parameter classification problems typically utilize a categorical distribution which lacks a mechanism to express beliefs about such uncertainty. The authors argue that this lack of representation for aleatoric uncertainty in standard likelihoods (such as the softmax likelihood in classification tasks) leads to inadequate fits to training data exaggerated by the application of data augmentation techniques that soften the likelihood resulting in models that are underconfident on the training data.  Contributions:  The introduction of the noisy Dirichlet model as an alternative to cold posteriors explicitly controlling the level of aleatoric uncertainty without the need for tempering.  Theoretical insights into how data augmentation in approximate inference leads to softening of the likelihood resulting in underconfidence and misrepresentation of aleatoric uncertainty.  Empirical evidence showing that modeling aleatoric uncertainty significantly improves the performance of BNNs in classification tasks with a detailed analysis on the effects of data augmentation and posterior tempering on representing this uncertainty.  Strengths:  Theoretical Insights: The paper provides a significant conceptual advancement in understanding and representing aleatoric uncertainty in BNN classification bridging a crucial gap in the literature.  Practical Implications: By introducing the noisy Dirichlet model and analyzing the effects of data augmentation and posterior tempering this work offers practical guidelines for enhancing the performance of BNNs in realworld classification tasks.  Empirical Validation: The rigorous empirical evaluation across different datasets and augmentation policies confirms the theoretical hypotheses and demonstrates the utility of the proposed approach in achieving better model performance and uncertainty representation.  Weaknesses:  Generalization across Diverse Datasets: While the paper includes experiments on CIFAR10 and Tiny ImageNet further validation across a more diverse set of classification tasks and datasets would strengthen the claims.  Analysis of Computational Overheads: The paper lacks a detailed discussion on the computational implications of adopting the noisy Dirichlet model especially when compared to traditional approaches and in the context of largescale datasets.  Indepth Analysis of Parameter Sensitivity: More extensive exploration on the sensitivity of model performance to the hyperparameters of the noisy Dirichlet model like the noise parameter (\u03b1\\xcf\\xb5) could provide deeper insights into the robustness and practical feasibility of this approach.  Questions and Suggestions for the Authors: 1. Generalization Capability: How does the noisy Dirichlet model perform across a wider array of datasets especially those with higher levels of intrinsic noise or label inconsistency 2. Computational Cost: Can the authors provide insights or empirical evaluations on the computational overheads introduced by the noisy Dirichlet model compared to standard BNN classifiers 3. Hyperparameter Sensitivity: Further elaboration on the sensitivity analysis of the models hyperparameters could be beneficial for practical implementations. Are there any guidelines or automated approaches recommended for choosing these parameters  Limitations and Societal Impact: The authors have not discussed the potential negative societal impacts of their work particularly in the context of misclassification risks in sensitive applications (e.g. medical diagnosis criminal justice). Moreover the paper does not address the limitations regarding the scalability of the proposed approach to very large datasets or complex network architectures. A discussion on these aspects would have provided a more comprehensive perspective on the applicability and implications of the research.  Constructive Suggestions for Improvement:  Expand the experimental validation to include datasets with diverse characteristics and complexities to ascertain the generalizability of the proposed approach.  Include a detailed analysis or discussion on the computational costs associated with the noisy Dirichlet model alongside potential strategies to mitigate such costs in practice.  Address the limitation regarding the selection of hyperparameters more thoroughly potentially by exploring automated tuning mechanisms or providing empirical guidelines based on dataset characteristics.  Consider discussing the ethical implications and potential negative societal impacts especially related to misclassification risks in critical applications and suggest mitigation strategies to counteract such issues.", "tfkeJG9yAX": "The paper explores the theoretical limits of neural network capabilities by establishing a framework to gauge how accurately neural networks can approximate a set of functions. The authors develop lower bounds on approximation errors between target function sets and networkgenerated function sets using Lp norms for evaluation. Specifically the paper proves a general lower bound that depends on the packing number of the target function set the range of these functions and the fatshattering dimension of the neural network function set. It then applies this theoretical result to dissect how well feedforward neural networks with piecewisepolynomial activations can approximate two types of function sets: H\u00f6lder balls and multivariate monotonic functions. Findings demonstrate that although ReLUactivated networks approximation rates for H\u00f6lder balls in Lp norms parallel those in sup norms the story diverges for monotonic functions where approximation depends on the Lp norm chosen. Strengths: 1. Theoretical Depth and Rigor: The paper presents a solid theoretical contribution by proving a generic lower bound on the approximation error applicable across different types of function sets and neural networks. The formula developed is a versatile tool for understanding the intrinsic limits of neural networks approximation capabilities. 2. Relevance and Novelty: Addressing an open question in the field the paper successfully extends the understanding of neural network approximation beyond sup norms to Lp norms which are more indicative of practical performance in many realworld scenarios. 3. Clear Application: By applying the newly developed lower bound to specific cases (H\u00f6lder balls and monotonic functions) the authors provide a concrete demonstration of how their theoretical contributions can be used to derive new insights into known problems bridging the gap between theory and potential applications. Weaknesses: 1. Complexity and Accessibility: The paper is mathematically dense and may be challenging for readers without a strong background in the specific mathematical tools and concepts employed. This could limit its accessibility to a broader audience within the neural network research community. 2. Empirical Validation: Despite the strength of the theoretical analysis the paper does not present any empirical validation or experimentation to showcase how the theoretical bounds hold up in practical scenarios. Including such validations could significantly enhance the impact and credibility of the findings. 3. Comparison with Existing Methods: While addressing and solving an open question the paper could benefit from a more detailed comparison with existing approaches specifically discussing how this new method advances or diverges from the current stateoftheart. Questions and Suggestions for the Authors: 1. Clarification on Assumptions: Can the authors further elucidate the assumptions made regarding the function sets and neural network configurations Understanding the boundaries of these assumptions is crucial for grasping the generalizability of the results. 2. Potential for Empirical Validation: Are there specific plans or potential methodologies the authors contemplate for empirically validating the theoretical lower bounds established in this study 3. Extension to Other Norms or Function Sets: Could the authors speculate on how their approach might be adapted or extended to other norms or function sets not covered in this paper Limitations and Societal Impact: The paper could improve by discussing potential limitations more thoroughly particularly those related to the assumptions ingrained in the mathematical models used. For example the practical applicability of the results might be limited by the idealized conditions under which the theoretical bounds were derived. Concerning societal impact while the paper primarily focuses on theoretical advancements elaborating on how these findings might influence the design and deployment of neural networks in sensitive applications (e.g. healthcare autonomous vehicles) could be valuable. It would be beneficial for future versions of this work to consider the broader implications of their findings including ethical considerations around the use of increasingly powerful neural networks.", "oDoj_LKI3JZ": "Peer Review for \"An MCMCBased Approach to Bayesian Sparse Gaussian Processes Regression\" 1) Brief Summary This paper introduces a novel algorithm for sparse Gaussian process regression that significantly enhances sampling efficiency by employing Markov chain Monte Carlo (MCMC) methods to sample from the hyperparameter posterior within a variational inducing point framework. The work aims to address the limitations of the TypeII maximum likelihood (MLII) approach in sparse Gaussian processes (GPs) notably its inability to account for hyperparameter uncertainty which can lead to biased estimates and an underestimation of predictive uncertainty. The proposed method avoids sampling inducing points improving efficiency especially in cases with Gaussian likelihoods. This advancement is substantiated through comprehensive computational analysis and comparison with natural baselines including stochastic variational GPs (SVGPs). 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper successfully leverages MCMC within a sparse variational GP framework presenting a novel approach to bypass the need for sampling inducing points which significantly improves efficiency.  Comprehensive Analysis: Through extensive computational analysis and comparisons with established methods like SVGPs the paper provides a solid empirical basis to support its claims.  Theoretical Contribution: The paper extends the sparse variational formulation to incorporate Bayesian treatment of hyperparameters effectively a noteworthy theoretical advancement. Weaknesses:  Generalization Concerns: While the paper focuses on Gaussian likelihoods it is not entirely clear how well the proposed method generalizes to nonGaussian likelihoods. More thorough analysis or discussion on this aspect could strengthen the paper.  Complexity and Accessibility: The papers complexity particularly around the derivation of the doubly collapsed bound and implementation details might limit its accessibility to a broader audience. Simplifying these explanations or providing additional resources could be beneficial.  Limited Discussion on Limitations: While the paper presents a novel approach it provides only a limited discussion on the potential limitations or scenarios where the proposed method might not perform optimally. 3) Questions and Suggestions for the Authors  Generalizability to NonGaussian Likelihoods: Could the authors elaborate on how the proposed method might perform or be adapted for scenarios involving nonGaussian likelihoods  Comparison with StateoftheArt Methods: Since the paper establishes a comparison with methods like SVGPs including comparisons with other stateoftheart Bayesian GP methods could provide a clearer picture of the proposed methods efficacy.  Impact of Hyperparameter Sampling Efficiency: The paper highlights improved sampling efficiency. Could the authors provide more insight into how this efficiency contributes to overall performance improvements especially in realworld applications  Implementation Details: Given the complexity of the proposed method providing further details about the implementation possibly through supplementary materials like pseudocode or a link to an opensource implementation would be greatly appreciated. 4) Limitations and Societal Impact The paper does not fully discuss the potential limitations or the negative societal impact of the proposed work. Addressing these aspects could improve the overall quality and relevance of the research. For instance:  Robustness to Model Misspecification: Discussion on the methods robustness to model misspecification and how it might perform in the presence of significant noise or outliers would be valuable.  Computational Resources: Given the computational demands of MCMC discussing the computational efficiency and potential scalability issues especially for very large datasets would be important.  Societal Impact: An exploration of the societal implications especially considering applications in critical domains (e.g. healthcare or finance) where overconfidence in predictive uncertainty could lead to negative consequences would be prudent. Suggestions for Improvement:  Enhanced Discussion on Generalization and Limitations: Broadening the discussion to cover nonGaussian likelihoods potential scalability issues and the methods robustness would offer a more comprehensive view of its applicability.  Accessibility: Providing additional resources for implementation and simplifying complex theoretical discussions could make the paper more accessible and appealing to a broader audience.  Impact Analysis: Incorporating a section on potential negative societal impacts and ethical considerations related to the application of the proposed method could make the paper more wellrounded and pertinent to current research norms.", "yW5zeRSFdZ": " Review of \"Suppressing Outliers in Transformer Models through Gamma Migration and TokenWise Clipping\"  Summary This paper introduces a novel outlier suppression framework aimed at enhancing the efficiency of Transformerbased NLP models through the process of quantization making them suitable for deployment on resourcelimited devices. The authors identify that the scaling parameter \u03b3 in LayerNorm acts as an amplifier for outliers which hampers quantization performance. By migrating \u03b3 into subsequent modules (Gamma Migration) and implementing TokenWise Clipping to efficiently find a suitable clipping range the framework successfully suppresses outliers without additional computational costs. This approach is validated through extensive experiments across various models (BERT RoBERTa BART) and tasks (text classification question answering summarization) demonstrating significant improvements in lowbit quantization performance even achieving fullprecision levels in some cases.  Strengths 1. Novelty and Contribution: The paper tackles the overlooked aspect of outlier influence in transformer quantization. The concepts of Gamma Migration and TokenWise Clipping are innovative and directly address the root causes of the quantization issues substantially contributing to the field. 2. Extensive Validation: The comprehensive set of experiments across multiple models and tasks effectively demonstrates the applicability and effectiveness of the proposed method. This thorough validation is a significant strength of the paper. 3. Practical Relevance: By enabling more efficient deployment of large NLP models on devices with limited resources without sacrificing performance this work tackles a crucial practical challenge in the deployment of AI technologies. 4. OpenSource Contribution: The authors have shared their code which is excellent for fostering further research and allowing for easy replication and validation of the results.  Weaknesses 1. Comparison with StateoftheArt (SoTA): While the paper provides extensive experimental results a more detailed comparison with the latest SoTA methods especially those emerging in parallel to this work could further solidify the frameworks positioning. 2. Diversity in Datasets and Languages: The experiments mainly focus on wellknown benchmarks and tasks. Including a wider range of datasets possibly from different languages or less commonly addressed NLP tasks could enhance the robustness of the findings. 3. Analysis of Computational Efficiency: Despite stating no extra computational burden the paper lacks a detailed analysis of the computational efficiency (time and resources required) compared to baseline methods which could be critical for deployment in resourceconstrained environments.  Questions and Suggestions 1. Broader Applicability: Could the authors comment on the potential applicability of their outlier suppression framework to other domains such as computer vision where quantization is also critical 2. Impact on Different Model Sizes: The paper predominantly showcases results on large models. It would be interesting to see how the framework performs with smaller or even larger transformer models including the latest generative models like GPT3. 3. Addressing Bias and Fairness: Given the manipulation of model parameters an assessment of how these changes might affect the bias and fairness of model predictions would be beneficial.  Limitations and Societal Impact The paper briefly discusses open problems but could further elucidate on limitations particularly how the outlier suppression might interact with model interpretability and robustness to adversarial attacks. While reducing resource usage is positive there should also be consideration on whether the altered quantization could inadvertently introduce or amplify biases in model outputs. Constructive suggestions include providing guidance on monitoring for and mitigating such potential issues along with encouraging the exploration of these aspects in future work. Overall the paper presents a significant advancement in the efficient deployment of NLP models with the potential for broad impact across various AI applications.", "ttC9p-CtYT": "Review of \"Enhancing Normalized Advantage Functions for Continuous Deep Reinforcement Learning\" 1. Summary of the Paper This paper delves into the realm of continuous deep reinforcement learning by focusing on the improvement of Normalized Advantage Functions (NAF). It presents a novel class of quadratic functions developed from the principles of NAF which demonstrates superior approximation capabilities for the Qfunction concerning action variables in a range of continuous reinforcement learning problems. These problems originate from the discretization of optimal control problems characterized by affine control ordinary differential equations and quadratic control cost functionals. Theoretical proofs back the suitable approximation properties of the proposed function family furthering the capabilities of the NAF algorithm. New algorithms are proposed based on these theoretical insights and their effectiveness is validated through experiments with various control tasks. 2. Assessment of Strengths and Weaknesses Strengths:  Theoretical Contribution: The paper introduces significant theoretical advancements with the proposition of a new family of quadratic functions addressing a key challenge in the application of NAF for continuous action spaces. The theorems provided offer a rigorous foundation for the practical improvements suggested.  Practical Implications: By applying the theoretical insights to propose new algorithms that enhance NAF the paper directly contributes to solving more complex continuous reinforcement learning problems effectively. The experimental results demonstrating the superiority of the proposed algorithms in several control tasks underscore the practical relevance of the research.  Comprehensive Evaluation: The thorough experimental setup comparing the proposed algorithms not only to NAF but also to a wellknown algorithm like DDPG across multiple tasks provides a solid basis for evaluating the improvements made. Weaknesses:  Assumption Specificity: The reliance on problems that can be described by affine control ordinary differential equations and quadratic control cost functionals could limit the generality of the approach. This specificity might restrict the application of the proposed enhancements in broader classes of reinforcement learning problems.  Experimental Details: While the experiments broadly demonstrate the efficiency of the proposed improvements the paper could benefit from more detailed discussions on the experimental settings such as the choice of hyperparameters and their impacts on the overall performance.  Comparison Depth: Expanding the comparison to include more stateoftheart algorithms in continuous deep reinforcement learning could have provided a clearer picture of where the proposed algorithms stand in relation to the current research frontier. 3. Questions and Suggestions for the Authors  Generalization to Other Problems: Could the authors elaborate on potential strategies or future work that might extend the applicability of their approach to reinforcement learning problems not fitting the specific conditions (affine control and quadratic cost functionals) discussed  Algorithm Adaptability: How adaptable are the proposed algorithms to varying problem sizes and complexities Additional insights into scalability would be valuable.  Hyperparameter Sensitivity: It would be beneficial if the authors could discuss the sensitivity of the proposed algorithms to hyperparameter settings and provide guidelines or best practices for tuning in different scenarios.  Limitations and Societal Impact: The authors do not extensively discuss the limitations or the potential negative societal impacts of their work. While the specificity of the application domain may inherently limit broader societal implications a deeper reflection on this aspect could enhance the papers perspective. For instance considering the growing concern around the energy consumption of training large models in deep learning an evaluation of the computational efficiency and environmental impact of the proposed algorithms would be valuable. Additionally exploring potential misuse scenarios especially given the relevance of control problems to various sensitive applications (e.g. autonomous systems) could foster a more responsible approach to research dissemination in this area. Constructive Suggestions for Improvement:  The authors should consider including a section discussing the broader implications and limitations of their work more extensively possibly incorporating considerations around computational efficiency and environmental impact.  A deeper exploration into the adaptability of the proposed algorithms to diverse problem settings and a detailed discussion on hyperparameter sensitivity and tuning strategies would strengthen the practical utility of the research. Overall the paper makes a noteworthy contribution to the field of continuous deep reinforcement learning by addressing a significant challenge in the application of NAF algorithms and providing a solid theoretical foundation accompanied by practical algorithmic advancements.", "zbt3VmTsRIj": "Review of \"A Novel SingleLoop Algorithm for Inverse Reinforcement Learning with Superior Demonstration Efficiency and Policy Transferability\" 1. Summary of the Paper This paper addresses a critical challenge in inverse reinforcement learning (IRL): the computational complexity of traditional nestedloop algorithms especially in highdimensional environments. The authors argue that existing methods either compromise on reward estimation accuracy for computational efficiency or suffer from unstable performance due to sensitive hyperparameters. To tackle these issues they propose a novel singleloop algorithm for IRL that efficiently recovers accurate reward functions without sacrificing computational efficiency. Their method alternates between policy update steps and stochastic gradient steps for reward optimization offering theoretical convergence guarantees. The algorithms effectiveness is demonstrated through superior performance in both policy estimation and reward recovery on robotics control tasks within MuJoCo including success in transfer learning scenarios. 2. Strengths and Weaknesses Strengths:  Novel Algorithm Design: The singleloop structure with alternating policy and reward optimization steps is a significant departure from the nestedloop architecture of existing IRL algorithms. This approach addresses the computational efficiency problem while ensuring the accuracy of reward estimations.  Theoretical Contribution: The paper provides strong theoretical underpinnings for their algorithm including finitetime convergence guarantees. This is a strong addition to the literature where many algorithms lack rigorous theoretical analysis.  Empirical Validation: Extensive experiments demonstrate the algorithms superior performance against stateoftheart IRL methods particularly in highdimensional control tasks and transfer learning scenarios. This empirical evidence robustly supports the authors claims about the algorithms effectiveness. Weaknesses:  Limited Exploration of Reward Parameterization: While the paper discusses linear and nonlinear parameterizations of the reward function there might be a broader spectrum of reward structures (e.g. hierarchical or composite rewards) that could impact the algorithms performance.  Concerns on Scalability and Generality: Despite promising results on robotics control tasks in MuJoCo it remains unclear how the proposed algorithm would perform in vastly different domains or at scale especially in environments with sparse rewards or highly complex dynamics.  Lack of Comparative Analysis on Computational Efficiency: While the algorithm is designed for efficiency the paper does not provide a detailed comparative analysis of computational resources against other methods which would be crucial to validate the efficiency claims. 3. Questions and Suggestions  Question on Reward Parameterization: Could the authors elaborate on how the algorithm might adapt or need modifications to cater to more complex or structured forms of reward functions  Suggestion for Further Validation: It would be beneficial to test the algorithm in a wider variety of environments especially those with sparse rewards or nonMuJoCo tasks to evaluate its generality and scalability.  Suggestion on Computational Analysis: Including a detailed comparative analysis on the computational efficiency (e.g. time complexity computational resources and convergence speed) against benchmark algorithms could strengthen the papers claims on efficiency. 4. Limitations and Societal Impact The authors mention the potential negative societal impacts of using IRL with biased or unsafe expert demonstrations which is appreciated. However the discussion on limitations primarily focuses on the scope of empirical validation and the online training requirement. Expanding on potential limitations related to the scalability of the algorithm its applicability across different domains and the need for robustness against adversarial examples in reward recovery could improve the paper. Suggestions for Improvement:  Explore Robustness: Investigating the algorithms robustness to adversarial manipulations in the demonstrations could be crucial especially for sensitive applications.  Detailed Efficiency Analysis: Providing a more granular analysis of computational efficiency potentially through ablation studies or comparisons under controlled computational resources would be valuable.  Consideration of Ethical Safeguards: Given the potential negative impacts it would be beneficial if future work could incorporate mechanisms to detect and mitigate biases in the expert dataset automatically. Overall the paper introduces a promising approach to IRL with significant theoretical and empirical contributions. Addressing the limitations and suggestions could further strengthen the impact and applicability of the proposed algorithm.", "m8YYs8nJF3T": " Review of \"The Sliced Wasserstein Process: A New Approach to Analyzing Wasserstein Distances in High Dimensions\"  Summary of the Paper This paper introduces the Sliced Wasserstein Process a novel stochastic process based on the empirical Wasserstein distance measured across all onedimensional projections of empirical probability measures. Addressing the computational and statistical complexities associated with the highdimensional Wasserstein distances this work presents a unified statistical framework capable of analyzing Wasserstein distances derived from onedimensional projections. By proving a uniform distributional limit theorem for this process the authors offer a consolidated method for studying sample complexity and distributional limits across various Wasserstein distance modifications including averaging random sampling and maximizing methods. The paper extends existing knowhow by deriving distributional limits for several variations of the sliced Wasserstein distance where previous studies fell short.  Strengths of the Paper 1. Innovative Concept: The introduction of the Sliced Wasserstein Process stands as a significant advancement in handling highdimensional data for Wasserstein distance computations. This concept tackles the \"curse of dimensionality\" and computational burdens effectively. 2. Unified Framework: Offering a unified analytical framework to study different variants of sliced Wasserstein distances represents a substantial contribution to statistical methodology within machine learning with broad implications for transfer learning generative modeling and other domains. 3. Rigorous Mathematical Analysis: The mathematical rigor in proving the uniform distributional limit theorem and subsequent theoretical results exemplify highquality research. The clear linkage between theoretical advancements and practical implications underscores the papers strength. 4. Empirical Validation: Through simulation studies the paper not only validates the theoretical findings but also demonstrates the applicability and reliability of the proposed process in practical scenarios enhancing the contributions credibility.  Weaknesses of the Paper 1. Assumptions: The reliance on certain assumptions like compact support and the connectivity of supports within Rd could limit the general applicability of the results. Although necessary for theoretical tractability these assumptions might not always hold in realworld datasets. 2. Complexity for Practitioners: The high level of mathematical sophistication while a testament to the papers rigor could pose accessibility issues for practitioners in the field with less mathematical background potentially hindering wider adoption. 3. Limited Discussion on Societal Impact: The paper lacks a thorough discussion regarding the limitations and potential negative societal impacts of the proposed methods. While this is common in theoretical works including such considerations could enrich the papers perspective especially with the growing concern over ethical AI.  Questions and Suggestions for the Authors 1. Extension to NonCompact Supports: Could the authors discuss potential strategies or future directions for extending the Sliced Wasserstein Process to probability distributions without compact supports Any insights into handling unbounded supports would be valuable. 2. Accessibility for Practitioners: It would be beneficial if the authors could provide a section or supplementary material aimed at translating the core contributions into more accessible terms for practitioners. This might include practical guidelines or software implementations. 3. Societal Impact Discussion: Considering the increasing scrutiny on the ethical implications of machine learning research the authors are encouraged to add a section discussing potential limitations and negative societal impacts of their work. This could involve misuses of the proposed methods or inaccuracies in certain applications.  Limitations and Societal Impact While the authors have laid a strong foundation by discussing the assumptions required for their theoretical framework a deeper exploration of the limitations particularly those affecting realworld applicability would strengthen the paper. Furthermore the absence of a dedicated discussion on the societal impact of the proposed methods is a noticeable gap. Constructively the authors could explore scenarios where the misuse of such distance measures could lead to biased outcomes or privacy concerns offering suggestions for mitigating these potential issues.", "lblv6NGI7un": " Review of \"Efficient Graph Similarity Computation via Alignment Regularization (ERIC)\"  1) Summary of the Paper The paper introduces a novel framework named ERIC for the task of graph similarity computation (GSC) based on graph edit distance (GED) approximation. Unlike existing stateoftheart methods that rely on expensive nodelevel matching modules for capturing finegrained interactions between pairwise graphs ERIC proposes a simpler and computationally efficient approach. By introducing an Alignment Regularization (AReg) term during the training stage ERIC enforces a nodegraph correspondence constraint on the GNN encoder eliminating the need for nodetonode matching during inference. This results in faster inference times without sacrificing accuracy. Additionally ERIC incorporates a multiscale GED discriminator to further enhance the representational capability of the learned features. Empirical results on realworld datasets demonstrate that ERIC outperforms baseline methods in terms of efficiency effectiveness and transferability.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of Alignment Regularization (AReg) as a means to avoid expensive nodelevel interactions is a significant contribution. This approach cleverly balances the need for capturing finegrained interactions while maintaining computational efficiency.  Performance: ERIC demonstrates superior performance over existing GSC methods across multiple realworld datasets showcasing its effectiveness in achieving highquality graph similarity computations.  Efficiency: The method significantly reduces both computational cost and inference time by eliminating the matching model during inference addressing a critical challenge in graph similarity computations.  Transferability: The modelagnostic nature of AReg and the proposed frameworks ability to be applied to other GNNbased models enhance its applicability and potential impact across various tasks and domains. Weaknesses:  Generalizability to Different Graph Types: While ERIC shows impressive performance on undirected unweighted graphs its effectiveness on directed weighted or attributed graphs is not explicitly discussed.  Scalability Concerns: Despite improvements in computational efficiency the scalability of ERIC to extremely large graph databases remains unclear especially in comparison to approximative or heuristicbased methods.  Limited Evaluation of Societal Impact: Although ERIC is positioned as computationally efficient the paper lacks a detailed discussion on the broader societal implications including potential misuse or biases inherent in the models predictions.  3) Questions and Suggestions for the Authors  Generalizability: Can the authors provide insights or empirical evidence on how ERIC performs on directed weighted or highly attributed graphs This would help in understanding the methods applicability to a wider range of GSC tasks.  Scalability Analysis: It would be beneficial to see a scalability analysis that benchmarks ERIC against stateoftheart methods on very large graph datasets or databases. Understanding its performance in these scenarios would clarify its practical utility.  ARegs Transferability: While the paper mentions the transferability of AReg to other models a more thorough exploration or guidelines on integrating AReg with different architectures could drive adoption and innovation.  Societal Impact: Given the potential of GSC tasks across various domains including sensitive areas like drug discovery or social network analysis a discussion on ethical considerations potential biases and misuse prevention would be valuable.  4) Limitations and Societal Impact The paper does not delve deeply into potential limitations related to the versatility of ERIC across diverse graph types and its scalability to massive graph databases. Additionally while emphasizing computational efficiency the discussion on the broader societal impact including both positive implications and potential risks is missing. It is recommended that the authors provide a more nuanced view of how ERIC could interact with various application domains and propose mechanisms to prevent misuse ensure fairness and mitigate biases in the models predictions.", "wUUutywJY6": "Review of \"SoLar: Optimal Transportbased Framework for LongTailed PartialLabel Learning\" 1) Summary This paper introduces a novel framework named SoLar (Sinkhorn Label Refinery) to address the challenges of longtailed distributions in partiallabel learning (PLL). PLL is where each sample is associated with a set of candidate labels with the true label hidden within. Conventional PLL methods which assume classbalanced scenarios underperform with longtailed distributions due to biases towards more common classes. SoLar employs an optimal transportbased approach to disambiguate labels ensuring the pseudolabel distribution matches the actual class prior distribution. Additionally it proposes methods for estimating class prior distributions within the PLL context which inherently comes with label ambiguity. The paper provides comprehensive evaluations on standard benchmarks where SoLar outperforms stateoftheart PLL methods particularly improving tailclass accuracy. 2) Assessment Strengths:  Innovation: SoLars incorporation of an optimal transport objective to match pseudolabels to class prior distributions is novel in the context of PLL. This approach notably improves the models ability to identify true labels from candidate sets especially for underrepresented classes.  Estimation Technique: The papers strategy for class prior estimation in the PLL setup is both innovative and practical. The movingaverage technique and reliable sample selection mechanism overcome the common issues of inaccurate initial predictions and bias towards dominant classes.  Experimental Validation: The extensive experiments convincingly demonstrate the effectiveness of SoLar across multiple datasets with varying levels of imbalance and ambiguity. The improvement in accuracy for tail classes is significant and a compelling argument for SoLars utility. Weaknesses:  Theoretical Contribution: While the empirical results are strong the paper could enhance its theoretical foundation. Specifically a deeper exploration of the conditions under which SoLars optimal transport approach guarantees improvements would be beneficial.  Generalizability: The primary focus is on image classification tasks. Additional experiments in domains like text or audio could further validate the generality of SoLar\u2019s approach.  Complexity Discussion: There is a lack of discussion on the computational complexity of SoLar especially concerning the Sinkhorn iteration process. This information is crucial for understanding the scalability of the proposed method. 3) Questions and Suggestions  Could the authors detail the impact of hyperparameter settings especially \u03bb in the SinkhornKnopp algorithm on the performance and convergence of SoLar  Considering SoLars apparent compatibility with longtailed learning (LTL) techniques like logit adjustment could future work explore a more integrated approach that combines SoLars label disambiguation with LTL from the outset  It would be instructive to explore how SoLar performs in scenarios with extremely sparse tail classes or when the ambiguity in candidate label sets is significantly high. Can SoLar maintain its performance edge under these conditions  A deeper dive into the failure cases of SoLar especially where it might underperform compared to baseline models could offer insights into potential improvements or limitations of the approach. 4) Limitations and Societal Impact  The authors have not thoroughly discussed the computational requirements and scalability of the SoLar framework. For realworld applications especially in resourceconstrained settings these aspects are critical. Suggesting optimizations or efficiency improvements could make the paper stronger.  Regarding societal impact the paper does not delve into potential negative consequences. Given the focus on improving performance on underrepresented classes a discussion on how such technical advancements could inadvertently impact the representation or perpetuate biases would add depth to the paper. A call for responsible use of such technologies in diverse and fair applications would be a constructive addition. Overall this paper contributes significantly to the PLL domain particularly addressing the longtailed distribution challenge. With some enhancements in theory generalizability and discussion on computational aspects and societal impacts it could have a broader and more profound influence.", "y8FN4dHdxOE": "Review of \"MultiMode Tensor Clustering via Fused Orthogonal Alternating Least Squares (FusedOrthALS)\" 1) Summary This paper presents a method for multimode tensor clustering termed Fused Orthogonal Alternating Least Squares (FusedOrthALS). This technique is designed to simultaneously factorize and cluster highdimensional tensors by leveraging a fused version of the ALS algorithm. Main contributions include the establishment of statistical convergence rates for the recovery and clustering of data with underlying lowrank CP decomposition structure even in the presence of noise. The authors demonstrate the efficacy and computational efficiency of FusedOrthALS through simulations and application on real datasets. Their method aims to fill the gap in clustering highorder tensors where conventional matrix biclustering methods fall short due to the complexity of tensor structure. 2) Assessment Strengths:  Originality: The paper addresses a current gap in tensor clustering methods by extending the applicability of the ALS algorithm through fusion and orthogonality which is novel in this context.  Theoretical Contribution: It establishes statistical convergence rates and clustering consistency providing a strong theoretical foundation alongside practical implementation.  Versatility: Demonstrated effectiveness in both synthetic and realworld scenarios showcases the methods versatility and potential applicability across various fields. Weaknesses:  Assumption Transparency: While the paper does highlight several assumptions (e.g. tensor structure noise model initialization criteria) a clearer discussion on the practical implications and limitations of these assumptions would further strengthen the paper.  Comparison with Existing Methods: Although the paper compares FusedOrthALS with other methods a more detailed analysis particularly on how these methods fail on specific tasks where FusedOrthALS succeeds would provide a deeper understanding of FusedOrthALSs advantages.  Scalability Discussion: The computational complexity is touched upon but a more thorough investigation into scalability especially with very large tensors would be beneficial for understanding practical limitations. 3) Questions and Suggestions for the Authors:  Rigorous Comparison: Could the authors provide a more detailed comparison with existing tensor clustering methods specifically highlighting scenarios where FusedOrthALS significantly outperforms others  Impact of Assumptions: How sensitive is FusedOrthALS to its underlying assumptions Perhaps a sensitivity analysis could illuminate the robustness of the method under varying conditions.  Extension to Higher Order Tensors: The paper mentions order three tensors how straightforward is the extension to higherorder tensors Any potential challenges that might arise  Scalability and Performance: Could further details be provided on the algorithm\u2019s scalability Especially in terms of memory requirements and computational time as the dimensionality of the data increases significantly. 4) Limitations and Societal Impact: The authors claim no foreseeable negative societal impacts which seems reasonable given the methods nature. However discussing potential misuse in sensitive areas (e.g. surveillance) could be insightful. Regarding limitations the authors could further explore the implications of their assumptions on practical applicability especially in lesscontrolled realworld scenarios. A constructive suggestion for improvement would be to investigate adaptive techniques for parameter selection to enhance robustness and scalability especially for tensors of varying sizes and complexity. Overall the paper presents a significant step forward in the field of tensor clustering with strong theoretical contributions and promising results on both synthetic and real datasets. Addressing the highlighted questions and suggestions could further enhance the papers relevance and applicability.", "lHy09zPewmD": " Peer Review for \"A Generalization of Bandits with Knapsacks for NonMonotonic Resource Utilization\"  1) Brief Summary The paper introduces a significant extension to the Bandits with Knapsacks (BwK) problem termed as nonmonotonic resource utilization in sequential decisionmaking under uncertainty. Unlike traditional BwK problems where resource budgets only deplete this generalization allows resources to either increase decrease or remain constant over time better modeling scenarios like dynamic pricing with inventory replenishments. The authors propose two main contributions: a Markov decision process (MDP) policy with constant regret and a learning algorithm yielding logarithmic regret both benchmarked against a linear programming (LP) relaxation when the true outcome distributions are known and unknown respectively. Furthermore the paper provides a reduction from the classical BwK model to this new generalization proving that the proposed regret bounds are competitive with existing literature.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The generalization to allow nonmonotonic resource utilization is a significant theoretical advancement that makes the BwK model more applicable to realworld problems involving resource replenishment.  Solid Theoretical Underpinning: The development of the MDP policy with constant regret and a learning algorithm with logarithmic regret against an LP relaxation is rigorous and grounded in solid mathematical reasoning.  Practical Relevance: This generalization directly impacts applications like dynamic pricing inventory management and ad placements where resources can be replenished or renewed significantly increasing the models applicability. Weaknesses:  Model Complexity: The introduction of nonmonotonic resource utilization increases the models complexity potentially making practical implementations more challenging.  Assumptions Limitations: While the assumptions (e.g. initial budget limits positive gap between best and secondbest arms) aid in theoretical tractability they might limit the generalizations applicability in scenarios where such conditions are not met or are unrealistic.  Experimental Validation: The paper lacks empirical evaluations of the proposed algorithms. Realworld data experiments or simulations would strengthen the paper by validating theoretical results and demonstrating practical effectiveness.  3) Questions and Suggestions for the Authors  Model Assumptions: Can the authors discuss the implications of the models assumptions in more depth particularly regarding their impact on the generalizations applicability to a broader range of realworld problems  Computational Complexity: Given the increased complexity introduced by nonmonotonic resource utilization how do the computational requirements of the proposed algorithms compare to classical BwK solutions  Empirical Evaluation: Would it be possible to include experiments using realworld datasets or simulations to empirically validate the theoretical advances Such validation could significantly increase the papers impact.  Comparison with Existing Work: While the paper situates itself within the existing literature a more detailed comparison especially with respect to performance and applicability could provide clearer insights into the proposed generalizations benefits.  Limitations and Constructive Suggestions The paper does not thoroughly address the practical implementation challenges or the empirical validation of the proposed algorithms. Including simulations or realworld data experiments would greatly enhance the papers contribution by demonstrating practical applicability and effectiveness. Further a more detailed discussion on the limitations regarding the models assumptions and how they might be relaxed or addressed in future work could provide a clearer path forward for the research community. Lastly elaborating on the societal impacts particularly in scenarios where resource utilization is crucial (e.g. healthcare logistics) could underscore the importance of this work. Overall the paper makes a significant theoretical contribution to the field of sequential decisionmaking under uncertainty. The authors are encouraged to address these suggestions to strengthen the papers practical relevance and applicability.", "lmmKGi7zXn": "Review of \"Leveraging InfiniteWidth Autoencoders in Recommendation Systems: A Novel Data Distillation Framework (\\xe2\\x88\\x9eAE  DISTILLCF)\" 1. Summary: This paper introduces two novel contributions to the field of recommendation systems: the \\xe2\\x88\\x9eAE an infinitewidth autoencoder and DISTILLCF a data distillation framework for creating compact highfidelity data summaries. The \\xe2\\x88\\x9eAE model utilizes the Neural Tangent Kernel (NTK) to operate with infinitely wide bottleneck layers enabling efficient training and inference with a single hyperparameter and a closedform solution. DISTILLCF leverages the \\xe2\\x88\\x9eAE model within a bilevel optimization framework to synthesize small data summaries from massive useritem interaction datasets thus enhancing model scalability and efficiency while addressing privacy and environmental concerns. The authors demonstrate that these techniques significantly outperform stateoftheart models with DISTILLCF synthesized data summaries enabling \\xe2\\x88\\x9eAE to achieve nearoriginal performance with only a fraction (as little as 0.1) of the dataset size. 2. Assessment: Strengths:  Innovativeness: The paper presents a novel use of infinitewidth neural networks in recommendation systems particularly through the development of an autoencoder that simplifies optimization and enhances expressiveness. The concept of using data distillation through DISTILLCF to compress datasets without significant loss in model performance is also notable.  Performance: The empirical results convincingly show that \\xe2\\x88\\x9eAE outperforms complex stateoftheart models across various datasets and metrics. Further DISTILLCF\u2019s capability to produce small yet effective data summaries showcases potential for considerable improvements in model scalability and efficiency.  Practical Relevance: Addressing the scalability challenges and environmental impact of training largescale recommendation models is highly relevant. In addition the paper contributes to the discourse around the quantity vs. quality of data suggesting a shift towards a more datacentric approach in AI. Weaknesses:  Generalizability of DISTILLCF: While DISTILLCF shows promise its dependence on \\xe2\\x88\\x9eAE in the inner optimization loop might restrict its applicability with other models. Exploring DISTILLCF\u2019s compatibility with a broader range of recommendation models could enhance its utility.  Complexity and Implementation Details: The paper could have elaborated more on the practical complexities of deploying \\xe2\\x88\\x9eAE and DISTILLCF including any significant technical challenges and potential solutions. More detailed discussions on implementation aspects particularly for DISTILLCF\u2019s multistep Gumbelsampling would benefit practitioners. 3. Questions and Suggestions:  Extend DISTILLCFs Applicability: Could the authors explore adapting DISTILLCF to work with recommendation models other than \\xe2\\x88\\x9eAE to assess its flexibility and utility across different approaches  Detailed Complexity Analysis: A more granular analysis of computational and memory complexities particularly for DISTILLCF would aid in understanding its scalability and practical deployment considerations.  Robustness to Diverse Data Conditions: It would be insightful to see experiments on datasets with varying levels of noise sparsity and heterogeneity to evaluate the robustness of \\xe2\\x88\\x9eAE and DISTILLCF under diverse realworld conditions. 4. Limitations: The authors have made an effort to address the limitations and potential negative societal impacts notably through DISTILLCF\u2019s design that aims to reduce training data size thus lowering computational resources and environmental impact. However there could be a more explicit discussion on the tradeoffs involved in distilling data summaries especially concerning model bias and fairness. Constructively the paper could explore mechanisms to ensure the distilled data summaries do not amplify existing biases or introduce new ones. In conclusion this paper presents significant advancements in the recommendation systems domain through \\xe2\\x88\\x9eAE and DISTILLCF. The proposed techniques not only push the envelope in model performance but also contribute valuable insights towards scalable dataefficient and environmentally conscientious AI practices. With further exploration on its generalizability and practical deployment considerations the work has the potential to set a new standard in the field.", "lSqaDG4dvdt": "Review of \"EZNAS: Evolutionarily Generated ZeroCost Neural Architecture Scoring Metric\" 1) Briefly summarize the paper and its contributions The paper introduces EZNAS a genetic programming framework aimed at automating the discovery of zerocost proxies for neural architecture scoring. It leverages a novel approach to evolve ZeroCost Neural Architecture Scoring Metrics (ZCNASMs) that are generalizable interpretable and efficient. By using expression trees for the evolutionary search the authors manage to discover an interpretable and generalizable zerocost proxy EZNASA which demonstrates stateoftheart scoreaccuracy correlation across multiple datasets and neural architecture search spaces including NASBench201 and Network Design Spaces (NDS). The authors highlight the efficiency of their method in discovering highquality ZCNASMs in terms of computational time and carbon footprint reduction as compared to traditional NAS methods. 2) Thorough assessment of the strengths and weaknesses Strengths:  The paper addresses a significant problem in the realm of Neural Architecture Search (NAS) by reducing the carbon footprint associated with evaluating multiple architectures through traditional NAS methods.  The approach towards discovering ZCNASMs is novel and shows a clear path towards reducing human bias and intervention in the design of such metrics.  The paper presents a comprehensive and thorough experimental evaluation clearly demonstrating the superiority of EZNASA against existing zerocost proxies across numerous datasets and design spaces.  The methodology has a considerable potential to increase the endtoend efficiency of NN architecture search significantly as evidenced by the reported results. Weaknesses:  Although the paper claims the ZCNASMs discovered are generalizable their validation across a broader spectrum of datasets and network designs beyond those mentioned is necessary to truly substantiate these claims.  The paper could benefit from a more detailed discussion on the potential failure cases or the conditions under which the proposed ZCNASM may not perform as expected.  There might be a concern regarding the scalability of this approach when applied to extremely largescale neural network architectures or highly complex dataset scenarios which was not addressed thoroughly. 3) Questions and suggestions for the authors  Could the authors elaborate on how the EZNAS approach might adapt or scale with the increasing complexity and size of neural architecture design spaces  It would be beneficial to see a comparison or analysis of how the discovered ZCNASM performs in realworld applications outside of the benchmark datasets and design spaces.  Integrating domainspecific architectures or designs could further assess the realworld applicability and flexibility of EZNAS. Have the authors considered exploring this further 4) Limitations and societal impact The authors have addressed the limitations concerning scalability and generalizability to some extent however a more indepth discussion on these aspects could make the paper stronger. Particularly exploring the limitations of EZNAS in contextspecific applications and on architectures beyond those tested would provide a more comprehensive picture. On the societal impact front the paper presents a significant contribution towards reducing the environmental impact of intensive computational processes involved in NAS. However a discussion on potential negative impacts such as misuse of these efficient methods for creating malicious architectures swiftly would be a valuable addition. Suggestions for mitigating such risks could also be helpful. Overall the paper presents a strong contribution to the field of neural network architecture search offering a promising direction towards more sustainable and efficient architecture evaluation methods. Further exploration on scalability realworld applicability and addressing potential negative impacts would enhance its significance and applicability.", "rA2tItoRUth": "Peer Review for \"LanguageGuided Denoising Network for VideoLanguage Modeling\" 1. Summary: This paper introduces the LanguageGuided Denoising Network (LGDN) an innovative approach to videolanguage modeling that dynamically filters misaligned or redundant frames in videos under language supervision. Unlike existing methods that rely on densely sampled video frames for videolanguage representation which can be computationally costly and may incorporate irrelevant information LGDN smartly selects 24 salient frames per video for precise crossmodal tokenlevel alignment. This method significantly outperforms stateoftheart models in extensive experiments across five public datasets. The authors also conduct a detailed ablation study to emphasize the importance of addressing the noise issue in videolanguage modeling. Strengths:  Innovative Approach: The introduction of a LanguageGuided Denoising Network that selects salient frames under language supervision presents a novel and effective way to reduce noise and computational cost in videolanguage modeling.  Impressive Performance: The models superior performance as compared to existing approaches on five public datasets demonstrates the effectiveness of the proposed methods.  Detailed Ablation Study: The comprehensive ablation study provides valuable insights into the importance of each component of the LGDN model demonstrating how addressing noise significantly enhances performance. Weaknesses:  Generalizability Concerns: While the model shows outstanding performance on the datasets tested the paper does not discuss its performance on videos with minimal text description or on datasets with varied noise levels. Understanding how LGDN handles these scenarios could provide further insights into its robustness and generalizability.  Lack of RealWorld Application Discussion: The paper could benefit from a discussion on practical applications of LGDN such as in video recommendation systems or content moderation to highlight the models potential realworld impact. Questions and Suggestions for the Authors: 1. Robustness to Different Noise Levels: Can the authors provide insights or additional results on how LGDN performs on datasets with varying levels of noise or irrelevant information in the video frames This information would enhance understanding of the models robustness. 2. Handling of Minimal Text Descriptions: How does LGDN perform in scenarios where the text description is minimal or vague Incorporating examples or discussion around this would be valuable. 3. Practical Applications: It would be beneficial if the authors could discuss potential realworld applications of LGDN such as its use in content moderation video recommendation systems or educational tools and any challenges that might arise in these contexts. 4. Temporal Information Aggregation: The conclusion mentions considering temporal information on salient frames for future work. Could the authors elaborate on potential methods they are considering for incorporating temporal information and how they expect it might improve the models performance Limitations and Societal Impact: The authors have not extensively addressed the limitations and potential negative societal impacts of their work. One limitation could be the model\u2019s dependency on the quality and availability of text descriptions. In cases where descriptions are absent or misleading the models performance could be affected. Furthermore the application of LGDN in content filtering could inadvertently introduce biases if not carefully monitored possibly reinforcing stereotypes or censoring certain content disproportionately. To improve the authors could consider including a section discussing these limitations and potential societal impacts. Suggestions for future research might include exploring unsupervised methods for salient frame selection to reduce dependency on text descriptions and investigating bias mitigation techniques in videolanguage models to ensure fair and ethical use. Overall this paper contributes significantly to the field of videolanguage modeling by addressing a critical challenge of noise and irrelevant information. With further research and refinement LGDN has the potential to set a new standard for videolanguage representation models.", "lYZQRpqLesi": "Review for \"Alternating Sparse Training (AST) for Efficient and Dynamic Inference\" 1) Summary of the Paper: This paper addresses the challenge of training neural networks that can efficiently adjust their computational complexity posttraining to adapt to varying hardware resources and application requirements. It introduces an Alternating Sparse Training (AST) method that trains multiple sparse subnetworks within a single training process eliminating the need for additional training costs associated with obtaining models of varying sparsity. The key innovation is the use of a novel alternating training scheme coupled with a gradient correction mechanism to minimize the interference in weight updates among the subnetworks while maintaining optimization generalization. The authors demonstrate that the AST method achieves similar or better accuracy than stateoftheart sparse training methods but at a reduced computational cost. Comprehensive experiments are conducted on CIFAR10 CIFAR100 and ImageNet datasets showing that AST outperforms existing dynamic inference methods in accuracy without additional training overhead. 2) Strengths and Weaknesses:  Strengths:  Originality and Practicality: The paper presents a novel approach to training multiple subnetworks for dynamic inference without extra training costs which is a significant step forward in model efficiency and adaptability.  Comprehensive Evaluation: The authors conduct extensive experiments covering various datasets and network architectures to validate the effectiveness of the AST method. The comparison with stateoftheart methods is thorough.  Significant Results: The results indicate that the AST method can achieve comparable or better accuracy than existing methods while significantly reducing the training overhead which could have a considerable impact on resourceconstrained environments.  Weaknesses:  Lack of Theoretical Insights: While the paper provides intuitive explanations for the methods used it lacks a deeper theoretical analysis that could give insights into why the method works and under what conditions it might fail.  Limited Exploration of Limitations: The paper briefly mentions extending to support a larger number of subnets as a future direction but does not thoroughly discuss the limitations of the current approach or the potential decrease in performance with the increase in the number of subnets.  Potential Bias Towards Specific Datasets: Although the paper evaluates on three popular datasets the choice of datasets might still be too restricted to generalize the findings across all types of data and applications. 3) Questions and Suggestions for the Authors:  Theoretical Foundation: Could the authors provide more theoretical analysis or insights into the gradient correction mechanism and its impact on the generalization capability of the model  Limitation Discussion: It would be beneficial if the authors could discuss the limitations of AST in more detail particularly concerning the scalability of the method to a large number of subnets and its impact on performance.  Broader Applicability: Have the authors considered evaluating the AST method on tasks other than image classification such as natural language processing or reinforcement learning to demonstrate its applicability more broadly 4) Limitations and Potential Negative Societal Impact: The authors have not thoroughly addressed the limitations of their work particularly in terms of the scalability of their method to a large number of subnets and different tasks beyond image classification. A more indepth discussion on these aspects would have strengthened the paper. Moreover considering the potential negative societal impact there is a lack of discussion. One can foresee that if AST enables much more computationally efficient models it could potentially lead to wider deployment of artificial intelligence systems raising concerns about privacy automationinduced unemployment and misuse in surveillance. The authors could include a section discussing these potential issues offering guidelines for responsible use and suggesting mitigation strategies. Constructive suggestions for improvement:  Extend the theoretical analysis to offer insights into the conditions under which AST performs optimally and its limitations.  Broaden the evaluation scope to include different types of tasks beyond image classification to demonstrate the methods versatility.  Discuss potential negative societal impacts more thoroughly and offer mitigation strategies to ensure responsible use of the technology.", "oNnv9XjClGK": " Review of \"Advancing Reinforcement Learning with Direct Advantage Estimation (DAE)\"  Summary The paper introduces a novel methodology named Direct Advantage Estimation (DAE) aimed at advancing Reinforcement Learning (RL) by addressing the issue of assigning credit to actions based on their actual impact on the expected return rather than the traditional expected return itself. The authors argue that the traditional approach leads to a high variance in value estimation and subsequently slows down the learning process. By leveraging insights from causality literature the paper posits that the advantage function can be perceived as the causal effect of actions thereby holding properties similar to causal representations. This basis allows for a more stable and direct estimation of the advantage function circumventing the need for (action) value function anticipation. Through empirical evaluation on three discrete control domains the paper demonstrates that DAE can outperform Generalized Advantage Estimation (GAE) in policy optimization tasks highlighting its potential for better scalability and efficiency in modern actorcritic methods.  Strengths 1. Theoretical Foundation: The paper is grounded in a solid theoretical framework that innovatively connects the domains of causality and reinforcement learning. The interpretation of the advantage function through the lens of causal effects is particularly compelling and offers a fresh perspective on credit assignment in RL. 2. Methodological Innovation: DAE represents a significant methodological advancement as it enables direct advantage estimation without the prerequisite of learning value functions. This direct approach has the potential to reduce the variance typically associated with return based estimations which is a notable contribution to the field. 3. Empirical Validation: The empirical evaluation of DAE across three different domains demonstrates its effectiveness and superiority over the baseline (GAE) in a majority of the environments tested. This broad testing base lends credibility to the proposed methods applicability and robustness. 4. Ease of Integration: The authors claim that DAE is easy to implement and can be readily adapted into existing actorcritic methods which is a practical advantage. This quality enhances its appeal to researchers and practitioners alike encouraging wider adoption and experimentation.  Weaknesses 1. Limited Domain Scope: While the paper evaluates DAE across three domains all are limited to discrete control tasks. The efficacy of DAE in continuous action spaces or more complex environments remains untested which may limit the generalizability of the findings. 2. Potential for Bias in Early Training Phases: The paper acknowledges that DAE might be biased at the beginning of the training process. Given that early training stability is crucial for learning efficiency and final policy performance this might be a significant drawback that requires further examination. 3. Lack of Discussion on Computational Efficiency: The paper does not delve into the computational aspects of implementing DAE specifically whether the direct estimation approach introduces additional computational overhead or how it compares with traditional methods in terms of execution speed.  Questions and Suggestions for Authors 1. Continuous Action Spaces: Can the authors elaborate on potential methods or modifications required to apply DAE in environments with continuous action spaces Experimentation in such settings could significantly enhance the methods applicability. 2. Impact of Initial Bias: It would be beneficial if the authors could provide more insights into the impact of the initial bias associated with DAE on the overall training process and outcomes. Are there strategies that could mitigate this bias in the early stages 3. Computational Overhead: Could the authors comment on the computational efficiency of DAE compared with traditional advantage estimation techniques Insights into the tradeoffs between variance reduction and computational demands would be valuable.  Limitations and Societal Impact The authors have not explicitly discussed the limitations and potential negative societal impacts of their work. While the methodology presents a clear advantage in discrete control domains its relevance and efficacy in broader or realworld applications remain to be seen. Furthermore as with any RL technique theres potential for misuse in scenarios where reinforcement learning models can learn unethical or undesired behaviors. To mitigate these concerns future research should explore the applicability of DAE in diverse settings including those with continuous actions and realworld applications and incorporate mechanisms to ensure ethical use and transparency.  Suggested Improvements  Expanding experimentation to continuous action spaces or more complex and realistic environments would significantly enhance the papers contributions.  Addressing the initial bias issue in DAE explicitly through theoretical analysis or empirical evidence could provide a more comprehensive understanding of its implications on learning outcomes.  A detailed discussion on computational efficiency and practical implementation challenges would complete the paper by providing a more holistic view of DAEs applicability and potential limitations in diverse settings.", "pNHT6oBaPr8": " Review of \"Partial Group Equivariant Convolutional Neural Networks\"  1) Brief Summary of the Paper This paper introduces Partial Group equivariant Convolutional Neural Networks (Partial GCNNs) which aim to learn layerwise levels of equivariance to discrete and continuous groups as part of training. While traditional GCNNs impose equivariance to symmetries present in the whole group leading to overly constrained models in cases where the data exhibits only partial symmetries Partial GCNNs adjust their level of equivariance dynamically. This capability allows Partial GCNNs to retain full equivariance when beneficial and adjust it when necessary thereby outperforming conventional GCNNs in tasks where full equivariance is harmful and matching their performance otherwise. The authors empirically validate the effectiveness of Partial GCNNs on various tasks including rotated MNIST face pose classification and natural image classification showcasing the models ability to adaptively learn the optimal level of equivariance.  2) Assessment of the Paper Strengths:  The paper addresses a significant limitation of GCNNs namely the inability to adapt to the level of equivariance required by different tasks or data.  It presents a novel method to learn layerwise levels of equivariance directly from data which is both innovative and practically useful.  The empirical results including experiments on the rotated MNIST and natural image datasets convincingly demonstrate the superiority of Partial GCNNs over traditional GCNNs in settings where full equivariance is not beneficial.  The introduction of partial group convolutions and their theoretical underpinnings provide a strong mathematical basis for the proposed method. Weaknesses:  The paper could better explain the training dynamics and stability concerns of learning partial equivariances especially with discrete groups.  There is limited discussion on the computational and memory complexities compared to traditional GCNNs which is crucial for practical applications.  While the paper briefly mentions the potential negative societal impact a deeper analysis of these aspects especially in the context of biased or skewed datasets could further strengthen the paper.  3) Questions and Suggestions for the Authors  Could the authors elaborate on the criteria used to adjust the level of equivariance during training How does the model decide when to retain full equivariance or to adjust it  In the experiments section how does the performance of Partial GCNNs compare to other stateoftheart methods (beyond traditional GCNNs) on the same datasets  It would be beneficial to see a more detailed analysis of the models performance in realworld scenarios where data distribution may significantly vary. Can the authors provide insights or additional experiments in this regard  Regarding the potential negative societal impacts can the authors discuss any safeguards or ethical considerations when deploying Partial GCNNs in sensitive applications  4) Limitations and Suggestions for Improvement The authors mention the potential limitations regarding the unstable training on discrete groups and the challenges of scaling to very large groups. Its commendable that these limitations are acknowledged. As a suggestion for improvement the paper would benefit from a deeper exploration of these aspects possibly including strategies or ongoing work to address these limitations. Additionally a discussion on the computational overhead introduced by learning partial equivariances and how it compares to traditional GCNNs would be valuable for readers to fully assess the practicality of the proposed method. Overall this paper presents a significant advancement in the field of equivariant neural networks by introducing a method to learn partial group equivariances adaptively. This work opens up new possibilities for designing more flexible and efficient neural network architectures capable of handling complex realworld data.", "wYgRIJ-oK6M": "Review of \"Advanced Techniques for the Binarization of PreTrained Transformers\"  Summary: This paper addresses the challenge of deploying stateoftheart pretrained transformer models in resourceconstrained environments by introducing novel binarization techniques that reduce model size and computational complexity without significantly compromising accuracy. The authors propose a robust twoset binarization scheme an elastic binary activation function with learnable parameters and a multidistillation approach for progressively distilling higher precision models into lower precision students. The techniques are collectively able to narrow the performance gap between binarized transformers and their fullprecision BERT baseline on the GLUE benchmark to as little as 5.9. The work makes binary transformers practical for the first time by achieving competitive accuracy levels.  Assessment: Strengths: 1. Innovation in Binarization Techniques: The introduction of a twoset binarization scheme an elastic binary activation function and a novel multidistillation approach represent significant advancements in the binarization of neural networks particularly for the complex transformer architecture. 2. Practical Impact: The paper tackles an important problem of making stateoftheart AI technologies accessible in resourceconstrained environments. The solutions provided could drastically increase the deployment of advanced AI models on mobile and wearable devices. 3. Detailed Methodological Explanation and Evaluation: The authors provide a comprehensive and clear exposition of their methods followed by thorough experimentation and comparison to stateoftheart techniques. This rigorous evaluation adds credibility to their claims. Weaknesses: 1. Lack of Broader Evaluation: While the focus on GLUE and SQuAD benchmarks is understandable given their popularity in NLP tasks the absence of evaluation on tasks from other domains like image and speech processing limits the demonstrated generality of the proposed binarization techniques. 2. Potential Overfitting to Specific Benchmarks: The paper could benefit from a discussion on how the techniques might need to be adapted for different types of transformer models or tasks as theres a risk that the approaches are somewhat overfitted to the tested benchmarks. 3. Computational Costs and Practicality: While binarization reduces the deployment costs in terms of model size and computational complexity the paper does not discuss the computational resources required for training especially the multidistillation process. This information is crucial for evaluating the practicality of the proposed method.  Questions and Suggestions: 1. Generalizability to Other Domains: Can the authors provide insights or preliminary results on applying their binarization techniques to transformer models in domains other than NLP such as image or speech processing 2. Impact of Binarization on Model Fairness and Bias: Have the authors analyzed how binarization affects the fairness and bias characteristics of the transformer models Binarization could potentially exacerbate or mitigate such aspects and it would be beneficial to understand these impacts. 3. Efficiency of Training Process: The multidistillation approach seems resourceintensive. Could the authors comment on the computational costs involved in training using their proposed method and suggest ways to optimize it  Limitations and Societal Impact: The authors have not thoroughly discussed the potential limitations and negative societal impacts of their work. A discussion on how the reduced precision might affect the fairness and bias of models especially in sensitive applications is missing. Furthermore while the work facilitates the deployment of advanced models on resourceconstrained devices it might further increase the digital divide between those with access to cuttingedge technology and those without. Its also recommended that the authors consider the environmental impact of the computational resources required for the training process. To improve the authors should investigate and report on the fairness and bias implications of binarized models consider broader evaluations across different domains and models and provide details on the training computational costs to better assess the overall practicality and potential negative impacts of their techniques.", "rY2wXCSruO": " Peer Review of \"DeepInteraction: A Multimodal Interaction Strategy for 3D Object Detection in Autonomous Driving\"  1) Summary of the Paper and Contributions: This paper presents a novel approach to 3D object detection for autonomous driving by proposing a multimodal interaction strategy embedded within a dualcomponent architecture named DeepInteraction. Unlike traditional methods that merge features from different modalities (LiDAR and camera images) into a singular hybrid representation DeepInteraction learns and maintains separate modalityspecific representations allowing for dynamic intermodality interactions. The architecture comprises a multimodal representational interaction encoder that facilitates continuous exchange and refinement of feature representations between modalities and a multimodal predictive interaction decoder that uses the enriched features for precise object detection. The methods effectiveness is validated on the nuScenes dataset where it outperforms existing solutions marking a new state of the art and achieving first place on the nuScenes object detection leaderboard.  2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The modality interaction strategy is a significant advancement over the conventional fusion tactics enabling the model to leverage the unique characteristics and strengths of each modality fully.  Performance: DeepInteraction demonstrates superior performance on a highly competitive dataset providing substantial empirical evidence of its effectiveness.  Architecture Design: The design of the DeepInteraction architecture is methodically thought out with a clear separation between representational interaction and predictive interaction which contribute to its outstanding performance. Weaknesses:  Complexity and Computational Demand: The dualcomponent architecture and the interaction mechanism while effective likely introduce considerable computational complexity potentially limiting its application in lowresource situations.  Generalization: The paper primarily focuses on the nuScenes dataset for evaluation. Additional validation across varied datasets is necessary to demonstrate the approachs generalizability and robustness in different scenarios or environments.  Objective Evaluation on Limitations: While the paper presents a groundbreaking method there seems to be a lack of thorough discussion regarding its limitations potential biases in modal data and how it may impact or skew the detection performance.  3) Questions and Suggestions for the Authors:  On Computational Efficiency: Can the authors elaborate on the computational requirements of DeepInteraction compared to existing methods Suggestions on optimizing for lowerpowered systems could broaden its application scope.  Regarding Generalization: Have the authors considered evaluating the models performance across other datasets or realworld scenarios Insights into its adaptability and scalability would be valuable.  Discussion on Limitations: Could the authors provide a more detailed exploration of any intrinsic limitations or potential biases within the interaction strategy and how they might be mitigated  4) Limitations and Societal Impact: The paper briefly touches upon the methods performance advantage but does not provide an extensive discussion on limitations particularly regarding computational efficiency potential biases arising from modalityspecific information and how they might affect fairness or inclusivity in detection. Further there is minimal consideration of the potential negative societal impacts such as privacy concerns or misapplications in surveillance. Constructive suggestions include incorporating a more indepth examination of these aspects possibly introducing methods to ensure privacy protection (e.g. anonymization techniques) and discussing ethical considerations surrounding the deployment of such technologies in public spaces.", "w5DacXWzQ-Q": " Peer Review of \"StructurallyAware Vision Transformer Pruning for Efficient Vision Processing\"  1) Summary of the Paper This paper introduces a novel method for pruning Vision Transformers (ViTs) by considering the joint importance of heterogeneous components within the network. Unlike previous works which prune components independently leading to suboptimal performance this approach integrates essential structuralaware interactions between components to perform collaborative pruning. A Taylorbased approximation is utilized to evaluate joint importance facilitating a balanced reduction across all components and reducing algorithm complexity. The method showcases versatility across various tasks notably achieving a 0.7 accuracy increase on ImageNet and being the first to report only a 0.3 mAP drop on COCO with significant FLOPs reduction (50 on ImageNet and 70 on COCO). The framework generalizes well to more complicated architectures like detection networks. The effectiveness of this approach is underpinned by thorough experimental validation across different models and tasks.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper pioneers the integration of interactions between different components in ViTs for pruning addressing a gap in current methodologies.  Theoretical and Practical Balance: The authors provide a solid theoretical basis for their approach with the Taylorbased approximation and solve an optimization problem for practical application.  Extensive Validation: The thorough experimentation across various tasks including object detection and models highlights the methods versatility and effectiveness.  Efficiency Improvements: Demonstrated improvement over existing stateoftheart methods in reducing computational costs while maintaining or even improving performance is noteworthy. Weaknesses:  Generalization to Other Architectures: While the paper demonstrates the methods effectiveness on ViTs its applicability and efficiency on other architectures or nonvision tasks are not addressed.  Complexity of Implementation: The methods efficiency is highlighted but the complexity of implementing such a collaborative pruning approach compared to simpler albeit less effective methods is not discussed.  Limited Discussion on Limitations: The paper could benefit from a more detailed discussion on the limitations of their approach including potential degradation in specific scenarios or with certain types of data.  3) Questions and Suggestions for the Authors  Generalization Capability: Could the authors comment on how this collaborative pruning method could be adapted or applied to other deep learning architectures beyond ViTs  Implementation Complexity: Can the authors provide insights into the implementation complexity and potential challenges in adopting this pruning approach in existing systems  Potential Limitations: Are there specific scenarios tasks or data types where the proposed method might not perform as well How can these limitations be addressed  4) Limitations and Societal Impact The authors have demonstrated the potential of their approach in reducing the computational burden of ViTs which can contribute positively by enabling efficient edge device deployment. However the paper lacks a detailed discussion on the limitations especially concerning the application to architectures beyond vision tasks. Addressing these limitations would enhance the methods robustness and applicability. From a societal impact perspective while the paper does not directly tackle negative societal impacts the push towards more efficient computational methods has inherent value. It democratizes access to advanced machine learning models by lowering the entry barriers related to computational resources. Nonetheless the authors could consider including a discussion on ensuring that the benefits of such technological advancements are accessible and equitable across different societal strata. Suggestions for Improvement:  Expand on the methods potential applicability or limitations to other deep learning architectures and tasks.  Provide a more detailed discussion on the complexity of the collaborative pruning process and practical implementation challenges.  Consider adding a section discussing the direct and indirect societal impacts of their work ensuring it covers both the positive aspects and any potential negatives.", "q6bZruC3dWJ": " 1) Brief Summary and Contributions The paper delves into the counterintuitive phenomenon observed in knowledge distillation (KD) where a larger seemingly more capable teacher model does not necessarily lead to a betterperforming student model. The authors attribute this inefficiency to the presence of \"undistillable classes\" which the student model fails to learn from the teacher due to insufficient capacity. The paper provides an empirical study to support the hypothesis introducing the \"Teach Less Learn More\" (TLLM) distillation framework. TLLM aims to identify and exclude undistillable classes during the training process allowing the student model to focus on learnable aspects. The approach is validated across multiple datasets and network architectures showing superior performance to existing stateoftheart methods.  2) Assessment of Strengths and Weaknesses  Strengths:  Novelty: The paper presents a unique perspective by identifying undistillable classes as a key barrier to effective knowledge distillation challenging the prevailing notion of \"larger teacher equals better student.\"  Empirical Evidence: The authors provide thorough empirical analyses to support their claims including experiments across different datasets architectures and a wide range of distillation techniques.  Practical Framework: The TLLM framework is intuitive and exhibits remarkable effectiveness in enhancing the performance of distilled models by focusing on distillable knowledge only.  Generalizability: Validation on multiple datasets and architectures confirms the methods broad applicability and potential for wide adoption.  Weaknesses:  Lack of Theoretical Foundation: While the empirical results are strong the paper lacks a deep theoretical exploration of why certain classes become undistillable which could strengthen the overall argument.  Potential Overfitting Concern: By excluding undistillable classes during training theres a risk of overfitting to the distillable classes. This aspect is not thoroughly discussed or analyzed.  Computational Efficiency: The process of identifying undistillable classes though stated as computationally efficient could potentially add significant overhead especially for large datasets with many classes. More insights into the computational impact would be beneficial.  3) Questions and Suggestions for the Authors  Theoretical Insights: Can the authors provide a more detailed theoretical hypothesis or model explaining why certain classes are inherently undistillable  Overfitting Analysis: It would be insightful to include an analysis of whether the models performance on distillable classes is artificially inflated due to the exclusion of undistillable classes potentially leading to overfitting.  LongTerm Impact: How do the authors view the longterm impact of their findings on the broader machine learning community especially in terms of designing efficient and compact models  4) Limitations and Societal Impact The authors have made an attempt to address the limitations of their work primarily focusing on the computational efficiency of identifying undistillable classes. However a more comprehensive discussion on the potential of overfitting to distillable classes and its implications would enrich the paper. Furthermore the societal impacts especially in terms of energy efficiency and accessibility of ML models in constrained environments are not discussed. To improve the authors could:  Conduct a more detailed analysis of the computational cost and propose strategies to mitigate potential inefficiencies.  Explore the implications of excluding undistillable classes on model robustness and generalization capabilities.  Elaborate on the potential negative societal impacts such as the reliance on large datasets and models and propose guidelines to balance performance with ethical considerations.", "qTCiw1frE_l": "Review of \"Understanding Policy Churn in Valuebased Deep Reinforcement Learning\" 1. Summary of the Paper: This paper investigates the phenomenon of policy churn  the rapid shift in the greedy policy during valuebased reinforcement learning processes as observed in deep RL settings like DQN on Atari games. Policy churn leads to significant alterations in greedy actions across numerous states after only a few learning updates. The authors empirically characterize this phenomenon debunking the assumption that its restricted to specific algorithmic or environmental conditions. Through a series of ablations they explore plausible causes with deep learning involving highvariance updates emerging as the most likely explanation. The paper also posits that policy churn could serve as an implicit method of exploration reducing reliance on \u03b5greedy strategies and advocating for a reevaluation of their role in exploration. Key contributions include a detailed empirical study of policy churn across various settings an analytical exploration of its effects on agent behaviour and hypothetical explorations on the origins and potential benefits of policy churn. 2. Assessment of the Paper: Strengths:  Originality: The study of policy churn is a relatively unexplored area within deep reinforcement learning. The authors present a novel analysis of how rapid changes in policy can potentially benefit exploration strategies offering fresh insights into the dynamics of RL learning processes.  Empirical Rigor: The paper presents a comprehensive empirical investigation across multiple settings employing a variety of deep RL configurations and environments. This thorough analysis strengthens the claim that policy churn is a pervasive phenomenon not bound to specific conditions.  Exploration of Implications: The speculative discussion on how policy churn could reframe our understanding of exploration in RL adds depth to the paper. It not only highlights the significance of the findings but also opens up avenues for future research on leveraging policy churn for more efficient learning. Weaknesses:  Lack of Theoretical Foundation: While the empirical evidence is compelling the paper falls short in providing a solid theoretical grounding for why policy churn occurs and its exact mechanisms. A deeper theoretical analysis could have strengthened the argument and provided clearer guidance for harnessing policy churn effectively.  Potential for Bias in Empirical Analysis: The reliance on Atari games and specific deep RL setups (e.g. DQN) might introduce bias in the findings. Its unclear if policy churn would manifest similarly in other complex environments or under different learning paradigms.  Limited Exploration of Solutions: Though the paper speculates on the potential benefits of policy churn it offers limited concrete strategies for intentionally exploiting this phenomenon. Future research could benefit from a more focused exploration of methods to control and utilize policy churn for optimizing exploration. 3. Questions and Suggestions for the Authors:  Clarification on Theoretical Underpinnings: Can the authors provide more theoretical insight into why policy churn occurs Any mathematical models or deeper analyses would be beneficial.  Expanding the Empirical Setting: Have the authors considered testing the presence and effects of policy churn in environments beyond Atari such as continuous control tasks This could help validate the universality of the phenomenon.  Exploring Solutions: Could the authors elaborate on potential strategies or algorithms that explicitly harness policy churn for improved exploration Proposing experimental setups or preliminary algorithms to test these strategies would be valuable. 4. On Limitations and Societal Impacts: The authors touch upon the limitations of their empirical focus and the speculative nature of some discussions but could further address the broader implications of intentionally leveraging policy churn. While the paper proposes that policy churn could enhance exploration without additional noise its crucial to consider if such methods could lead to unstable learning dynamics or unexpected behavior in critical applications (e.g. autonomous vehicles or medical treatment plans). Suggestions for future work include a deeper analysis of the stability implications of policy churn and frameworks for ensuring that its exploitation does not lead to adverse outcomes in sensitive applications. In conclusion this paper offers a pioneering look into policy churn within deep reinforcement learning providing compelling empirical evidence and prompting a reevaluation of exploration strategies in RL. Further theoretical grounding and exploration of practical exploitation methods could enhance its contributions significantly.", "rTTh1RIn6E": " Review of \"Conditionali: Conditional Independence Modeling for OutofDistribution Detection\"  1) Brief Summary This paper introduces a novel approach to OutofDistribution (OOD) detection by constructing a latent variable model named Conditionali. The key innovation lies in the utilization of Independent Component Analysis (ICA) techniques specifically leveraging the Conditional Independence assumption to improve OOD detection performance. This is accomplished by incorporating class condition into the model and employing the HilbertSchmidt Independence Criterion (HSIC) for optimizing dependencies. A memory bank architecture is designed to facilitate efficient training. The method showcases significant improvements over stateoftheart (SOTA) methods on benchmarks in both computer vision and NLP tasks. The paper provides theoretical underpinnings for its approach demonstrating that the proposed training strategy effectively bounds error in OOD detection scenarios.  2) Assessment of Strengths and Weaknesses Strengths: a. Innovative Approach: The introduction of a conditional independence model based on ICA for OOD detection is novel and represents a significant paradigm shift from traditional discriminative or generative methods. b. Comprehensive Theoretical Support: The paper goes beyond empirical results by providing rigorous theoretical justifications for its approach lending credibility to its claims of superiority over existing methods. c. Versatility and Efficiency: Conditionalis ability to operate effectively with or without training OOD data in addition to its unique memory bank architecture presents a versatile and computationally efficient solution for OOD detection. d. Empirical Validation: Extensive experiments across computer vision and NLP tasks with comparisons to SOTA methods validate the methods effectiveness and applicability. Weaknesses: a. Technical Complexity: The introduction of conditional independence through HSIC and memory banks might increase the learning curve for implementing and tuning the model potentially restricting its accessibility to practitioners. b. Generalization Concerns: While the paper mentions its method outperforms others in handling training on fake OOD data it could benefit from a deeper discussion on the models generalization abilities especially in extremely diverse or unseen OOD scenarios. c. Dependency on Memory Banks: The necessity of memory banks for practical training implementation could introduce additional computational overhead especially for very largescale problems or realtime applications.  3) Questions and Suggestions for the Authors Questions: a. Can the authors provide more insights into the potential computational overhead introduced by the memory bank architecture especially in largescale scenarios b. How does Conditionali behave in scenarios with extremely noisy or adversarially crafted OOD samples Is there a potential for performance degradation and how can it be mitigated Suggestions: a. Experimental Validation with Adversarial OOD Samples: Extending experimentation to include adversarially crafted OOD samples could further validate the robustness of Conditionali. b. Comparison with Fewer Dependency Methods: A comparison with methods that do not rely on extra architectural components like memory banks might help in highlighting the tradeoffs involved in adopting Conditionali. c. Further Exploration on Generalization: Delving deeper into the models generalization capabilities across a broader spectrum of OOD scenarios could strengthen the paper.  4) Limitations and Societal Impact The authors have done a commendable job in addressing limitations related to the need for (fake) OOD training data which acknowledges a foundational challenge in OOD detection. However a more thorough discussion on the computational efficiency and scalability especially in the context of the memory bank architecture could enrich the paper. Regarding societal impact while Conditionali advances AI safety by improving OOD detection  critical for domains like healthcare and autonomous driving  the paper could discuss potential misuse scenarios such as adversarial attacks designed to bypass OOD detection. Enhancing the models robustness against such attacks and exploring defensive mechanisms would be beneficial.", "vphSm8QmLFm": "Review of \"Global Batch gradients Aggregation (GBA) for Enhanced Distributed Training of Recommendation Models\" 1) Summary: This paper addresses the efficiency and accuracy challenges in distributed training of recommendation models particularly when switching between highconcurrency asynchronous training on a Parameter Server (PS) architecture and highperformance synchronous training on an allreduce (AR) architecture. The core contribution is the proposal of Global Batch gradients Aggregation (GBA) a method designed to facilitate seamless switching between these training modes without the need for hyperparameter tuning. GBA aggregates gradients with the same global batch size as synchronous training and implements a tokencontrol process to manage gradient staleness from slow workers. The paper provides convergence analysis demonstrating that GBA has comparable properties to synchronous training and shows through experiments on industrialscale recommendation tasks that GBA can significantly improve both accuracy and training speed. 2) Assessment: Strengths:  Novelty: The GBA method introduces a novel approach to managing the balance between asynchronous and synchronous training modes in a distributed environment which is a significant contribution to the efficient training of recommendation models.  Comprehensive Analysis: The paper provides an indepth convergence analysis and extensive empirical evidence to support the effectiveness of GBA making a strong case for its benefits.  Practical Impact: Given the noted improvements in accuracy and training efficiency without the need for hyperparameter retuning GBA represents a potentially valuable advancement for the training of largescale recommendation systems. Weaknesses:  Generalizability: The experimentation is focused on recommendation models and its not clear how GBA might perform with other types of largescale deep learning models such as those used in natural language processing or computer vision.  Limitations on Dynamic Clusters: While GBA is designed to adapt based on the cluster status theres limited discussion on how it handles extremely volatile environments where cluster resources can fluctuate rapidly.  Detail on TokenControl Mechanism: The mechanism is central to the operation of GBA yet the explanation lacks detail on how token values are assigned and managed throughout the training process. 3) Questions and Suggestions:  More Detail on the TokenControl Mechanism: Can the authors provide a more detailed explanation or pseudocode for the tokencontrol mechanism specifically on how tokens are initially assigned and updated  Extension to Other Models: Have the authors considered extending the evaluation of GBA to other types of models beyond recommendation systems It would be useful to understand its applicability to a broader range of machine learning tasks.  RealWorld Deployment Considerations: The paper mentions GBA has been deployed in a shared training cluster. Can the authors share insights or challenges faced during this deployment especially related to dynamically changing training modes in a live environment 4) Limitations and Societal Impact: The authors do not provide an explicit discussion on the broader limitations or the potential negative societal impact of their work. Given the focus on recommendation models its crucial to consider how improving the efficiency and accuracy of these systems might inadvertently amplify existing biases perpetuate echo chambers or contribute to overconsumption through more effective recommendation engines. Suggestions for Improvement:  Bias and Fairness Analysis: Future work could include an analysis of how GBA affects the fairness and bias of the trained recommendation models. Investigating whether more efficient training correlates with amplifying or mitigating these concerns would be valuable.  Adaptability to Resource Variability: The authors should consider exploring more about how GBA adapts to sudden and drastic changes in available computing resources as this could significantly impact its practical deployment in shared dynamic environments.  Impact on Energy Consumption: Considering the potential energy implications of faster training and more frequent switching between training modes it would be beneficial to include an analysis of GBAs impact on overall energy consumption and efficiency. Overall the paper presents a compelling advance in the field of distributed training for recommendation systems with potential for a broad impact on the efficiency and accessibility of training largescale models. Further exploration of its broader applicability and limitations will be crucial for understanding its full potential.", "zdmYnIRXvKS": "Review of \"Developing biologically plausible spiking neural networks for efficient coding\" 1. Summary: This paper presents significant advancements in the theory and application of efficient coding within the context of spiking neural networks specifically focusing on achieving biological plausibility. The authors propose a comprehensive model based on a generalized leaky integrateandfire (LIF) framework with distinct excitatory and inhibitory neurons. Their model incorporates several sophisticated elements like fast and slow synaptic currents spiketriggered adaptation rebound currents and structured lowrank connectivity aiming to bridge the gap between theoretical models and biological circuits functional and dynamic properties. By successfully incorporating these elements the paper presents a leap towards understanding how neural computations could be efficiently realized in contexts resembling the structural and dynamical intricacies of biological neural networks. 2. Assessment:  Strengths: 1. The introduction of a biologically plausible spiking neural network model that extends efficient coding theories significantly broadens our theoretical understanding and presents a valuable tool for future research in neuroscience and related fields. 2. The papers rigorous mathematical formulation coupled with simulations that demonstrate the behavior of the network under various conditions provides a solid foundation for the claims made. 3. Addressing biological plausibility by incorporating features like fast and slow synaptic currents and structured lowrank connectivity lends the model a high degree of relevance to real neural systems.  Weaknesses: 1. While the simulations showcase the networks behavior under specific configurations there is a lack of extensive validation against empirical data from real neural tissues which would strengthen the models claims of biological relevance further. 2. The discussion on the potential computational advantages or insights gained from the specific features of the model (e.g. structured connectivity heterogeneous firing thresholds) is somewhat limited. A detailed analysis comparing the proposed model with existing models could further elucidate its benefits and uniqueness. 3. The societal implications and potential ethical considerations of applying such advanced neural network models while not directly relevant to the study\u2019s primary goals are not addressed. 3. Questions and Suggestions:  Could the authors provide more details on how they envision empirical validation of their model to be accomplished Are there specific experiments or data sets they believe would be particularly fitting  It would be beneficial if the paper could include a comparison even if only theoretical with other prominent spiking neural network models to highlight the unique advantages and potential limitations of the proposed model.  A discussion on how this model could be extended or adapted for various computational tasks or for mimicking other brain areas beyond the cortical circuits considered would be insightful. How flexible is the model in its current form 4. Limitations and Societal Impact: The authors have made an excellent effort to advance the biological plausibility of spiking neural network models. However the limitation concerning the evaluation of the models predictive power with real neural data stands out. Incorporating empirical validation would significantly elevate the works credibility and impact. On the societal impact front given the increasing interest in leveraging neural network models for AI applications a discussion on the responsible use of such biologically inspired models would enrich the paper. This should include potential misuse privacy concerns and the importance of transparency in model development and application. In summary this paper represents a valuable contribution to the field of computational neuroscience pushing forward our understanding of efficient coding in neural networks that mirror the complex dynamics of biological brains. With further validation and extension the proposed model has the potential to significantly influence both theoretical research and practical applications in neuroscience artificial intelligence and beyond.", "rlN6fO3OrP": "Review of \"Exploring the Vulnerability of Continuous Prompts to Backdoor Attacks in FewShot Scenarios\" 1. Summary The paper undertakes an innovative exploration of the security challenges within the promptbased learning paradigm particularly focusing on its susceptibility to backdoor attacks in fewshot learning scenarios. Through meticulous analysis and experimentation the study introduces \"BadPrompt\" a novel algorithm designed for executing taskadaptive backdoor attacks on continuous prompt models. Leveraging trigger candidate generation and adaptive trigger optimization BadPrompt stands out by effectively inserting backdoors into models while ensuring high performance on clean datasets across various NLP tasks. Furthermore the provision of source code enhances the papers contribution by facilitating the validation of findings and encouraging further research in the field. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty: This paper addresses an understudied area in NLP security focusing on the vulnerability of continuous promptbased models to backdoor attacks which is both timely and significant due to the growing reliance on these models in various applications.  Methodology: The proposed BadPrompt algorithm is robust featuring a sophisticated approach to generating and optimizing triggers in a manner that balances attack success rate (ASR) with clean accuracy (CA). The adaptive trigger optimization in particular is a clever solution to the challenge of trigger effectiveness variability across samples.  Empirical Validation: The comprehensive experimental setup across five datasets and two model architectures provides extensive evidence of the effectiveness of BadPrompt especially when compared to baseline methods. The inclusion of clean accuracy and attack success rate as primary metrics offers a thorough analysis of the models performance. Weaknesses:  Generalization to Other Models: While the paper investigates the applicability of BadPrompt on RoBERTabased prompt models the generalizability to other PLMs and architectures such as GPT3 or T5 is not discussed. This limits the understanding of how widely applicable BadPrompt could be across the diverse landscape of NLP models.  Defensive Measures: The discussion on potential defenses against backdoor attacks facilitated by BadPrompt is somewhat limited. While the limitations section briefly mentions future exploration of defenses providing an initial investigation into the effectiveness of existing defense strategies would significantly enrich the papers contribution.  Societal Impact Consideration: Although the paper briefly touches upon the potential negative societal impacts of backdoor attacks a more thorough exploration of these implications particularly in highstakes applications would be valuable. 3. Questions and Suggestions for the Authors  Have the authors considered extending the evaluation of BadPrompts effectiveness across other PLMs or NLP frameworks beyond continuous promptbased models to gauge its broader applicability  It would be beneficial to include a more detailed discussion or exploration of defense mechanisms against backdoor attacks in continuous prompts. Could the authors speculate on the potential effectiveness of certain defense strategies based on their current findings  A suggestion for future work could be to explore the impact of varying the complexity and length of the triggers further. Specifically investigating the tradeoff between trigger invisibility (to humans and detection algorithms) and the effectiveness of the attack in scenarios with more stringent constraints on trigger obtrusiveness might yield interesting insights. 4. Limitations and Societal Impact While the paper makes a significant contribution to understanding the vulnerabilities of promptbased models the discussion on limitations and societal impact is somewhat brief. Expanding on the potential implications of these vulnerabilities in critical applications (e.g. legal healthcare) where trust in model outputs is paramount could enhance the paper. Additionally offering a more comprehensive exploration of possible defense mechanisms or preventive measures (both existing and speculative) would not only balance the narrative but also provide a starting point for addressing these vulnerabilities. Recommendation for Improvement: A more detailed consideration of the generalizability of findings across different PLMs an indepth discussion on defensive strategies and a broader exploration of the societal implications of backdoor attacks in NLP models would greatly enhance the papers impact and relevance.", "mzze3bubjk": " Review of the Paper  1) Summary of the Paper and Its Contributions This paper presents a rigorous exploration of computational hardness in highdimensional statistical inference problems. The authors focus on bridging two primary approaches to evidencing computational intractability: lowdegree hardness which is algebraic in nature and the freeenergy barriers approach which is geometric. Their significant contributions are twofold: first they formally define a freeenergybased criterion for computational hardness (the FranzParisi criterion) and successfully connect it to the concept of lowdegree hardness for a wide range of statistical problems including Gaussian additive models and models with a sparse planted signal. Second the authors use this connection to prove new lowdegree lower bounds for sparse linear regression a result they claim was difficult to establish via direct methods. These contributions not only enhance our understanding of the computational landscape of statistical inference problems but also introduce new technical tools for proving computational lower bounds.  2) Assessment of the Strengths and Weaknesses of the Paper Strengths:  Theoretical Foundation: The paper is strong in its theoretical foundation rigorously defining and connecting two major concepts in the study of computational hardness.  Novelty: The connection made between freeenergy criteria and lowdegree hardness especially in the context of Gaussian additive models is novel and insightful.  Impact: The implications for understanding the computational limits of highdimensional inference problems are profound providing both conceptual insights and technical tools for future research.  Clarity: Despite the complexity of the subject matter the paper is wellstructured and articulate making it accessible to readers familiar with the field. Weaknesses:  Practical Applications: The paper is heavily theoretical with less emphasis on practical applications or the implications of these findings in realworld scenarios.  Generalizability: While the authors mention the potential limitations of the FranzParisi criterion outside Gaussian Additive Models the exploration into the criterions broader applicability is left somewhat open which may limit the immediate utility of the findings.  Empirical Validation: The paper lacks empirical validation of the theoretical findings which could strengthen the claims and provide a more comprehensive understanding of the practical implications.  3) Questions and Suggestions for the Authors  On Generalizability: Can the authors provide more insight into how the FranzParisi criterion might be adapted or extended to apply more broadly across different models especially those not covered in this study  On Practical Implications: It would be beneficial if the authors could discuss potential practical applications of their findings particularly in how understanding these computational limits could influence the design of algorithms for highdimensional inference problems.  Empirical Validation: Are there plans to empirically validate the theoretical predictions made in this paper Such efforts could greatly enhance the papers impact and its relevance to a broader audience.  4) Addressing Limitations and Potential Societal Impact The authors do mention investigating the accuracy of the FranzParisi criterion beyond Gaussian Additive Models and acknowledge that the criterion can make erroneous predictions in simple discrete settings. This transparency about the limitations is appreciated but more proactive steps towards understanding and mitigating these limitations would be beneficial. In terms of societal impact the paper remains neutral focusing on theoretical advancements. However understanding computational hardness has implications for data privacy and security. It might be worthwhile for the authors to discuss in future work how these findings could influence the development of secure algorithms against adversarial attacks for example. Constructive Suggestions for Improvement:  Extend the exploration of the FranzParisi criterions applicability to other models potentially using simpler settings as stepping stones.  Incorporate a section discussing the broader societal implications of understanding computational hardness particularly in terms of privacy and security in statistical inference.  Consider conducting or suggesting empirical studies that could validate the papers theoretical findings thus bridging the gap between theory and practice.", "q41xK9Bunq1": " Peer Review of \"Neural Attentive Circuits (NACs): A General Purpose Modular Neural Architecture\"  1) Summary: This paper introduces Neural Attentive Circuits (NACs) a novel architecture aimed at blending the robustness and adaptability of modular neural network designs with the generality and scalability of generalpurpose models. NACs operate by dynamically learning both the parameterization of neural modules and their sparse connectivity bypassing the need for domainspecific knowledge. The architecture is organized into two main components: a circuit generator to determine module configuration and a circuit executor that processes input data according to this configuration. Through a series of experiments across various data modalities including image and point cloud classification symbolic processing and text classification from raw bytes the authors demonstrate NACs competitive performance in tasks requiring outofdistribution robustness and fewshot learning. Additionally NACs show an 8x inference speedup with minimal performance loss by pruning modules showcasing their computational efficiency.  2) Assessment:  Strengths:  Generalizability and Versatility: NACs are tested across a wide range of tasks and data modalities demonstrating their ability to operate as a truly generalpurpose architecture. This versatility is a significant advancement over existing models which are often tailored to specific kinds of data.  Modular Efficiency: The ability to dynamically prune modules without significant performance loss potentially offering a substantial reduction in computational resources is impressive. The 8x speedup at inference time is a notable achievement.  Innovative Approach to Modularity: The methodology for jointly learning module parameterization and connectivity is a novel contribution potentially opening new avenues for research in modular neural network architectures.  Weaknesses:  Lack of Explanation on Module Behavior: While the paper demonstrates the effectiveness of NACs it provides limited insight into how and why certain modules are activated for different tasks or data types which could offer further understanding of the models internal mechanisms.  Comparative Analysis: The paper could benefit from a more thorough comparison with existing modular architectures particularly in terms of quantitative performance metrics and computational resources required.  Implementation Details: Certain aspects of the architecture such as the specifics of the circuit generator and executor could be described in more detail to facilitate replication and further investigation by others in the field.  3) Questions and Suggestions:  Module Behavior Analysis: Can the authors provide more insight or analysis into how different modules are utilized across various tasks Understanding if certain modules specialize in specific types of processing could be enlightening.  Expanded Comparative Analysis: A more detailed comparison with existing architectures (both general and modular) might strengthen the paper. Specifically details on computational efficiency adaptation capabilities and performance in domainspecific tasks would be useful.  Clarification on Implementation: More detailed information on the implementation of key components of NACs such as the circuit generator and executor would aid in reproducibility and further exploration by the research community.  4) Limitations and Societal Impact: The paper briefly touches on the computational efficiency of NACs which is commendable. However a more indepth discussion on the broader limitations and any potential negative societal impacts would enhance the paper. For example while NACs are designed for general use considerations around bias fairness and environmental impact (e.g. energy consumption during training) are important areas to address. Constructive suggestions for improvement might include:  Incorporating mechanisms for identifying and mitigating biases that may arise due to the modular nature of NACs particularly when deployed in varying contexts.  Proposing strategies for reducing the environmental impact of training large generalpurpose models which could involve more efficient training techniques or leveraging sparsity beyond inference. Overall the paper presents an exciting advancement in the development of modular generalpurpose neural architectures. Addressing the highlighted questions and suggestions could further strengthen the work making it a substantial contribution to the field of neural networks.", "xOK40an4ag1": " Peer Review  1) Briefly summarize the paper and its contributions This paper explores the relationship between network dynamics and connectivity in Recurrent Neural Networks (RNNs) trained for tasks without any specific constraints on their connectivity. The authors demonstrate that despite the highdimensional connectivity in trained RNNs a lowdimensional functionally relevant subspace within the weight matrix can be identified. This subspace defined through operative dimensions has a significant impact on local network dynamics. The paper shows that a weight matrix constructed from a few operative dimensions is sufficient for the network to perform at the original level suggesting that much of the trained connectivity is functionally irrelevant. This discovery could simplify the process of linking connectivity to network dynamics and potentially help in avoiding catastrophic forgetting in continual learning by compartmentalizing independent network functions into specific subspaces.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Novelty and Insight: The paper introduces an innovative approach to understand and interpret the weight matrix of RNNs by identifying a functionally relevant lowdimensional subspace which is a significant contribution to neural network interpretability and flexibility.  Methodology: The methodology for identifying operative dimensions is wellconceived with a clear rationale and a stepbystep process that is logical and wellsupported by the analyses presented.  Applicability: The notion that a large proportion of the network connectivity is not required for task performance is both surprising and valuable offering new avenues for network optimization and understanding.  Clarification of Concepts: The paper does an excellent job of clarifying complex concepts and making them accessible contributing to both theoretical frameworks and practical applications in machine learning and neuroscience. Weaknesses:  Generalization of Findings: While the findings are quite compelling within the scope of the tasks and RNNs studied generalization to other types of neural networks or more complex tasks is not thoroughly examined.  Assumptions on Sampling Locations: The methodology assumes that sampling locations along condition average trajectories adequately cover relevant dynamics. This could limit the identification of operative dimensions in situations where this assumption does not hold.  Potential Overemphasis on LowDimensionality: The heavy focus on identifying and utilizing a lowdimensional subspace might overlook the significance or utility of the highdimensional aspects of network connectivity especially in tasks with complex dynamics or those requiring highdimensional computation.  3) Please list up and carefully describe any questions and suggestions for the authors Questions:  Could the authors comment on how the approach for identifying operative dimensions might be adapted or expanded to other forms of neural networks such as Convolutional Neural Networks (CNNs) or Transformers  How sensitive is the identification of operative dimensions to the choice of initialization parameters for the network weights Could different initialization strategies lead to markedly different subspaces being identified as functionally relevant Suggestions:  It would be beneficial to include a broader variety of tasks or models to demonstrate the versatility and general applicability of the proposed method for identifying operative dimensions.  Given the potential implications for network compression and continual learning some experimentation or discussion on these applications would greatly enhance the papers impact.  Limitations  Potential Negative Societal Impact The authors have addressed limitations particularly focusing on the assumption regarding sampling locations and generalization across task complexities. One suggestion to improve is to explore alternative methods for selecting sampling locations to ensure broad applicability and functionality. Additionally while increasing the interpretability of neural networks is generally positive caution should be exercised in how these insights are applied especially in privacysensitive applications. By showcasing whether and how private information can be protected or obscured within these dimensionally reduced spaces the paper could address potential negative societal impacts more directly.", "wKhUPzqVap6": "Review for \"Evaluating the Quality of Learned Representations in the Presence of Spurious Correlations\" 1. Summary: This paper presents an indepth study focusing on how well neural networks can learn nonspurious core features of data in the presence of spurious correlations a prevalent challenge in deep learning. The authors investigate the efficacy of standard Empirical Risk Minimization (ERM) and specialized group robustness training methods (including group distributionally robust optimization  group DRO) in learning highquality representations. Utilizing the Deep Feature Reweighting (DFR) technique the paper assesses the learned feature representations by retraining only the last layer of models on a dataset where the spurious correlation is absent. The experiments span across multiple datasets in image and text classification domains revealing that features learned via ERM can decode core features nearly as effectively as those learned via group robustness methods. Intriguingly the paper demonstrates that the model architecture pretraining strategy and design choices play more critical roles in learning quality representations than the training method itself. Leveraging these insights the study substantially improves upon previous benchmark results on challenging datasets like Waterbirds CelebA and WILDSFMOW. 2. Strengths and Weaknesses:  Strengths: 1. Comprehensive Evaluation: The paper undertakes a pragmatic and extensive evaluation of how deep learning models trained with and without group robustness methods encode core and spurious features. This contributes significantly to our understanding of feature learning in the presence of spurious correlations. 2. Robust Methodology: The use of DFR for the evaluation offers a novel perspective on assessing the quality of learned representations circumventing the confounding effects of the models last layers. 3. Significant Practical Implications: The findings highlighting that simple ERM with strategic model choices can match or outperform complex group robustness methods provide actionable insights for practitioners.  Weaknesses: 1. Limited Discussion on Limitations: While the paper discusses its findings in detail it lacks a thorough examination of the limitations or potential failure modes of its methodology particularly in more complex or less controlled environments. 2. Potential for Broader Dataset Exploration: The study focuses on a few datasets. Expanding the dataset diversity especially incorporating more complex and less studied spurious correlation scenarios could enrich the findings. 3. Societal Impact Considerations: The paper could benefit from a deeper analysis of potential negative societal impacts especially considering the increasing reliance on automated decisionmaking systems in critical domains. 3. Questions and Suggestions for the Authors:  Can the authors elucidate on how the models performance might vary with extremely imbalanced datasets where spurious features are dominantly represented  The paper could benefit from further discussion on the applicability of its findings across different model architectures especially emerging ones. Could the authors speculate or provide preliminary insights on this  A suggestion for future work could involve exploring the interaction between data augmentation techniques and the learning of spurious versus core features. Could such techniques exacerbate or mitigate reliance on spurious features 4. Limitations and Societal Impact: The paper presents a novel and insightful analysis yet could delve more into explicitly discussing its limitations such as potential biases introduced by the pretraining data or the DFR methods applicability across varied domains. Regarding societal impact while the paper takes a step forward in understanding spurious correlations addressing the broader implications of deploying models trained with these insights in realworld applications\u2014such as potential biases in automated decisionmaking\u2014would enhance its contributions. Constructively the authors might consider incorporating a section on ethical considerations such as bias amplification risks and mitigation strategies in future iterations.", "vK53GLZJes8": "Review of \"On the Ineffectiveness of Regularization in Mitigating Divergence in Offpolicy Temporal Difference Learning\" 1. Summary of the Paper This paper scrutinizes the efficacy of \\xe2\\x84\\x932 regularization in offpolicy Temporal Difference (TD) learning particularly under the conditions of the deadly triad\u2014TD learning combined with offpolicy sampling and function approximation. The authors challenge the prevailing assumption that regularization serves as a panacea for instability and unbounded errors in this context. Through a series of novel counterexamples it is demonstrated that regularization can fail to mitigate and in some cases exacerbate divergence in TD learning. The experiments cover scenarios with both linear function approximation and neural networks as function approximators underscoring the pervasiveness of the issue across different settings. The work calls for a reevaluation of regularizations role in TD methods advocating for more rigorous theoretical and practical examination of its application in reinforcement learning (RL). 2. Assessment of the Paper Strengths:  Novelty and Importance: The paper addresses a significant gap in the RL literature by rigorously challenging a widespread assumption about regularization in offpolicy TD learning. It advances our understanding of the limitations and potential pitfalls of commonly employed regularization techniques in RL.  Rigorous Analysis: The authors provide a comprehensive and mathematically sound analysis of the problems associated with regularization in the context of the deadly triad. The use of counterexamples is particularly effective illustrating the complexity of the issue in an accessible manner.  Relevance to the Field: By highlighting fundamental issues in widely used RL methods the paper has the potential to influence future research directions prompting the development of more robust approaches to offpolicy TD learning. Weaknesses:  Limited Exploration of Alternatives: While the paper effectively demonstrates the limitations of regularization it offers limited insight into potential solutions or alternative approaches that could address the identified weaknesses. The conclusion mentions adaptive regularization schemes and EmphaticTD as promising avenues but without a deeper exploration or guidance on these fronts the paper might leave readers seeking more actionable outcomes.  Generalizability of Counterexamples: The counterexamples are crucial for the papers argument but may also be seen as its limitation. Specifically it would be beneficial if the paper explored the extent to which these counterexamples generalize across a broader range of offpolicy TD learning scenarios. 3. Questions and Suggestions for the Authors  Generalization of Findings: Can the authors discuss the generalizability of their counterexamples to other common settings in offpolicy TD learning not covered in the paper Are there specific scenarios (e.g. different problem domains or types of function approximators) where regularization might still play a beneficial role  Potential Solutions: While acknowledging the papers focus on identifying the problem it would be insightful if the authors could expand on potential solutions. Are there other forms of regularization or entirely different approaches that could circumvent the identified issues Any preliminary ideas or ongoing work in this direction would be valuable.  Implications for RL Practice: Can the authors elaborate on the practical implications of their findings for RL practitioners How should these results inform the deployment and development of RL systems especially in critical applications where divergence can have significant consequences 4. Limitations and Societal Impact The paper clearly identifies crucial limitations in the application of regularization in offpolicy TD learning. However a discussion on the broader implications of these limitations particularly in terms of potential negative societal impacts is notably absent. For example systems that rely on RL algorithms for decisionmaking in critical domains (e.g. healthcare autonomous vehicles) could be adversely affected by the instability and divergence detailed in this work. A more explicit acknowledgment of these limitations and a call for careful responsible application of RL algorithms in sensitive areas could strengthen the papers societal impact considerations. Additionally suggestions for mitigating risks in practical implementations possibly by adopting more robust learning strategies or thorough validation and testing protocols would be a constructive addition. Overall the paper makes a significant contribution to the field by elucidating the limitations of regularization in offpolicy TD learning. Despite its strengths the paper could benefit from an expanded discussion on potential solutions and a clearer articulation of its practical implications and societal impacts.", "y-E1htoQl-n": "Title of the Manuscript: \"WocaRRL: A Framework to Improve the Robustness of Deep Reinforcement Learning Agents against Adversarial Perturbations\" 1) Summary: The paper introduces Worstcaseaware Robust RL (WocaRRL) a novel framework designed to enhance the robustness of deep reinforcement learning (RL) policies against adversarial attacks. Existing methods in this domain largely treat the adversarial robustness of deep RL either by regularizing the neural network to minimize the impact of perturbations on actions which may neglect the environments dynamics or by alternately training the RL agent and attacker which increases computational and sample demands. WocaRRL aims to address these issues through three primary innovations: a worstattack Bellman operator for estimating the policys lower bound under attacks policy optimization to preferentially select actions with higher worstcase rewards and valueenhanced state regularization to improve the networks tolerance to perturbations in critical states. The authors demonstrate through experimental evaluations that WocaRRL not only outperforms existing stateoftheart methods in terms of robustness but also in training efficiency across various environments. 2) Strengths and Weaknesses:  Strengths: a. Novelty and Importance: The approach of directly estimating the worstcase reward to improve policy robustness without needing to train an adversary is novel and significant. It addresses a critical gap in the efficiency of training robust RL agents. b. Comprehensive Evaluation: The paper presents an extensive set of experiments across different environments comparing WocaRRL with stateoftheart methods to demonstrate its superior robustness and efficiency. c. Theoretical Foundation: The introduction of the worstattack Bellman operator and its utilization for robust policy training is wellgrounded in RL theory and contributes to the methods effectiveness.  Weaknesses: a. Reliance on Convex Relaxation: The method\u2019s dependency on convex relaxation to approximate the set of adversarial actions might limit its capacity to identify the genuinely worstcase scenarios potentially affecting the robustness guarantees. b. Specific Threat Model Consideration: While `pnorm attacks are widely studied the approachs adaptability to other forms of adversarial threats such as patch attacks is not discussed. This limitation could narrow the methods applicability in realworld settings. 3) Questions and Suggestions for the Authors: a. Generalizability to Other Threat Models: Could the authors elaborate on potential adaptations of WocaRRL to address non`p threat models It would enhance the frameworks applicability to a broader range of realworld scenarios. b. Impact of Convex Relaxation Accuracy: How sensitive is WocaRRLs performance to the accuracy of the convex relaxation method used A discussion on this aspect could help in understanding potential limitations. c. Comparative Analysis on Computation Overhead: Can the authors provide a more detailed comparative analysis of the computational overhead of WocaRRL compared to methods that train an adversary This will help in quantifying the efficiency gains more precisely. 4) Limitations and Societal Impacts: The authors have not fully discussed the potential negative societal impacts of their work. While enhancing the robustness of RL agents against adversarial attacks is crucial a discussion on the ethics of robustness such as preventing misuse in applications where high robustness could be exploited for unethical purposes is missing. It would be beneficial for the authors to consider these aspects and suggest guidelines or countermeasures to mitigate potential negative impacts. Furthermore expanding on the limitation regarding the specific threat model and the assumption of convex relaxation would provide a clearer scope of the frameworks applicability and avenues for future improvement.", "vF3WefcoePW": " Peer Review for \"Training Differentiable Logic Gate Networks for Efficient Inference\"  1) Summary of the Paper This paper introduces a novel approach to train logic gate networks through the use of differentiable logic gate networks effectively enabling gradientbased optimization methods for architectures traditionally considered nondifferentiable. The authors propose using a combination of realvalued logic and a softmax parameterization to learn the probability distribution over the possible logic gates for each neuron in the network. This approach addresses the challenge of training logic gate networks that are inherently sparse and lack the realvalued parameters necessary for gradient descent. The key contribution is the successful training of such networks with a method that significantly accelerates inference speed while preserving a high level of accuracy demonstrating this through various datasets including MNIST and CIFAR10. The authors position their work as a potentially transformative approach for applications requiring fast inference speeds such as edge computing and embedded machine learning.  2) Assessment of Strengths and Weaknesses Strengths:  Innovation in Training Logic Gate Networks: The paper addresses a significant limitation in the training of logic networks through an innovative differentiable approach making gradientbased optimization feasible for this class of networks.  Significant Inference Speed Improvement: The method greatly outperforms existing architectures in inference speed offering a solution more than an order of magnitude faster than previous stateoftheart especially highlighted in the MNIST experiments.  Comprehensive Experimental Validation: The work is validated across several benchmark datasets showing its effectiveness not only in terms of inference speed but also maintaining competitive accuracy thereby proving its practical applicability.  Detailed Theoretical and Methodological Description: The paper provides a thorough explanation of the proposed approach including mathematical formulations and a comprehensive discussion on related work which aids in understanding the significance of the contribution. Weaknesses:  Lack of Exploration of Societal Implications: The paper does little to address any potential negative societal implications of deploying such efficient models especially in sensitive domains.  Limited Discussion on Training Cost: While inference speed is significantly improved there is less emphasis on the cost associated with training these models which the authors mention is higher than conventional neural networks.  Potential for Overfitting in Smaller Datasets: The manuscript does not thoroughly investigate the models performance on smaller datasets or its generalization capabilities outside the datasets evaluated.  3) Questions and Suggestions for the Authors  Generalization Capabilities: Can the authors provide insights or additional experiments on how the differentiable logic gate networks perform in terms of generalization on unseen datasets or smaller datasets to better understand the models applicability in realworld scenarios  Training Cost Analysis: A more detailed analysis comparing the training cost of differentiable logic gate networks with traditional neural networks would be beneficial. Can the authors provide computational time and resource usage statistics  Societal Impact Considerations: Given the significant improvement in efficiency and potential applications in various domains including sensitive ones like healthcare can the authors discuss any potential negative implications or ethical considerations  Extension to Other Types of Neural Networks: Are there plans or possibilities to extend this training methodology to other types of neural architectures such as convolutional or recurrent neural networks  4) Limitations and Potential Societal Impact The authors have not thoroughly addressed the limitations related to the higher computational cost of training these networks. Additionally the potential negative societal impacts such as misuse in surveillance technology due to the efficiency of inference are not discussed. A suggestion would be to include a section discussing these aspects in depth providing guidelines or recommendations on ethical considerations and potential safeguards against misuse. Additionally exploring strategies to reduce the training cost or techniques to improve the networks generalization capabilities could help in mitigating some of the outlined limitations.", "wUctlvhsNWg": "1. Summary of the Paper and Its Contributions This paper addresses the challenges in online federated multikernel learning (MKL) focusing on communication efficiency and heterogeneous data distribution among clients. The novel contribution is the Personalized Online Federated MKL algorithm (POFMKL) which leverages random feature (RF) approximation to reduce communication costs and computational complexity while providing personalized MKL models for each client. The authors present theoretical analysis showing that each client enjoys sublinear regret concerning the RF approximation of its best kernel indicating effective handling of data heterogeneity. Experiments on real datasets demonstrate the superiority of POFMKL in terms of accuracy and efficiency compared to existing online federated kernel learning algorithms. 2. Assessment of Strengths and Weaknesses Strengths:  Innovation: The paper introduces a novel POFMKL algorithm that addresses significant challenges in federated learning including communication efficiency and data heterogeneity among clients.  Theoretical Foundation: It provides a robust theoretical analysis proving sublinear regret for both clients and the server which is a significant achievement in federated learning.  Experimental Validation: The authors conduct comprehensive experiments on real datasets showing POFMKLs effectiveness compared to existing methods. Weaknesses:  Scalability Concerns: While demonstrating improvements over existing algorithms the paper does not fully explore the scalability of POFMKL when dealing with an extremely large number of clients and kernels.  Limited Discussion on Parameter Selection: The impacts of parameter choices (e.g. the number of random features D and the subset size M) on performance are not thoroughly discussed which could be crucial for practical implementations.  Privacy Considerations: Despite mentioning privacy improvement brought by POFMKL the paper lacks a detailed discussion on potential privacy risks and mitigation strategies. 3. Questions and Suggestions for the Authors  Scalability Analysis: Can the authors provide insights or additional results on how POFMKL scales with an increased number of clients and kernels This information would be valuable for understanding its applicability in larger federated learning scenarios.  Parameter Selection Guidelines: It would be beneficial if the paper included guidelines or a more detailed discussion on how to select the parameters especially M and D for achieving optimal performance in various settings.  Privacy Enhancements: Considering the growing concerns about privacy in federated learning could the authors elaborate on potential privacy threats and propose specific enhancements to POFMKL to mitigate these risks 4. Limitations and Suggestions for Improvement The authors have not fully addressed the scalability of POFMKL and the privacy implications inherent in federated learning. To improve the paper could benefit from:  Incorporating a scalability analysis with a larger set of clients and kernels to evaluate the algorithms performance and computational efficiency in more demanding scenarios.  Providing a more detailed discussion on selecting key parameters (M and D) that significantly influence the algorithms performance to aid practitioners in deploying POFMKL effectively.  Enhancing the discussion on privacy considerations by identifying potential vulnerabilities in POFMKL and proposing specific strategies or modifications to better protect participant data. Overall the paper delivers a significant contribution to federated multikernel learning addressing key challenges with innovative approaches supported by strong theoretical and empirical evidence. With improvements in scalability analysis parameter selection guidelines and privacy considerations the work could have a broader impact and applicability in federated learning research and applications.", "monPF76G5Uv": "Review of \"Globally Normalized Autoregressive Transducer (GNAT) for Streaming Speech Recognition\" 1. Summary This paper introduces the Globally Normalized Autoregressive Transducer (GNAT) a novel approach for addressing the label bias problem in streaming automatic speech recognition (ASR). The GNAT model operates by employing global normalization significantly reducing the word error rate (WER) gap between streaming and nonstreaming ASR models by more than 50 on the Librispeech dataset. The model is built within a modular framework that encompasses common neural ASR models like crossentropy models connectionist temporal classification (CTC) and others facilitating the creation of new models and controlled comparison of modeling choices. The authors have also opensourced a JAX implementation of their models. 2. Strengths and Weaknesses Strengths:  The paper addresses the critical issue of label bias in streaming ASR proposing an innovative solution that demonstrates significant improvement in WER performance.  The modular framework is a notable contribution enabling flexibility in experimentation and comparison among different ASR model architectures.  Opensourcing the JAX implementation provides a valuable resource for the community encouraging replication and further innovation based on the authors work. Weaknesses:  The paper could benefit from a more detailed exposition on the practical implications of switching from local to global normalization in terms of computational overhead and resource consumption.  It lacks an extensive discussion on the practicability of integrating GNAT into existing realworld applications where constraints on computational resources might limit the feasibility of adopting globally normalized models.  The evaluation mainly focuses on the Librispeech dataset additional experiments across diverse datasets could strengthen the generalizability of the proposed approach. 3. Questions and Suggestions for the Authors  Could you elaborate on the computational efficiency of GNAT particularly in comparison with locally normalized models considering both training and inference phases  It would be beneficial if the authors could discuss potential strategies for incorporating GNAT into resourceconstrained environments especially for edge devices.  Can the authors provide insights or preliminary results on the performance of GNAT across different languages or more diverse and challenging acoustic environments than Librispeech 4. Limitations and Societal Impact The authors have not thoroughly discussed the limitations and potential negative societal impacts of their work. While the advancements in ASR technologies facilitated by GNAT could lead to more accessible communication technologies enhancing inclusivity there is a lack of discussion on potential biases that might be amplified through global normalization techniques especially in underrepresented languages or dialects. Constructive suggestions for improvement include:  Conducting extensive bias and fairness assessment across diverse datasets to identify and mitigate potential biases that GNAT might introduce or exacerbate.  Exploring energy consumption implications of GNAT in deployment scenarios and seeking optimization strategies to ensure broader applicability including in lowresource settings. Overall this paper presents a significant step forward in addressing the label bias problem in streaming ASR offering both theoretical insights and practical advancements with the introduction of GNAT. However a more comprehensive understanding of its computational implications and strategies to ensure its fair and efficient application across various contexts would be beneficial.", "uOii2cEN2w_": " Peer Review for \"Bayesian Optimisation via DensityRatio Estimation (BORE) and Extensions to the Batch Setting\"  1) Brief Summary of the Paper This paper presents an innovative approach to Bayesian Optimization (BO) using densityratio estimation named BORE which reformulates the acquisition function as a probabilistic binary classifier. This allows for the elimination of an explicit prior over functions enhancing scalability. The authors introduce a theoretical analysis of BOREs regret and propose BORE an extension that incorporates improved uncertainty estimates. Furthermore the paper extends BORE to a batch optimization setting presenting algorithms with theoretical performance guarantees and demonstrating their efficacy through various experiments against traditional BO baselines.  2) Assessment of Strengths and Weaknesses  Strengths: 1. Theoretical Novelty: The reformulation of the acquisition function as a probabilistic binary classifier is a significant theoretical contribution. It elegantly simplifies the BO process by eliminating the need for an explicit prior over functions. 2. Scalability: The introduction of BORE and its extension BORE address the scalability issues present in traditional BO methods. This makes them suitable for highdimensional problems and large datasets. 3. Theoretical Guarantees: The paper provides a rigorous theoretical analysis of BORE and BORE including regret bounds. These guarantees lend credibility to the proposed methods and establish a strong foundation for future work. 4. Empirical Validation: Extensive empirical evaluations demonstrate the superiority of BORE and BORE over traditional GPbased and other nonGP BO baselines across various settings including realdata benchmarks.  Weaknesses: 1. Complexity of Implementation: The transition from traditional BO models to probabilistic classificationbased models may increase the complexity of implementation for practitioners unfamiliar with such methods. 2. Limited Exploration of Classifier Choices: While the paper provides an excellent foundation it could benefit from a deeper exploration into the impact of different classifiers on the performance of BORE and BORE. 3. Potential for Overfitting: The reliance on probabilistic classifiers especially in highdimensional settings raises concerns about overfitting. The paper could discuss strategies for mitigating this issue in more detail.  3) Questions and Suggestions for Authors 1. Impact of Classifier Choice: Can the authors provide insights into how different classifiers (e.g. support vector machines decision trees) impact BOREs performance Incorporating results from using various classifiers could enrich the analysis. 2. Overfitting Concerns: It would be beneficial if the authors could discuss potential overfitting issues when using probabilistic classifiers in highdimensional settings and suggest mitigation strategies. 3. Comparison with Recent BO Methods: How does BORE compare with the latest BO methods that also aim for scalability and handling highdimensional problems  4) Limitations and Societal Impact The paper does a commendable job in addressing the limitations and potential negative societal impacts of the work. However a more detailed discussion on the computational requirements and environmental impact (e.g. energy consumption for training complex models) of implementing BORE and BORE would be valuable. Suggesting ways to reduce these impacts could make this research more sustainable. Additionally exploring the ethical implications of deploying such optimization methods in sensitive applications (e.g. healthcare autonomous systems) could enhance the discourse on responsible AI development.  Conclusion The paper makes significant contributions to the field of Bayesian optimization by proposing BORE and its extension BORE. The theoretical insights combined with empirical evidence demonstrating improved scalability and performance over traditional BO methods position this work as a substantial advancement in optimization techniques. Additional exploration into classifier choices and discussions on mitigating overfitting and environmental impacts would further strengthen the paper.", "mTXQIpXPDbh": "Review of \"Back Razor: Asymmetric Sparse Transfer Learning for MemoryEfficient OnDevice Learning\" 1. Summary of the Paper The paper introduces Back Razor an innovative transfer learning framework designed to significantly reduce the memory footprint during the transfer process on edge devices. It achieves this by asymmetrically pruning the activations stored for backpropagation while maintaining dense forward activations. This approach is based on the insight that activations dominate memory usage and are only necessary for backpropagation. Back Razor is a plugandplay solution compatible with common neural network architectures including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). The authors provide a theoretical analysis of the convergence rate of Back Razor showing it retains a convergence rate comparable to vanilla Stochastic Gradient Descent (SGD) under certain conditions. Extensive experiments demonstrate that Back Razor achieves up to 97 sparsity and 9.2\u00d7 memory savings without compromising accuracy across various datasets and tasks. 2. Assessment of Strengths and Weaknesses  Strengths: 1. The Back Razor framework addresses a critical challenge in deploying deep learning models on memoryconstrained devices making it highly relevant for ondevice learning. 2. Providing a plugandplay solution that does not require changes to the original model architecture is a significant advantage for broader adoption. 3. The theoretical analysis adds rigor to the work solidifying the foundation of the proposed method. 4. The extensive experimental validation across different models tasks and datasets convincingly demonstrates the methods effectiveness and versatility.  Weaknesses: 1. The theoretical framework is limited to CNNs and does not fully encompass the challenges or peculiarities of applying Back Razor to ViTs where assumptions made for CNNs might not hold. 2. While the paper provides a compelling empirical case it lacks a deeper exploration of the impact of different sparsity levels on the various types of models and tasks beyond memory savings and general accuracy measures. 3. The discussion on the potential negative societal impacts of the work is missing which is crucial for a holistic understanding of its implications. 3. Questions and Suggestions for the Authors 1. Could the authors elaborate on how the Back Razor framework might impact the latency of backpropagation during training considering the additional computations required for pruning and reconstructing activations 2. It would be beneficial to include a comparison with techniques that offer similar memory savings but through different strategies such as model quantization or knowledge distillation to contextualize Back Razors advantages and limitations more fully. 3. A more detailed discussion on the choice of pruning thresholds would help understand the balance between sparsity and performance. Are there any guidelines or heuristics for selecting these parameters to achieve optimal results 4. Limitations and Potential Negative Societal Impact While the authors successfully demonstrate Back Razors efficacy in reducing the memory footprint for transfer learning the limitations are primarily discussed concerning the theoretical frameworks applicability to CNNs but not ViTs. Expanding the theoretical underpinnings to include ViTs would strengthen the work. Additionally potential negative societal impacts such as the misuse of memoryefficient models for malicious purposes or the environmental cost of deploying more models on edge devices were not addressed. Constructive suggestions include providing a more thorough consideration of these aspects possibly outlining ethical guidelines for the use of such technology or suggesting mechanisms to mitigate its negative impacts.", "wGF5mreJVN": " Peer Review  1) Brief Summary of the Paper This paper introduces a novel approach for web navigation on a graph structure using Wikipedia as a case study. The authors propose a method based on behavioral cloning from randomly generated trajectories to learn effective navigation policies. The approach converts Wikipedia into a graph with nodes representing web pages (paragraphs) and edges representing hyperlinks. By training a policy that learns to navigate from any starting node to a designated target node the authors demonstrate high success rates in navigating between nodes that are 5 to 20 steps apart. Moreover they show that this navigation capability significantly enhances performance in downstream tasks such as fact verification and question answering (QA) offering competitive results against stateoftheart methods without sophisticated retrieval and reranking mechanisms.  2) Assessment of the Strengths and Weaknesses Strengths:  Novel Approach: The paper presents a unique methodology for navigating the web by learning from the structure of Wikipedia addressing a common shortfall of search engines with a solution that intelligently navigates hyperlinks.  Scalability: The methods applicability to a graph structure as large as Wikipedia (38M nodes and 387M edges) and its potential scalability to the general web is impressive.  Enhanced Performance in Downstream Tasks: The paper demonstrates the utility of the navigation model in improving the efficiency and accuracy of fact verification and QA tasks outperforming existing searchbased methods.  Substantive Experimental Evaluation: The extensive experiments and comparisons with baselines and previous works provide a solid validation of the methods effectiveness. Weaknesses:  Limitation to Wikipedia: While the results on Wikipedia are promising the paper does not provide evidence of the methods performance on the wider internet which may have more complex and less structured link dynamics.  Dependency on Behavioral Cloning: The reliance on behavioral cloning from randomly generated trajectories might not capture optimal navigation strategies particularly in more complex or adversarial web environments.  Lack of Discussion on Computational Resources: The paper does not discuss the computational resources required for training and deploying this model which could be significant given the scale of the graph.  3) Questions and Suggestions for the Authors 1. Generalization to Other Domains: Have the authors considered or tested how this approach might generalize to other domains or the broader web beyond Wikipedia Any insights into challenges and potential solutions would be valuable. 2. Comparison with StateoftheArt: While the paper mentions competitive results against stateoftheart methods in downstream tasks a more detailed comparison highlighting the differences in methodology accuracy and computational efficiency would provide clearer insights into the advantages and tradeoffs of the proposed approach. 3. Handling of Web Dynamics: The web is a dynamic entity with pages and links constantly being added removed or changed. How might the proposed navigation policy adapt to such changes Would it require retraining on a regular basis 4. Resource Consumption: Can the authors provide some details on the computational resources required for training their model as well as the latency involved in the realtime application of the navigation policy  4) Limitations and Societal Impact The paper briefly mentions the works scalability and applicability to general web settings but does not address the limitations of transitioning from a relatively structured environment like Wikipedia to the broader more chaotic web. A discussion on how this approach could handle dynamically changing web content and structure would strengthen the paper. Additionally while the potential for enhancing information retrieval tasks is clear the paper lacks a dedicated section discussing any negative societal impacts such as privacy concerns or misuse in navigating to and distributing harmful content. Potential safeguards or ethical considerations for deploying such technology would be beneficial to address. Suggestions for Improvement:  Expanding the discussion on potential challenges and solutions for applying this method beyond Wikipedia.  Including a detailed comparison with stateoftheart methods in terms of methodology performance and resource consumption.  Providing insights into how the model might adapt to the dynamic nature of the web and discussing the computational resources required for implementation.  Adding a section dedicated to discussing the potential negative societal impacts and ethical considerations of the technology.", "z9poo2GhOh6": "Title: Dynamics of Large Batch SGDM on Least Squares: A Volterra Equations Perspective 1) Brief Summary and Contributions: The paper presents an analytical study of the behavior of stochastic gradient descent with momentum (SGDM) on highdimensional least squares problems. By considering large batch sizes relative to the dataset size and highdimensional feature spaces the authors derive that the dynamics of SGDM can be accurately described by a deterministic discrete Volterra equation in the limit of large dimensions. This framework allows the introduction of the Implicit Conditioning Ratio (ICR) a novel stability metric that delineates two distinct operating regimes for SGDM: a large batch regime where convergence rates match those of fullbatch momentum methods and a small batch regime where performance aligns more closely with that of singlebatch SGD. Additionally explicit learning rate and momentum parameter selections are provided to optimize convergence within these regimes. The analytical findings are supported by empirical results on synthetic and realworld datasets highlighting the practical relevance and applicability of the theoretical analyses. 2) Assessment of Strengths and Weaknesses: Strengths:  Novel Analytical Framework: The use of deterministic discrete Volterra equations to analyze the dynamics of SGDM in highdimensional settings is novel and provides a solid mathematical foundation for understanding SGDM behavior.  Introduction of ICR: The Implicit Conditioning Ratio (ICR) is a significant contribution offering a clear and theoretically grounded way to differentiate between large and small batch regimes and their impact on convergence rates.  Practical Relevance: The provision of explicit parameter choices for optimizing SGDM performance substantiated by empirical validations demonstrates the practical utility of the theoretical insights enhancing the papers relevance to machine learning practitioners. Weaknesses:  Assumption Specificity: The results are obtained under specific conditions (orthogonal invariant data matrix high dimensions) which might not be universally applicable across all least squares problems encountered in practice. This could limit the generalizability of the findings.  Complexity of Theoretical Analysis: The heavy reliance on advanced mathematical constructs (Volterra equations ICR) might make the paper less accessible to a broader audience including practitioners seeking to apply these insights without a deep mathematical background. 3) Questions and Suggestions for the Authors:  Empirical Validation on More Diverse Datasets: While the paper presents empirical results to support the theoretical findings the scope of datasets (isotropic features model and MNIST) is somewhat limited. Could the authors explore and report the performance of their proposed method on a broader range of datasets particularly those with nonorthogonal invariant features  Impact of Noise and Initialization: The paper assumes specific initialization and noise conditions. How sensitive are the convergence properties and the efficacy of the ICR to deviations from these conditions Some sensitivity analysis could enrich the discussion.  Comparison to Other Momentum Methods: Given the focus on SGDM it would be beneficial for readers if the authors could compare the performance and dynamics of SGDM as analyzed in the paper with other popular momentumbased optimization methods such as SGD with Nesterov momentum in the context of largescale least squares problems. 4) Limitations and Societal Impact: The paper provides a comprehensive analysis of the limitations associated with the assumptions made (high dimensions orthogonal invariance) but could further discuss potential mitigation strategies for when these conditions are not met. Regarding societal impact while the paper focuses on a technical mathematical analysis with specific applications in machine learning it would be helpful for the authors to speculate on broader implications of their work including potential negative consequences (e.g. biased optimization outcomes due to the model assumptions). Suggesting guidelines for responsible application of their findings could enhance the papers contribution to the machine learning community.", "pqCT3L-BU9T": " Peer Review of \"Matformer: Periodic Graph Representation Learning for Crystal Material Property Prediction\"  1) Summary The paper introduces a novel transformer architecture Matformer designed for representation learning on periodic graphs specifically targeting crystal materials. Unlike typical graphs periodic graphs model the repeating unit cell structure inherent to crystals posing unique challenges such as the need for periodic invariance and the explicit encoding of repeating patterns. The Matformer addresses these challenges by encoding periodic patterns using geometric distances between the same atoms in neighboring cells and ensuring the models output is invariant to different orientations translations and periodic boundary shifts of the crystal lattice. The approach significantly outperforms existing baseline methods on various material property prediction tasks across two common benchmark datasets. The work contributes to the advancement of material science by facilitating the discovery of new materials with desired properties through more accurate crystal representation learning.  2) Assessment of Strengths and Weaknesses Strengths:  Innovation: The paper addresses unique challenges in periodic graph representation through novel methods like periodic invariance and explicit encoding of repeating patterns which are not thoroughly explored in existing literature.  Experimental Results: The Matformer demonstrates superior performance over baseline methods across different materials property prediction tasks indicating its effectiveness.  Practical Relevance: Accurate crystal material representation is crucial for material discovery which has implications in various fields including renewable energy and electronics. Weaknesses:  Model Complexity: While the paper mentions the Matformers superior efficiency compared to the ALIGNN model there is limited discussion on the overall complexity training times and hardware requirements for the model.  Generalization: The experiments focus primarily on crystal material property prediction. It remains unclear how the proposed techniques could generalize to other types of periodic graph representation tasks outside material science.  Limitations Discussion: The discussion on potential negative societal impacts is very brief. Further elaboration on this aspect including how these impacts might be mitigated could improve the paper.  3) Questions and Suggestions for the Authors  Model Interpretability: Can the authors provide insights into how the model makes predictions and whether there are any interpretable features or aspects of the Matformer  Scalability: How does the Matformer scale with increasing graph size or complexity A discussion on the scalability of the approach would be beneficial.  Comparative Analysis: It would be interesting to see a more indepth comparison with baseline methods specifically focusing on where those methods fail and how Matformer addresses those failures.  Extension to Other Domains: Do the authors believe that modifications to Matformer could make it applicable to other periodic structures beyond crystal materials such as in biological systems or engineered nanostructures  4) Limitations The authors mention potential negative societal impacts but do not delve into specifics. As materials science could significantly impact various aspects of society and the environment an indepth analysis of these impacts and how the new capabilities introduced by Matformer might be used responsibly would be valuable. Suggestions for improvement include a detailed discussion on the broader impacts of accelerated material discovery and the establishment of ethical guidelines for the use of advanced predictive models in material science. Overall the paper presents a significant advancement in periodic graph representation learning with its novel Matformer architecture. Addressing the questions and suggestions outlined could further strengthen the paper making it an important contribution to the field of computational material science.", "u3vEuRr08MT": "Review of the Paper on Memorization and Forgetting Dynamics in Language Models 1. Summary of the Paper This paper provides an analytical exploration of the memorization and forgetting dynamics in deep learning specifically within the domain of language modeling. It focuses on how these dynamics evolve across varying model sizes and throughout the training process. The contributions can be summarized in three primary findings: larger language models memorize training data more rapidly and forget less over time a detailed study on the memorization rates of different parts of speech and an empirical demonstration that nouns and numbers are memorized faster likely serving as unique identifiers for training data memorization. These insights contribute to an enhanced understanding of the implications of scaling up model size on memorization and forgetting in language models. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty and Depth of Analysis: The paper addresses a relatively underexplored area in the general understanding of deep learning models by quantifying the memorization and forgetting mechanisms. The detailed analysis including controlled experiments and the consideration of various influencing factors such as model size dataset size and parts of speech provides valuable insights into language model dynamics.  Empirical Evidence: The study is rich in empirical data that support its claims. By utilizing several model sizes and configurations along with rigorous experimentation the paper effectively demonstrates the trends and patterns in memorization and forgetting.  Relevance and Implications: The findings have significant implications for both theory and practice including understanding overfitting model generalization and privacy concerns associated with large language models. These implications are crucial for guiding future model development and training strategies. Weaknesses:  Generalization of Results: While the study provides indepth analysis on specific language models further research is needed to generalize these findings across different types of neural networks and their applications beyond language modeling.  Limitation on Data and Model Diversity: The paper primarily relies on two datasets and a certain category of models (Transformers). Expansion to diverse datasets and other neural network architectures could enhance the findings robustness.  Potential Overemphasis on Model Size: The pivotal role of model size in memorization and forgetting dynamics is wellargued but the discussion might benefit from a broader consideration of training strategies regularization techniques and architecture optimizations that could also influence these dynamics. 3. Questions and Suggestions for the Authors  Clarification on PartofSpeech Memorization: It would be beneficial to expand upon why nouns and numbers are thought to act as unique identifiers more so than other parts of speech. Further experiments or theoretical backing could enrich this argument.  Exploration of Forgetting Mechanisms: The paper introduces the concept of a forgetting curve. Can the authors discuss potential mechanisms behind the observed slower rate of forgetting in larger models  Investigation of Bias and Stereotypes Memorization: Given the mention of biases in training data does the faster memorization rate of larger models exacerbate the risk of memorizing and perpetuating biases If so do the authors have suggestions for mitigating this risk 4. Addressing Limitations and Societal Impact The paper briefly touches upon the implications of its findings for privacy and bias yet a more thorough analysis of these limitations would strengthen its contribution. It is crucial to delve deeper into the potential negative impacts such as privacy violations through data memorization and the perpetuation of biases present in training data. Suggestions for future research could include developing methodologies for measuring and mitigating these risks such as differential privacy techniques fairnessaware model training procedures and more transparent model auditing methods. Furthermore a discussion on the tradeoffs between model size memorization capabilities and societal impacts would provide a balanced perspective on scaling language models.", "x26Mpsf45P3": "Review of \"Approximating Bellman Equations with Test Functions for ModelFree Offline Reinforcement Learning\" 1. Summary of the Paper This paper presents a novel approach within the area of modelfree offline reinforcement learning (RL) focusing on approximation of the Bellman equations through the empirical orthogonalization against a set of userdefined test functions. The core contributions of the work are twofold:  Firstly a method is proposed to construct confidence intervals and perform policy optimization by approximating the Bellman residual along selected test functions.  Secondly the paper introduces a samplebased approximation of this principle which incorporates selfnormalization and regularization to offer general guarantees applicable to both parametric and nonparametric problem settings. The significance of these contributions lies in their departure from traditional importance sampling and regressionbased methods for offline policy evaluation and optimization. By employing diverse choices of test function spaces the authors demonstrate the versatility and broad applicability of their approach across several domains providing novel results and theoretical guarantees under conditions where past methods like Bellman closure do not hold. 2. Strengths  Theoretical Rigor: One of the papers outstanding strengths is its theoretical depth. The authors provide a rigorous statistical analysis proving an oracle inequality for their policy optimization procedure and establishing highprobability guarantees.  Novelty: The proposed method introduces a novel perspective for tackling offline RL problems specifically through the lens of weighted learning algorithms and test function spaces. This approach is both innovative and compelling offering a fresh direction for the field.  Broad Applicability and Generality: The paper does an excellent job of illustrating the versatility of its approach by applying it to a range of domains and scenarios. This not only showcases the methods robustness but also its potential to address various challenges in modelfree offline RL.  Empirical and Implementation Insights: The exploration of linear function approximation and the assurance of polynomialtime implementations offer practical implications for the adoption of these methods in realworld applications bridging the gap between theoretical analysis and practical implementation. 3. Weaknesses  Empirical Validation: The paper predominantly focuses on theoretical contributions without providing substantial empirical evidence to support the effectiveness of the proposed methods in practice. Adding empirical results especially comparisons with stateoftheart approaches could strengthen the paper.  Accessibility and Clarity: While the theoretical analysis is thorough the density of the material and the complexity of formulations might be challenging for a broader audience. Simplifying explanations or providing more intuitive illustrations could help in making the paper more accessible. 4. Questions and Suggestions for the Authors  Empirical Evaluation: Can the authors provide empirical results or case studies that demonstrate the practical efficacy of their approach compared to existing methods It would be beneficial to see how the theoretical guarantees translate into realworld performance.  Choice of Test Functions: The selection of test functions appears to be pivotal to the approach. Could the authors elaborate on the criteria or guidance for choosing or designing these test functions for different application domains  Extension to NonLinear Settings: While the paper explores linear function approximation in depth how do the authors envision extending or adapting their approach to more complex nonlinear function approximations 5. Limitations The paper does not thoroughly address limitations regarding empirical validation and potential challenges in the practical implementation of the proposed methods. Acknowledging these aspects and suggesting directions for empirical research could provide a more balanced view. Additionally addressing potential negative societal impacts particularly biases introduced by the choice of test functions or data representation would enhance the paper\u2019s thoroughness. Constructive Suggestions for Improvement  Empirical Validation: Including experiments or simulations that highlight the practical advantages or challenges of the proposed methods would substantially enrich the paper.  Accessibility: Providing a more gradual introduction to the theoretical concepts possibly supplemented with illustrative examples or visual aids could enhance the papers accessibility.  Discussion on Limitations: A more detailed discussion on the practical challenges limitations and potential societal impacts of the proposed methods would provide a more balanced and comprehensive presentation of the work. Overall the paper is an exciting and theoretically robust contribution to the field of reinforcement learning with the potential for significant practical impact pending empirical validation.", "r-CsquKaHvk": "Summary: The paper introduces a novel approach to survival analysis in dynamic settings by leveraging temporal consistency inspired by temporaldifference learning from reinforcement learning. The authors propose algorithms that estimate a discretetime survival model by considering sequences of states and their survival distributions. This methodology which can be integrated with any parametric survival model and handles rightcensored observations efficiently is shown to offer higher sample efficiency and predictive performance over traditional regressionbased methods. The work bridges two academic domains incorporating dynamic programming concepts from reinforcement learning into the survival analysis framework with demonstrated applicability to both synthetic and realworld datasets. Strengths: 1. Innovative Approach: The papers novel integration of temporaldifference learning principles with survival analysis addresses the limitations of previous methods by improving sample efficiency and exploiting sequential datas dynamic nature. 2. Theoretical Foundation: The introduction of a temporalconsistency condition and its application through a recursive algorithm provides a solid theoretical foundation contributing significantly to the fields understanding of dynamic survival analysis. 3. Empirical Validation: The comprehensive evaluation on both synthetic and realworld datasets not only validates the proposed methodologys effectiveness but also demonstrates its practical utility in realworld scenarios. 4. Interdisciplinary Contribution: By synthesizing insights from reinforcement learning and survival analysis the paper paves the way for crossdisciplinary innovations and applications potentially impacting a wide range of domains. Weaknesses: 1. Assumption Clarity: The paper assumes sequences of states follow a Markov chain without extensive discussion on the implications or limitations of this assumption particularly in complex realworld scenarios where this condition may not hold accurately. 2. Implementation Details: While the empirical evaluation is robust there is a lack of detail regarding implementation challenges or considerations which could be vital for practitioners attempting to replicate or adapt the method. 3. Limitations and Societal Impact: The discussion on potential negative societal impacts especially pertaining to privacy and ethical considerations in medical applications is insufficient. A more thorough examination of these aspects including data sensitivity and bias would strengthen the paper. Questions and Suggestions for the Authors: 1. Markov Assumption: Could you discuss the robustness of your method in situations where the Markov assumption may not strictly apply Are there modifications or extensions that could accommodate such scenarios 2. Algorithm Complexity: How does the computational complexity of your algorithm compare to traditional methods especially in large state spaces or highdimensional data 3. Generalization to Nonbinary Events: Your method focuses on survival analysis. Could it be extended to accommodate competing risks or multistate models offering a more detailed view of possible outcomes 4. Bias and Data Sensitivity: Given the application of your method to sensitive areas like patient data what measures do you suggest to mitigate potential biases or ensure data privacy Limitations and Potential Negative Societal Impact: The authors do not fully address the limitations related to the Markov chain assumption and its potential inaccuracies in complex reallife data. Moreover the discussion on the potential negative societal impacts particularly in sensitive applications such as healthcare is absent. To improve the authors should elaborate on the implications of their assumptions explore the algorithms applicability beyond ideal conditions and examine ethical considerations data privacy and bias mitigation strategies in their methodologys application.", "o8vYKDWMnq1": "The paper tackles the theoretical study of deep neural function approximation in reinforcement learning (RL) specifically within the framework of \u03b5greedy exploration under an online setting. It elucidates on deep reinforcement learning (RL) beyond the linear regime focusing on valuebased algorithms with \u03b5greedy exploration via deep and twolayer neural networks. The main contributions include:  Providing theoretical insights into how the width and depth of neural networks should be scaled for learning with sublinear regret in Besov spaces and for twolayer neural networks in Barron spaces.  A novel approach for estimating the temporal difference error under deep neural function approximation translating it to a generalization problem under the noniid setting.  Establishing a relationship between the problemdependent smoothness of the Qfunction and regret bounds offering guidance on deep RL architecture design. Strengths: 1. Novelty of Theoretical Analysis: The thorough theoretical analysis bridges the gap between practical deep RL applications and their theoretical underpinnings. It notably extends beyond the typically studied linear regime into more complex nonlinear function approximations. 2. Relevance and Impact: Given the widespread application of deep RL in various domains understanding the theoretical basis is critical for the design of more efficient and effective learning algorithms. This works findings could significantly impact how future deep RL architectures are designed particularly in terms of network width and depth. 3. Comprehensive Methodology: The use of Besov and Barron spaces for articulating the problem is noteworthy. The rigorous problem setup and the consequent proofs provided for sublinear regret under certain conditions add depth to the analysis. Weaknesses: 1. Complexity and Accessibility: The paper is highly technical which might limit its accessibility to a broader audience. Simplifying the presentation or providing more intuitive explanations could enhance its reach. 2. Empirical Validation: While the theoretical contributions are substantial the lack of empirical validation might leave readers wondering about the practical applicability of the proposed theories. Including simulations or realworld experiments that corroborate the theoretical findings would strengthen the paper. 3. Discussion on Limitations: While the paper mentions assumptions such as independence of stateaction pairs through experience replay a more thorough discussion on the practical implications of these assumptions and any potential limitations they might introduce would be beneficial. Questions and Suggestions: 1. Empirical Validation: Can the authors provide empirical evidence or simulations that demonstrate the practical efficacy of their theoretical models Such validation could significantly enhance the credibility and applicability of the findings. 2. Comparison with Existing Methods: How do the proposed theoretical insights compare with existing deep RL exploration strategies in practical settings A discussion comparing the \u03b5greedy exploration with confidencebound based schemes or other exploration policies could provide further context. 3. Impact of Assumptions: The paper assumes the independence of stateaction pairs facilitated by experience replay. How robust are the theoretical findings to violations of this assumption Discussing the sensitivity of the results to this and other assumptions would add depth to the analysis. Limitations and Societal Impact: The paper briefly touches upon the assumptions necessary for its theoretical framework but could benefit from a deeper exploration of potential limitations such as the feasibility of the independence assumption in realworld applications and the scalability of the proposed methods. Additionally discussing the potential negative societal impacts is largely absent. Given deep RLs application in critical decisionmaking areas understanding how inaccuracies in function approximation or misalignment in exploration strategies could lead to unintended consequences is crucial. Suggestions for mitigating these risks would be valuable.", "kuJQ_NwJO8_": "The paper proposes SUbgraph Retrievalaugmented GEneration (SURGE) a novel framework for generating dialogue responses that are consistent with and relevant to the given context leveraging structured knowledge from Knowledge Graphs (KGs). This framework addresses the limitations of pretrained generative language models in producing contextually and factually accurate responses by first retrieving relevant subgraphs from KGs and then ensuring consistency across generated responses through a graphtext multimodal contrastive learning technique. The performance of SURGE is validated on the OpendialKG dataset demonstrating its superior ability in generating highquality knowledgeconsistent dialogues compared to existing methods. Strengths: 1. Novel Approach: The paper introduces a unique technique for improving dialogue generation that combines graph retrieval and contrastive learning not extensively explored in prior work. 2. Improved Performance: The SURGE framework significantly outperforms baseline models in generating responses that accurately reflect the knowledge from KGs as evidenced by various metrics including BLEU ROUGE and a novel Knowledgeverifying Question Answering (KQA) metric. 3. EndtoEnd Trainability: SURGE is trained endtoend enabling the joint optimization of subgraph retrieval and response generation which is a noteworthy feature over models that handle these aspects separately. 4. Comprehensive Evaluation: The paper provides a thorough evaluation including ablation studies that demonstrate the contribution of each component of SURGE. The addition of human evaluation further strengthens the credibility of the proposed method. Weaknesses: 1. Dataset Limitation: The primary validation is conducted on a single dataset OpendialKG. Extending the evaluation to more datasets could help establish the generalizability of the approach. 2. Lack of Comparison with Latest Models: While the paper compares SURGE against several baselines it does not benchmark against the very latest models in knowledgegrounded dialogue generation potentially limiting the understanding of its relative performance. 3. Complexity and Computational Cost: The increased complexity of SURGE involving subgraph retrieval and contrastive learning likely leads to higher computational costs. However the paper does not thoroughly discuss the tradeoffs between performance gains and computational efficiency. Questions and Suggestions for Authors: 1. Broader Applicability: Can SURGE be easily adapted for languages other than English or for domainspecific dialogue generation tasks 2. Scalability Concerns: How does SURGE perform in terms of computational efficiency and scalability especially when dealing with largescale KGs or extensive dialogues 3. Extended Benchmarks: To further validate the effectiveness of SURGE can the authors provide comparison results with more recent stateoftheart models 4. Limitations and Societal Impact: While the paper briefly mentions limitations and societal impact in the supplementary material a more detailed discussion within the main text regarding potential misuse or ethical considerations would be valuable. Limitations and Societal Impact: The authors have addressed the limitations and societal impact of their work in the supplementary material however a brief mention in the main text regarding the potential negative implications of erroneous or biased output especially in sensitive applications would enrich the paper. Additionally considering the computational intensity of SURGE suggesting ways to mitigate environmental impact could be a meaningful improvement. Recommendations for future work on enhancing the efficiency robusting against biases in KGs and exploring applications beyond dialogue systems would also be beneficial.", "t3X5yMI_4G2": "Review of \"Reincarnating Reinforcement Learning: A Workflow to Leverage Past Computational Work for Accelerated Model Development\" 1. Summary: The paper posits a significant shift from the traditional tabula rasa approach in reinforcement learning (RL) by introducing and developing reincarnating reinforcement learning (RRL). This alternative workflow aims to leverage past computational work such as learned policies and datasets to expedite the training process in RL. Focusing on the transfer of suboptimal policies to standalone valuebased RL agents the authors highlight the limitations of existing approaches and propose the QDagger algorithm as a remedy. Demonstrations on various platforms\u2014Atari 2600 games a complex locomotion challenge and a realworld task involving stratospheric balloons\u2014showcase RRLs capacity to outperform the conventional tabula rasa methodology suggesting a more resourceefficient pathway for future RL research and application. 2. Assessment: Strengths:  The conceptual shift towards RRL is innovative addressing the critical inefficiency and resourceintensity of the prevailing tabula rasa approach in RL. This is likely to resonate well with a wider research community facing computational resource constraints.  The paper is comprehensive with empirical demonstrations that span simulations and a realworld application providing a robust validation of the proposed RRL framework and QDagger algorithms effectiveness.  Opensourcing the code and trained agents aligns with the ethos of democratizing RL research facilitating replication and further exploration of the concept. Weaknesses:  While the paper is comprehensive in its coverage the sheer breadth might have come at the expense of depth in certain areas particularly in the theoretical underpinnings of the QDagger algorithm and the precise mechanics of its superiority over existing methods.  The discussion on societal impacts though present feels cursory. Given the potential of RRL to reshape how RL research is conducted a deeper analysis of its broader implications including how it might shift the power dynamics in AI research fields and its potential for misuse would be beneficial.  The limitations section could benefit from a more critical examination of the scenarios where RRL might not be applicable or where the transfer of past computational work could introduce biases or inefficiencies into new models. 3. QuestionsSuggestions:  Could the authors elaborate on the theoretical foundations of the QDagger algorithm potentially providing a more formal analysis comparing it to other methods  Considering the vast potential applications of RRL can the authors discuss any safeguards or considerations to prevent the perpetuation or amplification of biases from past computational work  A deeper exploration into the failure modes of RRL and QDagger would be informative. Under what conditions does RRL underperform compared to tabula rasa RL and how might these scenarios be mitigated 4. Limitations and Societal Impact: While the authors briefly touch upon the potential positive societal impacts and limitations of RRL a more detailed exploration could enhance the paper. Specifically addressing how RRL might contribute to the concentration of capabilities within certain entities possessing extensive computational work archives or discussing strategies to ensure the ethical use of RRL would add depth. Suggestions for improving this aspect could involve including a framework or guidelines for ethical considerations in RRL applications and a more nuanced discussion on the computational and environmental tradeoffs involved. In summary the paper makes a compelling case for reincarnating RL presenting a paradigm that could very well redefine future pathways in reinforcement learning research and application. Filling in the gaps mentioned especially around theoretical clarity and societal impact could further solidify its contributions and applicability.", "wwyiEyK-G5D": "Peer Review of \"Revisiting Visual Representation in KnowledgeBased Visual Question Answering\" 1. Summary of the Paper The paper presents a novel approach named REVIVE aimed at enhancing visual representation in knowledgebased Visual Question Answering (VQA). Building upon the observation that current stateoftheart methods either overlook the importance of object regions in images or do not effectively utilize visual features in answering models REVIVE incorporates objectcentric visual representations throughout both the knowledge retrieval and answer generation stages. To achieve this the authors introduced a method that utilizes explicitly identified object regions for more effective knowledge retrieval and integrates these regional visual features with contextual and external knowledge within a transformerbased answering model. The experimental validation on the OKVQA dataset demonstrates that REVIVE surpasses previous methods achieving stateoftheart performance with a significant margin. 2. Assessment of Strengths and Weaknesses Strengths: a. Novel Approach: REVIVEs emphasis on using both explicit information of object regions in knowledge retrieval and integrating these visual features into the final answering model is innovative and addresses a gap in knowledgebased VQA. b. Empirical Results: The significant improvement in performance over previous stateoftheart methods on a standard dataset underlines the effectiveness and potential of the proposed method. c. WellExplained Methodology: The detailed explanation of the methodological steps involved in REVIVE from regional feature extraction to knowledge retrieval and encodingdecoding for answer generation provides clarity and insight into the approach. Weaknesses: a. Limited Discussion on Limitations: While the authors briefly mention the impact of the constructed Wikidata subset and the detectors quality on performance a more thorough discussion on intrinsic limitations potential failure modes and mitigation strategies would add depth to the analysis. b. Potential Bias and Ethical Considerations: The paper lacks a critical examination of the societal and ethical implications of deploying such VQA systems especially considering biases embedded in training datasets that could propagate through the models predictions. 3. Questions and Suggestions for the Authors a. The paper would benefit from a deeper dive into how REVIVE might handle visually complex scenes where multiple object regions might contain overlapping or relevant information for a single question. How does the model prioritize or integrate data from these regions b. Could the authors elaborate on how REVIVE might adapt to different domains or datasets with varying visual and knowledge complexity c. A discussion on the computational efficiency of REVIVE especially in comparison to previous models would be insightful. Given the enhanced use of regional visual features what are the implications for computational resources and response times d. Regarding the broader impacts could the authors reflect on potential mitigation strategies for biases in training data and propose approaches to make the models predictions more equitable across diverse visual and cultural contexts 4. Limitations and Potential Negative Societal Impact The authors have briefly mentioned the influence of the Wikidata subset quality and detector performance on model outcomes which hints at broader limitations in generalizability and reliability. Further the paper does not adequately address the potential for perpetuating biases present in the training data which can lead to unfair or stereotypical representations in the models answers. These limitations could have negative implications especially in applications directly impacting users like educational tools or interactive AI systems. Suggestions for improvement include incorporating a detailed examination of failure cases attributed to biases and proposing a framework for regular bias and fairness assessments in different application contexts. This could help in identifying and mitigating biases not only in the dataset but also in the models architectural and operational framework.", "qSs7C7c4G8D": "Review of \"Unified Convergence Analysis for Federated Learning with Arbitrary Client Participation\" 1) Summary: This paper addresses the important challenge in federated learning (FL) concerning intermittent client availability and the impacts of partial client participation on convergence properties. The authors introduce a generalized version of the Federated Averaging (FedAvg) algorithm that includes a novel mechanism for amplifying parameter updates after a specified number of FL rounds. They present a unified convergence analysis framework that considers arbitrary client participation patterns obtaining competitive upper bounds for the convergence rates under a broad set of participation scenarios including both nonstochastic and stochastic cases. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: The generalized FedAvg algorithm that amplifies updates over multiple rounds addresses an underexplored aspect of FL contributing to the fields theoretical foundations.  Comprehensive Analysis: The paper provides a detailed and rigorous convergence analysis offering universal bounds that apply to a wide range of client participation patterns. This is a significant advancement over existing studies that either focus on idealized scenarios or provide less competitive results.  Insights and Recommendations: The discussions surrounding various insights such as implications for nonIID data and linear speedups and the clear set of recommendations based on theoretical findings add great value for both theoreticians and practitioners in FL. Weaknesses:  Assumptions: Some of the assumptions made such as the mathematical convenience in assuming clients compute local updates regardless of participation may not fully capture the practical constraints of FL deployments potentially limiting the applicability of the conclusions drawn.  Complexity of Implementation: The proposed methods practicality might be hindered by the complexity of correctly choosing the amplification interval \\(P\\) and other algorithm parameters for different FL scenarios without detailed guidance.  Empirical Validation: While the paper does include experimental results the scope and scale of these might be insufficient to fully validate the theoretical claims especially considering the diversity of FL applications and environments. 3) Questions and Suggestions for the Authors:  Have the authors considered exploring how deviations from the assumptions (e.g. when clients\u2019 data distributions are severely nonIID) might affect the convergence rates of the generalized FedAvg  It would be beneficial to provide more detailed guidance on selecting the amplification interval \\(P\\) in practice possibly supported by sensitivity analyses under various FL scenarios.  Consider expanding the empirical evaluation to include realworld datasets and more complex models which could provide deeper insights into the practical performance and limitations of the proposed method. 4) Limitations and Societal Impact: The authors do not explicitly address the potential negative societal impacts of their work. While the paper is primarily theoretical considerations around the ethical use of federated learning particularly in handling sensitive data and ensuring privacy would enhance its relevance. Furthermore an explicit discussion on the limitations related to the assumptions and practical applicability of the proposed algorithm would also be valuable. Suggestions for Improvement:  Expand the discussion of potential negative societal impacts particularly focusing on privacy concerns and how the proposed method could be adapted or misused in sensitive applications.  Include a more thorough consideration of the limitations of the study particularly regarding the assumptions made and their implications for the practical deployment of the proposed FL method.", "n6QYLjlYhkG": "Peer Review of \"HYPRO: A Hybridly Normalized Neural Probabilistic Model for LongHorizon Prediction of Event Sequences\" 1. Summary of the Paper This paper addresses the challenging task of making longhorizon predictions of event sequences which is crucial in various domains like healthcare urban planning and commerce. Previous models being autoregressive struggled with this task due to cascading errors in predictions. The authors introduce HYPRO a novel model combining an autoregressive base with an energy function to reweight predicted sequences enhancing the realism of predictions. They propose efficient training objectives and inference algorithms significantly outperforming stateoftheart models in experiments. The methodology includes comprehensive ablation studies to understand the contribution of each model component. 2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The introduction of a hybrid model that combines autoregressive and energybased components is notably innovative. This addresses the cascading error problem inherent in traditional autoregressive models making a substantial contribution to the field.  Comprehensive Evaluation: The paper presents a thorough evaluation on multiple datasets demonstrating the models superiority over existing methods. Including a range of ablation studies adds depth to the findings substantiating the effectiveness of each component in the HYPRO model.  Practical Relevance: The application domains highlighted point to the practical relevance of the research addressing realworld needs in healthcare urban planning and ecommerce among others.  WellStructured Training and Inference: The training objectives and inference method proposed are not only novel but are also shown to be computationally efficient which is critical for adoption in practical settings. Weaknesses:  Model Complexity: While the hybrid approach is innovative it potentially introduces complexity in terms of understanding implementing and tuning the model. The balance between the autoregressive model and the energy function might require delicate tuning which could limit accessibility for some practitioners.  Limitations in Diverse Applications: Though the paper presents experiments in a few domains its uncertain how the model performs across a wider range of event sequence prediction tasks beyond those tested.  Assumption on Base Model and Energy Function Compatibility: The effective combination of the base autoregressive model and the energy function presumes a degree of compatibility and synergy between the two that may not always be present especially with different data characteristics. 3. Questions and Suggestions for the Authors  Model Interpretability: Can the authors discuss the interpretability of the HYPRO model especially in terms of understanding how the energy function reweights predictions  Broader Application: Could the authors provide insights or preliminary results on how HYPRO performs in other domains not covered in the experiments such as music or dialogue systems  Energy Function Variants: Have alternative energy functions been considered or tested to assess the robustness of the model to different energy formulations  Impact of Noise: How sensitive is HYPRO to noise in the input data given its dual components and what strategies could potentially mitigate such sensitivity 4. Limitations and Societal Impact The authors have made an effort to address limitations particularly through their ablation studies showcasing the models robustness across different configurations and datasets. However discussions on the potential misapplications or negative societal impacts of the model particularly in sensitive areas like healthcare are lacking. This absence is a missed opportunity for encouraging responsible AI development. Suggestions for Improvement:  Detailing Model Complexity and Parameter Tuning: Providing additional guidance or discussion on handling model complexity and parameter tuning could make the model more accessible.  Exploring Broader Applications: A section discussing potential applications in other domains even theoretically would broaden the papers impact and relevance.  Addressing Societal Impact: Incorporating a discussion on ethical considerations and potential negative impacts could enrich the paper making it a more comprehensive and responsible piece of research. In summary the paper presents a significant advancement in the domain of event sequence prediction with its novel HYPRO model. Despite its strengths opportunities remain to enhance its accessibility applicability and responsible use discourse.", "nE6vnoHz9--": "Review of \"CrossDomain Predictor for Efficient Neural Architecture Search\" 1. Summary of the Paper The paper presents a novel CrossDomain Predictor (CDP) aimed at addressing the high search cost associated with Neural Architecture Search (NAS). CDP utilizes existing NAS benchmark datasets to predict highperformance architectures within largescale search spaces. The key innovation lies in applying a progressive subspace adaptation strategy alongside the creation of an assistant space significantly reducing the search cost to 0.1 GPU Days for stateoftheart performance on ImageNet and CIFAR10. The authors claim this approach circumvents the need for extensive computational resources traditionally required for annotating new architectures thus democratizing access to efficient NAS solutions. 2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The CDPs utilization of existing benchmark datasets to predict performance in new architectures marks a significant step towards efficient NAS. This method cleverly leverages preexisting resources reducing the need for computationally expensive training from scratch.  Progressive Subspace Adaptation: The strategy to mitigate the domain discrepancy through progressive subspace partitioning and the integration of an assistant space is both logical and effective. It ensures a smoother transition and adaptation process which is supported by empirical results.  Empirical Evidence: The paper provides compelling empirical evidence demonstrating CDPs superiority in finding highperformance architectures quickly and efficiently compared to existing methods. Weaknesses:  Scalability and Generality Concerns: While CDP shows impressive performances on ImageNet and CIFAR10 the paper lacks discussion on its scalability and performance across a broader range of tasks and datasets. The varied nature of datasets in realworld applications might present unforeseen challenges.  Detail on Assistant Space Design: The concept of an assistant space is innovative but the paper could benefit from a more detailed explanation of how these spaces are designed and validated as effective intermediaries.  Comparison with StateoftheArt: While the paper compares CDP with existing NAS methods and demonstrates superior efficiency a more exhaustive comparison including computational overhead beyond GPU Days might provide a more comprehensive view of its practicality. 3. Questions and Suggestions for Authors  Scalability: How does CDP perform when tasked with searching architectures for more complex or diverse datasets beyond ImageNet and CIFAR10  Assistant Space Design: Can the authors provide further insights into the criteria for designing assistant spaces Are there specific guidelines or metrics that dictate the optimal configuration of these spaces  Comparative Analysis: Can the authors expand on the comparative analysis by including factors like energy consumption memory requirements and scalability across different hardware environments  Future Directions: What are the authors views on extending CDPs applicability to other domains beyond image classification such as object detection or semantic segmentation 4. Limitations and Societal Impact The paper briefly touches on the limitations but does not extensively discuss the broader implications of deploying CDP in varying computational environments or its adaptability to different tasks. Including a discussion about the limitations in scaling CDP to more complex tasks or datasets and exploring the potential negative societal impacts (e.g. perpetuating biases present in benchmark datasets) could strengthen the paper. Constructive suggestions include exploring adaptive mechanisms that allow CDP to dynamically adjust based on task complexity or diversifying benchmark datasets to minimize biases.", "ksVGCOlOEba": " Peer Review for \"An Efficient and Unified Framework for PostTraining Deep Neural Network Compression\"  1. Briefly summarize the paper and its contributions The paper addresses the problem of deep neural network (DNN) model compression in a posttraining context where a trained model must be compressed efficiently using a small amount of calibration data and without retraining. The authors propose a new framework that unifies weight pruning and quantization for compression utilizing the classical Optimal Brain Surgeon (OBS) technique and extending it to handle modern DNN scales and inclusion of weight quantization. This framework named the Optimal Brain Compressor (OBC) significantly improves the accuracycompression tradeoffs in comparison to existing methods and demonstrates the feasibility of combining pruning and quantization effectively in a onestep posttraining phase. The technical novelty significant performance improvements over stateoftheart methods and practical applications in model deployment on hardware are highlighted as key contributions.  2. Thorough assessment of the strengths and weaknesses of the paper Strengths:  Novelty and Technical Contribution: The extension of the OBS framework to include quantization and its adaptation to modern DNN sizes without losing accuracy represents a significant technical advancement. The proposed method fills a crucial gap in the literature by providing a unified and efficient solution for posttraining compression.  Practical Significance: The ability to compress DNN models in a posttraining context without retraining and with limited calibration data is highly relevant for realworld applications especially considering the constraints of deploying models on edge devices.  Experimental Validation: Comprehensive experiments across different models and tasks demonstrate the effectiveness of the approach. The experiments not only show superior performance compared to existing methods but also explore practical scenarios of model deployment. Weaknesses:  Complexity and Computation Cost: Although the paper addresses computational efficiency the actual resource requirements especially in terms of time and computational power (GPU usage) could be prohibitive for very large models. The paper could benefit from a clearer discussion on this aspect including potential scalability issues.  Generalizability: While the results are impressive for the models and tasks explored it remains unclear how well the framework generalizes across a broader range of models including the latest generation of DNNs that might have different architectural features.  Lack of Comparison with Knowledge Distillation: As an alternative to pruning and quantization knowledge distillation is another popular technique for model compression. A comparison or discussion on how the proposed method stands in relation to knowledge distillation approaches would provide a more rounded assessment of its position within the model compression landscape.  3. Questions and suggestions for the authors  Scalability Concerns: Can the authors provide more insights into the scalability of the proposed method especially when applied to stateoftheart models with billions of parameters  Generalization Across Architectures: How would the framework perform with newer or less common DNN architectures such as Transformerbased models or models with more exotic layer types  Comparison with Knowledge Distillation: Could the authors discuss how their framework compares with knowledge distillation techniques both in terms of compression efficiency and the preservation of accuracy  4. Limitations and societal impact The paper touches upon the limitations related to the computational cost and the potential infeasibility for extremely large models but could benefit from a deeper discussion on scalability. Moreover addressing the generalizability of the framework to a wider array of DNN architectures would strengthen the paper. Regarding societal impact while the paper primarily focuses on the technical aspects a brief consideration of the potential negative impacts such as the misuse of compressed models for malicious purposes could be warranted. Suggestions for future work include exploring the adaptation of the framework for a broader range of architectures and comparing it with alternative compression techniques like knowledge distillation. Overall the work presented in this paper represents a significant step forward in the domain of posttraining DNN model compression offering valuable insights and practical solutions for research and applications in this area.", "pBpwRkEIjR3": "Review of \"Enhanced Bilevel Optimization via Bregman Distance Methods (BiOBreD SBiOBreD and ASBiOBreD)\" 1) Summary: This paper introduces a novel approach to solve bilevel optimization problems particularly those with a nonconvexstronglyconvex structure possibly incorporating nonsmooth regularization. The centerpiece of this method is the utilization of Bregman distance leading to the development of three specific algorithms: BiOBreD for deterministic problems SBiOBreD for stochastic scenarios and an accelerated variant ASBiOBreD employing variance reduction techniques to further enhance computational efficiency. The authors provide thorough convergence analysis demonstrating that their algorithms not only achieve but exceed the best known computational efficiencies for such optimization problems under certain conditions. Empirical evaluations involving data hypercleaning and hyperrepresentation learning tasks further showcase the superior performance of these proposed methods against existing optimization algorithms. 2) Strengths and Weaknesses: Strengths:  Novelty: The integration of Bregman distance into the bilevel optimization framework particularly in addressing nonconvexstronglyconvex problems with potential nonsmooth regularization presents a novel and potentially impactful approach.  Comprehensive Theoretical Analysis: The paper offers a thorough convergence analysis for all proposed methods providing solid theoretical underpinnings for the algorithms efficacy.  Empirical Validation: Utilizing realworld tasks like data hypercleaning and hyperrepresentation learning for benchmarking not only validates the algorithms practical relevance but also demonstrates their superiority over existing methods. Weaknesses:  Complexity of Algorithms: While the authors have made strides in reducing computational complexity the practical implementation of these algorithms especially in terms of setting hyperparameters (e.g. stepsizes minibatch sizes) might still be challenging for endusers.  Generalization of Results: The empirical evidence though strong is based on a limited set of tasks. Additional experiments across variegated domains could provide a more robust testament to the algorithms versatility and effectiveness.  Discussion on Nonsmooth Regularization: The paper addresses nonsmooth regularization within the proposed methods framework a significant feature. However a deeper exploration or examples showcasing this aspects benefits would enrich the readers understanding and applicability of the methods. 3) Questions and Suggestions:  Hyperparameter Sensitivity: Could the authors elaborate on the sensitivity of algorithm performance to hyperparameter settings especially the stepsizes and minibatch sizes Any guiding principles for their selection would be beneficial.  Expansion to Other Domains: How transferable are these algorithms to other machine learning domains or optimization problems not covered in the paper It would be insightful to see discussion or preliminary results if explored.  Handling Nonsmooth Regularization: Further clarification or examples on handling nonsmooth regularization within the proposed methods would be appreciated. Specifically how the Bregman distance aids in this regard compared to other approaches. 4) Limitations and Societal Impact: The authors have made commendable efforts in addressing the limitations inherent in bilevel optimization particularly regarding computational complexity. However discussions on the limitations stemming from the practical implementation of their proposed algorithms such as computational resource requirements for largescale problems or potential issues in convergence behavior due to improper hyperparameter tuning are somewhat lacking. As for the societal impact while the paper focuses on the methodological advancements in optimization a brief discussion on potential negative implications such as misuse in adversarial machine learning scenarios could provide a more holistic view. Nevertheless the enhanced efficiency and applicability to tasks like data cleaning and representation learning suggest positive contributions towards more robust and accurate machine learning models. Suggestions for Improvement: 1. Incorporate a more detailed discussion on the practical challenges and limitations associated with implementing the proposed algorithms. 2. Extend the experimental evaluation to a broader range of tasks or domains to underline the algorithms general applicability. 3. Provide additional insights or examples on leveraging nonsmooth regularization within this optimization framework aiding in a comprehensive understanding of its benefits and implementation. Overall this paper presents a significant methodological contribution to the field of bilevel optimization offering novel algorithms supported by strong theoretical and empirical evidence. With minor enhancements in discussion and exploration the work holds potential for wideranging impacts in optimization and machine learning research communities.", "xvZtgp5wyYT": "PeerReview Report: 1) Brief Summary: The paper introduces Latent Evolution of PDEs (LEPDE) a novel method for simulating and optimizing the time evolution of Partial Differential Equations (PDEs) in largescale systems. The key innovation is the use of a global latent space to model system dynamics avoiding the need to update each discretized cell at every timestep which is a significant computational bottleneck in both traditional solvers and existing deep learning surrogate models. By developing a method to evolve system dynamics in a compact latent space and utilizing learned models for evolution LEPDE achieves remarkable speedups and reduction in dimensionality required for updates while maintaining competitive accuracy. The paper also introduces new learning objectives for more stable and accurate longterm prediction and presents techniques for efficient inverse optimization of boundary conditions applied to benchmarks including 1D nonlinear PDEs and 2D NavierStokes flows. 2) Strengths and Weaknesses: Strengths:  Innovation: The approach of using a global latent space for evolving PDE dynamics is novel and addresses a critical computational issue in PDE simulation and optimization.  Efficiency: Demonstrated speedups and dimensionality reductions are significant with up to 128x reduction in dimensions to update and up to 15x improvement in speed.  Comprehensive Evaluation: The methodology is evaluated across a range of benchmarks including both forward simulation and inverse optimization tasks comparing favorably against stateoftheart methods. Weaknesses:  Generalizability: While the method shows impressive results on the benchmarks tested its not clear how well LEPDE would generalize to other types of PDEs or more complex boundary conditions not evaluated in the study.  Implementation Details: The paper could benefit from more detailed description of the model architecture and hyperparameter settings to aid in reproducibility.  Discussion on Limitations and Societal Impact: The discussion on the limitations and potential negative societal impacts of the work is brief and could be expanded. 3) Questions and Suggestions for the Authors:  Generalizability: Could the authors elaborate on the potential of LEPDE to other classes of PDEs and its limitations in handling more complex systems or boundary conditions  Model Details: More information on the architecture and hyperparameters used in experiments would be appreciated for clarity and reproducibility.  Scalability: How does the method scale with the increase in system complexity and dimensionality Are there computational or memory bottlenecks  Societal Impact: Could the authors discuss in more detail any potential negative societal impacts particularly if the technology could be misused or if there are ethical considerations in its application in certain domains 4) Limitations and Societal Impact: The paper briefly mentions limitations and potential societal impact but could benefit from a more thorough analysis. Specifically there is little discussion on the scalability of the method to extremely highdimensional or complex systems beyond the benchmarks tested. Moreover the potential for negative societal impacts is not deeply explored. For instance the technology might be used in simulations that have dualuse implications such as military applications or environmental manipulation. As a constructive suggestion the authors could incorporate a discussion section that more deeply considers these aspects perhaps suggesting guidelines for ethical use or identifying areas where further research is needed to mitigate potential downsides.", "pF8btdPVTL_": " Peer Review:  1) Summary of the Paper: This paper addresses the phenomenon of benign overfitting in the context of training twolayer convolutional neural networks (CNNs) through an indepth theoretical analysis. The work pivots on the observation that under certain conditions on the signaltonoise ratio (SNR) a CNN can achieve low training and test losses embodying benign overfitting. Conversely with inadequate SNR conditions the network succumbs to harmful overfitting marked by constant level test losses. The authors introduce a novel proof technique based on signalnoise decomposition shedding light on the learning dynamics of CNNs by characterizing the separate contributions of signal learning and noise memorization. This framework leads to a nuanced understanding of the conditions under which neural networks navigate the delicate balance between fitting and overfitting. The paper extends the discourse on benign overfitting beyond the confines of linear models and kernel methods marking a significant stride in the theoretical exploration of neural network learning dynamics.  2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper offers a breakthrough theoretical analysis of benign overfitting in deep learning a topic of substantial importance yet largely unexplored within the context of CNNs.  Novel Proof Technique: The signalnoise decomposition technique is innovative and provides a fresh perspective on analyzing nonconvex optimization problems potentially applicable to deeper networks.  Clarity and Depth of Analysis: The theoretical findings are substantial and presented with commendable clarity offering deep insights into the learning behavior of neural networks under varying conditions of data quality. Weaknesses:  Model Specificity: The analysis is confined to twolayer CNNs with specific architectural constraints which might limit the generalization of the findings to more complex networks or different architectures.  Assumption on Data Generation Model: The synthetic data generation model while useful for theoretical analysis might not fully capture the complexities of realworld datasets. This could impact the relevance of the theoretical findings to practical applications.  Empirical Evaluation: The paper lacks empirical validation of the theoretical results. Including experiments that corroborate the theoretical predictions could significantly strengthen the papers impact.  3) Questions and Suggestions for the Authors:  Generalization to Deeper Networks: Could the authors discuss the potential pathways (and challenges) in extending their analysis to deeper convolutional networks Are there specific obstacles that might impede the direct application of the signalnoise decomposition approach  RealWorld Data Applications: How might the findings translate to CNN training on realworld datasets It would be beneficial if the authors could provide insights or hypothesize about the implications of their work in practical settings.  Empirical Validation: Are there plans to empirically validate the theoretical predictions Experiments demonstrating benign overfitting in practical scenarios would greatly complement the theoretical contributions.  4) Limitations and Suggestions for Improvement:  The scope of analysis being limited to a specific model architecture is a notable limitation. Suggestion: Future work could aim to generalize the findings to a broader range of neural network architectures possibly leveraging the same proof concept.  The absence of empirical evidence to support the theoretical claims is a weakness. Suggestion: Incorporating experiment results possibly on synthetic data aligned with the theoretical model or on realworld datasets would substantiate the theoretical insights and demonstrate their applicability.  While the paper thoroughly discusses the implications of the signaltonoise ratio on benign and harmful overfitting the analysis of potential negative societal impact is missing. Suggestion: Reflecting on how misinterpretations or misapplications of benign overfitting might lead to overreliance on overparameterized models in sensitive applications and offering guidelines to mitigate such risks would be a valuable addition.  Conclusion: The paper makes a significant theoretical contribution to understanding the phenomenon of benign overfitting in the context of CNNs. By elucidating the conditions under which neural networks can learn effectively without succumbing to overfitting the work paves the way for more nuanced training strategies. However broadening the scope of the analysis to more complex networks and providing empirical validation could further elevate the works impact and relevance to practical deep learning applications.", "lhl_rYNdiH6": " Peer Review on \"Contrastive Graph Structure Learning via Information Bottleneck for Recommendations\"  1) Summary This paper introduces a novel model named Contrastive Graph Structure Learning via Information Bottleneck (CGI) tailored for recommendation systems. The CGI model addresses two critical problems in Graph Convolution Networks (GCNs) used for recommendations: popularity bias and interaction noise. Popularity bias refers to the disproportionate attention received by popular items or active users while interaction noise comes from inaccurate or misleading useritem interactions. Existing methods often rely on random or predefined graph augmentation techniques for mitigating these issues potentially capturing irrelevant information for the recommendation task. CGI however adaptively learns to drop nodes or edges to optimize graph structures integrating an Information Bottleneck (IB) in the contrastive learning process. This approach aims to minimize the capture of irrelevant information while enriching the final recommendationrelevant representation. The paper provides extensive experiments demonstrating CGIs superiority over existing baselines on multiple datasets.  2) Assessment Strengths:  Innovative Methodology: The integration of learnable graph augmentation with Information Bottleneck (IB) for minimizing irrelevant informations inclusion in recommendation models is a novel and promising approach that addresses the fundamental limitations of existing GCNbased models.  Comprehensive Experimental Validation: The paper conducts a series of extensive experiments to validate the models effectiveness including comparisons with stateoftheart methods ablation studies and evaluations against various challenges like popularity bias and noise robustness. This thorough validation process strengthens the papers claims.  Significant Practical Relevance: The models ability to reduce popularity bias and interaction noise in recommendations has immediate practical applications in improving the user experience across recommendation systems making the paper highly relevant to the current technological landscape. Weaknesses:  Lack of Theoretical Insights: While the experimental results are strong the paper could benefit from a deeper theoretical analysis of why and how the proposed CGI model outperforms other methods. Such insights could help in understanding the models behavior under different conditions.  Potential Overfitting Concern: The models complexity with learnable graph augmentations and the Information Bottleneck principle might lead to overfitting especially on smaller datasets. An analysis or discussion around this potential issue would be beneficial.  Generality of the Approach: The paper focuses on recommendation systems but it remains unclear how easily the proposed CGI model can be adapted or extended to other domains within graphstructured data analysis.  3) Questions and Suggestions  Can the authors elaborate on how the CGI model would perform on dynamically changing graphs such as evolving social networks or timebased recommendation systems  It would be interesting to see a discussion on the computational complexity of CGI especially in comparison to the baseline models to better understand its scalability.  A detailed analysis or commentary on how the model behaves with extremely sparse or dense data sets could provide further insights into its applicability and limitations.  Given the significant impact of hyperparameters (especially in complex models like CGI) could the authors provide more details on their selection process or considerations for tuning them effectively  4) Limitations and Societal Impact While the authors provide a comprehensive evaluation of the CGI model the discussion on potential limitations and negative societal impacts is sparse. Specifically the paper could address:  Model Complexity and Computation Resource Requirements: Given the adaptive learning of graph structures and the integration of the Information Bottleneck principle CGI may require significant computational resources limiting its applicability in resourceconstrained scenarios.  Bias and Fairness Considerations: The paper discusses alleviating popularity bias but it does not thoroughly explore potential biases introduced by the model itself or how it might impact minority groups within the recommendation system. Addressing these concerns is crucial for ensuring fair and unbiased recommendations.  Providing specific suggestions for improving these aspects could further enhance the papers contribution. For instance incorporating fairnessaware components in the model design or establishing resourceefficient variants of CGI could be potential directions for future work. Overall this paper presents a significant advancement in leveraging graph convolution networks for recommendation systems by intelligently addressing popularity bias and interaction noise. With some enhancements in theoretical understanding discussion on potential limitations and exploration of broader impacts the paper could have an even more substantial contribution to the field.", "rAVqc7KSGDa": " Peer Review for \"Stochastic Active Sets for Scaling Gaussian Process Decoders\"  1) Summary of the Paper and its Contributions The paper presents a novel methodology to improve the scalability and efficiency of Gaussian Process Latent Variable Models (GPLVMs) by revisiting active set approximations. The authors introduce Stochastic Active Sets (SAS) an approach that provides a stochastic estimate of the logmarginal likelihood demonstrating its advantage in robustness and computational efficiency compared to traditional methods using inducing points. The integration of crossvalidation insights and stochastic optimization makes it possible to apply GP decoders to largescale datasets with enhanced robustness and efficiency. Contributions of the work include:  Development of a stochastic estimate for the logmarginal likelihood for GP decoders.  Introduction of the SAS approach to approximate this metric efficiently.  Empirical demonstration of superior performance compared to existing methods in terms of representation learning scalability and computational cost.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper addresses a significant challenge in GPLVMs regarding scalability and computational efficiency. The introduction of SAS is both novel and convincingly demonstrated to be effective.  Robust Methodology: The methodology combining insights from stochastic optimization and crossvalidation is solid. The stochastic nature of SAS is welljustified making the training of GP decoders more feasible for large datasets.  Empirical Evidence: The empirical comparisons with stateoftheart methods particularly the variational autoencoder showcase the practical advantages of the proposed SAS approach in realworld scenarios. Weaknesses:  Limited to Gaussian Likelihoods: The assumption of a Gaussian likelihood restricts the applicability of the proposed method to continuous data potentially limiting its usefulness for discrete or categorical datasets.  Lack of Theoretical Justification: Though the empirical results are strong the paper lacks a theoretical analysis of the proposed methods properties such as convergence guarantees or conditions under which it outperforms alternative approaches.  3) Questions and Suggestions for the Authors  Extension to NonGaussian Likelihoods: Given the limitation to Gaussian likelihoods could the authors speculate on how SAS might be adapted to work with nonGaussian likelihoods particularly for discrete data  Theoretical Underpinnings: Can the authors provide any theoretical insights or guarantees regarding the convergence properties of the SAS approach This would greatly strengthen the case for its robustness.  Comparison with Deep Learning Approaches: The paper highlights that SASbased GP decoders can achieve representations on par with neural networkbased models. Can the authors elaborate on situations where SAS might outperform deep learning models or where it might fall short  4) Limitations and Potential Negative Societal Impact The authors address the limitations of their work notably the focus on Gaussian likelihoods which restricts the methods applicability to continuous data. However broader discussions on the potential negative societal impacts are lacking. For instance the misuse of efficient GP decoders in privacysensitive applications could be problematic. Suggestions for Improvement:  The authors might consider extending their discussion on limitations to include potential negative societal impacts such as privacy concerns or biases that might emerge when the method is applied to socially sensitive datasets.  Exploring extensions or modifications to accommodate other types of likelihoods could make the methodology more universally applicable and valuable to a broader audience. Overall the paper presents a significant contribution to the field of GPLVMs specifically in improving the scalability and efficiency of GP decoders. Addressing the mentioned weaknesses and considering the suggestions could further enhance the papers impact and applicability.", "wOI0AUAq9BR": " Review of the Paper: Enhancing Size Generalization in Graph Neural Networks via Regularization  1) Brief Summary of the Paper and Its Contributions The paper addresses the challenge faced by Graph Neural Networks (GNNs) in generalizing to graph sizes different from those seen during the training phase. It notes that while GNNs are theoretically capable of handling graphs of any size their performance empirically degrades when dealing with graphs substantially larger or smaller than the training samples. Previous efforts to tackle this issue have either relied on tailoring models based on assumptions of the generative processes behind the graphs or required access to graphs from the test distribution both of which have clear limitations. To overcome these challenges this work proposes a novel regularization strategy that can be applied universally across different GNN architectures without needing access to the test distribution. This strategy simulates a distributional shift towards larger graphs by employing graph coarsening techniques and enforces model robustness against such shifts. The authors show that this approach leads to substantial performance improvements on the task of graph classification across several benchmark datasets testing GNNs ability to generalize from smaller to larger graphs.  2) Assessment of Strengths and Weaknesses Strengths:  Universality: The proposed method is modelagnostic allowing its application across a wide range of GNN architectures without modelspecific modifications.  Innovative Approach: The use of graph coarsening for simulating size shifts and enforcing robustness is a novel and clever strategy that directly addresses the problem of size generalization without needing external data.  Empirical Evidence: The authors provide extensive empirical validation across multiple datasets and GNN models demonstrating significant improvements in size generalization which is a strong point of the paper. Weaknesses:  Theoretical Underpinnings: While the empirical results are strong the paper lacks a deeper theoretical analysis of why the proposed regularization method works. For instance an exploration of the properties preserved by the coarsening process that contribute to improved generalization would strengthen the paper.  Impact on Training Complexity: The method introduces additional computational overhead during training (reported as 50 more time required) which might limit its practicality for very large graphs or datasets.  Generalization Across Size Decreases: The paper focuses on generalizing from smaller to larger graphs. It would be beneficial to understand how the method performs in the opposite scenario or within a broader range of size variations.  3) Questions and Suggestions for the Authors  Theoretical Justification: Can the authors provide more insight or theoretical justification for why the coarsening technique leads to better size generalization For instance are there specific properties of graph data that this technique exploits  Bidirectional Size Generalization: Have the authors considered testing the methods effectiveness not just for generalizing from smaller to larger sizes but also the reverse This could significantly expand the methods applicability.  Comparison with Domain Adaptation Techniques: Given the regularization approach somewhat mimics domain adaptation a comparison with stateoftheart domain adaptation methods even those not specifically designed for GNNs might provide further insight into the methods efficacy and positioning.  Impact of Coarsening Technique: The paper uses the SGC coarsening algorithm by default. It would be interesting to see a more detailed analysis of how different coarsening techniques affect the outcomes considering the diverse ways they might preserve or distort graph properties.  4) Limitations and Societal Impact The authors recognize that their approach assumes the presence of transferrable information across different graph sizes which might not always be the case. While they have empirically shown significant performance improvements the methods effectiveness might diminish as the size disparity grows. Addressing this perhaps including larger graphs in the training set or combining this strategy with others could be explored in future work. On the societal impact while the paper does not extensively discuss this aspect improving the size generalizability of GNNs can have broad implications especially in fields like drug discovery or social network analysis where graph sizes vary widely. However care must be taken to ensure these improvements do not inadvertently exacerbate biases present in the training data a point not deeply covered in the current manuscript.  Constructive Suggestions for Improvement  Expanding the theoretical exploration of why graph coarsening improves size generalization could bolster the papers contributions.  Experimenting with and analyzing the impact of various coarsening techniques in more depth could unveil insights into their disparate effects on GNN training and generalization.  Considering and testing the approachs efficacy in a wider range of size generalization scenarios including reducing graph sizes could enhance the methods robustness and applicability.", "nC8VC8gVGPo": "Reviewers Comments 1. Brief Summary The paper introduces the Local Tandem Learning (LTL) rule a novel training method for Spiking Neural Networks (SNNs) aimed at overcoming the limitations of existing training approaches. By employing a teacherstudent learning paradigm to transfer knowledge from pretrained Artificial Neural Networks (ANNs) to SNNs the LTL rule facilitates rapid network convergence showcases compatibility with various neuron models and ensures low computational complexity. The authors demonstrate the effectiveness of LTL through experiments on CIFAR10 CIFAR100 and Tiny ImageNet datasets achieving competitive accuracies with reduced training epochs and computational demand. Additionally the paper elaborates on the LTL rules hardwarefriendly design which addresses the issue of device nonideality making it suitable for implementation on mixedsignal neuromorphic computing chips. 2. Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The LTL approach combines the strengths of ANNtoSNN conversion and gradientbased training while enabling easier knowledge transfer and compatibility with different neuron models. This is a novel contribution to the field of neuromorphic computing.  Efficiency: Demonstrating rapid convergence within five training epochs for CIFAR10 while maintaining low computational complexity is impressive and of significant practical relevance.  Compatibility and Robustness: The hardwarefriendly design of LTL capable of addressing device nonideality issues enhances the practicality of deploying SNNs on neuromorphic chips.  Comprehensive Evaluation: The authors conduct a thorough experimental comparison not just with ANN models but also with other SNN training methods across various datasets. Weaknesses:  Limited Exploration of Learning Rule Generalizability: While the results are promising for image classification tasks the paper does not explore the application of the LTL rule to other domains or problem types such as regression tasks or unsupervised learning scenarios.  Potential for Scalability Challenges: The scalability of the LTL rule especially in terms of training very deep networks or handling significantly larger datasets remains somewhat unclear based on the current evaluations.  Insufficient Discussion on Limitations: While the paper touches on the robustness against hardwarerelated noises a deeper analysis of the potential limitations or drawbacks of the LTL rule itself is missing. 3. Questions and Suggestions for the Authors  Can the authors elaborate on how the LTL rule might be adapted or extended to tasks beyond image classification such as timeseries analysis or unsupervised learning  A discussion on the scalability of the proposed method for more complex architectures or larger datasets would provide a fuller picture of the LTL rules applicability.  The paper would benefit from a more detailed discussion on the potential limitations of the LTL rule including any scenarios where its performance might be less optimal. 4. Limitations and Potential Negative Societal Impact The authors have adequately addressed the limitations related to the device nonideality issues and demonstrated the LTL rules robustness against them. However potential limitations regarding the broader applicability of the LTL rule and its performance in nonimage classification tasks remain unexplored. As for the societal impact while the paper does not directly address potential negative consequences it is important to consider the broader implications of deploying highly efficient SNNs such as the potential for misuse in surveillance technologies or the exacerbation of digital divides if access to neuromorphic computing resources becomes unevenly distributed. Suggestions for improvement include:  A more nuanced exploration of the limitations and potential failure modes of the LTL rule would strengthen the manuscript.  A discussion on ethical considerations and the responsible deployment of SNN technologies particularly in sensitive applications would be a valuable addition.", "kRgOlgFW9aP": "The paper under review presents a novel exploration of Thompson Sampling (TS) for the control of diffusion processes characterized by uncertain drift matrices. Specifically it introduces and analyses an algorithm designed to harness Thompson Sampling for stabilizing and efficiently controlling a linear diffusion process to minimize a quadratic cost function under uncertainty regarding the drift parameters. This research steps into relatively uncharted territory by addressing the critical challenge of ensuring system stability\u2014a prerequisite not generally applicable in traditional reinforcement learning scenarios with finite state spaces. Contributions: 1. Introduction of a novel algorithm leveraging Thompson Sampling for the dual objectives of stabilizing the system and minimizing a quadratic cost function in a linear diffusion process with unknown drift matrices. 2. Theoretical guarantees that demonstrate the algorithms ability to stabilize the diffusion process with a small and exponentially decaying failure probability and achieve a regret bound that grows at the squareroot rate of time. 3. Empirical validation through simulations that compare the proposed methods performance against existing stateoftheart algorithms demonstrating its superior efficiency in exploration and regret minimization. Strengths:  The paper pioneers the application of Thompson Sampling to the control of diffusion processes contributing significant theoretical insights along with practical algorithms that demonstrate efficient explorationexploitation tradeoffs and system stabilization under uncertainty.  Rigorous theoretical analysis including stability guarantees and regret bounds provides a strong foundation for the effectiveness of the proposed approach. The methodology for deriving these results such as the novel eigenvalue perturbation analysis and the use of matrixvalued curves for establishing Lipschitz continuity is a significant contribution to the field.  The empirical work while serving primarily to validate theoretical findings also offers practical insights into the algorithms performance including its efficiency and robustness across different settings. Weaknesses:  A more thorough discussion on the selectiveness of empirical validation settings (e.g. flight control of X29A airplanes) would provide clarity on the algorithm\u2019s applicability scope. Benchmarking against a broader set of realworld problems would enhance the papers impact.  The paper briefly touches on the societal implications and potential negative impacts of its research but does not delve deeply into this aspect. Given the growing concern for ethical AI a more comprehensive treatment of how control algorithms may inadvertently lead to systemic risks would be valuable.  While the theoretical analysis is compelling the assumptions required for these results (e.g. stabilizability of the process specific properties of the Wiener process) might limit the applicability of the findings to more general or nonlinear systems. A discussion about extending the approach to broader classes of systems would be beneficial for future research directions. Questions and Suggestions for the Authors: 1. Can the authors provide further insights into the choice of empirical settings Exploring additional application domains could demonstrate the algorithms versatility. 2. A discussion on the potential societal impacts including negative aspects of deploying such control algorithms in sensitive systems (e.g. critical infrastructure financial markets) would be enlightening. Offering guidelines or safeguards against misuse could enhance the paper\u2019s value. 3. Exploring the extension of the proposed algorithm to nonlinear systems or those with more complex uncertainty structures would be an exciting future direction. Are there immediate theoretical or practical hurdles that the authors foresee in this expansion Limitations and Potential Negative Societal Impact: The authors have made an initial attempt to address the potential limitations and negative societal impacts of their work particularly emphasizing the necessity for ensuring system stability to avoid unintended consequences. However this discussion could be expanded. For instance considering the algorithms deployment in automated trading or autonomous vehicle control systems even minor instabilities or inaccuracies in the model could lead to significant financial losses or endanger human lives. Constructively the paper could explore builtin safeguards or failsafes within the algorithm that can mitigate such risks. Additionally elaborating on the ethical considerations and ensuring transparency in the deployment of such algorithms could help in minimizing potential misuse or negative societal impacts.", "siG_S8mUWxf": " Review for \"Subequivariant Graph Neural Networks for Physical Dynamics Prediction\"  1) Summary of the Paper This paper introduces Subequivariant Graph Neural Networks (SGNN) a novel architecture that addresses the challenges faced by current graph neural networkbased models in simulating physical dynamics particularly in scenarios involving symmetry and the diverse properties of physical objects. Recognizing that symmetry in physical laws plays a crucial role in model generalization the authors propose a subequivariant approach that relaxes the full equivariance constraint to accommodate external fields like gravity. The SGNN model applies a novel subequivariant objectaware message passing mechanism in a hierarchical structure enabling the accurate modeling of interactions among multiple objects with different shapes and sizes. The presented results demonstrate the models superior performance over existing stateoftheart simulators in terms of contact prediction accuracy and rollout Mean Squared Error (MSE) alongside strong generalization capabilities and data efficiency.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The concept of subequivariance introduced in this work elegantly addresses the limitations imposed by full equivariance in the presence of external fields such as gravity. This allows for a more accurate and generalized representation of physical dynamics.  Hierarchical Modeling: The hierarchical structure of SGNN effectively captures both longrange and complex object interactions demonstrating a significant enhancement in modeling capabilities over flat GNN architectures.  Empirical Results: The models superior performance on the Physion and RigidFall datasets coupled with its demonstrated generalization across scenarios and strong data efficiency solidifies its practical relevance and potential impact on the field. Weaknesses:  Dependency on ParticleBased Representation: The reliance on particlebased representations might limit the models applicability to realworld systems where such data is not readily available or is noisy and imprecise.  Lack of Explicit SelfContact Modeling: The paper does not address the modeling of object selfcontact which is a critical aspect of physical dynamics simulation particularly for flexible or deformable objects.  3) Questions and Suggestions for the Authors  Question on SelfContact Modeling: Could the authors elaborate on potential extensions or modifications to SGNN to explicitly account for selfcontact within objects  Suggestion on Robotic Applications: Given the models strong performance in simulation integrating SGNN within a robotic control framework to predict and respond to physical interactions in realtime could be a compelling direction for future work. Could the authors provide insights on how SGNN might be adapted or extended for such applications  Limitations and Improvements The authors have provided a detailed discussion on the limitations and future directions of their work emphasizing the reliance on particlebased representations and the absence of direct modeling for selfcontact scenarios. To address these limitations the following suggestions are provided:  Hybrid Representations: Exploring hybrid approaches that combine particlebased models with other data forms (e.g. mesh or voxel representations) could enhance the models applicability to a broader range of physical systems and scenarios.  SelfContact Augmentation: Extending the messagepassing framework to explicitly handle selfinteractions possibly by introducing additional objectspecific edge types or incorporating geometric constraints within the hierarchical model could improve the simulation of deformable objects. In conclusion SGNN represents a significant step forward in the modeling of physical dynamics using graph neural networks. By introducing subequivariance and a novel hierarchical messagepassing scheme this work addresses key challenges in the field and opens up avenues for further research and application.", "wtuYr8_KhyM": " Peer Review for \"Adaptive SwisH: A Novel Activation Function for Enhanced Deep Learning Model Performance\"  1) Summary of the Paper This paper introduces Adaptive SwisH (ASH) a new activation function designed to dynamically adjust the threshold potential of neurons based on the context of inputs as well as their position within the network. ASH aims to address the limitations of current activation functions which cannot effectively represent the variability in neuron threshold potentials across different neural locations and contexts. The primary contributions of this work include the development of ASH as a more adaptive trainable version of the Swish activation function providing a generalized mathematical formulation and demonstrating its efficacy across a range of deep learning tasks. The authors validate ASH through extensive experimentation in classification detection segmentation and image generation showing that it outperforms existing activation functions in terms of prediction accuracy convergence speed and processing of contextaware inputs.  2) Assessment of Strengths and Weaknesses Strengths:  Originality: The concept of an adaptive activation function that tunes its threshold based on input context is innovative and addresses a known limitation in the field.  Mathematical Rigor: The paper provides solid mathematical derivations that support the functionality and advantages of the ASH activation function over traditional functions.  Empirical Validation: Extensive empirical testing across various tasks and comparison with several baseline activation functions convincingly demonstrate the superiority of ASH in terms of accuracy and convergence times.  Broad Applicability: The authors ensure that ASH is tested on a wide array of tasks showcasing its versatility and potential wide applicability. Weaknesses:  Implementation Details: While the paper provides a link to the code it lacks detailed discussion on the computational complexity of implementing ASH compared to other functions which is crucial for assessing its practical viability.  Theoretical Justification for Adaptive Thresholding: The paper could benefit from a more thorough theoretical discussion on why adaptive thresholding better models human neuron behavior providing a stronger rationale for ASHs design.  3) Questions and Suggestions for the Authors  Clarification of Computational Overheads: Can the authors provide more insight into the computational costs associated with employing ASH especially in largescale models Any benchmarks comparing computational overheads with traditional activation functions would be valuable.  Extension to Other Domains: ASH demonstrates impressive performance in the tasks evaluated. Could the authors speculate on or provide preliminary results for the functions applicability in domains such as natural language processing or reinforcement learning  Impact of Parameter Initialization: How sensitive is ASHs performance to the initialization of its trainable parameters particularly in complex models Some discussion or additional experiments on this aspect could be enlightening.  4) Limitations and Societal Impact The paper could strengthen its discussion on limitations by addressing potential challenges in adopting ASH in resourceconstrained environments given any additional computational overhead. Additionally addressing how ASH might behave with adversarial inputs or scenarios where data distribution significantly changes could highlight its robustness. Regarding societal impact while the paper primarily focuses on technical advancements a brief discussion on potential negative implications such as exacerbating biases in datasets when used in predictive policing or hiring would be prudent. Suggestions for mitigating such risks alongside emphasizing the importance of ethically and justly curated training datasets would enhance this discussion.  Suggested Improvements  Enhance the discussion on computational efficiency and practical implementation concerns.  Provide a broader theoretical rationale for adaptive thresholdings ability to mimic human neuronal behavior.  Broaden the examination of limitations particularly in the context of robustness to adversarial attacks and changes in data distribution.  Include considerations on the ethical use of ASH especially in applications with significant societal impacts. Overall the paper presents a significant contribution to the field of deep learning by proposing an innovative activation function that addresses a wellidentified issue in current models. The suggestions provided aim to fortify the papers impact ensuring it is not only technically solid but also broadly applicable and considerate of wider implications.", "rF6zwkyMABn": "Review: Summary of the Paper: This paper tackles the intriguing problem of online learning within the context of general directed feedback graphs. The authors present an innovative set of algorithms that perform admirably in both adversarial and stochastic environments. These algorithms are designed based on the followtheregularizedleader (FTRL) approach augmented with novel learning rate update rules. The work is notably significant for resolving an open question posited by Erez and Koren [2021] about achieving nearly tight regret bounds that are informed by the graph structure\u2014specifically the independence and domination numbers. The efficacy of the proposed algorithms is rigorously established through formal theorems positioning this study as a critical advancement in online learning with feedback graphs. Assessment of the Paper: Strengths: 1. Novelty and Technical Merit: The authors introduce algorithms that significantly contribute to the domain of online learning with feedback graphs. The employment of directed feedback graphs coupled with advanced FTRL techniques signifies a substantial technical prowess. 2. Theoretical Contributions: The paper resolves a noted open question providing algorithms with nearly tight regret bounds for strongly and weakly observable cases. This theoretical foundation is likely to inspire further research and applications in online learning systems. 3. Broad Impact: Given the wide utility of online learning frameworks across multiple disciplines the proposed BOBW algorithms have the potential for broad applicability. Their flexibility to adapt to both adversarial and stochastic settings enhances their relevance to realworld applications. Weaknesses: 1. Complexity and Implementation Details: While the theoretical contributions are clear the paper could be improved by providing more details on the practical implementation of the algorithms including computational complexity and memory requirements. 2. Empirical Validation: There is a notable absence of empirical testing or application examples. Including case studies or performance metrics from real data sets would strengthen the paper by validating the theoretical advantages of the proposed algorithms in practice. 3. Comparison to Existing Work: Although the paper positions itself within the existing body of work a more detailed discussion comparing the proposed algorithms\u2019 performance against existing methods particularly in realworld scenarios would offer readers a clearer understanding of the practical benefits. Questions and Suggestions for the Authors: 1. Implementation Details: Can the authors provide insights into the practical aspects of implementing their algorithms such as computational complexity and scalability 2. Empirical Evidence: It would be beneficial if the authors could supplement their theoretical findings with empirical tests showcasing the algorithms performance on real or synthetic datasets. 3. Broader Applicability: How do the authors foresee the application of their algorithms beyond the scope of online learning perhaps in decisionmaking systems or optimization problems where similar feedback mechanisms are present Limitations and Societal Impact: The authors could enhance the discussion on potential limitations by addressing the practicality of their algorithms in diverse computational environments and their resilience against evolving adversarial tactics. Furthermore a deeper exploration of the ethical and societal implications especially in contexts where decisionmaking can have significant consequences (e.g. financial markets or healthcare systems) is warranted. Constructive Suggestions for Improvement: 1. Enhanced Practical Insights: Augmenting the theoretical exposition with guidelines for practical implementation and examples of realworld application scenarios could make the paper more accessible to a wider audience including practitioners. 2. Empirical Validation: Incorporating a section on empirical performance testing perhaps through simulations or case studies would substantiate the theoretical advantages claimed in the paper. 3. Discussion on Limitations: Expanding the discussion on potential practical limitations and ethical considerations surrounding the deployment of the proposed algorithms would offer a more balanced view and encourage responsible use of the technology. In summary this paper makes a notable contribution to the field of online learning with feedback graphs pushing the boundaries of existing theoretical frameworks. However enhancing the practical insights providing empirical validation and discussing broader societal implications would significantly elevate its impact and utility.", "sBrS3M5lT2w": "Peer Review of \"Global Convergence and Stability of Stochastic Gradient Descent under General Nonconvexity and Noise Assumptions\" 1. Summary and Contributions This paper addresses a significant gap in the understanding of Stochastic Gradient Descent (SGD) when applied to machine learning problems characterized by nonconvex objectives and complex noise. The authors challenge the restrictive assumptions typically made in SGD theory and demonstrate these limitations through examples in neural networks and regression models. They then introduce relaxed assumptions conducive to a broader class of problems and develop innovative analysis techniques that establish two main results: global convergence to a stationary point or divergence of SGD iterates and the convergence of the objective function to a finite random variable despite potential divergence of the iterates. These contributions significantly advance the applicability and theoretical understanding of SGD in practical nonidealized settings. 2. Assessment of Strengths and Weaknesses Strengths: a. Relevance and Novelty: The paper tackles a deeply relevant problem in machine learning optimization  the theoretical grounding of SGD in scenarios depicting realworld complexity. The relaxed assumptions and introduced analysis techniques are novel contributions that push the boundary of current understanding. b. Rigorous Theoretical Framework: The theoretical exposition is rigorous with clear definitions assumptions and proofs. The transition from restrictive assumptions to more realistic settings is welljustified and logically presented. c. Practical Implication: By demonstrating the conditions under which SGD iterates converge or the stability of the objective function the paper provides a foundational basis for applying SGD with greater confidence in complex machine learning problems. Weaknesses: a. Assumption Interpretability: The authors themselves acknowledge the difficulty in interpreting one of the key additional assumptions introduced for the stability analysis (Assumption 5). This limits the immediate practical applicability of the theoretical results as practitioners may find it challenging to assess when and how these conditions are met in specific applications. b. Empirical Evaluation: The paper lacks empirical validation of the theoretical findings. Including practical experiments demonstrating the implications of the relaxed assumptions and the proposed analysis strategies would significantly enhance the papers impact. c. Comparison to Related Works: While the paper discusses existing literature a more detailed comparison and explicit discussion on how the proposed approach improves over existing methods would be beneficial. Specifically a clearer delineation of the novel aspects of the assumptions and analysis techniques in relation to past studies would be helpful. 3. Questions and Suggestions for the Authors a. Assumption Interpretability: Can the authors provide more intuition or practical guidance on how Assumption 5 might be verified in realworld applications b. Empirical Evidence: Are there planned or ongoing empirical studies to validate the theoretical claims made in this paper If not the authors should consider incorporating such studies to strengthen their contributions. c. Broader Implications: Could the authors discuss the broader implications of their findings on the design and analysis of other optimization algorithms beyond SGD 4. Addressing Limitations and Societal Impact The limitations related to the interpretability of assumptions and lack of empirical validation should be addressed to enhance the papers applicability and impact. Additionally a discussion on the potential negative societal impacts such as misuse of the findings to justify poorly understood optimization in sensitive applications (e.g. facial recognition autonomous systems) should be included. Suggestions for improvement include:  Providing supplementary material or an online appendix with more detailed discussions or examples on interpreting the new assumptions.  Outlining potential empirical studies that could validate the theoretical results and offering datasets or problem settings where these contributions could be most impactful.  Including a section on ethical considerations and responsibility in deploying machine learning systems optimized using SGD especially under the new theoretical frameworks proposed. Overall this paper represents a significant contribution to the field of machine learning optimization pushing the theoretical understanding of SGD in complex settings. Future work addressing the highlighted limitations and incorporating empirical validation will further solidify its impact.", "nN3aVRQsxGd": " Review of \"KPGNN: Improving the Expressive Power of Graph Neural Networks by Leveraging Khop Peripheral Subgraphs\"  1) Summary of the Paper This paper addresses the limitations in the expressive power of Graph Neural Networks (GNNs) caused by the prevalent 1hop message passing mechanism. By introducing a Khop messagepassing paradigm and specifically focusing on peripheral subgraph information at each hop the authors propose the KPGNN framework. The contributions can be summarized as:  Theoretical characterization of Khop message passings expressive power highlighting its advantage over 1hop message passing and identifying its bounding by the 3WL test.  Introduction of KPGNN a novel GNN framework that enhances Khop message passing by utilizing peripheral subgraph information theoretically shown to improve expressive power beyond that of existing GNNs including those based on distance encoding.  Extensive empirical validation across simulation and realworld datasets demonstrating KPGNNs competitive performance and its ability to distinguish graphs where traditional models like 1WL and even 3WL tests fail.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Foundation: The paper provides a solid theoretical analysis that not only places Khop message passing within a formal framework but also elucidates the limitations of existing paradigms. The introduction of KPGNN is underpinned by rigorous theoretical development showing its superior expressive power.  Empirical Validation: Extensive experiments across a broad range of datasets demonstrate the practical effectiveness of KPGNN. The comparison with both traditional GNNs and newer models underscores the significant improvements offered by leveraging peripheral subgraph information.  Broad Applicability: KPGNN is designed to be easily integratable into existing Khop GNNs with minimal modifications offering a path to enhance many current models without substantial computational overhead. Weaknesses:  Limited Exploration of Complex Datasets: While the experimental validation is comprehensive the paper could further investigate the performance of KPGNN on more complex or largerscale datasets especially those where the graph structure plays a crucial role in tasks beyond classification such as in dynamic or temporal graphs.  Computational Overhead Analysis: Although claimed to be minimal a detailed analysis comparing the computational overhead of implementing KPGNN over traditional GNNs especially for large K values or dense graphs is lacking. A more thorough discussion on scalability and efficiency would be beneficial.  3) Questions and Suggestions for the Authors  Question on Generalization: Could the authors discuss how KPGNN generalizes to graphs with dynamic structures or those evolving over time Are there potential modifications or considerations to ensure its applicability in such scenarios  Suggestion on Limitations and Societal Impact: While the theoretical and practical implications are well discussed the paper could benefit from a deeper exploration of potential limitations such as scenarios where KPGNN might fail or introduce biases. Furthermore a discussion on the societal impact considering the increasing role of GNNs in decisionmaking processes would add value.  Further Comparative Analysis: It would be interesting to see a comparison of KPGNN with stateoftheart models in graph machine learning that do not strictly fall within the GNN category to further contextualize its performance and utility.  4) Limitations and Societal Impact The authors have addressed limitations related to the expressive power of GNNs and computational overhead. However a more explicit discussion on the scenarios where KPGNNs complexity might become prohibitive or its potential misapplications could enhance the paper. Although the positive implications of improved GNN performance are clear its crucial to also consider the ethical implications such as privacy considerations in social network analysis or biases in data that could be amplified by more powerful models like KPGNN. Suggestions for mitigating such risks would complement the papers contributions.  Conclusion This paper makes a significant advance in the field of graph neural networks by effectively addressing the known limitations of the 1hop message passing mechanism and proposing the theoretically and empirically robust KPGNN framework. While the benefits of KPGNN are clear further exploration of its limitations broader applicability and potential societal implications would make the research even more impactful.", "vsNQkquutZk": "Review of \"Dynamic Error Bounds in Time Series Forecasting with WaveBound Regularization\" 1) Summary The paper introduces WaveBound a novel regularization technique for time series forecasting aimed at addressing the issue of overfitting and unstable training in deeplearningbased models. The method dynamically adjusts error bounds on the training loss for each time step and feature at every iteration. This adjustment allows the model to ignore unpredictable data patterns to some extent thereby stabilizing training and enhancing generalization across various realworld datasets. The authors benchmarker their method against several stateoftheart models over six realworld datasets showing significant improvements in performance metrics. 2) Assessment Strengths:  Novelty of Approach: The proposal of dynamically adjusting error bounds on training loss is innovative and directly addresses known limitations in time series forecasting models including overfitting and the inability to generalize across inconsistent data patterns.  Comprehensive Evaluation: The authors conduct extensive experiments across multiple datasets and compare WaveBound with both traditional and stateoftheart models providing solid evidence of its effectiveness.  Theoretical Underpinnings: The inclusion of a theoretical discussion on how WaveBound optimizes mean squared error (MSE) adds depth to the methodology alongside practical validation. Weaknesses:  Lack of Comparison with Different Regularization Techniques: While the method is compared against stateoftheart models there is scant analysis on how WaveBound performs in conjunction with or against other known regularization techniques.  Limited Discussion on Hyperparameter Sensitivity: The paper mentions the use of a hyperparameter (\\(\\xcf\\xb5\\)) within WaveBound but does not extensively discuss its sensitivity selection process or impact on different types of datasets.  Potential Computational Overhead: The method introduces additional complexity by maintaining and updating a target network with an exponential moving average of the original network. However the discussion on the computational implications of this approach is lacking. 3) Questions and Suggestions for Authors:  Could the authors provide more insight into the selection process and sensitivity analysis of the hyperparameter (\\(\\xcf\\xb5\\)) Understanding its impact on different datasets could be valuable.  It would be beneficial if the authors could compare WaveBound directly with other regularization methods (e.g. Dropout L1L2 regularization) to contextualize its improvements.  A discussion regarding the computational overhead introduced by WaveBound especially in terms of training time and resource requirements would be valuable for assessing the practical applicability of the method in largescale scenarios. Limitations and Societal Impact:  The authors did not thoroughly discuss the computational overhead that WaveBound introduces which is a significant limitation for practitioners considering the adoption of this method.  The potential negative societal impacts of time series forecasting improvements (e.g. misuse in financial speculation increased surveillance capabilities) were not addressed. While these are indirect effects acknowledging and proposing guidelines to minimize harmful uses could enhance the papers contribution to responsible AI development. Suggestions for Improvement:  Providing a detailed analysis of the computational cost and proposing strategies to mitigate any significant overhead would make the paper more comprehensive.  Including case studies or discussions on the ethical use and potential misuse of improved time series forecasting could enrich the societal implications section of the paper.", "tvwkeAIcRP8": "Review of \"SingleView Neural Reflectance Field Optimization for 3D Scene Reconstruction\" 1) Summary: This paper addresses a novel challenge in the field of 3D scene reconstruction from singleview images specifically focusing on the optimization of a neural reflectance field from images captured under various point lighting conditions. The core innovation lies in the utilization of monocular shading and shadow cues in lieu of multiview photoconsistency to infer scene geometry. This approach allows the recovery of a complete 3D geometry including occluded and backfacing surfaces which traditional singleview methodstypically restricted to a 2.5D representationfail to achieve. The methods robustness is attributed to the neural representation of reflectance fields which effectively captures both geometry and Bidirectional Reflectance Distribution Functions (BRDFs) of the scene. The findings are demonstrated across multiple challenging datasets indicating the methods potential to support applications such as novelview synthesis and relighting. 2) Assessment: Strengths:  Novelty: The approach to 3D scene reconstruction from singleview images using neural reflectance fields based on shading and shadow cues is innovative and addresses a significant gap in the literature.  Completeness of Scene Geometry: Unlike previous methods that often recover only visible surfaces this methods ability to reconstruct complete scene geometry including occluded parts is an impressive advancement.  Efficiencies in Shadow Computation: The methods efficient online computation for shadow casting greatly contributes to the optimization process enhancing the practical applicability of the technique.  Demonstrated Robustness and Utility: The methods robust performance across various datasets and its support for applications like novelview synthesis and relighting are noteworthy strengths. Weaknesses:  Light Position Requirement: The necessity for known light positions introduces constraints on the methods applicability and practical deployment particularly in casually captured scenes.  Complex Backgrounds and Occlusions: The paper hints that highly complicated scene backgrounds might detrimentally affect performance although this limitation is not fully explored.  Reflectance of Invisible Parts: The inability to accurately constrain the reflectance of invisible regions points to an area where the method could be improved.  InterReflection Effects: The neglect of interreflections in image formation might limit the methods accuracy in certain scenarios indicating room for future enhancements. 3) Questions and Suggestions for Authors:  Generalization to RealWorld Scenes: Can the authors discuss how the method might be adapted or extended to work with uncalibrated lighting and less controlled environments commonly encountered in realworld applications  Handling Complex Backgrounds: How does the presence of a complex background impact the methods performance and what strategies might be suggested to mitigate this impact  Improvements on Invisible Region Reflectance: Could the authors elaborate on potential approaches to better constrain the reflectance properties of invisible parts of the scene  InterReflection Consideration: It would be beneficial to hear the authors perspective on integrating interreflection effects into their method. What implications might this have on the current framework and results 4) Limitations and Societal Impact: The authors have moderately addressed the limitations such as the requirement for known light positions and potential performance drops in complex scenes. However the discussion on the methods limitations and their societal impacts could be expanded. Specifically providing a more detailed exploration of the implications of these limitations in realworld applications would enrich the paper. For example a discussion on potential biases in the reconstructed scenes due to oversights in occluded region reflectance or interreflection effects might offer a more comprehensive understanding of the methods boundaries. Furthermore considering the potential misuse of 3D reconstruction technologies in privacysensitive scenarios a brief discussion on ethical considerations and mitigating strategies could be beneficial. Improvement Suggestions:  Expanded Limitations Discussion: Provide a more detailed exploration of the methods limitations particularly in the context of realworld applications.  Ethical Considerations: Briefly explore the ethical implications of 3D reconstruction technologies including potential privacy concerns and suggest strategies for responsible use. Overall this paper presents a significant step forward in the field of 3D scene reconstruction from singleview images. The methods innovative approach to leveraging shadow and shading cues for neural reflectance field optimization is commendable. However further discussions on the methods applicability to uncontrolled environments strategies for addressing complex backgrounds and a more thorough consideration of limitations and ethical implications would enhance the papers impact and applicability.", "z2cG3k8xa3C": " Peer Review  Summary of the Paper This paper delves into the approximation properties of Gaussiansmoothed optimal transport (GOT) in the context of comparing discrete measures \u03bc and \u03bd in R under small Gaussian noise. It identifies a critical threshold for the smoothing parameter \u03c3 which delineates a phase transition in the approximation behavior of the smoothed Wasserstein2 distance to the unregularized Wasserstein distance. Specifically below this threshold the difference between these distances scales exponentially with \u03c3 while above it the scaling is linear. This revelation is particularly insightful in the realm of optimal transport (OT) a mathematical framework with burgeoning applications in machine learning given its implications for improving the statistical fidelity of OT estimations in highdimensional settings. The authors build on a robust framework by defining strong cyclical monotonicity and implementability as measures of stability for optimal transport plans providing precise insights that bridge the gap in understanding the behavior of GOT vis\u00e0vis traditional OT especially in the finite support case.  Strengths and Weaknesses Strengths: 1. Novelty and Technical Depth: The paper introduces an innovative analysis of the Gaussiansmoothed optimal transport particularly addressing a nuanced behavior in the small noise regime which was previously unexplored. The mathematical rigor and the introduction of new definitions (like strong cyclical monotonicity and implementability) advance the field substantially. 2. Relevance to Applications: By tackling the computational and statistical challenges inherent in applying OT to machine learning the findings have significant implications for domains like generative modeling image processing and domain adaptation making the paper highly relevant. 3. Theoretical Contribution: The establishment of a phase transition in the approximation quality of GOT distances dependent on the smoothing parameter \u03c3 is a profound theoretical contribution. This not only complements existing literature but also opens avenues for further research. Weaknesses: 1. Limited Empirical Evaluation: While the numerical examples section contributes to understanding the paper could benefit from a more comprehensive empirical evaluation across a broader range of scenarios and datasets to validate the theoretical findings. 2. Accessibility to a Wider Audience: The heavy mathematical notation and the depth of technical details might limit the papers accessibility to readers not already wellversed in optimal transport and its nuances.  Questions and Suggestions for the Authors 1. Empirical Evaluation: Could the authors extend the numerical example section to include a comparison with existing methods or benchmarks on realworld datasets This could significantly bolster the papers claims and make it more appealing to practitioners. 2. Theoretical Extensions: The paper stops short of characterizing rates for nonatomic measures. Can the authors speculate on possible pathways or challenges in extending their results to this broader class 3. Software Implementation: Is there an available implementation of the methodologies discussed and if so could the authors detail its computational performance especially in comparison with traditional OT computation methods  Limitations and Societal Impact The authors briefly touch upon limitations related to the scope of their results (focusing on finitely supported measures) but could provide a more indepth discussion particularly regarding the practical implications of these limitations for applications in machine learning and statistics. Furthermore while the paper is largely theoretical a discussion on potential negative societal impacts such as misuse of the technology in surveillance or deep fakes (in the context of image processing applications) could have made the discourse around ethical considerations more comprehensive. Suggestions for Improvement:  Expanding the empirical evaluation section to include diverse datasets.  Further discussion on the extension of results to nonatomic measures.  Including software implementation details and computational performance analysis.  A more thorough examination of limitations and potential negative societal impacts providing guidance for ethical and responsible application of the findings. In summary this paper makes substantial theoretical contributions to the understanding of Gaussiansmoothed optimal transport with promising applications in machine learning. Addressing its few weaknesses could make it an even more valuable resource to both theorists and practitioners.", "vsShetzoRG9": "Review of \"INSNET: An Expressive Insertionbased Text Generator with Efficient Training and Flexible Decoding\" 1) Summary The paper introduces INSNET an innovative insertionbased text generator designed to overcome the inefficiencies in training and limitations in decoding flexibility associated with current insertionbased text generation models. By integrating a novel insertionoriented position encoding named \"offset\" and a lightweight slot representation strategy INSNET enables efficient onepass context encoding during the training process. Additionally the INSNETDinic algorithm is proposed to adjust the parallelization of insertion operations dynamically offering a balance between parallel and sequential decoding to cater to various task requirements. The performance and efficiency of INSNET are demonstrated through experiments on lexically constrained text generation and machine translation tasks showing notable improvements over existing models in training speed inference efficiency and generation quality. 2) Assessment Strengths:  The paper directly addresses the significant problem of inefficiency in the training of insertionbased models due to the need for repeated context reencoding after each insertion. By enabling computation sharing and minimizing reencoding operations INSNET represents a substantial improvement in training efficiency.  The introduction of an insertionoriented position encoding scheme (\"offset\") is innovative and effectively captures the dynamic nature of token positions during the insertionbased generation maintaining high model capacity.  INSNETDinics algorithm for adjusting the parallelization level of insertions depending on task requirements is novel offering muchneeded flexibility between generation quality and decoding efficiency.  The paper is thorough in its experimental evaluation tackling both lexically constrained text generation and machine translation tasks to verify the models efficiency flexibility and performance advantages over previous models. Weaknesses:  The paper could provide more insight into the limitations or potential drawbacks of INSNET including scenarios or task types where it might not perform as well.  While the experimental results are strong a detailed comparative analysis especially qualitative examples where INSNET outperforms or underperforms versus existing methods would offer deeper insights into its practical improvements and limitations.  The societal impact of the proposed method is not discussed. Given the increasing emphasis on ethical AI development acknowledging potential misuse or biases in generative models and suggesting mitigation approaches would have been valuable. 3) Questions and Suggestions:  Could the authors elaborate on how INSNET performs in scenarios with highly irregular insertions or when generating text with complex hierarchical structures  It would be beneficial to include more details about the computational complexity comparison especially a runtime comparison on identical hardware to quantify the \"efficient training\" claim.  The paper could benefit from a broader discussion on potential failure modes or limitations of INSNET including any sensitivity to hyperparameters such as the \u03c4 value in the INSNETDinic algorithm.  A discussion or a preliminary investigation into how INSNET could be adapted or extended to other NLP tasks beyond text generation and machine translation would be intriguing. 4) LimitationsSocietal Impact: The paper does not explicitly address potential limitations or the negative societal impact of its work. It is crucial in future iterations to consider these aspects discussing for instance how the models enhanced generation capabilities could be misused for disinformation or how biases in training data could propagate through INSNETgenerated text. Suggestions for improvement include incorporating fairness and bias evaluation metrics in the experimental analysis exploring mechanisms to detect and mitigate potential biases in generated text and establishing guidelines for the responsible use of INSNET in applications.", "kb33f8J83c": "Peer Review of the Paper \"FreeForm CLIP (FFCLIP) for TextDriven Image Manipulation\" 1) Summary of the Paper: The paper presents a novel approach FreeForm CLIP (FFCLIP) for manipulating images in the StyleGAN latent space using freeform text prompts. The primary intention behind this method is to map the textual embedding space of CLIP to the visual latent space of StyleGAN for effective image attribute manipulations. Unlike previous methods that rely on empirical designs limiting their functionality to single fixed text prompts FFCLIP introduces an automatic latent mapping allowing for the application of freeform text prompts. FFCLIP employs a crossmodality semantic modulation module to enable this functionality which includes semantic alignment and injection phases. The method has been tested on three types of images (human portraits cars and churches) showing it can produce semantically accurate and visually realistic images. The project and its source code have been shared for public access. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty in Methodology: The introduction of an automatic latent mapping addresses the existing challenge of empirical design in texttoimage manipulation allowing for more flexible and versatile applications.  Semantic Modulation Module: The design including both alignment and injection components is a robust solution for incorporating semantics from text prompts into the visual latent space which is crucial for achieving highquality image manipulation.  Extensive Evaluation: The paper provides a comprehensive evaluation of the proposed method across different datasets and compares the performance with existing stateoftheart methods presenting both qualitative and quantitative results to substantiate the effectiveness of FFCLIP.  Public Availability: By sharing the project page and code the authors encourage reproducibility and further research which is commendable. Weaknesses:  Limited Disentanglement in StyleGAN Latent Space: The paper acknowledges the limitation in fully disentangling the StyleGAN latent code W which remains an open issue affecting the precision of semantic modifications.  Potential Bias in CLIP: The reliance on CLIP for text embedding transfers humanlike biases into the manipulated images which can have negative societal impacts particularly concerning minority representations.  Generalization Concerns: The paper mentions that FFCLIPs effectiveness is contingent on the similarity of training and testing text prompts hinting at potential limitations in generalizing across significantly diverse prompts. 3) Questions and Suggestions for the Authors:  Mitigating CLIP Biases: Could the authors elaborate on potential strategies to mitigate the biases present in CLIP embeddings aside from exploring text inversion  Generalization to Unseen Prompts: It would be beneficial to understand how FFCLIP performs with completely novel text prompts not closely related to the training prompts. Could the authors provide insights or additional results regarding this  Exploration of Alternative Text Embeddings: Have the authors considered leveraging other text embedding methods alongside or instead of CLIP to compare effectiveness and possibly reduce biases 4) Limitations and Suggestions for Improvement: The paper identifies key limitations including the challenge in fully disentangling the StyleGAN latent code and the propagation of biases from the CLIP model. The authors suggest exploring text inversion for mitigating biases which is a positive step. However more proactive approaches could be investigated to enhance the models robustness against biases and improve generalization across varied text prompts. Additionally experimenting with alternative or supplementary text embedding models could offer avenues to reduce biases and enhance the semantic richness of the text prompts. Addressing these limitations with concrete strategies would significantly bolster the robustness and applicability of FFCLIP in diverse scenarios minimizing potential negative societal impacts.", "qcRgqCXv1o2": " 1) Summary of the Paper and its Contributions The paper addresses the vulnerability of graph classifiers like Graph Convolutional Networks (GCNs) to adversarial structural attacks. Specifically the authors note the limitation of existing robustness certificates that only consider local and global edge perturbations overlooking critical graph structures such as isomorphism. To overcome this they propose leveraging the orthogonal GromovWasserstein (GW) discrepancy as a measure of perturbation facilitating the construction of a robustness certificate and an attacking algorithm that reflects more refined structural discrepancies. Key contributions include:  Proposing a novel method to measure perturbations using orthogonal GW discrepancy conducive to convex optimization.  Developing a tight outer convex approximation for resistance distance on graph nodes through a novel use of matching loss and Fenchel biconjugation.  Demonstrating the application of these techniques in graph classification with GCNs where both the proposed certificate and attack algorithm show effectiveness.  2) Assessment of Strengths and Weaknesses  Strengths:  Innovative Approach: The paper introduces an original method of measuring perturbations in graph structures that accurately account for isomorphism a significant advancement over previous threat models.  Solid Theoretical Foundation: The development of a tractable convex relaxation for resistance distance through matching loss and its effective use in obtaining a convex envelope discrepancy between graphs are theoretically sound and wellarticulated.  Practical Relevance: The application of the proposed methods in improving the robustness of graph classification not only validates the theoretical model but also demonstrates practical relevance especially given the rising prominence of GCNs in various domains.  Weaknesses:  Complex Implementation: The theoretical complexity and the detailed steps involved in implementing the proposed convex relaxation might pose a barrier to adoption in practical settings.  Evaluation Scope: While the experiments verify the effectiveness of the methods the scope is somewhat limited to proof of concept. A broader evaluation across diverse datasets and comparison with a wider array of existing techniques would strengthen the claims.  Discussion on Limitations: The paper briefly touches upon the computation costs but does not thoroughly discuss limitations or potential downsides of their approach particularly in terms of scalability and computational efficiency.  3) Questions and Suggestions for the Authors 1. Scalability: Given the computational complexity of the approach how does it scale with larger graph datasets Including experiments or a more detailed discussion on this aspect would be beneficial. 2. Comparative Analysis: The paper lacks a comprehensive comparative analysis with existing methods. How does the proposed method perform relative to stateoftheart robustness certificates and attack algorithms in varied settings 3. Realworld Applications: The experimental validation primarily focuses on synthetic or semisynthetic datasets. Can the authors provide insights or results on the application of their method to realworld graph data where the structure might be more complex 4. Extension to Other Models: While the paper demonstrates the application to GCNs it would be valuable to discuss or explore the adaptability of the proposed method to other graph neural network architectures or even nongraph models susceptible to structural perturbations.  4) Limitations and Societal Impact The authors briefly mention computation costs but do not thoroughly assess other limitations of their work such as the potential inaccuracies in modeling realistic adversarial threats or the challenges in adapting the method to other models beyond GCNs. A more detailed discussion on these limitations would improve the paper. Regarding societal impact while the paper focuses on improving the robustness of graph classifiers a discussion on the potential misuse of such techniques for adversarial purposes could have been included. Constructive suggestions for mitigating these risks such as developing defensive techniques alongside attack methods would be valuable. Improvement could be made by further elaborating on the computational efficiency of the techniques for practical application and exploring the adaptability of the method to different graph structures and other types of adversarial attacks.", "zkk_7sV6gm8": " Peerreview of \"Learnable Impulse Control Reinforcement Algorithm for ActionCost Optimization in RealWorld Environments\"  1) Brief Summary The paper introduces the Learnable Impulse Control Reinforcement Algorithm (LICRA) an innovative framework designed to address the challenge of optimally deciding when and which actions to take in environments where actions are associated with costs. In such settings acting indiscriminately could lead to suboptimal outcomes due to the accumulation of costs or damage from wear and tear. LICRA combines reinforcement learning (RL) with impulse control policy to learn optimal actiontaking policies that incorporate action costs effectively. The approach is validated through theoretical proofs and empirical evaluations in the Lunar Lander and Highway driving simulators in OpenAI gym as well as a financerelated problem  a variant of the Merton portfolio problem.  2) Assessment of Strengths and Weaknesses  Strengths:  Novelty and Relevance: The paper addresses a significant yet understudied challenge in reinforcement learning  learning to minimize action costs in realworld scenarios. The integration of RL with impulse control policy is innovative.  Theoretical Contributions: The paper provides rigorous theoretical support for LICRA including proof of convergence and optimality. Such theoretical grounding is valuable for the domain.  Empirical Evaluations: The empirical evaluations across diverse domains (simulation and finance) show the versatile applicability and effectiveness of LICRA. The comparative analysis demonstrates its superiority over benchmark RL methods in terms of performance and efficiency.  Weaknesses:  Complexity and Implementation Barrier: The nested structure combining RL with impulse control may be conceptually and technically challenging potentially hindering wider adoption without more accessible implementation guides or public code repositories.  Limited Discussion on Hyperparameter Sensitivity and Tuning: While the paper demonstrates the effectiveness of LICRA there is limited discussion on the sensitivity of LICRAs performance to hyperparameter settings and the need for tuning across different environments.  Potential Overfitting Concerns: With elaborate structures and mechanisms theres a risk of overfitting especially in environments not covered in the empirical validations. Further studies on generalization and robustness are needed.  3) Questions and Suggestions for the Authors  Hyperparameter Tuning: Could the authors provide insights into how sensitive LICRAs performance is to hyperparameter choices Guidelines on tuning for new environments would be valuable.  Implementation Details: Are there plans to release a publicly accessible codebase for LICRA This would significantly aid in reproduction and adoption by the community.  Overfitting and Generalization: Could the authors comment on steps taken to mitigate overfitting Additionally experimenting with environments having different characteristics (e.g. higher dimensionality continuous action spaces) would provide insights into LICRAs generalizability.  Comparison with StateoftheArt: While LICRA is compared against benchmark RL methods how does it perform relative to the latest stateoftheart RL algorithms that incorporate action cost or budget constraints  4) Limitations and Societal Impact The authors have made commendable efforts to acknowledge and address potential limitations within their work such as the challenge of hyperparameter tuning and ensuring broad applicability across different domains. However there\u2019s a lack of discussion about the potential negative societal impacts especially in the context of finance and autonomous vehicles. Incorporating ethical considerations especially when deploying in sensitive or highstakes environments is crucial. Suggestions for improvement include a more thorough exploration of the ethical dimensions and potential misuse coupled with guidelines or mechanisms to mitigate such risks.", "noyKGZYvHH": "Peer Review of \"CoVariance Neural Networks (VNN): Advancing Stability and Transferability over PCAbased Methods\" 1) Summary of the Paper and Its Contributions This paper introduces a novel deep learning architecture the CoVariance Neural Network (VNN) which is designed to operate on sample covariance matrices and is inspired by the structure and functionalities of Graph Neural Networks (GNNs). Unlike Principal Component Analysis (PCA) that is susceptible to instability due to the sensitivity of principal components associated with closely spaced eigenvalues VNNs are theoretically shown to offer stability against perturbations in the sample covariance matrix. This stability is crucial for ensuring the reliability of statistical outcomes. Additionally the paper establishes that VNNs distinct from PCAbased methodologies are capable of performance transferability across covariance matrices of varying dimensions thus overcoming limitations inherent to PCA regarding dataset resolution. The manuscript validates these claims through theoretical analysis and empirical evaluations on realworld and synthetic datasets focusing on neuroimaging data to demonstrate the practical relevance of VNNs. 2) Assessment of Strengths and Weaknesses Strengths:  The paper addresses a significant problem in data analysis related to the stability of PCA and introduces a novel neural network architecture as a solution.  The proposed VNN architecture leverages an insightful connection between PCA and GNNs presenting a creative approach to designing deep learning models based on covariance matrices.  The study is theoretically sound providing a formal stability analysis that ties the performance of VNNs to the quality of sample covariance matrices. This is a substantial contribution that highlights the benefits of VNN over traditional PCA.  Empirical results on realworld datasets convincingly demonstrate the advantages of VNN in terms of stability and transferability making a compelling case for the practical utility of the proposed architecture.  Additionally the paper is wellstructured providing a clear exposition of the concepts and a solid review of related work thus positioning the contributions effectively within the broader research context. Weaknesses:  The paper could benefit from a more indepth discussion on the computational complexity of VNNs especially when dealing with highdimensional datasets as the scalability of the proposed method is crucial for its applicability to largescale problems.  While the experimental validation is strong the paper might have overlooked the exploration of VNNs performance on a wider variety of tasks such as classification or anomaly detection which could further elucidate the architectures versatility and limitations.  The potential for overfitting a common concern in machine learning models especially those with numerous parameters is not adequately addressed. Insights into regularization techniques or model comparisons on this aspect would have been beneficial.  The discussion on the impact of VNN on societal aspects or ethical implications is missing. Given the increasing awareness of the social consequences of machine learning models a brief consideration of these aspects would have rounded out the paper. 3) Questions and Suggestions for the Authors  Could the authors elaborate on the computational resources required for training VNNs on highdimensional datasets and whether there are techniques to mitigate potential computational bottlenecks  It would be interesting to see how VNNs perform on various machine learning tasks besides regression (e.g. classification clustering). Can the authors provide insights or preliminary results on such explorations  Regarding overfitting have the authors experimented with or considered implementing regularization techniques within VNNs to prevent it Details on such efforts would add value to the manuscript.  Can the authors comment on any potential negative societal impacts of VNNs or ethical considerations relevant to their deployment in realworld applications 4) Addressing Limitations and Societal Impact The authors have made significant theoretical and empirical contributions to improving stability and transferability in neural networks for data analysis. However they could improve the manuscript by discussing the computational efficiency of VNNs and their scalability. Including additional experimental validations across different tasks and addressing the models vulnerability to overfitting would strengthen the paper. Regarding societal impact the authors have not discussed this aspect which is increasingly becoming an essential consideration for machine learning research. It would be beneficial for the authors to reflect on potential misuse scenarios biases or ethical considerations particularly given the application of VNNs to sensitive areas such as neuroimaging. Overall this paper makes a valuable contribution to the field of deep learning for data analysis and with some enhancements it has the potential to significantly influence future research and applications.", "u8FDFtoMKp2": " 1. Summary of the Paper and Its Contributions The paper introduces a novel zeroshot transfer learning method for heterogenous graphs (HG) named Knowledge Transfer Network (KTN) that tackles the issue of label imbalance across different node types in HGNs. The authors identified that existing heterogeneous graph neural networks (HGNNs) struggle with transferring knowledge across node types due to distinct feature extractors for each node type which leads to performance degradation in labelscarce node types. KTN leverages the relational structure within HGNs to enable knowledge transfer from labelabundant to labelscarce node types showing significant performance improvements in inference accuracy for the latter. The main contributions include a new problem definition for zeroshot crosstype transfer learning in HGs a theoretic foundation for KTN based on feature extractor transformations and performance evaluations demonstrating improvements over stateoftheart domain adaptation methods.  2. Assessment of Strengths and Weaknesses  Strengths: a. Innovation: The paper tackles a previously underexplored issue in HGNNs with a novel approach that significantly enhances model performance on labelscarce node types. b. Theoretical Grounding: The derivation of a transformation between feature extractors for different node types is wellfounded and provides a strong theoretical basis for KTN. c. Generalizability: KTN is shown to be applicable across various HGNN models and different types of heterogeneous graphs indicating its wide applicability. d. Empirical Validation: Extensive experiments demonstrate the methods effectiveness with substantial improvements over existing methods quantitatively measured using robust metrics (e.g. MRR).  Weaknesses: a. Complexity: The implementation of KTN might be complex given the need to understand and manipulate feature extractor transformations within HGNNs. b. Limitation in Scope: The current method is limited to knowledge transfer within node types sharing the same task potentially reducing its applicability to more diverse tasks within heterogenous graphs. c. Sensitivity Analysis: While the authors conducted a sensitivity analysis using synthetic heterogenous graph generators realworld implications of feature and edge distribution on KTNs performance were not thoroughly explored.  3. Questions and Suggestions for the Authors a. Extension to TaskVariant Transfer: Can KTN be extended or adapted to support transfer learning across node types with different tasks or classification schemes b. RealWorld Application and Validation: Could the authors provide insight into how KTN performs in realworld scenarios especially where node and edge attributes might exhibit high variability c. Complexity and Scalability Concerns: Is there a significant computational overhead when implementing KTN on largescale heterogeneous graphs If so suggestions for optimization or scalability measures might be beneficial.  4. Limitations and Societal Impact The authors have clearly stated the limitations regarding the scope of KTNs applicability to node types with shared tasks and proposed future directions to extend their work. However a detailed discussion on scalability and practical implementation challenges would enhance understanding of potential limitations. Regarding societal impact while the authors briefly mention positive applications a thorough consideration of potential negative implications especially in sensitive areas (e.g. privacy concerns in social networks) would be responsible. Constructive improvement could include guidelines or mechanisms to ensure the ethical use of KTN in various applications.", "lxsL16YeE2w": "Peer Review of \"UViM: A Unified Vision Model for Efficient Modeling of Various Computer Vision Tasks\" 1. Summary of Contributions: The paper presents UViM a novel unified approach for computer vision (CV) that aims to standardize the modeling process across a diverse set of tasks without requiring taskspecific modifications. UViM consists of two main components: a base feedforward model that predicts raw vision outputs and an autoregressive language model that generates a guiding code to direct this prediction. This setup leverages learned discrete codes to effectively handle the structural complexity inherent in CV outputs. The authors demonstrate UViMs applicability and competitive performance on three different CV tasks: panoptic segmentation depth prediction and image colorization suggesting its potential as a general solution for a wide range of CV applications. 2. Strengths and Weaknesses:  Strengths: 1. Generality: UViMs ability to handle multiple diverse CV tasks without taskspecific tuning is a significant advancement towards a more unified and efficient model design in the field. 2. Innovative Approach: The introduction of a guiding code via an autoregressive language model represents an innovative fusion of language modeling techniques with CV providing a fresh perspective on addressing highdimensional structured output prediction. 3. Competitive Performance: The experiments show that UViM achieves near stateoftheart results on the tasks it was evaluated on demonstrating its practical effectiveness.  Weaknesses: 1. Limited Exploration of TaskSpecific Architectures: While the paper argues that UViM eliminates the need for taskspecific modifications it doesnt extensively compare its performance with highly optimized taskspecific models. Such comparisons could provide a clearer picture of UViMs relative advantages or limitations. 2. Computational Efficiency Concerns: The twostage training process and the use of both a base model and an autoregressive model might introduce significant computational overhead. The paper could benefit from a more detailed analysis of computational costs and efficiency comparisons with singlestage or less complex models. 3. Questions and Suggestions for the Authors:  Exploring TaskSpecific Benchmarks: Could the authors provide additional benchmarks comparing UViMs performance against highly specialized models for the evaluated tasks This would help in understanding any tradeoffs between generality and peak performance.  Detailed Computational Cost Analysis: It would be beneficial if the paper included a detailed analysis of UViMs training and inference computational costs possibly compared to traditional models. Understanding the resource tradeoffs better could be critical for practical applications.  Extension to More Tasks: Are there plans to test UViM on a wider variety of CV tasks Exploring its applicability to a broader set of problems could further substantiate its claim of generality. 4. Limitations and Societal Impact:  The authors recognize the potential negative societal impact stemming from the misuse or misunderstanding of generalpurpose models like UViM. However a more thorough discussion on specific safeguards or ethical considerations in deploying such models could enhance the paper. For instance outlining specific scenarios where UViM\u2019s output could be misused and proposing mitigation strategies could be valuable. Suggestions for Improvement:  To address the limitation of not exploring taskspecific architectures extensively the authors could include a section comparing UViMs performance with topperforming models designed specifically for the tasks in question. This would offer insights into where UViM stands in terms of performance for those willing to tradeoff some generality for taskspecific optimization.  On the topic of computational efficiency it would be helpful if future versions of the paper could explore techniques or modifications that might reduce UViMs computational overhead without significantly compromising its performance or generality.", "p9lC_i9WeFE": "Review of the Paper \"Generalization Error of Message Passing Neural Networks in Graph Classification and Regression\" 1) Summary: This paper investigates the generalization error of Message Passing Neural Networks (MPNNs) within the scope of graph classification and regression tasks. Utilizing a theoretical approach the paper introduces novel generalization bounds for MPNNs when applied to graph data that is assumed to be sampled from different random graph models representing various classes or regression outcomes. Particularly it demonstrates that a MPNN can achieve improved generalization from a small dataset of large graphs suggesting that the complexity of a MPNN and the size of the training graphs are crucial factors for generalization. The analysis is centered around the uniform convergence principle establishing that MPNNs approximate their continuous counterparts on the underlying space from which graphs are sampled providing bounds that show dependency on the number of training samples MPNN complexity and graph sizes. 2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper makes a substantial theoretical contribution by introducing a novel generalization bound for MPNNs that incorporates graph size as a factor providing insights into how MPNNs generalize across graphstructured data.  Practical Implications: The findings have significant practical implications offering guidance on designing MPNNs for various graphbased machine learning tasks particularly in selecting appropriate model complexities and understanding the influence of graph sizes.  Rigorous Analysis: The mathematical analysis is thorough and rigorous covering a wide range of scenarios under different assumptions about the data generation models. The use of random graph models (graphons) as the theoretical foundation offers a robust framework for the analysis. Weaknesses:  Model Assumptions: The papers results depend heavily on the assumption that graphs are sampled from different random graph models. While broad these assumptions might not capture all realworld graph data scenarios potentially limiting the applicability of the conclusions.  Complexity of Results: The complexity of the theoretical results and bounds may be challenging to translate into straightforward practical guidelines for designing and training MPNNs.  Empirical Verification: Although the paper is theoryfocused more extensive empirical validation on realworld datasets could have strengthened the claims demonstrating the practical relevance of the theoretical findings. 3) Questions and Suggestions for the Authors:  Model Assumptions: Could the authors discuss how the assumption of graphs being sampled from random graph models aligns with realworld graphstructured data possibly limiting the findings applicability  Empirical Evidence: It would be beneficial to include more empirical studies to validate the theoretical findings especially on realworld datasets across varied domains. Can the authors elaborate on potential future work in this direction  Impact of Model Complexity: The paper suggests that MPNNs complexity influences its generalization capability. Could the authors provide more intuitive explanations or guidelines on how to select appropriate model complexities for given tasks 4) Limitations and Societal Impact: The authors have made commendable efforts in addressing the limitations of their work particularly by acknowledging the model assumptions and the potential gap between the theoretical model and realworld data. However discussing the implications of the findings on societal applications such as privacy in social networks or fairness in recommendation systems could add depth to the paper. Regarding improvement further empirical validation against realworld datasets and consideration of model robustness and fairness implications could enhance the works relevance and applicability. In conclusion this paper provides valuable theoretical insights into the generalization capabilities of MPNNs fundamentally advancing our understanding of how these models work with graphstructured data. While the heavy reliance on theoretical assumptions and the lack of extensive empirical validation are notable limitations the work sets a solid foundation for future research that bridges these gaps.", "zXE8iFOZKw": " Brief Summary The paper introduces the DynamicsAware Hybrid OfflineandOnline Reinforcement Learning (H2O) framework addressing the challenge of learning effective reinforcement learning (RL) policies in realworld complex tasks with imperfect simulators and limited realworld data. H2O combines offline and online RL by adaptively penalizing or rewarding the Qfunction learning based on the dynamics gap between simulated and realworld data. The authors demonstrate H2Os superior performance through extensive simulations realworld tasks and theoretical analysis offering a novel paradigm that could influence future RL algorithm designs for practical applications.  Strengths 1. Innovative Approach: The H2O framework is groundbreaking in its combination of offline and online RL tackling the significant challenge of the simtoreal gap that hinders RLs applicability in realworld scenarios. 2. Theoretical and Empirical Validation: The paper provides both theoretical insights and extensive empirical evidence to support the effectiveness of the H2O framework. This dual approach strengthens the credibility of the results and their potential applicability. 3. Practical Relevance: The motivation behind addressing the limitations of offline RL and the unreliability of simulated environments is highly relevant for advancing the practical deployment of RL in complex realworld tasks.  Weaknesses 1. Complexity and Scalability: The proposed H2O framework while powerful appears to be complex in its implementation and optimization. This complexity might challenge scalability and adaptability to different RL problems and domains. 2. Generalization Concerns: While the paper showcases impressive performance improvements in specific tasks and environments it is unclear how well the H2O framework generalizes across a broader spectrum of RL tasks and realworld scenarios. 3. Dependency on the Quality of Offline Data: The effectiveness of H2O seems to hinge significantly on the availability and quality of offline realworld data. The paper could benefit from a deeper exploration of how the framework performs under varying qualities and quantities of offline data.  Questions and Suggestions for the Authors 1. Scalability and Complexity: Could the authors provide insights or examples of the H2O framework being applied to more complex or highdimensional RL tasks This would help understand scalability and practical implementation challenges. 2. Impact of Offline Data Quality: It would be beneficial if the authors could explore the performance implications of varying the quality and coverage of the offline datasets. Are there thresholds of data quality below which H2Os advantages diminish 3. Comparison with Existing Methods: Can the authors expand on the comparative analysis by including more stateoftheart methods in both offline and online RL especially those that might have emerged since the initial drafting of this work  Limitations and Societal Impact While the authors discuss the H2O frameworks effectiveness and theoretical underpinnings there is a minimal exploration of its limitations particularly in terms of computational resources and scalability. Additionally the potential negative societal impacts including the risks of deploying RL in sensitive realworld applications without fully understanding the limitations of combined offline and online learning are not adequately addressed. Constructive suggestions include:  A dedicated section on the limitations of H2O discussing computational efficiency data requirements and potential barriers to wider applicability.  A thoughtful analysis of potential negative societal impacts especially in highstakes applications (e.g. healthcare autonomous vehicles) and guidelines to mitigate these risks.  Propositions for future research directions that could address these limitations and societal concerns possibly by developing more dataefficient versions of H2O or methods to quantify and mitigate the risks of wrongful policy actions in critical applications.", "sPNtVVUq7wi": " Review of \"CONDOT: Conditional Optimal Transport Maps via Partially Input Convex Neural Networks\"  1) Summary of the Paper This paper introduces CONDOT a novel framework designed to estimate optimal transport (OT) maps conditioned on contextual variables. By utilizing partially input convex neural networks (PICNNs) and integrating them with a hypernetwork approach CONDOT learns a parameterized family of contextaware transport maps from labeled pairs of source and target measures each associated with a context. The framework aims to generalize well to unseen contexts enabling the prediction of effects from novel combinations of contexts such as genetic or therapeutic perturbations on cells. The authors apply CONDOT to various singlecell sequencing datasets to demonstrate its capability in predicting cell responses to different dosages of drugs the effects of treatments on different cell types and outcomes of novel genetic perturbations including combinations thereof.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The integration of PICNNs with a hypernetwork model to produce contextaware OT maps is a novel contribution. This approach allows for a versatile framework capable of handling a wide range of contexts including scalars vectors and discrete actions.  Generalization to Unseen Contexts: CONDOT demonstrates impressive capabilities in generalizing to new unseen contexts. This is a significant step forward in applying OT to realworld scenarios where the exact conditions might not be known a priori.  Applicability to SingleCell Genomics: The application to predict cell responses under various contexts such as drug dosages and genetic perturbations showcases the practical utility of CONDOT in biomedical research particularly in personalized medicine. Weaknesses:  Complexity of Architecture and Training: The architecture while powerful is relatively complex combining PICNNs with embedding and combinator modules. This complexity might make the model hard to interpret and could pose challenges in training and parameter tuning.  Limited Discussion on Computational Efficiency: There is a lack of detail on the computational requirements and efficiency of the proposed framework which is crucial for understanding its applicability to largescale problems.  Potential Overfitting to Experimental Conditions: While the model generalizes well to unseen contexts within the presented experiments there is a concern about its potential to overfit specific experimental setups or conditions not covered in the evaluation.  3) Questions and Suggestions for the Authors  Could the authors elaborate on the computational efficiency of CONDOT especially when applied to largescale datasets typical in singlecell genomics  Have alternative methods for context embedding been explored particularly for handling unknown or novel contexts It would be beneficial to understand why certain embeddings were preferred over others.  Is there potential for simplifying the CONDOT architecture without significantly sacrificing performance A simplified model might help in broader adoption and ease of interpretation.  Suggestions:  It would be valuable to include a more detailed analysis of the models performance as the complexity and size of the context space increase.  Further exploration into the robustness of the model to noisy or incomplete data could improve its applicability to realworld datasets where such issues are common.  An opensource implementation of CONDOT would significantly benefit the research community by facilitating further experimentation and development.  4) Addressing Limitations and Societal Impact The authors have made some attempt to address the limitations of their work particularly in terms of model generalization. However more detailed discussion on the computational aspects and potential overfitting concerns would be beneficial. Additionally while the paper touches on the positive implications of applying CONDOT to personalized medicine a more extensive discussion on the ethical considerations and potential negative impacts is warranted. For example how might biases in the training data affect the models predictions and what steps can be taken to mitigate such issues Suggestions for Improvement:  Include a section discussing the ethical implications of deploying such models in sensitive domains like healthcare and propose guidelines for their responsible use.  Expand the limitations section to cover potential biases in data collection and model prediction offering strategies for identifying and mitigating these biases.", "sc7bBHAmcN": " Review of \"Exploring the Expressive Power of Subgraph Neural Networks\"  Summary of the Paper The paper presents a comprehensive study of Subgraph Graph Neural Networks (GNNs) a class of GNNs that process graphs by applying neural networks to their subgraphs. The study primarily focuses on nodebased Subgraph GNNs which generate subgraphs centered around nodes using specific policies (e.g. node deletion egonetworks). The authors seek to answer two main questions about Subgraph GNNs: their upper bound of expressive power and the family of equivariant message passing layers applicable to these subgraphs. Through theoretical analysis they identify a symmetry group smaller than those used in previous studies improving understanding of Subgraph GNN architectures. They propose a general layer formulation for Subgraph GNNs introduce a new model named Subgraph Union Network (SUN) and empirically demonstrate its superior performance on several benchmarks.  Assessment of Strengths 1. Theoretical Contribution: The paper makes a significant theoretical contribution by thoroughly analyzing the symmetry group underlying nodebased Subgraph GNNs establishing a tighter upper bound on their expressive power related to the 3WL test. This work enhances understanding of the potential and limitations of Subgraph GNNs. 2. Novel Model Architecture: Introducing SUN the authors not only unify existing nodebased Subgraph GNN architectures but also provide empirical evidence of its superiority in performance. This demonstrates the practical impact of their theoretical findings. 3. Comprehensive Evaluation: The evaluation across both synthetic and realworld datasets coupled with consideration for generalization from limited data provides a strong argument for the models effectiveness and efficiency.  Assessment of Weaknesses 1. Model Complexity: While SUNs performance is impressive the complexity of fully understanding and implementing such a model may be high for practitioners. The paper could benefit from a more detailed discussion on the computational resources required and any potential bottlenecks in scalability. 2. Exploration of Design Space: The authors introduce an extended design space for Subgraph GNNs through ReIGN(2) but mainly focus on SUN. Explorations beyond SUN particularly using operations not included in the SUN architecture could reveal further insights into the capabilities of Subgraph GNNs. 3. Comparison with NonSubgraph Models: The paper positions SUN and other Subgraph GNNs against traditional GNN models. However a more thorough comparison involving a greater variety of stateoftheart GNN models could strengthen the argument for the distinctive advantages of Subgraph GNNs.  Questions and Suggestions for the Authors 1. Model Complexity: Can the authors comment on the computational efficiency and scalability of SUN especially in comparison to simpler GNN models 2. Extended Design Space: The paper mentions future work exploring operations not included in SUN. Could the authors speculate on potential models or designs that could emerge from this unexplored space 3. Impact of Subgraph Selection Policy: The choice of subgraph selection policy appears crucial to the architectures success. Could the authors provide more insights or guidelines on choosing or designing effective policies for various applications 4. Potential for Generalization: The authors highlight SUNs strong generalization in lowdata regimes. Could this be further explored or explained What implications does this have for areas like fewshot learning or transfer learning in graph neural networks  Limitations and Societal Impact The authors have consciently addressed the limitations related to their theoretical focus and the exploration of the design space for Subgraph GNNs. Additional discussion on computational demands and potential scalability issues would be beneficial. Regarding societal impact the stated possible positive effects on pharmaceutical research underline the beneficial applications of this work. However it would also be valuable to consider and discuss any potential for misuse or unintended negative consequences albeit not immediately apparent to ensure a holistic evaluation of impact. Overall this paper represents a substantial step forward in understanding and harnessing the power of Subgraph GNNs with promising implications for both theoretical exploration and practical application in various domains.", "wzJcEb5Mm4": " Peer Review of \"LogPolar Space Convolution (LPSC) for Expanding Local Receptive Fields in CNNs\"  1) Brief Summary The paper introduces a novel convolution method termed LogPolar Space Convolution (LPSC) aiming to expand the Local Receptive Field (LRF) in Convolutional Neural Networks (CNNs) without increasing the parameter count. Unlike traditional convolution methods that use regular square kernels LPSC employs an elliptical kernel that divides the local receptive field into regions based on relative directions and logarithmic distances thereby enabling exponential growth of the LRF with the number of distance levels. The authors propose a way to implement LPSC via conventional convolutions using logpolar space pooling allowing easy integration into existing CNN architectures. They validate the efficacy of LPSC through experiments on various datasets and tasks comparing its performance with conventional and dilated convolutions.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper presents a unique perspective on expanding the LRF in CNNs without the drawback of increased parameter count addressing a significant limitation in conventional methods.  Theoretical Justification: The logpolar space approach is wellfounded drawing on the natural structure and importance of pixel proximity and directionality in image processing.  Integration Ease: LPSC can be seamlessly added into existing CNN models offering potential widespread applicability and usefulness.  Empirical Validation: The extensive experiments across different tasks and datasets demonstrate LPSCs effectiveness and robustness providing convincing evidence for its benefits. Weaknesses:  Hyperparameter Sensitivity: The introduction of additional hyperparameters (Lr L\u03b8 and g) adds complexity to model configuration which could pose a challenge in optimization and general applicability.  Memory Overhead: The implementation of LPSC via logpolar space pooling while innovative may induce significant memory overhead impacting its efficiency on resourceconstrained environments.  Limited Application Scope: While shown to be effective on semantic visual data the application of LPSC might be limited for semantically dense data like speech signals or text potentially narrowing its utility.  3) Questions and Suggestions for the Authors Questions:  How does the performance of LPSC compare in deeper networks beyond the tested architectures especially in networks with complex structures or in Transformerbased models  Could the authors provide more insights into the tradeoff between the exponential increase in the LRF and potential losses in finegrained details especially in highfrequency regions of images Suggestions:  Experimenting with automated methods for hyperparameter selection (like Lr L\u03b8 and g) could enhance LPSCs userfriendliness and broaden its applicability.  Investigating and proposing optimizations to reduce the memory overhead introduced by LPSC implementation might make it more appealing for use in resourcelimited contexts.  4) Limitations and Societal Impact The authors have discussed the limitations regarding LPSCs potential loss of finegrained information and its unsuitability for dense data types. Further the memory overhead issue has been acknowledged but not thoroughly addressed. From a societal perspective the paper does not discuss the potential negative impacts which could include exacerbating biases present in visual data when LPSC is used in sensitive applications (e.g. surveillance facial recognition). It would be beneficial for the authors to consider these aspects and suggest ways to mitigate potential risks ensuring the responsible use of LPSC across various domains.", "mowt1WNhTC7": "The paper under review provides a critical and comprehensive analysis of ImageNet one of the most celebrated benchmarks in the field of computer vision. The focus is particularly on the multilabel subset of ImageNet where the authors seek to understand and categorize the mistakes made by stateoftheart (SOTA) models. By expertly reevaluating the supposed errors made by these models the paper uncovers that a significant portion of these mistakes are in fact not mistakes at all. The reassessment leads to the identification of new valid multilabels suggesting that the performance of current models on ImageNet has been significantly underestimated. The paper also introduces ImageNetMajor1 a curated subset of ImageNet consisting of 68 examples where SOTA models commonly make major errors offering a novel benchmark for future model evaluations.  Strengths 1. Novelty and Relevance: The approach of manually reviewing and categorizing errors made by leading models on ImageNet is fresh and highly relevant. As machine learning models inch closer to humanlevel performance on benchmarks like ImageNet understanding and revising the nuances of model errors becomes crucial. 2. Comprehensive Analysis: The depth of analysis involving the reevaluation of mistakes to identify incorrect annotations and overlooked multilabels is impressive. This meticulous scrutiny is valuable for the community as it offers clear insights into the shortcomings of current evaluation methods. 3. Constructive Benchmarking Proposal: The creation of ImageNetMajor1 as a benchmark focusing on major errors is an exemplary model of constructive criticism. It not only highlights the paper\u2019s findings but also provides a clear pathway for future research direction for the community.  Weaknesses 1. Scale of Manual Review: The reliance on expert manual review while thorough and insightful raises questions about scalability and reproducibility. The review process is laborintensive and might not be readily applicable to larger datasets or for future more complex models. 2. Subjectivity: As the paper acknowledges the categorization of errors into major and minor is inherently subjective. Despite efforts to mitigate biases the influence of the panelists worldviews on the classifications can still be a point of contention. 3. Potential for Overfitting: The process used to curate the ImageNetMajor1 might inadvertently lead to models that are specifically tuned to avoid major mistakes as defined by this paper which could limit the generalizability of future model improvements.  Questions and Suggestions for the Authors 1. Scale of Manual Review: Could the authors elaborate on the possibility of scaling the manual review process Are there automated or semiautomated methods that could aid in this extensive task without significantly compromising accuracy 2. Balancing Subjectivity: How do the authors envision future studies to balance the subjectivity inherent in categorizing model errors Are there frameworks or methodologies that can be standardized to reduce individual bias 3. Beyond ImageNetMajor1: What are the authors thoughts on applying their methodology to other datasets or benchmarks in computer vision or related fields Could similar inconsistencies or opportunities for reevaluation be present and impactful  Limitations and Societal Impact The paper acknowledges the limitations related to the scale of manual review and the subjectivity involved in mistake categorization. However further exploration into how these limitations might affect the broader applicability of their findings would strengthen the paper. Additionally the potential negative societal impact of their work such as overfitting to a newly proposed subset needs more attention. A constructive suggestion for improvement would be to develop guidelines for using ImageNetMajor1 judiciously ensuring it complements rather than overshadows other evaluation metrics or datasets. In conclusion this paper provides valuable insights into the performance of stateoftheart models on the ImageNet benchmark by unveiling inaccuracies in error categorization and dataset labeling. The introduction of ImageNetMajor1 presents an innovative approach to focusing model improvement efforts on eliminating egregious errors. However addressing the limitations regarding scalability subjectivity and potential for overfitting will be crucial for the longterm utility and impact of these contributions.", "krV1UM7Uw1": "Review of \"Robust Regression via Hard Thresholding and Bayesian Reweighting Against Adaptive Adversarial Attacks\" 1. Summary of the Paper: The paper introduces a novel approach for robust regression in the presence of adaptive adversarial attacks (AAAs) leveraging prior information and Bayesian reweighting. The authors argue that conventional robust leastsquares regression methods falter against AAAs due to a low breakdown point the maximum fraction of corruptions the algorithm can handle. To address these limitations they propose two algorithms: TRIP (hard Thresholding approach to Robust regression with sImple Prior) and BRHT (robust Bayesian Reweighting regression via Hard Thresholding) which enhance the breakdown point and reduce estimation errors by incorporating priors on regression coefficients and employing Bayesian reweighting for resilience against AAAs. Theoretical convergence of the algorithms under mild conditions is proven and extensive experiments demonstrate superior performance of the proposed methods against existing benchmarks. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Combination: The integration of prior information with robust regression and Bayesian reweighting is original and addresses a significant gap in the literature on robust regression amidst AAAs.  Theoretical Foundation: The authors provide a solid theoretical backing for the proposed methods including convergence guarantees which is commendable.  Experimental Validation: Extensive comparative experiments underscore the algorithms effectiveness in resisting AAAs providing practical relevance to the theoretical contributions. Weaknesses:  Prior Specification: The methodology heavily relies on the specification of priors. While the authors suggest options for prior setting the sensitivity and robustness of the algorithms to misspecified priors are not thoroughly investigated.  Limitations on Data Corruptions: The focus is predominantly on response variable corruption. Realworld data may experience feature matrix (X) and response vector (y) corruption simultaneously an aspect not extensively covered.  Algorithm Complexity: The paper lacks a clear discussion on the computational complexity of TRIP and BRHT which is crucial for understanding the practical scalability of the proposed methods. 3. Questions and Suggestions for the Authors:  Prior Sensitivity Analysis: Can the authors provide more insights or additional experiments on the sensitivity of TRIP and BRHT to the misspecification of priors This would enhance the robustness validation of the proposed methods.  Extension to Feature Matrix Corruption: Could the authors elaborate on potential modifications or extensions of TRIP and BRHT that could handle scenarios where both the feature matrix and response vector are corrupted  Computational Complexity: It would be beneficial if the authors could discuss the computational complexity of the proposed algorithms. This discussion could help in understanding the tradeoffs between accuracy and computational efficiency. 4. Limitations and Suggestions for Improvement: The paper demonstrates a significant advance in robust regression amid AAAs however it falls short in comprehensively dealing with the limitations around prior specification and algorithm complexity. To improve the authors should:  Conduct sensitivity analyses on the effect of prior misspecification on the performance of TRIP and BRHT.  Explore extensions of the algorithms to more complex data corruption scenarios including corrupted feature matrices.  Provide a detailed analysis of the computational complexity of the proposed methods and possible optimizations to enhance scalability. In its current form the paper does not adequately address the potential negative societal impacts. Integrating a discussion on ethical considerations especially in applications where robust regression is critical such as in finance or healthcare would be meaningful. This could include considerations around bias fairness and transparency when deploying these algorithms in sensitive areas.", "tWBMPooTayE": "Peer Review for \"FreGAN: FrequencyAware Generative Adversarial Network for HighQuality Image Synthesis in Limited Data Regimes\" 1. Summary of the Paper: The paper introduces FreGAN a novel method designed to enhance the quality of images generated by Generative Adversarial Networks (GANs) when trained under limited data conditions. Recognizing the challenges posed by discriminator overfitting memorization issues and the tendency of GANs to neglect highfrequency signals that contain critical details for realistic image synthesis FreGAN proposes to increase the frequency awareness of both the generator and discriminator within GANs. This is achieved through decomposing images into different frequency components using Haar wavelet transformations employing a HighFrequency Discriminator (HFD) and implementing HighFrequency Alignment (HFA) as well as Frequency Skip Connections (FSC) to preserve and emphasize highfrequency details. The paper claims that FreGAN significantly improves generated image quality particularly with very small datasets (fewer than 100 samples) and can be easily integrated with existing GAN models to further enhance performance. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The paper tackles the underexplored problem of synthesizing highquality images with GANs under limited data by focusing on frequency signals particularly highfrequency details which are crucial for achieving realistic textures and fine details.  Comprehensive Methodology: The proposed method is welldesigned combining multiple components like HFD HFA and FSC which work together to significantly elevate the quality of the generated images.  Strong Experimental Results: The authors provide extensive experimental evidence demonstrating that FreGAN outperforms stateoftheart methods across several datasets with varying degrees of data scarcity. This wideranging evaluation underscores the effectiveness and generalizability of the approach. Weaknesses:  Complexity of Implementation: The multicomponent architecture of FreGAN while effective adds complexity to the model which may affect the ease of implementation and computational efficiency.  Lack of Theoretical Analysis: The paper primarily focuses on empirical results and a deeper theoretical exploration of why focusing on frequency components particularly highfrequency signals leads to better performance would strengthen the paper.  Potential for Overemphasis on HighFrequency Details: While increasing highfrequency signal synthesis can improve realism theres a risk of generating overly sharp or noisy images if not balanced correctly an aspect not thoroughly discussed in the paper. 3. Questions and Suggestions for the Authors:  Could the authors provide further insights or theoretical reasoning on why highfrequency details are often neglected by GANs and how FreGANs approach specifically addresses this issue more effectively compared to previous methods  The paper would benefit from a deeper discussion on the potential tradeoffs introduced by FreGAN such as increased model complexity and computational demands. How do these aspects weigh against the improvements in image quality  Given the emphasis on highfrequency details how does FreGAN ensure that the overall coherence and realistic appearance of images are maintained without introducing excessive noise or sharpness  A comparative analysis highlighting the specific contributions of each component (HFD HFA and FSC) to the final image quality\u2014and under what conditions each component is most beneficial\u2014would be valuable for understanding the inner workings of FreGAN. 4. Assessment of Limitations and Societal Impact: The authors acknowledge the challenge FreGAN faces with datasets that are not only small but also highly varied. This acknowledgment is crucial highlighting an area for future improvement. However the paper could further address the potential negative societal impacts such as the misuse of realistic image synthesis in creating deepfakes or spreading misinformation. Providing guidelines or discussing potential safeguards against such misuse would be a constructive addition. Constructive Suggestions for Improvement:  The authors should consider adding a section discussing the ethical implications of their work including potential misuse and suggest ways the community can mitigate these risks.  A supplementary theoretical analysis on the role of frequency components in image synthesis could enrich the paper providing a foundation for why this approach is effective.  Exploring and discussing scalability and computational efficiency in greater depth would be beneficial offering insight into how FreGAN can be adopted in practical applications with limited computational resources.", "xubxAVbOsw": "Peer Review of \"DiversityPromoting Collaborative Metric Learning (DPCML) for Recommendation Systems\" 1. Summary The paper introduces DiversityPromoting Collaborative Metric Learning (DPCML) a novel recommendation system algorithm that addresses the limitation of unique user representation in traditional Collaborative Metric Learning (CML) frameworks. It targets the scenario where users hold multiple categories of interests arguing that unique user representations bias the system toward majority preferences and neglect minority interests. To overcome this DPCML employs multiple representations (embeddings) for each user leveraging the diversity within these representations through a Diversity Control Regularization Scheme (DCRS). The paper theoretically analyzes DPCML demonstrating its generalizability and experimentally validating its superiority over existing methods across various datasets. 2. Assessment Strengths:  Innovativeness: The introduction of multiple embeddings per user to tackle preference diversity is a significant advancement over traditional singleembedding approaches. The DPCML framework is a creative solution to the problem of preference bias in recommendation systems.  Rigorous Theoretical Foundation: The authors provide a comprehensive theoretical framework including generalization bounds to support the robustness and effectiveness of DPCML. This theoretical backing is often lacking in similar works.  Empirical Validation: Extensive experiments across several benchmark datasets demonstrate the practical effectiveness of DPCML compared to stateoftheart methods. The thorough evaluation adds credibility to the claims of superiority for the proposed method. Weaknesses:  Efficiency Considerations: While the paper touches on training efficiency there is insufficient detail on the computational complexity of DPCML especially regarding the scalability of employing multiple user embeddings as the dataset size increases.  Discussion on Parameter Sensitivity: Although the paper presents sensitivity analyses for certain parameters (e.g. C \\xf0\\x9d\\x9c\\x82) it lacks a deeper discussion on the robustness of the model to these hyperparameters which could be crucial for practical implementations.  Limited Scope on Feedback Types: The paper primarily focuses on implicit feedback scenarios which might limit its applicability to domains where explicit feedback is prevalent. A discussion or extension to explicit feedback scenarios could enhance the papers impact. 3. Questions and Suggestions for the Authors  Scalability: Can the authors provide more insights into the scalability of DPCML particularly how the embedding diversity impacts training time and memory requirements as the number of users and items grows  Hyperparameter Robustness: It would be valuable if the authors could expand on the models sensitivity to key hyperparameters (beyond C and \\xf0\\x9d\\x9c\\x82) and suggest practical guidelines for their selection.  Application to Explicit Feedback: How might DPCML be adapted to work with explicit feedback Adding a section or discussion on this topic could broaden the methods applicability. 4. Limitations and Societal Impact The paper does not thoroughly discuss the limitations especially regarding the computational efficiency and the challenge of hyperparameter tuning in practical scenarios. Moreover potential negative societal impacts such as reinforcing existing biases through minority interest learning are not addressed. Suggestions for improvement include:  A detailed analysis of computational requirements and strategies to mitigate scalability issues.  Discussion on the fairness of recommendations and measures to ensure the algorithm does not inadvertently exaggerate niche interests at the expense of user exposure to diverse content.  Exploring the extension of DPCML to explicit feedback scenarios for broader application.", "zYc5FSxL6ar": " Peer Review of \"SynergyOriented LeARning (SOLAR) for Modular Reinforcement Learning\"  Summary of the Paper The paper introduces the SynergyOriented LeARning (SOLAR) framework a novel approach aimed at addressing the challenges of controlling robots with a high degree of freedom (DoF) using modular reinforcement learning (RL). The key insight of SOLAR is the incorporation of the muscle synergy concept from neuroscience which involves grouping actuators into synergies to streamline the control process. This is achieved through an unsupervised learning module that identifies synergies based on functional similarity and morphological context and a synergyaware actorcritic architecture that learns to control these synergies. The authors provide extensive evaluation on various robot morphologies using the MuJoCo locomotion benchmarks demonstrating superior efficiency and generalizability of SOLAR especially in complex robots such as Humanoid and UNIMALs.  Assessment of Strengths 1. Novel Approach: The paper proposes an innovative framework that cleverly applies the concept of muscle synergies to modular RL. This significantly reduces the complexity of controlling robots with many joints. 2. Unsupervised Synergy Discovery: The unsupervised learning module for identifying synergies is a notable strength. It allows SOLAR to dynamically adapt the synergy structure based on both the morphological context and policy learning progress. 3. Architecture Design: The synergyaware architecture with its intra and intersynergy attention mechanisms succinctly captures the hierarchical nature of robot control enabling a more efficient policy learning process. 4. Comprehensive Evaluation: The extensive experiments across multiple robots and tasks along with comparisons to stateoftheart methods provide a strong validation of the effectiveness of SOLAR.  Weaknesses and Areas for Improvement 1. Limited Discussion on Synergy Dynamics: The paper could benefit from a deeper exploration of how synergy structures evolve over time in more complicated scenarios and how these dynamics impact longterm learning efficiency. 2. Scalability Concerns: While the results are promising the paper does not thoroughly discuss the scalability of the proposed method to even larger numbers of actuators or more complex tasks outside the tested environments. 3. Potential Overfitting to Simulation: The experiments are confined to simulated environments. Realworld robotics applications may introduce challenges not captured in simulation such as sensor noise and actuator nonlinearity. 4. Negative Societal Impacts: The discussion on societal impacts is brief and somewhat dismissive. Given the potential military or surveillance applications of advanced robotic control systems a more nuanced discussion would be appropriate.  Questions and Suggestions for the Authors 1. Synergy Evolution: Can the authors elaborate on the potential mechanisms or criteria for the evolution of synergy structures over time particularly in response to increasingly complex tasks 2. RealWorld Application: How might the proposed SOLAR framework adapt to the challenges inherent in transferring learned policies from simulation to real robots including dealing with imperfect actuator control and sensor noise 3. Scalability Analysis: Could future work expand on how SOLAR scales with the number of actuators or task complexity Are there computational or practical limitations to the proposed synergy discovery method 4. Societal Impact Consideration: Could the authors discuss any ethical considerations or mitigation strategies to prevent potential misuse of advanced robotic control technologies enabled by methods like SOLAR  Assessment of Limitations and Societal Impact The authors briefly mention that they do not foresee obvious negative societal impacts of their method which might be an oversight. Advanced robotic systems especially those with enhanced autonomous capabilities could have significant implications for labor markets military applications and privacy concerns. A more comprehensive discussion and analysis of these aspects alongside potential safeguards or ethical guidelines would strengthen the paper. In sum the SOLAR framework represents a significant advancement in modular RL for robot control with promising results and innovative approaches to synergy discovery and utilization. Addressing the highlighted questions and suggestions could further enhance the robustness and applicability of the proposed method.", "w6fj2r62r_H": "Peer Review of \"Torsional Diffusion for Rapid Generation of Molecular Conformers with Boltzmann Generators\" 1) Summary of the Paper: This paper introduces a novel diffusion model approach for the generation of molecular conformers focusing on torsion angle variation called \"torsional diffusion.\" The framework operates in a nonEuclidean domain (hypertorus) and incorporates an extrinsictointrinsic score model for accurate prediction of molecular structures. The authors assert that their methodology outperforms stateoftheart cheminformatics and machine learning methods in generating conformer ensembles especially in terms of Root Mean Square Deviation (RMSD) and chemical property preservation. Torsional diffusion is significantly faster than previous models and provides the added advantage of computing exact likelihoods facilitating the creation of systemagnostic Boltzmann generators. This work represents a meaningful step forward in computational chemistry particularly in the efficient and accurate simulation of molecule structures. 2) Assessment of Strengths and Weaknesses: Strengths:  The introduction of torsional diffusion as a nonEuclidean diffusion process is an innovative approach that offers a compelling solution to the dimensionality and flexibility issues inherent in molecule conformer generation.  The paper demonstrates clear advantages over existing methods with significant improvements in speed and accuracy. The reported superiority over commercial software OMEGA is particularly noteworthy.  The provision of exact likelihoods for generated conformers and the development of systemagnostic Boltzmann generators are significant technical achievements that could have wideranging implications for computational chemistry and drug discovery.  The authors have made their code available promoting reproducibility and community engagement with their work. Weaknesses:  While the paper mentions the potential for faster parsimonious models to benefit drug discovery and protein design there is limited discussion on the specific application areas outside conformer generation where torsional diffusion could be transformative.  The experimental validation though thorough primarily compares the method against machine learning and cheminformatics benchmarks. Inclusion of realworld case studies could have strengthened the paper showcasing the practical implications of the observed improvements in ensemble generation.  The discussion on the limitations and potential negative societal impacts of the work is notably absent leaving readers to speculate on the broader consequences of this technology. 3) Questions and Suggestions for the Authors:  Could the authors elaborate on the potential applications of torsional diffusion beyond molecular conformer generation particularly in areas like drug discovery and materials science  It would be beneficial if the authors could include a section discussing the limitations of their approach especially concerning its applicability to larger or more complex molecular systems.  The societal impact of accelerating molecule simulation capabilities both positive and potentially negative deserves attention. Can the authors discuss how their work might influence computational chemistrys future including the ethical considerations of automated drug design  A comparison with realworld applications or case studies where torsional diffusion\u2019s advantages could translate into tangible outcomes would be insightful. Can the authors provide examples or anecdotal evidence to bolster their claims 4) Limitations and Societal Impact Discussion: The paper does not explicitly address the limitations or the societal impact of torsional diffusion. Constructively it would be beneficial for the authors to consider and discuss the following:  Limitations: The scalability of torsional diffusion to larger molecules or systems with higher complexity could be considered a limitation. Additionally the dependence on training data quality and the potential for overfitting to particular molecular structures deserve attention.  Societal Impact: As with any advancement in computational chemistry there is the dualuse concern where increased capabilities in drug design could be misused. Moreover the automation of such processes might impact research jobs in the field. Discussing these aspects would provide a more balanced view of the technologys impact. In conclusion this paper presents a significant advancement in the use of machine learning for computational chemistry. However by addressing the above points the authors could strengthen the manuscript providing clearer guidance on the methods applicability limitations and broader societal implications.", "yQDC5ZcqX6l": " Peer Review  1) Brief Summary In this paper the authors present a novel method for biclustering bipartite graphs using Optimal Transport (OT) a technique that has garnered significant interest in machine learning. The proposed approach which they refer to as BCOT is designed to efficiently tackle the biclustering problem by directly integrating the biclustering objective thus offering a more direct and potentially more effective solution than previous methods which derive coclusters as a consequence of convergence. The approach is demonstrated to be computationally and memory efficient particularly with sparse dyadic data such as documentterm matrices. Additionally the authors introduce a fuzzy variant of biclustering that leverages entropic regularization to handle uncertain or overlapping cluster assignments.  2) Assessment Strengths:  Novelty and Practicality: The integration of the biclustering objective from the outset is a notable advance. This could provide a more straightforward pathway to biclustering compared to methods that arrive at biclusters as a byproduct of their approach.  Applicability to Sparse Data: The methods tailored approach to sparse dyadic data is highly relevant given the prevalence of such data in various domains.  Computational Efficiency: Demonstrating improved efficiency in both computation and memory usage over existing OT biclustering methods is significant. This makes the method more viable for handling largescale datasets.  Extension to Fuzzy Biclustering: The introduction of a fuzzy variant addresses a common issue in clustering where items may belong to more than one cluster to varying degrees. Weaknesses:  Comparison Baseline: The paper could benefit from a broader comparison against nonOT biclustering methods. While outperforming OTbased methods is impressive a comparison with a wider array of stateoftheart biclustering techniques could further validate the proposed methods effectiveness.  Realworld Applications: Although the method is demonstrated on documentterm matrices further exploration in other domains (e.g. genomics social network analysis) could enhance its appeal.  Parameter Selection: More detail on the selection process for key parameters particularly for the fuzzy variant would be beneficial. Understanding the sensitivity of the method to these choices could guide practitioners in applying the technique.  3) Questions and Suggestions for Authors 1. Broader Baseline for Comparisons: Could the authors provide comparisons with leading nonOTbased biclustering methods This would help assess the methods performance in a broader context. 2. Parameter Sensitivity Analysis: It would be beneficial to include an analysis of the methods sensitivity to its main parameters such as the regularization strength in the fuzzy variant. How robust is the method to variations in these parameters 3. Applications to Other Domains: Could the authors comment on the applicability and potential adjustments required for implementing the method in other domains such as genomics 4. Opensource Implementation: While the authors mention releasing their code ensuring comprehensive documentation and examples can significantly aid in the adoption of the method. Could the authors ensure these are provided  4) Limitations and Societal Impact The authors have not explicitly discussed potential limitations or negative societal impacts. While the computational efficiency and effectiveness in sparse data scenarios represent clear strengths there could be use cases where the methods assumptions or computational demands might not be suitable. Additionally any automatic clustering or classification system can be subject to biases present in the input data. To improve the authors could:  Discuss potential biases: Particularly in documentterm datasets where biases in the source material could be reflected in the clustering outcomes. Suggestions for mitigating these biases would be constructive.  Assess Generalizability: Offer more insight into scenarios where the method might not perform optimally and potential strategies to address these instances.  Consider Ethical Implications: Any discussion on the ethical use of biclustering especially in sensitive domains would enrich the paper. Overall while the proposed method presents a promising advance in biclustering with optimal transport bolstering the comparison baseline providing deeper insights into parameter sensitivity and applications and addressing potential limitations and ethical considerations would enhance the works contribution.", "oQIJsMlyaW_": " Peer Review for \"Sparsity via Integrated Gradients Estimation (SInGE): A Novel Criterion for Deep Neural Network Pruning\"  1) Summary of the Paper The paper introduces a novel technique for pruning deep neural networks (DNNs) named Sparsity via Integrated Gradients Estimation (SInGE) which combines integrated gradients\u2014a concept borrowed from the field of attribution methods\u2014with a structured pruning strategy. The authors argue that existing heuristics for DNN pruning such as magnitude and gradientbased methods have limitations particularly in identifying the least important neurons to prune. SInGE defines the importance of DNN weights through an integrated gradient criterion considering the integral of the gradient variations towards neuron removal. Furthermore the paper proposes a pruning and finetuning process that\u2019s closely intertwined to preserve the accuracy of the pruned DNN. The authors present extensive validation of SInGE across multiple datasets architectures and both structured and unstructured pruning scenarios demonstrating its superior performance in achieving higher accuracy versus pruning ratio compared to stateoftheart pruning methods.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The innovative use of integrated gradients for pruning is a significant conceptual advancement. It skillfully combines insights from the domain of explainable AI with practical DNN compression techniques.  Comprehensive Validation: The thorough experimental validation across multiple scenarios with comparisons to numerous stateoftheart methods provides strong evidence of the effectiveness and robustness of SInGE.  Practicality: By embedding the pruning process within a retraining (finetuning) framework SInGE not only improves the sparsity of DNNs but also ensures the preservation of their predictive accuracy which is crucial for realworld applications. Weaknesses:  Computational Complexity: While the paper highlights the superiority of SInGE in accuracy vs. pruning rate it does not thoroughly address the computational cost associated with integrating gradients and how it compares to existing methods.  Limitations and Societal Impact: The paper does not discuss the limitations related to the scalability of the method for extremely large DNN models or the potential negative impacts such as if the pruning process could inadvertently introduce or amplify biases present in the training data.  3) Questions and Suggestions for the Authors  Computational Cost: Can the authors provide more details on the computational overhead introduced by the integrated gradients calculation compared to more straightforward pruning criteria This would help in evaluating the practical applicability of SInGE in resourceconstrained environments.  Scalability Concerns: How does SInGE perform on stateoftheart extremely large models such as GPT3 or the latest ResNets Are there any foreseeable scalability issues  Impact on Model Bias: Given the sensitivity of integrated gradients to input features is there a risk that SInGE by focusing on what neurons are important might also prune neurons that help in mitigating biases in the models predictions  4) Limitations and Suggestions for Improvement The authors have not comprehensively addressed the limitations concerning SInGEs computational complexity and its potential societal impact. To strengthen the paper I recommend:  Detailed Computational Analysis: Including a section that details the computational requirements of SInGE perhaps compared with other pruning methods to provide a clearer picture of its efficiency and suitability for various deployment scenarios.  Discussion on Bias and Fairness: An exploration of how SInGE might affect model bias and fairness would be beneficial. The authors could perform additional experiments to investigate whether the pruning process disproportionately affects the models performance across different demographically divergent groups or look into strategies for mitigating any discovered negative impacts.  Scalability and Future Directions: A discussion on scalability challenges and potential solutions for applying SInGE to larger models would be valuable. It would also be interesting to explore how SInGE could be combined with other DNN optimization techniques such as lowrank factorization or quantization to further enhance performance and efficiency. By addressing these points the authors could significantly strengthen the manuscript providing a more holistic view of SInGEs potential and applicability.", "uCBx_6Hc7cu": " Peer Review of \"A Variational Inference Formulation of Autoassociative Memories for Contentbased Image Retrieval\"  1) Summary of Contributions This paper introduces a novel approach to enhancing autoassociative memory (AM) models by incorporating a memorydependent prior probability distribution. The authors methodological innovation lies in employing variational inference to merge perceptual inference and memory retrieval into a unified mathematically grounded framework. Specifically the paper:  Presents a novel formulation of AM that uses a memorydependent prior to guide the variational inference process aiming to enhance memory retrieval by making latent representations gravitate towards previously stored patterns.  Proposes four distinct AM models leveraging this variational inference approach drawing parallels between Predictive Coding (PC) and modern continuous Hopfield networks.  Conducts comparative evaluations of the new models against established associative memory models (e.g. Hopfield Networks Neural Turing Machines) using CIFAR10 and CLEVR datasets showcasing their efficiency in memory retrieval tasks.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The integration of a memorydependent prior in the variational inference process for AMs is a significant theoretical advancement offering a fresh perspective on merging memory retrieval with perceptual inference.  Theoretical Depth: The paper provides a comprehensive theoretical foundation linking the newly proposed method to wellestablished concepts like Predictive Coding and variational inference which enhances the credibility and novelty of the approach.  Empirical Validation: The comparative analysis using standard datasets provides solid empirical evidence of the proposed models effectiveness in memory retrieval tasks outperforming several existing approaches. Weaknesses:  Complexity and Accessibility: The highly technical content and dense mathematical formulations may limit the papers accessibility to readers not wellversed in variational inference or deep learning methodologies.  Scope of Experiments: While the evaluations demonstrate the models effectiveness the experiments focus mainly on image retrieval tasks. Additional domains or more varied tasks could further validate the models applicability and robustness.  Analysis of Limitations: The discussion on the models\u2019 limitations and potential societal impact is somewhat limited. Greater reflection on these aspects including the potential for biased retrieval or misuse would provide a more rounded view of the work.  3) Questions and Suggestions for the Authors  Experimental Diversity: Could the authors expand the scope of their experiments to include nonvisual memory retrieval tasks or other types of data (e.g. text or audio) This could provide insights into the models versatility and limitations.  Comparison with Recent AM Models: How do the proposed models perform relative to the most recent advancements in autoassociative memory models not mentioned in the paper  Impact of Hyperparameter Tuning: The paper could benefit from a more detailed discussion on the sensitivity of the proposed models to hyperparameter settings. Could the authors provide guidelines or insights on this aspect  4) Addressing Limitations and Potential Negative Societal Impact The authors have provided an initial discussion on the limitations and potential impacts of their work but could expand this section to offer a more comprehensive analysis. Specifically they should consider:  Bias and Fairness: Reflect on how biases in training data could affect memory retrieval and propose methodologies or precautions that could mitigate such biases.  Misuse Potential: Elaborate on potential misuse cases (e.g. enhancing surveillance technologies) and suggest ethical guidelines or safeguards that could be implemented.  Computational Resources: Discuss the computational efficiency of the proposed models especially in practical resourceconstrained scenarios and suggest potential avenues for optimization. Overall this paper contributes valuable insights and methodologies to the field of associative memory models although it could benefit from enhanced accessibility broader empirical validation and a more thorough discussion of its broader implications and limitations.", "peFP9Pl-6-_": " Peer Review  1) Brief Summary of the Paper The paper introduces a novel variational framework designed for efficient sampling in constrained domains characterized by general equality constraints. The authors propose an orthogonalspace gradient flow named OGradient which separates the gradient into two components: one focused on guiding samplers towards the constrained manifold (G0) and another for minimizing the KullbackLeibler divergence within the orthogonal space. This method sidesteps the need for samplers to be initialized directly on the manifold offering a more flexible approach compared to existing methods. The paper provides a proof of convergence for OGradient presents algorithms (OLangevin and OSVGD) implementing this framework and empirically validates its effectiveness across several machine learning tasks including fairness and robust Bayesian deep neural networks.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper addresses the problem of sampling on implicitly defined constrained domains extending beyond the capabilities of existing methods which generally require initializations on the manifold.  Theoretical Foundations: The authors present a solid theoretical background including the convergence proof of OGradient which is robust under mild conditions. This theoretical contribution particularly the new Stein characterization of the conditional measure could significantly impact further research in constrained sampling.  Practical Relevance: Demonstrating the applicability of their approach across a variety of realworld tasks including constraints for fairness and robustness illustrates the methods versatility and relevance. Weaknesses:  Limited to Equality Constraints: The current framework only considers equality constraints potentially limiting its applicability to a broader range of problems that involve inequality constraints.  Approximation to Manifold: While practical the approach does result in samples that are not exactly on the constrained manifold but rather close to it. This could be problematic for applications requiring strict adherence to constraints.  Empirical Validation: Although the empirical studies illustrate the methods effectiveness a deeper analysis comparing the performance against a broader set of constrained and unconstrained sampling methods could provide more insight into its practical benefits and limitations.  3) Questions and Suggestions for the Authors  Inclusion of Inequality Constraints: Could the authors discuss potential extensions of the OGradient framework to accommodate inequality constraints and what challenges such extensions might entail  Projection Step Consideration: Would incorporating a projection step to enforce exact adherence to the manifold postsampling improve or worsen the methods performance It could be beneficial to see an analysis or discussion regarding this aspect.  Comparison with Unconstrained Sampling Methods: Expanding the empirical validation to include a comparative analysis with stateoftheart unconstrained sampling methods might highlight the unique advantages or limitations of the proposed approach more clearly.  4) Limitations and Societal Impact The authors have not fully addressed the implications of using approximations to the manifold particularly in scenarios where strict enforcement of constraints is crucial for safety or fairness. Suggesting methods or parameters to control the proximity to the actual manifold would improve the paper. Additionally discussing the potential for negative societal impacts especially in applications where the approximation might lead to biased or unsafe outcomes would provide a more complete view of the works implications. Constructive suggestions for improvement include providing guidelines for selecting hyperparameters that control the tradeoff between convergence to the manifold and the exploration of the constrained domain. The authors could also explore the impact of their method on model transparency and interpretability given the increasing societal emphasis on understandable and fair AI systems.", "mMuVRbsvPyw": "Review for \"GMMSeg: Generative Mixture Models for Semantic Segmentation\" 1. Summary: The paper introduces GMMSeg a novel segmentation framework leveraging generative mixture models (GMMs) for semantic segmentation tasks. Unlike traditional approaches that rely on discriminative classifiers and directly model the class probability given an input (p(classpixel feature)) GMMSeg adopts a generative perspective by modeling the joint distribution p(pixel feature class) using Gaussian Mixture Models (GMMs) for each class. This approach enables capturing the classconditional densities p(pixel featureclass) effectively. The paper demonstrates that GMMSeg through its hybrid training strategy combining online EMbased classifier optimization and endtoend discriminative representation learning outperforms existing discriminative methods on closedset datasets and shows remarkable performance on openworld datasets without modifications. The work suggests a shift towards generative classifiers in semantic segmentation backing its claims with extensive experiments across various architectures and datasets. 2. Assessment: Strengths:  Innovative Approach: The paper presents a pioneering approach by integrating generative models (GMMs) with discriminative learning in an endtoend fashion for semantic segmentation addressing the limitations of solely discriminative models.  Broad Evaluation: Comprehensive evaluation is conducted using multiple segmentation architectures and backbone networks across different datasets including closedset and openworld scenarios showcasing the generality and adaptability of GMMSeg.  Outperforming Baselines: GMMSeg demonstrates noticeable improvements over traditional discriminative methods in both closedset and openworld settings highlighting its effectiveness.  Theoretical and Practical Contribution: The paper not only proposes a novel framework but also offers substantial insights into the benefits of combining generative and discriminative models for semantic segmentation. Weaknesses:  Computational Complexity: While the paper briefly mentions the inference speed it does not thoroughly discuss the computational overhead introduced by the GMM component especially during training which could be a pivotal factor for realworld applications.  Analysis of GMM Components: The paper could benefit from a deeper exploration into the effects of the number of components in GMMs on the segmentation performance and the interpretability of these components.  Limitations on Backward Compatibility: It is unclear how GMMSeg integrates or performs with older or less common segmentation architectures and whether there might be limitations in terms of model compatibility or performance tradeoffs. 3. Questions and Suggestions for Improvement:  How does the computational complexity of GMMSeg compare to traditional models during training particularly with respect to memory consumption and computational time  Could the authors elaborate on how the number of Gaussians in the mixture model was determined Is there an empirical method or heuristic that could be used for selecting this parameter for new datasets or segmentation tasks  While GMMSeg shows promising results in outofdistribution detection can the authors provide insights into its performance in dense prediction tasks within highly cluttered scenes or under drastic illumination changes  It might be beneficial to include experiments or discussions on the backward compatibility of GMMSeg with older segmentation models or architectures that are not explicitly covered in the study. 4. Limitations:  The authors have not fully explored the computational implications of integrating GMMs into the segmentation models which could affect the practical applicability of GMMSeg for resourceconstrained devices.  While GMMSegs ability to improve anomaly detection is highlighted a more nuanced exploration of how it manages inherent dataset biases and class imbalances would be informative.  The paper lacks a detailed discussion on potential negative societal impacts especially considering its application in sensitive areas such as autonomous driving or surveillance. Suggestions for Improvement:  A comprehensive analysis of the computational tradeoffs involved in adopting GMMSeg would provide readers with a clearer understanding of its practicality.  Further experiments on a broader range of segmentation tasks including those with complex backgrounds and varying lighting conditions could solidify GMMSeg\u2019s adaptability and robustness.  Addressing the models handling of dataset biases and providing strategies for mitigating negative impacts especially in critical applications would make the paper more rounded and responsible. Conclusion: The paper introduces a promising new direction in semantic segmentation by effectively combining generative and discriminative models. GMMSeg not only challenges the current stateoftheart discriminative paradigms but also opens up avenues for future research in integrating generative models within deep learning architectures for segmentation tasks. With further refinement and exploration GMMSeg has the potential to significantly impact both the research community and practical applications.", "px87A_nzK-T": "Review of \"Kernel Memory Networks for Maximally Robust Neural Pattern Storage\" 1. Summary: This paper presents a novel approach Kernel Memory Networks (KMN) for creating neural networkbased memory models that can store patterns with maximum robustness to noise. The approach is rooted in optimal weight determination and state update rules derived from training individual neurons for kernel classification or interpolation with minimal weight norms. The authors extend these concepts to both feedforward and recurrent networks to generalize and optimize various associative memory models including modern Hopfield networks and Kanerva\u2019s sparse distributed memory (SDM). A notable contribution is the modification of Kanervas model to support the storage of an exponential number of continuousvalued patterns. This work introduces a unifying theoretical framework that not only enhances the understanding of storage capacity in memory models but also suggests new biological interpretations in terms of dendritic nonlinearities and synaptic crosstalk. 2. Assessment: Strengths:  Theoretical Advancement: The paper makes significant theoretical contributions by establishing a comprehensive framework that unifies different strands of memory network models under the rubric of kernel methods. This is a laudable achievement that could have broad implications for both theoretical neuroscience and machine learning.  Interdisciplinary Relevance: Bridging concepts from statistical learning neural network design and biological neuroscience the proposed KMN framework offers insights that could be instrumental in advancing the development of bioplausible models of memory.  Practical Contributions: By demonstrating how to enhance noise robustness and storage capacity the work tackles practical challenges in neural memory design potentially impacting areas such as pattern recognition and data compression. Weaknesses:  Complexity and Accessibility: The paper is densely packed with mathematical formalisms and theoretical derivations which while necessary for substantiating the claims might deter a broader audience including practitioners from fully grasitating and applying these insights.  Empirical Validation: The paper predominantly focuses on theoretical formulation and lacks extensive empirical studies to validate the proposed models performance especially in practical realworld scenarios. 3. Questions and Suggestions for the Authors:  Empirical Study: Can the authors provide empirical evidence or simulations demonstrating the practical performance of KMN particularly in comparison with existing models such as MHNs or SDMs under varied conditions  Implementation Details: Further elaboration on the practical implementation aspects of KMN including computational complexities and scalability issues would be beneficial.  Biological Plausibility: While the paper proposes new biological interpretations how do these align with current empirical findings from neuroscience research Could the authors discuss potential experiments that might validate these interpretations  Extension to Learning Dynamics: The focus has been on memory storage capabilities. Could the authors speculate on or provide insights into how this framework might be extended to learning dynamics over time such as synaptic plasticity 4. Limitations and Societal Impact: The authors adequately address the theoretical foundations and biological relevance of their work offering a promising avenue for both technological applications and biological understanding. However the discussion on potential limitations including considerations of computational efficiency and scalability is somewhat lacking. Moreover while the paper touches on new biological interpretations a more thorough examination of the implications for our understanding of biological memory systems would elevate its impact. A critical reflection on the potential negative societal impacts of highly efficient memory models such as privacy concerns or misuse in surveillance technologies is also absent. While the papers focus is understandably on theoretical and technical contributions acknowledging and discussing these aspects would demonstrate a broader awareness of the implications of such research. Suggestions for Improvement:  Empirical Studies: To strengthen the paper providing empirical validation including comparisons with stateoftheart models would significantly enhance its credibility and impact.  Computational Considerations: Discussing the proposed models computational demands and potential strategies for overcoming scalability challenges would be beneficial.  Broader Implications: Expanding the discussion to include potential societal impacts and ethical considerations surrounding advanced memory technologies could enrich the paper further.", "szt95rn-ql": "Review of \"Bursting CorticoCortical Networks: A Model for Singlephase Efficient Deep Learning in the Brain\" 1. Summary and Contributions The paper presents a novel artificial neural network model called Bursting CorticoCortical Networks (BurstCCN) which aims to address the credit assignment problem in multilayer networks through a mechanism inspired by the bursting activity shortterm plasticity (STP) and dendritetargeting interneurons observed in cortical networks. This model is posited as a more biologically plausible means of backpropagating error signals through several layers of a network using a singlephase learning process an approach that stands in contrast to previous models that require multiphase learning processes or are ineffective at deep error propagation. The authors demonstrate that BurstCCN can backpropagate errors effectively its learning approximates backpropderived gradients and it can handle complex image classification tasks. The results suggest that the integration of specific biological mechanisms can underpin efficient deep learning which resembles brain learning processes. 2. Strengths and Weaknesses Strengths:  The paper tackles the highly relevant and challenging problem of making the backpropagation of errors in neural networks more biologically plausible.  It introduces an innovative model that integrates wellestablished cortical features offering a fresh perspective on singlephase learning in deep neural networks.  The authors provide a comprehensive evaluation demonstrating the models efficacy both empirically and analytically and its capability to handle complex tasks which solidly supports their claims.  The integration of bursting activity STP and dendritetargeting interneurons is a novel approach that could inspire further developments in the field of biologically inspired artificial intelligence. Weaknesses:  While the authors aim for biological plausibility the model still abstracts many aspects of actual neural dynamics and might still be far from replicating the complexity of learning in the brain.  The comparison between BurstCCN and other models such as EDNs and Burstprop could be expanded including more detailed discussions on performance differences and potential reasons.  The authors might have underestimated the potential negative societal impacts of promoting models that mimic human brain processes especially concerning privacy autonomy and misuse. 3. Questions and Suggestions for the Authors  How does the performance of the BurstCCN model scale with even larger and more complex datasets than MNIST and CIFAR10  Could the authors expand on the implications of their model for our understanding of learning in the biological brain Are there specific experimental predictions that could be tested  A suggestion for improvement would be to include a more thorough discussion on the limitations regarding the abstraction levels chosen for the model and the implications for biological plausibility.  It might be beneficial to explore and discuss alternative or complementary mechanisms observed in cortical networks that could enhance the models efficiency or biological accuracy. 4. Limitations and Potential Negative Societal Impact The authors have not fully addressed the potential negative societal impacts of their work. While the development of models that closely mimic human learning processes can advance artificial intelligence and neuroscience it also raises ethical concerns including the potential for creating systems that operate in unpredictable or uncontrollable ways or the misuse of such technologies in surveillance automation and military applications. Suggestions for Improvement:  The paper would benefit from a section dedicated to discussing the ethical considerations of creating models that simulate human learning processes including potential misuse and longterm societal impacts.  Exploring and mentioning safeguards or ethical guidelines for the development and application of such models could mitigate these concerns.", "znNmsN_O7Sh": "Review for \"Object Scene Representation Transformer (OSRT): Unsupervised Neural Scene Decomposition and Rendering\" 1) Summary: This paper introduces the Object Scene Representation Transformer (OSRT) a novel endtoend model designed for learning 3Dconsistent decompositions of complex scenes into individual objects without supervision. The paper leverages novel view synthesis (NVS) as a learning paradigm allowing the model to predict unseen views of a scene and through this process naturally learn to separate and represent objects within the scene. The critical innovation in OSRT is its Slot Mixer decoder which significantly increases the efficiency of compositional rendering\u2014alleviating the computational and memory burden associated with previous methods while enabling the model to handle more complex and diverse scenes. The authors empirically demonstrate the superiority of OSRT over existing stateoftheart methods in terms of both performance and efficiency. 2) Assessment: Strengths:  Innovation: The Slot Mixer decoder is a highly innovative component that effectively reduces the computational complexities inherent in scene rendering directly addressing a significant limitation of previous models.  Performance: OSRT outperforms existing benchmarks across various datasets not only in the quality of the rendered novel views but also in the efficiency of the rendering process achieving a remarkable speedup.  Generality: The authors successfully demonstrate the ability of OSRT to ingest scenes of high complexity and variability without the need for explicit supervision or depth information proving the approachs robustness and versatility.  Unsupervised Learning: The models ability to learn objectcentric scene representations unsupervised from RGB supervision alone is particularly impressive and valuable for future research in the field. Weaknesses:  Complexity and Usability: The paper does not fully address the potential difficulties in implementing or adapting the proposed OSRT model for other applications or datasets which could be a barrier to wider adoption.  Quantitative Metrics: While the empirical results are strong the paper could benefit from a more diverse range of quantitative measures particularly to assess the qualitative aspects of the scene decompositions such as object integrity and separation.  Scene Dynamics: The current version of OSRT is evaluated only in static scenes. The ability to handle dynamic scenes with moving objects may substantially increase its applicability and usefulness. 3) Questions and Suggestions:  Dynamic Scene Handling: Could the authors speculate on or suggest approaches for extending OSRT to handle dynamic scenes or video data effectively  Implementation Details: More detail on the implementation particularly any specific challenges encountered when designing the Slot Mixer decoder would be useful for readers aiming to replicate or build upon this work.  Quantitative Metrics Expansion: Including additional metrics that could more thoroughly assess the quality of the object decompositions might provide a more comprehensive picture of the models performance. 4) Limitations: The paper does an excellent job of articulating the innovations and advantages of the OSRT model. However it could provide a more thorough discussion on how OSRT could be extended or adapted to dynamic scenes which would significantly increase its applicability. Additionally while the impact on societal aspects is not discussed its crucial for future work to consider any potential negative uses of such technology particularly in privacysensitive applications such as surveillance. Suggestions for improvement include expanding on how OSRT could be used responsibly and exploring safeguards against its misuse in scenarios where consent and privacy are paramount. Conclusion: Overall the OSRT model represents a significant advance in the field of unsupervised learning for objectcentric 3D scene representation. Its introduction of the Slot Mixer as a means to improve efficiency in scene rendering is particularly noteworthy and addresses a crucial bottleneck in existing methods. Despite some limitations and areas for further research the papers contributions are likely to inspire future work and applications in neural scene representation and beyond.", "zD65Zdh6ZhI": "Summary of the Paper: The paper delves into the computational complexity of \u03b4sufficient reasons for decision trees within the domain of Formal Explainable AI (XAI). It argues that the wellstudied concept of sufficient reasons while offering deterministic explanations may be too rigid for practical applications. The paper suggests a more relaxed probabilistic approach denoting explanations that classify instances with a given probability \u03b4. This research addresses two critical open problems by establishing that both finding minimal size and inclusionwise minimal \u03b4sufficient reasons are computationally hard unless P equals NP. This is a notable departure from the deterministic case where finding such explanations is relatively straightforward. Additionally the paper proposes certain decision tree structures that allow tractable computation of \u03b4sufficient reasons and demonstrates the practicality of using SAT solvers for this purpose thus offering insights into solving NPhard problems in practical settings. Assessment of Strengths and Weaknesses: Strengths: 1. Originality and Relevance: The paper tackles a novel issue within the explainable AI domain providing significant contributions to understanding the computational complexity of explaining AI decisions probabilistically. The topic is highly relevant given the increasing demand for interpretable AI systems. 2. Theoretical Contribution: By addressing and resolving open problems the research contributes valuable insights into the computational limitations of XAI expanding the boundaries of known theory. 3. Practical Implications: Through the identification of tractable decision tree structures and the demonstration of SAT solvers effectiveness the paper offers practical pathways to overcome the inherent computational challenges bridging a gap between theoretical complexity and practical applicability. Weaknesses: 1. Accessibility and Generalizability: The highly technical nature of the investigation while necessary could limit its accessibility to a broader audience. Additionally the focus on decision trees may restrict the generalizability of the findings to other types of models. 2. Empirical Validation: While the use of SAT solvers is demonstrated the paper could further strengthen its argument with more extensive empirical evaluations particularly in comparing the proposed methods against existing benchmarks. 3. Discussion on Societal Implications: The discussion on potential negative societal impacts of the work seems limited. Given the importance of explainability in critical applications of AI a deeper exploration of how inaccuracies or computational limitations might affect societal trust in AI systems would be beneficial. Questions and Suggestions for the Authors: 1. Generalization to Other Models: Can the techniques and findings be extended or adapted to other machine learning models beyond decision trees Exploring this could significantly enhance the papers impact. 2. Empirical Evaluations: Could the authors provide additional empirical evaluations especially comparing the SAT solver approach with existing strategies for computing \u03b4sufficient reasons This would greatly aid in understanding the practical utility of the proposed methods. 3. Societal Impact Discussion: Could the authors expand on the potential negative societal impacts especially considering scenarios where these computational limitations might fail to provide accurate explanations in critical applications Limitations and Societal Impact: The authors have identified tractable scenarios and practical solutions to computational challenges yet a detailed examination of the limitations and negative societal impacts is less pronounced. Constructive suggestions include a stronger emphasis on the potential for misinterpretation or misuse of probabilistic explanations particularly in highstakes scenarios where inaccurate interpretations could have serious consequences. Enhancing this discussion would not only address ethical considerations but also provide a more comprehensive view of the researchs implications paving the way for balanced advancements in XAI.", "xwBdjfKt7_W": "Review of \"Robustifying Spiking Neural Networks Against Adversarial Attacks through Regularized Adversarial Training\" 1) Brief Summary This paper delves into the realm of Spiking Neural Networks (SNNs) with a focus on their deployment in realtime and safetycritical applications amid the advancements in neuromorphic computing. Unlike traditional Analog Neural Networks (ANNs) SNNs emulate the neuronal behaviors of biological brains introducing distinct challenges and opportunities particularly in the domain of adversarial robustness. The authors highlight the intrinsic vulnerability of SNNs to adversarial attacks due in part to their unique training algorithm requirements and the inclusion of the temporal dimension in their operational paradigm. The paper proposes a novel Regularized Adversarial Training (RAT) scheme that theoretically and experimentally addresses these vulnerabilities by leveraging a derived Lipschitz constant for spike representation and differentiable approximation techniques. This approach not only constrains the spiking Lipschitz constant to enhance robustness but also employs a mixed strategy of identified strong singlestep adversarial attacks for improved generalization across adversarial neighborhoods. The efficacy of the proposed method is substantiated through experiments on image recognition benchmarks where it significantly outperforms existing defenses against a suite of powerful adversarial attacks. 2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Foundations: The authors provide a comprehensive theoretical analysis that establishes a sound mathematical grounding for their proposed adversarial training scheme. The derivation of a Lipschitz constant specific to spike representation is noteworthy and contributes to a deeper understanding of SNNs robustness properties.  Innovative Approach: Introducing a regularized adversarial training regimen specifically tailored to SNNs represents a significant advancement in enhancing the resilience of neuromorphic computing models against adversarial perturbations.  Experimental Validation: The performance of the proposed method is thoroughly evaluated against traditional models under various adversarial scenarios. The results convincingly demonstrate its superiority in improving the adversarial robustness of SNNs across different benchmarks. Weaknesses:  Model Complexity and Training Overheads: The additional computational overhead introduced by the regularized adversarial training and weight regularization could potentially limit the practical applicability of the proposed method especially in scenarios where computational resources are constrained.  Generalization to Other Domains: While the paper focuses on image recognition tasks the generalizability of the proposed RAT scheme to other application domains of SNNs remains unexplored and could benefit from further investigation.  Discussion on Negative Societal Impacts: The paper does not comprehensively address potential negative societal impacts that might arise from encouraging more resilient SNNs particularly regarding misuse in surveillance and automated decisionmaking systems. 3) Questions and Suggestions for the Authors  Could the authors elaborate on the potential realworld implications of deploying the proposed RAT scheme in SNNs especially in terms of energy efficiency and computational costs  A deeper exploration into the adaptability of the proposed RAT scheme across different SNN architectures (beyond VGG11 and WideResNet16) would enrich the paper. Are there particular SNN configurations where the proposed method might be less effective  The discussion on the societal impact is somewhat lacking. Considering the potential for dual use of more robust SNNs a discussion on measures to mitigate negative impacts such as increased surveillance capabilities or biases in decisionmaking systems would be beneficial. 4) Limitations and Constructive Suggestions  Addressing Limited Generalization: The paper could benefit from additional experiments across a broader spectrum of tasks and SNN architectures to validate the general applicability of the proposed method.  Computational Efficiency: To improve the practical viability of the proposed approach the authors might explore optimization strategies or lightweight approximations that could reduce the training overhead without significantly compromising robustness.  Societal Impacts: The paper should include a more profound discussion on the ethical considerations and potential misuse of robust SNNs. Offering guidelines or principles for responsible deployment and use could enhance the papers impact and relevance in the broader context of AI ethics and governance. In summary this paper is a substantial contribution to the field of neuromorphic computing and adversarial robustness presenting a novel and theoretically grounded approach to enhancing the adversarial resilience of SNNs. Addressing the highlighted weaknesses and considering the suggested improvements could further elevate its value and applicability.", "qtQ9thon9fV": "Peer Review for \"Fourier Occupancy Field (FOF) for Monocular Realtime Human Reconstruction\" 1. Summary: This paper introduces Fourier Occupancy Field (FOF) a novel 3D geometry representation tailored for realtime accurate monocular human reconstruction. FOF employs a 2D field orthogonal to the view direction compactly encoding the occupancy field of an object along the view direction using the terms of the Fourier series. It bridges the gap between 3D geometries and 2D images by allowing storage as a multichannel image compatible with 2D CNNs. The FOF framework facilitates realtime human shape reconstruction at over 30 FPS and can integrate parametric models for robustness. The authors provide comprehensive analyses comparing FOF with existing representations like voxel grids meshes and implicit neural representations demonstrating FOFs superiority in terms of quality and efficiency. 2. Assessment: Strengths:  Novelty: The introduction of FOF as a 3D representation is novel and addresses the realtime reconstruction challenge effectively.  Efficiency and Quality: FOF stands out by offering highfidelity reconstruction at realtime speeds a significant improvement over existing methods.  Flexibility and Compatibility: FOFs ability to integrate with existing 2D CNN frameworks and easily convert between meshes and FOF is impressive.  Comprehensive Evaluation: Extensive experiments and comparisons clearly demonstrate FOFs advantages in terms of computational efficiency and reconstruction quality. Weaknesses:  Representation Limitations: The paper acknowledges that FOF struggles with objects that are too thin due to the nature of Fourier series expansion. This limitation might restrict FOFs applicability to a broader range of scenarios beyond human reconstruction.  Potential for Bias in Data: The datasets used (Twindom THuman2.0) have their limitations in pose diversity and clothing variability which might affect the generalizability of FOF\u2019s performance.  Lack of Focus on Negative Societal Impacts: While the paper briefly mentions potential privacy and ethical concerns a more detailed discussion on mitigating these risks would have been beneficial. 3. Questions and Suggestions for Authors:  Could the authors elaborate on potential approaches to overcome FOF\u2019s limitations in representing thin objects Exploring wavelet transformation is mentioned but further details or alternative strategies would be insightful.  The paper would benefit from a more diverse dataset covering a wider range of human forms clothing and poses. Are there plans to evaluate FOF on additional datasets  Could the authors provide more concrete examples or scenarios where FOFs realtime capabilities could be particularly impactful beyond the mentioned applications  A deeper discussion on ethical implications and privacy concerns related to realtime human reconstruction technology would enhance the paper. Specifically suggestions on safeguarding user data and preventing misuse should be explored. 4. Limitations and Societal Impact: The authors acknowledge the representation limitations of FOF particularly for thin objects and propose future exploration of alternative transformations. However the broader applicability of FOF and potential optimization for diverse shapes merit further investigation. While the paper mentions privacy and ethical concerns a comprehensive framework for addressing these issues is lacking. Constructive suggestions include developing secure data handling protocols anonymizing user data during processing and creating ethical guidelines for the use of reconstruction technology in sensitive applications. Addressing these concerns proactively will bolster FOF\u2019s societal acceptance and ensure responsible deployment in realworld applications.", "p_g2nHlMus": " Review for \"Fewshot classification via reweighted embedding similarity\"  1) Summary of the Paper: This paper addresses the challenge of supervision collapse in fewshot learning (FSL) where models trained on datasets with single imagelevel annotations struggle to identify less dominant or outoftask classes at inference time. The novelty of their approach FewTURE lies in decomposing images into patches and utilizing Vision Transformers (ViTs) combined with selfsupervised pretraining (Masked Image Modeling) to learn a generalized statistical structure of the data. A unique aspect is their method of dynamic token importance reweighting at inference time based on the support set which not only aids classification but also provides interpretability by identifying the most relevant portions of the image for the task. This results in stateoftheart performance on popular fewshot classification benchmarks including miniImageNet and tieredImageNet.  2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The method of splitting images into patches and encoding these via Vision Transformers introduces a novel way to tackle supervision collapse by focusing on local image regions with higher entity dominance.  Selfsupervised Pretraining: Leveraging Masked Image Modeling for selfsupervised pretraining allows the model to understand the general structure of the data beyond the limited scope of imagelevel annotations.  Dynamic Token Importance Reweighting: This mechanism is cleverly designed to adaptively adjust the weight of each patch token based on its relevance to the classification task ensuring that only the most pertinent image regions influence the models decision.  StateoftheArt Results: The paper demonstrates superior performance on several fewshot classification benchmarks showcasing the effectiveness of the proposed approach. Weaknesses:  Complexity at Inference Time: The dynamic reweighting process despite being beneficial adds computational complexity to the inference phase which may limit practicality in realworld applications requiring fast decisionmaking.  Dependence on Multiple Examples: The method is seemingly more effective in scenarios with multiple examples per class potentially reducing its efficacy in strict oneshot learning situations.  Limited Dataset Scope: While the paper shows impressive results on popular datasets the generalizability of the method to highly specialized or less common datasets remains uncertain.  3) Questions and Suggestions:  Generalization to Other Tasks: Can the authors comment on how their method might perform on tasks beyond classification such as object detection or segmentation in fewshot settings  Efficiency Improvements: Are there specific strategies the authors consider for reducing the computational overhead introduced by the token reweighting process during inference  Extended Dataset Evaluation: It would be beneficial if future work could test the approach on a broader range of datasets particularly those with higher complexity or less conventional class distributions.  4) Limitations and Constructive Suggestions: The authors have touched upon the limitations regarding inference time complexity and the methods efficacy primarily in multishot scenarios. However more discourse on potential challenges in training stability especially given the Transformers general data hunger and the reliance on selfsupervised pretraining would enrich the discussion. A suggestion to address computational efficiency could involve exploring approximations or heuristics that can predict token importance without full online optimization potentially preserving most of the methods advantages while streamlining the process. Additionally extending the evaluation to include more varied datasets could help affirm the methods robustness and applicability across a wider spectrum of fewshot learning challenges. Lastly a more detailed exploration of the societal and ethical implications of deploying such models especially in sensitive applications could provide useful insights into responsible usage. Overall this paper contributes significantly to the field of fewshot learning by providing an innovative approach to overcoming supervision collapse backed by substantial experimental validations. With some improvements in addressing the identified limitations and questions this work has the potential to inspire further research and applications in the domain.", "pDUYkwrx__w": "Review for \"Understanding the Privacy Loss in Stochastic Gradient Descent with Noise\" Summary of the Paper: The paper addresses foundational theoretical questions about the privacy loss inherent in training machine learning models on sensitive user data using the Stochastic Gradient Descent with Noise (NOISYSGD) algorithm. This approach is widely adopted in the industry for its simplicity and apparent effectiveness in maintaining privacy. Yet despite its widespread use fundamental questions about its privacy loss specifically its behavior over the number of iterations have remained unanswered. The paper resolves these open questions by characterizing the differential privacy (DP) loss of NOISYSGD up to a constant factor for a broad range of parameters in scenarios involving smooth convex losses over a bounded domain. Contrary to previous analyses that suggest privacy loss increases indefinitely with more iterations this study shows that after a certain \"burnin\" period running NOISYSGD longer does not leak any additional privacy. This finding corrects a significant misunderstanding in previous works and introduces new techniques based on optimal transport and the Sampled Gaussian Mechanism for analyzing privacy loss. Assessment of Strengths and Weaknesses: Strengths: 1. Theoretical Contribution: The primary strength of this paper lies in its theoretical contribution to understanding the privacy implications of the NOISYSGD algorithm. By rigorously characterizing the DP loss it fills a significant knowledge gap and corrects the prevailing belief that privacy loss indefinitely increases with more iterations of NOISYSGD. 2. Novel Techniques: The paper introduces innovative analytical techniques departing from traditional analysis methods that have failed to provide satisfying answers to fundamental privacy questions. The use of optimal transport and techniques related to Privacy Amplification by Sampling and Privacy Amplification by Iteration offers a fresh perspective and can potentially inspire new lines of inquiry in privacypreserving machine learning. 3. Practical Implications: The findings have significant practical implications suggesting that with proper parameter settings NOISYSGD can be used effectively for longer training periods without compromising privacy. This could lead to improvements in the optimization of machine learning models trained on sensitive data. Weaknesses: 1. Assumptions on Convexity and Boundedness: The papers analysis assumes convex loss functions and boundedness of the domain which may limit the generalizability of its results. Machine learning models particularly deep neural networks often deal with nonconvex loss landscapes and the boundedness assumption may not always hold in practical scenarios. 2. Lack of Empirical Validation: While the papers theoretical contribution is solid it lacks empirical evidence to support its claims about the practical benefits of the new understanding of NOISYSGDs privacy behavior. Including experimental results comparing the optimized models trained with traditional and proposed understanding of NOISYSGD could strengthen the paper. Questions and Suggestions for the Authors: 1. Generalizing Beyond Convexity: Can the insights gained from your analysis of NOISYSGD in convex settings be extended to nonconvex optimization problems Are there specific nonconvex scenarios where your findings may still hold possibly with some modifications 2. Empirical Validation: Could you include empirical studies or simulations that validate your theoretical findings demonstrating how the new understanding of privacy loss could impact the training of realworld machine learning models 3. Impact of Assumptions: Please discuss more thoroughly the implications of the convexity and boundedness assumptions on the applicability of your results. How critical are these assumptions in practice and are there ways to relax them without significantly affecting your analysis Limitations and Societal Impact: The limitations of this work largely stem from the assumptions of convexity and boundedness which may not always hold in practical machine learning scenarios possibly reducing the applicability of the findings. The authors have not thoroughly addressed these limitations or the potential negative societal impact of their work. One constructive suggestion would be to more explicitly discuss the scenarios where their analysis may not apply and to explore potential pathways or modifications that could make their results more widely applicable especially in nonconvex settings. Additionally an exploration of the ethical implications of deploying models trained with NOISYSGD particularly in sensitive applications would be beneficial. Conclusion: This paper makes significant theoretical contributions to the understanding of privacy loss in NOISYSGD offering new analytical techniques and challenging previous beliefs. While the assumptions on convexity and boundedness along with the lack of empirical validation limit its immediate practical applicability the insights provided have the potential to significantly impact the design and understanding of privacypreserving algorithms in machine learning. Further work both theoretical and empirical is needed to fully realize the implications of these findings.", "ldxUm0mmhl8": " Peer Review Report  1) Brief Summary of the Paper and Its Contributions This paper addresses a significant gap in the research on temporal point processes by introducing a methodology for generating counterfactual realizations based on a causal model of thinning. The authors propose an augmentation of Lewis thinning algorithm with a GumbelMax structural causal model which allows for the identification of counterfactual dynamics in temporal point processes. Through simulation experiments with both synthetic and real epidemiological data the paper demonstrates the potential of these counterfactual realizations to provide insights that could improve targeted interventions in highstakes applications such as epidemiology. The paper also offers an opensource implementation of their algorithms contributing valuable tools for further research in this area.  2) Assessment of Strengths and Weaknesses  Strengths:  Novelty: The paper addresses a unique and significant problem by enabling counterfactual reasoning in temporal point processes. The approach is innovative and not previously explored meeting an urgent need in various highstakes domains.  Technical Solidity: The methodology is built on a strong technical foundation leveraging proven concepts like Lewis thinning and the GumbelMax SCM while introducing original augmentations for counterfactual reasoning.  Practical Relevance: The application to epidemiological data underlines the methods practical relevance showing its potential to inform better decisionmaking in critical areas such as public health.  Reproducibility: By providing an opensource implementation and detailed algorithms the paper encourages reproducibility and offers a tangible asset to the research community.  Weaknesses:  Generalizability and Scalability: While the proposed method shows promise its applicability to broader types of temporal and potentially spatial point processes such as nonlinear or neural Hawkes processes remains unclear. The scalability of the approach to larger datasets and more complex scenarios also needs further evaluation.  Evaluation Depth: Although the synthetic and epidemiological applications demonstrate the methods potential a more extensive evaluation across different contexts and comparison with baseline methods could strengthen the case for its versatility and effectiveness.  3) Questions and Suggestions for the Authors  Generalizability:  Can the authors discuss the potential or limitations for extending their method to spatial point processes or to temporal point processes with nonlinear intensity functions  Scalability:  Are there inherent limitations in the proposed algorithms that might affect scalability to larger datasets or more complex models Modifications or optimizations that could address scalability concerns would be helpful.  Evaluation:  It would be useful for the authors to include a comparison with baseline methods or benchmarks that do not employ counterfactual reasoning to more concretely highlight the benefits and advancements their approach provides.  The authors might consider applying their method to additional realworld datasets or scenarios to further demonstrate its versatility and impact.  4) Limitations and Potential Negative Societal Impact The authors have not detailed any specific limitations or potential negative societal impacts of their work. Given the highstakes nature of the domains of application especially epidemiology it is crucial to address these aspects. Misinterpretations or misuse of counterfactual realizations could lead to detrimental decisions especially in sensitive interventions. Constructively the authors could:  Discuss Ethical Considerations: Briefly explore ethical considerations and scenarios where the methods misuse could lead to adverse outcomes providing guidelines for responsible usage.  Acknowledge Limitations: More explicitly address the specific conditions under which the proposed method is most effective and its limitations to guide potential users about when and how to use it appropriately.  Societal Impact Reflection: Reflect on the broader societal implications of enabling more accurate counterfactual reasoning both positive and potentially negative particularly concerning privacy and surveillance in sociotechnical systems where such models might be applied.", "rDT-n9xysO": " Peer Review of the Paper on Symbolic Distillation for TCP Congestion Control  1) Brief Summary of the Paper This paper addresses the challenge of designing highperformance TCP congestion control (CC) algorithms by proposing a novel twostage solution that combines deep reinforcement learning (RL) with symbolic regression (SR). Initially a deep RL agent is trained to grasp complex network conditions for CC. Subsequently the learned policy is distilled into symbolic rules creating a lightweight interpretable formulation suitable for implementation in constrained environments. This process utilizes a novel symbolic branching algorithm enhancing the adaptability of the distilled rules across diverse network conditions. The authors demonstrate that their approach termed SymbolicPCC not only preserves but often improves upon the performance of stateoftheart neural network policies while offering significant computational efficiency gains.  2) Assessment of Strengths and Weaknesses  Strengths:  Innovative Approach: The paper pioneers a hybrid methodology combining deep RL with SR offering a new pathway toward efficient and interpretable congestion control mechanisms.  Improved Interpretability and Efficiency: By distilling complex neural network policies into symbolic rules the proposed method makes strides in interpretability a notable limitation in existing approaches. Moreover the simplified symbolic formulation enhances computational efficiency a critical aspect for realtime CC operations.  Comprehensive Evaluation: The authors provide extensive validation of their approach through simulations and emulations clearly illustrating the advantages of SymbolicPCC over traditional and neural networkbased CC algorithms in various network conditions.  Weaknesses:  Generalizability Concerns: While the results are promising the papers focus on TCP congestion control limits the demonstrated applicability of the proposed method. Further research is needed to explore its potential across a wider range of RL applications.  Lack of Discussion on Tradeoffs: The paper could benefit from a more detailed discussion on the tradeoffs involved in the distillation process particularly regarding the potential loss of nuanced decisionmaking capabilities inherent in complex neural networks.  Potential Overhead in RealWorld Scenarios: While the symbolic rules are designed to be lightweight the paper does not fully address the overhead introduced by the branching algorithm especially in highly dynamic network scenarios where rapid context switching could impact performance.  3) Questions and Suggestions for the Authors  Generalizability to Other Networks: Can the authors discuss the potential applicability and modifications required to apply SymbolicPCC to other network protocols or RLbased system optimizations  Tradeoffs in Distillation: It would be beneficial if the authors could provide insights into the design choices and tradeoffs encountered during the symbolic distillation process including any challenges in maintaining the balance between simplification and performance retention.  Impact of Context Switching: The authors are encouraged to delve deeper into the impact of context switching within the branching algorithm on CC performance particularly in environments with frequent network condition changes.  4) Limitations and Societal Impact The authors have made a commendable effort in addressing the limitations related to interpretability and computational efficiency in TCP congestion control. However the discussion on potential negative societal impacts such as fairness concerns across different network types and users remains absent. Constructively it is suggested that future work could explore the fairness implications of employing SymbolicPCC in diverse network scenarios ensuring equitable access to network resources. This consideration is vital for upholding the principles of fairness and nondiscrimination in internet access worldwide. In conclusion this paper presents a forwardthinking approach to advancing TCP congestion control algorithms. By focusing on enhancing interpretability and efficiency the authors chart a promising path forward. Nonetheless expanding the scope to address the methods applicability across broader domains and its societal implications will be crucial for its longterm impact and success.", "r4RRwBCPDv5": " Review of \"Understanding Double Descent in Large Neural Networks: A VCTheoretical Analysis\"  1) Summary of the Paper The paper offers an indepth VCtheoretical analysis of the \"double descent\" phenomenon observed in the generalization performance of large neural networks particularly focusing on its occurrence despite zero training error. It challenges the existing consensus that VCdimension and classical statistical learning theories cannot explain the generalization capabilities of deep learning (DL) models particularly during the second descent phase where models are highly overparameterized. Utilizing theoretical insights and empirical illustrations the authors demonstrate that classical VC generalization bounds effectively model and predict double descent across several classifiers (SVM Least Squares Multilayer Perceptron). The paper further discusses misinterpretations in the DL community regarding VCtheory particularly addressing the training algorithms impact on VCdimension and the relevance of model complexity measured through the weights norm rather than the parameters count.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Contribution: The paper demystifies the double descent phenomenon with a solid theoretical foundation that bridges traditional statistical learning theory with modern deep learning practices.  Clarity in Contradictions: It successfully addresses and clarifies common misconceptions about VCdimension and its relationship to the number of parameters training algorithms and data distributions.  Empirical Validation: The empirical results provided are robust encompassing various classifiers and demonstrating the practical application of VC bounds in predicting double descent.  Insightful Discussion: The analysis of potential reasons behind the misinterpretation of VCtheoretical results in the DL community is thoughtprovoking and opens up new avenues for research. Weaknesses:  Limitations in Generalizability: The direct application of VCtheoretical analysis is acknowledged to be challenging outside simplified or restricted network settings which may limit the broader applicability of the findings.  Lack of Code and Reproducibility Details: The absence of shared code data and explicit details necessary for reproducing the experimental results could hinder the validation and extension of this work by other researchers.  Societal and Ethical Considerations: The paper does not address potential negative societal impacts particularly the consequences of promoting highly overparameterized models in terms of environmental and computational resource utilization.  3) Questions and Suggestions for Authors  Expansion on Generalizability: Could the authors elaborate on how their VCtheoretical framework might be adapted or extended to analyze more complex and practical DL network architectures beyond the simplified settings  Clarification on Hyperparameter Impact: The paper discusses controlling VC dimension through hyperparameters like network size or the number of training epochs. Can the authors provide more insights on how other hyperparameters e.g. regularization strength or learning rate could influence VCdimension and the double descent phenomenon  Reproducibility: It is recommended that the authors share the code datasets or at least a more detailed procedural description to facilitate independent reproduction of the results.  4) Addressing Limitations and Potential Negative Societal Impact The authors have outlined the direct application of VC bounds is constrained to certain settings. It would be beneficial to explore approximations or heuristics that could extend the applicability of these analyses to a broader range of practical DL models. Regarding potential negative societal impacts an acknowledgment and brief discussion on the computational and environmental costs associated with training large overparameterized models would enhance the paper. Suggestions for future work could include the development of more efficient training methods or architectures that mitigate these costs while maintaining generalization performance.", "yJV9zp5OKAY": " Review of \"LabelPreserving Adversarial AutoAugment (LPA3): Automatic DomainAgnostic TaskInformed Data Augmentation\"  Summary: This paper introduces a novel approach to data augmentation LabelPreserving Adversarial AutoAugment (LPA3) which automatically generates domainagnostic but taskinformed data augmentations. LPA3 is grounded in representation learning principles specifically aiming to maintain the minimal sufficient information necessary for preserving class labels while generating \"hard positive examples\" for augmentation purposes. The method does not rely on predefined augmentation operations or domainspecific knowledge making it broadly applicable across various machine learning tasks including supervised semisupervised and noisylabel learning. The authors demonstrate the efficacy of LPA3 through extensive experiments showing significant improvements in learning efficiency and final performance across multiple tasks notably in medical image classification scenarios where traditional augmentation techniques tend to underperform.  Strengths: 1. Innovative Approach: The paper\u2019s primary strength lies in its innovative approach to data augmentation. By focusing on generating augmentations that preserve label information while being agnostic to domainspecific knowledge LPA3 addresses a significant gap in the literature. 2. Broad Applicability: The methodology is designed to be compatible with a wide array of machine learning tasks making it a versatile tool in scenarios where domain knowledge is scarce or traditional augmentation methods fail. 3. Experimental Validation: The extensive experiments provide solid evidence of the methods benefits particularly its capability to enhance model performance significantly across different learning tasks including challenging domains like medical imaging.  Weaknesses: 1. Computational Efficiency: While the method is designed to be integrated with existing models seamlessly the paper lacks a detailed analysis of the computational costs involved especially when compared with traditional augmentation methods or other stateoftheart augmentation techniques. 2. Hyperparameter Sensitivity: The sensitivity analysis of the labelpreserving margin and data selection ratio is valuable yet it highlights potential challenges in tuning LPA3 for optimal performance across varied tasks and domains. This aspect could benefit from further exploration or automated tuning strategies. 3. Lack of Theoretical Justification: The paper provides a strong empirical basis for the proposed method but falls short in offering a deeper theoretical underpinning of why and how the specific form of the objective function and its surrogate lead to the observed improvements.  Questions and Suggestions for Authors: 1. Hyperparameter Tuning: Could the authors provide more insights or guidance on choosing the labelpreserving margin and data selection ratio Are there any rules of thumb or automated strategies that could simplify this process for practitioners 2. Comparison with Other Methods: How does LPA3 compare in terms of computational efficiency and scalability with other stateoftheart automated augmentation techniques especially in largescale datasets or more complex learning tasks 3. Extension to Other Tasks: The current work showcases LPA3s applicability to supervised semisupervised and noisylabel learning. Are there plans to explore its adaptation to fully unsupervised learning scenarios or other types of learning tasks not covered in this paper 4. DomainSpecific Augmentation Performance: While LPA3 is designed to be domainagnostic have the authors observed scenarios or patterns where it particularly excels or struggles Providing such analysis could further refine the understanding of its strengths and limitations.  Limitations and Societal Impact: The paper acknowledges the versatility and efficacy of LPA3 across various tasks and highlights its potential to work without domain knowledge. However it does not thoroughly address the computational cost implications which are crucial for practical deployments. Additionally there is a missed opportunity to discuss the potential negative societal impacts especially concerning privacy and safety in sensitive applications like medical imaging where aggressive augmentations might inadvertently introduce biases or distortions. Improvements in detailing these aspects would make the contributions more robust and socially responsible.", "ucNDIDRNjjv": "Peer Review for \"Nonstationary Transformers for Time Series Forecasting\" 1) Summary of the Paper The paper introduces a novel framework named Nonstationary Transformers to address the challenges in time series forecasting particularly concerning nonstationary realworld data. The main contributions of this work are twofold: Firstly the Series Stationarization module aims to enhance the predictability of time series by unifying the statistics of each input series followed by a Denormalization module that restores the statistical properties to the models output. Secondly to mitigate the issue of overstationarization which previous models fall into  losing the ability to capture significant temporal dependencies due to processing stationarized data the authors propose a Destationary Attention mechanism. This mechanism seeks to reintroduce the inherent nonstationary information into the model thereby enabling it to learn distinguishable attentions from the raw series. Experimental results across several time series forecasting benchmarks show that the proposed Nonstationary Transformers substantially outperform existing stateoftheart models indicating its potential to enhance predictive performance on nonstationary data. 2) Assessment of Strengths and Weaknesses Strengths:  The paper addresses a significant and practical problem in time series forecasting related to nonstationary data which is prevalent in various realworld applications.  The proposed framework introduces a novel approach to balance the predictability of stationarized series and the capability of models to capture essential temporal dependencies through Destationary Attention.  Experimental results demonstrate considerable improvement over stateoftheart models on multiple benchmarks showcasing the effectiveness of the proposed method.  Clear explanation and justification of the methodology accompanied by a thorough comparison with existing works. Weaknesses:  The experimental section lacks a detailed discussion on the computational complexity of the proposed framework compared to baseline models. Given the modifications to the Transformer architecture understanding the tradeoff between performance gain and computational overhead is essential.  While the paper mentions the potential negative societal impact briefly it does not provide an indepth discussion or mitigation strategies for potential misuse or biases that the model might introduce or exacerbate.  The paper could benefit from a broader exploration of the models performance on a wider range of nonstationary time series datasets including those with different characteristics or from other domains. 3) Questions and Suggestions for Improvement  Could the authors elaborate on the computational complexity of the Nonstationary Transformers compared to standard Transformer models particularly in terms of training time and resource requirements  While the Destationary Attention mechanism is innovative it would be beneficial to explore its scalability and adaptability across different types of time series beyond those evaluated. Are there scenarios where this mechanism might not be as effective  The impact of hyperparameter settings particularly in the Series Stationarization and Destationary Attention modules on model performance needs further exploration. The paper could provide more insights into the sensitivity of the model to these settings.  A deeper discussion on potential negative societal impacts and biases alongside strategies to mitigate these would strengthen the papers consideration of ethical implications. 4) Limitations and Societal Impact The authors briefly touch upon the potential negative societal impacts of their work but could provide a more comprehensive analysis. It is vital to consider how the model might perform on skewed or biased data and whether its predictions could inadvertently reinforce existing inequalities. Constructive suggestions for improvement include implementing fairnessaware techniques in the preprocessing or model training phases and conducting more exhaustive audits on the models behavior across diverse datasets to identify and mitigate bias. In summary the paper presents a compelling approach to enhancing time series forecasting on nonstationary data. Addressing the mentioned weaknesses and considerations could further solidify the works contributions and applicability in realworld scenarios.", "wo-a8Ji6s3A": "Review of \"Loopless Projection Stochastic Approximation Algorithm for Linearly Constrained Problems with Federated Learning\" 1. Brief Summary The paper introduces a novel algorithm called Loopless Projection Stochastic Approximation (LPSA) for tackling linearly constrained stochastic approximation problems with a special focus on Federated Learning (FL). The algorithm innovatively incorporates a probabilistic projection mechanism to adhere to feasibility constraints efficiently. This is conducted by performing projections randomly based on a probability parameter that varies with each iteration. The significant contributions involve theoretical analysis demonstrating the algorithms convergence properties both from asymptotic and nonasymptotic viewpoints. Theoretical results reveal a biasvariance tradeoff influenced by the relative magnitude of the projection probability and step size offering strategic insights into optimizing these hyperparameters to minimize projection complexity and improve efficiency. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper presents an innovative algorithm that simplifies the traditional doubleloop structure characteristic of similar stochastic approximation methods potentially improving computational efficiency in FL contexts.  Theoretical Depth: The rigorous theoretical analysis including asymptotic behaviors and convergence rates under various hyperparameter settings provides a solid foundation for understanding the algorithm\u2019s performance and potential applications.  Practical Relevance: Addressing computation and communication efficiency in FL represents a timely and impactful contribution given the growing importance of distributed machine learning systems.  Technical Rigor: The proposed novel proof technique exploiting jump diffusion approximation is noteworthy enhancing the technical sophistication of the stochastic optimization field. Weaknesses:  Experimental Validation: While the paper provides theoretical insights it relies heavily on synthetic datasets for empirical validation. Experiments involving realworld datasets and practical FL scenarios would greatly reinforce the claims.  Limitation Discussion: The discussion on limitations particularly on the practical implementation challenges and computational costs in realworld settings appears to be minimal. 3. Questions and Suggestions for the Authors  Scalability: Have the authors considered the scalability of LPSA in extremely large distributed systems It would be beneficial to discuss or experimentally demonstrate the algorithms performance in highly scalable environments.  Comparison: A more thorough comparison with existing algorithms especially in terms of computational complexity and communication efficiency in realworld FL setups would be helpful.  Hyperparameter Sensitivity: The paper could benefit from a deeper exploration into the sensitivity of the algorithms performance to the hyperparameter settings especially in nonideal conditions.  RealWorld Applicability: Providing case studies or examples of practical applications beyond synthetic data might enhance the papers appeal and relevance to practitioners in the field. 4. Limitations and Societal Impact The authors have outlined theoretical limitations and provided robust mathematical analysis to back their claims. However a detailed discussion on the practical limitations concerning the implementation of LPSA especially in diverse and possibly nonideal realworld environments would be valuable. Additionally mentioning the potential negative societal impacts if any associated with the deployment of this algorithm in sensitive applications (e.g. privacy concerns in FL) is recommended. To address these concerns the authors could consider proposing guidelines or safeguards that mitigate potential risks enhancing the responsible adoption of LPSA in various contexts. Constructive Suggestions for Improvement:  Augment the experiments with realworld datasets to validate the algorithms effectiveness and efficiency in practical FL scenarios perhaps comparing against stateoftheart methods.  Expand the discussion on the computational complexity and potential bottlenecks when deploying LPSA at scale providing insights into optimization or mitigation strategies.  Incorporate a section detailing practical implementation challenges that practitioners might face accompanied by potential solutions or workarounds to encourage wider adoption of the algorithm.", "k_XHLBD4qPO": "Review of the Paper \"Improving ClassIncremental Semantic Segmentation using Shared Wider Convolution Modules and Dropout Regularization\" 1. Summary of the Paper: This paper addresses the challenge of ClassIncremental Semantic Segmentation (CISS) within the domain of Continual Learning (CL) in computer vision. The authors identify and focus on the problem of semantic drift of the background class which exacerbates catastrophic forgetting. Unlike existing methods that rely on pseudolabeling knowledge distillation or model freezing the authors propose a novel approach to alleviate these issues. They suggest improving the expressiveness of the learned representations by (1) incorporating shared but wider convolution modules before the final classifiers to better scale for new continual tasks and (2) introducing dropout in the encoderdecoder architecture to enhance regularization and minimize overcompression of information. Empirical results demonstrate improvements in Intersection over Union (IoU) metrics across different CISS scenarios alongside maintaining a lower memory and computation footprint. Furthermore the paper introduces a novel benchmark setting aiming to align closer with realistic lifelong learning conditions. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The novel methodologies for mitigating semantic drift and enhancing model expressibility through wider convolution modules and dropout regularization present a significant step forward in addressing CISS challenges.  Empirical Validation: The paper provides compelling empirical evidence showing improvement in IoU scores across various CISS scenarios while maintaining computational efficiency.  Benchmark Contribution: The introduction of a new benchmark setting is commendable as it attempts to bridge the gap between theoretical models and practical realworld lifelong learning applications. Weaknesses:  Limited Validation Scope: While the empirical results are promising the validation appears to be limited to the PASCAL VOC 2012 dataset. Expanding experiments to include more diverse and complex datasets could further validate the models generality and effectiveness.  Potential Overfitting Concerns: Introducing dropout aims at reducing overcompression but there is a minimal discussion on whether these models could be prone to overfitting especially in scenarios with highly varied or sparse data distributions.  Societal and Ethical Considerations: Although the paper briefly mentions the positive societal impacts a more detailed exploration of the ethical implications of deploying such models especially in sensitive areas such as surveillance or autonomous navigation is lacking. 3. Questions and Suggestions for Authors:  Dataset Diversity: Could the authors provide insights or preliminary results on how their proposed method performs on more complex or largerscale datasets beyond PASCAL VOC 2012  Overfitting Analysis: It would be beneficial to include a discussion or analysis on the susceptibility of the proposed model to overfitting considering the introduction of dropout techniques and wider convolution modules.  Ethical Considerations: Can the authors elaborate on any potential ethical concerns and mitigation strategies related to deploying CISS models in realworld applications  Future Work Directions: What are the authors views on integrating their approach with unsupervised or semisupervised learning paradigms to further enhance model adaptability and efficiency in CISS tasks 4. Addressing Limitations and Societal Impact:  The authors have made an effort to address the limitations inherent to their work such as the potential for improved model generality by exploring additional datasets and learning paradigms. However a more substantial discussion on mitigating overfitting risks and ethical considerations would strengthen the paper.  While the paper acknowledges the positive societal impacts of more efficient and privacypreserving continual learning models a deeper analysis of potential negative impacts especially considering biases and misuse in surveillance applications would provide a more balanced perspective. Constructive suggestions include incorporating fairness and bias evaluation metrics into the benchmark and exploring differential privacy techniques to enhance data privacy further. Overall this paper contributes significantly to the field of ClassIncremental Semantic Segmentation within Continual Learning by proposing novel and effective solutions to enhance model expressibility and mitigate semantic drift. Expanding experimental validation addressing overfitting concerns and deeply considering societal impacts would further solidify this works contribution and applicability in advancing realworld continual learning applications.", "suHUJr7dV5n": " Peer Review for \"HessianFree Approach for Bilevel Optimization in Machine Learning Using a ZerothOrderLike Jacobian Estimator\"  1) Summary of the Paper The paper addresses the computational challenges in bilevel optimization prevalent in several machine learning applications by proposing a Hessianfree approach named PZOBO (Partial ZerothOrderlike Bilevel Optimizer). The crux of this approach lies in approximating the response Jacobian matrix via a zerothorderlike method which significantly reduces the computational demand without compromising performance. The authors theoretically analyze the convergence rate of PZOBO and its stochastic variant (PZOBOS) marking the first known nonasymptotic performance guarantee for Hessianfree bilevel algorithms through this novel approximation. Extensive experimental work demonstrates superior performance over existing methods across various applications including fewshot metalearning and hyperparameter optimization.  2) Strengths and Weaknesses  Strengths:  Novelty: The approach of approximating only the response Jacobian matrix as a component of the hypergradient in a bilevel optimization setup is innovative. This enables a significant reduction in computational complexity.  Theoretical Contribution: Providing a convergence rate analysis adds theoretical depth to the paper setting a groundwork for future studies in Hessianfree bilevel optimization strategies.  Experimental Validation: The comprehensive experiments conducted across multiple applications including the challenging domain of fewshot metalearning validate the effectiveness and generalizability of the proposed methods.  Weaknesses:  Lack of Comparison with Recent Advancements: While the paper presents comparison results with several baseline methods it might have overlooked some recent advancements in the domain that could serve as stronger baselines.  Limited Exploration of Limitations: Although PZOBO shows impressive performance the discussion on its limitations and potential failure cases is somewhat limited. A more thorough exploration of scenarios where PZOBO might underperform would be beneficial.  Potential Overemphasis on ZerothOrder Approximation: The paper primarily focuses on the benefits of zerothorder approximation possibly at the expense of underestimating or not fully exploring the benefits or applicability of first or secondorder methods in certain scenarios.  3) Questions and Suggestions for the Authors  Question: Could you elaborate on how PZOBO would scale with exponentially increasing data size particularly in the stochastic variant  Suggestion: It would be insightful to include a discussion on how the choice of hyperparameters (e.g. the number of Gaussian vectors Q) affects the performance and convergence rate of PZOBO and PZOBOS along with guidelines for practical applications.  Suggestion: A future direction could be exploring adaptive mechanisms within PZOBO to dynamically adjust the estimation accuracy of the response Jacobian matrix based on the optimization progress potentially enhancing both efficiency and effectiveness.  4) Limitations and Societal Impact The authors have made notable efforts to address the limitations by providing a convergence rate analysis and testing the proposed algorithms on various bilevel problems. However a more rigorous discussion on the scenarios where PZOBO might not offer substantial benefits especially concerning problem complexity and computational resources is needed. Regarding potential negative societal impacts the paper does not delve into this aspect which is understandable given the technical nature of the work. Yet it would be beneficial for the authors to consider how the adoption of such advanced optimization techniques could influence the development and deployment timelines of machine learning models potentially exacerbating the digital divide between entities with varying computational capabilities. Constructive Suggestion for Improvement: Incorporating a section that discusses ethical considerations and societal impacts \u2013 such as potential biases in optimizationfocused machine learning models or unequal access to computational resources \u2013 could provide a holistic understanding of the implications of these advanced algorithms.", "mjUrg0uKpQ": " Review of \"I2DFormer: A Transformerbased Framework for Zeroshot Learning from Noisy Textual Descriptions\"  Summary This paper presents a novel approach to zeroshot learning (ZSL) named I2DFormer which leverages unsupervised semantic embeddings derived from noisy online textual documents such as Wikipedia to align images with text in a shared embedding space without relying on humanannotated attributes. The core innovation is the introduction of a crossmodal attention module that refines the alignment between image patches and document words enabling the model to capture visually discriminative properties from textual descriptions effectively. The authors demonstrate that I2DFormer substantially outperforms stateoftheart methods in both zeroshot and generalized zeroshot learning tasks across multiple datasets providing qualitatively interpretable results where document words are grounded in corresponding image regions.  Strengths 1. Novelty and Relevance: The move towards using unsupervised semantic embeddings from freely available textual documents addresses a significant bottleneck in ZSL\u2014reliance on humanannotated attributes. This approach is both novel and highly relevant offering scalable solutions for ZSL. 2. Strong Quantitative Performance: The empirical results are compelling showcasing significant improvements over existing unsupervised semantic embedding methods and benchmarks in ZSL performance. 3. Interpretability: The ability of I2DFormer to ground document words in image regions enhances the interpretability of ZSL models contributing to the understandability and trustworthiness of AI systems in decisionmaking. 4. Generality: The demonstrated ability to integrate the learned document embedding with any existing ZSL model to improve its performance is a significant advantage potentially broadening the impact of this work across various ZSL applications.  Weaknesses 1. Limited Scope of Textual Sources: While Wikipedia and online encyclopedias are rich sources of textual information they may not cover all domains equally. The approachs effectiveness in domains with scant or highly specialized online textual resources remains untested. 2. Potential for Bias: Relying on online textual documents could introduce biases present in those texts into the ZSL models. Although the paper mentions the possibility of unintended biases as a downside it does not provide a clear mitigation strategy. 3. Complexity and Computational Resources: The transformerbased framework especially with the crossmodal attention module may require substantial computational resources for training potentially limiting accessibility for researchers with limited resources.  Questions and Suggestions for the Authors 1. Domain Limitations: Can the authors comment on the applicability of the I2DFormer approach to domains where textual descriptions are scarce highly technical or ambiguous 2. Bias Mitigation: It would be beneficial if the paper could discuss potential methods or practices to identify and mitigate biases in the unsupervised semantic embeddings derived from online documents. 3. Efficiency Improvements: Are there strategies the authors would suggest or are considering to reduce the computational demands of the model without significantly sacrificing performance  Limitations and Societal Impact While the authors mention the potential for unintended biases from unsupervised online content a more thorough examination and explicit strategies for addressing these biases are necessary to ensure the ethical use of the technology. Additionally discussing the environmental impact of training computationally intensive models like transformers would add valuable context regarding model sustainability. Constructive suggestions include exploring model quantization or pruning techniques to improve efficiency and reduce the carbon footprint and developing guidelines for sourcing and preprocessing textual content to minimize bias. In summary I2DFormer represents a significant step forward in zeroshot learning by effectively utilizing unsupervised semantic embeddings from noisy textual descriptions. With improvements in addressing biases computational efficiency and domain limitations this approach has the potential to make ZSL more accessible and ethically sound across a wider range of applications.", "lme1MKnSMb": " Peer Review of \"Simplifying Neural Video Compression with Transformers\"  1) Summary: This paper introduces a novel application of transformers to the problem of neural video compression a field that has traditionally relied on convolutional neural networks (CNNs) with complex architectural biases such as motion prediction and warping operations. The authors propose a transformerbased model the Video Compression Transformer (VCT) that independently maps input frames to representations and models temporal dependencies among these representations simplifying the video compression process. VCT demonstrates superior performance on standard video compression datasets and handles complex motion patterns effectively as shown through experiments with synthetic data. The paper also emphasizes ease of implementation and provides code for further research.  2) Assessment: Strengths:  Simplification and Performance: The paper successfully simplifies the video compression pipeline by eliminating traditional CNN biases and operations yet improves performance on benchmark datasets. The ability of VCT to learn complex motion patterns without explicit motionrelated components is indeed impressive.  Theoretical Innovation: The adaptation of transformers traditionally used in NLP to the video compression domain encapsulates a significant leap in thought. This not only serves the specific goal of improved video compression but opens avenues for transformer applications beyond language and static image processing tasks.  Extensive Experiments and Open Source: The comprehensive experiments across real and synthetic datasets coupled with the opensourcing of code and models are commendable. These efforts significantly contribute to replicability and future research in the field. Weaknesses:  Lack of Comparative Analysis on Encoder and Decoder Architectures: While the focus on transformers is clear the paper does not sufficiently explore or discuss the possibility of bottleneck factors within the encoderdecoder architecture used. A comparative analysis or a discussion on how different architectures might impact the overall performance could provide deeper insights.  Limited Consideration of Runtime Performance and Efficiency: There is scant discussion on the runtime performance or computational efficiency of the proposed method particularly in comparison to existing approaches. Given the inherent complexity of transformers understanding their operational efficiency in realworld scenarios remains crucial.  Potential for Broader Analysis of Temporal Window and Memory: The paper mentions future work could explore scaling up the temporal window or introducing more longterm memory. However an initial exploration or discussion on potential implications and the current limitations would have been beneficial for a deeper understanding.  3) Questions and Suggestions for Authors:  Runtime and Efficiency: Can the authors provide insights into the runtime efficiency and computational demands of VCT especially in comparison with existing CNNbased methods Suggestions for optimizing these aspects could also be beneficial.  Impact of EncoderDecoder Architecture: How might different encoder and decoder architectures impact the performance of VCT A discussion or even preliminary experiments in this direction could enrich the current findings.  Extended Temporal Dependency Models: The paper briefly touches upon the potential for exploring broader temporal windows or different models of memory. Could the authors speculate on the possible models or mechanisms that might be particularly suited for this expansion  Training Data Diversity and Bias: Given the reliance on learned temporal dependencies how does the diversity or bias within training datasets affect the models generalizability and performance across varied video content  4) Limitations and Societal Impact: The paper provides a brief discussion on its societal impact highlighting potential benefits in bandwidth reduction and storage. However it falls short in addressing the limitations adequately. Specifically the implicit assumptions around independence in block processing and the conditional independence within tokens deserve more attention regarding their impact on compression performance and error resilience. Additionally while the authors mention the lack of reliance on architectural biases as a strength the ramifications of this choice on the adaptability of the model to unforeseen video content types or compression scenarios could constitute a limitation. Suggestions for Improvement:  Expanding the Limitations section to address the above concerns and including a discussion on the environmental impact of deploying more computationally expensive models like transformers in largescale video compression tasks.  A more nuanced discussion on the potential negative societal impacts such as privacy concerns with more efficient video storage and transmission could provide a wellrounded view of the technologys impact.", "mNtFhoNRr4i": "Summary and Contributions: The paper presents a comprehensive approach to evaluating hierarchical classifiers which are crucial for problems where classes can be organized into a treelike structure. The primary contributions include an efficient algorithm for generating operating characteristic curves for hierarchical classification which allows for a detailed comparison of classifiers across different specificitycorrectness tradeoffs. This work challenges the prevailing focus on leafnode prediction by demonstrating the utility and performance of classifiers on nonleaf nodes. Two novel loss functions are introduced: a soft variant of the structured hinge loss termed as the softmaxmargin loss which significantly outperforms traditional flat classifiers and the softmaxdescendant loss though found ineffective. The paper also addresses the performance of topdown classifiers elucidating their comparative effectiveness in recognizing unseen classes. Strengths: 1. Novelty in Evaluation: The introduction of an algorithm to generate operating characteristic curves for hierarchical classifiers represents a significant methodological advance enabling a more nuanced comparison of classifiers than previously possible. 2. Comprehensive Comparative Analysis: The empirical study especially the comparison across multiple loss functions and classifier strategies on benchmarks like iNat21 and ImageNet1k provides valuable insights into the landscape of hierarchical classification approaches. 3. Robustness to Unseen Classes: The papers exploration into the ability of hierarchical classifiers to generalize to unseen classes addresses an important aspect of classifier performance which is especially relevant in realworld applications where the training data may not cover all possible scenarios. 4. Accessible Resources: Providing code online enhances the replicability of the research and facilitates further exploration by the community. Weaknesses: 1. Limited Discussion on Softmaxdescendant Loss: While the paper introduces two novel loss functions the softmaxdescendant loss is found to be ineffective and the discussion around its limitations or potential improvements is somewhat lacking. 2. Scalability Concerns: The paper mentions the challenge of scaling fullyconnected prediction layers to millions of classes but does not thoroughly explore potential solutions or alternative architectures that could address this issue. 3. Margin Manual Design: The reliance on manual design for the margin in the softmaxmargin loss could introduce subjectivity or require extensive tuning to optimize performance across different datasets or classification tasks. Questions and Suggestions: 1. Improving Softmaxdescendant Loss: Could further refinements or alternative formulations of the softmaxdescendant loss make it more effective Exploring this could potentially enhance the papers contributions. 2. Scalability Solutions: It would be beneficial to discuss or even explore preliminary approaches to addressing the scalability issues mentioned. Techniques like embeddings or metric learning could be considered. 3. Automated Margin Tuning: Could methods for automatic margin tuning be devised for the softmaxmargin loss to alleviate the need for manual design and potentially improve model performance Limitations and Societal Impact: The authors touch upon essential considerations regarding the limitations and potential negative societal impacts of their work. However a more detailed exploration into mitigating strategies for the inherent risks associated with automatic classification could strengthen the paper. For instance addressing potential biases in hierarchical classification and exploring safeguards against misclassification in sensitive applications could be valuable additions. Constructively further research into automatically adapting the class hierarchy based on emerging data or trends could help address some scalability and adaptability concerns.", "t6O08FxvtBY": "Review of \"Model Pruning via Bilevel Optimization\" 1) Summary: The manuscript introduces a novel model pruning method that leverages bilevel optimization (BLO) to efficiently find highquality subnetworks or winning tickets within large deep learning models. The proposed Bilevel Optimizationoriented Pruning method (BIP) employs a technicallygrounded construction allowing for an effective coupling of the pruning and retraining processes without the prohibitive computational overhead associated with iterative magnitude pruning (IMP). The research demonstrates through extensive experiments that BIP outperforms IMP and various oneshot pruning methods in finding better winning tickets across multiple datasets and architectures while also proving to be computationally more efficient. 2) Assessment: Strengths:  The paper addresses a critical bottleneck in deep learning model deployment\u2014computational efficiency\u2014while maintaining or improving model accuracy via advanced pruning techniques. The novel use of bilevel optimization in this context is both technically interesting and practically valuable.  The proposed BIP method shows marked improvements over existing stateoftheart methods notably IMP in terms of pruning efficiency and the quality of discovered subnetworks across several benchmark datasets and model architectures.  The authors provide a comprehensive experimental setup and thorough analysis of results which substantiate the claims made regarding BIPs superior performance and efficiency. The availability of code is also beneficial for reproducibility and further research. Weaknesses:  While the paper is strong in its technical contributions and empirical validation it could offer a more explicit discussion regarding the limitations of the proposed method. For instance does BIPs performance scale across different domains beyond standard image classification tasks  The manuscript could benefit from a deeper exploration of the theoretical implications of employing BLO for model pruning particularly in terms of understanding the landscape of possible pruning solutions and the convergence properties of BIP. 3) Questions and Suggestions:  Could the authors comment on any domainspecific limitations they anticipate for BIP Are there types of models or datasets where BIP might underperform relative to IMP or other baseline methods  It would be insightful to see a comparison not just in terms of model accuracy and sparsity but also in computational savings when deployed in a realworld scenario. Can the authors provide some insights or preliminary results on this  A technical deep dive into the reasons behind BIPs success in finding winning tickets more consistently than IMP would add to the papers depth. Is it purely due to the computational efficiency of BLO or are there other factors at play  The choice of hyperparameters (e.g. learning rates \u03b1 and \u03b2 regularization parameter \u03b3) appears critical to BIPs performance. It would be useful for the authors to elaborate on the sensitivity of BIP to these hyperparameters and offer guidance on their selection for future users. 4) Limitations and Societal Impact: The manuscript briefly touches on the implications of model pruning for deployment in resourceconstrained environments but could further discuss potential negative societal impacts. For example while model efficiency is broadly positive the increased accessibility of powerful models could facilitate malicious uses of AI technology. Additionally while not a limitation per se the paper does not address how pruning might affect model interpretability or bias. Offering suggestions for how to mitigate such risks or guiding questions for future research in this direction would enhance the papers contribution to the field. In conclusion the paper represents a significant step forward in model pruning research introducing an innovative method that outperforms existing approaches in both efficiency and effectiveness. With additional discussion on the broader implications and potential limitations of the proposed method the paper would be wellpositioned for a strong impact within the machine learning community.", "wA7vZS-mSxv": "Peer Review of \"Estimating Intrinsic Dimensionality using Normalizing Flows\" 1) Summary This paper introduces a novel method for intrinsic dimensionality (ID) estimation in datasets. Traditional approaches notably those reliant on nearest neighbor statistics face scalability issues with large or highdimensional datasets due to their quadratically scaling complexity. The proposed method circumvents these limitations via the use of Normalizing Flows (NFs) which are capable of efficiently handling datasets of substantial size and dimensionality. The approach relies on calculating the changes in the singular values of the flow\u2019s Jacobian concerning varying magnitudes of injected noise differentiating between manifold (large variability) and offmanifold (small variability) directions. The authors substantiate their methodology through tests on several datasets including highresolution RGB images and achieve stateoftheart results in estimating the ID. 2) Assessment Strengths:  Novelty: The method presents a significant departure from nearest neighbor statistics addressing the scalability issue headon. The application of NFs for ID estimation and the theoretical foundation based on singular value behavior are highly original.  Scalability and Dimensionality: The demonstrated capability to scale to large and highdimensional datasets without compromising accuracy is a considerable advantage broadening the utility of ID estimation in practical realworld scenarios.  Versatility and Performance: The ability of this method to accurately estimate the ID across different types of manifolds and datasets including complex RGB image datasets is impressive. The demonstrated results are compelling and represent a considerable advance in the field. Weaknesses:  Complexity and Practical Implementation: The methodology while powerful is complex and may pose implementation challenges for practitioners unfamiliar with NFs. Additionally the computational demands particularly for training multiple NF models across different noise magnifications could be a limiting factor in certain applications.  Sensitivity to Parameters: The methods success hinges on the selection of noise magnitudes and the number of NFs trained. Though the paper argues that three NFs are generally sufficient this may not hold across all dataset types and sizes requiring further exploration.  Limited Exploration of Limitations: While the paper briefly mentions adapting the methodology to bounded data it does not fully address potential challenges or limitations in this context. More detailed exploration and solutions would enhance the applicability and robustness of the proposed method. 3) Questions and Suggestions for the Authors  On Noise Magnitude Selection: Can the authors provide more clarity or guidelines on selecting noise magnitudes and the number of NFs to train Are there heuristics or adaptive methods that could simplify this process  On Computational Efficiency: While scalability is a strength the computational demands of the approach are not extensively discussed. Could the authors comment on the computational efficiency and potential optimizations  On Bounded Data: The methods adaptation to bounded data (e.g. RGB images) is intriguing but briefly touched upon. Could the authors elaborate on this adaptation and any potential challenges or considerations Limitations and Societal Impact The paper could benefit from a more detailed discussion on the limitations related to data bounds and computational efficiency. Addressing these points would provide a clearer picture of the methods applicability and potential challenges. Additionally the societal impact section mainly notes the absence of direct negative impacts due to the theoretical nature of the work. However expanding on the potential positive impacts especially in areas reliant on efficient highdimensional data analysis (e.g. bioinformatics image analysis) would underscore the significance of this contribution. Constructively exploring any indirect consequences such as data privacy concerns in applied contexts or the environmental impact of computationally intensive processes could provide a more comprehensive assessment.", "v9pljSdlUNP": " Peer Review on \"Stochastic Backpropagation for Memory Efficient Training of Deep Neural Networks\"  1) Summary The paper presents an indepth study of Stochastic Backpropagation (SBP) for training deep neural networks in image classification and object detection tasks aiming at reducing GPU memory usage and computational costs. SBP works by utilizing only a subset of feature maps during the backward pass to calculate gradients applying what the authors call \"backpropagation dropout.\" This results in considerable memory savings and a slight reduction in training time with minimal impact on model accuracy. The authors explore the design space of SBP formulate its process and demonstrate its generalizability and efficacy through experiments with image models specifically using ViT and ConvNeXt architectures. They also evaluate different design strategies for SBP such as the extent of gradient dropout gradient keepratio and masking strategies for sampled gradients.  2) Assessment Strengths:  Innovation: The paper introduces an efficient and generalized implementation of SBP a novel approach that significantly reduces memory requirements during training without substantially sacrificing accuracy.  Depth of Analysis: The authors provide a thorough analysis of SBPs effects on gradient calculations and explore various design strategies to optimize performance tradeoffs.  Experimental Validation: The experiments are well designed and demonstrate SBPs effectiveness across different tasks (image classification and object detection) and architectures (ViT and ConvNeXt) showcasing the approachs practical applicability. Weaknesses:  Comparative Analysis: The paper lacks a detailed comparison of SBP with other memoryefficient training techniques beyond theoretical discussion. An empirical comparison could better position SBP within the existing landscape of memorysaving methods.  Detail on Implementation Challenges: While the paper provides a highlevel overview of implementing SBP it may benefit from a deeper dive into potential challenges or nuances encountered during implementation or integration into existing frameworks.  Impact on Training Dynamics: The paper could further explore how SBP affects other aspects of training dynamics such as convergence rates the stability of training with highly stochastic gradients or the need for hyperparameter adjustments.  3) Questions and Suggestions for the Authors  Clarification on Dropout Strategy: Could the authors elaborate on the criteria used for selecting the feature maps that undergo dropout Is there a principled method or is it purely random  Impact on Model Robustness: Have the authors considered studying the impact of SBP on the robustness of the trained models to adversarial attacks or noise given the altered gradient propagation  Extension to Other Domains: The study focuses on image classification and object detection. Could the authors speculate on or provide insights into how SBP might perform in other domains such as natural language processing or generative models  HardwareSpecific Performance: The paper mentions GPU memory savings but could the authors discuss the applicability or potential benefits of SBP on other types of hardware like TPUs or specialized AI accelerators  4) Limitations and Societal Impact The authors have acknowledged the slight accuracy tradeoff entailed by SBP and noted the minimal increase in training speed. They could further elaborate on potential strategies to mitigate these accuracy losses or improve speed gains. Additionally while the paper briefly touches on the potential misuse of deep learning models trained with SBP a more detailed discussion on the specific negative societal impacts and how they might be mitigated or prevented would be beneficial. Suggestions include enhancing transparency around the use cases of SBPtrained models and implementing ethical guidelines for their deployment.", "vKBdabh_WV": "The paper introduces a novel approach called Meta Optimal Transport (Meta OT) which leverages amortized optimization to predict optimal transport (OT) maps between different measures significantly reducing the computational time required to solve subsequent OT problems by utilizing information from previously solved problems. Standard methods treat each OT problem as isolated ignoring the potential benefits from already computed solutions. The authors instantiate Meta OT models in both discrete and continuous (Wasserstein2) settings across various data types such as images spherical data and color palettes. Their approach improves computational efficiency by orders of magnitude compared to conventional OT solvers. Strengths: 1. Innovative Approach: The concept of Meta OT is original and addresses an important limitation in the current optimal transport methods by reusing information across related OT problems thus improving efficiency. 2. Broad Applicability: The authors demonstrate the versatility of Meta OT across different settings and data types showcasing its wide applicability. 3. Significant Performance Gains: The paper reports substantial improvements in computational time which could be transformative for applications that require solving multiple OT problems. Weaknesses: 1. Generalization Concerns: While Meta OT shows impressive computational savings the paper lacks a detailed exploration of how well these models generalize across significantly different OT problems. 2. Limited Comparison with Baselines: The comparisons mainly focus on computational efficiency. A more comprehensive comparison including the quality of the transport maps and their applications would be beneficial. 3. Model Complexity and Overhead: The process of training Meta OT models especially in continuous settings and the prerequisites for amortization could introduce significant overhead. The paper could further clarify these aspects. Questions and Suggestions for the Authors: 1. Generalization Ability: Could you provide more insights or empirical evaluations on how well Meta OT models generalize to OT problems that are substantially different from those seen during training 2. Comparison with Nonamortized Approaches: Besides computational time how do the transport maps predicted by Meta OT compare with those from traditional methods in terms of quality and applicability in downstream tasks 3. Model Overhead and Training Complexity: Can you elaborate on the complexity of training Meta OT models especially for continuous settings and discuss any strategies to mitigate potential overheads Limitations and Societal Impact: The authors should discuss potential limitations regarding the scalability of Meta OT models with increasing diversity and complexity of OT problems. While the computational benefits are clear understanding the tradeoffs in terms of training resources and model complexity is important. Additionally addressing the potential negative societal impact although not immediately obvious should involve considering scenarios where the rapid computation of OT maps could be misused such as privacy invasions in data transfer applications. Suggestions for improvement could involve proposing mechanisms to ensure the ethical use of Meta OT and incorporating fairness considerations in the model development process to prevent biased outcomes in transport map predictions.", "pGcTocvaZkJ": "Review of \"Censored Quantile Regression Neural Network (CQRNN): A Novel Approach for Predicting Quantiles in Censored Data Using Neural Networks\" 1) Brief Summary: This paper introduces an innovative algorithm for performing quantile regression on censored data leveraging neural networks (NNs) contributing to the field of survival analysis. Traditional methods tackle quantile regression in linear models and face efficiency issues with neural networks due to the necessity of training individual networks for each quantile. The authors adeptly apply an existing algorithm for linear models to neural networks and present their primary contribution: a novel algorithm named CQRNN. This algorithm allows simultaneous optimization of multiple quantiles within a single neural network architecture. Through theoretical analysis they establish that CQRNN can be viewed as a form of expectationmaximization with a selfcorrecting nature ensuring more accurate calibration of quantile estimates across various datasets. The effectiveness of CQRNN is validated through rigorous experimentation against existing methods across both synthetic and real datasets showing superior calibration in most cases. 2) Assessment: Strengths:  Novelty: The work is commendable for its innovative approach to extending Portnoys estimator to neural networks for quantile regression addressing inefficiencies inherent in previous methods.  Theoretical Insights: The paper provides a solid theoretical foundation for CQRNN showing it can be interpreted through the lenses of expectationmaximization and demonstrating a beneficial selfcorrecting property.  Empirical Validation: Extensive experiments across a diverse set of datasets showcase the method\u2019s robustness and superior performance which significantly strengthens the papers claims.  Practical Relevance: By enhancing the efficiency and accuracy of quantile prediction in censored data this work has significant practical implications especially in fields like biomedical sciences and reliability engineering where understanding uncertainty is crucial. Weaknesses:  Global Convergence: The authors acknowledge that the theoretical analysis does not guarantee convergence to a global optimum which might limit the algorithms efficacy in certain scenarios.  Domain Specificity: Although CQRNN shows promise across tested datasets the majority are from the biomedical domain. Its performance and applicability to data from other fields with different characteristics remain less clear.  Handling of Extreme Quantiles: Its mentioned briefly that cases where higher quantiles are undefined should be approached with caution. This limitation could be significant in practice and more discussion or a proposed solution would strengthen the paper. 3) Questions and Suggestions:  Global Convergence: Can the authors provide further insight or potential strategies for ensuring global convergence of the CQRNN algorithm Even speculative approaches would be valuable for future research directions.  DomainGeneralization: Have the authors considered applying CQRNN to datasets from domains markedly different from biomedical sciences such as finance or social science where censored data also commonly occurs  Extreme Quantiles: It would be beneficial if the authors could elaborate on the issues encountered with undefined higher quantiles and propose a preliminary strategy or adjustment to the CQRNN algorithm to address this challenge.  Software Implementation: Is there a plan to extend the provided code with more userfriendly tools or integrate it into existing machine learning frameworks to facilitate wider adoption of the CQRNN approach 4) Limitations and Societal Impact: The authors adequately discuss the limitations including the lack of guarantee for global convergence and the algorithms primary testing within the biomedical domain. Delving deeper into the treatment of datasets where higher quantiles are undefined would make the discussion more comprehensive. The potential negative societal impact seems minimal yet a brief consideration of ethical implications especially regarding the deployment in sensitive domains like healthcare could be insightful. Encouragingly this work sets a solid foundation for more accurate and efficient predictions in survival analysis likely benefiting fields with high stakes in understanding data uncertainty. Improvements:  Expanding the theoretical framework to explore conditions under which global convergence might be more reliably achieved.  Broadening the empirical evaluation to include more varied domains which would enhance the generalizability of the findings.  Offering strategies or modifications to better handle cases with undefined quantiles enhancing the methods practical applicability. In conclusion this paper makes a significant contribution to the field of survival analysis by introducing an efficient theoretically grounded method for quantile regression on censored data using neural networks. The novel CQRNN algorithm represents a substantial advancement over existing methods promising to impact various fields where accurate quantification of uncertainty is crucial.", "xuw7R0hP7G": " Review of the Paper on Gradient Flow and Stochastic Gradient Descent Convergence Analysis  Summary This paper presents a comprehensive analysis of the convergence properties of Stochastic Gradient Descent (SGD) in the context of nonconvex optimization problems. Leveraging the convergence properties of Gradient Flow (GF) on the population loss the study proposes a novel framework characterized by general conditions under which SGD converges assuming the convergence of GF on the population loss. The centerpiece of the authors argument is a converse Lyapunovlike theorem establishing that convergence in GF implies the existence of a Lyapunov potential under mild assumptions. Through this the paper articulates a connection between the convergence rates of GF and the geometrical attributes of the objective function as well as demonstrating the necessity of selfbounding properties for ensuring the convergence of both Gradient Descent (GD) and SGD. Notably the analysis spans various setups including classical convex losses K\\xc5\\x81 functions and complex problems like Phase Retrieval and Matrix Square Root enriching current perspectives on SGDGDs operational mechanism beyond conventional settings.  Strengths and Weaknesses Strengths: 1. Novel Theoretical Contributions: The paper introduces significant theoretical advancements especially in linking the convergence of GF with SGD through the converse Lyapunov theorem. This not only contributes to the deeper understanding of SGDs behavior in nonconvex settings but also broadens its potential applicability. 2. Unified Framework: By providing a unified analysis applicable to a variety of loss functions and problem settings the paper offers a versatile toolset for understanding and predicting the performance of SGD and GD in scenarios that were previously challenging to analyze. 3. Practical Insights: Through the exploration of specific applications like Phase Retrieval and Matrix Square Root problems the study bridges theoretical findings with practical considerations potentially guiding future algorithmic improvements and applications. Weaknesses: 1. Assumptions and Generality: While the paper makes strides in generalizing the analysis of SGD convergence the reliance on certain assumptions (such as the properties of the Lyapunov potential and noise regularity conditions) might limit the frameworks applicability. The exact conditions under which these assumptions hold in practical scenarios could benefit from further clarification. 2. Complexity of Theoretical Framework: The high degree of mathematical sophistication and the complexity of the proposed framework might pose barriers to understanding and implementation for practitioners. Simplification or a more intuitive explanation of key concepts could enhance accessibility.  Questions and Suggestions for the Authors 1. Clarification on Assumptions: Could the authors provide more detailed examples or discussions on the practical scenarios where the assumptions made in the paper (e.g. selfbounding regularity conditions) hold true 2. Impact of Hyperparameters: The paper briefly mentions the selection of step sizes but does not delve deeply into how other hyperparameters (e.g. minibatch size in SGD) might influence the convergence properties. Can the authors comment on this aspect or suggest how the framework might accommodate these factors 3. Extension to More Complex Settings: Given the rapid advancement in machine learning models particularly deep learning an interesting direction would be to explore how the framework proposed in this paper applies to modern overparameterized models. Do the authors plan to extend their analysis to such settings  Limitations and Societal Impact The paper presents a thoughtful discussion on the limitations pertinent to the assumptions made for the theoretical framework. However a more explicit discussion on the potential negative societal impacts especially in scenarios where the SGD fails to converge due to unmet assumptions could be valuable. This might include the implications of model failure in highstakes applications (e.g. healthcare autonomous driving) or biases induced by convergence behaviors. Constructive suggestions for improvement include enhanced elucidation of the practical implications of the theoretical framework and a broader discussion on the applicability and limitations in realworld scenarios. Additionally future work could explore extending the theoretical analysis to encompass a wider variety of learning scenarios including those involving complex data distributions and model architectures.", "sMezXGG5So": "Review of \"NODEFORMER: Scalable Transformers for Efficient Message Passing in Graph Neural Networks\" 1) Summary: This paper introduces NODEFORMER a novel model that addresses critical challenges in Graph Neural Networks (GNNs) notably the issues tied to oversquashing heterophily managing longrange dependencies edge incompleteness and particularly the handling of data without inherent graph structures. By innovatively proposing an allpair messagepassing scheme that scales to large networks without sacrificing performance NODEFORMER establishes a pioneering approach to node classification on large graphs. The model employs a kernelized GumbelSoftmax operator to efficiently learn latent graph structures enabling the reduction of algorithmic complexity from quadratic to linear with respect to node numbers. Extensive experiments are presented demonstrating the models effectiveness in various tasks such as node classification on graphs (including those with up to 2 million nodes) and graphenhanced applications where input graphs are missing. 2) Assessment: Strengths:  Scalability: The proposed solution adeptly scales to large graphs a significant strength given the computational challenges associated with handling largescale networks in traditional GNNs.  Innovative Approach: The kernelized GumbelSoftmax operator is a novel addition to the field offering a practical approach to learning latent graph structures efficiently.  Extensive Experiments: The varied dataset experiments including both node classification tasks and graphenhanced applications robustly showcase the models versatility and effectiveness.  Theory and Practice Balance: Providing accompanying theory along with practical application and code availability is a commendable strength ensuring reproducibility and deeper understanding. Weaknesses:  Generalization across Datasets: While the experiments demonstrate significant improvements a deeper exploration into the models performance on a wider range of datasets and tasks could better establish its generalizability.  Complexity and Interpretability: The intricate design and proposed solutions while effective may pose challenges in understanding and extending the model without a thorough grasp of its underlying principles.  Comparison with SOTA: The paper presents a comparison with stateoftheart models however a more comprehensive set of comparisons including recent advancements in scalable GNNs could provide clearer positioning of NODEFORMER. 3) Questions and Suggestions for the Authors: 1. Model Extensions: Could NODEFORMER be readily extended or modified to address graphlevel prediction tasks and if so what changes would be necessary 2. Theoretical Analysis: The paper briefly mentions theoretical guarantees. Could the authors expand on any limitations of the theoretical model especially in terms of practical applicability 3. Impact of Hyperparameters: The GumbelSoftmax operators temperature coefficient seems critical. Could the authors comment on the sensitivity of model performance to this and other hyperparameters 4. Comparison Breadth: Adding comparisons with other recent scalable GNNs or Transformer models could enhance the papers impact. Are there reasons specific models were chosen for comparison over others 4) Limitations and Societal Impact: While the authors have made strides in addressing algorithmic complexities in largescale graph networks the paper could benefit from a deeper discussion on the limitations particularly regarding the models adaptability to varying data characteristics and potential biases introduced by learned latent structures. As for societal impact there is a passing mention of potential applications but a more thorough exposition on how NODEFORMER could potentially influence areas with sensitive ethical considerations (e.g. social network analysis surveillance) would be valuable. Suggestions for improvement include:  Expanding the discussions section to include potential limitations and biases in learned structures.  Providing guidelines or considerations for ethical use particularly in sensitive application areas. Conclusion: NODEFORMER represents a significant advancement in the field of graph neural networks particularly for scalable and efficient message passing in largescale graph data. Despite its strengths a deeper exploration into its limitations broader applicability and ethical considerations would further enhance its contribution to the field.", "uOJZ_zU9qZm": "Review of \"Automatic Propagation Software for Composable Construction of Learning Algorithms\" 1. Summary of the Paper: The paper introduces a pioneering software framework known as Automatic Propagation (AP) software aimed at addressing the limitations posed by existing automatic differentiation (AD) frameworks in terms of flexibility for composing learning algorithms. The authors develop Proppo a prototype built atop the Pytorch AD framework to instantiate their concept of AP software. Proppo enhances the potential for modular and efficient composition of complex learning algorithms by introducing the concept of propagators which automate the execution of customized learning algorithms underlying computations. The software is demonstrated through the implementation of advanced Monte Carlo gradient estimation techniques showcasing significant improvements in gradient estimation accuracy especially in conditions typified by chaotic system dynamics. This work is positioned within the broader discourse on enabling more versatile and efficient computational frameworks to accelerate the development of novel machine learning algorithms. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovation: The concept of AP software represents a significant leap forward in ML software design enabling a level of modularity and flexibility in algorithm implementation previously unattainable with traditional AD frameworks.  Implementation and Demonstrations: The authors successfully operationalize their concept through Proppo and provide concrete examples demonstrating its efficacy in improving gradient estimation techniques thereby providing both proofofconcept and practical utility.  Impact Potential: Given the foundational nature of learning algorithm design in machine learning research and application the proposed AP framework could have farreaching implications for the future of ML development making it easier to experiment with and deploy new algorithms. Weaknesses:  Breadth of Evaluation: While the improvements in gradient estimation accuracy are compelling the evaluation is somewhat narrow. Expanding the scope of experimentation to include a wider range of learning algorithms and more complex realworld scenarios could strengthen the case for the broad applicability and utility of Proppo.  Comparative Analysis: The paper would have benefited from a more comprehensive comparison with existing approaches or frameworks that aim to overcome similar limitations of AD frameworks. This could include a performance benchmarking against these methods in terms of computation speed memory efficiency and flexibility.  User Accessibility: While the paper elucidates the technical architecture of Proppo and its theoretical advantages it provides limited insight into the user experience. Understanding the learning curve and ease of adoption for ML practitioners could be crucial for its widespread acceptance. 3. Questions and Suggestions for the Authors:  Could the authors provide further examples or case studies where Proppo has been applied to diverse machine learning domains beyond what was detailed in the paper  It would be interesting to see a more detailed comparison possibly through a benchmarking exercise against other AD frameworks or tools designed to increase the flexibility of learning algorithm composition. How does Proppo perform in terms of computational efficiency ease of use and flexibility  A discussion on the potential roadmap for Proppo particularly in terms of community engagement library support and integration with other ML tools could provide readers with insight into its longevity and potential for impact. 4. Limitations and Societal Impact: The authors make a significant leap in addressing the papers limitations and societal impact implicitly through the potential of Proppo to accelerate ML research and application development. However a more explicit discussion on the limitations regarding scalability potential barriers to adoption and any inherent biases that might be propagated through the use of this tool would make the paper more comprehensive. Additionally while the positive implications of accelerated ML algorithm development are clear addressing the potential for misuse or unintended consequences especially in sensitive applications could provide a more balanced view of the societal impact. Suggestions for Improvement:  Expanding the experimental evaluation to include a broader range of algorithms and realworld scenarios could demonstrate the versatility and robustness of Proppo.  A comparative analysis with existing techniques would help in positioning Proppo more clearly within the landscape of ML tools.  The authors are encouraged to include a usercentric discussion or qualitative study on the adoption and implementation of Proppo to shed light on its practicality for everyday use by ML practitioners. In conclusion this paper presents a groundbreaking approach to ML software development that with further development and exploration has the potential to significantly enhance the efficiency and innovation in machine learning algorithm design and implementation.", "uxWr9vEdsBh": " Review of \"Active Learning via Lagrangian Duality (ALLY)\"  Summary: The paper addresses the poolbased active learning problem which aims to efficiently label a subset of training data to improve model performance under a constrained budget. The authors introduce a novel method named Active Learning via Lagrangian duality (ALLY) leveraging constrained learning and a primaldual approach. By optimizing primal and dual variables where dual variables indicate the informativeness of samples ALLY selects a diverse set of unlabeled samples for labeling. The paper shows that ALLY performs comparably or better than stateoftheart methods on classification and regression tasks. It also illustrates how the approach can generate maximally informative samples through active sampling.  Strengths: 1. Novelty: The introduction of Lagrangian duality into the active learning framework is novel and intriguing. The methods ability to inherently balance diversity and informativeness through dual variables is a significant theoretical contribution. 2. Unified Framework: ALLY provides a unified framework applicable to both classification and regression tasks. This versatility is a considerable strength given the usual focus of active learning methods on classification tasks only. 3. Generative Capability: The papers exploration of using the model in a generative mode to create informative samples is innovative and may open new research avenues in active learning and generative modeling.  Weaknesses: 1. Complexity and Scalability: The method seems computationally intensive especially with the need to train a dual regression head in addition to the primary model. The paper lacks a thorough discussion on the algorithms scalability to large datasets and complex models. 2. Empirical Evaluation Details: While the results are promising the paper might benefit from a more detailed analysis of empirical results including failure cases or scenarios where ALLY underperforms compared to other methods particularly on simpler datasets like MNIST. 3. Discussion on Practicality: Theres limited discussion on the practical implications of the method especially in terms of realworld applications cost analysis of labeled vs. unlabeled data and the method\u2019s effectiveness under extremely limited label budgets.  Questions and Suggestions for Authors: 1. Empirical Evaluation: Can the authors provide more insights into the scenarios where ALLYs performance is not leading especially on simpler tasks such as MNIST It would help understand the methods limitations or potential areas for improvement. 2. Scalability Discussion: Given the computational complexities involved in training dual variables and the dual regression head how does ALLY scale with dataset size and model complexity Any strategies for mitigating computation costs could be beneficial. 3. Realworld Application: It would be interesting to see a discussion or a case study on applying ALLY in a realworld scenario focusing on the tradeoffs between labeling cost computational resources and model performance improvements.  Limitations and Societal Impact: The authors have discussed the methods limitations concerning the model capacity and dataset redundancy. However further discussion on computational scalability and the algorithms performance in highly imbalanced datasets or datasets with rare classes would be valuable. On societal impact while the paper doesnt directly address negative implications the potential biases introduced by active sampling and the generative models use should be considered especially in sensitive applications. A suggestion would be to include an analysis of bias and fairness when selecting samples to label ensuring the models utility across diverse and potentially imbalanced datasets. In summary ALLY is a promising approach to active learning introducing novel use of Lagrangian duality to select informative samples effectively. However further exploration on scalability detailed empirical analysis and discussions on practical implementations would enhance its contribution.", "xUK4E1jpV7z": "The paper delves into the domain of sample compressibility within learning theory focusing on unlabelled compression schemes for concept classes. It builds on existing conjectures and theories related to Probably Approximately Correct (PAC) learnability and the VapnikChervonenkis (VC) dimension aiming to establish conditions under which certain classes can be compressed efficiently. The manuscript achieves strides by simplifying and extending previously established proofs regarding unlabelled compression schemes for maximum classes and presenting a novel proposition that all intersectionclosed classes with VC dimension d can be compressed to at most 11d. Furthermore it introduces a criterion for the compressibility of socalled extremal classes positing a roadmap for the potential compressibility of broader class families.  Strengths 1. Contributions to Compression Schemes: The paper extends current understanding by simplifying existing proofs and proposing new schemes that broaden the applicability of unlabelled compression particularly for extremal and intersectionclosed classes. This extension is both significant and innovative offering a new lens through which to examine class compressibility. 2. Theoretical Depth: The authors exhibit a deep grasp of the theoretical underpinnings of sample compression in learning theory. The connection between compression schemes and PAC learnability through the VC dimension is well articulated and contributes to advancing foundational learning theory. 3. Methodological Rigor: The methodology utilized in the extension of proofs and the formulation of the new criterion for extremal classes demonstrates rigor and precision. The authors have meticulously structured their arguments providing clarity to complex concepts.  Weaknesses 1. Complexity and Accessibility: While the paper is theoretically rich its complexity may render it less accessible to readers who are not deeply versed in learning theory and compression schemes. Simplifying explanations or providing more introductory context could improve its accessibility. 2. Empirical Validation: The paper primarily focuses on theoretical developments without empirical validation. Including case studies or simulations showcasing the practical efficiency and applicability of the proposed compression schemes could strengthen the paper\u2019s impact. 3. Consideration of Previous Work: The discussion largely builds on specific prior works without a broad survey of related efforts in the domain of sample compression. A more extensive review might reveal additional perspectives or underexplored areas.  Questions and Suggestions for the Authors 1. Accessibility Improvement: Can the authors provide a simplified overview or a visual representation of the core concepts for a broader audience including those less familiar with the specific nuances of learning theory 2. Empirical Validation: Would the authors consider adding a section that applies the proposed compression schemes to realworld datasets or simulated environments to demonstrate their practical viability and efficiency 3. Comparison and Novelty: How do the proposed compression schemes for intersectionclosed classes compare to existing methods in terms of compressibility efficiency and computational overhead This comparison could highlight the novel contribution more distinctly.  Limitations and Societal Impact The authors have not clearly articulated the limitations or the potential negative societal impacts of their work. It would be beneficial for the paper to include a discussion on the limitations related to the feasibility of implementing the proposed compression schemes in practical applications considering computational resources and scalability. Moreover a reflection on how these compression schemes might impact data representation and bias especially in sensitive applications would be a valuable addition. Constructive suggestions for improvement could include:  Developing guidelines for the responsible application of compression schemes to prevent exacerbation of bias in dataset representations.  Proposing future research directions that could mitigate the identified limitations such as scalability and computational efficiency improvements. Acknowledging and addressing these aspects would enhance the paper\u2019s completeness and responsibility towards broader societal impacts.", "uAIQymz0Qp": "Review for \"DMAP: Distributed Morphological Attention Policy for Robust Locomotion in Morphologically Perturbed Environments\" 1) Summary: This paper introduces a novel architecture named Distributed Morphological Attention Policy (DMAP) aimed at addressing the challenge of locomotion in environments with morphological perturbations. The authors focus on exploring how agents can adaptively locomote in continuous control environments where significant changes to body part length and thickness occur. Building on biological principles such as independent lowlevel processing gated connectivity and distributed control DMAP incorporates an attention mechanism to dynamically gate sensory information enabling it to adapt to varying morphologies without direct knowledge of those changes. The architecture is evaluated across four classical continuous control environments adapted for morphological perturbations. Comparison with baseline agents demonstrates DMAPs superior performance matching or even surpassing an oracle agent with explicit morphological information in most scenarios. 2) Assessment of Strengths and Weaknesses: Strengths: a) Novel Architecture: DMAPs introduction of an attentionbased policy network that leverages principles from biological motor control is innovative. It effectively addresses the challenge of robust locomotion under morphological perturbations filling a gap in current research. b) Biologically Inspired Design: The application of biological principles such as independent processing distributed control and dynamic gating towards solving sensorimotor tasks in robotics and AI is compelling and theoretically wellgrounded. c) Comprehensive Evaluation: The paper rigorously evaluates DMAP across different environments and compared to several baseline agents showcasing its adaptability and robustness across a range of morphological variations. d) OpenSource Contribution: By making their code available the authors foster further research and reproducibility in the area of adaptive locomotion in perturbed environments. Weaknesses: a) Generalizability and Complexity of Environments: The study is limited to four specific continuous control environments. Expanding the variety of testing environments including those with more complex dynamics or irregular terrains could better demonstrate the generalizability and robustness of DMAP. b) Interpretability of the Attention Mechanism: While the attention mechanism is central to DMAPs performance the paper does not deeply analyze or interpret how this mechanism prioritizes sensory information and its correlation with the physical principles governing locomotion. c) Potential for RealWorld Application: The paper focuses on simulated environments. Discussion on the applicability of DMAP in realworld robotic systems or the challenges therein would add valuable insight. 3) Questions and Suggestions: a) Interpretability and Analysis of Attention Dynamics: Can the authors provide deeper analysis or qualitative examples of how the attention mechanism dynamically prioritizes different sensory inputs particularly in the most challenging morphological perturbations b) Application to More Complex or Realistic Environments: How would DMAP perform in environments with significantly more complexity e.g. those with unpredictable terrains or in realworld robotic applications Are there anticipated challenges or modifications required for such applications c) Comparison with Other Biologically Inspired Approaches: The paper mentions related works leveraging biological principles. Could a comparison with these or discussion on how DMAP integrates or diverges from these approaches provide more context on the innovation and utility of DMAP 4) Limitations and Societal Impact: The authors have made a commendable effort in addressing the limitations of their work particularly regarding the complexity of environments and the interpretability of the attention mechanism. Further discussion on the realworld applicability and scalability of DMAP would be beneficial. On societal impact the paper briefly touches upon the potential ethical concerns of automation and autonomous agents. An expanded discussion on how DMAP or similar technologies might specifically impact sectors such as assistive robotics automation and employment both positively and potentially negatively would provide a more rounded perspective on its broader implications. Suggestions for mitigating negative impacts such as implementing ethical guidelines for deployment or emphasizing humanintheloop systems could be valuable additions. Overall this paper makes significant contributions to the field of adaptive locomotion in robotics presenting a novel biologically inspired approach that has the potential to influence future research and applications in this area and beyond.", "x-i37an3uym": "Review for \"ModuleAware Optimization for Auxiliary Learning (MAOAL)\" 1) Summary: The paper presents a novel approach for auxiliary learning termed ModuleAware Optimization for Auxiliary Learning (MAOAL) which innovatively addresses the issue of modulelevel influence in auxiliary learning. Unlike conventional methods that balance losses modelwide MAOAL optimizes auxiliary loss importance at a module level acknowledging that certain auxiliary losses may benefit or harm specific model modules differently. The methodology involves a bilevel optimization strategy where in the lower optimization model parameters are updated with importanceparameterized gradients and in the upper optimization modulelevel auxiliary importance is refined using implicit gradients from a development dataset. The authors claim this is the first work to investigate the modulelevel influence in auxiliary learning and provide extensive experimental evidence showing MAOALs superiority over existing stateoftheart approaches across various datasets and auxiliary loss scenarios. 2) Assessment: Strengths:  The paper addresses a critical and previously underexplored aspect of auxiliary learning \u2014 the differing influence of auxiliary losses on model modules.  It proposes a unique and theoretically sound method MAOAL which leverages modulelevel optimization to enhance auxiliary learning efficacy.  The extensive experiments conducted demonstrate clear advantages over existing methods showcasing the approachs broad applicability and potential to serve as a generic tool for auxiliary learning.  The detailed explanation of the methodology including the bilevel optimization process and the implicit gradient calculation for updating modulelevel importance provides a clear understanding of the approach. Weaknesses:  The computational cost associated with the proposed method may limit its applicability in scenarios where computational resources are constrained. The paper acknowledges this but does not offer solutions or workarounds.  While the experiments cover a range of datasets and auxiliary loss types the evaluation primarily focuses on performance metrics (e.g. accuracy RMSE). There is a lack of indepth analysis on how different modulelevel importances critically influence the primary tasks outcomes or model interpretability.  The paper does not discuss the potential challenges of applying MAOAL to extremely deep or complex models where module interactions and dependencies could complicate the optimization process. 3) Questions and Suggestions for the Authors:  Have the authors considered alternative strategies for reducing the computational burden For instance could a more selective or sparse approach to importance weighting mitigate this issue  Could the authors provide further insights or case studies on how the modulelevel importances learned correlate with model behavior or interpretability This could enhance the understanding of why certain modules benefit more from specific auxiliary tasks.  It would be beneficial to explore the application of MAOAL to more complex or unconventional model architectures (beyond ResNet Bert and MLPs) to test its robustness and adaptability. 4) Limitations and Societal Impact: The authors briefly mention the computational cost as a limitation but do not delve into potential solutions. Moreover the discussion on societal impact is lacking. While the authors claim no negative societal impact any optimization technique that significantly enhances model performance can inadvertently amplify biases present in the data or model design. It would be constructive to suggest guidelines for ethical use and monitoring of MAOAL in applications where biases or ethical concerns are prevalent. Constructive Suggestions for Improvement:  Expanding the discussion on computational efficiency and exploring strategies to alleviate the additional computational burden would make the method more accessible.  Incorporating an analysis of the impact of modulelevel importance on model interpretability and behavior might provide deeper insights into the model optimization process.  Addressing the application of MAOAL to more complex and varied model architectures could help establish the methods versatility and robustness.", "q16HXpXtjJn": "Review of \"Unified Algorithms for Distribution Functional Estimation in Online and Offline Settings\" 1) Summary of the Paper This paper addresses the infinitearmed bandit problem with a novel perspective on estimating distributional functionals beyond just the maximum average reward. The authors propose metaalgorithms that operate in both online and offline modes to estimate various distribution functionals including the mean median maximum and trimmed mean. They successfully demonstrate that online sampling can significantly reduce sample complexity for certain functionals relative to offline methods. The work is innovative in its use of different Wasserstein distances to derive matching upper and lower bounds for the sample complexity required to estimate these functionals. The problem formulation is based on the setting where the distribution of each arms reward is unknown and the rewards can be sampled to obtain noisy estimates. 2) Assessment of Strengths and Weaknesses Strengths: a. Innovative Approach: The paper presents a unique angle on the infinitearmed bandit problem extending the scope beyond finding the arm with the maximum average reward to estimating various distribution functionals. This broadens the application of bandit problems to more complex statistical tasks. b. Comprehensive Analysis: The work provides a thorough analysis including both upper and lower bounds for sample complexity demonstrating the efficiency and limitations of the proposed algorithms. c. Practical Implications: Given its discussion on both online and offline settings the paper has implications for a wide range of applications including those where continuous interaction with the environment is not feasible. d. Theoretical Contributions: The identification of a thresholding phenomenon in the noise level for distinguishing distributions after Gaussian convolution is a novel theoretical insight that could fuel further research. Weaknesses: a. Limited Discussion on NonIndicatorBased Functionals: While the paper beautifully handles a class of indicatorbased functionals it briefly mentions the potential extension to nonindicatorbased functionals without providing detailed analysis or examples. b. Assumption on Noise Level: The analysis heavily relies on Gaussian noise assumptions. Realworld applications might have different noise characteristics making it unclear how the proposed methods would adapt or perform under those conditions. c. Complexity of Implementation: The proposed metaalgorithms while theoretically sound seem to require significant computational and statistical sophistication for practical implementation. The paper lacks a discussion on the practicality of deploying these algorithms in realworld scenarios. 3) Questions and Suggestions for the Authors a. Extension to NonGaussian Noises: Can the authors discuss how the proposed metaalgorithms might be adapted or how their performance might be impacted under nonGaussian noise distributions b. Practical Implementation Details: It would be beneficial for the readers if the authors could include a section discussing the practical implementation aspect of these algorithms possibly accompanied by pseudocode and a discussion on computational complexities. c. Case Studies or Applications: Are there specific case studies or applications where the authors have tested these algorithms Insights from such evaluations would greatly enhance the papers appeal to a broader audience. 4) Evaluation of Limitations and Societal Impact The authors briefly mention the possibility of extending their analysis to more complex functionals and the potential high cost (in terms of sample complexity) in some scenarios. However there is insufficient discussion on the practical limitations of implementing these algorithms in realtime systems or environments with nonideal noise characteristics. Regarding societal impact the paper does not touch upon any potential negative implications. Given the broad applicability of these algorithms across different domains (e.g. recommendation systems clinical trials) a section dedicated to discussing ethical considerations biases and fairness in the context of distribution functional estimation would have been valuable. Suggestions for Improvement:  Expand the discussion on the practical challenges and computational requirements for implementing the proposed algorithms.  Include a detailed analysis or a roadmap for extending the current work to nonindicatorbased functionals and nonGaussian noise environments.  Discuss potential societal impacts especially negative ones that might arise from the application of these algorithms in sensitive areas such as healthcare finance and public policy. This work is undoubtedly a significant step forward in the statistical analysis of infinitearmed bandit problems providing new insights and methods for distribution functional estimation. However addressing the mentioned weaknesses and considering the suggestions could substantially increase its contribution and applicability to realworld problems.", "qm5LpHyyOUO": "Review of \"MCMAE: Enhancing Vision Transformers with Masked Convolution for Multiscale AutoEncoding\" 1. Summary: The paper introduces a novel selfsupervised learning framework MCMAE (Masked Convolution Mask AutoEncoding) aimed at boosting the performance of Vision Transformers (ViT) by leveraging a hybrid convolutiontransformer architecture for pretraining visual representations. Notably it combines multiscale feature extraction with a masked autoencoding scheme similar to MAE but introduces masked convolutions to efficiently handle the original masking strategys computational costs and pretrainingfinetuning discrepancy. The MCMAE framework significantly improves the finetuning accuracy on ImageNet1K and demonstrates remarkable performance on downstream tasks such as object detection and semantic segmentation surpassing previous methods with fewer pretraining epochs. The paper also extends the proposed framework to video understanding showing its versatility and effectiveness across different mediums. 2. Assessment: Strengths:  Originality: The introduction of a blockwise masking strategy coupled with masked convolutions is a novel adjustment to the masked autoencoding paradigm effectively addressing its main drawbacks.  Efficiency and Performance: MCMAE shows outstanding efficiency and performance improvements on various tasks. Notably the ability to maintain computational efficiency while enhancing the discriminative capacity of the extracted features is impressive.  Extensibility: The frameworks extension to video understanding (VideoMCMAE) and demonstration of effectiveness without relying on pretraining with image datasets indicate its significant potential for broader applications. Weaknesses:  Limited Exploration of Masked Convolution Variants: While the paper effectively demonstrates the benefits of masked convolutions it does not explore or discuss the potential of different masked convolution strategies or their impact on the models effectiveness.  PretrainingFinetuning Discrepancy Analysis: The paper claims to address the pretrainingfinetuning discrepancy problem but offers limited detailed analysis on how the MCMAEs design specifically impacts this issue across different tasks and datasets.  Discussion on Societal Impacts: While the authors briefly mention the lack of negative societal impacts a more thorough examination of potential unintended consequences particularly in the context of video understanding applications would have been insightful. 3. Questions and Suggestions for the Authors:  Further Elaboration on Masked Convolution Variants: Can the authors discuss or provide insights into how different masked convolution strategies might affect the models learning efficiency and performance  Detailed Analysis on PretrainingFinetuning Discrepancy: Could the authors provide a deeper analysis or additional empirical evidence on how the MCMAE framework specifically mitigates the pretrainingfinetuning discrepancy across a broader array of tasks and conditions  Exploration of Societal Impacts: Given the frameworks extension to video understanding could the authors elaborate on potential misuse scenarios such as privacy invasion or unauthorized surveillance and suggest mitigating strategies 4. Limitations and Constructive Suggestions for Improvement: While the paper successfully addresses several limitations of the masked autoencoding scheme and demonstrates the frameworks efficiency and versatility it would benefit from a more detailed exploration of different masked convolution strategies and their potential implications on the models performance. Furthermore a deeper analysis of the pretrainingfinetuning discrepancy issue including additional empirical evidence could strengthen the papers findings. Lastly considering the frameworks applicability to video understanding a more comprehensive discussion on potential negative societal impacts and mitigation strategies would enhance the papers consideration of ethical aspects.", "xxgp42Qz6dL": " Review of \"EnergyGuided Stochastic Differential Equations for Realistic and Faithful Unpaired ImagetoImage Translation\"  1. Summary of the Paper This paper introduces a novel method named EnergyGuided Stochastic Differential Equations (EGSDE) for unpaired ImagetoImage Translation (I2I). The key innovation of EGSDE is the utilization of an energy function trained across both source and target domains to guide the inference process of a pretrained Stochastic Differential Equation (SDE). This approach aims to preserve domainindependent features while discarding domainspecific ones thus enhancing both the realism and faithfulness of image translation. The method is validated against stateoftheart (SOTA) baselines on three I2I tasks exhibiting superior performance in terms of Frechet Inception Distance (FID) without compromising faithfulness. The paper also explores the theoretical underpinnings by framing EGSDE within the context of product of experts further elucidating its functioning mechanism.  2. Assessment of Strengths and Weaknesses Strengths:  The paper addresses a critical limitation in current SBDMbased methods by efficiently leveraging training data from both source and target domains which is a significant advancement in the I2I translation domain.  Methodologically the integration of an energy function with SDE is innovative and wellmotivated. The dual focus on preserving domainindependent features while eliminating domainspecific ones is compelling and theoretically substantiated.  Empirical results demonstrate marked improvements over existing SBDM and GANbased methods across several metrics reinforcing the efficacy of EGSDE. The provision of comprehensive evaluation including qualitative assessments and ablation studies provides a robust validation of the proposed approach.  The opensource code and thorough documentation contribute positively to the reproducibility of the results and facilitate further research exploration. Weaknesses:  While the paper mentions the potential negative societal impact related to generating fake images it does not thoroughly explore or substantially address these concerns. This omission is significant given the increasing ethical considerations in the application of generative models.  The reliance on a simple lowpass filter as the domainindependent feature extractor might limit the methods capacity to capture more nuanced or complex features. Although acknowledged as a limitation the exploration of more sophisticated alternatives could enhance the methods applicability and effectiveness.  3. Questions and Suggestions for the Authors  Could the authors elaborate on potential strategies or safeguards that could be implemented to mitigate the negative societal impacts of their method especially concerning the creation and dissemination of misleading or harmful imagery  Given the mention of potentially employing more sophisticated methods for the domainindependent feature extractor are there preliminary results or insights on how such approaches might compare to the current implementation using a lowpass filter  The paper provides a compelling explanation of EGSDE through the lens of the product of experts model. Are there other theoretical frameworks or models that could similarly elucidate or supplement the understanding of EGSDEs effectiveness  4. Limitations and Societal Impact The paper briefly mentions the employment of a simplistic approach for the domainindependent feature extractor and acknowledges this as a limitation. However the exploration of more sophisticated alternatives and their potential impact on improving the translation quality could be further detailed. On the topic of negative societal impact while the potential for misuse in generating deceptive imagery is acknowledged a more thorough discussion on ethical considerations potential misuse and mitigation measures is warranted. Constructive suggestions include proposing guidelines for responsible use exploring watermarking techniques to distinguish AIgenerated images and advocating for policy and regulatory discussions aligned with the advancement of these technologies.", "qC2BwvfaNdd": "Review of \"DataIQ: A DataCentric Framework for Identifying Subgroups in Tabular Data to Improve Model Performance and Robustness\" 1. Summary of the Paper This paper introduces DataIQ a novel framework designed for the tabular data format commonly found in critical fields like healthcare and finance. Recognizing the challenge presented by outcome heterogeneity  where individuals with similar features may have disparate outcomes  DataIQ stratifies data into subgroups (Easy Ambiguous Hard) based on predictive confidence and aleatoric uncertainty during model training. This systematic stratification allows for robust characterization of data aiding in feature acquisition dataset selection and reliable model deployment. The authors empirically validate DataIQ\u2019s utility across four medical datasets demonstrating its robustness across various models and its practical usefulness in enhancing model performance and generalization. 2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The framework\u2019s focus on aleatoric uncertainty for subgroup stratification is a significant deviation from traditional methods. This approach yields a more principled and accurate understanding of data subsets particularly in tabular settings.  Versatility: DataIQ\u2019s compatibility with a wide range of machine learning models including but not limited to neural networks and gradient boosting enhances its practicality and applicability in diverse tabular data environments.  Empirical Validation: The extensive testing across multiple realworld medical datasets provides strong empirical evidence of the frameworks effectiveness in enhancing model robustness and performance.  Comprehensive Contribution: The paper not only proposes a novel framework but also rigorously shows its application in guiding feature acquisition dataset selection and reliable model deployment making significant contributions to datacentric AI research. Weaknesses:  Limited Focus on NonTabular Data: While the paper briefly discusses the extension of DataIQ to text and image data in the appendices the main focus is on tabular data. This limits understanding of the framework\u2019s effectiveness in more complex data types.  Model Dependency: While DataIQ shows robustness across different models the degree of its effectiveness depending on model choice and parameterization is not deeply explored. More insights into this aspect could help practitioners make informed decisions.  Complexity and Implementation Challenges: The paper provides a detailed theoretical foundation and empirical validation but it may not adequately address the practical challenges and complexity involved in implementing the framework especially for practitioners without a deep technical background. 3. Questions and Suggestions for the Authors  Could the authors provide more insights or case studies on the application of DataIQ in nontabular datasets especially to understand its limitations and strengths in those settings  It would be beneficial if the paper could discuss the impact of model choice more deeply. How does DataIQ\u2019s performance vary with different model architectures and are there models with which it integrates less effectively  Implementing such a framework can be technically challenging. Could the authors provide additional guidelines perhaps in a supplementary document on practical implementation especially focusing on potential pitfalls and how to avoid them  Considering the importance of data privacy especially in healthcare are there privacypreserving aspects of DataIQ that could be highlighted or explored in future work 4. Limitations and Societal Impact The paper briefly touches upon the implications of DataIQs design with respect to its applicability and robustness. However a more thorough discussion on the potential negative societal impacts particularly in sensitive areas like healthcare would strengthen the manuscript. Addressing potential biases that may arise from model and data selection processes is crucial. Additionally guidelines on how to interpret and act on the subgroup classifications responsibly especially when it comes to the potential for misclassification or overreliance on automated classifications in highstakes decisions could ensure the responsible deployment of DataIQ. Suggestions for Improvement:  A section dedicated to discussing the ethical considerations potential biases and mitigation strategies would be valuable. Including case studies or hypothetical scenarios where misinterpretations of the subgroup classifications could lead to negative outcomes might help highlight the importance of cautious application.  An exploration of DataIQs usability and integration within existing data pipelines in a privacypreserving manner would also be beneficial providing a pathway for responsible adoption in sensitive industries.", "oDWyVsHBzNT": " Peer Review for \"Modelbased Offline Reinforcement Learning with PessimismModulated Dynamics Belief\"  1. Summary This paper introduces a novel approach to modelbased offline reinforcement learning (RL) that aims to reliably optimize policy using a previously collected static dataset without further interaction with the environment. Traditional methods either constrain policies too closely to the behavior policy or penalize the Qvalue for outofdistribution (OOD) stateaction pairs often resulting in overly conservative policies. The authors propose to address these issues by maintaining a belief distribution over system dynamics and optimizing policy via biased sampling from this belief termed PessimismModulated Dynamics Belief (PMDB). This method derives from an alternating Markov game (AMG) formulation and theoretically supports the premise that biased sampling naturally updates the dynamics belief with a policydependent reweighting factor. The paper provides empirical results that display superior performance on a range of benchmark tasks compared to stateoftheart methods indicating the approachs effectiveness.  2. Strengths and Weaknesses Strengths:  Novelty: The approach of using a belief distribution for dynamics and optimizing policy through biased sampling is innovative. It moves away from explicit dynamics uncertainty quantification commonly used in similar works offering a potentially more flexible and efficient way to handle dynamics uncertainty.  Theoretical Foundation: The paper thoroughly establishes the theoretical underpinnings of the proposed method specifically how the AMG formulation leads to an updated belief distribution that is modulated by pessimism.  Empirical Performance: Demonstrated improvement over stateoftheart methods in various benchmark tasks supports the practical relevance and applicability of the proposed approach. Weaknesses:  Complexity and Practicality: The methods complexity especially in understanding and implementing the proposed alternating Markov game formulation and the iterative regularized policy optimization process could hinder its accessibility and practical deployment.  Lack of Discussion on Belief Choice: While the paper emphasizes the importance of the initial dynamics belief it does not adequately discuss how to choose or construct such beliefs for different tasks which is crucial for the methods success.  Scalability Concerns: The experiments are conducted on benchmarks with relatively simple environments. The scalability of the approach to more complex realworld scenarios is not addressed.  3. Questions and Suggestions  Question: How sensitive is the method to the choice of the initial dynamics belief and how can one construct or select an appropriate belief for complex tasks where expert knowledge is scant  Suggestion: It would be beneficial to include a more detailed discussion or guide on selecting or constructing the dynamics belief for various tasks. Additionally exploring datadriven methods to derive these beliefs could increase the approachs applicability.  Suggestion: Investigating and reporting the performance of the proposed method on more complex realworld tasks would provide clearer insights into its scalability and limitations.  Suggestion: A comparison between the proposed method and traditional modelfree approaches in terms of computational complexity and resource requirements might elucidate its practical deployment challenges.  4. Limitations and Potential Negative Societal Impact The paper does not thoroughly discuss the scalability of the proposed method to complex realworld problems which is a significant limitation. Moreover the potential negative societal impact of deploying policies optimized in an offline setting especially in critical applications like healthcare or autonomous driving is not addressed. Suggestions for Improvement:  Expand on Scalability: Elaborating on the scalability challenges and potential strategies to cope with complex scenarios would strengthen the paper.  Societal Impact Discussion: Including a discussion on the ethical considerations and potential negative impacts of misapplications or failures of the proposed method in sensitive areas would be valuable for a holistic understanding of the approachs implications. In summary while the paper presents a novel and theoretically sound approach to modelbased offline RL with promising empirical results addressing its practical deployment challenges scalability and possible negative societal impacts would enhance its contribution to the field.", "xvLWypz8p8": "Review of \"MarginBased Generalisation Bounds for Majority Votes\" 1) Summary: This paper provides a theoretical advancement in the understanding of the generalization properties of majority voting on finite ensembles of classifiers. By combining margin bounds with the PACBayes theory and leveraging Dirichlet posteriors the authors introduce new margin bounds for nonrandomised majority votes that are applicable to multiclass classification tasks. Unlike previous works that primarily focused on stochastic majority votes these new bounds provide stateoftheart guarantees for a number of classification tasks. Empirically the bounds compare favorably to existing ones and demonstrate remarkable sharpness adding significant depth to the discourse on \"margins theory\" and its implications for ensemble classifiers generalization. 2) Assessment: Strengths:  The paper addresses a crucial aspect of machine learning theory by improving our understanding of why and how ensemble methods generalize well in practice. Specifically it extends existing work on margin bounds within the PACBayes framework offering a novel perspective by employing Dirichlet posteriors.  The empirical evaluation convincingly demonstrates the superiority of the proposed bounds over existing margin and PACBayesian bounds across several datasets validating the theoretical contributions.  The methodology is rigorous and the proofs provided in the appendices are detailed enhancing the reliability of the results.  The work has practical relevance as it offers a foundation for developing new learning algorithms optimized with these tighter bounds offering potential improvements in the generalization of votingbased classifiers. Weaknesses:  The high level of technical detail and assumption of prior knowledge might make the paper less accessible to readers less familiar with PACBayes theory or Dirichlet distributions. A more detailed introduction or explanatory section for such audiences could enhance the papers accessibility.  While the empirical results are strong the paper could benefit from a broader discussion on the implications of these results for practical applications in machine learning beyond the theoretical advancement.  The authors could explore and discuss the potential limitations or challenges in applying these bound improvements to other ensemble methods not covered in this paper such as those involving more complex voter dependencies or configurations. 3) Questions and Suggestions for the Authors:  Could the authors elaborate on how these new bounds might influence the design of new ensemble learning algorithms Specifically are there implications for algorithms beyond majority voting schemes such as weighted voting or boosting methods  The paper primarily focuses on binary and multiclass classification tasks. How do the authors anticipate their bounds might extend to or be adapted for regression tasks or other types of machine learning problems  A more extensive discussion on potential future research directions inspired by these results would be valuable. For example exploring bounds for ensembles with dynamically changing voters or ensembles trained in an online learning context. 4) Limitations and Societal Impact: While the authors thoroughly address the theoretical limitations and the scope of their work there is little discussion on the potential negative societal impacts. Given the increasing scrutiny on algorithmic bias and fairness in machine learning it would be beneficial for the authors to consider whether these new generalization bounds could inadvertently exacerbate or mitigate biases present in the training data when applied to realworld tasks. Suggestions for future work could include extending this theoretical framework to encompass considerations of fairness and bias ensuring that advancements in ensemble methods contribute positively to societal outcomes.", "prQkA_NjuuB": "Review of \"DivergenceFree Neural Networks for Modeling Continuity and Conservation Laws\" 1) Summary This paper introduces a novel approach for parameterizing deep neural networks (DNNs) to inherently satisfy the continuity equation through modeling as divergencefree vector fields. Recognizing the challenge of incorporating exact physical constraints in neural network architectures the authors propose a methodology that utilizes differential forms and automatic differentiation to construct neural networks that ensure divergencefree behavior. Two specific parameterization strategies are developed: a matrixfield approach that benefits from computational efficiency and a vectorfield approach that offers flexibility for adding further constraints. Through theoretical proofs the paper demonstrates the universality of the proposed models i.e. their capability to approximate any divergencefree vector field. Empirically the paper validates the approach by applying it to fluid dynamics simulations Hodge decomposition and dynamic optimal transport problems showcasing the methods advantage in precisely satisfying the continuity equation without the need for additional penalty terms or numerical simulations. 2) Assessment of Strengths and Weaknesses Strengths:  The paper tackles a fundamental problem in modeling physical systems with neural networks by embedding the continuity equation directly into the architecture addressing accuracy and computational efficiency issues present in current methods.  The theoretical foundation is robust with a welljustified argument on the universality of the proposed models supported by theorems and proofs. This strengthens the claim of the models ability to represent any divergencefree vector field.  Experimentally the paper demonstrates the practical applicability of the method in various scenarios such as fluid dynamics and optimal transport providing evidence of its potential to improve the accuracy and efficiency of simulations in physicsinformed neural networks. Weaknesses:  The paper is computationally intensive particularly the use of automatic differentiation for calculating divergences which could limit its application in very highdimensional systems or realtime scenarios.  While the universality of the models is a theoretical strength the paper lacks a comprehensive comparison with existing approaches especially in terms of scalability and performance in practical applications beyond the presented experiments.  The implications of imposing additional constraints such as nonnegativity in density modeling are not thoroughly explored specifically regarding their impact on model expressiveness and training dynamics. 3) Questions and Suggestions  Could the authors provide more insights into the computational complexity of their approach particularly compared to traditional methods that use penalty terms or numerical simulation Additionally suggestions for optimizing computational efficiency would be valuable.  In the experiments section a detailed comparison with stateoftheart methods including those employing soft constraints for continuity and conservation laws would help in understanding the relative performance of the proposed approach.  The paper mentions the potential of the vectorfield construction to incorporate further constraints. It would be beneficial to see a more detailed exploration of this aspect possibly with examples of applications where this flexibility leads to significant improvements.  A discussion on the implications of choosing between the matrixfield and vectorfield approaches in different scenarios would be useful for practitioners. Are there guiding principles for selecting one over the other based on the specific needs of the application 4) Limitations and Suggestions for Improvement The authors have made a commendable effort in addressing the limitations of their work particularly noting the computational challenges. However they could further elaborate on potential negative societal impacts especially considering applications in domains with significant ethical implications such as autonomous systems or environmental modeling. In terms of constructive suggestions it would be valuable to explore how emerging techniques in efficient computation of neural network divergences could be applied to mitigate computational bottlenecks. Additionally developing heuristic guidelines for model choice and parameter tuning in applicationspecific contexts could significantly enhance the utility of the proposed methods. Overall this paper presents a promising direction for integrating physical laws into deep learning architectures with strong theoretical underpinnings and evident potential for impact across various domains of science and engineering.", "uV_VYGB3FCi": "Peer Review of \"Code Editing: A New Paradigm for Flexible Neural Image Compression\" 1) Briefly summarize the paper and its contributions This paper introduces \"Code Editing\" a novel approach for enhancing the flexibility of neural image compression (NIC) methods enabling a single decoder to manage continuous ratedistortion (RD) tradeoffs region of interest (ROI)based coding and multidistortion tradeoffs. The authors employ semiamortized inference and adaptive quantization to achieve variable bitrate control and improve upon the limitations of current conditional coding methods which offer minimal flexibility. Notably the approach is compatible with all differentiable decoder NIC methods and can be applied to pretrained models. The experimental results demonstrate that Code Editing outperforms existing variable bitrate NIC methods providing a highly flexible solution for image compression. 2) Assessment of the strengths and weaknesses of the paper Strengths:  Innovation: The paper presents a novel technique combining semiamortized inference and adaptive quantization that significantly advances the flexibility of NIC.  Broad Compatibility and Practicality: The approach is applicable to preexisting models without the need for retraining which greatly enhances its practical deployment.  Experimental Validation: The comprehensive experiments validate the superiority of Code Editing over existing methods across continuous RD tradeoffs ROIbased coding and multidistortion tradeoffs. Weaknesses:  Complexity and Computational Demand: The methods reliance on iterative optimization for each data point might introduce significant computational overhead especially in realtime scenarios.  Limited Discussion on Encoder Design: While the focus on the decoders flexibility is thorough there is less emphasis on how the encoder can be optimized or modified to complement the proposed method.  Empirical Studies Scope: The paper notably acknowledges the lack of a comprehensive empirical study on a more compounded target (e.g. Eq. 10) indicating potential gaps in the evaluation of the methods full capabilities. 3) Questions and Suggestions for the Authors  Encoding Time Optimization: Could the authors propose techniques to reduce the computational overhead of the Code Editing process especially for realtime applications  Encoder Optimization: How might the encoder be further optimized or designed to support the proposed Code Editing approach more effectively  Extending Empirical Studies: It would be beneficial to extend the empirical studies to evaluate the methods performance on the compounded target mentioned in Eq. 10. What are the authors plans for these additional studies 4) Limitations and Societal Impact The authors have adequately discussed the limitations related to encoding time efficiency and the potential implications for realtime applications. Suggestions for improvement include exploring optimization techniques to minimize the computational demands of Code Editing and considering the adoption of vector \\xe2\\x88\\x86 for extending bitrate flexibility in ROIbased coding. Regarding potential negative societal impacts while the paper does not directly address these it is essential to consider the broader implications of more efficient image compression such as increased data collection and surveillance capabilities. Future work could benefit from a more extensive discussion on the ethical considerations and potential safeguards to mitigate adverse societal impacts.", "yjybfsIUdNu": "The paper under consideration introduces a novel Reinforcement Learning (RL) approach coined as Multifidelity Monte Carlo RL (MFMCRL) which utilizes multifidelity data\u2014information of different levels of accuracy and computational cost\u2014to enhance RL agents learning efficiency. By leveraging a unique estimator that accounts for crosscorrelations between data of varying fidelity levels the authors aim to reduce variance in the estimation of the stateaction value function thereby improving policy evaluation and policy improvement outcomes within a constrained highfidelity data budget. Strengths: 1. Theoretical Foundation: The paper provides a solid theoretical analysis of the proposed MFMCRL algorithm including proofs and concentration inequalities that underscore the algorithms potential for improved learning efficiency. 2. Innovative Approach: Introducing a multifidelity aspect into RL offers a promising research direction. It creatively addresses RLs common challenge of expensive or sparse highfidelity data by systematically exploiting cheaper lowfidelity data. 3. Practicality and Generalizability: The approach is practically oriented potentially beneficial in realworld scenarios where data of varying fidelity is common. The synthetic and realworld experiments demonstrate the algorithms applicability across different domains. 4. Empirical Validation: The authors undertake comprehensive experiments including a neural architecture search (NAS) use case demonstrating MFMCRLs superior performance over traditional RL methods particularly under a limited highfidelity data budget. Weaknesses: 1. Complexity and Computational Demand: While the methodology is promising the actual computational demand especially in terms of the required volume of lowfidelity data to achieve significant variance reduction is not thoroughly evaluated. 2. Assumption Rigidity: The performance critically depends on the correlation between low and highfidelity data. The exploration of how misestimations of correlations could impact the outcomes is limited. 3. Scalability and Generalization to Continuous Environments: Most demonstrations are in synthetic or controlled environments. The scalability to larger more complex or continuous action and state spaces remains in question. 4. Comparison Depth: The comparison with existing stateoftheart RL methods leveraging multifidelity concepts or data efficiency is somewhat shallow which impedes a robust understanding of the methodologys relative advantage in a broader context. Questions and Suggestions: 1. On Computational Efficiency: Could the authors provide an analysis comparing the computational overhead of using MFMCRL against potential gains in learning efficiency 2. On Assumption Relaxation: It would be beneficial to explore the robustness of MFMCRL under less stringent correlation assumptions or in the presence of misestimated correlations between fidelity levels. 3. Extension to Continuous Spaces: Future work should consider extending the investigation to more complex and continuous environments to underline MFMCRLs adaptability and scalability. 4. Detailed Comparative Analysis: A more comprehensive comparison with both multifidelity and dataefficient RL methods could provide deeper insight into MFMCRLs unique benefits and potential drawbacks. Limitations and Societal Impact: The authors acknowledge potential positive impacts such as the creation of more resourceefficient RL agents. However the paper falls short in thoroughly addressing the limitations associated with the scalability and adaptability of the proposed method particularly in realistic uncontrolled environments. Moreover a deeper discussion on negative impacts is lacking\u2014especially in cases where the correlation assumptions fail significantly which can lead to inefficiencies or inaccuracies in learning. Constructively further research should aim to mitigate these limitations by exploring adaptive mechanisms for correlation estimation and extending the methodologys applicability to diverse and complex scenarios. In conclusion the paper introduces a promising direction for exploiting multifidelity data in Reinforcement Learning. With improvements in computational efficiency analysis robustness to assumption violations scalability and a broader comparative framework the methodology has the potential to significantly contribute to the field of efficient RL.", "nJWcpq2fco3": "Review for the paper \"A Representation Learning Framework for Spatial Trajectories\" 1) Summary: This paper introduces a novel representation learning framework specifically designed for spatial trajectories. The main innovation lies in representing partial observations of trajectories as probability distributions within a learned latent space. This approach elegantly handles uncertainties about unobserved portions of the trajectory. By transforming these uncertainties into distributive representations the framework can interpolate and extrapolate trajectory samples at any continuous point in time. Additionally the proposed method allows for direct manipulation of trajectory attributes (e.g. pace) and supports the integration of diverse partial observations into unified representations. The versatility and effectiveness of this framework are demonstrated through superior performance in prediction tasks against established baselines using human movement datasets for empirical validation. 2) Assessment: Strengths:  Innovative Approach: The methods ability to deal with uncertain parts of a trajectory by representing them as probability distributions in latent space is both novel and theoretically sound. This represents a significant advance over traditional representation methods that might struggle with uncertainty and continuity in spatial and temporal domains.  Flexibility and Applicability: The frameworks design allows for extensive flexibility including varying input lengths irregular sampling rates and the capacity for both interpolation and extrapolation. This makes it widely applicable across various scenarios where trajectory data is central.  Empirical Validation: The comprehensive experiments conducted using human movement datasets not only serve as a robust validation of the frameworks capabilities but also set a benchmark for future work in the domain especially given the frameworks superior performance over baselines. Weaknesses:  Complexity and Scalability: While the approach is innovative the complexity of the method \u2014 particularly the handling of probability distributions in latent space and the operations performed on them (e.g. intersection computations) \u2014 could pose challenges in terms of both computational efficiency and scalability to very large datasets or realtime applications.  Generalization Across Domains: The experiments are focused on human movement datasets. While impressive it would be beneficial to see the frameworks performance across a broader range of trajectory types and domains to fully understand its generalization capabilities.  Interpretability and Manipulation: Given the abstract nature of the latent space and the representations within it interpretability and the intuitive manipulation of trajectories (other than demonstrated modifications like pace adjustment) could be challenging for users not intimately familiar with the models inner workings. 3) Questions and Suggestions:  Could the authors elaborate on the computational efficiency of their framework especially in scenarios requiring realtime prediction or handling exceptionally large datasets  It would be insightful if the authors could provide examples or case studies of the framework being applied to trajectory types beyond human movement such as vehicular traffic or animal migration patterns to demonstrate its versatility.  A deeper exploration into the interpretability of the learned latent space could add value perhaps by visualizations or mappings that make understanding and manipulating trajectory attributes more intuitive for users. 4) Limitations and Societal Impact: The paper briefly touches upon the frameworks versatility and performance superiority but does not extensively discuss its computational demands or limitations in scalability. Addressing these concerns with optimization strategies or insights into computational resource requirements would be beneficial. Additionally given the potential for this technology to influence areas such as surveillance or behavioral prediction a discussion on ethical considerations and the potential for biased outcomes or privacy infringements would make the paper more comprehensive. Constructive suggestions include incorporating ethical guidelines for use and exploring bias mitigation techniques in trajectory prediction to ensure responsible and equitable use of the technology.", "s0AgNH86p8": "Review of \"TransBoost: Enhancing Deep Neural Networks Performance in Transductive Settings\" 1. Summary of the Paper This paper introduces TransBoost a method to enhance the accuracy of deep neural neural networks in transductive learning settings. It does so by finetuning pretrained models on a provided unlabeled test set using a novel large margininspired optimization procedure. The critical insight is to leverage the test set for finetuning which differs significantly from inductive settings where the goal is to generalize to unseen data. The authors demonstrate TransBoosts efficacy across a broad range of architectures and datasets notably achieving stateoftheart transductive performance on ImageNet and showing effectiveness on various image classification benchmarks. The method is simple efficient and can be applied to any deep learning model making it a versatile tool for improving performance in scenarios where unlabeled test data are available during training. 2. Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of a simple and efficient transductive finetuning procedure that can be universally applied across different architectures and datasets is highly novel.  Empirical Validation: The comprehensive set of experiments across various architectures (e.g. ResNets MobileNetV3L EfficientNetB0 and ViTS) and datasets including the challenging ImageNet benchmark provides strong empirical support for the method\u2019s effectiveness.  Clarity: The paper is wellstructured with a clear delineation of problem formulation related work the methodology of TransBoost and experimental results making it accessible to readers.  Open Source: Providing the implementation of TransBoost enhances the papers impact by allowing others to directly use and build upon this work. Weaknesses:  Analysis of Mechanisms: While the results are impressive the paper lacks a deep dive into why TransBoost works so effectively across different settings. A more detailed analysis or visualization of the features or representations learned during transductive finetuning would add valuable insights.  Comparison with StateoftheArt (SOTA): The paper primarily compares TransBoost with baseline inductive models and some existing SSL and tFSL methods. Including or discussing more direct comparisons with other SOTA transductive learning methods could fortify the significance of the contributions.  Limitations on Dataset Diversity: Despite testing on a variety of image classification datasets the evaluation mainly focuses on visual tasks. The applicability and effectiveness of TransBoost in other domains like NLP are not explored which limits understanding of its versatility. 3. Questions and Suggestions for the Authors  Mechanism Insight: Could the authors provide more insights or theoretical justification for why TransBoost significantly outperforms other methods Any specific properties of deep models or the datasets that it exploits would be informative.  Beyond Image Classification: Have the authors considered testing TransBoost in domains beyond image classification such as text or speech If so could preliminary results or challenges faced be shared  Architectural Compatibility: Is there any analysis on architectures where TransBoost does not perform as expected Understanding limitations or cases where TransBoost is less effective would be helpful for readers. 4. Limitations and Societal Impact The authors do provide implementation details and a comprehensive set of evaluations but the discussion on limitations and potential negative societal impacts is lacking. Specially addressing potential biases that TransBoost might reinforce or introduce during the transductive finetuning process given it optimizes performance based on the given test set is crucial. To improve this aspect the authors should:  Analyze potential biases that can emerge from applying TransBoost in scenarios with unbalanced or biased datasets.  Discuss the limitations of TransBoost when applied to smaller models or datasets with limited representational diversity.  Broadly consider the societal implications of deploying models finetuned with TransBoost especially in sensitive applications like medical diagnosis mentioned in the paper. Constructive suggestions for improvement would include undertaking an ethical and fairness audit of models finetuned using TransBoost exploring regularization techniques that could mitigate potential biases and extending the research to understand its applicability and effects beyond visual classification tasks.", "wgRQ1IM4g_w": "Review of \"A Unified Framework for Constrained Kernelized Bandits with Soft Constraints\" 1. Summary: The paper presents a comprehensive study of constrained kernelized bandit (CKB) optimization under conditions of soft constraints which diverges from the typical hard constraints seen in prior work. Through developing a primaldual optimizationbased framework the authors tackle the problem of stochastic bandit optimization wherein both the reward and constraint functions are unknown nonlinear and lie within a Reproducing Kernel Hilbert Space (RKHS). The key contribution lies in the establishment of a sufficient condition underpinned by primaldual optimization which caters to a variety of exploration strategies beyond Upper Confidence Bound (UCB) and Thompson Sampling (TS) aiming for a balance between complexity regret and constraint violation. Numerical validations demonstrate the enhanced performance of the proposed algorithms over existing methods with realworld applications underscoring the practical relevance of their approach. 2. Assessment: Strengths:  Theoretical Contribution: The introduction of a sufficient condition for general exploration strategies is novel and significant bridging the gap between theoretical optimization and practical application constraints. This contribution is foundational and opens avenues for future exploration in bandit problems.  Comprehensive Analysis: The meticulous comparison between convex optimization tools and the Lyapunovdrift argument offers a deep dive into the nuances of analyzing constrained bandits and MDPs. This comparison not only illuminates the study but also serves as a valuable resource for the community.  Experimental Validation: The extensive simulation studies leveraging both synthetic and realworld datasets provide strong empirical support for the claimed advantages of the proposed framework. The realworld applicability especially in scenarios with heavytailed noise underscores the robustness and versatility of the CKB algorithms. Weaknesses:  Readability and Accessibility: Despite the papers strengths in content the density of the technical discourse may pose barriers to accessibility for readers not deeply versed in the specificities of bandit optimizations or primaldual methods.  Discussion on Practical Implications: While the paper provides extensive theoretical background and experimental validation it could benefit from a more detailed discussion on the direct practical implications and potential applications in various fields beyond the examples provided. 3. Questions and Suggestions:  It would be enlightening if the authors could elaborate on how the proposed framework adapts or performs in dynamically changing environments where the assumptions about the reward and constraint functions stationarity might not hold.  A deeper exploration into the computational scalability of the proposed algorithms especially in highdimensional settings or when the action space is exceedingly large would be valuable. Can the authors propose methods to further optimize computational efficiency in such cases  The authors mention incorporating slackness to achieve bounded or zero constraint violation at the cost of slight regret increase. Could more details be provided on how to select the slackness parameter in practice considering tradeoffs between constraint violation and regret  Given the significant theoretical advancements are there specific domains or practical problems where the authors foresee their framework having the most impact Insight into specific challenges or industries could amplify the papers practical relevance. 4. Limitations and Societal Impact: The authors have adequately addressed the methodological limitations particularly concerning the assumptions underpinning the models theoretical properties. However a broader consideration of potential negative societal impacts especially concerning algorithmic fairness and bias in decisionmaking processes influenced by soft constraints would enrich the discussion. Moreover acknowledging and proposing ways to mitigate potential dependencies on inaccurately estimated models or data anomalies in realworld applications could further strengthen the papers contribution. Constructive Suggestions for Improvement:  Augment the discussion around practical implications potentially through case studies or collaborations with specific domains to showcase the frameworks utility in solving realworld problems.  Include a section that explicitly discusses potential biases or ethical considerations arising from algorithmic decisions based on the presented framework accompanied by mitigation strategies.  Enhance the papers accessibility through a simplified summary or tutorialstyle appendix that outlines the main contributions and methods in a more accessible language. Conclusion: This paper makes a compelling and theoretically rigorous advancement in the study of constrained kernelized bandits with soft constraints. The proposed framework and the sufficiency condition for exploring general exploration strategies are notable contributions that likely will spur further research and application in the field. With improvements in accessibility and extended discussions on practical applications and societal implications the paper could have an even broader impact.", "tzNWhvOomsK": "1) Summary of the Paper and its Contributions: The paper presents a theoretical framework for analyzing ZeroShot Learning (ZSL) with attributes an approach that allows for the classification of objects from categories not seen during training leveraging attribute information instead. The prime contributions include:  The formulation of the first nontrivial lower bound on the worstcase error in ZSL with attributes even with perfect attribute detectors introducing a way to quantify the inherent difficulty of ZSL based on the classattribute matrix.  A method to compute this lower bound through linear programming alongside a proof that this bound is tight by showing an existence of a randomized classifier that achieves an expected error within this lower bound.  Extensive experiments that compare theoretical bounds with empirical errors of existing ZSL methods demonstrating the predictive power of the established lower bounds regarding which classes are likely to be confused. 2) Assessment of the Papers Strengths and Weaknesses: Strengths:  The work bridges a significant gap in the ZSL literature by providing a theoretical underpinning to understand the bounds of performance in attributebased ZSL a domain that previously lacked rigorous analytical groundwork.  The methodology to compute the lower bound is clearly defined and practical allowing for both theoretical insights and applicable use in predicting the behavior of ZSL methods.  The empirical validation of the theoretical findings against realworld datasets and in controlled synthetic setups presents a strong case for the utility and accuracy of the proposed bounds in practical scenarios. Weaknesses:  The focus on attributebased ZSL while necessary for the theoretical analysis limits the applicability of the findings across the broader spectrum of ZSL approaches that use varying auxiliary information such as text descriptions or structured knowledge.  The computational complexity of calculating the lower bound particularly for a large number of attributes could be a significant constraint limiting the ease of applying this framework in practice on highdimensional datasets without simplifications or approximations.  While the paper does touch upon the inherent errors due to domain shifts and misclassification due to attributes a deeper exploration of these aspects within the context of the lower bounds derived could offer more nuanced insights into the challenges of ZSL. 3) Questions and Suggestions for the Authors:  Could the authors elaborate on potential methods to extend the computational feasibility of the lower bound calculation to higherdimensional attribute spaces without significant loss of detail or accuracy  Have the authors considered a comparative analysis of their theoretical model against empirical errors in generalized ZSL settings where both seen and unseen classes are involved in testing Such an analysis could broaden the papers implications.  A suggestion would be to explore collaborating with practitioners in ZSL to test the realworld applicability of these bounds in more diverse domains beyond the datasets explored to further validate and possibly finetune the theoretical model. 4) Limitations and Societal Impact: The paper acknowledges the limitations posed by its focus on attributebased ZSL and the computational challenges in applying the theoretical model. Expanding the scope to accommodate other forms of auxiliary information and optimizing computational methods would be beneficial future directions. As for societal impact the authors rightly point out the necessity of careful interpretation of the theoretical bounds in practical applications ensuring no overestimation of a models capabilities. They also highlight the importance of thoroughly understanding potential errors including those arising from domain shifts and the attributebased approach itself which is crucial for deploying reliable ZSL systems in realworld scenarios. Further discussion on how these theoretical insights could potentially guide the development of more robust and generalizable ZSL models would enhance the papers relevance beyond its academic contributions.", "wOUH1VQ9Rcj": " Peer Review on \"Transformed Independent Noise (TIN) Condition for Causal Discovery under Measurement Error\"  1. Summary This paper introduces a novel approach termed the Transformed Independent Noise (TIN) condition to address the issue of causal discovery under measurement error\u2014a common problem in many realworld scenarios. In essence the TIN condition utilizes independence checks between linear transformations of some measured variables and other measured variables leveraging the nonGaussianity and higherorder statistics of data. This approach seeks to improve the identifiability of the causal structure among unobserved target variables without relying on the computationally and statistically inefficient overcomplete Independent Component Analysis (OICA) which current methods depend on heavily. The paper proposes a methodology that claims to achieve identifiability of the ordered group decomposition useful for causal discovery in scenarios affected by measurement error notably in cases where each latent variable is represented by a single measurement. Experimental results are presented for both synthetic and realworld data to validate the proposed approachs effectiveness and reliability.  2. Strengths and Weaknesses Strengths:  Innovation: The paper addresses a significant challenge in the field of causal discovery with an innovative solution. The introduction of the TIN condition as a means to explore the structure of unobserved variables through independence tests of observed variables is both novel and potentially impactful.  Broad Applicability: The problem of measurement error is pervasive across numerous domains making the proposed solution broadly applicable and valuable for researchers and practitioners across various fields.  Practicality: By overcoming the computational and statistical limitations of OICA the TIN approach presents a more practical tool for causal discovery in realworld scenarios.  Robust Evaluation: The paper includes a thorough evaluation section where the authors present results from experiments on synthetic and realworld data showcasing the methods effectiveness relative to existing approaches. Weaknesses:  Method Complexity and Accessibility: The paper is dense and complex possibly making it difficult for readers who are not experts in causal discovery or statistics to fully comprehend the proposed approach and its advantages.  Limitations and Societal Impact Not Fully Addressed: The discussion on the limitations of the proposed method and its potential negative societal impacts is somewhat lacking. Given the complexity of causal discovery tasks a more detailed discussion on these points would be beneficial.  3. Questions and Suggestions for the Authors  Clarification on Assumptions: Can the authors clarify the assumptions required for the TIN condition to hold especially in realworld scenarios where these assumptions might be violated  Comparison with StateoftheArt: While comparisons with existing methods are provided a deeper analysis of why TIN outperforms these methods in certain scenarios but not others would be insightful. Could the authors elaborate on the conditions under which TIN proves most beneficial  Usability and Implementation: Given the complexity of the proposed approach offering guidance tooling or software that facilitates the application of TIN in practical scenarios would greatly enhance its accessibility and impact.  4. Limitations and Suggestions for Improvement While the authors have showcased the promising capabilities of the TIN approach the paper would benefit from a clearer discussion on its limitations particularly in terms of scalability robustness to violations of underlying assumptions and potential biases introduced by measurement error models. Furthermore the societal implications of deploying such causal discovery techniques in sensitive domains (e.g. healthcare criminal justice) warrant greater attention. To mitigate potential risks the authors could explore incorporating fairness constraints or bias detection mechanisms within the TIN framework.  Conclusion The paper introduces a valuable advancement in the field of causal discovery by proposing a method to handle the pervasive issue of measurement error more efficiently than existing techniques. While promising the approach would gain from further clarity on its limitations a deeper analysis comparing it with current stateoftheart methods and additional guidance on its practical implementation and potential societal impacts.", "sRKNkpUMQNr": "Review of \"An InformationTheoretic Knowledge Distillation Approach for GAN Compression with EnergyBased Models\" 1. Summary: The paper introduces a novel approach for compressing Generative Adversarial Networks (GANs) through knowledge distillation designed to preserve the mutual information between teacher and student networks. The core methodology relies on the use of an energybased model as a variational distribution which enables effective compression of highdimensional data such as images by considering spatial dependencies between pixels. The proposed method is generic and can be easily integrated into various existing GAN architectures and compression techniques enhancing their performance. The paper presents empirical findings indicating significant improvements over stateoftheart methods across several datasets and tasks. 2. Evaluation of Strengths and Weaknesses:  Strengths: a. Novelty: The utilization of an energybased model for variational optimization in the context of GAN compression represents a unique contribution to the field of model compression. This approach allows for a flexible and effective representation of the variational distribution which is particularly suited for highdimensional data. b. Generality and Versatility: The methods compatibility with a broad range of GAN architectures and its ability to be integrated into existing compression algorithms without substantial modifications enhance its applicability and potential impact. c. Performance: The reported performance improvements validated through extensive experiments on various datasets and GAN configurations establish the efficacy of the proposed approach in enhancing GAN compression outcomes. d. Theoretical Foundation: The paper provides a solid theoretical grounding for the proposed method including a detailed description of the optimization process and the rationale behind the choice of the energybased model for the variational distribution.  Weaknesses: a. Complexity and Computational Demand: While the energybased model facilitates a more accurate representation of variational distributions it may also introduce additional computational complexity particularly during the optimization of the energy function. The paper could benefit from a more detailed discussion of the computational requirements and potential strategies for mitigating computational demands. b. Limited Exploration of Failure Modes: The paper could be enriched by a more thorough examination of scenarios or conditions under which the proposed method may underperform including potential limitations related to the scalability of the energybased model to extremely large datasets or complex GAN architectures. c. Comparison with Baseline Methods: While the paper provides empirical evidence of the proposed methods superiority over existing approaches a deeper analysis of the comparative performance including a breakdown of the specific contributions to performance gains could offer more insight into the relative advantages and disadvantages. 3. Questions and Suggestions for the Authors: a. Computational Efficiency: Could the authors comment on the computational efficiency of the proposed method especially in comparison to traditional knowledge distillation approaches for GAN compression b. Failure Modes and Limitations: Are there specific conditions or types of GAN architectures for which the proposed method is less effective Providing examples or theoretical underpinnings could help contextualize the methods applicability. c. Extensions to Other Domains: The paper primarily focuses on imagebased applications. Could the method be effectively adapted for other types of generative models such as those used in natural language processing or audio synthesis d. Hyperparameter Sensitivity: How sensitive is the performance of the proposed method to the choice of hyperparameters particularly the balancing parameter \u03bbMI Could the paper provide guidance on selecting this and other hyperparameters 4. Limitations and Societal Impact: While the authors have provided a comprehensive assessment of the methods strengths a discussion on the limitations especially regarding computational demands and potential biases in compressed models is missing. Further the paper does not explore the negative societal impacts that could arise from more efficient GAN models such as in contexts where realistic image or video generation could be misused. Suggestions for improvement include a dedicated section discussing these aspects to ensure responsible research practice and application of the proposed technology.", "uRSvcqwOm0": " Review of \"A Novel COLPBased Causal Model for Categorical Data\"  Summary The paper introduces a novel causal discovery methodology tailored to categorical data addressing a gap in existing research that has primarily focused on quantitative data. It presents a new classification model termed Classification with Optimal Label Permutation (COLP) that underpins the causal model. This is novel in that it allows for the identification of a causal model by comparing likelihood functions of causal and anticausal models facilitated by the parsimonious nature of COLP. The contributions include proving the identifiability of the COLPbased causal models under certain assumptions a simple learning algorithm for causal discovery and the demonstration of the methods superiority over existing approaches through experiments with both synthetic and real data. Additionally the authors provide an R package COLP which includes the causal discovery algorithm and a benchmark dataset of categorical causeeffect pairs.  Strengths and Weaknesses Strengths: 1. Novelty and Relevance: The proposed method addresses a significant gap in the literature on causal discovery for categorical data offering a novel approach that is both parsimonious and provably identifiable. The relevance of this work is evident given the prevalence of categorical data in various domains. 2. Soundness of the Methodology: The authors provide a solid theoretical foundation for their method including proofs for the identifiability of the COLPbased causal models. The approach is theoretically sound and has a logical flow. 3. Comprehensive Evaluation: The paper presents a thorough evaluation using synthetic and real data demonstrating the methods effectiveness and its superiority over existing methods. The provision of an R package is also a significant practical contribution that facilitates reproducibility and further exploration. 4. Broad Implications: The findings have broad implications for fields where causal discovery from categorical data is essential but challenging including social sciences genomics and epidemiology. Weaknesses: 1. Assumption of No Unmeasured Confounders: While the authors mention this as a limitation its significant because unmeasured confounders are common in observational studies which may limit the practical applicability of the method. 2. Scalability Concerns: The proposed greedy search algorithm may face scalability issues with a large number of categories which could limit its applicability to largescale datasets. 3. Limited Discussion on Potential Misapplications: The paper could further discuss the potential for misapplication especially in scenarios where the assumptions do not hold and provide guidelines to mitigate such risks.  Questions and Suggestions for the Authors 1. Addressing Unmeasured Confounders: Can the authors elaborate on potential extensions or modifications to the COLPbased causal discovery method that could account for unmeasured confounders 2. Scalability Enhancements: Are there any planned improvements or ongoing work to enhance the scalability of the greedy search algorithm for cases with a large number of categories 3. Guidance on Misapplication: Could the authors provide additional guidance on identifying scenarios where the COLPbased method might be misapplied and suggest strategies for researchers to validate the appropriateness of the method for their specific applications  Limitations and Societal Impact The authors acknowledge limitations including the assumption of no unmeasured confounders and the focus on bivariate causal discovery. Expanding the discussion to include the societal impact especially concerning potential biases when causal models are wrongly specified or identified would be beneficial. Constructive suggestions include exploring modifications to handle unmeasured confounders and extending the method to multivariate cases to increase its applicability. Engaging with these issues can help ensure that the methods application is both scientifically rigorous and socially responsible.", "yHFATHaIDN": "1) Summary of the Paper and Its Contributions The paper introduces a novel algorithm called MABSplit designed to substantially accelerate the training process of random forests and related treebased machine learning models without compromising the generalization performance. The algorithm adapts techniques from the multiarmed bandit (MAB) literature to efficiently identify optimal featurethreshold pairs for node splitting in decision trees thus reducing the sample complexity from linear to logarithmic relative to the size of the data set. This results in up to 100x faster training times in certain cases. The paper provides theoretical guarantees for the performance and efficiency of MABSplit demonstrates its applicability across various forestbased model variants and task types (classification and regression) and shows its superior performance in generalization and feature importance calculations under limited computational budgets compared to existing methods. 2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Technical Soundness: The paper introduces an innovative approach by incorporating multiarmed bandit techniques into the training process of treebased models. The theoretical grounding and empirical results validating the algorithm contribute significantly to its technical soundness.  Significant Performance Improvements: Demonstrating up to 100x speedup in training times without degrading performance is a substantial achievement. This could have widereaching implications for the application of machine learning models in resourceconstrained environments.  Versatility and Broad Applicability: The algorithm\u2019s effectiveness across different treebased model variants and both classification and regression tasks suggests a broad applicability making it a valuable contribution to the field.  Open Source Implementation: Providing a reproducible oneline script enhances the papers utility and facilitates further research and application of the proposed algorithm. Weaknesses:  Dependency on Distributional Assumptions: While MABSplit shows excellent performance under certain conditions its efficiency could potentially be impacted by the underlying data distribution. The paper could benefit from a more detailed investigation into how varying distributions affect the algorithm\u2019s performance.  Comparison to StateoftheArt Methods: Although comparisons are made with baseline models it would strengthen the paper to see how MABSplit performs against the latest stateoftheart methods specifically designed for accelerating treebased model training.  Practical Implementation Considerations: While the paper discusses theoretical aspects and provides a oneline script for reproducibility there is less focus on practical implementation challenges particularly in nonstandard computational environments. 3) Questions and Suggestions for the Authors  Distributional Sensitivity: Could the authors elaborate on how MABSplit\u2019s efficiency is impacted by the underlying data distribution Are there particular types of distributions where MABSplit might not offer significant improvements  Comparative Analysis Extension: It would be beneficial to include a comparison with the latest stateoftheart methods focused on accelerating treebased model training. This could help readers better understand MABSplits position within the broader landscape.  Practical Deployment Guidance: Can the authors provide insights or guidelines on deploying MABSplit in practical possibly resourceconstrained environments Particularly are there any specific computational architectures or configurations where MABSplit performs optimally or encounters limitations 4) Addressing Limitations and Societal Impact The authors have adequately demonstrated the theoretical underpinnings and empirical efficacy of MABSplit along with acknowledging its broad applicability in various settings. However further discussion around the computational resource requirements in different practical deployment scenarios would enhance the paper. Addressing possible limitations related to data distribution sensitivity and providing comprehensive comparisons to stateoftheart could also improve the work. Regarding societal impact while the paper highlights the potential benefits of applying MABSplit in various contexts a more detailed analysis of potential negative impacts especially in sensitive applications (e.g. healthcare criminal justice) would be valuable. Suggestions for future research could include mechanisms to ensure that the acceleration does not inadvertently introduce biases or negatively affect model fairness. Conclusion: MABSplit represents a significant step forward in the efficient training of treebased models. Addressing the highlighted concerns and questions could further solidify its contributions to the field.", "yLilJ1vZgMe": "Highquality Peer Review: 1) Summary of the Paper: The paper aims to bridge the theoretical and practical gap in the usage of neural kernels specifically in the context of the infinite width limit of neural networks. It primarily addresses the computational challenges and limitations associated with neural kernels such as their superquadratic runtime and memory complexities. Prior research mainly focused on ReLU activations due to its popularity and the computational difficulty of working with general activations. The key contributions of this work are as follows:  It expands the repertoire of activation functions for which neural kernels can be computed by introducing a method to approximate neural kernels for general activation functions using the truncated Hermite expansion.  It proposes a fast sketching method to approximate the Neural Network Gaussian Process (NNGP) kernel and Neural Tangent Kernel (NTK) matrices significantly speeding up the computation without requiring the data points to be on the unit sphere.  It introduces a subspace embedding for NNGP and NTK matrices which can be applied to any homogeneous dual activation functions effectively reducing runtime and memory requirements.  Empirical evaluations demonstrate the efficiency of the proposed methods particularly highlighting a 106\u00d7 speedup in computing the convolutional NTK (CNTK) for a specific neural network architecture on the CIFAR10 dataset. 2) Assessment of Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper significantly advances our understanding of neural kernels for general activation functions and proposes innovative methods to compute them efficiently. The theoretical foundations are solid with clear formulations and rigorous proofs provided in the appendices.  Empirical Validation: The speedup achieved in computing CNTKs is impressive indicating the practical value of the proposed methods. The empirical results are robust backed by comparisons against existing methods and demonstrate clear advantages in terms of computation speed without sacrificing accuracy.  Generalizability: The methods proposed are applicable to a wide range of activation functions making them highly versatile and potentially impactful across different domains where neural kernels are utilized. Weaknesses:  Scalability to Larger Networks and Datasets: While the paper reports significant computational improvements it mainly focuses on a specific architecture (Myrtle5) and the CIFAR10 dataset. Future work should investigate the scalability of the proposed methods to larger networks and datasets.  Exploration of Other Activations: Although the paper extends the list of activations beyond ReLU a more comprehensive exploration of various activation functions including recent advancements in activation designs would be beneficial.  Potential Negative Societal Impact: The discussion on limitations and potential negative societal impacts is somewhat limited. Given the increasing importance of ethical AI development a more detailed examination in this regard would be valuable. 3) Questions and Suggestions for Authors:  Scalability: Could the authors expand on the scalability of their methods to larger datasets and more complex neural network architectures Providing insights or preliminary results on this aspect would be valuable.  Comparison with Other Activation Functions: It would be interesting to see a broader comparison with more diverse activation functions especially those emerging from recent literature on activation design. Could the authors comment on the applicability or performance of their methods with such activations  Detailed Analysis of Computational Requirements: While the speedup is significant a deeper analysis comparing the computational and memory requirements of the proposed methods with existing approaches across different settings would add value.  Negative Societal Impact: Could the authors provide a more comprehensive discussion on the potential negative societal impacts of their work including any misuse scenarios and suggest possible mitigations 4) Limitations: The authors have made commendable efforts to address the limitations of their work. However a more detailed exploration of the scalability of the proposed methods to more extensive networks and datasets a broader comparison with a wider range of activation functions and a deeper discussion on potential negative societal impacts would strengthen the paper. Suggestions for improvements have been provided accordingly.", "wlqb_RfSrKh": " Peer Review of \"Selfsupervised Amodal Video Object Segmentation (SaVos)\"  Summary: This paper introduces a novel framework SaVos designed to tackle the complex problem of amodal object segmentation specifically in video data. Amodal perception refers to the capability of inferring the entirety of an object\u2019s shape despite parts of it being occluded. Achieving this computationally is particularly challenging as it requires synthesizing information beyond the immediately available visual data and handling the scarcity of wellannotated training data. The authors propose SaVos a selfsupervised learning paradigm leveraging video temporal sequences to infer objects amodal masks utilizing visible parts of objects in different frames as selfsupervision. They also introduce a novel architecture that improves generalization by learning from both type priors and spatiotemporal priors and showcases adaptability to new distributions via testtime adaptation demonstrating stateoftheart performance on FISHBOWL and KINSVideoCar benchmarks.  Strengths: 1. Novelty of Approach: The selfsupervised learning paradigm leveraging spatiotemporal data is an innovative approach to amodal segmentation. This approach effectively utilizes existing data without the need for extensive annotated datasets. 2. Comprehensive Architecture: SaVos architecture cleverly integrates learning from both type and spatiotemporal priors paving the way for better generalization to unseen object types and distributions. 3. Adaptation Capability: The inclusion of a testtime adaptation strategy that increases the models performance on novel distributions is particularly impactful for realworld applications highlighting the models potential for broad applicability. 4. Empirical Validation: The authors provide extensive empirical evidence demonstrating SaVos superiority over existing methodologies on benchmark datasets reinforcing the effectiveness of their proposed solution.  Weaknesses: 1. Complexity of Implementation: The sophisticated architecture while effective may pose a challenge in terms of implementation and computational resource requirements potentially limiting accessibility for broader research or practical usage. 2. Dependence on Video Data Quality: SaVos performance is inherently tied to the quality and variety of temporal sequences available in the dataset. This may limit its performance in scenarios where video data is sparse lowquality or lacks the necessary diversity. 3. Potential Bias Towards Visible Object Parts: While the method utilizes visible parts of objects in different frames as supervision this might introduce a bias towards more frequently visible parts possibly affecting the models learning of less visible or typically occluded object features. 4. Lack of RealWorld Deployment Evidence: Although the model shows impressive performance on synthetic and realworld benchmarks there is little evidence of its effectiveness and practicality in realworld deployment scenarios.  Questions and Suggestions for the Authors: 1. Mitigating Implementation Complexity: Can the authors suggest ways to simplify the implementation or reduce the computational requirements of SaVos making it more accessible 2. Addressing Video Data Quality Issues: How does SaVos perform under less ideal conditions such as poor quality or limited diversity in video datasets and what strategies might mitigate these limitations 3. Reducing Bias Towards Visible Parts: Is there a mechanism within SaVos to ensure balanced learning across both frequently and infrequently visible object parts avoiding potential bias 4. Exploring RealWorld Deployability: Can the authors provide insights or preliminary results on deploying SaVos in practical applications especially concerning adaptability and computational efficiency in dynamic environments  Limitations and Societal Impact: The paper does not deeply explore the limitations related to the models dependence on highquality video data and its potential biases. Additionally while the authors demonstrate SaVos capability for generalization and adaptation discussions on the ethical considerations and potential negative societal impacts of deploying such technology in sensitive domains (e.g. surveillance) are absent. It would be beneficial for future work to consider these aspects perhaps proposing guidelines for ethical use and data privacy protection. Suggestions for Improvement: 1. A more detailed examination of the models limitations and potential biases would strengthen the papers contribution. 2. Offering strategies or insights into overcoming data quality and diversity issues can enhance the model\u2019s applicability. 3. Including discussions on ethical considerations and potential societal impacts could provide a holistic view of SaVos deployment in realworld scenarios.", "rwyISFoSmXd": " Peer Review of \"Conformal Fair Quantile Regression for Demographic Parity\"  1. Summary of the Paper The paper introduces a novel approach to address fairness in algorithmic predictions focusing on quantile fairness rather than the commonly addressed mean fairness. The authors develop a framework that constructs prediction intervals which are fair with respect to demographic parity (DP) utilizing optimal transport and functional synchronization techniques. Their proposed method Conformal Fair Quantile Prediction (CFQP) guarantees distributionfree coverage and exact DP fairness across different quantiles of a predictive distribution. Theoretical proofs and empirical studies demonstrate the effectiveness of the CFQP method in achieving both accuracy and fairness over several benchmark datasets in diverse applications like societal and medical fields.  2. Assessment of Strengths and Weaknesses Strengths:  Originality and Significance: The paper addresses an underexplored yet crucial aspect of algorithmic fairness quantile fairness which is vital for creating more equitable machine learning models. The novelty of applying optimal transport for fair quantile estimations marks a significant contribution to the field.  Theoretical Rigor: The authors provide a solid theoretical foundation for their method with proofs to demonstrate coverage and fairness guarantees. These theoretical contributions enhance the papers credibility and scientific value.  Empirical Validation: Through extensive experiments on multiple datasets the paper effectively demonstrates the practical applicability and advantages of the CFQP method in achieving fair prediction intervals without sacrificing much on accuracy. Weaknesses:  Limitation in Methodological Diversity: The paper predominantly focuses on a postprocessing adjustment method for achieving fairness. While effective exploring and comparing with inprocessing methods could have provided a more rounded view of the methods comparative advantages or limitations.  Lack of Discussion on Computational Efficiency: Although the paper provides a solid theoretical and empirical foundation there is scant discussion on the computational efficiency and scalability of the proposed method an important aspect for realworld applications.  3. Questions and Suggestions for the Authors  On Computational Efficiency: Could the authors elaborate on the computational complexity of their method especially how it scales with large datasets or highdimensional data Providing such details would be invaluable for assessing the methods practical applicability.  Extension to Other Fairness Measures: The current work focuses on DP. Have the authors considered extending their method to accommodate other fairness measures like Equalized Odds or Individual Fairness If so what modifications would be necessary  Incorporation of InProcessing Methods: Is there potential for integrating the CFQP approach as an inprocessing fairness method rather than just postprocessing This could potentially offer more robust fairness adjustments directly during model training.  4. Addressing Limitations and Potential Negative Societal Impact The authors provide a thorough analysis of their methods effectiveness but could expand on discussing the limitations regarding computational demands and potential inapplicability to certain types of data or fairness scenarios. Furthermore while the paper briefly touches on the societal impact a deeper exploration of potential negative impacts especially misapplications of the fairness method would reinforce the papers thoroughness. For instance overreliance on demographic parity could ignore subtle biases at individual levels or between subgroups within sensitive attributes. Constructive Suggestions:  It would be beneficial to include a detailed analysis or scenariobased discussion on how the CFQP method performs in extremely imbalanced datasets or those with highly skewed distributions.  An expanded discussion on mitigating any identified negative societal impacts possibly through recommendations or ethical guidelines for implementing quantile fairness would be valuable.", "pl279jU4GOu": " Review of \"A New Strategy for Proving Convergence of Deep Learning Architectures by Gradient Flow\"  1. Summary This paper introduces a novel strategy to address one of the fundamental challenges in deep learning: proving the convergence of gradient flows in training deep learning architectures to zero loss regardless of whether it is for training or testing loss. The authors present their analysis based on the concept of Rayleigh quotients which they leverage to prove Kurdyka\\xc5\\x81ojasiewicz (K\\xc5\\x81) inequalities for a wide range of neural network architectures and loss functions. This approach unifies several existing convergence analysis methods and extends the spectrum of architectures and scenarios where convergence proofs can be obtained without requiring an infinite number of parameters or a finite number of samples. Through various sections the paper elaborates on their findings covering definitions for gradient flows neural tangent kernels the establishment of K\\xc5\\x81 inequalities through Rayleigh quotients and their application to both artificial and realistic deep learning scenarios including periodic signal recovery and logistic regression.  2. Assessment of Strengths and Weaknesses Strengths  Novelty: The innovation lies in utilizing Rayleigh quotients for establishing K\\xc5\\x81 inequalities offering a fresh perspective on proving convergence for deep learning models in scenarios previously deemed challenging.  Broad Applicability: By not restricting the analysis to scenarios with an infinite number of parameters or a finite dataset the authors significantly broaden the applicability of their theoretical findings to practical deep learning tasks.  Rigor: The mathematical rigor in defining terms stating propositions and deriving conclusions is commendable contributing to the clarity and depth of the analysis.  Insightful Case Studies: The diverse set of examples including the recovery of known bounds for linear models with quadratic loss and the novelty in periodic signal recovery showcases the methods versatility and practical relevance. Weaknesses  Accessibility: The high level of mathematical detail while a strength in terms of rigor might limit the papers accessibility to a broader audience who might benefit from these insights but lack the specific background in mathematics.  Empirical Validation: The paper is heavily theoretical and while this is understandable given its nature the inclusion of empirical validation of the proposed method on realworld datasets or architectures would have enriched the findings and demonstrated practical effectiveness.  Discussion on Limitations and Negative Impact: The paper lacks a dedicated discussion regarding its limitations or potential negative societal impacts. While theoretical mathbased research might seem distant from direct societal impacts a reflection on potential misapplications or limitations would provide a balanced view.  3. Questions and Suggestions for the Authors  Could the authors provide more intuition or simplified explanations for a broader audience without compromising the mathematical rigor This might include graphical interpretations of Rayleigh quotients or more accessible examples.  Are there scenarios or specific network architectures where the proposed method may not be as effective or applicable A discussion on this would help clarify the boundaries of the methods applicability.  It would be beneficial if the authors could explore or suggest potential pathways for empirical testing of their theoretical findings offering insights into how these theoretical advancements could be translated into practice.  4. Limitations and Societal Impact While the authors have made a significant theoretical contribution the paper does not address its limitations or the societal impact of the work. Its essential for the authors to consider scenarios where the approach might fail or not be as efficient. Additionally a reflection on how these theoretical frameworks could impact AI safety ethics or contribute to adversarial robustness could offer a more comprehensive view of the works significance. Suggestions for Improvement  Include a section discussing potential limitations including scenarios where the method may not hold or might require adaptations.  Reflect and comment on the broader societal implications of ensuring convergence in deep learning systems emphasizing both positive potential impacts (e.g. more reliable AI systems) and negative aspects (e.g. misuse in creating more potent adversarial attacks). Overall this paper presents a valuable theoretical contribution to the field of deep learning offering a novel perspective on proving convergence that could stimulate further research and practical advancements in the field.", "qbSB_cnFSYn": " Peer Review of \"Differential Equation GAN: Solving Differential Equations using Adversarial Networks\"  1) Summary of the Paper: The paper presents Differential Equation GAN (DEQGAN) a novel method for solving ordinary and partial differential equations (ODEs and PDEs) using generative adversarial networks (GANs) in an unsupervised manner. DEQGAN learns the optimal loss function for the neural network model to solve differential equations circumventing the need for predefined loss functions which lack theoretical justification in the context of physicsinformed neural networks (PINNs). The authors test DEQGAN on a suite of twelve differential equations showing superior performance in terms of solution accuracy compared to traditional PINNs that use standard loss functions like L2 L1 and Huber losses. Additionally DEQGANs performance is competitive with conventional numerical methods. The paper also introduces two techniques to enhance the robustness of DEQGAN against hyperparameter variations.  2) Assessment of Strengths and Weaknesses: Strengths:  Originality: DEQGAN introduces a groundbreaking approach to solve differential equations by \"learning\" the loss function using a GAN framework setting it apart from existing PINN methods.  Performance: The method achieves impressive accuracy outperforming classical PINNs on a diverse set of differential equations which includes highly nonlinear problems.  Robustness: The introduction of techniques to improve DEQGANs training stability and sensitivity to hyperparameters is commendable addressing a common challenge in training GANs. Weaknesses:  Complexity: The GANbased approach inherently involves a higher computational complexity compared to traditional PINNs. The paper could benefit from a detailed discussion or quantification of this aspect in comparison with conventional methods.  Generalization: While the paper shows DEQGANs efficacy across a suite of differential equations it does not thoroughly address its capability to generalize across a wider range of equations especially those not tested in the study.  Implementation Details: The manuscript could provide more implementation details or discuss the challenges encountered during the development of DEQGAN which would be beneficial for practitioners looking to apply this methodology.  3) Questions and Suggestions for the Authors:  Generalization: Can the authors elaborate on the potential of DEQGAN to adapt and perform on differential equations that exhibit greater complexity or different characteristics than those tested  Computational Resources: Could the authors provide insights into the computational resources required for DEQGAN especially in comparison to PINNs employing traditional loss functions  User Guidance: Are there specific recommendations or best practices the authors can provide for selecting hyperparameters or making modifications to DEQGAN for different types of differential equations  Limitations and Potential Negative Societal Impact: The paper briefly mentions the lack of theoretical understanding of the loss function learned by DEQGAN as a limitation. For a more comprehensive analysis the authors could also discuss the computational demands and potential environmental impact stemming from training highly complex models like GANs. Additionally exploring theoretical error bounds for DEQGANs solutions could greatly enhance its reliability and applicability in critical scientific and engineering tasks. Addressing these points would not only strengthen the paper but also contribute to the responsible advancement of machine learning in scientific computation.", "yRhbHp_Vh8e": " Review of \"VideoWhisperer: EndtoEnd Grounded Video Situation Recognition\"  1) Summary: The paper introduces VideoWhisperer a novel threestage transformer architecture tailored for Grounded Video Situation Recognition (GVSR). GVSR is an extension of the Video Situation Recognition (VidSitu) task that requires not only identifying and captioning multiple actions and involved entities (semantic role labeling SRL) but also localizing these actions and entities within a video in a weakly supervised setting. VideoWhisperer aims to address challenges in understanding complex video sequences by jointly predicting action verbs roleentity pairs and spatiotemporal grounding. The method consists of 1) learning contextualized video and object features to enable finegrained spatiotemporal reasoning 2) deploying verbrole queries to pool relevant information for localizing actions and entities and 3) generating descriptive captions for each pair. Evaluated on a groundingaugmented VidSitu dataset VideoWhisperer achieves stateoftheart performance showcasing significant improvements in entity captioning accuracy and the ability to localize verbrole pairs.  2) Strengths and Weaknesses: Strengths:  Novelty and Relevance: The integration of spatiotemporal grounding into the structured prediction task of video situation recognition addresses a significant gap in dense video understanding. This addition enriches the task by requiring models to provide more detailed and spatially anchored information about video content.  Architecture Design: The threestage transformer architecture cleverly disentangles the tasks of learning contextualized features verbrole querying and caption generation. This design enables the model to capture both global video context and detailed eventlevel semantics improving its overall predictive capacity.  EndtoEnd Training: By enabling joint predictions for verbs semantic roles and their spatiotemporal grounding without the need for groundtruth verbs during training the model simplifies the GVSR task into a more practical and scalable solution.  Performance: The reported improvements in CIDEr score and the successful demonstration of weaklysupervised grounding in the absence of explicit bounding box annotations during training are significant achievements showcasing the models practical effectiveness. Weaknesses:  Evaluation Methodology: Despite the impressive performance metrics the paper could benefit from a more thorough discussion on the robustness of the model across varied and potentially noisier datasets. Evaluating generalizability and performance in more challenging or less controlled environments would strengthen the papers contributions.  Model Complexity and Efficiency: Given the sophisticated architecture comprising multiple transformer stages and attention mechanisms theres a concern about the computational efficiency and scalability of the model especially for longer videos or those with higher frame rates. Discussing the models computational requirements and potential optimizations could offer a more complete picture of its practicality.  Limitations on Grounded SRL: While the paper makes strides in grounding semantic roles it is less clear how the model performs in disambiguating complex or closely related actions and roles especially when multiple entities or actions occur simultaneously. Further exploration into the models limitations in these aspects would be informative.  3) Questions and Suggestions for Authors:  Handling of Complex Scenes: Can the authors provide insight into how VideoWhisperer performs in scenes with multiple overlapping actions or where entities switch roles rapidly within a clip Suggestions on handling these complexities could be valuable.  Computational Efficiency: Could the authors discuss any tested or potential strategies for optimizing the models efficiency particularly for longer videos or realtime processing applications  Future Directions: Are there plans to explore the integration of audiovisual data to further enhance situation recognition and grounding given that auditory cues can offer significant context in videos  4) Addressing Limitations and Societal Impacts: The authors have made commendable initial efforts in discussing the limitations and potential negative societal impacts of their work particularly concerning biases in captioning and grounding. However a more detailed exploration into mitigation strategies for these biases and ensuring model fairness would be beneficial. Suggestions include incorporating diverse training datasets and exploring biaselimination techniques in model training. Furthermore a discussion on the potential of using such technology for surveillance without consent and ways to safeguard against unethical uses could add depth to the papers exploration of societal impacts.", "rwdpFgfVpvN": " Peer Review of \"A Rectified Online Optimization Algorithm for COCOHard\"  1) Brief Summary: This paper introduces a novel algorithm named RECtified Online Optimization (RECOO) designed for Constrained Online Convex Optimization (COCO) with hard constraints. Unlike existing works that focus on soft constraints where constraint violations can be negated by cautious decisions in future rounds RECOO tackles hard constraints ensuring minimal violation that cannot be compensated in later decisions. The paper investigates two scenarios: fixed constraints where constraint functions remain consistent but unknown to the learner and adversarial constraints which vary unpredictably. RECOOs performance measured in terms of regret and cumulative hard constraint violation is superior to existing algorithms under both scenarios. It achieves optimal theoretical bounds for fixed constraints and competitive results for adversarial constraints. The paper extends its analysis to include dynamic baselines further enhancing its contributions to the field of online learning and optimization.  2) Assessment of Strengths and Weaknesses: Strengths:  Novelty in Problem Addressing: The focus on hard constraints in COCO and the development of a versatile solution applicable to both fixed and adversarial scenarios are innovative and fill a significant gap in the literature.  Theoretical Contribution: RECOO not only meets but surpasses current benchmarks in several scenarios offering new theoretical bounds. This advancement is likely to spur further research and applications in the area.  Experimental Validation: Experiments conducted show RECOOs practical effectiveness over existing algorithms thus validating theoretical advancements with empirical evidence.  Clarity and Depth of Analysis: Extensive technical development and a clear exposition of the rectified decision and penalty update mechanisms provide insightful understanding into why RECOO performs better. Weaknesses:  Complexity Analysis is Absent: While the paper excels in theoretical and empirical performance analysis it lacks an examination of the computational complexity of RECOO which is critical for practical applications.  Limited Empirical Settings: Despite strong experimental results the tested scenarios seem somewhat narrow. A broader range of realworld applications demonstrating RECOOs versatility would enhance the papers impact.  Discussion on Parameter Sensitivity Lacking: Insights into how the selection of learning rates and other parameters affects performance are not provided which could be vital for the algorithms deployment in various settings.  3) Questions and Suggestions for the Authors:  Complexity Analysis: Could the authors provide a computational complexity analysis of RECOO Understanding its scalability is important for largescale applications.  Broader Empirical Validation: Are there additional domains where RECOOs effectiveness could be demonstrated Specifically can the authors explore its performance in highdimensional settings or under rapidly changing environments  Parameter Selection Guidance: Further insights into the robustness of RECOO to parameter settings and some practical guidelines for their selection would be highly beneficial.  4) Limitations and Societal Impact: The authors have made an excellent effort in explaining the significance of hard constraints in safetycritical applications thus highlighting the societal impact of their work. However the paper could further discuss the limitations related to computational requirements especially in contexts where decisionmaking needs to happen in realtime and the potential negative implications of constraint violations beyond theoretical bounds. Suggestions for improvement include:  Exploring Lightweight Variants: For scenarios where computational resources are limited it would be constructive to propose and analyze more computationally efficient albeit potentially suboptimal variants of RECOO.  Ethical Considerations in Application Areas: Expanding the discussion on applications to include ethical considerations especially where hard constraints might relate to privacy fairness or environmental impact would round out the papers exploration of societal implications. In summary this paper represents a significant step forward in online convex optimization with hard constraints offering both theoretical insights and practical advancements. Addressing the outlined weaknesses and considerations could further cement its contributions and applicability to a broader array of realworld challenges.", "lHj-q9BSRjF": "Review of \"Emergent InContext Learning in Transformers: The Role of Data Distribution\":  1. Summary of the Paper and its Contributions This paper investigates the underlying factors enabling large transformerbased models to perform incontext fewshot learning without explicit training. The authors hypothesize and demonstrate that the training datas distributional properties such as burstiness a large number of rarely occurring classes and dynamically interpreted items are pivotal for this emergent behavior. Through experiments with the Omniglot dataset and various configurations of training data distribution they provide evidence that transformers can exhibit both incontext and conventional weightbased learning under specific conditions. The distinction between transformers and recurrent neural network architectures in their capacity for incontext learning is also explored.  2. Assessment of Strengths and Weaknesses Strengths:  Novel Insight: The paper sheds light on the significant impact of the training datas distribution on the emergent property of incontext learning in transformers. This is a valuable contribution to understanding how large language models function.  Rigorous Experimental Design: The varied experiments using burstiness rarity of classes dynamic interpretation and data distribution manipulation offer a solid foundation to the authors claims. The use of the Omniglot dataset and transformer models adds relevance and rigor.  Broad Implications: Findings have broader implications for dataset design understanding cognitive development and future research directions making the paper valuable beyond the immediate scope of natural language processing. Weaknesses:  Lack of Comparison: While the paper compares transformers and recurrent models it lacks a broader comparison with other stateoftheart models or approaches that could further validate the uniqueness and significance of the findings.  Potential Overfitting: The experimental setup especially with manipulated training distributions raises questions about potential overfitting or the models generalizability to realworld data which exhibits even more complexity.  Limited Contextual Application: While findings are theoretically applicable beyond language practical evidence of this application in domains like vision or reinforcement learning is not provided.  3. Questions and Suggestions for Authors  Generalizability to RealWorld Data: How do the authors anticipate these findings will translate to more complex realworld datasets where distribution properties might not be as easily manipulated or identified  Comparison with Other Models: Could the authors extend their comparison to include other stateoftheart models in fewshot learning or other architectures providing a clearer benchmarking of transformers capabilities in this area  Limitation Addressing: In future work could the authors consider experiments in more varied domains such as vision or auditory processing to substantiate the broad applicability of their findings  4. Limitations and Societal Impact The authors provide an insightful discussion on broader implications yet the paper could benefit from a more detailed exploration of limitations especially concerning the models applicability to realworld complex datasets. Additionally a discussion on the potential negative societal impacts such as biases arising from skewed data distributions would be valuable. Strategies to mitigate such biases and ensure ethical use of incontext learning models in diverse applications could enhance the papers contribution to responsible AI development.", "zyrBT58h_J": "The paper presents a novel approach to the autobidding problem in online advertising where it introduces a Sustainable Online Reinforcement Learning (SORL) framework to address the problem of inconsistencies between online and offline (IBOO). Traditional approaches rely on training autobidding policies within a virtual advertising system (VAS) constructed from historical data. However discrepancies between the simulated environment (VAS) and the realworld advertising system (RAS) lead to suboptimal policy performance due to IBOO. The authors SORL framework proposes direct interaction with the RAS for training thus obtaining true rewards and state transitions and circumventing the IBOO issue. The SER (Safe and Efficient Online Exploration) policy within SORL ensures data collection safety from the RAS and their VarianceSuppressed Conservative Qlearning (VCQL) method enables stable and effective autobidding policy learning. The paper validates their approach through extensive simulated and realworld experiments showcasing superior performance compared to stateoftheart autobidding algorithms.  Strengths: 1. Innovative Approach: The shift from VASbased to RASbased training addresses the critical issue of IBOO directly allowing for realtime adjustments to the bidding policy based on current market conditions. 2. Comprehensive Analysis: The paper thoroughly investigates the causes and impacts of IBOO providing a solid foundation for their proposed solutions. 3. Theoretical Foundation: The proof of the Lipschitz smooth property of the Q function and the formulation of the safety zone for online exploration are wellfounded and contribute significantly to ensuring the safety and effectiveness of the SER policy. 4. Empirical Validation: The extensive simulated and realworld experiments accompanied by ablation studies robustly validate the effectiveness and efficiency of the SORL framework and its components.  Weaknesses: 1. Complexity and Scalability: The paper does not fully discuss the computational requirements or the scalability of the SORL framework particularly in extremely largescale RAS environments. 2. Generalizability: While the paper demonstrates the SORL frameworks efficacy in the context of online advertising its applicability to other domains with similar onlineoffline inconsistencies is not explored. 3. Longterm Impacts: There is limited discussion on the longterm effects of continually adapting bidding strategies directly in the RAS including potential market volatility or adversarial responses from competitors.  Questions and Suggestions: 1. Computational Overhead: Could the authors elaborate on the computational overhead introduced by the SORL framework especially in comparison to traditional VASbased methods 2. Applicability to Other Domains: How might the SORL framework be adapted for use in other domains facing similar onlineoffline inconsistencies Are there particular characteristics of the autobidding problem that uniquely enable the success of SORL 3. Market Impact Analysis: It would be beneficial to include an analysis of the potential market impacts of widespread adoption of SORLbased autobidding including effects on market prices and competitiveness.  Limitations and Societal Impact: The authors do not explicitly discuss the limitations related to computational resources and potential consequences on market dynamics due to the adaptive nature of SORL. It would be constructive to address these points and explore mechanisms to mitigate negative impacts such as increased market volatility or unfair competitive advantages. Additionally discussing potential safeguards against adversarial exploitation of the SORL framework in highly competitive markets could enhance the papers consideration of societal impacts. Overall this paper makes significant contributions to solving the IBOO problem in autobidding systems with a strong theoretical foundation and compelling empirical evidence supporting its claims. However a more comprehensive discussion on scalability generalizability and longterm market impacts would further strengthen the paper.", "kHrE2vi5Rvs": " Review of \"Symmetric Neural Combinatorial Optimization\"  1. Summary of the Paper and Its Contributions The paper introduces Symmetric Neural Combinatorial Optimization (SymNCO) a novel training scheme that enhances the performance of deep reinforcement learning (DRL)based combinatorial optimization (CO) methods. This scheme utilizes universal symmetricities such as rotational and reflectional invariance found in various CO problems to improve the generalization capabilities of DRLNCO solutions. The authors demonstrate the efficacy of SymNCO through experiments on four CO tasks achieving superior results over existing DRLNCO methods and a conventional solver on certain tasks without relying on problemspecific expert domain knowledge.  2. Assessment of Strengths and Weaknesses  Strengths: 1. Innovation: SymNCO represents a significant advancement in the field of neural combinatorial optimization particularly in leveraging inherent symmetricities of CO problems to improve model performance and generalization. 2. Broad Applicability: The methodology is applicable across a range of CO problems demonstrating versatility beyond problemspecific adaptations which dominate the field. 3. Empirical Validation: The conducted experiments provide strong empirical evidence of SymNCOs efficacy particularly the remarkable achievement in outperforming existing methods and even a conventional solver in the prize collecting TSP at a substantially faster speed. 4. Open Source Contribution: The decision to make the source code publicly available encourages reproducibility further research and practical applications of the work.  Weaknesses: 1. Limited Consideration of Other Symmetricities: While rotational and reflectional symmetricities are explored other potential symmetricities and their impacts on different types of CO problems are not extensively discussed. 2. Scalability Concerns: The experiments focus on relatively moderatesized problems. The performance and efficiency of SymNCO on much larger scales are not clear which may limit its applicability in realworld scenarios. 3. Impact on Combinatorial Space and Solutions Diversity: By focusing on symmetric solutions theres a potential concern regarding the impact on the combinatorial space explored and the diversity of solutions especially for problems where unconventional solutions might be beneficial. 4. Computational Resource Requirements: Although the paper notes improved speeds in some cases a comprehensive analysis of computational resources and time requirements across various tasks is missing.  3. Questions and Suggestions for the Authors  Further Exploration of Symmetricities: Can the authors elaborate on potential future research directions for incorporating other forms of symmetricities such as scaling and translational invariance  LargeScale Performance Analysis: It would be beneficial if the authors could include a performance and scalability analysis of SymNCO on larger problem instances which are often encountered in realworld applications.  Diversity of Solutions: Could the authors comment on how SymNCO affects the diversity of the solutions explored and whether it might be limiting the combinatorial space  Computational Efficiency: Further details on the computational resources required for SymNCO compared to both conventional solvers and existing DRLNCO methods would provide a clearer picture of its practical applicability.  4. Limitations and Constructive Suggestions for Improvement The paper does not provide a thorough analysis of the scalability of SymNCO for larger problem instances which may limit its applicability in realworld scenarios. A deeper investigation into the impact of SymNCO on solution diversity and the computational efficiency of the approach would also be beneficial. To address these concerns the authors could consider extending their experimental analysis to include larger problem sizes assessing the solution space diversity through qualitative studies or additional metrics and providing a detailed comparison of computational resources required for SymNCO versus other methods. Additionally exploring the application of SymNCO in nonEuclidean COPs could open new research avenues and practical applications.  Conclusion Overall the paper presents a significant advancement in neural combinatorial optimization by leveraging symmetricities in CO problems. While the demonstrated success of SymNCO is promising addressing the highlighted limitations and suggestions could enhance its contribution to the field further.", "nOw2HiKmvk1": "Review of \"Learning with Biased Committee for Debiased Classifier without Spurious Attribute Label\" 1) Summary: The paper introduces a novel method Learning with Biased Committee (LWBC) to train debiased classifiers without relying on labels for spurious attributes. The core idea revolves around using a committee of biased classifiers to identify and upweight biasconflicting samples\u2014those not exhibiting common spurious correlations present in the majority of training data. This committee operates as a bootstrapped ensemble ensuring diversity and a predominant bias towards guiding samples. Through consensus on prediction difficulty the committee efficiently pinpoints biasconflicting samples assigning them larger weights for the main classifiers training. Distinctively the committee is gradually debiased via knowledge transfer from the main classifier focusing increasingly on challenging samples. Additionally the paper explores the benefits of selfsupervised learning as a means to procure a less biased feature representation. Across five realworld datasets LWBC demonstrates superior performance against existing methods that do not use spurious attribute labels and in some cases even outstrips approaches that do. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty and Efficacy: The paper contributes significantly by introducing a method that robustly identifies and prioritizes biasconflicting samples without necessitating spurious attribute labels a tedious and often impractical requirement. The achieved performance on realworld datasets underscores the methods effectiveness.  Comprehensive Approach: Combining selfsupervised learning for feature extraction with a committee of biased classifiers for targeted debiasing is a comprehensive strategy that addresses both representation and predictionlevel biases.  Detailed Experimental Validation: The extensive experimental setup across varied datasets with both quantitative and qualitative analyses provides a thorough validation of the proposed method. Weaknesses:  Computational Efficiency: While the paper impresses with its methodological advancements it does not provide an explicit discussion on the computational overhead introduced by training multiple classifiers and managing a selfsupervised learning backbone. This aspect is crucial for scalability to larger datasets or more complex models.  Performance Stability: The technique involves randomness in the training process due to the bootstrapped ensemble raising questions about the stability of performance. Although statistically significant improvements are reported fluctuations in performance on certain datasets like CelebA suggest a need for further refinement to ensure consistency.  Limited Discussion on Limitations and Societal Impact: Despite its achievements the paper provides a somewhat cursory treatment of its limitations and potential negative societal impacts. Specifically theres a lack of depth in discussing how the method might inadvertently perpetuate biases not captured in the evaluation datasets. 3) Questions and Suggestions:  Computational Costs: Could the authors elaborate on the computational costs associated with LWBC especially in comparison to singleclassifier methods Suggestions for optimizing computational efficiency would be helpful.  Performance Stability: Given the performance variability particularly on the CelebA dataset could the authors propose strategies to enhance the stability of LWBCs performance across runs with different random seeds  Broader Societal Considerations: The authors are encouraged to delve deeper into the societal implications of their method. Are there biases that LWBC might not effectively counteract How does the method ensure that it does not amplify less prominent biases 4) Limitations and Suggestions for Improvement: The paper could greatly benefit from a more detailed discussion on both computational efficiency and the stability of performance. Addressing these limitations explicitly would make the proposed method more compelling for a wider array of applications. Moreover expanding the conversation around the potential for unintended negative societal impacts and suggesting mechanisms to mitigate such issues could elevate the significance and applicability of the work. Engaging with these aspects more rigorously in future iterations could substantially enhance the papers contribution. Overall the paper presents a significant advancement in the field of debiasing classifiers without relying on spurious attribute labels backed by solid experimental evidence. Addressing the highlighted concerns could further solidify its position and widen its impact.", "tglniD_fn9": "Review for the Manuscript \"Addressing Information Leakage in Concept Bottleneck Models through Autoregressive Predictors and SideChannel Concepts\" 1) Summary of the Main Contributions: The manuscript addresses the problem of information leakage in Concept Bottleneck Models (CBMs) which compromises the models interpretability and intervenability. Specifically it tackles the challenges of insufficient concept sets and inexpressive concept predictors in hard CBMs. The authors propose two novel approaches to mitigate these shortcomings: (i) introducing a sidechannel to learn latent concepts alongside the known concepts enhancing the models predictive power without relying on the information leakage and (ii) employing an autoregressive concept predictor to improve the flexibility of concept prediction and accurately capture concept correlations. The study validates these approaches through experiments on two datasets demonstrating that these modifications allow hard CBMs to achieve comparable predictive performance to soft CBMs significantly improving concept and intervention accuracy while avoiding leakage issues. The contributions are substantial as they enhance the practicality and utility of CBMs in highstakes domains requiring interpretable and transparent AI decisionmaking. 2) Assessment of the Strengths and Weaknesses: Strengths: a) Innovation and Technical Rigor: This work provides an innovative solution to a known problem in CBMs namely information leakage. The introduction of a sidechannel for latent concepts and the use of an autoregressive configuration for the concept predictor are technically sound approaches wellsupported by theoretical reasoning and empirical evidence. b) Relevance and Impact: Addressing leakage in CBMs is crucial for applications in sensitive fields where interpretability and the ability to intervene are paramount. Thus the proposed improvements have the potential for significant impact. c) Comprehensive Evaluation: The manuscript presents a thorough experimental evaluation using both synthetic and realworld datasets to demonstrate the effectiveness and practical utility of the proposed modifications. Weaknesses: a) Complexity and Computational Costs: The introduction of autoregressive models and sidechannels while effective increases the complexity and computational expense of CBMs. This may limit their applicability in scenarios where computational resources are constrained or realtime responses are required. b) Scalability: The scalability of the proposed approaches to very large datasets or concept sets is not thoroughly discussed. Given the added complexity it is uncertain how the approaches fare with significantly larger scales of data. c) Generality of the Proposed Solutions: The paper focuses on addressing leakage in CBMs without exploring whether the presented solutions could generalize to other types of models suffering from similar issues of interpretability and leakage. 3) Questions and Suggestions for the Authors: a) Handling of High Computational Costs: Could the authors suggest ways to mitigate the increased computational costs introduced by their methods For instance are there any simplifications or approximations that could retain most benefits while reducing the required resources b) Exploration of Scalability: It would be beneficial for the authors to discuss or even conduct preliminary experiments on the scalability of their methods especially regarding larger concept sets and datasets. c) Generalization Beyond CBMs: Do the authors believe that their solutions particularly the sidechannel for learning latent concepts and the autoregressive predictor could be adapted for other model architectures facing similar issues If so some discussion on potential adaptations would enrich the manuscript. d) Potential for HumanInterpretable SideChannel Concepts: The authors mention exploring methods to make the sidechannel information humaninterpretable as future work. Can they provide preliminary ideas on how they envision achieving this Could this involve techniques from explainable AI (XAI) for instance 4) Limitations and Suggestions for Improvement: The paper provides a compelling case for its proposed methods but could enrich its discussion on limitations and potential negative societal impacts. Especially the increased complexity and computational demand may not only limit practical applicability but also accessibility for practitioners with limited resources. Moreover while the study primarily addresses the positives of reducing leakage for interpretability and intervenability a more nuanced discussion on any potential tradeoffs or circumstances where the introduction of latent concepts through sidechannels could inadvertently introduce biases or affect the models fairness would be constructive. As a suggestion incorporating considerations around computational efficiency and ethical implications of the introduced concepts could strengthen the papers discussions on limitations and societal impacts.", "ldRyJb_cjXa": " Review of \"Star Temporal Classification (STC): Learning from Partially Labeled Sequential Data\"  1. Summary This paper introduces Star Temporal Classification (STC) a novel algorithm for handling partially labeled and unsegmented sequential data addressing a significant limitation in existing sequential loss functions like Connectionist Temporal Classification (CTC). By incorporating a unique star token within the weighted finitestate transducers (WFST) framework STC can account for any number of missing labels at arbitrary positions in the sequence. The authors implement this approach efficiently demonstrating its application to automatic speech recognition (ASR) and handwriting recognition tasks where it nearly matches the performance of fully supervised models despite the presence of significant label sparsity.  2. Evaluation Strengths:  Innovative Approach: The concept of using a star token to manage varying levels of missing labels is both innovative and elegantly applied offering a fresh perspective on dealing with partially labeled data.  Broad Applicability: The versatility of STC in handling different sequence classification tasks (ASR and handwriting recognition) and its potential applicability to other domains is a strong asset.  Efficient Implementation: The authors\u2019 focus on optimization for practical application particularly in minimizing the training time increase over baseline methods is commendable. Weaknesses:  Generalization to RealWorld Scenarios: While the paper presents promising results in controlled experiments it is somewhat unclear how STC would perform in realworld scenarios with highly imbalanced label sparsity or noisy labels.  Comparison with Existing Methods: Despite the cited related work a more detailed comparison with contemporary methods dealing with partial labels especially in terms of model complexity and runtime efficiency would provide clearer insights into STCs relative advantages.  Transparency in Hyperparameter Tuning: While the method requires additional hyperparameter tuning especially around the token insertion penalty (\\(\\lambda\\)) more detailed guidance or sensitivity analysis regarding these hyperparameters could aid in the adoption of STC.  3. Questions and Suggestions  Could the authors elaborate on the robustness of STC to highly imbalanced partial labeling scenarios or extreme cases where important labels are missing  Considering the mention of extending STC to noisy labels have preliminary experiments been conducted and if so what were the insights or challenges identified  It would be beneficial if the authors could provide a more detailed comparison or quantitative analysis on the efficiency (both in terms of computation and memory usage) of STC compared to the baseline methods and existing alternatives.  A deeper dive into the selection process for hyperparameters particularly for the token insertion penalty would be valuable. Perhaps incorporating adaptive techniques or learning these parameters could also be an area worth exploring.  4. Limitations and Societal Impact The authors briefly address the limitations of STC primarily focusing on the need for hyperparameter tuning and its application boundaries. However more discussion around the potential for model bias or the algorithms behavior in the presence of severely noisy or misleading labels would enrich the discourse. In terms of societal impact while the paper does not explicitly discuss negative implications one could extrapolate that any biases in training data especially in sensitive applications like ASR could be perpetuated or even exacerbated. Suggestions for mitigating such risks could include elaborating on strategies for ensuring diverse and representative training datasets and possibly incorporating mechanisms within the STC framework to identify and mitigate potential biases.", "x8DNliTBSYY": " 1) Summary of the Paper and Its Contributions This paper addresses the conditioning of the Neural Tangent Kernel (NTK) for deep neural networks in the challenging setup of sublinear layer widths where the number of neurons is of order \\xce\\xa9(\\xe2\\x88\\x9aN) and the number of parameters is of order \\xce\\xa9(N) with N being the number of training samples. The authors establish a key technical contribution by proving a lower bound on the smallest NTK eigenvalue for networks under minimum possible overparameterization. This result has significant implications for the memorization capacity and optimization guarantees for deep neural networks trained via gradient descent even when the layer widths are sublinear in the number of samples. The paper leverages a novel 3step centering technique for the Jacobian and utilizes tight estimates on the \\xe2\\x84\\x932 norms of feature vectors and their centered counterparts contributing to the understanding of the behavior of deep networks with minimal overparameterization.  2) Assessment of Strengths and Weaknesses  Strengths:  Theoretical Depth: The paper addresses a highly relevant and challenging question in deep learning and provides strong theoretical guarantees for the conditioning of the NTK in minimally overparameterized networks.  Novel Methodology: The introduction of a 3step centering technique and the provision of tight estimates on \\xe2\\x84\\x932 norms demonstrate an innovative approach to analyzing the NTK.  Practical Implications: The results concerning memorization capacity and optimization guarantees have direct implications for training deep neural networks particularly in resourceconstrained scenarios.  Weaknesses:  Assumption Limitations: The requirement for smooth activation functions and the assumption that layer widths do not significantly increase with depth may limit the applicability of the results to a broader class of network architectures used in practice.  Empirical Validation: The paper provides a comprehensive theoretical framework but lacks empirical experiments to corroborate the theoretical findings with practical outcomes.  Complexity of Analysis: While the depth of analysis is a strength it also makes the paper challenging to parse for readers not deeply versed in the intricacies of NTK and deep learning theory.  3) Questions and Suggestions for the Authors  Empirical Validation: Can the authors provide empirical evidence or simulations that support the theoretical findings particularly how the lower bounds on the smallest NTK eigenvalue affect network training in practical scenarios  Assumption Relaxation: Could the authors discuss the potential impact of relaxing some of the stricter assumptions such as the smoothness of activation functions or the loose pyramidal topology requirement Would the main results still hold under broader conditions  Extension to Other Learning Paradigms: How might the insights from this paper apply to other learning paradigms such as unsupervised learning or reinforcement learning where the role of the NTK might differ  4) Limitations and Societal Impact The authors have addressed the limitations related to the assumptions required for their theoretical analysis but may have overlooked the broader implications of their work especially concerning potential negative societal impacts. For example while optimizing the training of deep networks with minimal overparameterization could lead to more efficient use of computational resources it could also exacerbate issues related to the accessibility of stateoftheart models by entities with limited computational resources.  Suggestions for Improvement:  Discuss Broader Impacts: It would be beneficial for the authors to include a discussion on the broader impacts of their work including ethical considerations and potential for misuse.  Address Applicability Limitations: Providing a roadmap or suggestions for extending the applicability of the results to a wider array of network architectures and learning paradigms would increase the utility of the findings. Overall the paper makes significant theoretical contributions to the understanding of the NTK in deep learning offering insights that could potentially guide the design and training of more efficient neural network architectures.", "xaWO6bAY0xM": " Review of \"Certified Robustness Against Adversarial Examples with SortNet: A Lipschitz Network Approach\"  Summary This paper addresses the challenge of designing neural networks with bounded Lipschitz constant to obtain classifiers robust against adversarial examples especially under the \\ell\\infty perturbation setting. The authors present two main contributions: 1. Theoretical Insights: They first identify fundamental limitations of standard Lipschitz networks in capturing symmetric Boolean functions (SBF) showing that the expressive power of such networks significantly diminishes as the input dimension increases. This limitation extends to the inability of these networks to approximate certain classes of Lipschitz functions particularly order statistics within an acceptable error margin. Notably the paper explores how advanced architectures like GroupSort and \\ell\\inftydistance nets circumvent these issues by incorporating order statistics within neuron designs directly. 2. SortNet Architecture: Leveraging the insights from their theoretical analysis the authors propose SortNet a novel Lipschitz network framework that generalizes and outperforms previous approaches by efficiently integrating order statistics into its architecture. They demonstrate the effectiveness of SortNet through extensive experiments showcasing superior certified robustness across several benchmark datasets when compared to prior works.  Assessment Strengths:  Theoretical Depth: The paper offers a rigorous theoretical analysis significantly contributing to our understanding of the expressive limitations of standard Lipschitz networks and providing a solid basis for the proposed SortNet architecture.  Practical Impact: By providing a network design that exhibits both enhanced robustness and efficiency in training the paper addresses a crucial problem in the realm of adversarial machine learning with implications for developing more secure AI systems.  Experimental Validation: The authors conduct thorough empirical evaluations that not only validate their theoretical claims but also underscore the practical advantages of SortNet over existing methods. Weaknesses:  Scope of Robustness: The focus is primarily on \\ell\\infty robustness which while critical represents a specific aspect of adversarial robustness. The generalizability of the findings to other forms of perturbations or attack models remains an open question.  Complexity and Scalability: While the paper demonstrates SortNets scalability through experiments on datasets up to ImageNet size the architectural complexity and computational requirements for largerscale applications or higherdimensional data are not fully addressed.  Questions and Suggestions  Generalization: Can the authors comment on the potential applicability of their insights to combat other forms of adversarial perturbations especially in contexts beyond image classification  Extension to Convolutional Architectures: SortNet is presented primarily in the context of fully connected layers. An exploration into how these concepts can be adapted or extended to convolutional settings which are more common in imagerelated tasks could be valuable.  Computational Efficiency: It would be beneficial for the authors to discuss strategies to further optimize the computational efficiency of SortNet particularly for deep networks or when dealing with largescale datasets.  Limitations and Societal Impact The authors have not fully explored the broader implications of their work especially regarding computational efficiency and potential negative uses of robust neural networks. While discussing the advancements in adversarial robustness it is also critical to consider the ethical implications including the potential for misuse in deceptive practices or evasion of detection systems. Constructive suggestions include:  Expanding the theoretical analysis to cover a broader range of adversarial scenarios thereby ensuring that the benefits of robust classification are not narrowly focused on specific norms or datasets.  Further research into reducing the computational overhead of SortNet making it more accessible for realworld applications where resources are constrained. Overall this paper represents a substantial advancement in the field of adversarial machine learning offering both novel theoretical insights and a practical framework for achieving certified neural network robustness.", "owDcdLGgEm": "This paper presents an innovative approach to the cryoelectron microscopy (CryoEM) images 3D reconstruction problem by estimating their poses under the group synchronization framework. By exploiting the symmetries specific to cryoEM the authors argue that O(2) relative poses offer sufficient information for pose estimation which is a significant step forward from the existing methods that use SO(2) relative poses. They improve upon the multifrequency vector diffusion maps (MFVDM) method allowing for the use of all raw images directly in the 3D reconstruction process without losing resolution through clustering and averaging. Their method is validated using synthetic data and a theoretical framework is provided suggesting robust noise stability and recovery promises.  Strengths: 1. Novel Approach: The use of O(2) relative poses for pose estimation in cryoEM is a significant innovation that addresses the limitations of current methods relying on SO(2). This could potentially change how initial pose estimations are performed in the field. 2. Comprehensive Validation: The authors validate their method extensively using synthetic data showing its robustness and effectiveness in pose recovery. This is complemented by a theoretical discussion that backs the practical findings. 3. Enhancements to Existing Methods: The proposed modification to the MFVDM method harnesses all available data for 3D reconstruction potentially increasing the methods utility and accuracy in realworld applications. 4. Detailed Methodology: The stepbystep breakdown of the proposed method including the mathematical foundations and algorithmic implementations provides clarity and could serve as a solid basis for reproducibility and further research.  Weaknesses: 1. Assumption of Uniform Distribution: The methods assumption of uniformly distributed poses might not hold in reallife scenarios potentially limiting its applicability across different cryoEM datasets. 2. Computational Complexity: While the authors discuss the complexity of their method the practical computational costs and scalability to large datasets remain a concern that could benefit from a deeper analysis. 3. Limited RealWorld Testing: The primary validation is carried out on synthetic datasets. The performance in realworld scenarios especially with highly heterogeneous datasets common in cryoEM is not thoroughly tested.  Questions and Suggestions for the Authors: 1. Impact of NonUniform Distributions: How robust is the proposed method to nonuniform distributions of poses which are typical in real datasets Would incorporating renormalization techniques suffice to mitigate potential biases 2. Handling High Computational Costs: Could the authors suggest practical approaches to manage the high computational demands especially when dealing with large datasets without significantly compromising the methods effectiveness 3. Extension to RealWorld Datasets: Are there planned or ongoing efforts to apply and validate the method on realworld cryoEM datasets Insights from such applications could be invaluable for understanding the methods practical utility and limitations. 4. Addressing the Global O(3) Ambiguity: The paper mentions the intrinsic global O(3) ambiguity in cryoEM. Could the authors expand on potential strategies to identify or work around this issue especially in the context of their proposed method  Limitations and Societal Impact: The discussion on limitations acknowledges the assumption of uniform pose distribution and the challenge of high noise levels. Offering strategies like incorporating renormalization techniques or leveraging steerable PCA for dealing with nonuniform distributions and high noise would strengthen the paper. On potential negative societal impact the paper does not delve into this aspect which is understandable given the technical nature of the work. However elucidating any indirect consequences such as computational resource demands could provide a more comprehensive view. Constructively enhancing the method\u2019s efficiency and exploring its applicability in diverse biological investigations could amplify its positive impact on society by accelerating scientific discoveries. In conclusion the paper contributes a significant methodological advancement to the field of cryoEM by addressing a critical step in the 3D reconstruction process. Addressing the outlined questions and suggestions could further enhance its contribution and applicability in realworld settings.", "lIeuKiTZsLY": " Peer Review:  1) Summary of the Paper: This paper addresses a significant challenge in causal discovery particularly dealing with latent variables that form a complex hierarchical structure. Traditional causal discovery algorithms often assume there are no latent confounders an assumption that hardly holds in realworld scenarios. The authors propose a novel method focusing on a more challenging scenario where latent hierarchical causal models prevail including situations where latent variables influence both measured and unmeasured variables in a hierarchical fashion. The primary innovation lies in developing an efficient algorithm leveraging rank deficiency constraints over measured variables. This enables the identification of latent variables their cardinalities and the hierarchical structure they form. The algorithm is theoretically grounded with asymptotic guarantees provided under specified graph structure restrictions. Empirical validation on synthetic datasets demonstrates the efficacy of the proposed method which outperforms existing approaches in identifying causal structures involving latent variables.  2) Assessment of Strengths and Weaknesses: Strengths:  Novelty and Importance: The paper tackles a crucial problem in causal discovery enhancing the understanding and identification of complex latent structures in data.  Theoretical Contribution: Provides strong theoretical underpinning for the proposed method including identifiability conditions and asymptotic guarantees.  Practical Utility: Through empirical validation the method shows strong performance in uncovering latent hierarchical structures which holds practical significance in numerous domains requiring causal inference.  Comparison with Existing Methods: The comparison with existing methods like CLRG FOFC and GIN highlights the superiority of the proposed approach in various settings. Weaknesses:  Assumptions on Graph Structure: While the authors address a more complex scenario of latent variables the effectiveness of the method might be constrained by the assumptions made on the graph structure.  Scalability Concerns: The paper lacks a thorough discussion on the scalability of the proposed method especially for extremely large graphs encountered in practical scenarios.  Limited Validation: The empirical validation while promising is conducted only on synthetic datasets. The performance in realworld scenarios remains untested.  Potential for Overfitting: With complex models and algorithms theres an implicit risk of overfitting which isnt adequately addressed in the manuscript.  3) Questions and Suggestions for the Authors:  Scalability: Could the authors comment on the scalability of their proposed method for largescale problems Are there any practical limits to the size of the graph structure it can effectively handle  Realworld Applications: It would be beneficial if the authors could provide insights or preliminary results on applying their method to realworld datasets. How does the algorithm perform in situations where the theoretical assumptions might not hold strictly  Generalization to Nonlinear Models: The paper focuses on linear models. How feasible is it to extend the proposed methodology to nonlinear causal relationships  Overfitting Mitigation: Can the authors elaborate on potential overfitting issues and discuss strategies to mitigate this risk particularly in complex latent structures  4) Limitations and Societal Impact: The authors have made a commendable effort in identifying the limitations of their work particularly concerning the assumptions on graph structure and reliance on linear models. Expanding the methods capability to accommodate nonlinear relationships and further stresstesting it on realworld data would be valuable next steps. Regarding societal impact while the paper doesnt delve into specific negative implications its crucial to note that causal discovery tools can have profound implications especially when used in sensitive areas like healthcare or criminal justice. Ensuring ethical use and transparency about the assumptions and limitations of such algorithms is vital. Constructive Suggestions for Improvement:  Expanding Theoretical Framework: Consider extending the theoretical framework to include nonlinear causal models providing a more comprehensive tool for causal discovery.  Enhancement of Empirical Validation: Augment the empirical validation with realworld datasets offering insights into the algorithms practical effectiveness and limitations.  Discussion on Societal Impact: Include a section discussing the potential societal impact ethical considerations and areas where careful application of the proposed method is warranted.", "wiHzQWwg3l": "The paper titled \"Extrapolative ContinuousTime Bayesian Neural Networks for Unsupervised Domain Adaptation in NonStationary Streaming Data\" presents an innovative approach to handle the challenge of lowlatency testtime adaptation in realtime streaming applications through the lens of unsupervised domain adaptation (UDA). The authors propose a novel method extrapolative continuoustime Bayesian neural network (ECBNN) which leverages internal predictive modeling inspired by human cognitive abilities for processing nonstationary streaming data. ECBNN is formulated within a stochastic dynamical system framework integrating a continuoustime Bayesian filtering problem for dynamic adaptation of model parameters. The method outperforms stateoftheart UDA techniques by achieving low latency trainingfree testtime adaptation and gradually improving model alignment and performance over time. Strengths: 1. Innovative Approach: The incorporation of continuoustime Bayesian filtering into UDA for realtime applications is novel and addresses a significant challenge in the domain of streaming data processing. 2. Theoretical Foundation: The paper provides a sound theoretical basis for the proposed ECBNN model including the derivation of a particlefilterbased differential equation (PFDE) for timeevolving neural network parameters. 3. Empirical Results: The empirical studies demonstrate the effectiveness of ECBNN on both synthetic and realworld datasets showcasing its superiority in latency reduction and gradual performance improvement over conventional UDA methods. 4. Potential Impact: The methods ability to adapt in realtime without the need for retraining or finetuning on incoming streaming data has significant implications for various realworld applications requiring immediate response such as autonomous driving and realtime surveillance. Weaknesses: 1. Complexity and Scalability: The adoption of particle filters and differential equations could introduce computational complexity potentially impacting the scalability of ECBNN to very largescale streaming datasets. 2. Model Generalizability: While the proposed method shows promising results on selected datasets the paper lacks a comprehensive evaluation across diverse domains to fully ascertain the generalizability of ECBNN. 3. Parameter Sensitivity: With multiple components integrated into the ECBNN framework including PFDE and surrogate neural networks for solving differential equations the sensitivity of the model performance to the tuning of various parameters remains unclear. Questions and Suggestions: 1. Scalability Concerns: Can the authors elaborate on the computational complexity of ECBNN especially in the context of very largescale streaming data and suggest potential optimization strategies 2. Generalizability Testing: It would be beneficial if the authors could test ECBNNs adaptability and performance across a wider variety of realworld streaming datasets from different domains to strengthen the validation of the models applicability. 3. Parameter Sensitivity Analysis: A detailed analysis of the sensitivity of the models performance to different parameter settings including the choice of importance weights and the number of particles would provide better insights into the robustness of the model. Limitations and Societal Impact: The authors have provided a thoughtful examination of ECBNNs performance and theoretical contributions however a more detailed discussion on the limitations regarding computational demands and parameter tuning is warranted. Furthermore the societal implications of deploying ECBNN in realtime streaming applications particularly concerning privacy and ethical considerations in sensitive domains like surveillance and autonomous vehicles need to be addressed. Suggestions for improvement include a thorough exploration of model optimization techniques to enhance scalability and the development of guidelines or frameworks for responsible deployment of ECBNN to mitigate potential negative societal impacts.", "nVV6S2sb_UL": " Review of \"Secure Aggregation with MultiRound Privacy for Federated Learning\"  1) Summary The paper addresses a critical challenge in Federated Learning (FL) concerning the longterm privacy of user models across multiple training rounds. Current secure aggregation protocols safeguard user privacy in a single training round but fall short in scenarios involving multiple rounds especially considering partial user participation. This can lead to potential privacy breach risks where aggregated models from successive rounds could be exploited to reconstruct individual user models. The authors propose a structured user selection strategy within a novel framework MultiRoundSecAgg to enhance longterm privacy while also considering fairness and participation rates across rounds. They introduce a new metric for quantifying multiround privacy guarantees in FL and present both theoretical analysis and empirical evidence demonstrating the effectiveness of their approach on popular datasets like MNIST CIFAR10 and CIFAR100 under varying data distribution settings (IID and nonIID).  2) Assessment Strengths:  Significance and Novelty: The paper tackles an underexplored but significant problem in FL concerning multiround privacy. The introduction of a new metric for longterm privacy and a structured user selection strategy is both novel and crucial for advancing secure FL practices.  Theoretical and Practical Contributions: The authors not only propose a theoretical framework with MultiRoundSecAgg but also validate its effectiveness through extensive experiments demonstrating improvements over existing methods in terms of privacy and test accuracy.  Clarity and Depth of Analysis: The paper provides a clear and detailed explanation of the problem methodology theoretical analysis and empirical evaluations. The comparisons with baseline schemes and the discussion on the tradeoffs between privacy fairness and participation rates are insightful. Weaknesses:  Assumptions and Realworld Applicability: Some assumptions (like the availability of users and their dropout rates) and the batch partitioning approach might limit the realworld applicability or scalability of MultiRoundSecAgg. Further clarification on handling dynamic user populations and varying network conditions would enhance the paper.  Limitation Discussion and Societal Impacts: While the paper mentions potential negative societal impacts briefly a more detailed discussion on limitations and how the proposed method could be targeted or misused would strengthen the work. Including thoughts on mitigation strategies for potential negative uses would also be valuable.  3) Questions and Suggestions  Dynamic User Participation: How does MultiRoundSecAgg handle dynamically changing user sets especially in largescale FL settings where new users might join while others leave  Scalability Concerns: Could the authors elaborate on the scalability of their approach particularly regarding the computation and communication overhead involved in applying their user selection strategy across potentially thousands of users  Exploration of Nonlinear Reconstructions: While the paper mentions nonlinear reconstructions briefly could the authors provide insights or preliminary results on how their approach fares against such attacks  4) Limitations and Suggestions for Improvement The authors have adequately addressed many challenges related to longterm privacy in FL. However the discussion on realworld applicability scalability and a thorough exploration of potential negative societal impacts could be expanded. To mitigate these limitations the authors might consider:  Including case studies or scenarios that simulate realworld dynamics of user participation and network conditions.  Offering strategies or extensions to the proposed framework that could tackle the potential exponential increase in computational and communication overhead with large participant sets.  Providing a detailed ethical discussion on the use of secure aggregation especially considering scenarios where FL is employed for sensitive or critical applications and suggesting comprehensive guidelines or safeguards against misuse.", "tTWCQrgjuM": " Peer Review of \"Differentially Private Bayesian Inference via Data Augmentation MCMC\"  1) Summary The paper presents a novel Markov chain Monte Carlo (MCMC) framework for conducting Bayesian inference on privatized data addressing the challenges posed by differentially private mechanisms. These mechanisms introduce randomness into data to preserve privacy complicating statistical inference by making the likelihood function of privatized data intractable. The authors propose an MCMC algorithm that performs Bayesian inference by augmenting the model parameters with unobserved confidential data and updating each alternately. This method is applicable across a broad spectrum of statistical models and privacy mechanisms providing a generalpurpose solution without necessitating further algorithm design or tuning. The effectiveness of the approach is demonstrated through applications to a na\u00efveBayes loglinear model and a linear regression model underscoring its potential to maintain statistical validity in analyses involving privatized data.  2) Assessment  Strengths:  Novelty and Generality: The framework introduces a new way of conducting Bayesian inference on privatized data applicable to a wide array of statistical models and privacy mechanisms. Its generalpurpose nature positions it as a significant contribution to the field.  Theoretical Foundation: The authors provide a solid theoretical basis for their MCMC algorithm including computational complexity acceptance rate mixing properties and ergodicity conditions which strengthen the reliability of the proposed method.  Practical Utility: By demonstrating the frameworks applicability to both na\u00efveBayes loglinear models and linear regression the paper showcases the practical benefits and versatility of the proposed approach.  Weaknesses:  Assumption Specificity: The effectiveness of the algorithm relies on certain assumptions (e.g. record additivity and the availability of an existing MCMC algorithm for nonprivatized data) which may not hold in all practical scenarios. This limitation could affect the frameworks applicability in more complex data environments.  Potential computational efficiency concerns: While the authors argue for the computational efficiency of their algorithm practical implementation challenges such as highdimensional data settings and the choice of an efficient sampler for \\(p(\\theta  x)\\) are not thoroughly discussed.  Evaluation Scope: The papers evaluations mainly focus on demonstrating the applicability of the proposed framework rather than offering a comprehensive comparison with existing methods. An empirical comparison regarding computational efficiency convergence and inference accuracy would have been beneficial.  3) Questions and Suggestions  Q1: Could the authors provide more insights into how the proposed framework might perform in highdimensional data settings especially where the efficiency of existing samplers for \\(p(\\theta  x)\\) might be a concern Suggestion: If possible include an evaluation section comparing the proposed methods performance with existing approaches in highdimensional settings to highlight its strengths and limitations.  Q2: The paper assumes the privacy mechanism \\(\\eta\\) is known and can be evaluated. How robust is the proposed algorithm to inaccuracies or uncertainties in the estimation of these mechanisms Suggestion: An analysis or discussion section focusing on the robustness of the algorithm to inaccuracies in the privacy mechanism could enhance the papers practical significance.  Q3: Have the authors considered the implications of their method on the privacyutility tradeoff especially in terms of the potential increase in inferential uncertainty as privacy guarantees strengthen Suggestion: Including a discussion on the privacyutility tradeoff and how the proposed method navigates this balance could be informative for practitioners.  4) Limitations and Societal Impact The authors have laid down a foundational framework for conducting Bayesian inference on privatized data though certain assumptions may limit its broad applicability. Addressing these assumptions in future work will be crucial. Moreover the societal implications of enhanced analytical capabilities in the context of privatized data warrant careful consideration. While the method promotes privacypreserving data analysis discussions around the potential for misuse and the ethical considerations in deploying such technologies are surprisingly absent. Constructive suggestions include incorporating ethical guidelines and considerations into the frameworks use and further research into safeguarding against potential abuses. In summary the paper introduces a promising approach for Bayesian inference in the context of privacypreserving data analysis but would benefit from a deeper exploration of its limitations broader applicability and societal impacts.", "rUb6iKYrgXQ": " Peer Review for \"Conditional Robust Optimization: A Datadriven Approach to Uncertainty\"  Summary This paper introduces a novel framework that integrates datadriven decisionmaking under uncertainty with contextual information using Conditional Robust Optimization (CRO). This approach differs from traditional methods by learning datadriven uncertainty sets that adapt based on side information thereby personalizing the robust optimization problem. The paper makes significant contributions by proposing a novel framework for creating these uncertainty sets using deep learning techniques for clustering and classification establishing theoretical guarantees for the solutions provided by this model and empirically demonstrating the benefits of this approach through a simulated data study and a realworld portfolio optimization example.  Assessment of Strengths and Weaknesses Strengths: 1. Novel Approach: The integration of contextual information in designing uncertainty sets for robust optimization is a novel and significant contribution to the field. This approach can significantly enhance decisionmaking under uncertainty. 2. Theoretical Guarantees: The paper provides theoretical guarantees for the coverage provided by conditional uncertainty sets and for the valueatrisk performances which is vital for establishing the reliability of the proposed model. 3. Empirical Validation: Using both simulated and realworld data for empirical validation provides a comprehensive demonstration of the models scalability and practical utility in various application domains. Weaknesses: 1. Complex Implementation: The framework\u2019s reliance on deep learning for clustering and classification may make the implementation complex and computationally expensive limiting its accessibility for practitioners with limited resources. 2. Potential Overfitting: The methods performance heavily relies on the historical datas quality and the selected deep learning architectures which may lead to overfitting or biased uncertainty sets if not carefully managed. 3. Limited Exploration of Alternative Methods: While the paper mentions related work in stochastic programming and robust optimization it lacks a detailed comparison with alternative contemporary methods that also aim to integrate contextual information into decisionmaking under uncertainty.  Questions and Suggestions for the Authors 1. Scalability Concerns: Can the authors elaborate on the computational scalability of the proposed framework especially in scenarios with a large number of covariates and data points Suggestions for optimizing computational efficiency would be valuable. 2. Generalization to Different Domains: How generalizable is the proposed method to domains outside of portfolio optimization A discussion on the potential adaptability of the approach to other decisionmaking problems would enhance the papers impact. 3. Fairness and Bias: The authors mention the potential for fairness considerations in the conclusion. Can the authors discuss preliminary thoughts on how fairness constraints might be integrated into the framework without significantly compromising the optimization objectives  Limitations and Societal Impact The paper briefly mentions potential negative societal impacts related to fairness but does not delve into a comprehensive discussion of limitations or the broader societal implications. It is crucial especially in datadriven models to consider and address potential biases in the data that could lead to unfair or discriminatory outcomes. The authors should:  Explicitly discuss potential biases in covariate information and their implications for the models decisions.  Propose methodologies to identify and mitigate such biases possibly integrating fairness criteria into the loss function or the clusteringclassification schemes.  Consider the ethical implications of decisionmaking in uncertain environments especially in critical domains like healthcare or finance and suggest guidelines for responsible model deployment. Improvement in these areas will not only enhance the papers quality but also promote ethical practices in implementing datadriven optimization models.", "lDohSFOHr0": " Peer Review  1) Brief summary of the paper and its contributions: This paper addresses a notable challenge in semisupervised learning (SSL)  the classification of unseen classes alongside seen classes when not all classes in the training data are labeled. The authors introduce a novel SSL approach named NACH that incorporates two innovative modules: unseen class classification and learning pace synchronization. The unseen class classification module utilizes pairwise similarity to identify and classify similar example pairs into the same class employing a novel loss function and a similaritybased filter to enhance classification accuracy. The learning pace synchronization module introduces an adaptive threshold with distribution alignment to harmonize the learning progress between seen and unseen classes aiming to improve overall classification performance. The proposed method demonstrates substantial improvements over existing SSL and novel class discovery (NCD) methods through extensive experiments on three benchmark datasets: CIFAR10 CIFAR100 and ImageNet100.  2) Assessment of the strengths and weaknesses: Strengths:  The paper tackles a significant and underexplored problem in SSL providing a novel perspective on handling unseen classes in labeled data.  The proposed methodology particularly the unseen class classification and learning pace synchronization is innovative and welljustified through empirical results.  Extensive experiments including comparisons with stateoftheart SSL and NCD methods showcase the effectiveness of the approach. Weaknesses:  While the experimental results are robust there appears to be a gap in the discussion of the theoretical underpinnings of the proposed methods especially in terms of generalization ability and scalability.  The paper does not provide a detailed analysis of the failure cases or when the method may not work as intended which could give a more balanced view of its applicability.  The broader impact section lacks a discussion on potential negative societal implications. Given the importance of classification tasks its critical to contemplate the repercussions if the models assumptions do not hold in realworld scenarios.  3) Questions and suggestions for the authors:  Could the authors elaborate on the generalizability of the NACH method to datasets with significantly different characteristics such as text or tabular data  It would be beneficial if the authors could discuss any potential scalability issues especially in relation to large datasets with a vast number of unseen classes.  A sensitivity analysis of the hyperparameters (\u03bb1 \u03bb2 \u03b2) in realworld conditions could provide insights into how tuning these parameters affects performance across different datasets.  The methodology section could benefit from a clearer explanation or a dedicated subsection on the choice and impact of different data augmentations on the learning process.  Limitations and potential negative societal impact: The paper marginally addresses the limitations in its conclusion indicating the lack of theoretical guarantees as a future research direction. However a more thorough discussion on the method\u2019s limitations including potential biases arising from incorrect pairings or misclassifications would strengthen the paper. Consequences of such biases in sensitive applications such as healthcare or criminal justice where misclassification of unseen classes could have severe implications should be acknowledged. To mitigate these issues the authors could consider incorporating mechanisms to identify and handle highrisk misclassifications proactively.  Suggestions for improvement:  Provide a more indepth theoretical analysis or justification for the choice of methods and hyperparameter settings.  Expand the discussion on the potential limitations and explore the ethical considerations and negative societal impacts more thoroughly.  Consider including case studies or additional experiments that demonstrate the method\u2019s applicability to a broader set of realworld problems.", "nOdfIbo3A-F": " PeerReview of \"Exploiting Topology for Learning the Dynamics of Articulated Rigid Bodies with a Lagrangian Graph Neural Network\"  1) Summary of the Paper The paper introduces a model termed the Lagrangian Graph Neural Network (LGNN) designed to simulate the dynamics of articulated rigid bodies (such as ropes chains and trusses) by leveraging their structure (topology). It capitalizes on the Lagrangian mechanics framework incorporating these principles into a graph neural network (GNN) architecture to directly learn the dynamics from observed trajectories. The authors demonstrate that LGNN can generalize to bodies of arbitrary size and complex unseen topologies. Furthermore they illustrate the models capability to simulate dense structures like tensegrity structures with varying lengths and boundary conditions. Remarkably LGNN can infer the nontrivial nature of mass matrices showcasing adaptability to different system topologies.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The integration of Lagrangian mechanics with GNNs to model the dynamics of complex articulated rigid bodies is a groundbreaking approach that expands the applicability of physicsinformed neural networks.  Generalizability: LGNNs ability to generalize across different sizes and complex unseen topologies is a significant advantage over existing models which often require specific training for each new system size or configuration.  Empirical Evaluation: Comprehensive experiments and comparisons with baseline models (GNS LGN CLNN) provide compelling evidence of LGNNs superior performance in simulating rigid body dynamics accurately. Weaknesses:  Complexity and Computational Overhead: The intricate architecture involving decoupling of kinetic and potential energy computations while effective might introduce considerable computational overhead especially for very large and complex systems.  Assumption of Known Constraints: The authors assume the known constraints within systems. In realworld scenarios constraints might not be easily discernible or may vary dynamically potentially limiting the practical applicability of LGNN.  3) Questions and Suggestions for Authors  Model Interpretability: Could the authors comment on the interpretability of the LGNN model particularly how it determines the significance of different parts of the topology in predicting dynamics  Extension to Deformable Bodies: Given the focus on articulated rigid bodies is there a pathway for extending LGNN to also account for deformable bodies which introduce additional complexities  Handling Dynamic Constraints: The model assumes known static constraints. How could the LGNN framework be adapted to deal with dynamically changing constraints especially in realtime applications  4) Limitations and Constructive Suggestions While the LGNN demonstrates impressive capabilities its assumption of known constraints may not always hold in practical contexts. Also the current discussion on the model\u2019s limitations in handling deformable bodies or dynamically changing constraints is insufficient. Suggested improvements include:  Dynamic Constraint Learning: Future versions should incorporate mechanisms for learning and adapting to dynamic constraints within the system perhaps by integrating additional learning components that can update the models understanding of constraints in realtime.  Extending to Deformable Bodies: Investigating integration with techniques from deformable body dynamics could widen the application range of LGNN making it viable for a broader class of physical simulations.  Societal Impact: The authors could enhance the discussion on potential negative societal impacts of their work considering the implications of simulation errors in critical applications (e.g. civil engineering structures robotics) and strategies to mitigate these risks. In conclusion the LGNN model represents a significant advancement in the simulation of articulated rigid bodies with its novel integration of Lagrangian mechanics and GNNs. Addressing its limitations related to dynamic constraints computational efficiency and extending its applicability to deformable bodies could further cement its utility in various scientific and engineering domains.", "pCrB8orUkSq": " Peer Review  1) Brief Summary of the Paper and Its Contributions The paper scrutinizes the recent advancements in dynamic view synthesis (DVS) from monocular video identifying a significant discrepancy between practical capture scenarios and existing experimental protocols. The authors argue that current methodologies inadvertently introduce multiview signals during training which do not reflect the constraints of monocular video capture accurately. They introduce the concept of effective multiview factors (EMFs) to quantify the level of multiview information present based on relative camerascene motion. Furthermore the paper proposes a novel experimental protocol incorporating two innovative metrics: covisibility masked image metrics and correspondence accuracy aiming to address the limitations of current evaluation practices. A distinctive contribution is the introduction of a new iPhone dataset focusing on diverse reallife deformation sequences. The evaluation using the proposed metrics and dataset reveals significant performance drops in stateoftheart approaches when multiview cues are absent or in complex motion scenarios suggesting considerable room for improvement in DVS methods.  2) Assessment of Strengths and Weaknesses Strengths:  Novel Evaluation Metrics: The introduction of covisibility masked image metrics and correspondence accuracy presents a significant advancement in evaluating DVS models offering a more accurate assessment of their performance in practical scenarios.  Comprehensive Analysis: The paper provides a thorough analysis of existing datasets and evaluation protocols highlighting their limitations and the importance of considering effective multiview cues in dynamic synthesis.  New Dataset Contribution: The introduction of a new iPhone dataset enriches the research field with more challenging and diverse reallife sequences facilitating future works in DVS.  Insightful Findings: The empirical studies offer valuable insights into the impact of multiview cues on DVS performance challenging the current stateoftheart and suggesting directions for future research. Weaknesses:  Limited Discussion on Computational Efficiency: The paper does not discuss the computational efficiency of the proposed metrics and whether their implementation poses any constraints on the evaluation process.  Potential Generalization Concerns: While the new iPhone dataset is a commendable addition theres limited discussion on how representative this dataset is across various dynamic scenarios possibly affecting the generalizability of the findings.  3) Questions and Suggestions for the Authors  Generalization to Other Scenarios: Can the authors elaborate on how their findings might generalize to different dynamic synthesis scenarios outside the ones covered in the iPhone dataset  Impact of Computational Efficiency: It would be beneficial if the authors could discuss any computational implications of their proposed metrics on the evaluation process. Are there any scalability issues when applying these metrics to larger datasets or more complex models  Future Directions: Given the significant performance drops observed what specific aspects of DVS models do the authors believe should be the focus of future research Are there any immediate steps that could be taken to address the highlighted limitations  Limitations and Societal Impact The authors have made a commendable effort in addressing the limitations of current experimental protocols in DVS. However a broader discussion on the potential negative societal impacts of their work such as privacy concerns associated with the improved realism in synthesized views would enhance the paper. It is suggested that future work could include a section discussing ethical considerations especially when deploying these technologies in realworld applications. Constructively the paper could benefit from exploring mitigation strategies for any possible adverse impacts ensuring responsible use and development of DVS technologies.", "o3HXEEBKnD": " Review of \"Solving Single Positive MultiLabel Learning with LabelAware Global Consistency Regularization\"  Summary This paper addresses the challenge in Single Positive MultiLabel Learning (SPML) a scenario where each training instance is tagged with only one of its multiple positive labels while the status (positive or negative) of the rest remains unknown. The paper critiques previous approaches that simplify the problem by treating unobserved labels as negative and propose a novel LabelAware Global Consistency (LAC) regularization technique. The LAC aims to enhance the discovery of potential positive labels by leveraging manifold structure information. This is achieved through a dual focus: First pseudolabeling based on prediction probability followed by consistency regularization to balance the models fit on identified labels and exploration of potential positives. The second focuses on maintaining global consistency among labelwise embeddings to encourage the learning of more distinctive representations. The approach is rigorously tested across multiple benchmark datasets showing significant improvements over stateoftheart methods in SPML tasks.  Strengths 1. Novel Approach: The introduction of LabelAware Global Consistency (LAC) regularization is a novel approach that significantly differs from previous methods relying on the assumption of unobserved labels being negative. It innovatively uses manifold structure information to identify potential positive labels more effectively. 2. Comprehensive Evaluation: The method undergoes extensive evaluation on multiple benchmark datasets demonstrating its effectiveness compared to stateoftheart approaches. The rigorous experimental setup and detailed analysis strengthen the papers contributions. 3. Practical Implication: The proposed method addresses a realworld problem in a costeffective manner by reducing the need for exhaustive multilabel annotations potentially benefiting a wide array of applications such as image annotation and scene understanding.  Weaknesses 1. Complexity and Scalability: While the method shows promising results the paper does not thoroughly discuss the computational complexity or scalability of the proposed approach. Understanding how the method performs with significantly large datasets or label spaces would be beneficial. 2. Hyperparameter Sensitivity: The paper explores parameter sensitivity but could provide more indepth analysis regarding the robustness of the model against the wide range of hyperparameters especially in realworld scenarios where optimal tuning might not always be feasible. 3. Generalization: The paper could further discuss the generalizability of the proposed method to other forms of weaklysupervised learning or even fullysupervised tasks. Insights into how elements of LAC could be adapted or applied in other contexts would be valuable.  Questions and Suggestions 1. Computational Complexity: Can the authors provide more details on the computational complexity of the proposed method especially in comparison with traditional SPML approaches 2. Hyperparameter Tuning: While the paper presents a parameter sensitivity analysis could the authors discuss strategies to optimize hyperparameter tuning in practical applications where validation sets might be limited 3. Further Improvements: Given the significance of manifold structures in the proposed method could integrating more advanced manifold learning techniques further enhance the models performance 4. Generalization and Adaptability: How adaptable is the LAC regularization technique to other forms of learning tasks outside SPML Could it potentially benefit fully supervised multilabel learning scenarios  Limitations and Societal Impact The paper does not thoroughly discuss the limitations related to computational demands which could be significant given the dualphase training and additional regularization terms. Furthermore the societal impact especially the potential for biased outcomes when applied to diverse datasets with imbalanced label distributions is not addressed. Constructive suggestions include a deeper exploration of algorithm efficiency and strategies to mitigate potential biases to ensure the broader applicability and fairness of the proposed method.", "xdZs1kf-va": " 1) Briefly summarize the paper and its contributions The paper addresses the challenge of nonstationarity in fully decentralized multiagent reinforcement learning (MARL) where agents learn independently without access to global information such as the actions of other agents. To overcome the limitation of independent Qlearning which suffers from nonguaranteed convergence due to the nonstationary environment caused by other agents updating their policies the authors propose a novel variant I2Q (Ideal Independent Qlearning). This approach introduces the concept of stationary ideal transition probabilities induced by the optimal conditional joint policy of other agents. By performing Qlearning on these ideal transition probabilities modeled in a fully decentralized manner I2Q enables agents to converge to the optimal joint policy. The paper presents a theoretical foundation for the proposed method and demonstrates its effectiveness across various cooperative multiagent tasks outperforming conventional approaches in both deterministic and stochastic environments.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The introduction of stationary ideal transition probabilities as a foundation to address nonstationarity in decentralized MARL is both novel and intellectually stimulating.  Theoretical Foundation: The paper provides a strong theoretical basis for the proposed method including proofs of convergence to the optimal policy under certain conditions.  Empirical Validation: Demonstrating the effectiveness of I2Q across a wide range of multiagent tasks including both fully and partially observable environments adds significant credibility to the method.  Practical Relevance: By overcoming the challenge of nonstationarity without requiring global information I2Q is relevant for realworld applications where such information is not available or is expensive to obtain. Weaknesses:  Assumption of Determinism: While the method works in stochastic environments the theoretical guarantees are based on deterministic settings. This may limit the applicability or robustness of I2Q in highly stochastic realworld scenarios.  Scalability and Complexity: The scalability of the method as the number of agents increases or in highly complex environments is not thoroughly explored raising questions about practical utility in largescale applications.  Comparison with StateoftheArt: The comparison mainly focuses on traditional independent Qlearning variants and a more extensive comparison with other stateoftheart decentralized MARL methods could strengthen the case for I2Q.  3) Please list up and carefully describe any questions and suggestions for the authors Questions: 1. How does I2Q perform as the number of agents or the complexity of the environment increases significantly 2. Could the authors provide more insight into the impact of the assumptions of deterministic environments on the applicability of I2Q in realworld scenarios Suggestions: 1. Broadening Comparative Analysis: It would be beneficial to compare I2Q with a broader set of stateoftheart MARL approaches including recent advances in decentralized learning. 2. Exploring Scalability: Additional experiments or theoretical analysis on scalability focusing on larger numbers of agents or more complex environments would enrich the findings. 3. Enhancing Robustness: Investigating extensions or modifications to I2Q that could strengthen its theoretical backing and performance in highly stochastic environments would be valuable.  Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work If not please include constructive suggestions for improvement. The authors have discussed the limitations related to the assumptions of determinism and the initial focus on theoretical rather than practical nonstationarity. However there is a lack of discussion on the potential negative societal impacts of their work. As with any powerful technology MARL systems can be used in ways that may raise ethical concerns such as surveillance or autonomous weapons. Constructive Suggestions for Improvement:  Ethical Considerations: The authors should acknowledge potential misuse of MARL technologies and suggest ethical guidelines or safeguards to mitigate negative societal impacts.  Limitation on Realworld Applicability: Further addressing the limitations regarding the applicability of I2Q in highly stochastic or dynamic realworld environments would give a more balanced view of the methods utility.  Discussion on Scalability: An explicit discussion on the scalability challenges and potential approaches to address them would provide a clearer perspective on the methods broader applicability.", "yhZLEvmyHYQ": "Review of \"Incorporating BiasVariance Tradeoff in Gaussian Processes for Efficient Active Learning\" 1. Summary This paper addresses a critical challenge in active learning with Gaussian Processes (GPs) due to the biasvariance tradeoff particularly under conditions of limited data. The authors propose two novel acquisition functions a Bayesian QuerybyCommittee (BQBC) and a Query by Mixture of Gaussian Processes (QBMGP) to navigate the tradeoff effectively. These functions are developed from the insight that the optimal biasvariance balance for GPs is intertwined with the joint posterior distribution of hyperparameters. By approximating this joint posterior through MCMC sampling the proposed functions aim to minimize prediction error while efficiently selecting new data points for labeling. The empirical investigation across six simulators demonstrates that BQBC outperforms existing alternatives in achieving the most favorable marginal likelihood while QBMGP exhibits superior predictive performance. 2. Strengths and Weaknesses Strengths: a. Originality: The idea of utilizing the joint posterior of hyperparameters to guide active learning in GPs is novel. The proposed acquisition functions BQBC and QBMGP introduce a sophisticated theoryinformed approach that leverages the full Bayesian treatment of GPs. b. Empirical Evidence: The authors provide robust empirical evaluations across multiple simulators demonstrating the effectiveness of the proposed methods in comparison with traditional acquisition functions. This comprehensive testing underscores the practical value of the research. c. Theoretical Foundation: The paper fortifies its empirical contributions with a solid theoretical foundation presenting a clear rationale for why and how the biasvariance tradeoff can be navigated through the joint posterior of GP hyperparameters. Weaknesses: a. Computational Cost: Even though the paper acknowledges the high computational demand of fully Bayesian GPs and the proposed MCMCbased approach it does not provide detailed solutions for scaling the method to very large datasets or highdimensional problems. b. Generalization Beyond Simulators: While the experiments convincingly show the proposed methods effectiveness on benchmark simulators the paper does not deeply explore their applicability or performance on realworld datasets or settings with nonsimulated noise characteristics. c. Lack of Theoretical Bounds: The paper lacks a discussion on potential theoretical guarantees or bounds on the improvement of the proposed acquisition functions over existing methods which could further validate their superiority. 3. Questions and Suggestions a. Scalability: Could the authors elaborate on potential strategies or adaptations of the proposed methods to handle larger datasets or more complex models where MCMC sampling becomes prohibitively expensive b. Realworld Applications: It would be beneficial to see the application of BQBC and QBMGP on realworld datasets potentially in domains like healthcare or environmental modeling to assess the methods effectiveness outside of controlled simulations. c. Comparison with NonGaussian Models: How might the proposed acquisition functions perform in the context of nonGaussian processbased active learning frameworks and what adaptations if any would be necessary 4. Limitations and Societal Impact The authors touch upon the computational limitations inherent in fully Bayesian approaches to GPs but could further explore strategies to mitigate these challenges without significantly compromising performance. Moreover the potential for negative societal impacts such as biases in data selection or model predictions especially in sensitive applications is not thoroughly addressed. A more detailed analysis of these aspects along with proactive measures to ensure ethical and equitable use of the proposed methods would strengthen the paper. Constructive Suggestions for Improvement:  Investigating and discussing approximation techniques that could enhance the scalability of the proposed methods without major losses in performance.  Conducting case studies in realworld settings to validate the practical applicability and robustness of the acquisition functions beyond simulators.  Expanding the discussion on ethical considerations particularly how biases in data acquisition could be minimized and the implications of applying these methods in socially impactful domains. Conclusion This paper makes a significant contribution to the field of active learning with Gaussian Processes by introducing innovative acquisition functions that effectively balance the biasvariance tradeoff. While further work is needed to address computational efficiency and apply these techniques in realworld scenarios the proposed methods hold promise for enhancing the efficiency and accuracy of data labeling in Bayesian optimization and metamodeling contexts.", "xI5660uFUr": " Review of \"Selective Compression of Representations for VariableRate Neural NetworkBased Image Compression\"  1) Summary This paper introduces a novel method for neural network (NN)based variablerate image compression termed as Selective Compression of Representations (SCR). Unlike existing approaches that require multiple models for different target bit rates or additional network modules to support variable rates SCR selectively encodes latent representations based on their importance determined through a 3D importance map and importance adjustment curves. These elements are then used to create a 3D binary mask to identify essential elements for compression at various quality levels. The SCR method can be integrated into existing compression models with minimal overhead and supports continuously variablerate compression through interpolation among preset quality levels. The authors demonstrate that SCR provides comparable coding efficiency to separately trained models and reduces decoding time achieving efficient and flexible image compression.  2) Strengths and Weaknesses  Strengths:  Novelty: The SCR approach is a significant departure from traditional methods focusing on selective compression in a fully generalized target qualityadaptive manner. This method represents an innovative step in variablerate image compression.  Efficiency and Flexibility: SCR achieves comparable coding efficiency to existing standards with significantly reduced decoding time and model complexity. Its ability to interpolate between quality levels for continuously variablerate compression is a valuable feature.  Integrability: The methods design allows for easy integration with current NNbased compression models without significant modifications enhancing its applicability across various architectures.  Weaknesses:  Limited Evaluation on Diverse Applications: The paper focuses primarily on the technical efficiency and effectiveness of SCR in comparative terms but does not extensively evaluate the method in diverse realworld applications or datasets outside the Kodak image set.  Potential for Improved Explanation of Limitations: While the paper discusses the methods performance and efficiency there could be a more thorough examination of its limitations potential failure modes or scenarios where SCR might not perform as expected.  3) Questions and Suggestions for Authors  Question: Can the authors provide insights into how SCR performs on more diverse and potentially larger datasets especially those with more varied content types  Question: How does the method respond to extremely low or high bit rates beyond the tested ranges Are there diminishing returns or notable performance drops  Suggestion: It would be beneficial to see a more detailed discussion on the potential negative impacts of using SCR particularly in contexts where image fidelity is critical.  Suggestion: Exploring the integration of SCR with recent advancements in image processing and compression beyond traditional NNbased methods could provide valuable insights into its adaptability and future application potential.  4) Limitations and Societal Impact The authors have made an effort to address the limitations of SCR in terms of its architectural integration and the selective nature of compression. However a more extensive discussion on potential limitations in diverse application scenarios and the methods performance under extreme conditions would be beneficial. Regarding societal impact while the paper mentions the efficiency and timesaving aspects of SCR it does not delve into the broader implications such as the consequences of widespread adoption on data storage practices or the potential for enhancing accessibility to highquality digital media in bandwidthconstrained environments. A constructive suggestion would be to include these considerations in future work to fully assess the versatility and impact of SCR in a broader societal context.", "tz1PRT6lfLe": "Review: 1) Paper Summary This paper tackles the challenge of creating a communicationefficient federated learning (FL) framework SoteriaFL which integrates communication compression techniques with strong privacypreserving mechanisms. Specifically it blends local differential privacy (LDP) with advanced communication compression methods including the novel shifted compression approach addressing the critical tradeoffs between privacy utility (accuracy of the machine learning model) and communication cost. By moving beyond the straightforward application of compression directly to differentially private gradients (a method referred to in this work as CDPSGD) the authors propose a more advanced strategy that employs a diverse array of local gradient estimators and achieves better performance metrics. The contributions of this paper include a detailed analysis of the tradeoffs involved theoretical guarantees for the proposed framework and empirical validations of its superiority over existing approaches. 2) Strengths and Weaknesses Strengths:  The unified framework SoteriaFL is a significant advancement in private FL offering flexibility in choosing local computation protocols at the client level.  SoteriaFL\u2019s ability to simultaneously enhance communication efficiency and preserve privacy without sacrificing the utility of the learning process addresses a critical gap in federated learning research.  The theoretical analysis is comprehensive providing clear tradeoffs between privacy utility and communication efficiency supported by empirical evidence that confirms the frameworks effectiveness. Weaknesses:  Although the paper presents a detailed theoretical analysis the practical implementation details particularly around the configuration and tuning of the algorithm parameters for different scenarios are not extensively discussed.  The experiments while supportive of the theory are limited to a few datasets and scenarios. A broader empirical investigation across more diverse datasets and in realworld distributed environments could strengthen the claims. 3) Questions and Suggestions for the Authors Questions:  Can the authors provide more insights into the scalability of SoteriaFL in highly distributed environments especially where clients have significantly varied computational capabilities  How sensitive is SoteriaFLs performance to the choice of local gradient estimators and the configuration of hyperparameters Suggestions:  Exploring the practical aspects of deploying SoteriaFL in realworld FL scenarios such as dynamic network conditions and client availability could enhance the applicability of the proposed framework.  Extending the empirical evaluation to include a broader range of datasets and problem settings would validate the generalizability of SoteriaFL. 4) Addressing Limitations and Potential Negative Societal Impact The authors partially address the limitations by providing a thorough theoretical underpinning for SoteriaFL and validating its effectiveness with numerical experiments. However the paper could benefit from a deeper exploration into the practical challenges of implementing the framework in realworld scenarios. There is also a need for a more detailed discussion on the potential negative societal impacts particularly the risks associated with misconfigurations that could lead to insufficient privacy protection or excessive communication costs. Constructive Suggestions:  Including a section on potential mitigation strategies for the identified risks and limitations could be valuable.  Discussing the ethical implications of deploying such a privacypreserving framework in sensitive applications and how malicious entities might attempt to exploit possible weaknesses would contribute to a more responsible development and deployment of federated learning technologies.", "kI_kL5vq6Oa": " Peer Review:  1) Brief Summary of the Paper: This paper introduces a method for designing perception systems in safetycritical autonomous systems such as aviation and automotive applications with a focus on minimizing the risk associated with perceptual errors. The authors develop a riskdriven design approach that incorporates a formulated risk function into the loss function used during the training of perception modules. The risk function assesses the downstream effects on safety of various perceptual errors thereby guiding the training process to minimize the occurrence of highrisk errors specifically. This approach is evaluated on a visionbased aircraft detectandavoid system demonstrating a significant reduction in collision risk compared to a baseline system without risk consideration.  2) Assessment of Strengths and Weaknesses: Strengths:  Novel Approach: The introduction of a riskdriven design methodology addresses a critical gap in the design of autonomous systems perception modules by focusing on minimizing not just perceptual errors but those errors that have the most detrimental impact on safety.  Quantitative Risk Reduction: The paper presents compelling evidence that the proposed method can reduce collision risks by up to 37 a substantial improvement over traditional training methods.  Relevancy: As autonomous systems become increasingly prevalent the importance of their safety cannot be overstated. This work contributes to a vital area of research with potential realworld impacts.  WellDefined Risk Function: The development and integration of the risk function into the training process are well articulated with clear mathematical formulations and theoretical justifications. Weaknesses:  Markov Assumption: The paper assumes that the perception system behaves according to Markov properties which may not always be the case especially in complex environments with dynamic and unpredictable elements.  Generalization Concerns: While the method is demonstrated on an aircraft detectandavoid system it remains unclear how well this approach generalizes to other types of autonomous systems with different safetycritical tasks.  Data Dependency: The approach relies on the availability of risksensitive regions of the state space for data collection which may not always be practical or achievable in realworld scenarios due to limitations in data accessibility or quality.  3) Questions and Suggestions for Authors:  Generalization to Other Domains: Could the authors provide more insights or possibly conduct additional evaluations on how this riskdriven design approach could be adapted for other autonomous systems such as selfdriving cars or robotic surgery systems  Handling NonMarkovian Behavior: Are there plans or ideas on how to extend the methodology to account for perception systems that do not adhere to Markovian assumptions This could be vital for its applicability in more complex environments.  Data Collection Strategies: It would be beneficial if the paper could discuss potential strategies for overcoming the challenges associated with collecting data in risksensitive regions especially in cases where direct data collection might be hazardous or infeasible.  4) Limitations and Societal Impact: The authors have acknowledged the limitation regarding the Markovian assumption of the perception system which is commendable. However a more thorough discussion on how this limitation might affect the realworld deployment of the methodology would be beneficial. Regarding societal impact while the paper emphasizes the potential for increased safety it could delve deeper into the ethical considerations and potential negative implications such as overreliance on autonomous systems or the displacement of human roles in monitoring and control tasks. Constructive suggestions include exploring hybrid humanmachine systems that leverage the riskdriven approach while incorporating human oversight to mitigate potential overreliance on autonomous decisionmaking.", "xL8sFkkAkw": " Peer Review  1. Summary of the Paper The paper introduces a novel technique in the domain of automated machine learning specifically focusing on the aspect of neural network parameter initialization. The major contribution is the proposition of a differentiable quantity called GradCosine which assesses the initial state of a neural network through the cosine similarity of samplewise gradients with respect to initialized parameters. By leveraging a theoretical foundation the authors show that optimizing this quantity can enhance both training and test performance of a network under a gradient norm constraint. Building upon this they develop an algorithm called Neural Initialization Optimization (NIO) that automates the search for better initialization settings across different architectures showing improved performance on CIFAR10CIFAR100 and ImageNet datasets and facilitating stable training of large vision Transformer architectures without the need for a warmup phase.  2. Strengths and Weaknesses Strengths:  Theoretical Foundation: The paper is grounded in solid theoretical analysis providing a compelling argument for the proposed GradCosine quantitys effectiveness in improving network performance.  Generality and Applicability: The NIO algorithms architectureagnostic nature makes it a valuable addition to the toolbox for a wide array of neural network configurations and tasks.  Empirical Validation: Extensive experiments across standard datasets and comparisons with existing stateoftheart methods demonstrate the practical benefits of the proposed approach. Weaknesses:  Scalability Concerns: While the paper touches upon memory consumption issues and batch setting relaxations for large models it does not fully explore the scalability of the proposed method particularly for very large models and datasets.  Hyperparameter Sensitivity: The paper mentions the choice of the upper bound gradient norm (\u03b3) but does not provide extensive discussion or guidance on how to select this and other hyperparameters for optimal performance across different settings.  3. Questions and Suggestions for Authors  Scalability to Larger Models: Can the authors provide more insight or experimental results on applying NIO to larger models such as GPT3 or deeper Transformer models and the associated computational requirements  Hyperparameter Selection: It would be beneficial for the paper to include a more detailed analysis or guidelines on choosing the hyperparameters especially the upper bound gradient norm (\u03b3) to aid practitioners in applying NIO effectively.  Comparison with Other Initialization Techniques: Are there specific scenarios or model architectures where NIO might not perform as well compared to existing initialization strategies Any insights or hypotheses on this would enrich the discussion.  4. Limitations and Potential Negative Societal Impact The authors have briefly touched upon the limitations related to the choice of \u03b3 and memory consumption for larger models. However a more detailed discussion on these limitations and potential strategies for mitigation would strengthen the paper. For example exploring adaptive or dynamic approaches for selecting \u03b3 based on model size or complexity could be an interesting area for future work. Regarding societal impact the paper claims neutrality in this aspect. However any technique that potentially lowers the barrier to creating more powerful models also amplifies the dualuse nature of AI technology (i.e. beneficial as well as malicious uses). A discussion on ethical considerations safe use and guidelines could be an important addition emphasizing responsible research and application of such advanced optimization techniques.  Constructive Suggestions for Improvement  Expand the scalability analysis to include larger models and datasets possibly incorporating strategies to manage computational and memory requirements effectively.  Provide a more comprehensive guide on hyperparameter selection with empirical or theoretical insights to assist users in various scenarios.  Include a discussion on the ethical ramifications and encourage responsible use of the technology aligning with broader societal aims for the safe development and deployment of AI systems.", "yfNSUQ3yRo": "Review for \"Noise Attention Learning for Robust Deep Learning with Noisy Labels\" 1) Summary The paper presents a novel approach Noise Attention Learning (NAL) to address the challenge of training deep neural networks (DNNs) with noisy labels. NAL introduces an attention branch to generate attention weights based on sample representations. These weights help differentiate between clean and mislabeled samples thereby preventing the network from memorizing the latter. The method incorporates these attention weights into a specially designed loss function that maintains the models original learning direction. The authors provide both theoretical analysis and empirical evidence to support the efficacy of NAL. Experiments conducted on two synthetic noisy datasets (CIFAR10 CIFAR100) and three realworld noisy datasets (ANIMAL10N Clothing1M Webvision) demonstrate NALs superior performance over stateoftheart methods. The paper also includes a comprehensive empirical analysis including attention weight distribution gradient analysis and sensitivity to hyperparameters. 2) Assessment Strengths:  Novelty: The introduction of an attention mechanism to specifically address the issue of label noise in training datasets is innovative. The method is theoretically sound and addresses a significant challenge in the training of deep learning models.  Empirical Results: The method outperforms stateoftheart approaches on both simulated and realworld datasets with noisy labels indicating its effectiveness and robustness against label noise.  Theoretical Support: The theoretical analysis provided gives insights into how NAL mitigates the memorization of mislabeled samples adding credibility to the empirical results.  Comprehensive Evaluation: The extensive empirical analysis including attention weight distribution and gradient analysis provides a deeper understanding of the methods functionality and underlying mechanisms. Weaknesses:  Generalization to Other Tasks: The paper primarily focuses on image classification tasks. It does not discuss or demonstrate the applicability or performance of NAL on other types of tasks (e.g. text classification regression).  Hyperparameters Sensitivity: While the paper presents a sensitivity analysis of hyperparameters it also indicates that selecting optimal values for these parameters might be dataset specific which could limit the methods ease of use and applicability in a wider range of scenarios without further tuning.  Interpretability of Attention Weights: The methods reliance on an attention mechanism for weight generation is a strength yet the paper does not delve into the interpretability of these weights. Understanding why certain samples are weighted as they are could offer more insights into the models learning process. 3) Questions and Suggestions:  Applicability to Other Tasks: Could the authors comment on or demonstrate how NAL might be adapted or applied to tasks beyond image classification such as text or speech  Hyperparameter Guidance: Can the authors provide more guidance or heuristics for choosing the hyperparameters especially in different domains or with different dataset complexities  Attention Weight Interpretability: Exploring the interpretability of attention weights could be beneficial. Can the authors provide analysis or visualization that helps understand why specific samples are deemed clean or noisy based on these weights 4) Limitations and Potential Negative Societal Impact The authors have addressed the limitations regarding the focus on image classification tasks and the sensitivity of hyperparameters effectively. However they have not discussed potential negative societal impacts of their work. One constructive suggestion would be to include a discussion on the ethical considerations associated with deploying models trained with NAL especially considering the possibility of bias amplification if the noisy labels themselves are biased. Furthermore a discussion on the environmental impact of potentially increased computational requirements due to the additional attention branch would enrich the paper.", "m2JJO3iEe_5": "Review of \"Extending Randomized Smoothing for FewShot Learning Models\" 1. Summary: The paper presents an innovative approach to enhancing the robustness of fewshot learning models against adversarial perturbations by extending the concept of randomized smoothing\u2014a stateoftheart provable defense mechanism previously applied to classifiers based on class probabilities\u2014to models that classify based on distances in a normalized embedding space. The authors offer a theoretical analysis regarding the Lipschitz continuity of these models and introduce a robustness certificate against `2bounded perturbations. They also propose a novel method of using the dot product for confidence interval estimation in the embedding space to facilitate classification robustness verification. The effectiveness of the theoretical contributions is empirically validated across various datasets. 2. Assessment: Strengths:  Theoretical Contribution: The paper fills a significant gap by providing the first theoretical robustness guarantees for fewshot learning a notable contribution given the increasing relevance of these models in diverse applications.  Novel Methodology: The authors approach to extend randomized smoothing to fewshot learning by considering the smoothness of the embedding space and utilizing dot products for confidence estimation is both innovative and practically relevant.  Empirical Validation: The experimental results convincingly demonstrate the effectiveness of the proposed method across multiple datasets underlining its applicability and potential impact. Weaknesses:  Computational Complexity: Despite acknowledging it the authors do not provide a solution to the computational intensity inherent in their proposed method specifically concerning the Monte Carlo approximations demand for a high sample number.  Generalization to Different FewShot Learning Setups: While the paper demonstrates applicability to a fewshot learning scenario it does not explicitly explore or discuss the methods effectiveness or adaptability across a broader range of fewshot learning configurations beyond the ones tested.  Discussion on Scalability: There is limited discussion on scaling this approach to more extensive and potentially more complex datasets which might involve higher dimensions of embedding spaces and more class prototypes. 3. Questions and Suggestions for the Authors:  On Computational Efficiency: Could the authors explore potential methods or approximations to reduce the computational cost without significantly compromising the robustness guarantees or the accuracy of the confidence interval estimation  Generalization Across FewShot Learning Models: It would be beneficial if the authors could provide further insight or experimental evidence on how their approach could be adapted or applied across different fewshot learning algorithms and scenarios.  Scalability: Can the authors discuss potential strategies or future work directions to scale this approach for more complex datasets or models with large embedding spaces Are there inherent limitations in the randomized smoothing technique when applied to such contexts 4. Limitations and Societal Impact: The authors have touched upon the computational complexity of their proposed method but have not fully addressed potential strategies to mitigate this limitation. Moreover while the work primarily focuses on enhancing the robustness of fewshot learning models a deeper exploration of the societal impacts especially concerning privacy and security implications would provide a more comprehensive understanding. It could be constructive for the authors to consider and discuss these aspects perhaps proposing future research directions aimed at addressing these challenges while also exploring the methods applicability to privacysensitive or highsecurity domains. Suggestions for Improvement:  The addition of discussions or preliminary explorations on optimizing computational efficiency.  Expansion of the experimental validation to include a broader range of fewshot learning scenarios and complex datasets.  An indepth exploration of the potential negative societal impacts especially concerning privacy and security accompanied by suggestions on mitigating these risks.", "wJwHTgIoE0P": "Review for \"Scaling up Representation Learning with Large Collections of Procedural Programs\" 1) Summary: This paper introduces a novel approach to learning image representations by leveraging a large dataset comprising twentyone thousand programs each designed to generate synthetic images through procedural programming. These generative programs developed using OpenGL facilitate the training of neural networks for both supervised and unsupervised representation learning tasks significantly reducing the performance gap between models pretrained with real and synthetic images. The study underscores the scalability and efficiency of using procedural image programs for pretraining contributing to stateoftheart performance in representation learning without real data and providing an extensive analysis of the factors influencing the effectiveness of generative image programs. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: The paper presents a groundbreaking use of a massive collection of procedural programs for generating synthetic images advancing the field of synthetic data application in neural network pretraining.  Comprehensive Analysis: The authors provide a thorough investigation into how the performance scales with the number of generative programs and delve into the properties that contribute to a successful generative image program thereby laying a foundation for future experimental designs.  Accessibility and Reproducibility: Making the code models and dataset publicly available is commendable and encourages further exploration and validation by the research community.  Practical Impact: The reduced dependency on real images for training neural networks addresses significant concerns such as privacy bias and accessibility of labeled datasets making this approach highly beneficial for a wide range of applications. Weaknesses:  Limitation in Diversity: While the paper succeeds in demonstrating the utility of synthetic data it does not extensively compare the diversity of generated images against realworld datasets beyond ImageNet which might limit understanding of its effectiveness across varied domains.  Generalization to Other Tasks: The experiments focus primarily on image classification tasks. Although VTAB is mentioned the generalizability of the trained models to a wider array of tasks including detection or segmentation is not adequately explored.  Potential Computational Overhead: The paper underestimates the potential computational load and environmental impact resulting from generating a vast amount of synthetic data and training large models which could be a concern for replicating this approach at scale. 3) Questions and Suggestions for the Authors:  Considering the computational intensity of rendering highquality synthetic images could the authors elaborate on the expected computational resources needed for similar scale studies Additionally discussing strategies to mitigate these costs could be beneficial.  How do the authors envision overcoming the limitations regarding the diversity of generated images Would incorporating more complex or dynamic scenes significantly alter the frameworks efficiency or output quality  It would be insightful if the paper could expand on the performance of these synthetic datatrained models on tasks beyond classification to include object detection segmentation or even nonvisual tasks. This could significantly underscore the versatility of the approach.  A deeper dive into the ethical considerations especially concerning the potential for generating synthetic data that inadvertently introduces new biases or perpetuates existing ones would enhance the discussion on societal impact. 4) Limitations and Societal Impacts: The authors have outlined an innovative approach but have not fully addressed the potential limitations and negative societal impacts of their work. While the reduction in reliance on real images is advantageous there is little discussion on the environmental impact of largescale synthetic data generation and training. Additionally the paper could benefit from a more thorough exploration of the ethical implications especially in terms of bias in generated images and the broader consequences of further distancing training data from realworld scenarios. To improve the authors could:  Provide an environmental impact analysis regarding the computational resources required for their approach.  Investigate and discuss potential biases in the procedural generation of images and propose mechanisms to assess and mitigate such biases.  Explore and elaborate on the potential for this technology to create unrealistic or harmful portrayals and suggest safeguards against such outcomes.", "mMdRZipvld2": " PeerReview  1) Brief Summary The paper under review introduces a novel approach to adjust for the confounding effect present in the similarity measures (such as RSA and CKA) used to understand the functional similarities between neural networks. The authors identify that the traditional use of these metrics can lead to misleading conclusions due to the influence of input data structure on the representational similarity. They propose a deconfounding method leveraging covariate adjustment regression to mitigate this problem. Their approach retains the intuitive properties of the original similarity measures while enhancing their ability to discern functionally similar neural networks particularly in contexts such as transfer learning and model generalization to outofdistribution data. The methods efficacy is demonstrated through experiments on public image and natural language datasets using various neural network architectures.  2) Assessment of Strengths and Weaknesses  Strengths:  Originality: The paper presents an innovative technique to address the confounding effect in neural network similarity metrics which is a significant contribution to the field of neural network interpretability and analysis.  Relevance: Given the widespread adoption of transfer learning and the increasing importance of understanding neural network behaviors the proposed method is highly relevant to current research and practical applications in machine learning.  Methodological Rigor: The approach is methodologically sound with a clear theoretical foundation and a thorough evaluation process demonstrating its advantages over existing methods.  Practical Applicability: The technique is versatile and can be applied across different domains (image language) architectures and datasets enhancing its usefulness for a broad audience.  Weaknesses:  Generalization of Deconfounding Approach: While the paper argues for the general applicability of the deconfounding fix the specific implementation details and assumptions (e.g. linearity) might limit its efficacy across all scenarios especially for deep layers or highly complex architectures.  Computational Complexity: The deconfounding process increases computational demands (notably in matrix operations) which could hinder its adoption for very large models or datasets.  Potential for Overcorrecting: There is a theoretical risk that the deconfounding process could overadjust for the confounding effect potentially obscuring meaningful similarities that are partially dependent on the input data structure.  3) Questions and Suggestions for Authors  Domain Generalization: Could the authors elaborate on how the proposed method performs across a wider range of tasks especially those involving more complex data structures or highly abstract representations  Comparison with Alternative Approaches: Besides RSA and CKA did the authors consider comparing their deconfounded approach with other representational similarity measures not mentioned in the paper  Impact on Model Training: Could deconfounding the similarity metrics be beneficial or applicable during the model training phase perhaps as a regularizer or in a metalearning context  4) Limitations and Societal Impact The limitations associated with the assumptions of linearity in deconfounding and the increased computational demands were touched upon but could be more extensively discussed especially concerning potential impacts on the applicability to very large models. Additionally while the paper is primarily technical more consideration could be given to the broader societal implications such as the risk of misuse in creating seemingly dissimilar networks to evade detection algorithms or audit processes. A constructive suggestion would be to explore the scalability of the method to accommodate the computational complexity and to include a discussion on ethical guidelines or safeguards for preventing potential misuse of the technique.", "uytgM9N0vlR": " Peer Review of \"Rethinking SelfSupervised Pretraining in Graph Neural Networks for Molecular Representation\"  1) Summary of the paper and its contributions This paper conducts a comprehensive examination of the efficacy of selfsupervised pretraining in graph neural networks (GNNs) for molecular representation specifically within the context of drug discovery. The authors carefully dissect various components of the GNN pretraining process including the objectives data splits input features dataset scales and GNN architectures to understand their impact on downstream task accuracy. They present findings that challenge the widely held belief regarding the unconditional benefits of selfsupervised pretraining in molecular graph representation showcasing scenarios where such pretraining is not significantly beneficial or where supervised pretraining outperforms selfsupervised methods. The manuscript highlights the importance of hyperparameters and suggests a nuanced view towards the application of pretraining in GNNs especially emphasizing the circumstances under which pretraining may or may not be advantageous.  2) Assessment of strengths and weaknesses Strengths:  Comprehensive Exploration: The paper offers an exhaustive analysis of the pretraining landscape for GNNs in molecular representation. The systematic study across multiple factors provides a muchneeded clarity on the conditions under which pretraining benefits are visible.  Critical Insights: The findings that selfsupervised pretraining does not always result in significant improvements and highlighting the importance of hyperparameters and feature richness are valuable insights for researchers and practitioners in the field.  Empirical Evidence: The use of a largescale dataset for the analysis and the rigorous statistical approach lend credibility to the papers conclusions. The empirical evidences from various experiments underpin the nuanced discussion on pretrainings effectiveness. Weaknesses:  Generalization of Findings: The findings are primarily based on small molecular datasets. Though the authors mention this scope explicitly the limited diversity in datasets might influence the generalizability of the conclusions to other graph neural network applications.  Discussion on Negative Societal Impact: The paper lacks a dedicated discussion on the limitations and the potentially negative societal impacts of their findings. Such a discussion is necessary given the increasing importance of responsible AI practices.  3) Questions and suggestions for the authors  On Generalizability: How do the authors envision the extension of their work to larger or more complex molecular structures or to entirely different domains where GNNs are applicable  Feature Richness vs. Pretraining: Can the authors elaborate on scenarios where even with rich features pretraining could provide additional benefits This might help clarify the boundaries of feature engineering versus model sophistication.  Impact of New GNN Architectures: Given the rapid evolution of GNN architectures how do the authors foresee their findings holding up against newer models that might address some of the limitations of current GNNs in capturing molecular representations  4) Limitations and Suggestions for Improvement The paper provides a critical look at GNN pretraining for molecular representation yet the exploration is predominantly limited to small molecular graphs. Expanding this analysis to larger or more complex structures could widen the findings applicability. Furthermore a detailed discussion on the potentially negative societal impacts such as the implications of inefficient pretraining on computational resources and associated environmental impacts would strengthen the paper. To mitigate possible misconceptions the authors should encourage researchers to critically assess pretraining needs based on specific project requirements and available resources rather than defaulting to pretraining as a universally beneficial step.", "qmy23tNBvbh": " Summary and Contributions The paper introduces a novel approach to attention mechanisms by leveraging kernel exponential families and proposing a sparse variant kernel deformed exponential families. It significantly extends the scope of continuous attention mechanisms beyond what previous literature such as works [23\u201325] and [8] have established primarily by introducing multimodality and sparsity to the attention densities. Theoretical contributions include proving new existence results for the kernel exponential and deformed exponential families demonstrating their approximation capabilities especially in the sparse case and establishing exponential convergence for context vector numerical integration. The practical significance of this work is evidenced through experiments showing the superiority of kernel continuous attention over unimodal continuous attention in certain contexts and the ability of the sparse variant to effectively highlight peaks in time series data.  Assessment of Strengths and Weaknesses  Strengths: 1. Theoretical Depth: The paper provides rigorous theoretical underpinnings for the proposed kernel exponential and deformed exponential families covering aspects such as normalization approximation capabilities and numerical integration properties. These contributions markedly advance the understanding of continuous attention mechanisms. 2. Practical Relevance: Through experiments with realworld datasets the paper demonstrates practical advantages of kernel continuous attention especially in capturing the peaks of time series and handling multimodal attention densities effectively. 3. Novelty: Introducing kernel deformed exponential families for sparse support is a novel contribution that opens up new possibilities for attention mechanisms particularly in applications requiring the selective attention to specific regions of data.  Weaknesses: 1. Complex Implementation: The reliance on numerical integration and the lack of closedform expressions for context vectors may pose challenges in implementation especially concerning computational efficiency and scalability to highdimensional problems. 2. Limited Dimensionality Analysis: The paper\u2019s analysis predominantly focuses on 1D applications potentially limiting its immediate applicability to higherdimensional data without additional investigation into the extension of the proposed methods.  Questions and Suggestions for the Authors 1. Extension to Higher Dimensions: Can the authors comment on the potential extensions of their proposed methods to higherdimensional data Is there ongoing work to adapt the numerical integration strategies for such cases 2. Computational Efficiency: Given the computational complexities introduced by numerical integration what optimizations or approximations do the authors suggest for practical implementations especially in largescale applications 3. Impact of Sparse Support: Could the authors provide further insights or empirical evidence on the impact of sparse support on different types of datasets particularly in cases where data sparsity or peak significance varies significantly  Limitations and Societal Impact The authors have adequately addressed the limitations particularly highlighting the challenges related to numerical integration and the dimensionality of data. However a more detailed discussion regarding the scalability of their approach and potential strategies for highdimensional data would further strengthen the paper. Regarding societal impact while the paper hints at potential applications a thorough exploration of both positive and negative societal implications especially in sensitive applications like wearable sensors and NLP would be beneficial. Suggestions for future exploration could include the development of more computationally efficient methods for numerical integration and the investigation of the models behavior and utility across a broader range of application areas with diverse societal impacts.", "x7S1NsUdKZ": " Peer Review for \"Adaptive Sampling for Discovery (ASD): Maximizing Discoveries through InformationDirected Sampling\"  1) Summary of the Paper This paper introduces Adaptive Sampling for Discovery (ASD) a sequential decisionmaking framework aimed at extracting valuable discoveries from large unlabeled datasets particularly in scenarios where labeling is expensive or tedious such as in drug discovery. By leveraging a novel informationdirected sampling (IDS) strategy the paper presents a method that judiciously balances the explorationexploitation tradeoff inherent in such problems. The authors rigorously define the problem setup establish connections to the MultiArm Bandit (MAB) framework and propose the IDS algorithm with theoretical guarantees spanning linear graph and lowrank models. Empirical evaluations through simulations and application to realworld chemical data demonstrate the superiority of IDS over traditional approaches in identifying highvalue samples for labeling.  2) Strengths and Weaknesses of the Paper Strengths:  Novelty and Practical Significance: The paper addresses a realworld problem with significant implications in various domains particularly drug discovery. The introduction of IDS for ASD is innovative bridging a gap in the literature concerning adaptive sampling and discovery.  Rigorous Theoretical Analysis: The authors provide comprehensive theoretical guarantees for the IDS strategy across different model assumptions. This level of rigor is commendable and strengthens the papers contributions.  Empirical Validation: The use of both simulated environments and realworld datasets to validate the proposed IDS algorithm is a major strength. The results showcase the practical utility and superiority of the IDS approach over established baselines. Weaknesses:  Limited Discussion on Computational Efficiency: While the paper extensively discusses theoretical implications and experimental outcomes there is a noticeable lack of discussion related to the computational aspects of the IDS algorithm especially when scaled to very large datasets.  Assumption Clarity: Some assumptions made in the paper particularly around the theoretical models might benefit from additional clarification or justification as to their relevance or applicability in realworld settings.  3) Questions and Suggestions for the Authors  On Computational Efficiency: Can the authors provide insights on the computational complexity of the IDS algorithm especially in comparison to traditional methods Additionally are there strategies that can be employed to enhance its scalability  Model Assumptions: It would be helpful if the authors could further elaborate on the assumptions made within the linear graph and lowrank models. Specifically how robust is the IDS approach if these assumptions are slightly violated in practical scenarios  Exploration of Other Domains: Have the authors considered the applicability of the IDS strategy beyond chemical discovery perhaps in fields like genomics or material science Insights into potential adaptations or limitations in these contexts would be valuable.  Further Empirical Studies: While the current empirical results are compelling additional studies particularly longitudinal analyses on how the IDS strategy performs over extended periods of discovery could further bolster the papers claims.  4) Limitations and Societal Impact  Addressing Limitations: The paper could benefit from a more detailed discussion on potential limitations including scenarios where IDS might not perform as expected. Acknowledging these could help future research in developing more robust algorithms.  Societal Impact: The authors have not explicitly discussed the negative societal impacts particularly in the context of ethical considerations in automated discovery (e.g. automation bias data bias). It would be responsible to include a section discussing possible negative impacts and mitigation strategies.  Conclusion This paper presents a significant step forward in the adaptive sampling domain introducing a novel theoretically grounded and empirically validated strategy for discovery tasks. While it stands out for its innovative approach and practical relevance addressing the raised questions and suggestions could make it more comprehensive and better prepared for broader application.", "wmwgLEPjL9": "Summary and Contributions: The paper meticulously evaluates the efficiency of unitary scalarization against specialized multitask optimizers (SMTOs) in both supervised and reinforcement learning frameworks. Traditionally SMTOs have been favored due to their alleged superior handling of multitask learning by navigating pertask gradient conflicts among other issues often at the expense of increased computational overhead. This paper challenges the status quo by showing that simple unitary scalarization augmented with conventional regularization and stabilization techniques from singletask learning either matches or surpasses the performance of these more complex optimizers across various popular benchmarks. Importantly it proposes that the supposed advantage of SMTOs could be attributed to their implicit regularization effect offering a novel perspective on how these optimizers function. The key contributions include a comprehensive experimental evaluation across benchmarks a technical analysis reinterpreting the function of SMTOs as regularization and the provision of code for reproducibility. Assessment of Strengths and Weaknesses: Strengths: 1. Comprehensive Evaluation: The extensive experimental setup and rigorous analysis ensure a broad and detailed comparison enhancing the papers credibility. 2. Novel Perspective: Proposing regularization as a potential explanation for the benefits traditionally attributed to SMTOs is both innovative and thoughtprovoking. 3. Reproducibility: Offering code and detailed experimental protocols promotes transparency and allows the community to build directly upon this work. Weaknesses: 1. Generalization of Findings: While the paper covers several benchmarks the generalizability of its conclusions across all multitask learning scenarios remains uncertain. Additional benchmarks or realworld applications might be necessary to fully validate the claims. 2. Limited Exploration of SMTO Variants: By focusing on a selected set of SMTOs some newer or less conventional approaches might have been overlooked potentially missing out on contrasting results. 3. Discussion on Computational Resources: The comparison in terms of computational overhead is beneficial however a more detailed quantitative analysis of the tradeoffs involved including memory consumption and scalability would have been valuable. Questions and Suggestions for Authors: 1. Generalizability Concerns: Could the authors consider extending their experiments to include more diverse benchmarks or realworld scenarios to ensure the findings applicability across various MTL contexts 2. Consideration of Newer SMTOs: Are there plans to evaluate more recent SMTO developments potentially uncovering scenarios where these might outperform unitary scalarization even when enhanced by regularization techniques 3. Detailed Computational Overhead Analysis: Could the authors provide a more granular analysis of the computational resources required by unitary scalarization vs. SMTOs including not just runtime but also memory usage and scalability aspects Limitations and Constructive Suggestions: The authors have made a commendable effort in addressing potential limitations by providing a wellrounded experimental evaluation and novel analytical perspectives. However the work could be further strengthened by: 1. Expanding the Range of Benchmarks: To solidify the universality of the claims incorporating a wider array of benchmarks especially from emerging domains could be very telling. 2. More InDepth Computational Analysis: Offering a deeper dive into the computational demands possibly including a comparison of resource efficiency across a cloudbased computational grid would aid in understanding the practical implications of adopting unitary scalarization over SMTOs in different settings. 3. Addressing Potential Negative Societal Impacts: While not directly addressed any optimization techniques efficiency and resource demands can have broader implications including environmental impact through increased computational demands or potential biases exacerbated by regularization techniques. A brief discussion on these aspects could enrich the papers societal relevance. In summary the paper presents a compelling case for revisiting the efficiency of unitary scalarization in multitask learning backed by thorough experimentation and a fresh interpretive lens. Addressing the suggestions could further bolster its significance and applicability.", "vbPsD-BhOZ": "Review of \"Exploring Cellular Sheaves: A Novel Topological Perspective on Graph Neural Networks\" 1) Summary: This paper proposes a novel framework that utilizes cellular sheaf theory to address fundamental problems inherent in Graph Neural Networks (GNNs) specifically their performance in heterophilic graphs and oversmoothing behavior. By exploiting the underlying geometry of graphs through cellular sheafs\u2014which associate vector spaces to graph nodes and edges and linear maps to nodeedge pairs\u2014the authors develop a deeper understanding of how GNN performance can be tethered to the graphs geometric structure. The papers contributions include a thorough exploration of sheaf diffusion processes demonstrating their ability to achieve class separation in the limit and presenting Sheaf Convolutional Networks (SCNs) which offer improved control over asymptotic behavior compared to traditional GNNs. Furthermore the paper proposes methods for learning sheaves from data presenting sheaf diffusion models that outperform classical approaches in heterophilic settings. The work is grounded in solid mathematical theory provides a broad array of propositions and theorems to support its claims and offers practical implications by testing on realworld datasets. 2) Assessment: Strengths:  Theoretical Innovation: The integration of cellular sheaf theory into the analysis of GNNs is an innovative approach that contributes to both graph machine learning and algebraic topology.  Addressing Key Challenges: The paper effectively addresses two significant issues in GNNs providing theoretical insights and practical solutions.  Comprehensive Analysis: The theoretical framework is meticulously developed with rigorous analysis and detailed propositions that are wellsupported by experiments. Weaknesses:  Complexity: The high level of mathematical complexity might make the concepts less accessible to practitioners without a strong background in algebraic topology.  Generalization Properties: The paper does not delve into the generalization properties of the learned sheaves which could be a critical factor in broader applicability.  Computational Efficiency: The complexity analysis suggests a potential overhead compared to traditional GCNs particularly when nondiagonal sheaves are used. This could limit scalability to larger graphs. 3) Questions and Suggestions:  Generalization Performance: Could the authors discuss any potential approaches or future directions that could assess the generalization performance of sheafbased models  Scalability Considerations: Are there strategies that could be employed to mitigate the computational overhead especially for largescale datasets or more complex sheaf structures  Comparison Baseline: It might be beneficial to include comparisons with more recent advancements in GNN architectures that address heterophily and oversmoothing to contextualize the improvements provided by sheafbased models.  Interpretability: Can the authors provide insights into the interpretability of the learned sheaves Understanding how sheaf structures relate to graph properties could offer valuable insights. 4) Limitations and Societal Impact: The authors have provided a brief discussion on the limitations and potential societal impact. However a more detailed exploration of the practical limitations regarding scalability and applicability to a broader range of graphstructured data is warranted. Furthermore while immediate negative societal impacts are not anticipated any machine learning development could potentially be used in ways that exacerbate biases present in data. A discussion around ethical considerations particularly in how sheafbased models might amplify or mitigate data biases would be a valuable addition. Suggestions for Improvement:  Incorporate a discussion on generalization properties and potential methods to evaluate them.  Explore and propose scalability enhancements for practical application.  Broaden the comparison with stateoftheart GNN models to include those that specifically address the challenges tackled in this paper.  Enhance the discussion on interpretability and ethical considerations surrounding the application of sheafbased models. Overall this paper makes substantial theoretical contributions to the field of graph machine learning with practical implications that could pave the way for more geometrically informed models in future GNN research and applications.", "mkEPog9HiV": " Review of \"Neural Sewing Machine (NSM): A Learning Framework for Structurepreserving 3D Garment Modeling\"  1. Brief Summary of the Paper and its Contributions This paper introduces the Neural Sewing Machine (NSM) a novel learningbased approach for 3D garment modeling that focuses on preserving the intrinsic structure of garments with diverse shapes and topologies. The authors begin by outlining the limitations of existing methods in 3D garment modeling which are predominantly physicsbased simulators or templatebased methods that struggle with complexity computational efficiency and flexibility. The NSM addresses these issues by learning representations from sewing patterns which accurately describe the intrinsic structure and topology of garments allowing for accurate 3D reconstruction and controllable manipulation. The core innovation lies in its ability to unify sewing pattern embeddings for garments of various shapes and topologies decode these embeddings into 3D garments using UVposition maps with masks and incorporate novel structurepreserving losses (innerpanel interpanel and surfacenormal loss) for maintaining garment fidelity. The paper demonstrates NSMs superiority over stateoftheart methods through extensive experiments.  2. Assessment of Strengths and Weaknesses Strengths:  Innovation: The NSM presents a unique approach that significantly improves the modeling of 3D garments by using sewing patterns. This approach is both novel and highly effective in representing garments with intricate structures and topologies.  Extensive Evaluation: The authors conducted comprehensive experiments demonstrating the NSMs effectiveness in various tasks including 3D garment reconstruction and manipulation showing clear advancements over existing methods.  Practical Application Potential: The interpretable nature of the NSM combined with its accuracy in representation and manipulation holds significant promise for realworld applications in garment design and virtual tryon systems. Weaknesses:  Limited Discussion on Computational Efficiency: While the authors highlight the computational inefficiency of physicsbased methods there is little discussion on the computational demands of the NSM itself which is critical for realtime applications.  Lack of Realworld Data Application: The experiments primarily focus on a controlled dataset. The application and evaluation of NSM on realworld possibly noisy data is not discussed which is crucial for understanding its practical viability.  Limited Addressing of GarmentBody Interaction: The interaction between 3D garments and human bodies especially under dynamic movements is not explored which is vital for applications like virtual tryon.  3. Questions and Suggestions for the Authors  Question: Could the authors provide insights into the computational requirements of NSM especially in comparison to physicsbased methods  Suggestion: It would be beneficial to see experiments or evaluations of NSM applied to realworld datasets possibly containing images of real garments to assess its performance in less controlled environments.  Suggestion: Future work could also benefit from exploring the modeling of garmentbody interactions particularly for dynamic poses to enhance the applicability of NSM in virtual tryon and animation.  4. Addressing Limitations and Potential Negative Societal Impact The authors briefly discuss limitations including the current inability to model garment deformations with varying human poses and the challenge of irregular panel shapes found in specific cultural garments. Suggestions for overcoming these include the use of \"virtual garment bones\" or diffused skinning. Expanding the method to more irregular panel shapes in future work is encouraged which would increase the methods inclusivity and practical applicability. On the societal impact while the authors believe NSM does not have negative implications it would be prudent to consider and mitigate any potential misuse of the technology such as creating deepfakes or unrealistic beauty standards in virtual environments. Constructive mitigation strategies could be discussed in future works to ensure the responsible use of NSM in society.", "m7CmxlpHTiu": " 1) Brief Summary and Contributions This paper addresses a significant issue in longtailed recognition tasks by considering a more realistic scenario where the test class distribution is unknown and potentially nonuniform diverging from the typical assumption of a uniform test class distribution. The authors introduce a novel method called Selfsupervised Aggregation of Diverse Experts (SADE) which strategically leverages two main innovations: a skilldiverse expert learning strategy to train multiple experts on a single longtailed dataset which are adept at handling various class distributions and a selfsupervised testtime expert aggregation strategy to dynamically combine these experts strengths to effectively address unknown test class distributions. The paper demonstrates both theoretical justification and empirical validation showing that SADE outperforms current methods in both vanilla and testagnostic longtailed recognition tasks. Opensource code has been shared to encourage further research.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Practical Relevance: The paper tackles a practically relevant problem overlooked by most existing work providing a solution that is feasibly applicable in realworld scenarios.  Comprehensive Approach: SADE is a wellrounded approach that not only handles the diversity of training data but is also capable of dynamically adapting to unknown test distributions without requiring prior knowledge.  Theoretical and Empirical Validation: The authors present a strong theoretical foundation for their strategy and support it with promising empirical results across multiple datasets and scenarios reinforcing the methods efficacy.  Opensourced Code: Sharing the implementation details and source code enhances reproducibility and accelerates further research in this direction. Weaknesses:  Complexity for Deployment: While SADE shows impressive performance the necessity to train multiple experts and the dynamic aggregation process might introduce computational and deployment complexity.  Potential for Overfitting Test Distribution Simulations: The reliance on selfsupervision to simulate testagnostic distributions could in theory lead to overfitting certain distribution features though this is not evident in the empirical results presented.  Limited Exploration of Expert Diversity Mechanisms: The mechanism for generating skilldiverse experts while innovative could be further elaborated upon especially regarding how these diverse skills are defined and encouraged during training.  3) Questions and Suggestions for Authors  Diversity Mechanism Details: Can the authors provide more insights into the criteria or mechanisms used to ensure the diversity of the learned experts especially regarding the distribution of expertise  Scalability Concerns: How does SADE scale with an increasing number of classes and experts Are there computational or practical limitations to the number of experts that can be efficiently managed  Generalization to Other Tasks: Have the authors considered extending this approach to tasks beyond image recognition and if so what adjustments would be necessary for different data modalities or task structures  4) Limitations and Societal Impact The authors have addressed the limitations regarding the deployment complexity and potential overhead introduced by managing multiple experts. However a deeper discussion on the computational efficiency especially in resourceconstrained environments could enhance the paper. Additionally while the paper briefly discusses the novelty of the approach in handling unknown test distributions a broader contemplation on the societal impact such as the benefits and potential misuses in sensitive applications (e.g. surveillance) would be valuable. Suggestions for Improvement:  Efficiency Optimizations: Explore strategies for reducing the computational overhead possibly through pruning less effective experts based on the specific test distribution characteristics.  Broader Impact Analysis: Extend the discussion on the societal impact considering both the positive implications of improved model fairness across diverse test distributions and the need for ethical considerations in deployment scenarios. Overall the paper presents a significant contribution to the field of longtailed recognition with practical implications addressing both theoretical and applicationoriented challenges.", "kUOm0Fdtvh": "Review for the Submission to NeurIPS 2022: \"AdaFocal: CalibrationAware Adaptive Focal Loss for Improved Neural Network Calibration\" 1. Summary: The paper presents AdaFocal a novel loss function modification aimed at improving neural network calibration during training. Calibration the degree to which a models confidence scores reflect the actual likelihood of its predictions being correct is a known issue especially with modern complex neural networks. To counter poor calibration AdaFocal builds upon the concept of focal loss by allowing the regularization parameter \u03b3 to be adaptively modified across different training epochs and for different groups of samples based on the model\u2019s past behavior regarding overunderconfidence. This approach essentially aims to dynamically correct the model\u2019s calibration by steering \u03b3 based on the models performance in previous training stages. The evaluation of AdaFocal spans both image and text classification tasks showcasing its ability to maintain calibration while still achieving competitive accuracy levels compared to existing methods. Furthermore AdaFocal has shown promising results in improving outofdistribution (OOD) detection. 2. Assessment: Strengths:  Novelty: The introduction of an adaptive mechanism to modify \u03b3 in focal loss based on the models calibration performance is novel and addresses a pressing need in enhancing neural network reliability especially in sensitive applications.  Broad Evaluation: The method is rigorously evaluated across a variety of tasks and network architectures affirming its general applicability and effectiveness in different scenarios.  Improved OOD detection: The papers exploration into OOD detection performance and the reported improvements suggest that AdaFocal can significantly contribute to safer AI applications. Weaknesses:  Theoretical Grounding: The paper could benefit from a more detailed theoretical discussion on why and how the adaptive mechanism of AdaFocal leads to better calibration including potential tradeoffs.  Comparison Baselines: While the paper compares AdaFocal to several existing calibration methods it may not completely cover all stateoftheart approaches possibly overlooking recent advances that could serve as stronger baselines.  Hyperparameter Dependence: The selection of hyperparameters like \u03bb \u03b3maxmin and Sth appears to be done based on empirical observations without a clear methodology which might limit AdaFocals ease of use and adaptability. 3. Questions and Suggestions:  Could the authors clarify if there are specific scenarios or configurations where AdaFocal might not perform as expected For instance how does it behave with extremely unbalanced datasets or with tasks involving severe class imbalances  It would be beneficial if the authors could provide insights or guidelines on selecting optimal hyperparameters especially in applications different from the ones tested. Is there a systematic approach to tuning these parameters or will users have to rely on trial and error  A comparison with more recent calibration techniques possibly through a supplementary section could strengthen the papers claims. Are there any known limitations of AdaFocal when compared to these approaches 4. Limitations and Societal Impact: The authors have not thoroughly discussed the limitations and potential negative societal impacts of their work. While improved calibration and better OOD detection are certainly beneficial there could be specific use cases where the adaptability of AdaFocal introduces risks such as overfitting to particular aspects of the data that might not generalize well or unexpected behaviors in highstakes applications. Constructive suggestions include a more detailed exploration of failure modes and the provision of guidelines to mitigate any identified risks. Additionally a discussion on the ethical implications of deploying wellcalibrated models in sensitive applications (e.g. surveillance law enforcement) could provide a more comprehensive overview of the societal impact. In conclusion AdaFocal represents an important step forward in addressing the calibration challenge in neural networks. However for a more complete picture the outlined questions and suggestions should be considered alongside a candid discussion on the limitations and societal impacts.", "thirVlDJ2IL": "Review for the paper on Efficient Learning of Spherical Gaussian Mixtures in Low Dimensions 1) Summary of the Paper This paper addresses the challenging problem of learning mixtures of spherical Gaussians focusing on the lowdimensional regime where existing techniques face limitations. The authors present a novel algorithm capable of efficiently estimating the means of a mixture of k spherical Gaussians under a condition of separation that is fundamentally less restrictive than previous research suggests. Specifically the algorithm works in dimensions \\(d  O(\\log k \\log \\log k)\\) and requires a separation condition that is strictly smaller than what was previously considered necessary. The significant contribution of this work is the establishment of nearly optimal bounds on the separation necessary for efficient learning in low dimensions coupled with an analysis demonstrating the necessity of this separation criterion. Moreover the paper extends the findings beyond Gaussian mixtures to more general distributions under certain conditions related to their Fourier spectra. 2) Assessment of the Paper Strengths  Theoretical Contribution: The paper nearly closes the gap in understanding the critical separation threshold for efficiently learning spherical Gaussian mixtures especially in low dimensions a question that has remained partially open in previous studies.  Generality of Approach: The extension of the main results to nonGaussian distributions signifies a broad applicability of the proposed method making the contribution relevant to a wider range of learning problems.  Simplicity and Analysis: The algorithm and its analysis are praised for their simplicity and clarity. The use of Fourier transform estimation at carefully chosen frequencies presents an elegant and insightful approach to tackle the problem. Weaknesses  Practical Applicability: The paper lacks empirical validation of the proposed algorithm. Including experimental results demonstrating the algorithms performance on synthetic or realworld datasets would have strengthened the paper significantly.  Comparison with Related Work: Although the paper surveys relevant literature it could benefit from a more detailed and direct comparison with the most closely related methods particularly in terms of theoretical guarantees and efficiency.  Discussion on Limitations: The limitation section briefly mentions that the positive results apply primarily to spherical Gaussians in logarithmically dimensioned clusters. A more thorough discussion of the implications of this limitation and potential barriers to addressing more general cases would enhance the paper. 3) Questions and Suggestions for the Authors  Extension to Higher Dimensions: The authors might discuss or speculate on the challenges and potential strategies for extending the proposed algorithm to work efficiently in higher dimensions or under less restrictive separation conditions.  Experimental Validation: Are there plans to empirically test the algorithm Insights into its robustness sensitivity to parameter settings and comparison with stateoftheart methods on both synthetic and real datasets would be highly valuable.  Impact of Assumptions on Applicability: How critical are the assumptions regarding the Fourier spectrum of nonGaussian distributions for the generalization of the algorithm A discussion on the nature and typical scenarios where these conditions hold or fail would provide clarity. Limitations The authors acknowledge the limitation regarding the focus on spherical Gaussians and the dimensionality regime. However a more comprehensive discussion on potential negative societal impacts such as misuse in surveillance or biases in clustering applications would have been appropriate. Suggestions for future work might include exploring mechanisms to ensure fair and unbiased learning outcomes or safeguards against misuse in sensitive applications. Improvement Suggestions  Incorporate experiments to demonstrate the practical efficacy of the proposed method.  Expand the discussion on limitations and societal impacts including ethical considerations.  Provide more detailed comparisons with related methods highlighting the novel contributions more distinctly. Overall the paper presents a significant theoretical advancement in the field of learning mixtures of spherical Gaussians with potential implications for a broader class of learning problems. With improvements in experimental validation and a more indepth discussion on its limitations and societal impacts the paper could make a substantial contribution to the literature.", "vriLTB2-O0G": " Peer Review on \"A Novel LearningBased Method for Pareto Set Approximation in MultiObjective Bayesian Optimization\"  1) Brief Summary of the Paper This paper introduces a novel Pareto Set Learning (PSL) method aimed at efficiently approximating the whole Pareto set for expensive multiobjective optimization problems with a limited evaluation budget. The authors propose a learning model that maps tradeoff preferences to their corresponding Pareto solutions supported by a surrogate modelbased method and a batch evaluation strategy. Through experimental analysis on synthetic and realworld problems the paper demonstrates the methods capability to outperform existing MultiObjective Bayesian Optimization (MOBO) approaches in terms of performance and computational cost while providing a flexible framework for decisionmakers to explore tradeoff solutions.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Practicality: The paper addresses a significant gap in MOBO by proposing an innovative approach to learn the entire Pareto set. This is a practical advance aiding decisionmakers in exploring tradeoffs efficiently within a constrained evaluation budget.  Comprehensive Methodology: The authors provide a detailed explanation of the methodology including the use of scalarization methods the formulation of the Pareto set model and the integration with Gaussian Process models for batch selection.  Experimental Validation: The method is thoroughly validated on synthetic and realworld problems against various MOBO algorithms. The results convincingly show the proposed methods efficacy in capturing accurate and valuable Pareto fronts.  Low Computational Overhead: The comparative analysis on runtime demonstrates that the Pareto set learning approach is computationally efficient an essential attribute for dealing with expensive optimization problems. Weaknesses:  Limited Discussion on Robustness: The paper could benefit from a deeper analysis of the method\u2019s robustness across diverse problem types especially those with highly complex or nonconvex Pareto fronts.  Scalability Concerns: While the paper states the method\u2019s efficiency it does not thoroughly assess scalability particularly in problems with a higher number of objectives or in very large decision spaces.  Lack of User Study: Despite emphasizing the methods potential in enhancing decisionmaker interaction the paper does not include a user study to validate the user\u2019s experience and effectiveness in employing the proposed approach for tradeoff exploration.  3) Questions and Suggestions for the Authors  Scalability Analysis: Can the authors elaborate on the method\u2019s scalability and its performance on problems with many objectives or larger search spaces  Robustness to Model Assumptions: How sensitive is the method to the assumptions made in the surrogate models It would be beneficial to discuss its performance under model misspecification or when encountering unexpected problem structures.  User Experience Validation: Considering the emphasis on aiding decisionmakers conducting a user study could substantiate the papers claims about improved interaction and decisionmaking process. Any plans to include this  Limitations and Suggestions for Improvement The paper mentions potential limitations related to the surrogate models accuracy and the Pareto set learning algorithms performance in vast search spaces or under a tight evaluation budget. However it lacks a detailed discussion on mitigating these limitations such as adaptive sampling techniques or enhanced model robustness strategies. Furthermore the paper does not thoroughly address the potential negative societal impacts especially in cases where the optimization is applied to sensitive or critical domains. It would be constructive for the authors to consider and discuss safeguards or ethical guidelines when employing the proposed method in such contexts.", "tVHh_vD84EK": "Paper Summary and Contributions: The paper introduces a novel universal modeling framework dubbed UniST equipped with a unique automatic search strategy AutoST aimed at addressing the challenges in spatiotemporal (ST) sequence forecasting. This framework is notable for its versatility in modeling ST dependencies accommodating various modeling priorities (spatialfirst temporalfirst and spatiotemporal synchronous) through three specialized layers (S2T T2S and STS). The key contributions of the paper are the introduction of these three attentionbased modeling units the unified architecture they facilitate and the AutoST algorithm that automates the search for the optimal spatiotemporal modeling priority. Experimental validation across five realworld datasets demonstrates the efficacy of UniST and AutoST with results showing superior performance to existing stateoftheart methods. Strengths and Weaknesses: Strengths: 1. Innovative Approach: The paper addresses a significant gap in ST sequence forecasting by proposing a flexible and comprehensive framework that can adapt to different modeling priorities. This represents a significant leap forward in the field. 2. Comprehensive Evaluation: The extensive experimentation across multiple datasets and comparative analysis with baseline models lends credibility and robustness to the proposed approach. 3. Automated Priority Search: The AutoST strategy for automatically finding the optimal modeling priority is highly innovative reducing the need for manual tuning and potentially accelerating the deployment of ST forecasting models in various applications. Weaknesses: 1. Model Complexity: The introduction of three different modeling layers while innovative also adds complexity to the framework. The paper could benefit from a more detailed discussion on the computational efficiency and scalability of the proposed method. 2. Lack of Theoretical Backing: While the experimental results are strong the paper lacks a theoretical analysis of why and how the proposed method outperforms existing approaches. Including such an analysis could strengthen the arguments made. 3. Limited Discussion on Failure Cases: The paper presents overwhelmingly positive results but lacks a detailed discussion on scenarios where UniST or AutoST might underperform. Including such insights would provide a more balanced view of the frameworks capabilities. Questions and Suggestions for Authors: 1. Model Complexity and Efficiency: Can the authors provide insights into the computational efficiency of UniST and AutoST especially in comparison with the baseline models Suggestions include a comparison of training times or the computational resources required. 2. Failures and Limitations: Are there specific types of ST sequence forecasting tasks or datasets where UniST and AutoST might not perform as expected Understanding such scenarios could help refine the approach further. 3. Theoretical Insights: Could the authors elaborate on the theoretical underpinnings that enable UniST and AutoST to outperform other methods Including such analysis could enhance the papers contribution to the field\u2019s understanding of ST sequence forecasting. Limitations and Societal Impact: The authors have demonstrated the effectiveness of UniST and AutoST across several datasets showing substantial improvements over baseline models. However the papers discussion on the limitations and potential negative societal impacts appears to be lacking. For constructive improvement the authors could consider the following:  Model Interpretability: With the complexity of the proposed framework interpretability might be challenging especially in critical applications like medical diagnosis or traffic management. The authors could explore methods to enhance the interpretability of their model aiding in its adoption in sensitive fields.  Adaptation to Diverse Data Types: While proven effective on the tested datasets it would be beneficial for the authors to discuss the adaptability of their framework to other ST data types or scenarios where it might underperform.  Societal Impact: Addressing the potential negative impacts such as privacy implications when applied to personal data (e.g. location tracking or health monitoring) would provide a more comprehensive overview of the frameworks societal relevance. Addressing these aspects could substantially enrich the paper offering a clearer more balanced view of UniST and AutoST\u2019s potential impact and limitations.", "ohk8bILFDkk": " 1) Summary of the Paper and Its Contributions The paper introduces a novel framework termed semiinfinitely constrained Markov decision processes (SICMDPs) extending the conventional constrained Markov decision processes (CMDPs) by incorporating a continuum of constraints. This modification addresses scenarios in reinforcement learning where constraints are timeevolving or dependent on uncertain parameters which are not adequately represented in traditional CMDP frameworks. The authors propose an algorithm SICRL which leverages linear semiinfinitely programming (LSIP) transformed from the reinforcement learning problem and solved using the dual exchange method from LSIP literature. They provide theoretical analysis for SICRL including sample complexity and iteration complexity bounds and validate the efficacy of the model and algorithm through numerical experiments involving two examples: a toy SICMDP and a complex problem simulating the discharge of sewage.  2) Assessment of Strengths and Weaknesses  Strengths:  Innovative Framework: The development of SICMDPs to deal with a continuum of constraints is novel and addresses a significant limitation in existing CMDP frameworks.  Theoretical Contributions: The theoretical analysis providing bounds on sample complexity and iteration complexity contributes valuable insights into the behavior and performance of the proposed SICRL algorithm.  Practical Validation: Extensive numerical examples that illustrate the SICMDP model and validate the SICRL algorithm lend credibility to the proposed methods and demonstrate their applicability to realworld problems.  Weaknesses:  Complexity and Scalability: Despite demonstrating the applicability of SICMDPs to complex scenarios the paper does not thoroughly address the computational complexity and scalability of SICRL especially in highdimensional problems.  Limited Exploration of Alternative Methods: The authors focus extensively on the dual exchange method for solving LSIP problems but do not explore or compare with other potential methods that could offer different tradeoffs in terms of efficiency or accuracy.  Assumptions and Generalizability: The reliance on specific assumptions such as the nature of the constraints being Lipschitz continuous and the environment being known may limit the generalizability of the results to broader classes of problems where these assumptions do not hold.  3) Questions and Suggestions for the Authors  Could the authors elaborate on the potential computational bottlenecks of the SICRL algorithm when applied to highdimensional state and action spaces and suggest possible optimizations  It would be beneficial if the paper included comparisons with other methods for solving semiinfinitely constrained optimization problems to contextualize the efficiency and effectiveness of the dual exchange method used.  The paper assumes a known environment for the theoretical analysis and experiments. Could the authors discuss the implications of this assumption and how the algorithm might be adapted or extended to handle environments with uncertain or dynamically changing dynamics  Limitations and Constructive Suggestions for Improvement The authors have made substantial efforts to address limitations including detailing the assumptions required for their theoretical analysis and the specific scenarios where their model applies. However the potential negative societal impact particularly in the context of the discharge of sewage example could be more thoroughly explored. For instance optimizing for constraints in environmental applications should consider longterm and indirect effects beyond immediate constraints.  SocioEnvironmental Considerations: A discussion on the broader socioenvironmental implications of applying SICMDP in scenarios like pollution control and resource distribution could enrich the paper. This would include potential risks of optimizing for immediate constraints at the expense of longterm environmental health.  Ethical and Fairness Considerations: In scenarios involving optimization in public sectors (e.g. resource allocation) an analysis of ethical considerations and fairness implications would be valuable. Using utility functions and constraints that account for equity could be one avenue for exploration. In summary while the paper contributes significantly to the field of reinforcement learning with constraints further work exploring the computational efficiency scalability alternative methods and broader societal impacts of the proposed SICMDP framework would enhance its value.", "wQ2QNNP8GtM": " Peer Review of \"Cross Aggregation Transformer for Image Restoration\"  1. Summary of the Paper The paper presents a novel image restoration model named Cross Aggregation Transformer (CAT) aimed at overcoming the limitations of applying Transformer architecture to image restoration tasks typically dominated by convolutional neural networks (CNNs). Traditional Transformer methods suffer from high computational complexity and inadequate interwindow interaction when attempting to model longrange dependencies. The CAT model introduces a RectangleWindow SelfAttention (RwinSA) mechanism which operates attention over horizontal and vertical rectangle windows in parallel to expand the attention scope without increasing computational demand significantly. Additionally the model incorporates an AxialShift operation to enhance the interaction among different windows and a Locality Complementary Module (LCM) that integrates the strengths of CNNs into the Transformer framework by adding local spatial information processing capability. The authors report that their approach achieves stateoftheart results on several image restoration tasks including superresolution (SR) JPEG compression artifact reduction and real image denoising.  2. Assessment of the Paper Strengths:  The paper tackles a relevant and challenging problem in the field of image restoration aiming to bridge the gap between the global modeling capabilities of Transformers and the local feature extraction prowess of CNNs.  The introduction of the RectangleWindow SelfAttention mechanism is innovative and effectively addresses the computational complexity issues associated with traditional Transformer models while enabling more efficient longrange dependency modeling.  Incorporating the AxialShift operation and the Locality Complementary Module are also significant contributions enhancing interwindow interactions and embedding local spatial information within the Transformer framework respectively.  The comprehensive experimental evaluations across multiple datasets and tasks including comparisons with stateoftheart methods provide solid evidence of the models effectiveness and superiority. Weaknesses:  The paper does not fully discuss the architectural details and the theoretical underpinnings of the proposed CAT model particularly how the different components (RwinSA AxialShift and LCM) interact within the overall architecture.  While the paper claims computational efficiency it lacks a detailed analysis of the models computational complexity especially in comparison to existing methods. A deeper exploration of the tradeoffs between performance improvements and computational overhead would be beneficial.  The impact of the models various components on performance is not thoroughly dissected. An ablation study detailing the individual and combined effects of the RwinSA AxialShift operation and LCM on the models performance could provide clearer insights into their relevance and effectiveness.  There is minimal discussion on the challenges or limitations encountered during the development of the CAT model or on the scenes or conditions under which the model might underperform.  3. Questions and Suggestions for the Authors  Could the authors provide more detailed architectural diagrams and theoretical explanations of the CAT models components to better understand their interactions and contributions to the models overall performance  Would it be possible to include a more detailed computational complexity analysis comparing CAT with both traditional CNNbased and other Transformerbased image restoration models  An expanded ablation study highlighting the individual contributions of the RwinSA the AxialShift operation and the LCM towards the final performance metrics could enrich the evaluation section.  Discussing potential challenges limitations and underperforming scenarios of the CAT model could offer a more balanced view and suggest directions for future research.  4. Limitations and Societal Impact The authors have not explicitly addressed the limitations or the potential negative societal impacts of their work. Given the broad applicability of image restoration considering scenarios where enhanced image clarity could lead to privacy concerns or the misuse of restored images would be worth discussing. To address this the authors could consider adding a section on ethical considerations discussing scenarios where the technology might be misused and suggesting guidelines for the responsible use of image restoration technologies. Suggestions for Improvement:  A more thorough exploration and discussion of the model\u2019s limitations and potential negative impacts coupled with guidelines for ethical and responsible use would enhance the papers completeness and societal relevance.", "rH-X09cB50f": " Review of \"Understanding CrossDomain FewShot Learning: An Empirical Study\"  1) Summary of the Paper and Its Contributions This paper presents an indepth empirical study on the effectiveness of pretraining strategies in CrossDomain FewShot Learning (CDFSL) a scenario where the source and target domains diverge significantly. The key focus is on comparing supervised learning (SL) on the source domain and selfsupervised learning (SSL) on the target domain along with introducing mixedsupervised learning (MSL) and a twostage learning approach for pretraining. Through extensive experiments across various datasets the paper identifies scenarios where SSL surpasses SL especially when domain similarity is low or the target domain presents low fewshot difficulty. It contributes to the field by (a) challenging previous assertions about the ineffectiveness of SSL in CDFSL (b) quantitatively defining domain similarity and fewshot difficulty and (c) offering a nuanced understanding of when and how different pretraining strategies might synergize for enhanced performance in CDFSL tasks.  2) Assessment of Strengths and Weaknesses Strengths:  Empirical Rigor: The paper conducts systematic experiments across multiple source and target datasets providing a strong empirical foundation for its findings.  Novel Insights: By debunking previously held beliefs about SSL in CDFSL the paper contributes novel insights that could significantly impact future research directions and applications.  Practicality: Introducing and analyzing the utility of MSL and twostage learning approaches offer practical pathways for leveraging unlabeled data effectively in realworld scenarios. Weaknesses:  Generalization of Findings: While the paper presents extensive empirical results the generalizability of these findings across all possible CDFSL scenarios (including unseen datasets or domains) remains relatively unexamined.  Lack of Theoretical Support: The paper primarily focuses on empirical findings without delving into the theoretical underpinnings that could explain why certain pretraining strategies work better than others under specific conditions.  Potential Overfitting Concerns: In experiments especially those involving finetuning of hyperparameters (e.g. in MSL) theres limited discussion on the risk of overfitting to the target datasets which could influence the replicability of results.  3) Questions and Suggestions for the Authors  Theoretical Considerations: Can the authors provide any theoretical insights or models that could help explain the observed empirical phenomena particularly the effectiveness of SSL in certain CDFSL scenarios  Generalization Across Domains: How do the authors anticipate their findings will generalize to domains significantly different from those tested such as audio or text data in CDFSL settings  Overfitting Checks: Could the authors comment on any steps taken to mitigate overfitting especially in scenarios with hyperparameter optimization for the MSL and twostage learning approaches  4) Limitations and Societal Impact The authors have not deeply explored the limitations relating to the generalizability of their findings across vastly different domains or the potential overfitting to specific benchmark datasets. Furthermore there is little discussion on the potential negative societal impact such as the misuse of learned models in sensitive or biased applications across domains. It would be beneficial for the authors to include a broader discussion on these aspects perhaps suggesting guidelines for responsibly applying their findings in diverse realworld settings. Additionally considering the increasing concern around data privacy exploring how these pretraining strategies could be adapted or regulated to respect user privacy would add valuable depth to the discussion.", "s6ygs1UCOw1": " Peer Review Report  1) Brief Summary of the Paper and its Contributions This paper introduces a novel algorithm for creating Probably Approximately Correct (PAC) prediction sets within the context of meta learning. The authors propose a method that constructs prediction sets tailored for new tasks with a minimal number of training examples while maintaining a PAC guarantee with high probability for future tasks. This approach is tested across domains including healthcare autonomous vehicles and natural language processing showcasing its versatility and effectiveness. The algorithms significant contributions include:  Introducing a method to construct prediction sets that could adapt with few examples to new tasks.  Demonstrating the efficiency of their approach on datasets from various domains outperforming baselines in terms of PAC guarantee satisfaction and prediction set size.  Addressing a gap in existing meta learning predictions by ensuring correctness guarantees hold with high probability over future tasks.  2) Assessment of the Papers Strengths and Weaknesses Strengths:  Novelty: The algorithm presents a unique take on PAC prediction sets in meta learning focusing on adaptation to new tasks with limited examples.  Broad Applicability: The evaluation covers multiple domains illustrating the methods versatility and potential for various applications.  Empirical Results: Comparative analysis shows that the proposed method outperforms existing techniques backing theoretical claims with empirical evidence.  Theoretical Foundation: The paper thoroughly explains the theoretical underpinnings providing a solid rationale for the approach. Weaknesses:  Generalization to RealWorld Settings: While the paper discusses applications in theory it could benefit from indepth case studies or partnerships with domainspecific applications to further validate the efficacy in realworld scenarios.  Handling of Data Imbalance: The methodologys performance in the presence of highly imbalanced datasets is not clear a common challenge in practical applications.  Insufficient Discussion on Computational Complexity: There is a lack of detail regarding the computational demands of the proposed algorithm which could be crucial for its adoption in largescale applications.  Potential Misuse: While briefly touched upon the paper could expand on the possible misuse of the guarantees provided by the algorithm especially in sensitive domains like healthcare.  3) Questions and Suggestions for the Authors  Data Imbalance: Can the authors elaborate on how the algorithm performs under highly imbalanced datasets and whether additional modifications are necessary  Computational Complexity: It would be beneficial if the authors could provide insights into the computational requirements of their approach. Does this algorithm scale well with large datasets  RealWorld Applications: Are there ongoing or planned realworld applications of this algorithm Insights or preliminary results from such studies would significantly enhance the papers impact.  Clarification on Misuse: Could the authors provide more examples or scenarios where the proposed algorithms guarantees might be misunderstood or misused particularly in critical applications  4) Limitations and Societal Impact The authors have acknowledged that the approach relies on having sufficient calibration data points to satisfy the PAC guarantee which might limit its application in scenarios with scarce data. Additionally they mention a potential negative societal impact revolving around misuse of the algorithms guarantees. To mitigate these concerns the paper could benefit from:  Providing a clear guideline or framework on assessing the sufficiency of calibration data for effective application of the algorithm.  Expanding the discussion on ethical considerations and safeguards against the misuse of the algorithm especially in critical domains such as healthcare and autonomous driving. In conclusion this paper makes a valuable contribution to the field of meta learning by addressing the challenge of uncertainty quantification. However exploring its realworld applicability computational demands and ethical considerations in greater depth could enhance its impact and facilitate its practical implementation.", "kxXvopt9pWK": " PeerReview of \"Denoising Diffusion Restoration Models for Linear Inverse Problems in Imaging\"  1) Briefly summarize the paper and its contributions The paper introduces Denoising Diffusion Restoration Models (DDRM) a novel efficient and unsupervised method for solving linear inverse problems in image restoration tasks such as superresolution deblurring inpainting and colorization. Leveraging a pretrained denoising diffusion generative model and variational inference principles DDRM effectively samples from the posterior distribution of natural images given the measurements. This approach allows it to operate in a problemagnostic manner without the need for supervised training on specific degradation models. The authors demonstrate that DDRM surpasses current unsupervised methods in terms of reconstruction quality perceptual quality and runtime efficiency on several image datasets notably achieving significant speedups (5\u00d7 faster) compared to competing methods on the diverse ImageNet dataset.  2) Assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The paper successfully integrates diffusion models with linear inverse problem solving presenting an original unsupervised solution that does not rely on retraining for specific problems. This generalizability is a major advantage for applications in realworld settings where the exact degradation model may not always be known in advance.  Comprehensive Evaluation: The evaluation covers a wide range of tasks (e.g. superresolution inpainting) and datasets including challenging cases out of the training distribution. The extensive experimental section including comparisons with stateoftheart methods and qualitative examples provides solid evidence of the methods effectiveness.  Efficiency: The demonstrated efficiency (5\u00d7 faster than nearest competitors on ImageNet) without compromising on output quality is a noteworthy achievement. This makes DDRM particularly appealing for realworld applications where computational resources or time may be limited. Weaknesses:  Limitation in Addressing Nonlinear Inverse Problems: While the paper presents a significant contribution to linear inverse problems it does not extend to nonlinear inverse problems which represent a broad class of realworld challenges.  Lack of Discussion on Failures or Limitations: The paper could benefit from a more detailed discussion on scenarios where DDRM might fail or perform suboptimally offering insights into potential edge cases or limitations of the approach.  3) Questions and Suggestions for the Authors  Scalability to Higher Dimensions: How does DDRM scale with increasing image resolution or dimensionality It would be valuable to include experiments or discussions on the scalability especially for highresolution medical imaging applications.  Comparison with Supervised Methods: While DDRM outperforms unsupervised methods how does it compare with stateoftheart supervised methods in terms of quality and efficiency Including such comparisons could help contextualize the tradeoffs between supervised and unsupervised approaches.  Handling of Dynamically Changing Degradation Models: Could the authors elaborate on how DDRM can adapt to dynamically changing degradation models in a streaming or realtime processing context Any insights or potential adaptations of DDRM for such scenarios would be of interest.  Limitations and Societal Impact The authors have not fully addressed the limitations and potential negative societal impacts of their work. For example the paper does not discuss the potential misuse of image restoration technologies such as in creating deepfakes or unauthorized surveillance. Moreover while the efficiency of DDRM is highlighted the environmental impact of training largescale diffusion models is not discussed. Suggestions for Improvement:  Ethical Considerations: It would be advantageous for the authors to include a discussion on the ethical implications of their technology especially concerning privacy consent and possible misuse. Providing guidelines or suggestions for ethical use can help mitigate these concerns.  Environmental Impact: Acknowledging the computational cost and environmental impact of training the underlying models the authors could explore or suggest ways to reduce this footprint such as using more efficient architectures or promoting model sharing.", "x5ysKCMXR5s": "Review of \"Sparse PCA of Incomplete and Noisy Data via SDP Relaxation\" 1) Summary of the Paper This paper introduces a novel approach to Sparse Principal Component Analysis (PCA) applied to incomplete and noisy datasets. The core of the proposed method is to utilize a semidefinite program (SDP) relaxation of the l1regularized PCA problem to address data sparsity and incompleteness issues that are common in practical applications such as gene expression analysis. The primary contribution is the theoretical framework and empirical proof that SDP relaxation can exactly recover the true support of the sparse leading eigenvector from an incomplete and noisy dataset under certain conditions. These conditions include matrix coherence parameters the spectral gap between the largest and secondlargest eigenvalues observation probability and noise variance. The paper thoroughly validates these theoretical findings with both synthetic data and realworld gene expression data demonstrating the methods superiority over other sparse PCA techniques. 2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Depth: The paper contributes significantly to the sparse PCA literature by addressing the incomplete data problem with an SDP relaxation approach. The theoretical underpinning especially the provision of sufficient conditions for exact recovery adds considerable value.  Empirical Validation: The comprehensive experiments on synthetic and real datasets not only validate the theoretical claims but also demonstrate the practical applicability of the method. The comparison with existing sparse PCA techniques showcases the methods effectiveness.  Clarity and Rigor: The paper is wellstructured with clear definitions assumptions and explanations of the SDP approach. The use of notation and mathematical formalism is consistent contributing to the readability and comprehension of the proposed method. Weaknesses:  Generalization of Results: While the method shows promising results on the gene expression dataset it is unclear how well it generalizes to other types of incomplete and noisy datasets particularly those not characterized by uniformly missing data.  Computational Complexity: The paper does not thoroughly address the computational complexity of the SDP relaxations which could be a concern for very large datasets. A more detailed discussion on scalability and computational demands would be beneficial.  Limitation on Data Assumptions: The assumption of data missing uniformly at random might not hold for all realworld datasets limiting the applicability of the proposed method in scenarios where data is missing in a structured or nonrandom manner. 3) Questions and Suggestions for the Authors  Could the authors discuss the potential of their method to handle datasets where data is missing not at random (MNAR) This would provide insight into the robustness and versatility of the method.  A deeper analysis into the methods scalability and computational efficiency possibly including comparisons with other methods in terms of computational resources and run time would enhance the papers practical implications.  The paper could benefit from a broader discussion of potential implications and applications beyond the gene expression dataset. For instance exploring the methods performance on other biomedical datasets or different domains could highlight its general applicability. 4) Limitations and Societal Impacts The authors have addressed the limitations related to the assumptions of uniformly missing data and have primarily validated their approach on synthetic and a single realworld dataset. However a more comprehensive discussion on the limitations concerning scalability computational complexity and generalizability to other domains would be valuable. The potential negative societal impact seems minimal but there could be ethical considerations if the method is applied to sensitive datasets (e.g. genetic data) where incomplete data might introduce bias or privacy concerns. The authors could enhance the paper by providing guidelines on ethical considerations and suggesting mitigation strategies for potential biases introduced by data incompleteness and noise.", "qOgSCLE5E8": " Peer Review for \"Adaptive Distribution Calibration for FewShot Learning with Hierarchical Optimal Transport\"  1. Summary of the Paper The paper introduces a novel technique for tackling fewshot classification challenges an area where traditional classifiers suffer due to the scarcity of training data for novel classes. The authors method revolves around the concept of distribution calibration  specifically calibrating the feature distribution of fewsample classes by leveraging statistics from base classes enriched with ample examples. The novelty of their approach lies in the introduction of a hierarchical Optimal Transport (HOT) framework to determine adaptive weight matrices for transferring statistics between base and novel classes. The method aims to overcome the limitations of existing approaches by providing a more principled solution to decide the transfer weights. Experimental validation on standard benchmarks indicates that their model not only achieves superior performance when compared to existing methods but also exhibits commendable crossdomain generalization capabilities.  2. Assessment of Strengths and Weaknesses  Strengths:  Innovativeness: The idea of using hierarchical Optimal Transport for adaptive distribution calibration in fewshot learning is novel and addresses the shortcomings of previous approaches effectively.  Theoretical Robustness: The paper offers a thorough theoretical exposition of the proposed HOT method including the rationalization behind using hierarchical OT for learning adaptive transfer weights and the cost function between base and novel classes.  Empirical Validation: Comprehensive experimentation demonstrates the efficacy of the proposed method over current stateoftheart techniques in fewshot classification across several benchmarks.  CrossDomain Generalization: The models ability to generalize across different domains without significant performance degradation is a substantial advantage especially in realworld applications where training and target data often differ.  Weaknesses:  Computational Complexity: The introduction of high and lowlevel optimal transport problems could potentially increase computational overhead. The paper does not thoroughly discuss the computational implications of their approach.  Hyperparameter Sensitivity: The performance of HOT like many ML algorithms might be sensitive to the choice of hyperparameters (e.g. the entropic regularization term). The paper does not provide an extensive sensitivity analysis or guidelines on choosing these hyperparameters.  Limited Discussion on Failure Cases: While the paper presents impressive results there is scant discussion on scenarios where HOT might fail or underperform which could be crucial for practical applications.  Questions and Suggestions:  Could the authors elaborate on the computational complexity of HOT compared to conventional distribution calibration methods Any insights on making HOT more computationally efficient would be valuable.  It would be interesting to see a sensitivity analysis regarding the choice of hyperparameters especially the entropic regularization term to guide practitioners in applying HOT to new problems.  A discussion on potential failure cases or limitations of the proposed method in specific fewshot learning scenarios would help in understanding the methods applicability scope better.  3. Limitations and Societal Impact The authors briefly touch upon the limitations related to computational efficiency and potential sensitivity to hyperparameters. However a more indepth discussion or quantification of these aspects could provide clearer guidance to practitioners. On the subject of negative societal impacts the paper does not delve into this area. Its crucial to assess how the deployment of such models might affect fairness privacy or amplify biases in automated decisionmaking processes. Suggestions for improvement include incorporating a detailed analysis of the models behavior across diverse demographic groups or sensitivity settings and proposing mechanisms to mitigate any identified adverse effects.  Conclusion The paper presents a significant advance in fewshot learning through the development of an adaptive distribution calibration method underpinned by a hierarchical Optimal Transport framework. While it demonstrates promising results addressing its computational aspects and further elucidating on its limitations and societal implications would make the contribution more comprehensive and impactful.", "pcgMNVhRslj": "Review of \"Alignmentguided Temporal Attention for Video Learning Tasks\" 1. Summary: This paper introduces Alignmentguided Temporal Attention (ATA) a novel approach focusing on the challenge of efficiently and effectively managing the temporal information present in video frames for action recognition tasks. The authors identify a core issue in existing methods  the tradeoff between the computational efficiency of factorized (2D1D) spatialtemporal operations and the higher performance but computationally demanding joint (3D) spatialtemporal operations. They propose that direct interactions among similar parts of adjacent frames through ATA can significantly increase mutual information between frame representations thus enhancing the extraction of taskrelevant information without excessive computational costs. The method is validated across multiple benchmark datasets showing superior performance to existing approaches. 2. Assessment: Strengths:  The paper addresses a pertinent issue in video understanding tasks and provides a theoretical basis for the proposed ATA module convincingly demonstrating how it can resolve the identified problem.  The ATA approach is practical with a low additional computational cost and no extra parameters making it an efficient solution.  Experimentation is thorough covering multiple benchmark datasets and comparison against a comprehensive set of stateoftheart methods. This validates the effectiveness and generality of the ATA module.  The idea of exploiting framebyframe alignment to increase mutual information between frame representations is novel and welljustified with both theoretical and empirical evidence supporting the efficacy of the approach. Weaknesses:  The paper does not provide a detailed discussion on the computational complexity of the proposed solution in comparison to both factorized and joint spatialtemporal operations which would be important to comprehensively understand its efficiency claims.  While the ATA module is compatible with various image backbones the paper lacks an indepth analysis of its performance variability across different architectures which could provide insights into any potential limitations or considerations for its application.  The nondifferentiable nature of the ATA module and its parameterfree design may limit optimization opportunities during training which could cap its performance potential compared to alternative trainable solutions. 3. Questions and Suggestions:  Could the authors elaborate on the computational complexity of ATA in comparison to both factorized and joint spatialtemporal operations providing more detailed benchmarks on computational resources required  It would be informative to see an analysis of ATA\u2019s performance across different image backbones identifying any patterns in which backbones it works best with or where it might underperform and discussing possible reasons.  Given ATA\u2019s nondifferentiable design have the authors considered differentiable approaches that might allow for endtoend training and optimization I would encourage exploring this avenue to potentially enhance the methods performance further.  The potential for negative societal impacts is not discussed. Given the application domain considerations around privacy consent and misuse especially in surveillance should be acknowledged and addressed. 4. Limitations and Constructive Suggestions for Improvement: The authors have not thoroughly addressed the computational complexity of ATA nor its varying performance across different backbone architectures. A more detailed exploration of these aspects would strengthen the paper. Additionally the discussion around the modules nondifferentiable nature and lack of parameterization leaves open questions about possible performance ceilings and opportunities for improvement. Constructive suggestions include extending the evaluation to cover a broader range of backbone architectures and providing computational benchmarks against other methods to clarify efficiency claims. Exploring differentiable versions of ATA could unveil opportunities for performance gains. Lastly addressing potential negative societal impacts directly related to the applications of video action recognition technologies would provide a more rounded and responsible discourse. Overall this paper makes significant strides in advancing temporal modeling for video learning tasks with promising results and clear potential for impact in the field.", "uxc8hDSs_xh": "Review of \"A Geometric ScatteringBased Graph Neural Network for Approximating Maximum Cliques\" 1. Summary of the Paper This paper introduces a novel geometric scatteringbased Graph Neural Network (GNN) methodology aimed at approximating solutions for the NPhard maximum clique (MC) problem efficiently. The authors propose a twoterm loss function that enhances the detection and formation of highly connected nodes that constitute a clique. This is achieved through a specialized GNN architecture that leverages geometric scattering to alleviate the oversmoothing issue commonly seen in traditional GNNs. The methodology includes a rulebased decoder that utilizes the probability vector generated by the GNN to predict the constituents of the maximum clique. Empirical evaluations demonstrate that this approach surpasses existing GNN baselines and even conventional solvers like Gurobi under restricted time conditions in terms of both accuracy and inference speed while maintaining a significantly lower parameter count. 2. Assessment of Strengths and Weaknesses  Strengths:  Novel Approach: The incorporation of geometric scattering to tackle the oversmoothing problem in GNNs represents a significant and innovative stride towards solving complex NPhard problems.  Efficiency: Both in terms of parameter count and computational demands the proposed method showcases remarkable efficiency a critical aspect for scalability and practical application.  Empirical Validation: The authors present compelling empirical evidence that their approach not only outperforms existing methods in accuracy and speed but also offers a pragmatic solution under limited computational resources.  Weaknesses:  Generalizability Concerns: While the results are promising the paper primarily focuses on the maximum clique problem. Additional insights into the models adaptability or performance on a wider range of graphbased problems would enrich its contribution.  Decoder Design: The reliance on a greedy rulebased decoding step might limit the models capability to generalize across diverse graph structures or sizes potentially affecting the overall robustness of the solution. 3. Questions and Suggestions for the Authors  Question: The paper claims significant reduction in parameter count. Could the authors discuss how the models performance scales with increasing graph sizes or complexity  Suggestion: Investigating the models adaptability to other graphbased NPhard problems could amplify its contribution and applicability providing a broader perspective on its capabilities.  Suggestion: A deeper exploration into the rulebased decoders design and potential alternatives could offer insights into enhancing the models generalization and accuracy across varied datasets. 4. Limitations and Potential Negative Societal Impact The authors have outlined the limitations related to the specific focus on the maximum clique problem and the potential drawbacks of the decoding mechanism. However a discussion on the broader implications particularly regarding the potential misuse of powerful optimization tools or the energy consumption implications of deploying such models at scale would have been beneficial. Suggestions for Improvement:  Expanding the discussion on the societal and ethical considerations including potential misuse and environmental impact would provide a more holistic view of the proposed methodology.  Proposing strategies to mitigate identified limitations or potential negative impacts such as incorporating energyefficiency considerations into the model design or devising safeguards against misuse could further strengthen the papers contribution. Overall the paper presents a promising and innovative approach to addressing the challenging maximum clique problem. With certain enhancements particularly in broadening the models applications and addressing the highlighted limitations the work has the potential to make a substantial impact in the field of graph neural networks and combinatorial optimization.", "pELM0QgWIjn": " Review of the Paper on QuasiNewton Methods for StronglyConvexStronglyConcave Saddle Point Problems  Summary of the Paper The paper introduces a novel framework for solving stronglyconvexstronglyconcave saddle point problems by advancing quasiNewton methods tailored to the indefinite nature of the Hessian matrix in such problems. Specifically it proposes random updates from the Broyden family leveraging the square of the indefinite Hessian matrix significantly diverging from traditional quasiNewton methods in convex optimization. Two variants based on BFGStype and SR1type updates are highlighted for their enhanced local superlinear convergence rates backed by rigorous theoretical analysis and numerical experimentations showcasing superiority over classical firstorder methods in machine learning models.  Assessment of Strengths and Weaknesses Strengths: 1. Novel Approach: The adaptation of quasiNewton methods by approximating the square of the Hessian matrix for saddle point problems is innovative addressing the challenge of indefinite Hessians. 2. Theoretical Contributions: The paper offers a sound theoretical foundation providing explicit local superlinear convergence rates for the proposed methods a significant advancement over existing literature. 3. Practical Relevance: The application of these algorithms in various fields such as game theory machine learning and robust optimization is well justified demonstrating the broad impact of the work. 4. Superior Performance: Through meticulous numerical experiments the authors validate the superior performance of their algorithms against classical firstorder methods underscoring the practical value of their contributions. Weaknesses: 1. Computational Cost and Scalability: Despite improvements over classical secondorder methods the O(d2) space complexity for storing the approximate Hessian may still pose scalability issues for very highdimensional problems. 2. Limited Experimentation: The experimentation although thorough is somewhat limited in scope. Expanding the experiments to include additional machine learning models or realworld datasets could further establish the superiority of the proposed methods. 3. Comparison with ThirdParty Methods: The comparison mainly focuses on classical firstorder methods. Including comparisons with other stateoftheart secondorder or hybrid methods could have offered a more comprehensive performance evaluation.  Questions and Suggestions for the Authors 1. Scalability Concerns: Have the authors considered the applicability of their method in extremely highdimensional settings where storing the approximate Hessian may not be feasible Suggestions: Exploring limitedmemory variants of the proposed algorithms could be beneficial. 2. Broader Experimentation: Could the authors elaborate on the choice of machine learning models for experimentation Suggestions: Please consider including more diverse machine learning tasks potentially from different domains to validate the algorithms versatility. 3. ThirdParty Comparisons: It would be informative to understand how the proposed methods perform relative to modern quasiNewton or hybrid optimization methods. Suggestions: Expand the scope of comparison to include a few stateoftheart optimization techniques.  Limitations and Potential Negative Societal Impact The authors have addressed the limitations related to the computational cost of their algorithms. However further discussions on scalability issues especially for extremely largescale problems would be appreciated. As for potential negative societal impacts the paper does not delve into any specifics. Given the application domains mentioned (e.g. fairness in machine learning robust optimization) it is crucial to consider the broader implications of utilizing these optimization techniques. For instance in fairnessaware machine learning the optimization methods convergence rate could indirectly affect the fairness metrics of the trained models. Suggestions for improvement include a dedicated section discussing the ethical implications and potential negative impacts providing a holistic view of the technologys societal aspects.", "m8vzptcFKsT": "1) Summary of the Paper The paper embarked on a comprehensive analysis of average robustness in deep neural networks (DNNs) across various architectures and training paradigms. It dissected how factors such as network width depth and training regime (lazy vs. nonlazy) influence the robustness of neural networks to adversarial perturbations. By developing a notion of perturbation stability the study presents theoretical findings on how overparameterization and underparameterization affect robustness differently. The authors highlighted the complex interplay between network initialization and architecture presenting specific conditions under which network depth and width enhance or impair robustness. The theoretical findings were empirically validated on standard datasets confirming the nuanced impact of these factors on DNN robustness. 2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper contributes to the understanding of DNN robustness by offering a novel perspective on how different factors like width depth and training modes impact average robustness.  Broad Analysis: The extensive analysis covering various network architectures and training regimes provides a comprehensive view likely appealing to both theoreticians and practitioners.  Theoretical Contribution: The derivation of upper bounds for perturbation stability and the conditions for the transition between lazy and nonlazy training regimes are significant theoretical contributions.  Empirical Validation: The theoretical claims are substantiated with empirical evidence strengthening the papers credibility and relevance to realworld applications. Weaknesses:  Complexity and Accessibility: The theoretical depth while a strength also makes the paper less accessible to a broader audience potentially limiting its impact.  Limited Exploration of Practical Implications: While theoretical insights are provided the paper could benefit from a deeper exploration of the practical implications of its findings especially in terms of how it influences the design of robust DNNs.  Discussion on Limitations: The paper could elaborate more on its limitations particularly concerning the assumptions made for the theoretical analysis and how they may limit the generalizability of the findings. 3) Questions and Suggestions for the Authors  Clarification on Assumptions: Can the authors clarify the assumptions made concerning the data distribution and network initialization and discuss how these assumptions may influence the applicability of their results in practical settings  Broader Impact Discussion: Considering the potential societal impacts of robust DNNs especially in critical applications could the authors elaborate on any ethical considerations or negative impacts that their findings might have  Further Analysis on Initialization Methods: The paper states different impacts based on initialization methods (LeCun He and NTK). It would be enriching to see a more detailed analysis or a direct comparison to understand better how these initialization methods influence robustness distinctly.  Exploration of Other Architectures: The current study focuses on fullyconnected ReLU networks. Could the authors speculate or provide insights on how their findings might extend to other network architectures such as convolutional neural networks (CNNs) or transformers 4) Addressing Limitations and Potential Negative Societal Impact The authors provide an analysis centered around theoretical conditions and empirical evidence within controlled setups. However an indepth discussion regarding the limitations of these conditions and how they translate to realworld applications is somewhat lacking. Particularly the assumptions about data distribution and initialization could limit the findings applicability. Offering constructive suggestions extending the analysis to more diverse architectures and realworld datasets could make the insights more generalizable. Regarding potential negative societal impacts while the paper contributes valuable knowledge towards understanding DNN robustness it doesn\u2019t delve into the implications of deploying such robust models in sensitive or highstakes applications. Constructively the authors could enhance their paper by discussing how increased robustness may interact with issues of fairness privacy and security in AI systems providing guidelines or recommendations for ethical considerations in the application of their findings.", "zFW48MVzCKC": " Review of \"Multiagent QLearning with SubTeam Coordination (QSCAN): A Novel Value Factorization Framework\" 1. Summary This paper introduces a novel valuebased multiagent reinforcement learning framework called QSCAN (multiagent Qlearning with Subteam CoordinAtioN) addressing the challenge of central training with decentralized execution (CTDE) in cooperative multiagent reinforcement learning (MARL) tasks. Key contributions include:  A novel value factorization framework QSCAN that flexibly represents subteam coordination while satisfying the IndividualGlobalMax (IGM) condition. It allows for efficient factorization of the global action value into individual and subteam action values enhancing learning efficiency and coordination in decentralized execution.  The introduction of two implementations QPAIR and QSCAN which are based on pairwise coordination and selfattention mechanisms respectively to demonstrate the frameworks flexibility and efficiency in various MARL coordination tasks.  Empirical validation of QSCANs performance across several benchmarks including matrix games predatorprey tasks the Switch challenge and the StarCraft MultiAgent Challenge (SMAC). Results indicate that QSCAN either outperforms or is comparable to stateoftheart approaches like QMIX and QPLEX. 2. Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The proposed QSCAN framework is a fresh approach to factorizing value in MARL addressing limitations of previous methods by efficiently leveraging subteam coordination patterns.  Comprehensive Evaluation: The empirical evaluation is thorough covering a range of scenarios that convincingly demonstrate the versatility and efficacy of QSCAN across different coordination complexities.  Theoretical Insight: The paper provides substantial theoretical analysis including the formal definition of the QSCAN framework and proofs for the satisfaction of the IGM condition \u2013 offering a strong rationale for the proposed method. Weaknesses:  Generalization to Complex Scenarios: While QSCAN performs well in the tested environments its applicability and performance in even more complex or dynamically changing environments (e.g. with a varying number of agents or subteams) remain less explored.  Computational Efficiency Concerns: The introduction of subteam coordination might introduce computational overhead especially in scenarios with a large number of agents. Detailed analysis of computational resources and scalability is not discussed.  Lack of Comparative Baseline Discussion: Although QSCAN is compared against established methods like QMIX and QPLEX the paper could benefit from a more indepth discussion contrasting the architectural choices and theoretical underpinnings between these frameworks. 3. Questions and Suggestions for the Authors  Extension to Dynamic Agent Configurations: How would QSCAN adapt to scenarios where agent membership within subteams or the number of agents in the environment changes dynamically Providing insights or extensions to handle such cases could enhance the applicability of QSCAN.  Computational Resource Utilization: Can the authors provide details on the computational efficiency of QSCAN especially regarding the use of the selfattention mechanism in largescale MARL settings  Impact of Hyperparameter Choices: The paper could benefit from a discussion on how different architectural choices (e.g. the number of attention layers in QSCAN) and hyperparameters affect performance and learning efficiency. 4. Limitations and Societal Impact The authors have made a commendable effort in addressing the theoretical and practical implications of their work. However the discussion on limitations and potential negative societal impacts appears to be missing. For improvement the authors could consider:  Addressing Potential Limitations: Discussing scenarios where QSCANs performance might degrade or its application could be challenging (e.g. environments with highly mutable agent roles or tasks with unclear subteam formation) would provide a more balanced view of the frameworks capabilities.  Societal Impact: Reflecting on the broader implications of enhancing MARL systems such as their use in surveillance or military applications and suggesting guidelines for ethical use would be beneficial. Conclusion Overall this paper contributes significantly to the field of MARL by introducing a flexible and theoretically grounded approach to value factorization through subteam coordination. Despite some areas for further exploration and clarification the work presents a promising avenue for advancing cooperative learning strategies in complex multiagent systems.", "qHs3qeaQjgl": "The paper introduces an advanced approach to testing constrained samplers over highdimensional distributions with an emphasis on (\u03b5 \\xce\\xb7 \u03b4) guarantees which are critical for evaluating the accuracy of sampling algorithms in machine learning applications. The authors identify the major limitation of the existing stateoftheart algorithm Barbarik2 which suffers from exponential query complexity in relation to the dimension n of the distribution. In response they propose Barbarik3 an exponentially faster algorithm with linear query complexity in n enabling scalability to larger instances. The empirical evaluation demonstrates the superiority of Barbarik3 over Barbarik2 with significant reductions in the required number of samples to test the accuracy of two samplers wSTS and wUnigen3 under realworld scenarios. Strengths: 1. Practicality and Scalability: The linear query complexity in n represents a significant improvement in the practical application of the algorithm to highdimensional distributions addressing a critical barrier in the field. 2. Empirical Evaluation: The thorough empirical analysis against existing samplers (wSTS and wUnigen3) and the use of realworld benchmark datasets provide substantial evidence of the effectiveness and efficiency of Barbarik3. 3. Open Source Tool: Making Barbarik3 available as an opensource tool increases its accessibility and potential for further development by the community. Weaknesses: 1. Limited Parameter Range: The requirement for the closeness parameter \u03b5 to be in the narrow range of [0 \\xce\\xb711.6) for a given farness parameter \\xce\\xb7 could limit the applicability of Barbarik3 in scenarios where a broader parameter range is desirable. 2. Comparative Analysis: While the comparison to Barbarik2 is comprehensive a broader comparison to other existing methodologies (if available) could have provided more context on the positioning of Barbarik3 within the field. 3. Consideration of Computational Resources: While sample efficiency is significantly improved the paper lacks a detailed discussion on computational resource efficiency i.e. memory and CPU time required by Barbarik3 in comparison to Barbarik2 and others. Questions and Suggestions for the Authors: 1. Can the authors comment on the computational resource efficiency of Barbarik3 compared to Barbarik2 and any measures taken to optimize this aspect 2. How does Barbarik3 perform under the broader range of \u03b5 and \\xce\\xb7 parameters and are there any plans to extend its applicability 3. Given the open problems mentioned in the conclusion are there specific theoretical or methodological directions the authors believe could bridge the existing gap between the upper and lower bounds of the query complexity 4. It would be beneficial to discuss any potential biases introduced by the testing methodology itself especially considering the use of specific samplers and benchmarks. Limitations and Societal Impact: The authors partially address the limitations of Barbarik3 particularly the narrower range of acceptable parameter values compared to Barbarik2. A more extensive discussion on potential biases or limitations in testing methodologies including possible impacts on wider societal aspects would strengthen the paper. As machine learning applications permeate critical areas such as medicine and transportation ensuring the reliability and fairness of sampling algorithms is crucial. Suggestions for improvement include a broader exploration of parameter spaces for \u03b5 and \\xce\\xb7 and an indepth analysis of computational efficiency regarding time and resources. Additionally discussing the potential for unintended consequences or biases in sampled datasets and measures to mitigate such risks would enhance the papers consideration of societal impacts.", "r0bjBULkyz": " Review of \"Active Bayesian Causal Inference (ABCI): Integrating Causal Discovery and Reasoning\"  1) Summary of the Paper The paper introduces Active Bayesian Causal Inference (ABCI) a fully Bayesian active learning framework that seamlessly integrates causal discovery and causal reasoning within a single process. This novel approach is designed to efficiently infer posterior distributions over causal models and queries of interest (e.g. causal effects of interventions) by sequentially designing and conducting experiments. The method specifically caters to causally sufficient nonlinear additive noise models and employs Gaussian processes for modeling along with a combination of continuous graph parametrizations and Bayesian optimization for efficient posterior inference and experiment design. Through simulations the authors demonstrate that ABCI outperforms several baselines in terms of data efficiency thereby effectively learning causal queries with fewer samples and providing accurate uncertainty estimates.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty and Integration: The paper addresses a crucial gap in causal inference by proposing a unified framework that combines causal discovery and causal reasoning rather than treating them as separate tasks. This integrated approach promises improved efficiency and coherence in drawing causal inferences.  Bayesian Formalism: By adopting a fully Bayesian perspective the framework naturally accounts for epistemic uncertainty which is critical in causal inference especially when dealing with limited or interventionally gathered data.  Data Efficiency: The approach is demonstrated to be more dataefficient compared to methods focusing solely on full causal graph discovery which is practically beneficial given the high costs associated with experimental data collection.  Scalability and Flexibility: The utilization of Gaussian processes Bayesian optimization and a continuous latent probabilistic graph model (DiBS) for causal graph inference enables scalability to relevant problem sizes and flexibility in modeling complex nonlinear causal relationships. Weaknesses:  Computational Demands: The fully Bayesian treatment especially with the inclusion of Gaussian processes and continuous graph parametrizations likely leads to high computational demands which may limit the applicability of ABCI to very large datasets or very complex models.  Assumptions on Model Class: The focus on causally sufficient nonlinear additive noise models while providing tractability and a clear demonstration may limit the generality of the ABCI framework. Realworld systems often exhibit heteroscedastic noise unobserved confounding and cyclic causal relationships which are not directly addressed in this framework.  Evaluation Scope: While the simulated experiments demonstrate the potential advantages of ABCI the evaluation lacks realworld validation. Applications to real datasets would significantly strengthen the claims of data efficiency and scalability.  3) Questions and Suggestions for the Authors  Handling of Model Assumptions: Could the authors discuss potential extensions or modifications of the ABCI framework to relax some of the model assumptions (e.g. causal sufficiency linearity) How would these modifications impact the computational feasibility and the theoretical guarantees of the approach  Comparison with NonBayesian Methods: The comparison is primarily against Bayesian or semiBayesian baselines. Including nonBayesian active causal discovery and reasoning approaches in the evaluation might provide a more comprehensive understanding of ABCIs relative performance.  RealWorld Applications: Are there specific domains or types of datasets where the authors anticipate ABCI will be particularly beneficial or challenging to apply Insights into how ABCI could be adapted or extended for such scenarios would be valuable.  Scalability Considerations: Given the computational demands what strategies do the authors recommend for scaling ABCI to larger problems Additionally could approximate or variational Bayesian methods be employed to alleviate computational burdens while retaining most of the frameworks benefits  4) Limitations and Potential Negative Societal Impact The authors partially address limitations related to model assumptions and computational demands. An explicit discussion on the potential negative societal impacts such as biased or incorrect causal inferences due to model misspecification or data quality issues would strengthen the paper. Suggestions to mitigate these risks include developing robustness checks for the causal model assumptions and encouraging transparency regarding the uncertainty in the causal inferences drawn using ABCI.", "yhlMZ3iR7Pu": "Review of \"Leveraging Denoising Diffusion Probabilistic Models for HighDimensional Data Inference with Auxiliary Constraints\" 1) Summary of the Paper This paper introduces a novel framework for inferring highdimensional data that leverages the capabilities of denoising diffusion probabilistic models (DDPMs) as priors combined with differentiable auxiliary constraints (c(xy)). By integrating DDPMs with constraints derived from additional information (y) this framework broadens the applicability of DDPMs to tasks beyond their initial training domain. These tasks include conditional image generation semantic image segmentation and continuous relaxations of combinatorial problems. The methodology relies on an iterative process that evaluates fitness across various noised versions of the data employing a mechanism that encourages the exploration of the data space to identify feasible solutions that adhere to both the DDPM\u2019s generated distribution and the specified constraints. The paper offers comprehensive experiments including image generation conditioned on digit attributes using MNIST and face attributes using the FFHQ256 dataset semantic segmentation with the EnviroAtlas dataset and an innovative application to solving the traveling salesman problem (TSP). 2) Strengths and Weaknesses Strengths:  Originality: The application of DDPMs as plugandplay priors for highdimensional data inference in a constraintdriven setting is highly innovative. It showcases the versatility of DDPMs beyond generative tasks extending their utility to optimization and even combinatorial search problems.  Broad Applicability: Demonstrating the framework\u2019s applicability across diverse domains (image generation segmentation and combinatorial optimization) underlines its potential impact on various fields within machine learning and AI.  Clear Exposition and Methodology: The paper employs a clear and concise explanation of the novel search mechanism and the integration of DDPMs with auxiliary constraints. The methodology is welldesigned and the algorithmic description allows for reproducibility. Weaknesses:  Inference Cost: As admitted in the conclusion a critical limitation is the high computational cost of inference due to the iterative nature of the DDPMbased generation process. This might limit its practical usability especially for tasks requiring realtime responses or operating under strict computational constraints.  Lack of Comparison with Alternatives: While the paper delivers a convincing argument about the utility of the proposed framework it falls short in comparing its performance against existing alternative approaches. Including such comparisons could better contextualize its advantages or improvements over current methods.  Model Complexity and Accessibility: The approachs reliance on DDPMs and gradientbased optimization through various noise levels might pose barriers to entry for practitioners not deeply familiar with these models. Simplification or guidance on navigating these complexities would enhance the paper\u2019s impact. 3) Questions and Suggestions for the Authors  Scalability Concerns: Can the authors comment on the scalability of the proposed algorithm especially concerning the number of constraints and the dimensionality of the data  Extension to NonDifferentiable Constraints: The current framework assumes differentiable constraints. Exploring extensions or adaptations to handle nondifferentiable constraints could significantly broaden the models applicability.  Improving Inference Efficiency: Have the authors considered mechanisms to reduce the computational overhead of the inference process Potential avenues might include leveraging recent advancements in efficient DDPM inference methods or exploring conditional generative models that require fewer iterative steps. 4) Limitations and Societal Impact The authors mention the high cost of inference as a limitation which is a critical point especially for applications requiring efficiency or operating under computational constraints. While the potential negative societal impacts of this work are not directly addressed it is essential to consider the broader implications of such powerful generative models. There could be concerns around misuse in generating deceptive imagery or information and future work should also consider mechanisms to ensure the ethical use of such technologies. Suggestions for Improvement:  The paper would benefit from a more detailed discussion on computational efficiency perhaps including a comparison with simpler baselines or outlining specific scenarios where the computational cost is justified by the quality of the results.  Including a section on potential safeguards or ethical considerations when applying this framework could help anticipate and mitigate possible negative societal impacts. Overall this paper presents a significant advancement in leveraging DDPMs for complex inference tasks across various domains. With further refinement especially on efficiency and broader comparisons this work could pave the way for novel applications of generative models in data inference and optimization.", "yTJze_xm-u6": "Review of \"Variational Model Perturbation for SourceFree Domain Adaptation\"  Summary: The paper presents a novel approach to sourcefree domain adaptation (SFDA) by introducing variational model perturbations. This method aims to adapt a pretrained model on source domains to target domains where the source data is not available and the target data is unlabeled. To achieve this the authors propose to perturb the weights of the source model slightly to encode uncertainties related to domain shifts within a variational Bayesian inference framework. This technique transforms a deterministic model into a Bayesian neural network (BNN) allowing it to generalize to new domains. It also introduces a parameter sharing strategy to reduce the number of learnable parameters making the adaptation process more efficient. Experimental results on several benchmarks demonstrate the effectiveness of this method compared to existing ones.  Strengths: 1. Novel Approach: The paper introduces a unique perspective by applying variational model perturbation to SFDA adding to the domain adaptation literature where such a probabilistic method was not explored before. 2. Theoretical Underpinning: The connection established between variational model perturbation and the theoretical framework of learning BNNs adds significant value providing a solid foundation for the proposed method. 3. Efficiency: The parameter sharing strategy substantially reduces the number of parameters needed for adaptation making the method attractive for practical applications where computational resources are limited. 4. Extensive Evaluation: The paper reports comprehensive experiments across several datasets and varying SFDA settings thoroughly demonstrating the effectiveness of the proposed method.  Weaknesses: 1. Method Complexity: While the paper presents a novel approach the complexity of implementing variational Bayesian inference and perturbation operations might pose challenges in adoption for practitioners or researchers unfamiliar with Bayesian methods. 2. Limited Comparison: The comparison with existing methods is extensive but does not include a broader range of recent SFDA methods that might employ different but equally effective strategies. 3. Lack of Discussion on Failure Cases: The paper presents impressive results but lacks a discussion on the limitations or scenarios where the proposed method might not perform as expected which is crucial for readers to understand the methods applicability.  Questions and Suggestions for Authors: 1. Implementation Complexity: Could the authors provide more insights into the practical difficulties one might face while implementing this method especially related to the variational Bayesian inference process 2. Comparison with Recent SFDA Methods: Are there recent SFDA methods that have not been compared against in this study How might the proposed method perform relative to these 3. Failure Cases: Can the authors discuss any failure cases or limitations observed during the experiments This would help in understanding the methods robustness. 4. Societal Impact: Given the focus on preserving privacy and intellectual property through SFDA how does the proposed method ensure that these objectives are met especially in sensitive applications  Limitations and Societal Impact: The authors have made an effort to address the limitations and potential negative societal impacts of their work especially concerning data privacy and intellectual property rights by focusing on sourcefree domain adaptation. However the paper could benefit from a more detailed discussion on potential misuse scenarios such as adversarial attacks exploiting the model perturbation. As for constructive suggestions it would be beneficial if future work could explore explicit mechanisms or safeguards to prevent such misuse enhancing the methods applicability in sensitive or critical applications.  Conclusion: The paper presents a compelling and theoretically grounded approach to sourcefree domain adaptation through variational model perturbations showing significant promise in various benchmarks. Despite its strengths the complexity and broader applicability could be explored further. Nonetheless this work contributes meaningfully to the domain adaptation literature and opens up new avenues for future research in probabilistic model adaptation methods.", "yKYCwTvl8eU": " Review of \"Learning Concept Credible Models for Mitigating Shortcut Learning\"  1) Summary This paper introduces a novel approach to mitigate shortcut learning in machine learning models by incorporating domain knowledge in the form of known concepts (C) and allowing for the learning of unknown concepts (U). The study identifies the challenge of models relying on spurious correlations (shortcuts) for predictions which may not hold under distribution shifts leading to poor generalization. Two methodologies Concept Credible Models (CCM) RES and CCM EYE are proposed to address this issue without solely depending on known concepts. The former utilizes a twostaged process incorporating residual learning from unknown concepts while the latter extends an existing regularization penalty to diminish the reliance on shortcuts effectively. The paper demonstrates the effectiveness of both approaches using two realworld datasets and outlines conditions under which shortcuts can be mitigated. The contribution lies in presenting a robust framework (CCM) for learning that combines domain knowledge with the flexibility to incorporate new relevant information for predictive tasks especially where generalization across variable distributions is crucial.  2) Assessment Strengths:  Originality: The approach of combining known and unknown concepts to mitigate shortcut learning is novel and addresses a critical challenge in machine learning\u2014ensuring models generalize well under distribution shifts.  Practical applicability: By applying the proposed methods to realworld datasets and demonstrating their effectiveness the paper highlights the practical relevance of CCMs particularly in the medical domain where distribution shifts are common.  Theoretical and empirical validation: The paper presents both theoretical conditions for the mitigation of shortcuts and empirical evidence supporting the methodologies effectiveness providing a comprehensive view of the solutions robustness.  Addressing previous limitations: The study acknowledges and effectively addresses the limitations of previous methods that either rely solely on known concepts or fail to distinguish between robust features and shortcuts. Weaknesses:  Assumptions and robustness: The paper relies on certain assumptions (e.g. A1 and A2) regarding the invariance and redundancy of known concepts relative to shortcuts. While the authors examine the methods sensitivity to these assumptions its unclear how the models would perform in scenarios where such assumptions are significantly violated.  Scalability: The methods proposed particularly CCM EYE with its regularization penalty might face scalability issues in larger more complex datasets due to computational constraints though this is not fully explored.  Generalization beyond presented settings: While the paper shows the effectiveness of the proposed methods in the settings considered a more indepth discussion on the generalizability of these methods across different domains and types of data (beyond the medical field) would strengthen the work.  3) Questions and Suggestions  Scalability and computational efficiency: Can the authors elaborate on the computational resources needed for these methods especially CCM EYE and suggest ways to improve scalability  Extension to other domains: How applicable are the proposed methods to domains beyond medical images such as natural language processing or timeseries data Exploring these areas could significantly broaden the impact of this work.  Addressing violated assumptions: The paper could benefit from a deeper examination of scenarios where assumptions about concept invariance and redundancy do not hold. How can the proposed models be adapted or improved to handle such situations  4) Limitations and Societal Impact The authors have made a commendable effort to address the limitations of their work especially regarding the reliance on assumptions about known and unknown concepts. However a more extensive discussion on the potential for model misapplication especially in sensitive areas like healthcare where incorrect generalizations could have significant consequences would enhance the paper. It would also be beneficial for the authors to explore mechanisms for identifying and mitigating any potential biases introduced by the chosen known concepts or the data used to train these models. Additionally providing guidelines or frameworks for responsibly applying these models considering their limitations and the criticality of the application domains would further strengthen the papers societal impact considerations. Overall this work marks a significant step forward in mitigating shortcut learning in machine learning models with substantial implications for fields requiring robust generalization across varying distributions.", "pm8Y8unXkkJ": "Summary of the Paper: This paper addresses the limitations of existing statistical theory in binary classification particularly in scenarios characterized by class imbalance where one class significantly outnumbers the other. The authors argue that accuracy a common performance metric does not suffice to capture the practical implications of classification errors in such imbalanced settings. The papers main contributions include a novel characterization of classifiers that optimize general performance metrics derivable from a classifier\u2019s confusion matrix which notably allows for the inclusion of stochastic classifiers as potential optimizers where deterministic classifiers fall short. This generalization of the Bayesoptimal classifier theory is applied to develop relative performance guarantees and finitesample statistical guarantees for imbalanced binary classification focusing on how optimal classification performance depends on properties of class imbalance like the newly introduced notion of Uniform Class Imbalance (UCI). The authors demonstrate their theoretical findings using knearest neighbor (kNN) classification. Strengths and Weaknesses: One of the papers key strengths is its innovative approach to extending the concept of the Bayesoptimal classifier beyond accuracy as a performance metric. This contribution addresses a significant gap in the literature on statistical learning theory which has hitherto largely ignored the impact of class imbalance on classifier performance. Additionally the introduction of Uniform Class Imbalance as a specific subtype of class imbalance and the demonstration of its role in classification performance are insightful suggesting new avenues for research. The use of kNN classification to illustrate the application of the theoretical results provides a practical dimension to the papers contributions effectively bridging theory with applicability. However a potential weakness is the paper\u2019s reliance on somewhat complex theoretical constructs that may limit its accessibility to practitioners or researchers not deeply versed in statistical learning theory. While the numerical experiments validate the theoretical findings broader empirical testing especially on realworld datasets with various types of imbalance could strengthen the papers conclusions. Questions and Suggestions for the Authors: 1. Could you elaborate on the practical implications of your findings for realworld applications dealing with imbalanced datasets Specifically how can practitioners effectively apply the concept of stochastic classifiers in such contexts 2. The Uniform Class Imbalance concept is compelling. Could you provide more examples or case studies where this specific type of imbalance is observed and how your framework could improve classification performance over existing approaches 3. While the paper focuses on binary classification class imbalance also occurs in multiclass scenarios. Do you plan to extend your theory to cover such cases and if so what challenges do you anticipate Limitations and Societal Impact: The authors have made a significant theoretical contribution but the practical aspects particularly the implementation challenges of stochastic classifiers in realworld applications are not fully addressed. The potential negative societal impact of misclassification especially in sensitive applications (e.g. healthcare criminal justice) underlines the need for careful consideration of how these classifiers are deployed. Furthermore while the paper introduces techniques for derandomizing classifiers as a way to mitigate some risks associated with stochasticity further work is needed to balance optimization of imbalanced datasets with ethical considerations such as fairness and transparency. A constructive suggestion would be to explore and provide guidelines on the application of the theoretical constructs in practice emphasizing ethical considerations and the tradeoffs between performance optimization and societal impact.", "rrYWOpf_Vnf": " Review of the Paper on Statistical Limits in Solving Inverse Problems with Gradient Descent  Summary and Contributions The paper scrutinizes the statistical limits of employing gradient descent in solving inverse problems defined by randomly sampled noisy observations across a spectrum of objective functions inclusive of those used in kernel regression Deep Ritz Methods (DRM) and Physics Informed Neural Networks (PINN). A notable highlight is its consideration of a possibly infinitedimensional parameterization via a Reproducing Kernel Hilbert Space and a nuanced parameterization of problem severity grounded in kernel integral operators. The authors claim statistical optimality of gradient descent within their framework and elucidate an implicit acceleration aspect when utilizing a Sobolev norm as the objective function highlighting a differential operational benefit in conditioning and convergence speed for algorithms like DRM and PINN.  Assessment of Strengths 1. Theoretical Depth: The paper presents robust theoretical insights including proofs of statistical optimality for gradient descent in a complex setting that accommodates a vast class of objective functions. 2. Broad Applicability: By covering instances like DRM and PINN the paper spans a wide application range from kernel regression solving partial differential equations which enhances its relevance. 3. Innovative Approach: The notion of Sobolev implicit acceleration and its implications for training algorithms present an inventive stride in how objective functions can be tailored for efficacy in solving inverse problems.  Weaknesses 1. Practical Validation: Despite the theoretical depth theres a limited exploration into the practical implications of the findings. While numerical experiments are mentioned a more comprehensive realworld application or case study is needed to validate these theoretical results practically. 2. Accessibility to Broader Audience: The paper\u2019s heavy reliance on advanced mathematical concepts might limit its accessibility. A section discussing the implications of these findings in more basic terms or practical guides could enhance its reach. 3. Handling of Limitations: There\u2019s a brief mention of limitations regarding the highdimensional computation and absence of gradient supervision in scalable training. However more detailed discussions on these limitations and potential strategies to overcome them would strengthen the paper.  Questions and Suggestions for the Authors 1. Empirical Validation: Can the authors provide more insights into realworld applications where Sobolev implicit acceleration was crucial in solving an inverse problem 2. Comparison with Existing Methods: How does the proposed method fare against current stateoftheart algorithms in similar problem settings especially in terms of computational cost and accuracy 3. Handling Limitations: Detailed discussions on each limitation identified along with concrete suggestions or future research directions to address these could enhance the papers impact.  Addressing Limitations and Societal Impact The authors have modestly addressed the limitations of their work citing challenges in highdimensional computations and scalable training without gradient supervision. While they acknowledge these deeper explorations into potential solutions or workarounds would enrich the paper. Additionally a contemplative view on the societal impact particularly in datasensitive areas (e.g. medical imaging) where machine learningbased solutions for inverse problems are prevalent should be considered. Suggestions on mitigating risks associated with misinterpretations of inverse problem solutions could further empower the research community and industry practitioners in ethically leveraging this research.  Conclusion The paper stands out for its theoretical innovations and potential to reshape understanding and methodologies in machinelearningbased solutions for inverse problems. However strengthening its empirical validation broadening accessibility and elaborating on its limitations and societal impacts could further elevate its contribution to the field.", "zSdz5scsnzU": "Review of \"Expansive Latent Space Trees for GoalDirected Planning from Visual Inputs\":  Summary: The paper introduces a novel algorithm called Expansive Latent Space Trees (ELAST) that addresses the challenges of longhorizon goalreaching tasks in highdimensional state spaces using visual inputs. Classical planning algorithms struggle in such settings due to the complexity and unknown transition dynamics inherent to realworld scenarios. ELAST mitigates this by using a latent space approach where the highdimensional state space is mapped to a lowerdimensional embedding space. The method is inspired by classical samplingbased motion planning algorithms and grows a tree in the latent space optimising the exploration efficiency by biasing the sampling towards less represented areas. This exploration is facilitated by selfsupervised training via contrastive learning for acquiring a latent state representation and a latent transition density model. The authors embed ELAST within a modelpredictive control scheme and validate its performance on various visual control tasks demonstrating its superiority over existing baseline solutions.  Assessment: Strengths: 1. Innovation: ELAST presents a unique blending of ideas from classical motion planning and modern machine learning techniques. The approach to integrate explorative tree search within a learned latent space for planning is particularly novel and addresses a significant gap in the current literature. 2. Selfsupervised Learning: The use of selfsupervised contrastive learning for training the state encoder and the transition density model is a strong aspect of this work. It allows ELAST to operate without the need for expert data or explicit knowledge of environment dynamics. 3. Performance: The algorithm demonstrates significant improvements in task success rates and path quality across a range of challenging visual control tasks which speaks to its efficacy and potential applicability in realworld scenarios. 4. Flexibility: ELASTs design allows for planning in dynamic environments and is not restricted to static maps offering greater applicability to a variety of tasks including those with parameterised settings. Weaknesses: 1. Scalability: The paper does not thoroughly discuss the scalability of the method to extremely large state spaces or highly complex environments. The computational efficiency of the tree growth and optimization process in such contexts is unclear. 2. Generalizability: The performance of ELAST is validated on simulated environments and tasks. Its effectiveness in realworld applications especially dealing with noise and other uncertainties not present in simulations remains to be seen. 3. Limited Discussion on Limitations: While the paper briefly mentions potential future directions it lacks a detailed discussion on the limitations of the proposed method particularly in nonideal conditions or when dealing with partially observable states. 4. Societal Impact and Ethics: The paper does not discuss the potential negative societal impacts or ethical considerations of deploying such planning techniques in realworld applications.  Questions and Suggestions for the Authors: 1. Scalability: Can the authors elaborate on how ELAST might scale to larger and more complex environments Are there specific strategies that could be employed to maintain computational efficiency 2. RealWorld Applicability: Have there been considerations or preliminary efforts to test ELAST in realworld scenarios What challenges do the authors anticipate in such settings 3. Exploration vs Exploitation: Given the explorative nature of the planning process how does ELAST balance exploration versus exploitation especially in dynamic or partially observable environments 4. Limitations and Negative Impacts: It would be beneficial if the authors could provide a more detailed discussion on the limitations of ELAST and any potential negative societal impacts including suggestions for mitigation.  Constructive Suggestions for Improvement:  A deeper analysis of ELASTs scalability and computational efficiency in various settings could enhance the papers contribution to the field.  Including case studies or experiments in realworld scenarios could provide valuable insights into the practical applicability of the method.  Expanding the discussion on the potential limitations and societal impacts of planning algorithms like ELAST would enrich the paper offering a more balanced perspective on its applicability and ethical considerations. In conclusion \"Expansive Latent Space Trees for GoalDirected Planning from Visual Inputs\" is a promising contribution to the field of autonomous agent decisionmaking. The innovative approach and strong empirical results highlight its potential. However further exploration of its scalability realworld applicability and a more comprehensive discussion on its limitations would strengthen the work.", "zp_Cp38qJE0": " Peer Review 1) Brief Summary: The paper delves into the challenge of maintaining fairness in machine learning models under distribution shifts which is a crucial problem given the increasing deployment of these models in highstakes decisions. The authors identify that most existing fair ML models assume identical training and testing distributions\u2014an assumption that doesnt hold in many realworld scenarios leading to a collapse of fairness. The novelty of their work lies in tackling fairness under three types of distribution shifts: domain subpopulation and hybrid shifts. They draw on selftraining techniques and propose a novel algorithm that incorporates \"fair consistency regularization\" to ensure that fairness is preserved even when models are applied to different distributions than they were trained on. The paper provides both theoretical underpinning and empirical evidence using synthetic and real datasets to validate the effectiveness of their approach in transferring fairness alongside accuracy. 2) Strengths and Weaknesses: Strengths:  The paper addresses a highly relevant and underexplored problem in fair machine learning providing significant contributions to the field.  It offers a comprehensive analysis including a theoretical framework a practical algorithm for maintaining fairness across distribution shifts and extensive experiments using both synthetic and realworld datasets.  The proposition of fair consistency regularization is innovative and thoughtfully integrated into the selftraining framework showing clear benefits over traditional methods in maintaining fairness under distribution shifts.  The inclusion of a public code repository enhances the papers accessibility and facilitates further research. Weaknesses:  The theoretical sections while rigorous are dense and may benefit from additional clarification or simplified exposition to make the paper more accessible to a broader audience.  While the paper includes diverse types of distribution shifts and datasets in its experiments the evaluation on realworld datasets is somewhat limited. More expansive realworld testing could provide a stronger validation of the proposed approach.  The discussion on potential negative societal impacts is somewhat scant. Given the critical nature of fairness in ML applications a deeper exploration of this aspect could strengthen the paper.  Theres limited discussion on the scalability of the proposed method especially in contexts with extremely large or more complex datasets which might be a concern for practical applications. 3) Questions and Suggestions for the Authors:  Clarification: Could the authors provide more details on how they chose the parameters for their algorithms especially regarding fair consistency regularization and whether these settings generalize well across different datasets  Robustness: Have the authors considered the robustness of their method to extremely noisy or adversarially manipulated data Including such scenarios in the experiments might demonstrate the resilience of their approach.  Comparison with StateoftheArt: It would be helpful if the authors could compare their method more directly with other stateoftheart fairness interventions particularly those designed for similar distribution shift scenarios to better highlight the advantages of their approach.  Scalability and Efficiency: Can the authors comment on the scalability of their method and its computational efficiency particularly in comparison to existing approaches 4) Limitations and Societal Impact: The authors have touched upon the limitations of their work notably the reliance on a welldefined set of data transformations which might restrict the applicability of their method in more varied realworld scenarios. They acknowledge this and suggest future work to relax this reliance. However an indepth discussion on the potential for negative societal impacts is missing. For instance the propagation of biases due to incorrect assumptions about data distribution or the misuse of the fairness regularization in sensitive applications warrants further exploration. A constructive suggestion would be to include a more thorough analysis of these aspects possibly proposing guidelines or safeguards to mitigate such risks.  Conclusion Overall this paper represents a significant step forward in understanding and mitigating fairness collapse under distribution shifts in machine learning models. Despite some areas that could benefit from further refinement or deeper exploration the contributions are substantial and the work lays a solid foundation for future research in fair machine learning across diverse and shifting data landscapes.", "sde_7ZzGXOE": " Review of \"On the PAC Learnability of OutofDistribution Detection\"  1. Summary This paper addresses a crucial aspect of machine learning: outofdistribution (OOD) detection which is pivotal for deploying reliable machine learning models in realworld scenarios. The primary contribution is presenting a theoretical framework based on the probably approximately correct (PAC) learning theory to understand OOD detections learnability. Unlike traditional PAC learning settings OOD detection grapples with unseen data during training making it a complex problem. The authors identify necessary conditions for learnability prove several impossibility theorems under specific conditions and then characterize the learnability under practical scenarios by providing necessary and sufficient conditions. Additionally the paper offers theoretical support for several existing OOD detection algorithms thus bridging the gap between empirical and theoretical understanding of OOD detection.  2. Assessment Strengths:  Originality and Significance: The paper tackles an open problem with significant implications for deploying machine learning in realworld settings. It provides the first theoretical framework for understanding OOD detections learnability marking a substantial advancement in the field.  Rigor: The theoretical analysis is thorough and rigorous. The proofs of the necessity and sufficiency of conditions for learnability are especially noteworthy.  Practical Implications: Beyond theoretical insights the paper makes a commendable effort to connect its findings with practical applications. It offers theoretical guidance to practitioners for designing OOD detection algorithms tailored to specific scenarios. Weaknesses:  Assumptions and Simplifications: Some of the models assumptions while necessary for theoretical tractability may limit its applicability to realworld scenarios. Specifically the assumption of finite feature spaces may not hold in many practical problems.  Complexity of Theoretical Framework: The complexity of the presented theoretical framework including numerous conditions and theorems may be daunting for practitioners. A simplified summary or practical guide derived from the main findings could enhance its accessibility.  Empirical Validation: The paper lacks empirical validation of its theoretical findings. Including case studies or simulation results to demonstrate the practical implications of the theory on existing datasets could strengthen the paper.  3. Questions and Suggestions for the Authors  Could the authors elaborate on how their assumptions (e.g. finite feature spaces) may be relaxed or extended in future work to cover a broader set of realworld scenarios  It would be beneficial to see empirical comparisons or simulations that demonstrate the application and implications of the papers findings on wellknown OOD detection benchmarks.  Can the authors discuss the potential for integrating their theoretical insights with existing empirical methodologies for OOD detection to enhance both accuracy and reliability  4. Limitations and Societal Impact The authors have made an earnest effort to address the limitations inherent in their theoretical model particularly around the assumptions of finite feature spaces and the applicability to realworld scenarios. However further discussion on these limitations and potential pathways for addressing them in future work would enhance the papers impact. On the societal impact while the paper indirectly contributes to deploying more reliable machine learning models explicit discussion on the potential negative implications such as privacy concerns or algorithmic biases in OOD detection is lacking. Including a section to reflect on these aspects could provide a more balanced view.  Suggestions for Improvement:  Clarify the implications of the assumptions and explore possible extensions to more complex scenarios.  Include empirical demonstrations or simulations to validate the theoretical findings.  Expand the discussion on the societal impact focusing on both positive and negative aspects of OOD detection advancements.", "weoLjoYFvXY": " Review of \"A Scalable Algorithm for Root Cause Analysis in Microservice Architectures\"  Summary of the Paper The paper presents a novel algorithm named Root Cause Discovery (RCD) aimed at identifying the root causes of failures within complex microservice architectures. The authors address the difficulty of maintaining and debugging microservice systems by proposing a method that quickly detects failure origins through a hierarchical and localized learning approach. The algorithm leverages causal discovery techniques particularly focusing on treating a failure as an intervention to quickly pinpoint root causes without needing to learn the entire causal graph of the system. This approach is showcased to reduce computation time significantly while improving the detection accuracy of root causes. The authors validate RCD against synthetic datasets a realworld application (Sockshop) and data from a large cloud service provider illustrating the algorithms scalability and efficiency over traditional methods and the stateoftheart in root cause analysis (RCA).  Strengths and Weaknesses Strengths: 1. Novel Approach: The technique of viewing failures as interventions on the root cause nodes and using this perspective for RCA through a localized hierarchical learning algorithm is innovative and compelling. 2. Scalability and Efficiency: The proposed algorithms ability to rapidly pinpoint root causes in largescale microservice architectures without needing to learn the complete causal graph is a significant improvement over existing methods. 3. Broad Evaluation: The comprehensive evaluation using synthetic datasets a realworld application and actual data from a production environment effectively demonstrates the applicability and effectiveness of the RCD algorithm across different scenarios. Weaknesses: 1. Lack of Comparative Analysis Detail: While the paper mentions comparisons with existing methods a more detailed exposition of these comparisons including specific strengths and weaknesses of RCD relative to each method could provide a clearer picture of its advancements. 2. Potential Overreliance on Perfect Conditions: The algorithm assumes access to a perfect conditional independence oracle and fully observable systems without confounders which may not be practical in realworld situations.  Questions and Suggestions for the Authors 1. Handling of Confounders: Could the authors elaborate on how RCD might perform in situations where confounders are present Are there specific modifications or extensions to the algorithm that could be considered in such cases 2. Comparison with Other Methods: Can the authors provide more detailed comparisons with the baseline methods including specific scenarios where RCD might underperform to present a balanced view of its capabilities 3. Robustness to Sparse Data: Considering RCAs challenges in sparse datasets it would be beneficial if the authors could discuss RCDs performance in such scenarios and any potential strategies to mitigate performance drops.  Limitations and Societal Impact The authors have not fully addressed the limitations related to the practical applicability of RCD particularly concerning scenarios with latent confounders or sparse datasets. Acknowledging and providing potential directions to address these limitations would strengthen the paper. Regarding the societal impact while the paper focuses on technical aspects it briefly touches upon the potential revenue loss due to delays in root cause detection in cloud services. A more comprehensive discussion of the negative and positive societal impacts especially concerning privacy concerns related to data collection and analysis in microservice architectures could provide a more rounded perspective.  Constructive Suggestions for Improvement 1. Extend the Discussion on Limitations: It would enhance the papers robustness to include a more detailed discussion on limitations particularly how RCD could be adapted or extended to handle scenarios not covered in the current approach. 2. Analyzing Societal Impact: Elaborate on the societal implications of implementing RCD including any potential for misuse or privacy concerns and how they might be mitigated. 3. Broader Implications for Cloud Services: Discuss the broader implications of RCDs adoption for cloud services including potential impacts on service reliability customer satisfaction and economic benefits for cloud service providers and users. Overall the paper contributes significantly to the field of RCA in microservice architectures offering a novel scalable and efficient solution with practical implications for improving the reliability and performance of cloud services.", "sNcn-E3uPHA": "Peer Review: 1) Summary: The paper introduces a novel text classification algorithm inspired by key principles from quantum physics specifically leveraging the concept of superposition and the Born rule for calculating probabilities. This approach treats text as a superposition of words and calculates the wave function of a document to determine its class transition probability. The authors offer two implementations: a direct calculation of wave functions and an embedding within a neural network architecture. The paper tests the method using three benchmark datasets demonstrating advantages in classification performance explainability and computational efficiency. Furthermore the paper posits that this approach can be generalized beyond textual data. 2) Strengths and Weaknesses: Strengths:  Innovative Approach: The application of quantum physics principles to text classification is novel and could potentially open up new pathways for algorithmic development in machine learning.  Performance: The empirical results as shown indicate superior accuracy and computational efficiency compared to standard classifiers especially in handling imbalanced datasets.  Explainability: The method inherently offers explanations for its classifications addressing a significant challenge in many machine learning models today.  Generalization: The framework is not limited to text data suggesting a broader applicability to other types of data. Weaknesses:  Complexity of Concept: While innovative the quantum physics basis might be challenging for a broader audience to fully grasp potentially limiting its immediate adoption and further investigation by the community.  Lack of Theoretical Explanation: The paper demonstrates the algorithms effectiveness empirically but does not delve deeply into the theoretical reasons why quantuminspired approaches excel in this domain.  Potential Overfitting: Given the exceptional performance noted especially in smaller datasets there could be concerns about the model\u2019s generalizability to unseen data or its performance in realworld applications outside the benchmarks tested. 3) Questions and Suggestions:  Theoretical Grounding: Could the authors provide a more detailed theoretical discussion on why quantuminspired models might be inherently more suited for text classification tasks  RealWorld Application: Are there examples of this algorithms application in realworld scenarios outside of the benchmark datasets Additional insights into its performance in diverse environments would be valuable.  Comparison with Deep Learning Models: The comparison primarily focuses on traditional classifiers. Given the neural network implementation how does the algorithm compare against stateoftheart deep learning models specifically designed for text classification  Hyperparameter Sensitivity: The paper mentions the absence of hyperparameter tuning. However more explanation on the choice of model hyperparameters (a b h) in different scenarios would be informative. 4) Limitations and Societal Impact: The authors have made commendable efforts in presenting their algorithms limitations notably its complexity and current empirical basis of validation. However a more comprehensive discussion on the potential for misuse biases or ethical considerations especially in sensitive applications like sentiment analysis or content moderation would be beneficial. Additionally discussions on the computational resources required for larger datasets and realworld scenarios would provide a clearer picture of the algorithms scalability and environmental impact. Suggestions for Improvement:  Consider expanding the theoretical explanations behind the models success possibly by linking it with existing literature on quantum theory and machine learning.  Broaden the empirical evaluation to include more varied datasets and direct comparisons with advanced deep learning models to better understand the algorithm\u2019s unique advantages and limitations.  Discuss potential ethical considerations biases and societal impacts in more detail offering guidelines for responsible application and future research directions to mitigate these concerns. Overall the paper presents a fascinating approach with substantial implications for text classification and potentially other domains. With further theoretical exploration and broader empirical validation this work could significantly contribute to the development of quantuminspired machine learning algorithms.", "lgj33-O1Ely": " Review of \"TotalSelfScan: FullBody Avatar Creation from Monocular SelfRotation Videos\"  1. Summary of the Paper This paper introduces TotalSelfScan a novel approach to create highquality photorealistic fullbody human avatars from a series of monocular selfrotation videos focusing separately on the human body head and hands. The technique significantly improves the reconstruction and rendering quality of avatars particularly for the face and hands by employing a multipart representation strategy for different body parts and seamlessly fusing them into a unified model. The ability to generate detailed avatars from easily obtainable monocular videos without the need for extensive camera setup or physical assistance enhances the accessibility of highfidelity personal avatar creation.  2. Assessment of the Paper Strengths:  Innovative Approach: The methodological shift to multiple videos focusing on different body parts for better detail capture is a creative solution to the limitations faced by previous methods relying on a single video. This enhances the quality of avatar reconstruction significantly especially for the face and hands.  HighQuality Reconstruction: Experiments demonstrate that TotalSelfScan outperforms existing methods in terms of reconstruction and rendering quality providing more detailed and photorealistic avatars.  Practicality and Accessibility: By leveraging commonly available equipment (a fixed camera) without requiring specialized skills or setups the proposed approach significantly lowers the barrier to creating highquality personal avatars broadening its potential application.  Comprehensive Evaluation: The authors undertake an exhaustive evaluation including qualitative and quantitative analyses across newly created datasets showcasing the methods effectiveness. Weaknesses:  Lack of Error Analysis: The paper does not report error bars for the experimental results. Including such statistical measures would provide insights into the robustness and reliability of the proposed approach.  Training Time: Despite the method\u2019s advantages in output quality it requires a considerable amount of time for training. For practical applications this could be a limiting factor that needs addressing.  Potential for Negative Societal Impact: While the potential misuse of technology (e.g. creating unauthorized or harmful images) is acknowledged the discussion is somewhat brief. A deeper exploration into mitigating these risks would be beneficial.  3. Questions and Suggestions for the Authors  Error Bars and Statistical Robustness: Can the authors provide error bars or other statistical measures to showcase the robustness of their experimental results Incorporating such data could strengthen the validity of the findings.  Training Efficiency: Are there potential modifications or optimizations to the current method that could reduce training times without significantly compromising the quality of the reconstruction  Mitigating Negative Impacts: Could the authors propose specific strategies or safeguards to prevent the misuse of the technology such as unauthorized creation of personal avatars This discussion could significantly contribute to ethical considerations surrounding the deployment of such technologies.  4. Limitations and Societal Impact The authors have identified key limitations including the reliance on selfrotation videos that limit the scope of human poses the model can accurately reconstruct and the extensive training required. On the societal impact the brief discussion around the potential misuse of the technology is appreciated yet a deeper analysis of these issues and proposed countermeasures could improve the paper. Suggestions include exploring technical safeguards user consent mechanisms and collaboration with platforms to regulate the use of such technology responsibly. In conclusion TotalSelfScan is a promising advancement in the field of 3D human avatar reconstruction offering significant improvements over existing methods. Addressing the noted concerns particularly around training efficiency and ethical considerations could further enhance its contribution to the field.", "rjDziEPQLQs": "Review on \"AffineInvariant Cubic Newton Method\" 1) Briefly summarize the paper and its contributions This paper introduces a novel stepsize schedule for Newtons method designated as the AICN (AffineInvariant Cubic Newton) algorithm which promises fast global and local convergence without needing subproblem solutions  a significant simplification over existing methods. The papers core contributions center around:  Demonstrating an \\(O(\\frac1k2)\\) global convergence rate aligning with the bestknown global rates for the class of regularized Newton methods and offering the first such rate for a damped Newton method variant.  Establishing a local quadratic convergence rate which is on par with the topperforming secondorder methods.  Introducing a stepsize formula that is not only simple and explicit but also independent of solving subproblems rooted in geometrically natural affineinvariant assumptions. 2) Provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The introduction of a simple explicit formula for stepsize adjustment in Newtons method which obviates the need for ancillary subproblem resolution constitutes a significant methodological leap.  Solid Theoretical Foundation: The paper robustly underpins its findings with convergence proofs hinged on affineinvariance assumptions. This strengthens the reliability of the proposed method.  Comprehensive Performance Evaluation: By comparing AICN to a suite of baseline algorithms across different benchmarks the paper cogently illustrates the practical supremacy of its approach in terms of convergence speed and computational efficiency. Weaknesses:  Assumption Specificity: The reliance on affineinvariance and semistrong selfconcordance might limit the applicability of AICN to a narrower class of problems than firstorder methods or those not requiring such assumptions.  Lack of Broader Testing: Though the performance evaluations are thorough the selection of use cases primarily logistic regression and a theoretical function might not sufficiently capture the diversity of practical optimization problems where AICNs advantages could be tested more broadly.  Implementation Details: The simplification achieved by avoiding subproblem resolution is commendable yet the paper could benefit from a more detailed discussion on the practical implementation nuances especially concerning the computation of the smoothness constant \\(L\\textsemi\\) and its influence on the algorithms performance. 3) Questions and Suggestions for the Authors  Scalability and Computational Complexity: Can the authors provide more insights or empirical data on the scalability of AICN especially regarding higherdimensional problems including how the computational complexity compares with existing methods in such scenarios  Empirical Evaluation on Diverse Benchmarks: It would be beneficial if additional tests could be conducted on more diverse datasets or problem instances including nonconvex optimization tasks to further demonstrate the versatility of AICN.  Clarification on Implementation Challenges: Detailing any potential challenges in implementing the proposed method especially concerning estimating the smoothness constant might help practitioners gauge the ease of adoption of AICN. Limitations and Potential Negative Societal Impact The authors have succinctly addressed the limitations tied to the assumptions underpinning their method. However the potential societal impacts particularly the implications of optimizing AI algorithms that could amplify biases or introduce new ethical concerns are not discussed. While this may be beyond the scope of the current work considering and mitigating such impacts is crucial especially as optimization methods find widespread application in sensitive and impactful domains. Suggestions for Improvement:  Expanding the applications of AICN to a broader suite of problems including those with societal implications might not only showcase its versatility but also enable an assessment of potential biases or ethical issues that could arise from its application.  Incorporating discussions or guidelines on how potential negative impacts could be mitigated while employing AICN in critical areas such as healthcare finance or criminal justice could significantly enhance the papers relevance and responsibility towards societal concerns.", "kK200QKfvjB": "This paper investigates the impact of L2 regularization on feature learning in deep neural networks (DNNs) presenting a novel analysis of the loss surface in terms of layerwise activations and covariances of hidden representations. Two main reformulations of the regularized loss are proposed revealing insights into the dynamics of feature learning and sparsity induced by L2 regularization. The first reformulation aligns with an attractionrepulsion problem that guides hidden representations to balance between retaining necessary input information and achieving a desired output. The second applicable to networks with homogeneously nonlinear activations recasts the loss as a partially convex optimization over a convex cone demonstrating that an L2regularized DNN can achieve its minimal loss with a significantly reduced number of neurons. This work extends theoretical understanding by introducing a bound on the minimal number of neurons required in each hidden layer and empirically observes that actual networks often require fewer neurons to reach local minima. Strengths: 1. Theoretical Novelties: The papers theoretical contributions particularly the reformulation of loss in terms of hidden representations and covariances are novel and offer a deeper understanding of the behavior of DNNs under L2 regularization. 2. Insights on Sparsity: The establishment of a theoretical bound on the number of neurons required for achieving local minima contributes significantly to our understanding of model complexity and sparsity in neural networks. 3. Empirical Validation: The authors support their theoretical claims with numerical experiments showing that realworld datasets like MNIST require much fewer neurons than the proposed bound suggesting the practical relevance of their findings. Weaknesses: 1. Limited Scope of Nonlinearities: The second reformulation is restricted to positively homogeneous nonlinearities which may limit the applicability of the findings. Expanding this analysis to a broader class of activation functions could increase the papers impact. 2. Implementation Details and Computational Costs: While the paper mentions direct optimization on the proposed reformulations there is a lack of detailed discussion on the computational efficiency and practicality of these optimization methods. Clarifying these aspects could enhance the papers usefulness for implementation. 3. Discussion on Generalization: The paper hints at implications for generalization based on sparsity yet it doesnt provide a thorough analysis or evidence of how the observed sparsity affects the models generalization capabilities. Questions and Suggestions for the Authors: 1. Beyond Homogeneous Nonlinearities: Could the authors speculate on how their findings might extend to nonhomogeneously nonlinear networks Exploring this pathway could broaden the implications of the work. 2. Optimization Techniques: It would be beneficial if the authors could provide more insights into the practical implementation of the proposed optimization strategies including computational costs and stability considerations. 3. Generalization Impact: Can the authors elaborate on the implications of their findings for model generalization Providing empirical evidence on how the induced sparsity affects generalization would be valuable. 4. Replicability and Accessibility: To aid in the replication of the study and application of the findings the authors could consider releasing the code used for their numerical experiments. Limitations and Societal Impact: While the paper advances our theoretical understanding of DNNs under L2 regularization it stops short of directly addressing the limitations related to the specific class of nonlinearities studied and the practical computational demands of implementing the proposed reformulations. On the societal impact the manuscript could benefit from a discussion on the potential for these findings to contribute to more efficient and interpretable neural network models which is increasingly important in applications where model transparency and efficiency are critical. Overall incorporating these suggestions could enhance the papers contribution to the field.", "u6MpfQPx9ck": "Review of \"ProbCover: A Novel Active Learning Strategy for the LowBudget Regime\" 1. Summary: This paper introduces \"ProbCover\" an active learning (AL) algorithm designed to maximize efficiency in scenarios characterized by severely limited annotation budgets. By leveraging advancements in representation and selfsupervised learning ProbCover addresses the \"cold start\" problem endemic to many deep AL methods which traditionally struggle to achieve meaningful performance improvements with few labeled examples. ProbCover operates by selecting a subset of unlabeled data for annotation that maximizes the probability coverage of the data distribution thereby optimizing an upperbound on the generalization error of a 1NN classifier. The approach is thoroughly evaluated across multiple image recognition benchmarks demonstrating significant improvements over existing AL strategies in lowbudget scenarios and showing notable benefits when used in conjunction with semisupervised learning methods. 2. Assessment: Strengths:  The paper tackles the important and practical challenge of AL under tight budget constraints a scenario common in many realworld applications where expert annotations are costly.  Provides a novel theoretical framework for understanding and optimizing AL strategies in embedding spaces showing dual applications for both low and highbudget regimes.  ProbCover is a methodologically sound approach grounded in theory and validated through extensive experimentation showing clear improvements over stateoftheart AL methods in lowbudget contexts.  The use of selfsupervised learning to enrich data representations prior to AL is an innovative strategy that could have broader implications for how AL is approached in datasparse environments. Weaknesses:  While the work focuses on the lowbudget regime more insight into the transition between low and high budget scenarios along with practical guidance on when to switch strategies would enhance its applicability.  The empirical evaluation while comprehensive could benefit from a deeper analysis of failure cases or conditions under which ProbCover does not perform as expected.  The discussion on societal impact and ethical considerations particularly in light of the potential uses in sensitive applications (e.g. medical image analysis) is missing. This is a significant omission given the increasing importance of these aspects in AI research. Questions and Suggestions:  How does ProbCover handle data distributions with significant class imbalance Could an extension or modification to the algorithm improve its performance in these scenarios  The paper would benefit from a more thorough discussion on the dependencies and limitations of the selfsupervised learning approaches used to generate embeddings. Are there specific characteristics (e.g. data modalities or domains) where ProbCovers efficacy might be reduced  Considering ProbCovers reliance on a predefined ball radius \u03b4 have the authors explored adaptive or dynamic methods for determining \u03b4 based on the data or learning process  A potential extension of this work could investigate combining ProbCovers approach with other AL techniques such as uncertainty sampling in a hybrid model. This might help address situations where ProbCover alone is less effective. 3. Limitations and Societal Impact: The authors have not provided a detailed consideration of the limitations and potential negative societal impacts of their work. As AL can play a crucial role in determining the types of data models are exposed to theres a risk of perpetuating biases or overlooking minority classes especially in lowbudget scenarios where the data selection process is critical. Additionally the efficacy of ProbCover in diverse application contexts and its interactions with different data distributions remains to be fully understood. Future iterations of this work should aim to address these concerns by:  Conducting a sensitivity analysis to understand how ProbCover performs across a wider range of datasets particularly those representing underprivileged or minority groups.  Investigating the integration of fairnessaware or biasmitigation strategies within the ProbCover framework.  Providing clear guidance and considerations for practitioners to assess the potential ethical implications of deploying ProbCover in their specific context. In summary this paper presents a significant contribution to the field of active learning particularly for applications constrained by limited labeling resources. The introduction of ProbCover marks a promising step forward though further work is needed to fully understand its limitations ethical implications and broader societal impacts.", "pd6ipu3jDw": " Peer Review for \"Agent Transformer Memory Network for Multiagent Reinforcement Learning\"  1) Brief Summary The paper presents the Agent Transformer Memory (ATM) network a novel architectural framework tailored for multiagent reinforcement learning (MARL) to address the critical issue of partial observability in realworld tasks. The ATM network incorporates a transformerbased memory architecture that processes both the environmental entities and past agent interactions through a unified framework. By leveraging a working memory updating schema inspired by human cognitive processes the ATM dynamically maintains a fixedcapacity memory buffer for each agent which is crucial for decisionmaking. Additionally the network introduces the EntityBound Action Layer which semantically binds actions to their corresponding environmental entities thus providing a semantic inductive bias. The networks efficacy is demonstrated through comprehensive experiments on challenging environments such as SMAC and LevelBased Foraging showing notable improvements in learning acceleration and performance over existing MARL algorithms.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Integration of Transformers: The use of transformers for memory representation in MARL is a significant step forward offering a way to efficiently handle the complex dynamics of multiagent environments under partial observability.  HumanInspired Memory Processing: The working memory updating schema inspired by cognitive science provides an effective method for managing the agents memory ensuring that only relevant information is retained for future decisionmaking.  Semantic Action Binding: The introduction of the EntityBound Action Layer enables a more nuanced approach to action selection ensuring actions are tied to their environmental context which could enhance the explainability of agent behavior.  Empirical Validation: The extensive experiments offer strong evidence of the ATMs superiority in performance and learning speed across various settings thereby validating its practical applicability. Weaknesses:  Computational Complexity: The adoption of transformers despite their benefits introduces significant computational overhead which may limit the scalability of the ATM network to very large multiagent systems or realtime applications.  Generalizability Concerns: The paper mainly focuses on testing the ATM network on two environments. Additional evaluations across a broader range of tasks and settings would be beneficial to fully understand its adaptability and limitations.  Lack of Comparative Analysis: While the paper compares the ATM network with GRUbased models a more thorough comparative analysis with other stateoftheart MARL frameworks or transformerbased approaches would provide deeper insights into its relative advantages and drawbacks.  3) Questions and Suggestions for the Authors  Computational Efficiency: Can the authors discuss strategies to mitigate the computational demands of the ATM network especially in scenarios requiring realtime decisionmaking  Broader Evaluation: Could the authors extend their experiments to include more diverse environments or integrate the ATM network with other MARL algorithms to assess its generalizability further  Impact of Memory Buffer Size: The paper mentions the use of a fixedcapacity memory buffer. How sensitive is the performance of the ATM network to the size of this buffer and are there optimal configurations depending on specific types of environments  Comparison with Other Transformer Models: It would be valuable to see a comparative analysis of the ATM network with recent transformerbased models in MARL providing insights into how the specific architectural decisions in ATM contribute to its performance.  4) Limitations and Potential Negative Societal Impact The authors have not extensively discussed the limitations or potential negative societal impacts of their work. While the ATM network shows promise in simulated environments the realworld application of such technologies especially in sensitive domains like autonomous driving or surveillance could have ethical and privacy implications. Future iterations of this work could benefit from a section discussing these aspects including strategies to ensure that deployment of such technologies adheres to ethical standards and privacy regulations. Constructive suggestions include incorporating explainability and fairness considerations into the design of the ATM network to mitigate potential biases and ensure transparency in agent decisionmaking processes.", "r9b6T088_75": "Review for \"DegradationAware Unfolding HalfShuffle Transformer (DAUHST) for Hyperspectral Image (HSI) Reconstruction\" 1) Summary of the Paper and its Contributions This paper addresses the challenges within coded aperture snapshot spectral compressive imaging (CASSI) systems specifically in hyperspectral image (HSI) reconstruction. The authors identify shortcomings of existing deep unfolding methods that do not estimate degradation patterns and degree of illposedness specifically for CASSI systems and whose primarily convolutional neural network (CNN)based architectures exhibit limitations in capturing longrange dependencies. The contributions are threefold: (i) The development of a DegradationAware Unfolding Framework (DAUF) based on maximum a posteriori (MAP) theory to adaptively scale linear projection and provide noise level information with estimated parameters from the compressed measurement and physical mask. (ii) The introduction of a HalfShuffle Transformer (HST) that captures both local content and nonlocal dependencies integrated into DAUF to form the DAUHST the first Transformerbased deep unfolding method for HSI reconstruction. (iii) Demonstrations that DAUHST significantly outperforms stateoftheart methods in terms of PSNR and SSIM metrics while requiring lower computational and memory costs. 2) Assessment of Paper Strengths and Weaknesses Strengths:  Innovative Approach: The integration of the novel HST within a degradationaware unfolding framework addresses both the need for capturing longrange dependencies and the systemspecific degradation which is a notable advancement over existing methods.  Comprehensive Evaluation: The authors thoroughly test the proposed method against a wide range of stateoftheart techniques demonstrating superior performance not just in terms of qualitative reconstruction quality but also in computational efficiency.  Public Resources: Providing code and models enhances reproducibility and contributes to future research advancements. Weaknesses:  Inference Speed and Complexity: The paper acknowledges that improvements in performance come at the cost of reduced inference speed and increased model complexity which could limit practical deployment in timesensitive or resourceconstrained applications.  Limited Discussion on Generalization: While significant performance improvement is demonstrated on tested datasets the discussion on how well DAUHST generalizes to other notably more challenging or diverse datasets is minimal. 3) Questions and Suggestions for the Authors  Generalization: Can the authors provide insights or additional results on how the proposed DAUHST generalizes to other HSI datasets especially those with significantly different characteristics from those tested  Inference Speed Detail: It would be beneficial if the authors could provide more detailed metrics on inference speed comparison perhaps in frames per second to better assess the practical implications of the increased model complexity.  Extension to Other Imaging Modalities: Do the authors foresee the potential for adapting DAUHST to other imaging modalities beyond HSI Specifically could this approach be beneficial in something like MRI or CT reconstruction tasks and if so what adaptations would be necessary 4) Limitations and Societal Impact The authors acknowledge the main limitation regarding inference speed and model complexity which might affect the deployment in realworld scenarios. Nonetheless specific metrics quantifying this limitation would further clarify the extent of this issue. On the societal impact while the current focus and application of this research present no negative consequences a broader discussion on the ethical aspects and longterm implications of advanced image reconstruction techniques especially in sensitive areas such as surveillance medical imaging or privacy would enrich the paper. Suggestions for improvement include a detailed analysis or simulation on deployment scenarios to better understand the tradeoffs between performance and computational efficiency and an ethical discussion section considering wider impacts.", "qmm__jMjMlL": "Review of \"TARGETVAE: Translation and Rotation Group Equivariant Variational Autoencoder for Unsupervised Learning of Object Semantics\" 1) Summary of the Paper and its Contributions: This paper presents TARGETVAE a novel variational autoencoder (VAE) architecture designed to learn semantic representations of objects in images that are invariant to their pose and location in an unsupervised manner. The framework achieves this by introducing a groupequivariant encoder that decomposes image generative factors into semantic rotation and translation components. The model through its spatially equivariant generator and structured approximate posterior distribution successfully disentangles these components enabling accurate unsupervised inference of object pose and location. The paper provides comprehensive experiments demonstrating the models superior performance over existing methods in learning disentangled representations and improving clustering accuracy in the latent space. The work could potentially influence future approaches in unsupervised object detection and semantic analysis across various imaging modalities. 2) Strengths and Weaknesses:  Strengths: 1. Innovative Approach: The introduction of a translation and rotation groupequivariant VAE framework represents a significant advancement in unsupervised representation learning. It directly addresses the challenge of disentangling object semantics from spatial transformations. 2. Comprehensive Evaluation: The paper provides extensive experimental results showcasing the effectiveness of TARGETVAE in various setups which include different datasets and transformations. This thorough evaluation strengthens the claim of superiority over existing methods. 3. Practical Implications: By enabling more accurate unsupervised learning of object semantics in images TARGETVAE has the potential to revolutionize fields such as cryoEM microscopy where current methods fall short.  Weaknesses: 1. Computational Efficiency: The paper does not address the computational demands of TARGETVAE particularly in comparison to less complex models. The use of groupequivariant convolutions might introduce significant overhead a critical aspect for practical applications. 2. Generalization across Datasets: While the paper presents results on a few selected datasets there is insufficient discussion on the models generalizability to other imaging modalities or realworld scenarios where images could be more complex. 3. Comparison with StateoftheArt: The comparative analysis mainly focuses on traditional VAE and spatialVAE models. Including comparisons with current stateoftheart in unsupervised learning could provide a clearer picture of the improvements offered by TARGETVAE. 3) Questions and Suggestions for the Authors:  Questions: 1. Could the authors provide insights into the computational efficiency of TARGETVAE especially in scenarios involving large datasets or highresolution images 2. How well does TARGETVAE generalize to datasets with more complex transformations or multiple objects per scene 3. Have the authors considered extending the framework to 3D images directly and if so what challenges do they foresee  Suggestions: 1. Evaluation on Diverse Datasets: It would be beneficial to test TARGETVAE on a broader range of datasets including realworld scenarios with more complex object interactions and transformations. 2. Computational Benchmarking: Including comparisons of computational resources and training times with existing methods could help evaluate the practicality of TARGETVAEs deployment. 3. Extension to 3D Images: Exploring the application of TARGETVAE to 3D images directly could open up avenues in various scientific fields offering a potential direction for future work. 4) Limitations and Societal Impact: The paper does not sufficiently address the limitations regarding computational efficiency and the potential challenges in applying the model to more complex or heterogenous datasets. Additionally there is little discussion on the potential negative societal impact especially in applications with privacy or ethical considerations (e.g. surveillance imagery). To improve the authors could elaborate on these aspects possibly suggesting mitigation strategies for identified limitations and offering guidelines to avoid misuse of the technology in sensitive applications.", "nyn2ewuF-g9": " Peer Review  Brief Summary The paper addresses the challenge of fair reward allocation in collaborative machine learning (ML) environments where multiple parties contribute data to train improved ML models. It focuses on scenarios without an external source of reward proposing an allocation scheme that allows for a fair tradeoff between monetary payoffs and model rewards based on the contributions of each party. Two main approaches are developed: one based on the properties of parties\u2019 payoffs and another on the flow of payoffs between parties with a particular focus on managing budget constraints. The paper introduces the concept of the conditional Shapley value which uniquely determines fair payoff flows and facilitates a novel way of adjusting payoff rewards under budget constraints. Empirical demonstrations validate the proposed scheme across various scenarios.  Assessment Strengths: 1. Novelty: The work presents a novel approach to addressing fairness in reward allocation in collaborative ML particularly in scenarios without external rewards. The introduction of the conditional Shapley value as a mechanism to account for budget constraints and ensure fairness is innovative. 2. Rigor and Depth: The paper provides a rigorous mathematical formulation of the problem and the proposed solutions including proofs of uniqueness for the proposed allocation schemes. The exploration of desirable properties such as symmetry linearity balance and dummy party provides a solid theoretical foundation for the work. 3. Practical Relevance: Addressing fair allocation of rewards in collaborative ML has significant practical implications especially for smaller parties with valuable data but limited budgets. The proposed scheme could incentivize more equitable participation in ML collaborations leading to better and more diverse ML models. 4. Empirical Validation: The empirical demonstration using real datasets like MNIST and CIFAR10 provides concrete evidence of the viability and benefits of the proposed allocation scheme in realworld scenarios. Weaknesses: 1. Scalability: The paper does not thoroughly discuss the scalability of implementing the proposed schemes especially considering the computational complexity of calculating Shapley values for large datasets and numerous parties. 2. Generalization: While the paper considers budget constraints other practical constraints (e.g. data privacy concerns legal restrictions) that could affect collaborative ML are not extensively explored. A discussion on how the proposed scheme can accommodate these additional constraints would be beneficial. 3. Comparative Analysis: The paper lacks a comprehensive comparative analysis with existing models or schemes meant to address fairness in collaborative ML outside of those based on Shapley values.  Questions and Suggestions for the Authors 1. Scalability: Can the authors provide more insights or potential solutions on how the computational challenges of calculating Shapley values in largescale collaborations can be addressed 2. Extending the Model: Have the authors considered integrating other practical constraints into their model and if so how might these modifications affect the proposed allocation schemes 3. Comparative Analysis: Could the authors elaborate on how the proposed scheme performs relative to other existing fairness schemes in collaborative ML not based on Shapley values  Limitations and Societal Impact The authors have made an effort to address the limitations related to computational complexity and budget constraints. However the broader societal implications such as potential biases in model rewards based on the diversity of data contributions and the impact on smaller entities with less bargaining power could be further explored. Suggestions for improvement include:  Scalability: Developing approximations or heuristic methods for computing the conditional Shapley value in largescale settings.  Generalization: Extending the framework to integrate additional practical constraints and exploring their impacts on fairness.  Societal Impact: A deeper analysis of how the proposed schemes might enforce or mitigate biases in ML models and recommendations for ensuring equitable access and benefits for all parties involved in collaborative ML projects. In conclusion the paper makes a significant contribution to the field of collaborative ML by addressing the critical issue of fair reward allocation. With some enhancements in terms of scalability generalization and a more detailed societal impact analysis the work can pave the way for more equitable and efficient collaborative ML environments.", "q9XPBhFgL6z": "Summary of the Paper and its Contributions: This paper addresses the complex issue of defining harm concerning autonomous systems. Traditional definitions from philosophy have struggled to provide a clear and consistent account of harm especially in cases involving autonomous technologies. This papers innovation lies in its proposal of a formal definition of harm that utilizes causal models specifically leveraging the concept of actual causality based on contrastive causation. By introducing a default utility for outcomes this approach aims to provide a more nuanced understanding of harm capturing various scenarios that have posed challenges for existing definitions. The paper also illustrates the application of this definition across different domains including autonomous vehicles and healthcare highlighting its versatility and practical significance. Assessment of Strengths and Weaknesses: Strengths: 1. Innovative Approach: The papers use of causal models to define harm is a significant advancement. By leveraging formal tools like actual causality it provides a structured way to tackle the ambiguous and often subjective nature of harm. 2. Practical Relevance: The examples provided from autonomous driving to healthcare demonstrate the definitions applicability across a range of scenarios where autonomous systems interact with humans. This relevance is particularly important given the increasing integration of such technologies in everyday life. 3. Theoretical Depth: The paper dives deep into philosophical discussions and traditional definitions of harm showing a comprehensive understanding of the issue at hand. Its ability to address and propose solutions to longstanding philosophical problems is commendable. Weaknesses: 1. Complexity of Formal Models: While the use of causal models is a strength it also presents a challenge in terms of accessibility. The complexity of these models might limit the papers readability and hence its impact among a broader audience including policymakers. 2. Assumption of Default Utility: The concept of a default utility although innovative may introduce subjectivity into the definition. Determining what constitutes a default outcome could vary significantly across different cultural or legal contexts potentially complicating the application of this definition. 3. Quantitative Generalization: The paper mentions a quantitative account developed in another work [2] but the qualitative nature of the current definition might not capture the full spectrum of harms impacts especially where a scale of damage or harm is essential for policy or legal decisions. Questions and Suggestions for the Authors: 1. Clarification on Default Utilities: Can the authors provide more guidance or criteria for determining default utilities in diverse contexts Offering a framework or examples could enhance the applicability of the definition. 2. Quantitative Extensions: While a separate paper addresses this integrating a brief discussion or outline on transitioning from qualitative to quantitative harm assessments within this paper could provide a more holistic view. 3. Analysis of Societal Implications: How does the proposed definition of harm align with or diverge from existing legal standards and societal norms across different cultures An analysis of potential conflicts or adaptations needed could be beneficial. 4. Application to Nonautonomous Systems: Is this definition of harm limited to scenarios involving autonomous systems or could it be applied more broadly Clarification on this aspect could widen the papers relevance. Limitations and Suggestions: The authors have made a commendable effort to address the limitations by providing a nuanced and formally rigorous definition of harm. However the potential for subjective interpretation in determining default utilities and the papers complex formalism might limit its practical applicability without further elaboration or simplification. An exploration of how this definition could align with or necessitate changes in current legal frameworks particularly in a global context with varying legal and cultural norms would be valuable. Additionally providing a more straightforward pathway or toolkit for applying this definition in realworld scenarios possibly through case studies or simulations could enhance its utility for policymakers and technologists alike. In terms of societal impact while the papers focus is on clarifying the philosophical and regulatory discourse around harm in the context of autonomous systems a more explicit discussion on the ethical implications of this new definition particularly in terms of privacy bias and equity would strengthen its contribution. Addressing these dimensions could ensure that the proposed definition of harm not only advances conceptual clarity but also fosters more ethically aligned and socially just applications of autonomous technologies.", "oPzICxVFqVM": "Review of \"Exploring the Relationship Between ScoreBased Generative Models and Wasserstein Distance through Optimal Transport\" 1) Summary: This paper extends the theoretical understanding of scorebased generative models which have shown empirical success in tasks like image generation and audio synthesis but lack complete theoretical backing. The key contribution of this work is the establishment of a direct relationship between the training objective of scorebased generative models and the Wasserstein distance. Utilizing the theory of optimal transport the authors demonstrate that the Wasserstein distance between the data distribution and the models distribution is upper bounded by the square root of the scorematching loss enriched with multiplicative constants and a fixed constant offset. This provides a novel perspective on how scorebased models optimize distributions in the framework of optimal transport. Empirical evidence through numerical experiments aligns with theoretical findings reinforcing the reliability of the proposed bounds. Additionally the paper discusses techniques for obtaining tighter bounds in practice and evaluates the models robustness to initial distribution perturbations. 2) Assessment: Strengths:  Theoretical Contribution: The paper addresses a significant gap in the literature by connecting scorebased generative models with Wasserstein distance presenting a rigorous theoretical analysis supported by optimal transport theory. This is a substantial contribution deepening our understanding of how these models function from an optimization standpoint.  Empirical Validation: The experimental section leveraging synthetic datasets effectively validates the theoretical outcomes. This enhances the credibility of the proposed theoretical framework.  Practical Implications: By deriving upper bounds on the Wasserstein distance and exploring methods to tighten these bounds the paper provides practical guidelines for improving the performance of scorebased models which is beneficial for researchers and practitioners in the field. Weaknesses:  Lack of RealWorld Applications: Despite strong theoretical and empirical backing the paper does not demonstrate the application of its findings on realworld datasets which could limit the perceived applicability of the research.  Assumptions and Conditions: The main results rely on several assumptions about the model and loss function properties. While necessary for the theoretical development these assumptions may not always hold in practice potentially limiting the generalizability of the findings. 3) Questions and Suggestions for the Authors:  Extension to RealWorld Datasets: Could the authors extend their experimental validation to include realworld datasets particularly challenging ones in image and audio synthesis to demonstrate the practical applicability of their findings  Discussion on Assumption Relaxation: It would be beneficial if the authors could discuss the possibility and implications of relaxing some of the assumptions made in their theoretical analysis. How sensitive are the results to these assumptions and do they envision any methodologies for working under less stringent conditions  Additional Comparative Analysis: While the focus on Wasserstein distance provides valuable insights a comparative analysis with models optimizing other distances or divergences (e.g. KL divergence) could offer a more comprehensive understanding of the scorebased models positioning and advantages.  Exploration of Limitations in Model Robustness: The theoretic analysis of model robustness under initial distribution perturbations is interesting. Could the authors provide more detailed empirical evidence or case studies highlighting how this aspect can influence practical model applications 4) Limitations and Societal Impact: The paper provides some initial discussion on model robustness which indirectly addresses potential limitations concerning the stability of scorebased models. However a more explicit discussion on the limitations particularly regarding scalability to highdimensional data and the feasibility of assumptions in realworld applications would strengthen the paper. Regarding societal impact the paper does not delve into potential negative consequences of its findings. Given the use of generative models in deepfakes and misinformation a brief discussion on ethical considerations and mitigation strategies would be a valuable addition. Constructive suggestions include proposing guidelines for ethical use and exploring mechanisms to detect and counteract misuse of generative models developed based on the presented findings.", "sYDX_OxNNjh": " Review of \"Recurrent Skill Training (ReST) for Unsupervised Skill Discovery in Reinforcement Learning\"  Summary This paper introduces a novel methodology Recurrent Skill Training (ReST) aimed at addressing a significant challenge in unsupervised skill discovery within reinforcement learning (RL)  exploration degradation. Previous unsupervised skill discovery methods train multiple skills in parallel which the authors identify as a root cause for poor state coverage and limited skill diversity due to overlapping state visits by different skills. ReST proposes training skills sequentially (or recurrently) coupled with a state coveragebased intrinsic reward aiming to enhance state exploration and skill diversity. The methodology is tested across various 2D navigation and robotic locomotion environments demonstrating improved state coverage and skill diversity over existing parallel training techniques.  Strengths 1. Innovative Approach: The paper successfully identifies and addresses a critical issue  exploration degradation  within unsupervised skill discovery. The introduction of ReST as a shift from parallel to recurrent skill training supported by intrinsic rewards for state coverage is both novel and logically sound. 2. Significant Empirical Gains: The empirical evaluation demonstrates clear advantages of ReST over existing baselines in terms of state coverage and skill diversity. The inclusion of challenging environments enhances the credibility of these claims. 3. WellStructured Analysis: The paper provides a comprehensive theoretical analysis underpinning the exploration degradation phenomenon and rigorously compares the proposed approach against benchmarks offering both qualitative and quantitative insights.  Weaknesses 1. Sample and Computational Efficiency: One major downside as acknowledged by the authors is the reduced sample efficiency and increased computational demand due to the sequential nature of ReST. This could limit the scalability and practical applicability of the approach. 2. Limitation in Handling Continuous Latents: Another critical limitation is the inability of ReST to effectively scale to continuous latent spaces restricting its utility for a broader range of RL tasks that benefit from continuous skill representations.  Questions and Suggestions 1. Enhancing Efficiency: Could the authors elaborate on potential methods or ongoing work that might mitigate the sample and computational inefficiencies of ReST For instance could parallel hardware or approximation techniques to simulate concurrent training offer a compromise 2. Continuous Latents Extension: It would be beneficial if the authors could discuss theoretical or practical barriers to extending ReST to continuous latent spaces and suggest possible pathways to overcome these challenges. 3. Generalization to More Complex Environments: How well does the authors anticipate ReST would generalize to environments with higher complexity or those involving multiagent setups  Limitations and Societal Impact The authors have discussed key limitations notably around computational and sample inefficiency and scalability to continuous latents. Suggesting avenues for improvement in these areas could significantly enhance the paper. Moreover while the discussion around the potential negative societal impact of the work is absent it is crucial for completeness. The proposed method could in theory be applied in automation tasks that replace human labor raising ethical and societal job displacement concerns. For improvements specifically addressing computational efficiency through potential algorithmic optimizations or hardware adaptations could make the approach more practical. Furthermore extending the framework to handle continuous latents would broaden the applicability and utility of ReST in complex realworld tasks. In summary this paper contributes significantly to the field of unsupervised skill discovery in RL with the innovative ReST approach. Addressing the outlined weaknesses and exploring the proposed suggestions could further enhance its value and applicability.", "kMiL9hWbD1z": "Review of \"RTFormer: Efficient DualResolution Transformer for RealTime Semantic Segmentation\" 1) Summary: The manuscript introduces RTFormer an innovative architecture designed to tackle the challenge of realtime semantic segmentation. Leveraging the capabilities of transformer networks which have demonstrated exceptional performance in semantic segmentation RTFormer aims to address the computational inefficiencies that have traditionally limited their application in realtime contexts. The key contributions of the paper include the development of a novel transformer block RTFormer block which operates at dual resolutions to enhance performance while maintaining computational efficiency. Furthermore the paper proposes a GPUFriendly Attention mechanism that simplifies the attention process by omitting the multihead mechanism thus reducing the models computational complexity. Extensive experiments across several mainstream benchmarks including Cityscapes CamVid and COCOStuff demonstrate the superiority of RTFormer in achieving a harmonious balance between segmentation accuracy and computational efficiency with stateoftheart performance reported on all evaluated datasets. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The dualresolution transformer block and the GPUFriendly Attention mechanism are novel contributions that effectively address the computational bottlenecks associated with standard transformer models in realtime segmentation tasks.  Comprehensive Evaluation: The authors present extensive experimental validations across multiple benchmarks which significantly strengthens the credibility of the proposed method. The inclusion of a wide range of datasets showcases the versatility and general applicability of RTFormer.  Practical Impact: Given the growing demand for efficient realtime semantic segmentation in applications such as autonomous driving and robotics RTFormer presents a valuable advancement towards the deployment of transformerbased models in latencysensitive environments. Weaknesses:  Model Complexity Analysis: The paper lacks a detailed discussion on the computational complexity of RTFormer particularly in comparison to existing CNN and transformerbased methods. A thorough analysis highlighting the tradeoffs between model parameters inference time and segmentation accuracy would provide clearer insights into the efficiency of the proposed model.  Experimental Ablations: While the paper does include some ablation studies further exploration into the individual contributions of key components (such as the GPUFriendly Attention and crossresolution attention) could offer deeper understanding of their impact on the overall performance. Questions and Suggestions for the Authors:  Could the authors provide a comparative analysis of the computational complexity (in terms of FLOPs) of RTFormer against leading CNN and transformerbased methods for semantic segmentation  A deeper investigation into the failure cases of RTFormer would be instructive. Are there specific scenarios or classes of objects where RTFormer underperforms and can these limitations be mitigated  Experiments on additional datasets especially those featuring more diverse and challenging scenes could further validate the generalization capability of RTFormer. Are there plans to extend the evaluation to such datasets 3) Limitations and Societal Impact: While RTFormer presents an efficient solution to realtime semantic segmentation the authors could enhance the discussion on potential limitations. For instance the impact of the reduced model complexity on segmentation accuracy in highly complex scenes has not been thoroughly explored. Additionally addressing the scalability of RTFormer to lowpower devices would be valuable for further emphasizing its applicability in edge computing scenarios. In terms of societal impact the paper could benefit from a discussion on the possible negative consequences of deploying autonomous systems equipped with RTFormer such as privacy concerns and ethical considerations around automated decisionmaking. Constructive suggestions include incorporating mechanisms for ensuring model transparency and interpretability thereby facilitating responsible AI development.", "pMumil2EJh": "This paper introduces a novel forecasting model for multivariate time series (MTS) data named Temporal Polynomial Graph Neural Network (TPGNN). TPGNN leverages a dynamic graph structure to model the variable dependence in MTS data marking a significant advancement over traditional methods that employ static graphs. This dynamic graph structure is realized through the formulation of a temporal matrix polynomial which combines a static matrix basis with timevarying coefficients to accurately capture the evolving relationships among variables over time. The authors validate the effectiveness of TPGNN through comprehensive experiments on both synthetic and realworld datasets including traffic datasets and various benchmark datasets. Theoretical analysis is also provided to support the robustness and accuracy of TPGNN in capturing variable dependence. The main contributions include the novel approach to modeling dynamic correlations in MTS data and the empirical demonstration of TPGNNs superior forecasting performance across several benchmarks. Strengths: 1. Novelty in Approach: The use of a temporal matrix polynomial to model dynamic variable dependence in MTS data is a notable innovation. It effectively addresses the limitation of static graph models in capturing the evolving nature of variable correlations over time. 2. Comprehensive Validation: The authors present extensive experimental results to demonstrate the superior forecasting performance of TPGNN covering a wide range of datasets. This is further bolstered by a theoretical analysis that supports TPGNNs ability to accurately capture dynamic variable dependence. 3. Practical Relevance: Given the wide applicability of MTS forecasting across various domains such as traffic control and energy dispatch the improvements offered by TPGNN have significant practical implications. Weaknesses: 1. Model Complexity: While TPGNN demonstrates superior performance the complexity introduced by modeling dynamic variable dependence through a temporal polynomial might pose challenges in terms of computational cost and parameter tuning. 2. Generalization to Different Domains: The paper presents successful applications on traffic and benchmark datasets but it remains unclear how TPGNN performs across a broader spectrum of domains with possibly more complex and less predictable variable dependencies. 3. Exploration of Hyperparameter Space: The paper could benefit from a more indepth discussion on the sensitivity of TPGNNs performance to its hyperparameters especially the order of the polynomial and the dimensionality of the embedding space. Questions and Suggestions for the Authors: 1. Hyperparameter Sensitivity: Can the authors provide insights on how TPGNNs performance varies with changes in key hyperparameters This would be valuable for users trying to adapt TPGNN to different MTS forecasting problems. 2. Computational Cost: Could the authors comment on the computational cost of TPGNN compared to static graph models and provide recommendations on mitigating any potential issues especially for realtime applications 3. Extension to Different Domains: How adaptable is TPGNN to domains with more erratic or less predictable variable dependencies Some discussion on potential modifications or extensions to TPGNN for such scenarios would be beneficial. Limitations and Societal Impact: The authors have extensively discussed the robustness and applications of TPGNN but have not fully addressed its limitations and potential negative societal impacts. For example inaccurate forecasting in critical domains like healthcare or finance due to overlooked model limitations (e.g. response to rare events) could have significant consequences. Additionally the computational complexity of TPGNN may limit its accessibility to users with limited computational resources potentially widening the technology gap. To address these the authors could explore simplifications or optimizations that retain performance while reducing computational demands. Moreover an evaluation of TPGNNs robustness to rare or extreme events would be an important addition alongside a discussion on responsible and transparent usage guidelines to mitigate potential misuse or overreliance on forecasting results.", "vGQiU5sqUe3": "The paper positioned at the intersection of reinforcement learning (RL) and representation learning presents a novel framework called Contrastive RL which fundamentally rethinks the approach to representation learning in RL contexts. Instead of adding representation learning as an adjunct component to an existing RL algorithm the authors propose using contrastive learning methods\u2014traditionally confined to the domain of representation learning\u2014to solve RL tasks directly. Specifically they focus on goalconditioned RL tasks and demonstrate that by applying contrastive learning to actionlabeled trajectories the learned representations can effectively serve as goalconditioned value functions. This methodological pivot not only simplifies the RL algorithm design by removing the need for auxiliary losses or data augmentation but also delivers superior performance on a range of goalconditioned tasks particularly in offline RL settings and with imagebased inputs.  Strengths: 1. Innovative Approach: The conceptual leap to treat contrastive learning as an RL algorithm in its own right is both novel and impactful offering a fresh perspective on bridging representation learning with RL. 2. Performance Gains: The empirical results are impressive especially the demonstrated superiority of Contrastive RL over existing methods in offline RL scenarios and imagebased tasks without the crutches of data augmentation or auxiliary objectives. 3. Generalizability: The proposed method shows versatility across a variety of goalconditioned tasks indicating its broad applicability. 4. Simplicity and Efficiency: The fact that Contrastive RL negates the necessity for separate representation learning mechanisms simplifies the RL algorithm design and potentially reduces computational overhead.  Weaknesses: 1. Scope Limitation: The current focus on goalconditioned RL tasks might limit the applicability of the proposed method. How it extends to a broader set of RL problems remains to be fully explored. 2. Theoretical Underpinnings: While empirical results are strong the paper could benefit from deeper theoretical insights into why contrastive learning synergizes so well with RL especially under the framework proposed. 3. Comparison with Stateoftheart: For a more comprehensive evaluation comparisons with additional stateoftheart methods possibly including very recent advancements in RL might have been enlightening. 4. LongTerm Impact Unclear: It\u2019s uncertain what the longterm implications of this approach might be including potential pitfalls that are not immediately apparent from current evaluations.  Questions and Suggestions for Authors: 1. Applicability to Nongoalconditioned Tasks: Can the authors speculate on how Contrastive RL could be adapted for a wider array of RL tasks that do not inherently involve goalconditioning 2. Theoretical Insights: Any additional theoretical insights or intuitions the authors can provide would be valuable. Specifically what underlies the effectiveness of representing the goalconditioned value function through contrastive learning 3. Comparison Breadth: Including comparisons with more recent or diverse stateoftheart methods might strengthen the claim of superiority. Are there plans to conduct such comparisons 4. Limitations and Societal Impact: While the authors briefly mention limitations a more detailed discussion\u2014especially on potential negative societal impacts\u2014would be beneficial. How might the misuse of goalconditioned RL empowered by efficient representation learning affect privacy security or other critical areas  Limitations: The authors acknowledge the scope limitation of their work to goalconditioned RL problems but do not delve into potential broader implications or limitations of their approach. Particularly the discussion on societal impact is missing. Suggestions for Improvement: 1. Expanding the scope of tasks in future work to include a wider array of RL challenges beyond goalconditioned tasks. 2. Enhancing the theoretical discussion around why contrastive learning blends so effectively with RL in this context. 3. Broadening the comparative analysis to include a wider range of stateoftheart methods. 4. Providing a detailed discussion on the limitations and potential negative societal impacts offering guidance on ethical use and mitigation strategies for possible adverse effects.", "uOdTKkg2FtP": "Review: OffTeam Belief Learning for Enhanced ZeroShot Coordination in MARL 1) Overview and Contributions This paper introduces innovative methodologies aimed at addressing covariate shift challenges in MultiAgent Reinforcement Learning (MARL) particularly in the context of ZeroShot Coordination (ZSC). The authors propose two primary methods: OffTeam Belief Learning (OTBL) and OffTeam OffBelief Learning (OTOBL). The first method enhances the accuracy of a belief model for a policy over a wider array of inputs by leveraging trajectories somewhat aligned with the distribution induced by a differing policy. The latter method computes an OBL equilibrium by considering error weighting according to a distribution induced through crossplay between the training policy and a distinct fixed policy instead of selfplay. The proposals are meticulously examined through experiments within various setups of the game Hanabi showcasing their potential to ameliorate covariate shift issues and enhance generalization towards unseen agents. 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The paper addresses a significant gap in current MARL practices by tackling the covariate shift problem which is pivotal for improving ZSC.  Solid Experimental Design: The experiments are wellstructured covering a variety of scenarios within the Hanabi game to validate the efficacy of the proposed methods against traditional OBL.  Convincing Results: The methods demonstrate appreciable improvements in intraAXP performance and robustness against covariate shifts significantly enhancing zeroshot coordination capabilities.  Thorough Analysis: The paper provides an indepth analysis of the results including discussions on failure cases such as bombing out in Hanabi offering a holistic view of the methodologies impacts. Weaknesses:  Limited Generalization Beyond Hanabi: While Hanabi offers complex and significant challenges relevant to MARL the generalizability of the proposed methods to other domains or settings remains uncertain. Further empirical validation across various games or realworld scenarios would strengthen the papers claims.  Assumption Sensitivity: Both proposed methods rely on certain assumptions regarding the distribution induced by different policies and the approximations for training belief models and actionvalue functions. The sensitivity of these assumptions and their impact on the approaches efficacy outside controlled environments are not deeply explored. 3) Questions and Suggestions for the Authors  Generalization to Other Domains: Have the authors considered testing the proposed methods in environments other than Hanabi perhaps more complex realworld settings to validate their generalizability and practical utility  Analysis of Assumption Sensitivity: How sensitive are the proposed methods to their underlying assumptions An exploration of how varying these assumptions could affect outcomes might offer deeper insights into the methods robustness.  Comparison with StateoftheArt: While the paper positions OTBL and OTOBL against vanilla OBL adding comparisons with other stateoftheart methods in MARL addressing similar challenges could enrich the context and showcase the relative advantages more conclusively.  LongTerm Societal Impact: The potential longterm societal impacts of deploying these methods in realworld scenarios both positive and negative could be elaborated upon. Additionally addressing ethical considerations especially regarding autonomous decisionmaking in sensitive applications would add value. 4) Limitations and Societal Impact The authors acknowledge that the proposed methods are heuristic approximations and recognize the potential limitations arising from this especially in terms of distribution assumptions for OTBL. A more detailed discussion on strategies to mitigate or check the sensitivity of these approximations in more dynamic or unforeseen environments could enhance the paper. Regarding societal impacts the paper briefly touches upon the potential positive implications of improved ZSC capabilities in MARL applications but does not delve into potential negative impacts or ethical considerations. For improvement the authors could explore scenarios where improved autonomous coordination might lead to unintended consequences or require careful governance and oversight supplementing the technical contributions with a more nuanced discussion on broader implications. Conclusion This paper significantly advances the frontiers of research in MARL through the introduction of OTBL and OTOBL directly addressing the covariate shift problem in zeroshot coordination. The detailed experimental analysis coupled with a candid discussion on methods strengths limitations and implications offers valuable insights into MARLs future direction. With further research and development these methodologies hold great promise for enhancing autonomous agent coordination in complex dynamic environments.", "nxl-IjnDCRo": "Summary of the Paper: This paper delves into Diffusionbased Deep Generative Models (DDGMs) which have recently shown outstanding results in generating highquality samples. The core investigation centers around the transformation of a small amount of noise during the backward diffusion process. The authors identify a transitional point in the backward diffusion process that shifts its function from generating corrupted images from noise to denoising these images. With this observation they propose splitting DDGM into two distinct parts: a denoiser parameterized by a denoising autoencoder and a generator which is a diffusionbased model. The authors validate their approach termed DAED through experiments on three datasets (FashionMNIST CIFAR10 and CelebA) exploring its advantages and limitations in terms of data generation and generalization to different data distributions. Assessment of Strengths and Weaknesses: Strengths: 1. Originality: The novel division of DDGM into a distinct denoiser and generator is an innovative approach that adds a new perspective to the understanding and optimization of diffusion models. 2. Clarity and Depth of Analysis: The paper offers a clear and detailed investigation into how noise is managed within DDGMs supported by a robust mathematical and experimental framework. 3. Experimental Validation: The use of multiple datasets for experimentation demonstrates the versatility of the DAED model and strengthens the validity of the proposed approach. 4. Practical Implications: Introducing a distinct denoiser and generator could simplify the training process and improve the efficiency and performance of diffusion models. Weaknesses: 1. Complexity in Implementation: While the division into denoiser and generator parts offers advantages it may also complicate the architecture and training process potentially increasing the barrier to adoption. 2. Lack of Comparative Analysis: The paper lacks a detailed comparison with stateoftheart models in generative modeling which would help in understanding where DAED stands in the broader landscape. 3. Limited Exploration of Negative Societal Impacts: The discussion on potential negative societal impacts is absent which is crucial given the growing concerns around generative models. Questions and Suggestions for the Authors: 1. Generalization Across Datasets: The paper hints at DAEDs potential for generalization. Can the authors provide more insight or data on how DAED might perform on radically different datasets or in tasks beyond image generation 2. Training Complexity: How does the training time of DAED compare to standard DDGMs especially considering the dual architecture 3. Future Extensions: Could the authors speculate on how their proposed division of DDGM into denoiser and generator parts might inspire future architectures or algorithms in generative modeling Concerning Limitations and Potential Negative Societal Impact: The paper provides a promising methodology in the form of DAED but does not adequately address potential limitations and the broader societal impact. Specifically generative models like DAED can be misused for creating deceptive visual content raising significant ethical and societal concerns. Suggestions for Improvement:  Ethical Considerations: It would be beneficial if the paper included a section discussing ethical considerations and potential misuses of the technology alongside guidelines or recommendations for mitigating such risks.  Limitations and Challenges: A more thorough discussion of the potential limitations such as the scalability of DAED to more complex or diverse datasets and the challenges in balancing the performance between the denoiser and generator would provide a more balanced view of the proposed approach.", "oW4Zz0zlbFF": " 1) Briefly summarize the paper and its contributions The paper investigates the generalization performance of overparameterized meta learning models focusing on gradientbased methods such as modelagnostic metalearning (MAML) and its implicit variant (iMAML) within the context of meta linear regression. The authors address the phenomenon of \"benign overfitting\" where counterintuitively overparameterized models do not overfit and perform well on new tasks. A theoretical analysis is presented which explains the conditions under which benign overfitting occurs in metalearning settings through a threepart excess risk formulation that accounts for crosstask variance pertask variance and bias. The analysis highlights how data heterogeneity among tasks and specific conditions of the data covariance matrices influence the generalization performance of overparameterized models. Theoretical claims are supported by numerical simulations.  2) Thorough assessment of the strengths and weaknesses of the paper Strengths:  Novelty: The paper addresses an underexplored aspect of metalearning by providing theoretical grounding in the overparameterized regime specifically investigating benign overfitting.  Technical Rigor: The analysis is mathematically rigorous with a clear exposition of assumptions propositions and theorems. The authors methodically decompose the excess risk and clearly convey the conditions under which benign overfitting occurs.  Relevance: Given the rising popularity of metalearning in handling fewshot learning tasks the work is highly relevant. It provides insights that could guide the design of more effective metalearning algorithms.  Empirical Validation: The theoretical claims are corroborated by numerical simulations thus providing empirical evidence to support the theoretical findings. Weaknesses:  Generalizability: The papers focus on linear models may limit the applicability of its findings to more complex or practical metalearning settings that involve deep neural networks.  Complexity of Theoretical Analysis: While thorough the theoretical analysis is dense and could be challenging for practitioners to apply directly to improve metalearning methods.  Lack of Broader Empirical Evaluation: The numerical simulations while supportive are somewhat limited. More extensive empirical studies across different domains could strengthen the papers claims.  3) Questions and suggestions for the authors  Q1: How do the theoretical insights derived from the linear model setting translate to more complex models especially deep neural networks used in practical metalearning applications  Q2: The paper mainly focuses on benign overfitting. Could the authors comment on the potential for \"malign\" overfitting under different conditions or setups  Suggestion 1: It might be beneficial to extend the empirical analysis to include a broader range of tasks and data types especially those where overparameterization is prevalent in practice (e.g. image and text datasets).  Suggestion 2: A more detailed discussion on the implications of the findings for designing new metalearning algorithms could be helpful. Specifically how can we leverage the insights on benign overfitting to develop betterperforming models  4) Limitations and constructive suggestions for improvement The authors have not fully addressed the limitations concerning the generalizability of their findings beyond linear models and the limited empirical evaluation. To improve:  Extend Theoretical Analysis: Future work should consider extending the analysis to nonlinear models commonly used in metalearning applications. Techniques such as neural tangent kernels could be explored to bridge the gap between linear and nonlinear settings.  Broaden Empirical Studies: Conducting additional experiments with a wider array of tasks data types and model architectures would help validate the theorys applicability in practical settings.  Address Potential Negative Societal Impact: While the paper is focused on a theoretical question reflecting on how the findings might impact the development of AI systems (e.g. efficiency bias in fewshot learning) in societal contexts could provide a more holistic view.", "suplyBhTDjC": " Review of \"Clairvoyant Multiplicative Weights Updates for Convergence to Coarse Correlated Equilibriums in General Games\"  1. Summary The paper presents a novel algorithm Clairvoyant Multiplicative Weights Updates (CMWU) which converges to Coarse Correlated Equilibriums (CCE) in general games. The significant contribution of the CMWU algorithm is its ability to achieve constant regret thus enabling faster convergence to CCE as compared to the stateoftheart uncoupled online learning dynamics. This is achieved by allowing agents to update their mixed strategies based on the notional understanding of \"tomorrows\" actions rather than historical play. Importantly the paper demonstrates that despite CMWU requiring the computation of a fixed point which could make its dynamics computationally intensive and coupled such drawbacks can be mitigated. In particular it shows that when certain conditions on the stepsize are met the CMWU updates can be efficiently computed and implemented in a decentralized manner. The reconceptualization of the approach to online learning and gametheoretic convergence away from timeaverage behavior and towards strategic foresight is both novel and intriguing.  2. Strengths and Weaknesses Strengths:  Theoretical Innovation: The CMWU approach represents a significant theoretical advancement in the domain of online learning and game theory. It challenges the traditional reliance on adversarial regret minimization and instead introduces a more nuanced model of agent behavior that significantly improves convergence rates.  Efficient Implementation: The paper successfully addresses potential computational inefficiencies in implementing the CMWU algorithm by illustrating that under certain conditions updates can be computed quickly via a contraction map. This makes the theory practically implementable.  Improved Convergence Rates: CMWUs ability to achieve an improved convergence rate compared to existing solutions (O(nV logm log TT)) is a compelling advancement offering potential realworld applicability in more efficient algorithms for game theoretical analysis and other related domains. Weaknesses:  Assumption of Clairvoyance and its Practicality: While the theoretical model assumes agents can accurately predict future outcomes the practicality of this assumption in realworld scenarios especially in complex and dynamic environments is questionable.  Complexity and Accessibility: The mathematical rigor and complexity of the paper while a testament to the depth of the research might limit its accessibility to a broader audience who might benefit from understanding the implications of this work.  Potential for Unexplored Negative Societal Impacts: The discussion on the societal impact of CMWU particularly in the context of its applicability in modeling societal or economic systems is somewhat lacking. As algorithms influence more of our decisionmaking processes understanding their broader implications is crucial.  3. Questions and Suggestions for the Authors  Clairvoyance Mechanism Detailing: Could the authors provide more insights or illustrative examples on how the clairvoyance mechanism is implemented in practice especially in complex or rapidly changing game environments  Robustness to Strategic Misrepresentation: How robust is CMWU to strategic misrepresentations or manipulations by agents especially in competitive settings where agents might have incentives to deceive  Extendibility to MultiStage Games: Is the current formulation of CMWU extendible to multistage games or games with more complex dynamics than those covered in normalform games  4. Limitations and Suggestions for Improvement The authors have made a commendable effort in outlining the theoretical underpinnings and potential applications of CMWU. However the discussion on the limitations and the potential negative societal impact of their work could be improved. Specifically:  Broader Implications: Future iterations of this work could benefit from a dedicated section discussing the ethical implications and potential for misuse of such predictive algorithms in socioeconomic systems including oligopolistic markets or political strategy games.  Practical Applicability: More attention could be devoted to discussing the practical applicability and limitations when deploying CMWU in realworld scenarios including the computational resources required and potential performance tradeoffs. Overall this paper represents a significant contribution to the field of game theory and online learning presenting an innovative approach to achieving faster convergence to CCE with potential widereaching implications across various domains.", "sof8l4cki9": " 1) Brief Summary of the Paper and Its Contributions The paper focuses on providing a theoretical foundation for understanding the training dynamics of stochastic gradient descent (SGD) with weight decay (WD) on scaleinvariant losses particularly in neural networks that utilize normalization layers. At its core the paper affirms the Fast Equilibrium Conjecture proposed by Li et al. which suggests that extended training with SGDWD postlearning rate decay enhances final test accuracy even when training accuracy has plateaued. The main contributions are fourfold: 1. The paper offers a stochastic differential equation (SDE)based characterization for the limiting dynamics of SGDWD on scaleinvariant stochastic loss demonstrating that networks with normalization can benefit from SGDWD in terms of generalization. 2. It reveals that SGD without WD on scaleinvariant loss converges slower compared to SGDWD corroborating with empirical observations regarding weight decay\u2019s effect on generalization. 3. Through an analytic approach the paper shows that from any initialization the limiting dynamics of SGDWD converge to a unique stationary distribution under certain conditions providing a partial proof to the Fast Equilibrium Conjecture. 4. The authors perform experiments that not only support their theoretical findings but also highlight the practical implications of their work notably the mixing process and its benefit for generalization.  2) Assessment of Strengths and Weaknesses Strengths:  The paper addresses a significant gap in the literature by providing a rigorous mathematical framework for studying the dynamics of SGD with weight decay in the context of scaleinvariant losses.  The combination of theoretical analysis with empirical validation strengthens the claims and provides a comprehensive understanding of the phenomena under investigation.  By addressing the limitations of previous studies and offering a deeper insight into the Fast Equilibrium Conjecture the work contributes significantly to the theory of deep learning optimization. Weaknesses:  While the paper provides a strong theoretical foundation the assumptions made (e.g. the noise covariance matrix being nondegenerate and the minimizers forming a connected compact and analytic manifold) may limit the generalizability of the results to broader classes of neural networks and training settings.  The complexity of the mathematical analysis could pose a barrier to understanding for readers without a strong background in stochastic calculus and differential equations.  The practical implications of these findings particularly how they could be leveraged to improve training strategies or network designs are not thoroughly explored.  3) Questions and Suggestions for The Authors  Question: Could the authors provide more insight into the potential for extending their analysis to other popular optimization algorithms used in deep learning such as Momentum or Adam  Question: How sensitive are the results and conclusions to the assumptions made regarding the noise covariance matrix and the structure of the minimizers Could relaxing these assumptions significantly alter the findings  Suggestions: It may be beneficial to include a discussion on how practitioners can apply these insights to improve training regimens or network architectures effectively offering a bridge between theory and practice.  Suggestions: Further exploration into less restrictive conditions that still allow for similar analytical treatment could widen the impact of these findings.  4) Limitations and Societal Impact The authors have adequately addressed several limitations inherent to their study particularly regarding the assumptions necessary for their theoretical analysis. However a more detailed discussion on the implications of these limitations for the applicability of their findings in practical settings would be valuable. Regarding potential negative societal impacts the paper does not directly address this aspect. Given the focus on improving generalization in neural networks its unlikely that the work has immediate negative societal impacts. Nonetheless enhancing the understanding of optimization dynamics and potentially leading to more powerful AI systems could inherit the broader ethical and societal implications associated with advanced AI technologies. A brief discussion acknowledging this and suggesting responsible use of such advancements would be constructive. Improvements could include exploring broader conditions under which similar phenomena are observed thus increasing the relevance and applicability of the findings to a wider range of practical scenarios.", "zJNqte0b-xn": "Title: Riemannian Optimization for MinMax Problems: A Journey Beyond Euclidean Constraints 1. Summary of the paper and its contributions: The paper addresses minmax optimization problems within the context of machine learning focusing specifically on scenarios where both the minimization and maximization aspects are confined to Riemannian manifolds. By leveraging the framework established by Zhang et al. who demonstrated the existence of saddlepoint solutions in geodesic convex concave settings on Riemannian manifolds this paper extends the exploration into the realm of Riemannian corrected extragradient (RCEG) and Riemannian gradient descent ascent (RGDA) methods. It aims to determine whether Euclidean space algorithms performance could be matched in the more complex Riemannian setting. Remarkably it answers in the affirmative and proves that RCEG can achieve linear lastiterate convergence rates in the scenario of geodesically strongly convexconcave objectives and both RCEG and RGDA attain nearoptimal convergence rates in stochastic or nonsmooth landscapes factoring in the manifolds curvature. This work progressively opens the door to refining optimization strategies where constraints are inherently nonEuclidean such as in several machine learning and artificial intelligence applications establishing a significant step forward in the robust and efficient tackling of these problems. 2. Assessment of strengths and weaknesses:  Strengths:  Innovative Framework: The paper establishes a novel framework that successfully adapts and extends Euclidean minmax optimization strategies to Riemannian manifolds addressing a critical research gap.  Broad Applicability: By proving the effectiveness of the RCEG method in a Riemannian setting this work potentially broadens the applicability of robust optimization techniques across numerous machine learning applications involving nonEuclidean constraints.  Rigorous Proofs: The theoretical backing including the proof of linear lastiterate convergence for RCEG is thorough and robust providing a solid foundation for future work in this area.  Weaknesses:  Assumption Specificity: The impressive results hinge on specific assumptions such as geodesically strongly convexconcave settings which might limit the generalizability of the findings.  Practical Implementation Detail: Theres a lack of detailed discussion about practical implementation challenges or computational complexity considerations which are crucial for transitioning theoretical algorithms into practical tools.  Validation through Applications: Despite some experimental validation detailed applicationbased demonstrations or case studies of these methods in realworld problems are absent which could have further strengthened the papers impact. 3. Questions and suggestions for the authors:  Could the authors provide more insights into the practical implementation aspects especially concerning computational cost and scalability of the RCEG and RGDA methods when applied to largescale problems  It would be beneficial if the authors could address the possibility of extending their methodology to more generalized nonconvex settings considering the prevalence of such conditions in practical applications.  Further elaboration on the potential limitations or practical considerations when the assumptions (geodesically strongly convexconcave settings) do not hold strictly in applied scenarios would enrich the discussion. 4. Limitations and societal impact:  The paper acknowledges its theoretical nature and the potential limitations arising from the specificity of the assumptions required for its proofs. An improvement could involve exploring the alleviation of these assumptions and investigating the methods performance under less ideal conditions.  Concerning societal impact while the paper doesnt delve into direct implications its necessary to consider the ethical use of advanced optimization techniques ensuring they contribute to societal welfare without exacerbating inequalities or biases in machine learning models. Constructive suggestions include examining how these optimization techniques might affect model fairness and exploring safeguards against misuse.", "tIqzLFf3kk": "Review of \"Rank Diminishing in Deep Neural Networks\" Summary: This paper presents a comprehensive study on the rank behavior of deep neural networks (DNNs) focusing particularly on the phenomenon of rank deficiency as networks grow deeper. The authors theoretically establish that the rank of the network features monotonically decreases with the depth of the network a principle that is further expedited by specific network components and the intrinsic coupling of composite functions. To empirically validate their theoretical findings they propose novel numerical tools to efficiently measure the rank behavior of largescale networks. Their empirical analyses cover a range of network architectures (ResNets MLPs and Transformers) and demonstrate consistent rank diminishing behavior across layers. Moreover the study identifies an \"independence deficit\" phenomenon where classification confidence across different categories becomes linearly dependent indicating a reduced capability of networks to represent categories independently as they grow deeper. The authors provide code for their numerical tools potentially aiding further research in this area. Strengths: 1. Theoretical Contribution: The paper provides a strong theoretical foundation to understand why rank decreases in deep neural networks. The principles of rank diminishing based on differential and algebraic composition are novel and contribute significantly to the existing body of knowledge. 2. Empirical Validation: The empirical analysis across different network types and the development of numerical tools to study rank behavior are significant strengths. These tools overcome previous limitations and could be highly beneficial for the community. 3. Novel Phenomenon Discovery: The identification of the \"independence deficit\" phenomenon is intriguing and suggests new avenues for research into how neural network architectures can inadvertently lead to compromised classification confidence. 4. Interdisciplinary Impact: The paper sits at the intersection of algebra geometry and machine learning providing insights that could be beneficial for researchers across multiple domains interested in the structural properties of deep learning models. Weaknesses: 1. Generalizability Concerns: While impressive the empirical validations focus predominantly on popular architectures like ResNets MLPs and Transformers. It would be beneficial to see how these findings generalize to newer or less common architectures including those used in domains beyond vision and language processing. 2. Potential Overemphasis on Rank: While the rank of network layers is undeniably an essential metric the paper could benefit from a discussion on how these findings relate to other measures of network capacity and efficiency. Comparing rank deficiency with other network properties could provide a more holistic view. 3. Practical Implications: The discussion on the practical implications of rank diminishing and independence deficit is somewhat limited. Specifically how these phenomena might impact tasks beyond classification (e.g. generation reinforcement learning) is not discussed. Providing examples or case studies could enhance understanding and relevance. Questions and Suggestions for Authors: 1. Extension to Other Architectures: Can the authors comment on or provide preliminary results on how these rank behaviors manifest in architectures like GANs or Capsule Networks 2. Relation to Network Pruning and Compression: It would be interesting to see a discussion on the relationship between rank diminishing and network compression techniques. Do networks that exhibit slower rank diminishing also show more resistance to compression 3. Impact on Transfer Learning and Finetuning: How does rank behavior affect transfer learning scenarios where a pretrained model is adapted to a new task Is rank recovery observed or does rank diminishment continue to influence model performance Limitations and Societal Impact: The paper could enhance its discussion on limitations by incorporating potential biases introduced by rank deficiency in classification tasks especially in sensitive applications. Furthermore while the paper briefly mentions potential negative impacts a more exhaustive discussion on how these findings could be used responsibly considering the independence deficit would strengthen the paper. Methods to mitigate these effects deserve further exploration and could form an essential part of future work directions.", "lMrpZ-ycIaT": "Review of \"Exploring Selfsupervised Learning for Mental State Decoding from fMRI Data\" 1. Summary of the Paper This paper introduces novel selfsupervised learning frameworks for decoding mental states from functional Magnetic Resonance Imaging (fMRI) data drawing inspiration from successful techniques in Natural Language Processing (NLP). By creating analogs to sequencetosequence autoencoding causal language modeling and masked language modeling from NLP for neuroimaging data the authors aim to pretrain deep learning (DL) models on a broad dataset covering a diverse set of individuals experimental conditions and acquisition sites. Their pretrained models especially those based on a causal language modeling framework demonstrate superior performance in decoding mental states on benchmark datasets compared to baseline models trained from scratch suggesting a promising direction for utilizing largescale heterogeneous neuroimaging data for mental state decoding. 2. Assessment of Strengths and Weaknesses  Strengths: 1. Innovativeness: The paper innovatively bridges methodologies from NLP and neuroimaging adapting selfsupervised learning principles to a novel application area. 2. Broad Dataset: Utilization of a largescale and diverse neuroimaging dataset enhances the generalizability and robustness of the proposed models. 3. Public Accessibility: Making code training data and pretrained models publicly available promotes transparency and facilitates further research in the field. 4. Comprehensive Evaluation: The thorough evaluation against benchmark datasets provides a solid foundation for assessing the effectiveness of the proposed frameworks.  Weaknesses: 1. Lack of Comparative Analysis with Contrastive Learning: The paper does not explore or compare its approaches against current contrastive learning techniques which are also showing promise in similar domains. 2. Limited Explanation of Model Decisions: Despite acknowledging the importance of understanding the association between decoded mental states and brain activity the paper does not integrate or provide a mechanism for this insight missing an opportunity to enhance the interpretability of model decisions. 3. Questions and Suggestions for the Authors  Comparative Analysis with Contrastive Learning: Can the authors provide insights into how their selfsupervised learning frameworks might perform or benefit from integration with contrastive learning techniques  Enhancing Model Interpretability: Could the authors consider incorporating explainable AI (XAI) methods more directly into their frameworks to facilitate understanding of how model decisions relate to specific brain activities or regions  Impact of Data Heterogeneity: It would be helpful if the authors could discuss more explicitly how the heterogeneity of the datasets (in terms of acquisition parameters and experimental conditions) impacts model training and generalizability. 4. Limitations and Potential Negative Societal Impact The authors recognize the limitation in providing insights into the interpretability of model decisions regarding decoded mental states and brain activity associations. However they do not fully address this in their methodology. In future work incorporating techniques from XAI could substantially mitigate this limitation. Furthermore addressing how their frameworks compare with or could integrate contrastive learning methods would strengthen the paper. The potential negative societal impact seems minimal given the careful handling of data privacy and consent. Nonetheless the authors could discuss potential misuses of mental state decoding technology such as unauthorized surveillance or manipulation and suggest ethical guidelines or safeguards to prevent such issues. Improvement Suggestions: Integrating and discussing XAI and contrastive learning methods could substantially enrich the paper both in enhancing model interpretability and in broadening the comparative analysis of selfsupervised learning frameworks. Additionally a more detailed discussion on ethical considerations and safeguards against potential misuses of mental state decoding technologies would be valuable.", "rQ1cNbi07Vq": "Review for \"Quantifying the Contribution of Frequency Components to CNN Robustness Using Shapley Value\" 1) Summary of the Paper: This paper addresses a critical issue in the field of deep learning particularly focusing on the robustness of convolutional neural networks (CNNs) against adversarial attacks. The authors investigate the hypothesis that CNNs vulnerability is partly due to their reliance on highfrequency components (HFCs) which are less perceptible to humans. Unlike preceding studies that qualitatively analyze the frequency bias by dividing the image spectrum into high and low frequencies this work introduces a quantitative approach by leveraging the Shapley value from cooperative game theory. The Shapley value is employed to ascertain the positive or negative impact of individual frequency components on CNNs in a detailed manner. The paper presents an indepth analysis of adversarial training (AT) and adversarial attacks through the lens of frequency components. It also proposes a new data augmentation technique Classwise Shapley valueguided Augmentation (CSA) aimed at enhancing CNN robustness. The effectiveness of CSA is demonstrated through experimental results on standard image classification benchmarks. 2) Strengths and Weaknesses:  Strengths:  Novel Quantitative Approach: Utilizing Shapley value to quantify the impact of frequency components on CNN robustness is innovative and adds a rigorous mathematical framework to the understanding of adversarial vulnerabilities.  Comprehensive Analysis: The paper offers a thorough investigation into various aspects of CNN robustness including the distinction between standard and adversarial training the effectiveness of adversarial attacks on different frequency components and the role of frequency components in model fairness.  Practical Contribution: The proposed CSA technique is not only grounded in the quantitative analysis provided but also demonstrates practical benefits in improving CNN robustness across several benchmarks.  Reproducibility: By providing the codebase the paper encourages reproducibility and further research in the domain.  Weaknesses:  Limited Scope of Models: The study primarily focuses on CNNs and while it mentions the potential applicability to other architectures like GNNs and transformers these are not explored within the paper.  Potential Overemphasis on Frequency Components: While the paper convincingly argues the significance of frequency components it may risk oversimplifying CNN robustness by not adequately considering other factors or interactions that contribute to adversarial vulnerabilities. 3) Questions and Suggestions for the Authors:  Extension to Other Architectures: Can the authors provide preliminary insights or results on applying Shapley value quantification to architectures beyond CNNs such as transformers or GNNs  Comparison with Other Methods: How does the proposed CSA technique compare with other stateoftheart data augmentation or adversarial training methods in terms of both robustness and computational overhead  Impact of Other Factors: Besides frequency components are there other aspects (e.g. spatial relationships between components) that might significantly influence CNN robustness If so how might these be quantitatively analyzed in future works 4) Limitations and Societal Impact: The paper makes a notable contribution towards understanding and improving CNN robustness. However the focus is largely limited to CNNs and the frequency domain which might not capture the full complexity of adversarial vulnerabilities. Exploring a broader range of models and factors contributing to robustness could provide a more comprehensive understanding. Regarding potential negative societal impacts the paper suggests that there are no easily predictable consequences. Yet any advancement in adversarial robustness can inadvertently contribute to an arms race in adversarial attacks which could have implications for securitysensitive applications. Addressing this broader context might enrich the discussion around the societal impacts of such research. Suggestions for Improvement: The authors are encouraged to explore the applicability of their approach to a wider range of neural network architectures and consider a more multifaceted analysis that incorporates other factors influencing adversarial robustness. Further discussion on the potential dual use of adversarial robustness improvements could also provide a more rounded perspective on the work\u2019s societal impact.", "p_BVHgrvHD4": " Peer Review of \"Understanding Deep Learning Through an InformationTheoretic Lens\"  1) Summary This paper proposes an innovative informationtheoretic framework intended to offer a new perspective on analyzing the data requirements of machine learning algorithms specifically those involving deep and nonparametric neural networks. Traditional theoretical frameworks struggle to accurately predict the learning behavior of deep neural networks due to their vast depth and width. The authors introduce novel notions of regret and sample complexity within this framework allowing for an analysis of the sample complexity associated with deep ReLU neural networks and infinitely wide networks with a bounded sum of weights. They demonstrate that the sample complexity is at most linear and quadratic respectively in network depth. The paper claims these findings resolve the inconsistencies between empirical performance and previous theoretical bounds that either count parameters or exponentially depend on depth.  2) Strengths and Weaknesses Strengths:  Novelty: The proposed informationtheoretic framework pioneers in providing an elegant and intuitive methodology for analyzing deep learning processes that have hitherto been challenging through conventional theoretical lenses.  Theoretical Contribution: The author derives significant theoretical results specifically linear and quadratic bounds on the sample complexity for certain types of neural networks. These results offer a muchneeded theoretical underpinning for the empirically observed efficacies of deep and wide neural networks.  Comprehensive Approach: The paper is methodologically sound delving into the intricacies of prediction errors ratedistortion theory and optimal predictions within its unique framework. The theorems are wellsupported by rigorous proofs appended in the supplementary sections. Weaknesses:  Practical Implementability: While the theoretical framework is robust the paper lacks empirical validation of the derived bounds. An empirical analysis illustrating the practical applicability of the proposed framework could significantly bolster its contributions.  Complexity of Notation and Terminology: The high density of technical terms notation and theoretical concepts may render the paper less accessible to readers not deeply familiar with information theory or deep learning theory.  Limited Scope on Practical Algorithms: The discussion on practical implications of the framework for existing machine learning algorithms and architectures is sparse. Insights into how popular algorithms might fare under this new theoretical lens could enhance the paper\u2019s relevance to a broader audience.  3) Questions and Suggestions for Authors  Empirical Validation: Can the authors provide empirical evidence or simulations that validate the theoretical sample complexity bounds derived in the paper This could greatly enhance the credibility and impact of the findings.  Clarification on the Frameworks Applicability: How generalizable is the proposed informationtheoretic framework Is it applicable to other types of neural networks (e.g. convolutional recurrent) or learning settings (e.g. reinforcement learning)  Discussion on Practical Algorithms: It would be beneficial if the authors could discuss how common deep learning algorithms might be analyzed or modified in light of the proposed framework. Are there specific algorithms that would particularly benefit from or be challenged by this new perspective  Extension to Other Activation Functions: The paper focuses on ReLU neural networks how would the framework extend to networks using other activation functions such as sigmoid or tanh  4) Limitations and Societal Impact The paper provides a critical step forward in understanding the theoretical underpinnings of deep learning. However the authors do not thoroughly address the potential negative societal impacts such as the misuse of efficient learning algorithms for malicious purposes or exacerbating existing biases in data. Including a discussion on the ethical considerations and possible mitigation strategies for the adverse use of deep learning technologies could provide a more holistic view of the frameworks implications. Additionally exploring the limitations related to the practical implementation and scalability of algorithms based on this framework would be valuable. Suggestions for future work could involve extending the robustness of the framework to realworld data complexities and the evolving landscape of deep learning architectures.  Conclusion This paper makes a substantial theoretical contribution to the field of machine learning by introducing an informationtheoretic framework that offers new insights into the sample complexity of deep learning. While its theoretical implications are profound further exploration into the practical applications and empirical validation of the proposed framework would be invaluable for establishing its relevance to current and future machine learning challenges.", "uuaMrewU9Kk": " Peer Review of \"SkillBased MultiTask Offline Reinforcement Learning on Heterogeneous Datasets\"  1) Summary: The paper presents a novel approach to multitask offline reinforcement learning (RL) that leverages heterogeneous datasets of varying quality to learn a single policy capable of handling multiple tasks efficiently. By employing a skillbased task decomposition technique alongside a data augmentation scheme the authors aim to improve the efficiency and robustness of offline RL agents when trained on data lacking in quality or diversity. The technique introduces a joint learning mechanism where tasks are decomposed into subtasks using shared skill embeddings learned via a Wasserstein autoencoder (WAE) and further augments the training data with imaginary trajectories that align with highquality skills. The effectiveness of this approach is demonstrated through experiments with robotic manipulation and drone navigation tasks showing superior performance against stateoftheart algorithms.  2) Assessment:  Strengths:  Innovative Approach: The paper introduces an imaginative solution to the prevalent challenge in offline RL of dealing with heterogeneous datasets. The skillbased task decomposition and the qualityaware regularization are particularly novel aspects that stand out.  Practical Relevance: The work addresses a critical barrier to applying RL in realworld scenarios making it highly relevant. Specifically its focus on leveraging offline potentially suboptimal datasets reflects a practical scenario in many applications.  Experimental Validation: The experiments conducted offer a comprehensive evaluation of the proposed model spanning multiple tasks and settings. The comparison with stateoftheart methods and the performance improvement demonstrated are persuasive.  Weaknesses:  Limited Exploration of Task Heterogeneity: While the paper effectively deals with dataset quality variance it seems to underexplore the implications of task dissimilarity itself on the learning process. The assumption that tasks share common skills might not hold in all multitask settings.  Model Complexity: The models reliance on WAE for embedding tasks and skills into a shared latent space while innovative potentially adds significant complexity and computational overhead. Its unclear if the efficiency gains in offline training offset these costs.  Generalization to Broader Settings: Despite solid experimental results the paper could benefit from a more thorough discussion on the models generalizability across other domains and task settings beyond the robotic manipulation and drone navigation tasks.  3) Questions and Suggestions:  Could the authors clarify the impact of task heterogeneity on the learning process How does the model handle tasks with minimal common skills  Given the models complexity an exploration of computational costs and practical deployment concerns would be beneficial. Could you provide insights into the practical scalability of the proposed approach  The models potential generalizability is of interest. Are there specific domains or tasks where the authors anticipate the model might struggle or conversely excel  It may be constructive to explore the integration of more dynamic skill and task embedding techniques that can adjust to varying lengths and complexities of subtrajectories and tasks.  4) Limitations and Societal Impact: The authors briefly touch upon the challenges related to dataset quality but may not adequately address the broader limitations regarding task diversity and the possible computational inefficiency due to the models complexity. Additionally discussing the potential negative societal impacts such as biases introduced by the quality weighting of datasets or misuse in critical decisionmaking applications would have strengthened the paper. Suggestions for improvement include a deeper analysis of the limitations related to diverse realworld applications and a section dedicated to discussing ethical considerations and potential safeguards against misuse.", "toleacrf7Hv": "The paper under review introduces a novel quantitative criterion for selecting optimal convex potentials in the context of Optimal Transport (OT) problems particularly within a stochastic setting where the ground truth OT map is unknown. The authors leverage the Brenier formulation of OT and demonstrate theoretically and experimentally that their proposed semidual Brenier objective can effectively select the potential that minimizes the quadratic error to the true unknown OT map. This approach is tested against existing models like entropic regularization of OT input convex neural networks (ICNN) and smooth and strongly convex nearestBrenier (SSNB) models. A significant part of the paper also addresses the relevance of OT models in Domain Adaptation (DA) scenarios revealing that the potential closest to the true OT map may not necessarily yield the best results for downstream transfer classification tasks.  Strengths: 1. Theoretical Contribution: The paper presents a solid theoretical framework that backs the proposed method for convex potential selection. The leveraging of the Brenier formulation is both novel and insightful providing a fresh perspective on the problem. 2. Empirical Validation: The authors have conducted extensive synthetic experiments to validate their theoretical claims. These experiments are welldesigned and demonstrate the efficacy of the semidual Brenier objective in selecting optimal convex potentials for various models. 3. Insight into Domain Adaptation: The exploration of the semidual Brenier objectives relevance to DA tasks is particularly interesting. This analysis provides valuable insights into the practical implications of OT models in machine learning applications contributing to the ongoing discussion on the utility of OT in DA.  Weaknesses: 1. Limited Scope in Model Comparison: While the paper effectively showcases the utility of the proposed approach in selecting optimal potentials it primarily compares against a limited set of existing models. Expanding this comparison to include more recent or alternative OT models could provide a more comprehensive understanding of the proposed methods relative performance. 2. Assumptions and Constraints: The strong reliance on smoothness and strong convexity assumptions might limit the applicability of the proposed method especially in cases where these conditions are not met or hard to verify. Addressing these limitations or proposing alternatives could enhance the methods practical utility.  Questions and Suggestions for the Authors: 1. Model Comparison Expansion: Could the authors consider including a broader set of existing OT models in their experimental comparisons to demonstrate the versatility and robustness of the proposed semidual criterion 2. Practical Implications: Given the observed discrepancy between the potential closest to the true OT map and the one yielding the best DA results could the authors elaborate on potential practical guidelines for selecting OT models for DA tasks 3. Addressing Limitations: Are there potential extensions or modifications to the proposed method that could relax the assumptions of smoothness and strong convexity expanding its applicability  Limitations and Societal Impact: The authors acknowledge the limitations associated with the assumptions of smoothness and strong convexity for the potentials. However a more detailed discussion on how these limitations could be overcome or mitigated in practical applications would be beneficial. Moreover considering the broader implications of OT models in machine learning particularly in sensitive applications such as DA discussing the potential for misuse or ethical considerations could add depth to the paper. Constructively future work could focus on the development of more generalizable criteria that apply beyond the constraints of the current assumptions as well as providing clearer guidelines for practitioners in selecting OT models for specific applications.", "lbQTJN42uea": " Peer Review for \"SubquadraticTime Algorithms for Kronecker and Tucker Regressions\"  1) Summary of Contributions The paper introduces a novel algorithm that achieves subquadratictime complexity for solving Kronecker regression problems and its extensions including Kronecker ridge regression and updates to factor matrices in the Tucker decomposition within the Alternating Least Squares (ALS) framework. The algorithm achieves this by leveraging a combination of leverage score sampling iterative methods and a new method for multiplying sparsified Kronecker product matrices. The proposed method significantly reduces the computational complexity from the best previous subcubictime algorithms offering practical improvements in running time for both synthetic data and realworld image tensors which demonstrates its applicability in data mining machine learning and signal processing.  2) Assessment of Strengths and Weaknesses Strengths: a. Theoretical Significance: The paper addresses a computational bottleneck in tensor decompositions presenting the first subquadratictime algorithm for Kronecker regression. This theoretical advancement is substantial in the field of numerical linear algebra and tensor analysis. b. Comprehensive Extensions: The algorithm is not only applicable to Kronecker regression but also extended to other practical problems like Kronecker ridge regression and Tucker decomposition making it highly relevant for a broad range of applications in data analysis. c. Empirical Validation: Extensive empirical analysis demonstrates the efficiency and accuracy of the proposed algorithm providing a strong basis for its practical utility. d. Novel Techniques: The innovative use of leverage score sampling iterative methods and sparsified matrix multiplication techniques are commendable. Particularly the use of the original (nonsketched) Kronecker product as a preconditioner in Richardson iterations is a clever strategy that significantly boosts the algorithms performance. Weaknesses: a. Assumption Dependencies: The performance gains are contingent on certain assumptions such as the ability to compute a spectral approximation and leverage scores efficiently. Realworld applicability could be limited if these conditions are not met or are computationally expensive for certain tensors. b. Hardware and Scalability Not Addressed: There is little discussion on the scalability of the proposed algorithms especially concerning memory usage and parallelization capabilities which are crucial for largescale tensor decompositions. c. Comparison with StateoftheArt: While comparisons are made with the currentfastest algorithms it would be beneficial to include a broader range of benchmarks including stateoftheart tensor decomposition algorithms that do not necessarily rely on Kronecker regression to provide a more comprehensive performance analysis.  3) Questions and Suggestions for the Authors a. Scalability and Parallelization: Can the authors elaborate on the potential for parallelizing their algorithm Given its iterative nature understanding its behavior on multithreaded or distributed systems would be valuable. b. Robustness Analysis: How does the algorithms performance vary with differing tensor sizes ranks and noise levels A robustness analysis could provide deeper insights into its practical applicability. c. Extension to Other Tensor Decompositions: While the focus is on Tucker decomposition could the proposed techniques be adapted for other decompositions like CP or tensor train Insights or preliminary results in this direction could broaden the papers impact. d. Software Implementation: Is there or will there be an opensource implementation of the algorithm Making the code available could enhance reproducibility and facilitate further research.  4) Limitations and Societal Impact The authors have provided a clear discussion on the algorithmic limitations particularly highlighting the assumptions necessary for subquadratic performance. However they could further discuss potential misuse cases especially in sensitive applications involving image data to fully address societal impacts. Suggestions for mitigation such as guidelines for responsible use and ethical considerations would be beneficial for a comprehensive analysis. Improvement Suggestion: Including a section on the potential for bias in tensor data processing and guidelines for ethical use of tensor decomposition in sensitive areas (e.g. facial recognition and biometric data) could significantly contribute to the paper\u2019s societal impact discussion.", "wiBEFdAvl8L": "Review of \"GLIPv2: Unifying Localization and VisionLanguage Understanding\" 1) Summary of the Paper This paper introduces GLIPv2 a model designed to integrate localization (such as object detection and instance segmentation) and VisionLanguage (VL) understanding tasks (such as visual question answering (VQA) and image captioning) into a single framework. It accomplishes this by reformulating localization tasks into what the authors call \"VL grounding tasks\" allowing for a unified pretraining process that benefits from the shared weight updates across tasks. Three novel pretraining tasks are proposed: phrase grounding interimage regionword contrastive learning and masked language modeling which together facilitate this unification. The authors demonstrate that GLIPv2 achieves near stateoftheart (SoTA) performance across a variety of tasks in both categories showcasing its versatility as a generalpurpose vision system. 2) Assessment of Strengths and Weaknesses Strengths:  Unified Framework: The paper\u2019s approach to unifying localization and VL understanding into a single framework using \"VL grounding\" is innovative. It simplifies pretraining processes and resource consumption which are significant hurdles in applying VL models to practical applications.  Novel Pretraining Tasks: Introducing interimage regionword contrastive learning as a new pretraining task is a notable strength enhancing the models ability to learn discriminative features and improving performance across downstream tasks.  Performance: The reported nearSoTA performance on a diverse set of tasks and the strong zeroshot and fewshot adaptation capabilities demonstrate the effectiveness and the wide applicability of GLIPv2.  Opensourcing: Releasing the code is commendable as it encourages reproducibility and facilitates further research and development in the community. Weaknesses:  Data and Model Bias: Although the paper briefly mentions the need for further analysis of data and model biases it lacks a thorough discussion or evaluation on the steps taken to mitigate these issues which are critical in VL tasks due to their impact on fairness and ethical considerations.  Computational Requirements: Despite unification the approach could still be resourceintensive limiting accessibility to those with substantial computational resources. The paper does not discuss the computational costs or propose ways to make it more efficient.  Limited Discussion on Limitations: Other potential limitations of the model such as sensitivity to linguistic variations in prompts or performance across different languages and cultures are not adequately addressed. 3) Questions and Suggestions for Authors  Mitigation of Bias: Can the authors elaborate on any methods or considerations applied during model development to mitigate data and model biases Adding evaluations on bias and fairness could significantly enhance the papers impact and applicability.  Computational Efficiency: Could the authors discuss the computational efficiency of GLIPv2 in more detail Suggestions for making the model more accessible to users with limited computational resources would be valuable.  Language Sensitivity: How does GLIPv2 perform with linguistic variations (e.g. synonyms colloquial language) in prompts Experimentation with linguistic diversity and inclusion of nonEnglish data could broaden the models application scope.  Multilingual and Cultural Performance: Have the authors considered evaluating GLIPv2s performance across multiple languages and cultural contexts This could help understand its global applicability and identify areas for improvement in crosscultural understanding. 4) Limitations and Societal Impact The authors have briefly mentioned the necessity of additional analysis concerning data and model bias but have not included a detailed investigation or mitigation strategies especially considering the potential societal impact. In future iterations incorporating a more comprehensive discussion on these aspects including specific steps taken to ensure fairness and reduce bias would be beneficial. Constructive suggestions include the implementation of bias detection and mitigation techniques during preprocessing training and postprocessing stages as well as diversity and inclusion assessments with respect to the datasets used. Furthermore discussing the environmental impact of training largescale models and proposing efficiency improvements could address the computational limitation concerns.", "s776AhRFm67": "Review for the Paper \"Boosting Adversarial Robustness in Barely Robust Learners\" 1) Summary This paper introduces an innovative algorithm named RoBoost which aims at enhancing the adversarial robustness of machine learning models specifically targeting models that are \"barely robust.\" Barely robust learners are models that exhibit robustness against adversarial perturbations only on a minimal fraction of the data distribution with specific robustness parameters. RoBoost seeks to boost these learners into strongly robust learners capable of maintaining accuracy against larger adversarial perturbations across a more extensive portion of the data distribution. The paper demonstrates the equivalence between the problems of strongly robust learning and barely robust learning establishing a theoretical foundation for RoBoost by emphasizing the necessity of robustness against an expanded set of perturbations. The results are validated through theorems proving that if a model can be learned with barely robust criteria it can also achieve strong robustness with the aid of the proposed boosting algorithm. The paper also explores various implications including the use of unlabeled data for boosting robustness and extends the discussion to multiclass learning scenarios. 2) Strengths and Weaknesses Strengths:  Originality: The paper presents an original approach to enhancing adversarial robustness addressing a significant gap in the literature regarding the boosting of barely robust learners.  Theoretical Rigor: The theoretical grounding provided through the definition of barely robust learners and the subsequent proofs demonstrating the possibility of boosting to achieve strong robustness is both rigorous and compelling.  Practical Relevance: Given the increasing significance of adversarial robustness in machine learning applications the algorithm\u2019s potential for practical implementation especially in scenarios where only barely robust learners are available is highly relevant. Weaknesses:  Experimental Evaluation: The paper primarily focuses on the theoretical framework and offers limited empirical evidence supporting the algorithms effectiveness particularly in realworld datasets or against sophisticated adversarial attacks.  Assumptions and Generalizability: The assumptions required for the barely robust learning criteria and the specific perturbation sets might limit the algorithms applicability across various domains or adversarial settings.  Computational Efficiency: There is insufficient discussion about the computational demands of applying RoBoost in practice especially when dealing with large datasets or complex models. 3) Questions and Suggestions  Could the authors elaborate on the computational efficiency of RoBoost when applied to largescale datasets or elaborate models Any insights into the complexity analysis would be valuable.  It would be beneficial for the paper to include more empirical studies especially on popular datasets subjected to stateoftheart adversarial attacks to validate the theoretical claims.  How does RoBoost handle highly imbalanced datasets or classes with varying degrees of vulnerability to adversarial perturbations  A discussion comparing RoBoost with other adversarial robustness boosting techniques if available could offer readers a better understanding of the methods unique advantages and potential limitations. 4) Limitations and Societal Impact The paper while thorough in its theoretical foundations does not extensively address the algorithms computational efficiency or its scalability to large complex datasets a limitation that could impact practical applicability. Additionally there is minimal consideration of the potential negative societal impacts such as the use of this robustnessenhancing technique to create adversarially resistant harmful content. The authors are encouraged to discuss these aspects possibly suggesting future research directions towards mitigating such risks.", "skgJy0CjAO": " Review of \"LogiGAN: An Unsupervised Adversarial Pretraining Framework for Logical Reasoning Enhancement\"  1) Summary of the Paper This paper introduces LogiGAN a novel unsupervised adversarial pretraining framework designed to enhance the logical reasoning abilities of pretrained language models (PLMs). The methodology involves automatically identifying logical reasoning phenomena within large text corpuses through detection heuristics and subsequently pretraining language models to predict maskedout logical statements. LogiGAN utilizes an adversarial GeneratorVerifier architecture to simulate reflective thinking a process thought to facilitate learning. The frameworks novel approach enables sequential GAN training that overcomes the nondifferentiable challenge typically associated with such models making it computationally feasible for largescale pretraining efforts. The authors report significant performance improvements on 12 datasets requiring general reasoning abilities underscoring the foundational role of logic in reasoning and the effectiveness of the LogiGAN framework.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The adversarial GeneratorVerifier architecture presents a creative solution to enhancing logical reasoning in PLMs drawing inspiration from human learning processes.  Comprehensive Evaluation: The broad evaluation across 12 datasets demonstrates the frameworks effectiveness in improving general reasoning capabilities of PLMs.  Computational Efficiency: LogiGAN\u2019s design addresses and circumvents the nondifferentiable barrier in sequential GANs which is a notable achievement that enhances its applicability to largescale pretraining. Weaknesses:  Limitations in Logic Identification: The reliance on detection heuristics for identifying logical reasoning phenomena may omit complex logical constructs that don\u2019t follow identifiable patterns potentially limiting the diversity and depth of logical reasoning capabilities acquired.  Potential Bias: The models might inherit biases from the corpora used for training particularly in the application of logical reasoning to realworld scenarios which is not thoroughly addressed in the study.  Lack of Detailed Societal Impact Discussion: While the paper briefly mentions the importance of logic in reasoning it does not delve deeply into the potential negative implications of its application such as misuse in generating misleading narratives.  3) Questions and Suggestions for Authors  Clarification on Logic Identification Heuristics: Could the authors elaborate on how the detection heuristics were developed and whether they account for the complexity and diversity of logical reasoning across different domains  Bias and Fairness: How does LogiGAN address potential biases in the training data and are there mechanisms in place to ensure the fairness of logicenhanced models in diverse applications  Exploration of Transfer Learning: Have the authors considered evaluating the transferability of the enhanced logical reasoning abilities to domains or tasks not directly covered in the training datasets  4) Limitations and Potential Negative Societal Impact The paper briefly mentions leveraging detection heuristics for identifying logical phenomena but does not thoroughly discuss the criteria or limitations of these heuristics raising concerns about the completeness and diversity of logic captured for training. Additionally while the enhancement of logical reasoning in language models has significant benefits the authors do not adequately address potential negative societal impacts such as the propagation of biases or the use of these models in crafting persuasive yet misleading narratives. Constructive Suggestions for Improvement:  Expanding the Scope of Logic Identification: Diversifying the methods and heuristics for identifying logical reasoning could help capture a wider range of logical constructs enriching the models\u2019 reasoning capabilities.  Mitigating Bias: Incorporating explicit bias detection and mitigation strategies during the training process could help reduce the potential for perpetuating existing societal biases.  Societal Impact Analysis: Providing a more comprehensive analysis of potential negative impacts and setting forth clear guidelines for ethical use could enhance the responsible deployment of these technologies.", "ufRSbXtgbOo": "The paper tackles the problem of multiagent performative prediction (MultiPfD) where agents learn a common decision vector influenced by their decisions through a decentralized optimization problem. The authors introduce the concept of a multiagent performative stable (MultiPS) solution and analyze the conditions under which this solution exists and is unique. They extend the decentralized stochastic gradient (DSGD) algorithm with a greedy deployment scheme (DSGDGD) and prove its convergence to the MultiPS solution demonstrating that consensus behavior eases the conditions for performative stability compared to the singleagent scenario. The work is validated through numerical experiments on synthetic and real data. Strengths: 1. Novel Approach: The paper expands the performative prediction problem to a multiagent context filling a gap in the literature where most works focus on singleagent scenarios. The decentralized approach is particularly relevant for largescale and distributed systems. 2. Theoretical Contribution: The derivation of a necessary and sufficient condition for the existence and uniqueness of the MultiPS solution is a significant theoretical contribution. The conditions provided enhance the understanding of how consensusseeking behavior impacts performative prediction. 3. Practical Relevance: The DSGDGD scheme proposed is not only theoretically sound but practical addressing realworld issues like data distribution shifts. The application to decentralized learning is well justified given the widespread use of distributed systems in machine learning tasks. 4. Empirical Validation: The experiments conducted including the use of the spambase dataset for an email spam classification task provide strong evidence for the practical applicability of the proposed methods. Weaknesses: 1. Assumption Restrictiveness: The assumption of strongly convex losses may limit the applicability of the findings to broader realworld scenarios where nonconvex loss functions are prevalent. 2. Analysis of Asynchronous Updates: The paper does not address the scenario of asynchronous updates among agents which is common in distributed systems. Investigating how asynchronicity affects the convergence and stability of the proposed solution could be vital for practical deployment. 3. Discussion on Societal Impact: The paper mentions the potential negative societal impact of their work but does not delve deeply into this aspect. Given the nature of performative prediction and its implications a more thorough discussion on ethical considerations and misuse prevention would be beneficial. Questions and Suggestions for the Authors: 1. Could the authors discuss the potential extension of their framework to incorporate nonconvex loss functions This would significantly increase the relevance of their findings. 2. It would be valuable to consider the effect of asynchronous agent updates on the convergence and stability of the MultiPS solution. Could future work address this aspect 3. A deeper exploration into the potential negative societal implications of performative prediction models in a multiagent setting is warranted. Can the authors suggest mechanisms to mitigate such impacts 4. Could the robustness of the proposed DSGDGD scheme to adversarial agents in the network be evaluated Understanding how malicious agents could influence the consensus and stability of the solution is crucial for security. Limitations and Societal Impact: While the paper makes significant theoretical and practical contributions the limitations regarding the assumption of strongly convex losses and the lack of consideration for asynchronous updates need to be addressed in future work for broader applicability. The potential negative societal impacts such as privacy concerns and fairness issues arising from the deployment of multiagent systems were not thoroughly discussed. Given the increasing relevance of decentralized and distributed learning systems understanding and mitigating these risks is essential. Constructive suggestions include exploring regularization techniques to address nonconvex losses investigating asynchronous update scenarios and developing ethical guidelines for the deployment of multiagent performative prediction models.", "sexfswCc7B": " Peer Review  1. Summary of the Paper The paper addresses the issue of undesirable sentencelevel repetitions in largescale neural language models like GPT2 and BART during text generation tasks a phenomenon not commonly observed in human corpora. Through quantitative experiments three primary findings were highlighted: models tend to increase the probability of repeating previous sentences with a selfreinforcement effect sentencelevel repetitions are more likely when sentences have higher initial probabilities. To counteract these tendencies the authors propose a training method named DITTO (PseudoRepetition Penalization) which penalizes sentencelevel repetitions and effectively reduces them without sacrificing perplexity achieving better generation quality across tasks like openended text generation and summarization.  2. Assessment Strengths:  Novelty and Relevance: Tackling the sentencelevel repetition phenomenon in neural language models is crucial for improving text generation quality. This papers focus on understanding the underlying reasons for repetitions and proposing a method to mitigate them is both timely and significant.  Comprehensive Analysis: The quantitative investigation into the selfreinforcement effect of sentencelevel repetitions alongside the exploration of probability dynamics offers valuable insights into model behavior.  Effectiveness of Proposed Solution: The DITTO method not only addresses the repetition problem but does so without negatively impacting perplexity showcasing its effectiveness and practical applicability. Weaknesses:  Limited Discussion on Negative Societal Impacts: While the paper briefly discusses overpenalization a more thorough exploration of potential negative societal impacts such as perpetuating biases or affecting the diversity of generated text could enhance the work.  Generalization Across Languages and Domains: The experiments are focused on English datasets (Wikitext103 CNNDailyMail). Assessing DITTOs effectiveness across languages and different domains would provide a more comprehensive view of its applicability.  3. Questions and Suggestions 1. How does DITTO interact with other strategies to reduce repetition It would be insightful to explore how DITTO complements or competes with other methods like ngram blocking or stochastic sampling in combating repetitions. 2. Impact on Model Bias and Diversity: Could the authors comment on whether penalizing repetitions might inadvertently reduce the diversity of generated text or reinforce model biases Would incorporating a diversitypromoting mechanism within DITTO help 3. Extension to Multilingual and Multidomain Settings: How transferable is the DITTO approach to languages other than English or to specialized domains Investigating this could significantly broaden the methods applicability.  4. Limitations and Negative Societal Impact The authors could further scrutinize the limitations related to the potential negative societal impacts of their work. While DITTO significantly mitigates the repetition problem a discussion on whether this method might limit the diversity of generated content or amplify existing biases would add depth. For instance penalizing repetitions could inadvertently suppress minority viewpoints or nondominant narratives in text generation tasks. Constructive suggestions for improvement would include integrating fairnessaware mechanisms or diversitypromoting penalties into the DITTO framework ensuring that the approach enhances text generation quality without compromising content variety or inclusiveness.", "s_PJMEGIUfa": "Review for \"LanguageInterfaced FineTuning (LIFT) for NonLanguage Downstream Tasks\" 1) Brief Summary: This paper introduces LanguageInterfaced FineTuning (LIFT) a novel approach for applying pretrained language models (LMs) to a wide range of nonlanguage classification and regression tasks without altering the model architecture or loss functions. The authors propose to convert data samples into natural language sentences and then finetune LMs on these sentences. The methodology enables the use of LMs for tasks in domains like image classification and numerical prediction effectively transforming these models into versatile domainagnostic tools that cater to both language and nonlanguage tasks. The paper presents extensive empirical evidence showing that LIFT matches or in some cases exceeds the performance of traditional machine learning baselines on various lowdimensional classification and regression tasks. Moreover the paper explores several properties of LIFT including its inductive bias sample efficiency robustness and its potential for contextaware learning calibrated predictions and data generation. 2) Strengths and Weaknesses: Strengths:  Innovative Approach: LIFT creatively leverages the natural language processing capabilities of LMs for nonlanguage tasks without the need for architectural modifications or custom loss functions pushing the boundaries of \"nocode machine learning.\"  Extensive Evaluation: The comprehensive empirical study across diverse tasks demonstrates LIFTs versatility and effectiveness providing detailed insights into its strengths and potential limitations.  Practical Implications: This approach significantly lowers the barrier to applying deep learning by requiring less specialized knowledge to adapt models to various tasks potentially democratizing access to AI technologies.  Open Source Contribution: By making their code public the authors encourage replication and further exploration bolstering the research communitys understanding of LIFTs capabilities. Weaknesses:  Scalability to HighDimensional Data: The authors acknowledge LIFTs diminished performance on highdimensional regression tasks and large class sets in classification indicating a potential scalability issue.  Limited Exploration of NonLinear Relationships: The paper could benefit from a deeper investigation into LIFTs ability to model complex nonlinear relationships in certain kinds of data.  Potential Overfitting Concerns: While LIFT shows promising performance across various metrics there is little discussion regarding the risk of overfitting especially given the models high reliance on text conversion and sequence processing.  Memory Efficiency: LIFTs requirement for converting and processing data as text may pose efficiency and scalability challenges especially for large datasets or when operating under restricted computational resources. 3) Questions and Suggestions:  Enhancing LIFTs Scalability: Could the authors explore mechanisms or modifications to make LIFT more scalable to highdimensional tasks possibly incorporating dimensionality reduction techniques or more complex encoding schemes for feature representation  Addressing Overfitting: What strategies do the authors recommend or foresee to mitigate overfitting risks in LIFT ensuring the model generalizes well to unseen data  Comparative Analysis with TransformerBased Models for NonLanguage Tasks: How does LIFT compare with other transformerbased approaches explicitly designed for nonlanguage tasks such as Vision Transformers (ViT) for image classification  Memory Efficiency Improvements: Are there planned improvements or alternative approaches to increase the memory efficiency of LIFT possibly through advancements in LM architectures that optimize prompt processing 4) Limitations and Societal Impact: While the authors have identified limitations related to task dimensionality and memory efficiency a more profound exploration of potential negative societal impacts especially bias and fairness issues inherent in pretrained LMs is crucial. It would be beneficial if future work could include a detailed analysis of how biases in language models affect LIFTs predictions across various tasks and propose mechanisms to identify and mitigate such biases to ensure fair and ethical use of the technology.", "sQiEJLPt1Qh": " Review of \"Tighter Upper Bounds on the Complexity of Representing CPWL Functions with ReLU DNNs\"  Summary of the Paper The paper addresses a critical problem in the study of deep neural networks (DNNs) employing rectified linear units (ReLUs): the complexity of representing continuous piecewise linear (CPWL) functions with ReLU DNNs. Past literature has suggested that the number of neurons needed for representing any CPWL function could grow exponentially with the functions complexity making such representations seemingly impractical. Contrary to these daunting estimates this work proposes significantly tighter upper bounds on the required number of hidden neurons. The authors prove that representing any CPWL function requires at most a quadratic number of hidden neurons relative to the number of pieces in the function irrespective of the input dimension. Moreover they provide a polynomial time algorithm for constructing a ReLU DNN that adheres to these bounds for any given CPWL function. The paper also explores the relationship between the number of distinct linear components in a CPWL function and the complexity of its representing network showing that under certain conditions the neural complexity could be lower than previously thought.  Assessment of the Paper Strengths: 1. Significant Theoretical Contribution: The paper makes a groundbreaking stride in narrowing the gap between theory and practicality for ReLU DNNs representing CPWL functions. By proving much tighter bounds on the required number of hidden neurons it refutes the previously held belief that the complexity of such networks becomes prohibitively high as the complexity of the CPWL function increases. 2. Novel Algorithm: The introduction of a polynomial time algorithm for constructing a ReLU DNN based on given CPWL functions is a notable practical contribution. This algorithm not only demonstrates the feasibility of the theoretical bounds but also offers a new tool for researchers and practitioners to design efficient networks. 3. Comprehensive Analysis: The paper provides a thorough examination of the problem including various scenarios of CPWL functions based on the number of pieces and distinct linear components. The detailed exploration contributes to a more nuanced understanding of the ReLU DNNs expressivity for CPWL functions. Weaknesses: 1. Assumptions on Input Dimension: While the bounds are shown to be invariant of the input dimension the practical implications of constructing networks for very highdimensional CPWL functions are not fully explored. The computational costs related to dimensions particularly in terms of the polynomial time algorithms efficiency would benefit from a more detailed discussion. 2. Empirical Validation: The paper predominantly focuses on theoretical results with limited empirical examination. Incorporating more empirical studies or demonstrations showing the algorithms performance in practical settings would strengthen the papers impact. 3. Discussion on Broader Applications: Although the paper touches upon the potential for applying the findings to optimization problems and other related areas it remains somewhat narrow in exploring these applications. Expanding on how these tighter bounds could influence the design and understanding of DNNs in various realworld applications would be valuable.  Questions and Suggestions for the Authors 1. Empirical Validation: Can the authors provide more details on the empirical performance of the introduced algorithm especially concerning computational efficiency and scalability with the dimensionality of inputs 2. Broader Applications: How do the authors envision the application of their findings outside the realm of theoretical analysis Are there specific domains or types of CPWL functions where the proposed bounds and algorithm would be particularly transformative 3. Comparison with Prior Work: The paper would benefit from a more direct comparison with existing approaches for constructing ReLU networks representing CPWL functions. How do the proposed bounds and the algorithm compare in terms of efficiency and practicality  Limitations and Suggestions for Improvement While the authors have made a substantial theoretical contribution they could still address certain aspects to enhance the papers value further:  Empirical Studies: Incorporating comprehensive empirical studies to showcase the practical benefits of the proposed bounds and algorithm would significantly strengthen the paper. Demonstrating the algorithms application to highdimensional data and its scalability would be particularly insightful.  Broadening the Scope of Applications: Expanding the discussion on potential applications including specific examples where the theoretical advancements could lead to practical innovations would make the paper more impactful. Highlighting implications for complex optimization problems or designing AI systems could provide additional depth.  Addressing Limitations: Acknowledgment of the limitations related to the complexity of implementing the proposed algorithm especially for highdimensional CPWL functions would provide a more balanced view. Offering potential solutions or directions for future research to overcome these limitations would be beneficial. In summary the paper is a significant theoretical advancement in understanding and designing ReLU DNNs for representing CPWL functions. Addressing the highlighted questions and suggestions could further elevate its contributions and applicability.", "nYrFghNHzz": "The paper introduces a novel methodology called Supervised Clustering using Adaptive Fusion (SCAF) aimed at optimizing Individualized Treatment Rule (ITR) in precision medicine with a particular focus on cases with a large number of treatment arms. Traditional methods face challenges due to limited observations for specific treatments and the inability to capture the homogeneity within treatment effects. SCAF addresses these issues by utilizing an adaptive fusionbased method for clustering treatments with similar effects while estimating the optimal ITR through a single convex optimization. This approach leverages recent advancements in supervised clustering and introduces a grouplasso based algorithm for enhanced variable selection. Theoretical assurances for the recovery of the true underlying clustering structure are provided alongside empirical demonstrations of SCAF\u2019s superior performance through simulations and a real data application in cancer treatment. Strengths: 1. The innovative combination of supervised and unsupervised learning within a single convex optimization to simultaneously cluster treatments and estimate ITR represents a significant methodological advancement. 2. The inclusion of grouplasso for variable selection to manage dimensionality and improve model accuracy is a thoughtful addition that addresses a critical issue in analyzing highdimensional data. 3. The paper thoroughly substantiates the theoretical foundation of SCAF providing a clear theoretical guarantee of its ability to recover the true clustering structure of the treatments. 4. Empirical validations through simulation and realworld cancer treatment data not only demonstrate the practical utility of SCAF but also its superiority over existing methods. Weaknesses: 1. The paper while thorough in its methodological exposition and validation could benefit from a broader discussion on the computational efficiency and scalability of SCAF especially when dealing with extremely large datasets. 2. The potential impacts of model misspecification or unobserved confounders on the effectiveness of the proposed method have not been sufficiently explored. 3. While the authors briefly mention the possibility of extending SCAF to various types of outcomes (e.g. discrete survival) detailed discussions or preliminary investigations into these extensions are absent. Questions and Suggestions: 1. Could the authors elaborate on the computational requirements of SCAF compared to existing methods Insights into the algorithm\u2019s scalability with increasing data size would be valuable. 2. Given the reliance on clustering treatments based on their effects how robust is SCAF to model misspecification or the presence of unobserved confounders Potential strategies to mitigate such issues could enhance the method\u2019s robustness. 3. Exploring the applicability of SCAF to discrete outcomes and survival data within the paper even if only in a preliminary fashion could significantly broaden the method\u2019s appeal and applicability. Limitations and Societal Impact: The authors have adequately discussed the limitations and potential societal impacts of their work particularly emphasizing its contribution to decisionmaking in precision medicine. However further discussion regarding the ethical considerations around automated treatment recommendations would strengthen the paper. Suggested improvements include incorporating robustness checks against model misspecification detailed exploration of the method\u2019s computational requirements and elaborating on safeguards against the misuse of treatment clustering in clinical settings.", "sjaQ2bHpELV": "Review of \"Optimizing Mixed Policy Spaces Using Online Reinforcement Learning with Application to Combination Therapy\" 1. Summary of Contributions: The paper introduces an innovative approach to online reinforcement learning (RL) for selecting optimal treatment regimens from a policy space characterized by mixed stateaction scopes particularly focusing on combination therapy contexts. The authors develop a novel online algorithm that identifies the optimal policy within unknown Structural Causal Models (SCMs) and exhibits sublinear regret. They introduce a unique parametrization for SCMs that can represent interventional distributions with minimal sets of components enhancing computational efficiency while maintaining asymptotic regret bounds. The papers methodology is applied to various policy optimization scenarios in combination therapy depicting its potential in personalized medicine. 2. Assessment of Strengths:  Novelty: The paper presents innovative algorithms for optimizing mixed policy spaces in an online RL setting within the context of combination therapies contributing significantly to the field of personalized medicine and online learning.  Theoretical Soundness: The theoretical foundations including sublinear regret bounds and the introduction of a novel parametrization for general SCMs are wellestablished with rigorous proofs provided in the appendices.  Practical Relevance: The applicability to combination therapy for diseases like cancer HIV and depression underscores the methods potential for realworld impact especially in the era of personalized medicine.  Algorithmic Efficiency: The authors adeptly address computational intractiveness issues inherent in online RL with mixed policies by offering a more efficient algorithm based on Thompson sampling heuristics which is a significant advancement over existing methods. 3. Assessment of Weaknesses:  Empirical Verification: While the theoretical contributions are strong the paper could benefit from more comprehensive empirical validation. Specifically additional realworld datasets and comparison with stateoftheart methods could enhance the papers impact.  Details on Implementation and Reproducibility: The paper somewhat lacks detail on the practical implementation of the algorithms including computational requirements and scalability. More information could aid in reproducibility and applicability to other domains.  Discussion on Limitations and Societal Impact: The authors briefly touch upon potential negative societal impacts but do not provide an indepth analysis of the limitations particularly in how the algorithms perform under inaccurate model specifications or in the presence of data bias. 4. Questions and Suggestions for Improvement:  Extensive Empirical Validation: Can the authors provide additional empirical studies including applications to realworld datasets and comparison with benchmark methods to showcase the effectiveness and practical viability of the proposed algorithms  Implementation Details: It would be beneficial if the authors could include more details on the computational complexity scalability and requirements for implementing their proposed algorithms alongside guidelines for practitioners.  Limitations and Ethical Considerations: Could the authors expand on the potential limitations of their approach especially regarding model specification errors and data biases Additionally a more thorough discussion on ethical considerations and the mitigation of negative societal impacts would be valuable.  Comparison with Offline Learning Methods: Given the focus on online learning algorithms how do the proposed methods compare with offpolicy or offline learning approaches particularly in scenarios where historical data is abundantly available 5. Limitations and Societal Impact: While the authors briefly address the implications of their work there is room for a more comprehensive discussion on the limitations such as dependency on the accuracy of causal diagrams and potential issues with data quality and availability. Furthermore considering the application in healthcare a thorough exploration of ethical issues including patient consent and data privacy alongside strategies to mitigate biases and ensure equitable treatment across diverse populations would strengthen the paper. Constructive suggestions would include incorporating robustness checks against model misspecifications and advocating for transparency and fairness in algorithm deployment within healthcare settings.", "lMMaNf6oxKM": "Review of \"Towards General Powerful and Scalable Graph Transformers\" 1. Summary: This paper introduces a novel architecture termed GPS (general powerful scalable) for graph Transformers with linear complexity designed to achieve stateoftheart results on various benchmarks. The proposed approach represents a significant step forward in graph representation learning addressing a crucial gap in the existing literature  the lack of a principled framework for designing graph Transformers (GTs) that can scale to large graphs efficiently while maintaining or even improving performance. The authors identify and categorize different types of positional and structural encodings  local global and relative  and argue for a hybrid architecture that combines local message passing with global attention mechanisms. A specific contribution is the formulation of a general and scalable graph Transformer blueprint that incorporates local messagepassing mechanisms and global attention facilitated by a novel approach to handling positional and structural encodings. Their experimental setup conducted across 16 benchmarks demonstrates the architectures competitiveness and showcases the empirical benefits of the proposed modularity and strategy combination. The authors also contribute an opensource package GRAPHGPS intending to facilitate further research in this area. 2. Assessment: Strengths:  Novelty and Relevance: The proposed GPS architecture addresses a significant challenge in graph representation learning providing a scalable solution without sacrificing the model\u2019s expressiveness or performance on large graph datasets.  Comprehensive Empirical Evaluation: The paper includes an extensive set of experiments across multiple benchmarks supporting the claimed benefits of the proposed approach. This thorough evaluation underscores the models generalizability and efficacy.  Clarity and Depth in Positional and Structural Encoding Analysis: The clear categorization and indepth discussion of positional and structural encodings contribute significantly to the field\u2019s understanding setting a foundation for future work in designing more effective graph Transformers.  OpenSource Contribution: The availability of GRAPHGPS as an opensource package is a commendable aspect of the work that can accelerate advancements in graph Transformers research. Weaknesses:  Hyperparameter Sensitivity: A notable limitation is the models sensitivity to hyperparameters as pointed out by the authors. This could present challenges in practical applications where optimal settings may not be easily discernible.  Need for More Challenging Benchmarks: The authors acknowledge the lack of graph datasets that necessitate longrange dependencies hinting at a potential limitation in fully assessing the scalability and performance benefits of linear attention mechanisms.  Limited Discussion on Limitations and Societal Impact: While the authors briefly mention the work\u2019s limitations and potential societal impact a more detailed exploration would have been beneficial particularly in assessing the ethical implications of its applications. 3. Questions and Suggestions for Improvement:  Hyperparameter Optimization: Can the authors provide guidelines or insights on how to effectively approach hyperparameter tuning for different types of datasets especially given the architectures sensitivity to hyperparameters  Extensibility to Other Domains: How easily can the proposed GPS architecture be extended or modified to accommodate other types of structured data beyond the graph domain Are there inherent limitations in its current formulation  Benchmarking on More Challenging Datasets: While the current set of benchmarks is comprehensive it would be enlightening to see the architecture tested on more challenging datasets or scenarios that necessitate exploiting longrange dependencies to their fullest.  Further Discussion on Societal Impact: The authors could expand on the potential societal impacts both positive and negative of their work. For instance elaborating on specific applications (e.g. drug discovery privacy implications) could provide a nuanced understanding of where GPS might have significant societal implications. 4. Limitations and Societal Impact Improvement: The authors could strengthen the paper by providing a deeper analysis of the limitations particularly focusing on practical application scenarios where hyperparameter sensitivity might pose significant challenges. Moreover a more detailed discussion on the potential negative societal impacts guided by concrete examples would enrich the readers understanding of the cautious and responsible use of the proposed architecture. Suggestions include developing guidelines for ethical use and exploring mechanisms to mitigate risks of misuse in sensitive applications.", "ywxtmG1nU_6": "Review of \"Equivariant Graph Hierarchybased Network (EGHN) for Modeling MultiBody Systems\" 1. Summary This paper introduces the Equivariant Graph Hierarchybased Network (EGHN) a novel framework to model and analyze the dynamics of complex multibody systems through hierarchical graph neural networks maintaining Euclidean equivariance. The proposed architecture includes three main components: Equivariant Matrix Message Passing (EMMP) for enhanced message passing EquivariantPooling (EPool) for abstracting lowerlevel nodes into highlevel clusters and EquivariantUpPooling (EUnPool) for reconstructing detailed information from higherlevel data. By incorporating these mechanisms EGHN aims to overcome the limitations of flat message passing in current Equivariant Graph Neural Networks (EGNs) by capturing spatial and dynamical hierarchies enabling improved substructure discovery and information fusion. The effectiveness of EGHN is demonstrated through comprehensive evaluations on tasks including multiobject dynamics simulation motion capture and protein dynamics modeling showing superior performance over stateoftheart methods. 2. Evaluation Strengths:  Innovative Approach: The integration of a hierarchical structure into an equivariant graph neural network framework is a forwardthinking approach. It efficiently addresses the challenge of capturing complex spatial and dynamical hierarchies in multibody physical systems a significant leap over existing methods.  Solid Theoretical Grounding: The paper provides a sound mathematical basis for the equivariance of the proposed EPool and EUnPool procedures enhancing the credibility of the approach.  Comprehensive Evaluation: Extensive experiments across multiple domains including Mcomplex systems motioncapture and protein dynamics modeling not only corroborate the models effectiveness but also demonstrate its versatility. Weaknesses:  Model Complexity and Scalability: While EGHN shows outstanding performance the hierarchical structure and multiple components may introduce significant complexity potentially affecting scalability and computational efficiency particularly for very large systems.  Fixed Number of Clusters: The requirement to predetermine the number of clusters (K) could limit the models adaptability to systems of varying complexities and scales. Dynamically adjusting K could be a possible improvement but is not addressed in the current model. 3. Questions and Suggestions  Scalability Concerns: Can the authors comment on the scalability of EGHN especially in the context of systems with a very high number of nodes Suggestions to improve computational efficiency might be valuable.  Dynamic Cluster Adjustment: Could the authors explore methods for dynamically adjusting the number of clusters (K) within the EPool component to enhance the models adaptability to various system scales  Generalization to Other Domains: While the current evaluation focuses on physical systems could EGHN be applicable to other domains such as social network analysis or financial systems where hierarchical patterns also exist 4. Limitations and Societal Impact The authors have demonstrated the effectiveness of EGHN across various domains but havent explicitly discussed its limitations or potential negative societal impacts. While the fixed cluster size is a noted technical limitation potential broader impacts especially when deploying in critical realworld applications (like biomedicine) could have been addressed. Future iterations of this work could benefit from a section discussing ethical considerations misuse potential and mitigation strategies to ensure responsible use especially given the growing concerns around the societal implications of advanced AI technologies. Conclusion The proposed Equivariant Graph Hierarchybased Network represents a significant advancement in modeling complex multibody systems with its ability to capture intricate spatial and dynamical hierarchies conspicuously outperforming equivalent stateoftheart methods. While there are avenues for further enhancement particularly in model scalability and adaptability this work lays a solid foundation for future research in this area.", "xatjGRWLRO": "Review of \"SelfSupervised Pretraining for LargeScale 3D Point Clouds\" 1. Summary: The paper introduces a selfsupervised pretraining method that focuses on largescale 3D scenes targeting both indoor and outdoor environments represented through point clouds. This method leverages local and global contrastive learning specifically designed to handle largescale 3D point clouds like those obtained from LiDAR scans and dense depth cameras. The core innovation lies in a novel ClusterInfoNCE loss which efficiently performs contrastive learning across a large set of local volume features. The effectiveness of this method is demonstrated through significant improvements in downstream tasks of 3D object detection and semantic segmentation showing clear advancements over current baselines. 2. Strengths and Weaknesses:  Strengths:  The paper addresses a critical gap in the pretraining of models for largescale 3D point clouds a less explored area compared to 2D image pretraining.  Introducing a local and global contrastive learning approach tailored for 3D data is innovative. It effectively leverages spatial hierarchies and feature distributions unique to 3D point clouds.  The empirical results show substantial improvements over the baselines indicating the practical value of the proposed method.  The algorithm\u2019s flexibility to perform well on both indoor and outdoor scenes highlights its robustness and general applicability.  Weaknesses:  The paper does not fully detail the computational complexity and scalability of the proposed method especially in terms of training time and memory requirements.  There is a lack of discussion on the sensitivity of the model performance to the choice of hyperparameters such as the size of local volumes and the number of clusters in ClusterInfoNCE loss.  The potential applicability of the method to tasks beyond object detection and semantic segmentation or to data beyond LiDAR point clouds and depth camera data is not explored. 3. Questions and Suggestions:  How does the computational complexity of the proposed SSPL method particularly the training time and memory usage compare with existing methods like DepthContrast and PointContrast  It would be beneficial to include an analysis of the models sensitivity to various hyperparameters providing guidelines for practitioners on how to adjust these parameters based on different datasets or computational constraints.  Exploring the applicability of the proposed method to other 3D perception tasks (e.g. 3D reconstruction or pose estimation) or other forms of 3D data (e.g. structured light) could widen the impact of this work.  Given the improvements observed with the proposed method adding more detailed visualizations or qualitative analyses of the learned representations could offer deeper insights into why and how the method outperforms baselines. 4. Limitations and Societal Impact: The authors could more explicitly acknowledge the limitations related to the scalability of the method and its dependence on domainspecific data for pretraining. Addressing these limitations could involve discussing strategies for reducing computational costs or exploring unsupervised domain adaptation techniques. Regarding societal impact the paper briefly mentions potential improvements in the reliability and fairness of perception models stemming from less reliance on biased annotation processes. A more thorough discussion on this aspect including potential negative impacts such as privacy concerns or misuse in surveillance applications would contribute to a balanced view on the broader implications of this technology.", "kZnGYt-3f_X": "1) Briefly summarize the paper and its contributions This paper introduces a novel knowledge distillation method aimed at transferring structural information from computationally expensive 3D convolutional neural networks to more efficient 2D networks. The core contribution includes the Hilbert Distillation (HD) method which employs the Hilbert curve to map multidimensional data to a onedimensional space while preserving the structural information of the original data. Additionally the paper proposes a Variablelength Hilbert Distillation (VHD) technique that dynamically adjusts the Hilbert curve mapping based on feature activation focusing the learning process on the most informative features. The proposed methods significantly improve the performance of distilled 2D networks on two classification tasks making them competitive with their 3D counterparts and offering a potential reduction in computational costs for processing volumetric data. 2) Thorough assessment of the strengths and weaknesses of the paper Strengths:  The novel use of the Hilbert curve to maintain structural information in a dimensionality reduction context is innovative and theoretically sound.  The Variablelength Hilbert Distillation technique is a clever addition that improves the efficiency of knowledge transfer by focusing on areas of activation.  The paper provides extensive experimental evidence showing the superiority of the proposed methods over stateoftheart distillation techniques in terms of performance in specific tasks.  The potential of the distilled 2D networks to replace more cumbersome 3D models could have significant implications for computational resource reduction in realworld applications. Weaknesses:  The paper might not fully explore the limitations or the scalability of the proposed methods across a wider range of datasets and tasks outside of the two classification tasks investigated.  Theres a lack of discussion on the computational overhead introduced by the Hilbert curve computation and its optimization during the distillation process.  The method relies heavily on the quality and characteristics of the 3D teacher models and its unclear how variations in these models could impact the efficacy of the knowledge transfer to 2D networks. 3) Questions and suggestions for the authors  Could you elaborate on how the Hilbert curve computation and mapping process impact the overall computational efficiency of the distillation process  It would be beneficial if the paper could include a more detailed analysis of the proposed methods performance across a broader range of datasets particularly in domains distinct from video and medical imaging.  Considering the reliance on 3D teacher models are there specific characteristics (e.g. architecture training data) that make certain models more suitable for use with the HD and VHD methods  A suggestion is to explore the integration of the proposed distillation technique with network pruning or quantization methods to further optimize the efficiency of the resulting 2D networks. 4) Limitations and potential negative societal impact The authors somewhat address the limitations in terms of the methods application scope however there is a lack of discussion on the computational costs associated with the Hilbert curve computations themselves. Moreover the paper does not consider potential negative societal impacts directly. Its conceivable that if successfully adopted on a wide scale these methods could lead to obsolescence of existing 3D modelbased systems and associated skillsets potentially causing economic and professional disruptions. To improve the authors could examine both the computational scalability of their approach and consider conducting an ethical impact assessment of their proposed innovations especially focusing on longterm implications of widespread adoption in critical sectors like healthcare.", "yb3HOXO3lX2": "Title: On the Formalization of Reward Function Hackability and Unhackability in Reinforcement Learning 1) Summary of the Paper This paper introduces the first formal definition of reward hacking in the context of reinforcement learning (RL). Reward hacking occurs when the optimization of a proxy reward function (`R\\xcc\\x83`) diverges from the intended outcomes prescribed by the true reward function (`R`). The authors propose criteria for a proxy to be considered unhackable exploring the implications of their definitions through theoretical analyses and practical examples including a cleaning robot scenario. This work delineates between hackable and unhackable reward functions introduces the concept of simplification as a special case of unhackability and examines conditions under which nontrivial simplifications exist. The findings suggest a tension between the use of simplified reward functions for task specification and the broader goal of aligning AI systems with human values. 2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The paper addresses an underexplored yet critical aspect of RL\u2014how proxy reward functions can misalign with intended outcomes presenting a formal approach to defining and analyzing hackability.  Theoretical Depth: It provides a rigorous theoretical foundation for understanding the conditions under which reward functions can be considered hackable or unhackable alongside the notion of simplification.  Practical Relevance: The examples used (e.g. the cleaning robot scenario) aptly demonstrate the realworld implications of reward hacking enhancing the papers relevance to practical AI system design.  Implications for Future Research: This work opens multiple avenues for further exploration particularly in finding ways to design or identify unhackable reward functions in more complex and varied settings. Weaknesses:  Complexity of Theoretical Framework: The theoretical concepts while foundational are complex and may be challenging for readers not deeply versed in the mathematical aspects of RL.  Limited Empirical Analysis: The paper primarily focuses on theoretical insights with limited empirical validation of the proposed definitions and conditions.  Applicability to General Environments: The discussion is mainly constrained to finite MDPs and deterministic policies potentially limiting the generalizability of the findings to more complex or continuous environments. 3) Questions and Suggestions for the Authors  Empirical Validation: Can the authors present empirical studies or simulations that illustrate the occurrence of reward hacking in more diverse and complex environments possibly comparing different methods for identifying or mitigating hackable situations  Extensions to Continuous Environments: How might the concepts of hackability and simplification extend to continuous state and action spaces Are there additional challenges or considerations in such settings  Impact on Learning Algorithms: Could the authors speculate on or discuss how different RL algorithms might be more or less susceptible to reward hacking based on their optimization strategies and behaviors  Practical Guidelines: It would be valuable if the paper could offer more concrete guidelines or principles for designing unhackable reward functions or for modifying existing functions to mitigate hackability risks. Limitations and Societal Impact The authors recognize their works limitations particularly its focus on finite MDPs and deterministic policies leaving room for further research in more general settings. While the strict definition of hackability used is an excellent starting point for theoretical analysis it might not capture all nuances of reward hacking in practical applications. The paper could benefit from a more indepth discussion on the potential negative societal impacts of reward hacking such as unintended behaviors in safetycritical applications (e.g. autonomous driving healthcare) and suggestions on safeguarding against these risks. Constructive suggestions include expanding the scope to consider continuous environments and more complex nondeterministic policy sets and incorporating empirical studies to validate and extend theoretical findings.", "uOQNvEfjpaC": " Review of \"What Is Where by Looking: Weaklysupervised Openworld Phrase Grounding\"  Summary The paper introduces a novel weaklysupervised method for phrase grounding in an openworld setting referred to as \"What Is Where by Looking\" (WWbL). This task involves detecting and describing objects within an image without prior knowledge of the objects existence during training or the need for detailed bounding box annotations. The approach leverages two pretrained models CLIP and BLIP for imagetotext matching and image captioning respectively. The method is trained on the COCO dataset using only image captions and without the need for bounding box data. It involves a twophase process combining foreground segmentation object proposal extraction and caption generation to detect and describe objects. The proposed approach demonstrates stateoftheart performance in weakly supervised segmentation phrase grounding and the novel WWbL task compared to existing methods.  Strengths 1. Novelty: The paper addresses a unique and challenging problem of openworld visual phrase grounding without relying on bounding box annotations or predefined object classes which is a significant step forward in weakly supervised learning and computer vision. 2. Integration of Pretrained Models: Efficiently leveraging the strengths of CLIP for imagetotext matching and BLIP for caption generation exhibits an innovative use of existing technologies to solve a new problem showing good modular design. 3. Performance: The quantitative results presented show that the method outperforms existing stateoftheart methods in weakly supervised object localization and phrase grounding tasks while also demonstrating promising results for the novel WWbL task. 4. Resource Accessibility: Providing the code and a live demo is highly beneficial for reproducibility further research and practical applications fostering a transparent and open scientific community.  Weaknesses 1. Limited Exploration of Limitations: While the paper provides an exhaustive assessment of the methods strengths it could benefit from a more detailed discussion on the weaknesses and limitations particularly regarding the handling of multiple instances of the same object the methods performance on objects of varying sizes and potential biases within the datasets and pretrained models. 2. Societal Impact and Ethical Considerations: The paper briefly touches upon potential cultural biases inherent in the dataset and model training but does not delve deeply into the societal impact and ethical considerations of deploying such technology which could be crucial given its broad scope of application.  Questions and Suggestions for the Authors 1. Handling of Multiple Object Instances: Can the authors elaborate on how their method would perform with multiple instances of the same object within an image especially without explicit instance differentiation in the training phase 2. Generalization Capabilities: Further insights into how the model performs on datasets with significantly different distributions from COCO and BLIPs training data would be valuable. Are there any strategies that could be employed to improve generalization 3. Bias and Fairness: What measures can be taken to mitigate the cultural and sociodemographic biases that may arise from the datasets and pretrained models Additionally how can the impact of potential misidentification or misdescription of objects be minimized in critical applications 4. Extension to Video: The authors mention the potential extension to video data as future work. Could they provide preliminary thoughts on the challenges and considerations in adapting the current model to handle dynamic scenes and temporal information  Limitations The authors have acknowledged the potential cultural bias stemming from the training datasets and pretrained models which is commendable. However a more comprehensive discussion on strategies for addressing these biases and other limitations such as the models capability to deal with cluttered scenes and occlusions would further strengthen the paper. Suggestions for improvement include exploring data augmentation techniques synthetic data generation for underrepresented classes or incorporating fairnessaware learning algorithms to reduce bias.  Conclusion Overall the paper presents a significant contribution to the field of computer vision by tackling the challenging task of openworld weaklysupervised visual phrase grounding. Despite minor shortcomings in the discussion on limitations and societal impacts the methods innovative use of pretrained models impressive performance and potential applications mark a noteworthy advancement in weakly supervised learning and phrase grounding.", "wjSHd5nDeo": " Review of \"MultipleSample Neural Image Compression with Improved Importance Weighted Autoencoder\"  1) Summary This paper tackles the optimization challenge in lossy neural image compression (NIC) by introducing a novel approach dubbed MultipleSample Neural Image Compression (MSNIC). The study builds on the framework of importance weighted autoencoders (IWAEs) to propose a solution that enhances stateoftheart NIC methods. The fundamental proposition is to train NIC using a multiplesample IWAE target a method that is claimed to achieve tighter evidence lower bound (ELBO) leading to richer latent space learning. The authors identify and analyze the impact of NICs uniform variational posterior on the variance and bias of gradient estimators. They provide insights into the gradient variance of NIC training and propose MSNIC as an enhancement that can easily be applied to other neural compression tasks.  2) Assessment  Strengths:  Innovation: The paper presents an innovative approach to neural image compression by integrating the multiplesample IWAE target which is notably absent in the current literature.  Comprehensive Analysis: The analysis of the impact of uniform variational posterior on gradient estimators in NIC showcases a deep understanding of the problem and provides a solid theoretical foundation for the proposed MSNIC.  Experimental Results: The demonstrated improvements over stateoftheart NIC methods especially through experimental comparisons add significant value by showing practical advancements besides theoretical propositions.  Weaknesses:  Marginal Performance Improvement: The paper acknowledges that the improvement in RD (ratedistortion) performance is marginal particularly when benchmarked against models possessing spatial context models.  Limited Evaluation Scope: The evaluation seems restricted to a few stateoftheart methods and lacks a broad comparison across a wider range of NIC models. Including more recent and diverse methods could validate the effectiveness more convincingly.  Discussion on Computational Overheads: While enhancing RD performance and latent space learning theres little mention of the computational overheads or efficiency tradeoffs involved with implementing the MSNIC which is crucial for practical adoption.  3) Questions and Suggestions for the Authors:  Can you elaborate on the computational costs associated with MSNIC compared to conventional methods especially concerning training and inference times  Considering the marginal improvements in some cases what specific scenarios or conditions would most benefit from MSNIC Are there image types or qualities where MSNIC significantly outperforms conventional NIC methods  To strengthen your contributions could you consider evaluating the MSNIC on a broader set of NIC models possibly including those with different architectural designs or loss functions  Future Work: Youve briefly mentioned extending to neural video compression. Could you speculate on the potential challenges and adjustments needed for MSNIC to be applicable in the video compression domain  4) Limitations and Societal Impact The authors touch upon the limitations regarding the marginal improvements in RD performance with MSNIC particularly for models with spatial context models like Cheng et al. [2020]. A more extensive discussion on computational efficiency and scalability across different hardware or application scenarios would provide a wellrounded view of the proposed methods applicability and limitations. In terms of societal impact the paper does not address potential negative aspects. Future iterations could benefit from a discussion on the ethical considerations and environmental impact of deploying advanced compression models considering the increasing computational demands of stateoftheart machine learning models.  Suggestions for Improvement:  Providing a deeper analysis of the efficiency tradeoffs involved with MSNIC in comparison to standard approaches would be valuable.  Expanding the evaluation to include a more extensive array of NIC models could solidify the claims of MSNIC\u2019s advancements.  Discussing the broader societal and ethical implications including the environmental impact of implementing such advanced models would enrich the papers perspective on its contributions.", "w0QoqmUT9vJ": " Summary This paper introduces a theoretical framework for assessing the expressiveness of subgraphenhanced graph neural networks (GNNs) and proposes a new datadriven approach for learning to sample subgraphs which is shown to increase predictive performance while reducing computation time. The authors define kordered subgraph aggregation networks (kOSANs) and demonstrate their relationship with the WeisfeilerLeman hierarchy showing that increasing the size of subgraphs invariably leads to increased expressiveness. They employ a novel method for backpropagating through discrete structures to learn subgraph sampling in a datadriven manner contrasting with previous approaches that either considered all subgraphs used random sampling or relied on handcrafted heuristics. Empirical evaluations on standard benchmark datasets show the superiority of the proposed datadriven kOSAN models over nondatadriven counterparts in terms of prediction accuracy and computational efficiency.  Assessment Strengths: 1. Theoretical Contribution: The paper successfully bridges the gap between various subgraphenhanced GNN approaches by introducing a unified theoretical framework. This significantly contributes to the understanding of these models expressiveness and their relationships to the WeisfeilerLeman hierarchy. 2. Practical Innovation: The introduction of a datadriven subgraph selection mechanism is a notable advancement. It addresses a significant limitation in current subgraphenhanced GNN models by moving away from arbitrary or exhaustive subgraph selection towards a more efficient and performanceoriented approach. 3. Empirical Evaluation: The comprehensive empirical evaluation provides strong evidence of the proposed methods effectiveness. The comparison with nondatadriven approaches on multiple benchmark datasets highlights the practical benefits of the proposed model in terms of prediction accuracy and computation time. Weaknesses: 1. Generalization and Scalability: While the paper presents an evaluation on several benchmark datasets the analysis of the models scalability to very large graphs and its generalization capabilities across diverse domains is limited. Further exploration in this direction would strengthen the claims. 2. Comparison with StateoftheArt Models: Although the paper compares the proposed model with nondatadriven subgraphenhanced GNNs a direct comparison with the latest stateoftheart GNN models in similar tasks would provide a clearer picture of the proposed methods competitiveness. 3. Model Complexity and Interpretability: The introduction of datadriven subgraph sampling increases the models complexity which could pose challenges in interpreting the models predictions. The paper could benefit from a discussion on this aspect and potential strategies to address it.  Questions and Suggestions for the Authors 1. Scalability and Generalization: Can the authors provide additional insights into the models scalability to larger graphs not covered in the empirical evaluation Moreover how well does the model generalize across diverse domains beyond the evaluated datasets 2. Comparison with Latest GNN Models: It would be beneficial to include a comparison with the latest stateoftheart GNN models to understand the proposed methods position in the wider landscape of GNN research. 3. Model Interpretability: Considering the increased complexity introduced by the datadriven subgraph sampling method what strategies do the authors suggest for interpreting the models decisions especially for domain experts  Limitations and Societal Impact The paper does not thoroughly discuss the limitations related to the models scalability and interpretability. A discussion on these aspects along with potential negative societal impacts like the misuse of predictive models in sensitive applications would be a valuable addition. The authors are encouraged to explore these areas and provide constructive suggestions for improvement. Overall the paper makes significant theoretical and practical contributions to the field of graph neural networks. Addressing the highlighted concerns would further enhance its impact and breadth.", "o8H6h13Avjy": "Review of \"MExMI: Enhancing Active Model Extraction with Membership Inference for Robust Machine Learning as a Service\" 1. Summary of the Paper The paper presents an innovative framework MExMI targeting the intersection of Model Extraction (ME) and Membership Inference (MI) attacks within the realm of Machine Learning as a Service (MLaaS). It successfully demonstrates how exploiting the synergy between ME and MI can substantially enhance the efficacy of both attacks. The paper introduces a poolbased active model extraction mechanism that significantly improves ME attack fidelity while simultaneously reducing the query cost associated with MI attacks. The framework MExMI builds upon three core modules: MI PreFilter MI PostFilter and Semisupervised Boosting to refine the adversarys active learning process. The experiments showcased in the paper illustrate MExMIs capability to outperform existing stateoftheart techniques in both ME and MI attacks across various datasets and models. 2. Assessment of Strengths and Weaknesses Strengths: a) Innovativeness: The paper introduces a novel perspective by combining ME and MI attacks which to the best of the reviewers knowledge has not been explored in prior works. This crossutilization approach opens new avenues in the field of adversarial machine learning. b) Extensive Experimental Validation: The paper provides a comprehensive experimental setup encompassing different datasets victim models and comparison against stateoftheart methods. The availability of the codebase enhances the reproducibility of the results. c) Practical Significance: The framework addresses a critical gap in the MLaaS paradigm by highlighting and exploiting the vulnerabilities in current systems. The insights gained can heavily influence the development of more robust MLaaS solutions. Weaknesses: a) Complexity and Realworld Applicability: While the proposed method outperforms existing techniques the paper does not fully address the complexity and potential challenges in deploying such an attack in realworld scenarios. The assumptions about the adversarys capabilities and the availability of the data pool require further clarification. b) Ethical Considerations and Countermeasures: Although the paper briefly discusses potential defenses against MExMI there is a need for a more thorough exploration of ethical implications and comprehensive countermeasures to protect against such sophisticated attacks. 3. Questions and Suggestions for the Authors a) Data Pool Homogeneity: The paper emphasizes the importance of identifying homogeneous samples in the data pool for effective MI. However its unclear how the framework deals with highly heterogeneous data sources. Could the authors elaborate on the applicability of MExMI in such scenarios b) Adversary Model Assumptions: The threat model assumes a certain level of access for the adversary. I suggest providing a more detailed discussion on the minimal requirements for an adversary to successfully deploy MExMI considering various MLaaS configurations. c) Extended Validation: While the current experimental setup is robust extending the evaluation to include realworld MLaaS systems potentially in a redteam exercise could further validate the practical applicability of MExMI. 4. Limitations and Societal Impact The paper presents a compelling framework that significantly strengthens model extraction and membership inference attacks. However the discussion on limitations and negative societal impacts appears to be quite brief. Expanding this section to provide a deeper analysis of potential exploits and misuse of such a framework would be beneficial. Specifically constructing suggestions for MLaaS providers on detecting and mitigating such attacks without compromising user privacy or model performance could greatly enhance the papers contribution to the fields ethical landscape.", "q5h7Ywx-sS": " Peer Review for \"Stars: An Efficient Graph Building Framework via TwoHop Spanners\"  1) Summary of the Paper The paper introduces Stars a novel framework for constructing highly sparse yet informative similarity graphs using twohop spanners. This method significantly reduces the computational cost of graph building by minimizing the number of pairwise similarity comparisons needed and has practical applications in clustering graph learning and nearest neighbor search in massive datasets. The theoretical foundation of Stars ensures efficient performance with nearlinear time complexity and the method has been empirically validated on datasets at the TeraScale demonstrating substantial improvements over existing baselines in terms of comparison count and runtime without sacrificing downstream task quality.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Methodology: The introduction of twohop spanners for graph construction is novel. It cleverly addresses the scalability issue inherent in graphbased analyses of large datasets.  Significant Performance Gains: The empirical results show that Stars can achieve orders of magnitude improvement in the number of comparisons and runtime which is crucial for practical applications.  Theoretical Backing: The paper provides theoretical guarantees for the performance of Stars adding credibility to its claims of efficiency and effectiveness.  Extensive Evaluation: Comprehensive experiments on real and synthetic datasets demonstrate the framework\u2019s robustness and scalability. Weaknesses:  Comparative Analysis: While the paper claims significant improvements over baseline methods a broader comparison with other stateoftheart methods especially recent graph neural network approaches might be lacking.  Limitation Discussion: The discussion on limitations and potential negative societal impacts is somewhat limited. This is a complex area that could benefit from a more thorough examination especially considering the increasing attention on the ethical implications of algorithmic scalability.  Parameter Sensitivity: The paper describes various parameters (e.g. number of leaders in a bucket) that are crucial for the performance of Stars. However it lacks a detailed sensitivity analysis of how changing these parameters impacts the overall effectiveness and efficiency of the method.  3) Questions and Suggestions for the Authors  Comparative Analysis Extension: Could the authors extend the comparative analysis section to include more recent and comparable stateoftheart methods This would help in understanding the positioning of Stars in the current research landscape.  Parameter Sensitivity Analysis: It would be beneficial to include a detailed sensitivity analysis of key parameters to guide practitioners in effectively deploying Stars in various settings.  Handling Dynamic Datasets: The paper focuses on static datasets. How well does Stars adapt to dynamic datasets where data points and their relationships can change over time  Limitation and Societal Impact Discussion: The authors should provide a deeper discussion on the limitations of Stars and its potential negative societal impacts. For example what are the challenges in ensuring the privacy of data points in largescale similarity graphs  4) Limitations and Societal Impacts The paper could enhance its discussion on the limitations concerning the adaptability of Stars to different types of data and tasks. An explicit acknowledgment of scenarios where Stars might not be the optimal choice would be valuable. Furthermore a more comprehensive examination of potential negative societal impacts such as privacy concerns and the perpetuation of biases in massive datasets due to the sparsity of the constructed graphs is warranted. Suggesting ways to mitigate such issues perhaps through algorithmic adjustments or the incorporation of fairness constraints would be constructive.  Suggestions for Improvement  Consider expanding the comparative analysis to include a wider array of methods particularly those leveraging recent advancements in graph neural networks.  A detailed parameter sensitivity analysis could greatly assist in the practical application of Stars helping users to finetune the algorithm for specific use cases.  Delve deeper into the discussions surrounding the limitations of Stars especially in terms of its applicability to various data types and domains.  Enhance the discussion on potential societal impacts including privacy and bias suggesting possible pathways for mitigation.", "yJE7iQSAep": " Peer Review of \"Understanding and Simplifying Diagonal State Space Models\"  1. Summary The paper presents a comprehensive study on diagonal State Space Models (SSMs) notably introducing the S4D model a simplification of the S4 model that maintains its practical and theoretical benefits. The evolution from S4 that leverages a HiPPO matrix to model longrange dependencies to a more straightforward diagonal representation while preserving performance marks a significant advancement in the understanding of SSMs. S4D notably simplifies the algorithmic implementation and theoretical analysis without sacrificing performance achieving competitive or stateoftheart results across a range of benchmarks in image audio and medical time series data. Through systematic examination the work elucidates on the parameterization initialization and computation of diagonal SSMs providing insights into the critical aspects that maintain the model\u2019s efficacy.  2. Assessment  Strengths:  Theoretical Insight: The paper offers a deep theoretical inspection of diagonal SSMs particularly in demonstrating how a diagonal approximation maintains performance with a principled approach to initialization. This contributes to a more profound understanding of SSMs underlying mechanisms.  Practical Significance: By simplifying the computation and parameterization of SSMs to a diagonal form this work makes highperformance sequence modeling more accessible. The reduction to a simple kernel computation that requires minimal code is particularly appealing for broader adoption.  Empirical Validation: The authors provide a comprehensive empirical evaluation across diverse domains and tasks supporting their theoretical claims with significant evidence. The controlled ablation studies further strengthen the conclusions drawn about the effectiveness and efficiency of the proposed S4D model.  Clarity and Detail: The paper is wellstructured and clearly articulates the progression from complex SSMs to their simplified diagonal counterparts. The level of detail in describing the algorithmic and theoretical nuances of SSMs is commendable.  Weaknesses:  Comparative Analysis: While the paper presents a thorough comparison between S4 and S4D models it lacks a broader evaluation against other stateoftheart sequence models outside the SSM family. Such a comparison might have shown more clearly where S4D stands in the wider landscape of sequence modeling.  Generalization and Robustness: The paper might have benefited from a discussion on the generalization of diagonal SSMs to sequences beyond the ones tested. Understanding the conditions under which S4D might not perform as expected would provide a more nuanced view of its applicability.  Dimensionality Considerations: While the simplification to diagonal matrices is advantageous the impact of state dimensionality on performance and the tradeoffs between model simplicity and the ability to capture complex dependencies could have been explored further.  3. Questions and Suggestions  Could the authors provide more insight into how the choice of the HiPPO matrix influences the models performance and whether there are alternatives that could yield comparable results  It would be beneficial for the reader if the paper included a more detailed comparison of S4Ds performance against nonSSM stateoftheart sequence models highlighting scenarios where S4D might have distinct advantages or limitations.  Exploring the effects of varying the state dimensionality on both computational efficiency and model performance could offer valuable guidelines for practitioners in choosing appropriate model sizes for different applications.  Given the significant reduction in complexity from S4 to S4D are there practical contexts (e.g. lowresource devices) where S4D demonstrates a clear advantage over other models including S4 If so elaborating on these contexts could underscore the practical implications of this work.  4. Limitations and Societal Impact The authors have made a commendable effort in addressing the limitations of their work particularly the potential for reduced performance in scenarios not covered by the empirical evaluation. However a more explicit discussion on the models robustness to different sequence characteristics (e.g. variability in sequence length noise levels) would have been informative. Regarding societal impact while the paper does not directly address potential negative uses of S4D it is always prudent for works advancing AI capabilities to consider and discuss ethical considerations and misuse prevention. Suggestions include implementing and discussing mechanisms for ensuring the responsible use of such models especially considering their potential applicability in sensitive domains like medical data analysis. In conclusion this paper is a valuable contribution to the field of deep learning for sequence modeling advancing our theoretical understanding and practical capabilities with diagonal SSMs. The introduction of S4D and its thorough validation across multiple benchmarks sets a solid foundation for future explorations and optimizations in state space models.", "qj-_HnxQxB": " Review of \"Achieving OutofDistribution Generalization with Functional Indirection Neural Estimator (FINE)\"  1) Summary of the Paper This paper introduces a novel approach to achieving outofdistribution (OOD) generalization in machine learning models through analogymaking and indirection in functional spaces coined as the Functional Indirection Neural Estimator (FINE). The core idea is inspired by human cognitive abilities to abstract concepts and make analogies with FINE dynamically constructing neural network weights for novel inputs by leveraging a memory of basis weight matrices. This method is extensively tested on IQ tasks involving geometric transformations with datasets such as MNIST Omniglot and CIFAR100 showing superior performance in OOD generalization compared to existing approaches.  2) Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The conceptual basis of FINE employing analogymaking and indirection in functional spaces is a novel and wellmotivated approach that distinguishes it from existing models that generally operate within data spaces.  Comprehensive Evaluation: The paper provides a robust validation of the proposed model through a series of experiments across different datasets and task types demonstrating its ability to generalize well under conditions of distributional shift.  Clear Realworld Relevance: Solving OOD problems is a critical challenge in AI and the FINE model represents a significant step towards machines mimicking humanlike flexibility and reasoning. Weaknesses:  Complexity and Scalability: The models reliance on a substantial memory component for storing trainable weight matrices might limit its scalability and efficiency particularly for deep or complex network architectures.  Limited Domain Testing: Despite the comprehensive experimental setup FINEs evaluation primarily focuses on visual IQ tasks. Its effectiveness in other domains or with more nuanced or complex OOD scenarios remains unexplored.  Theoretical Justification: The paper could benefit from a deeper theoretical analysis that explains why the functional indirection approach outperforms other strategies in achieving OOD generalization.  3) Questions and Suggestions for the Authors  Scalability Concerns: Can the authors discuss potential methods to mitigate the scalability issues associated with the memory component of FINE especially for deeper networks  Application to Other Domains: It would be interesting to see how FINE performs on nonvisual OOD tasks such as those involving textual data or multimodal inputs. Can the authors speculate on possible adaptations for such scenarios  Comparison with Human Cognition: The analogy with human cognitive abilities is fascinating and serves as a strong motivation. Could the authors expand on how closely FINEs mechanism mirrors human thought processes particularly in cases where humans might struggle  Further Theoretical Insight: Additional theoretical insights or a formal proof could enrich the understanding of why FINEs approach is effective. Are there specific properties of functional spaces that make this strategy superior for OOD generalization  4) Limitations and Societal Impact The authors address some limitations related to the models complexity and potential for inefficiency with very deep networks. However further discussion on the implications of these limitations for realworld applications would be beneficial. Additionally while the paper alludes to the positive impact of improving OOD generalization considering potential negative societal impacts such as misuse in surveillance or decisionmaking systems without adequate transparency is crucial. Suggestions for mitigating these risks could include incorporating interpretability mechanisms into FINE or setting ethical guidelines for its application. Overall this paper presents a promising approach towards achieving humanlike OOD generalization in AI systems. Future work that expands FINEs application scope improves its efficiency and deepens its theoretical foundation could further solidify its significance in the field.", "lWq3KDEIXIE": "Review of \"Leveraging Local Inconsistent Statistics for Improving Tractable Probabilistic Models\" 1. Summary of Contributions: The paper presents a novel approach to enhance the performance of tractable probabilistic models (TPMs) such as cutset networks by incorporating a large set of local probability estimates. These estimates derived from various sources such as expert knowledge or external processes may often be inconsistent and are used to refine the parameters of an existing model learned from a small dataset. This is achieved through an optimization problem that minimizes a linear combination of KL divergences to integrate both global information (the datadriven model) and local information (the local probability estimates) despite their potential inconsistency. A gradientbased algorithm is developed with the computation of gradients shown to be efficiently solvable for tractable models. The experimental evaluation demonstrates significant improvements in both generative and predictive performance over original models learned from limited data indicating the effectiveness of leveraging local estimates. 2. Assessment of Strengths and Weaknesses: Strengths:  Innovative Methodology: The paper tackles an important problem of enhancing model performance in dataconstrained settings by innovatively combining global and local information sources a novel contribution to the domain of TPMs.  Theoretical Soundness: The formulation of the optimization problem and the development of a gradientbased solution are theoretically sound with a clear exposition of how the method addresses the challenge of inconsistent local estimates.  Empirical Validation: The comprehensive experimental study across 20 datasets convincingly demonstrates the improved performance of the proposed method thereby providing strong empirical evidence for its efficacy. Weaknesses:  Limited Discussion on Limitations: While the method shows promise the discussion on the limitations of the approach especially in terms of scalability and the potential for overfitting when integrating noisy local estimates is sparse.  Potential Negative Societal Impact: The paper does not discuss any possible negative societal impacts particularly in applications where misinterpretation of the probabilistic estimates could have serious consequences.  Comparison with Related Work: Although the paper briefly discusses related work it lacks a detailed comparison demonstrating the superiority or differences of the proposed method over existing approaches that integrate local information in learning TPMs. 3. Questions and Suggestions for the Authors:  Scalability Concerns: Can the authors elaborate on the scalability of the proposed method especially when applied to highly complex networks or exceptionally large datasets  Handling Overfitting: Given the use of local potentially noisy estimates how does the proposed method guard against overfitting Would regularization techniques or crossvalidation methods be applicable here  Comparative Analysis: It would be beneficial if the authors could provide a more detailed comparison with existing methods such as those based on iterative proportional fitting or other gradientbased learning algorithms for TPMs. How does the proposed method compare in terms of computational complexity and performance 4. Limitations and Constructive Suggestions:  Addressing Limitations: The authors should provide a more thorough discussion on the limitations of their approach including any situations where their method may not be applicable or could lead to degraded performance.  Potential Negative Societal Impact: It is recommended that the authors include a section discussing any potential negative societal impacts of their work. This could involve the misuse of probabilistic models in sensitive applications or the propagation of biases from local estimates.  Improvement Suggestions: To address the aforementioned weaknesses the paper could benefit from including additional experiments or discussions that benchmark the proposed method against existing techniques in more varied settings. Further exploring mechanisms to mitigate overfitting and explicitly discussing the methods scalability and computational requirements would enrich the paper. In summary the paper presents an important contribution to the field of tractable probabilistic models by proposing a novel method to enhance model performance using local inconsistent statistics. Despite its strengths addressing the outlined weaknesses and considering the provided suggestions could significantly strengthen the work.", "nEJMdZd8cIi": "Review for \"Efficient Learning with Projected Unitary Neural Networks (projUNN)\" 1) Summary of the Paper and Contributions: The paper presents an innovative approach projUNN for efficiently parameterizing and training unitary or orthogonal matrices within deep learning models. These models are particularly relevant for recurrent neural networks (RNNs) and convolutional neural networks (CNNs) where maintaining stability and preventing vanishing or exploding gradients is crucial. The key contribution is the development of two variants of the projUNN algorithm  Direct (projUNND) and Tangent (projUNNT)  which leverage lowrank approximations for efficient gradient updates. This method allows for a training runtime that scales linearly with the rank of the approximation and the dimension of the matrices significantly reducing computational overhead compared to previous approaches. The authors demonstrate the effectiveness of their method on RNNs and make preliminary explorations into orthogonalunitary CNNs showcasing the potential of projUNN to maintain high performance with reduced computational costs. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The idea of using lowrank approximations for efficient gradient updates in unitaryorthogonal matrices represents a significant advancement in computational efficiency for training deep neural networks.  Comprehensive Experiments: The paper provides a thorough evaluation of the method on RNN tasks showing that projUNN performs on par or better than stateoftheart unitary neural network algorithms. Preliminary investigations into CNNs also suggest potential broader applicability.  Theoretical Underpinnings: The authors provide a solid theoretical foundation for their algorithm including proofs and a detailed explanation of the mathematical background. Weaknesses:  Limited CNN Exploration: While the paper offers an initial foray into the use of projUNN for CNNs this aspect feels somewhat underdeveloped compared to the RNN experiments. More extensive experimentation and analysis in this area would provide a more rounded evaluation of projUNNs effectiveness.  Potential Overlook on Training Dynamics: The discussion on how the lowrank approximation affects training dynamics such as convergence speed and model robustness is somewhat lacking. Greater detail on these aspects would strengthen the paper.  Societal Impact Considerations: The paper does not discuss the potential societal impacts of the proposed method positive or negative. Given the increasing importance of ethical AI a brief consideration of these aspects would be beneficial. 3) Questions and Suggestions for the Authors:  CNN Performance Gap: The paper mentions that orthogonalunitary CNNs do not currently outperform stateoftheart models. Could the authors speculate on the reasons and potentially propose strategies for narrowing this performance gap  Impact of LowRank Approximations: Have the authors investigated how different ranks affect the training dynamics and final performance in more detail It would be insightful to understand the tradeoffs between computational efficiency and model accuracy.  Societal Impact: Could the authors comment on any potential societal impacts either positive (e.g. more efficient models reducing environmental impact) or negative (e.g. misuse of efficient models in adversarial settings) associated with their proposed method 4) Limitations and Societal Impact: The paper could be improved by addressing the limitations more directly especially regarding the exploration of CNNs and the detailed analysis of training dynamics under lowrank approximations. Additionally a brief discussion of the societal implications of their work would make the paper more holistic. Constructive suggestions include expanding the CNN experiments to cover a wider range of architectures and tasks and providing additional analysis on how different approximation ranks affect model learning and performance over time.", "pGLFkjgVvVe": " Peer Review of \"Enhancing ModelBased Offline Reinforcement Learning Using Geodesic Distances in Latent Spaces\"  1) Summary of the Paper This paper introduces a novel approach to enhance modelbased offline reinforcement learning (RL) by improving uncertainty estimation through the use of geodesic distances in a latent space. The authors propose a method that combines parametric and nonparametric techniques to estimate model error leveraging the geometry of latent spaces formed by generative models. Specifically they define a Riemannian metric in the latent space of a variational autoencoder to quantify the uncertainty. This metric is then used to penalize the agent in regions of high uncertainty effectively limiting overestimation caused by outofdistribution samples. The method named GELATO demonstrates superior performance over existing modelbased offline RL approaches in continuous control and autonomous driving benchmarks.  2) Assessment of Strengths and Weaknesses Strengths: a) Innovative Approach: The paper innovatively combines parametric and nonparametric methods for uncertainty estimation in offline RL. The introduction of a Riemannian pullback metric for error estimation in a latent space is novel and grounded in solid mathematical principles. b) Empirical Evidence: The authors provide extensive experimental validation across diverse benchmarks showing that their approach outperforms contemporary strategies. The thorough comparison with existing methods and detailed analysis of results showcase the method\u2019s effectiveness. c) Practical Relevance: The addressed problem of overestimation in modelbased offline RL is highly relevant. The proposed solution has clear applications in fields requiring reliable decisionmaking under uncertainty such as autonomous driving and robotic control. Weaknesses: a) Computational Efficiency: The authors acknowledge the approach is computationally more demanding being five times slower compared to traditional uncertainty estimation methods. This could limit its applicability in scenarios where computational resources are constrained or realtime responses are necessary. b) Complexity of Implementation: The methods reliance on advanced mathematical constructs (e.g. Riemannian geometry) might make it challenging for practitioners to implement and adapt. The paper could benefit from providing more detailed guidance or tools to facilitate replication and adoption. c) Generalizability of Results: While the paper presents results from a variety of benchmarks the generalizability to other domains or more complex environments is not explicitly addressed. Further studies are necessary to understand the method\u2019s performance in broader contexts.  3) Questions and Suggestions for the Authors 1. On Computational Efficiency: Can the authors propose any strategies or ongoing work aimed at improving the computational efficiency of their method For example approximate methods for geodesic distance computation or parallelization strategies 2. Clarification on the Choice of Benchmarks: The selected benchmarks are wellsuited to demonstrate the method\u2019s effectiveness. However it would be informative to understand the authors rationale behind choosing these particular environments and if they anticipate any environments where their approach might not perform as well. 3. On Broadening Applicability: The method is presented within the context of continuous control and autonomous driving. Could the authors discuss potential applications in other areas of RL or even beyond RL where geodesic uncertainty estimation might be beneficial 4. Detailed Implementation Guide: To aid reproducibility and adoption by the community could the authors provide more detailed pseudocode for the implementation of their methodology or consider releasing a reference software implementation  4) Limitations and Societal Impact The authors have made a clear attempt to address the limitations of their approach namely the increased computational demand. However theres room for improvement in discussing potential negative societal impacts. Given the applications in autonomous systems unintended consequences of errors in decisionmaking due to misestimated uncertainties could have grave implications. Constructive suggestions include a deeper exploration into failsafe mechanisms in realworld applications and ethical considerations in deployment scenarios. Furthermore expanding the discussion on the limitations regarding the scope of the methods applicability and potential biases in model training or benchmark data could enhance the papers rigor. Providing guidelines on monitoring and mitigating such biases would be valuable for responsible adoption of the technology. Overall the paper presents a significant advancement in offline RL with promising applications. Addressing the identified weaknesses and exploring the suggested questions could make it an even more substantial contribution to the field.", "se2oxj-6Nz": " Peer Review  1) Briefly summarize the paper and its contributions This paper addresses the underexplored area of leveraging image restoration to enhance object detection performance under adverse imaging conditions such as haze and low light. The novel approach integrates a targeted adversarial attack during the image restoration process to generate pseudo ground truths that closely resemble original sharp images thereby aiding in the improvement of object detection accuracy. The authors introduce a momentumbased ADAMlike iterative algorithm for generating these adversarial examples finetuning the restoration models accordingly. Extensive experiments demonstrate the superiority of this method over traditional training schemes and other advanced strategies in improving object detection outcomes following image dehazing and lowlight enhancement tasks. Contributions of this paper include:  A novel finetuning pipeline that leverages targeted adversarial attacks to improve both visual quality and detection accuracy of restored images.  A momentumbased ADAMlike iterative algorithm for more effective adversarial example generation.  Extensive validation of the proposed method across different restoration tasks and object detectors showcasing notable improvements over existing techniques.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The paper tackles the challenge of using image restoration as a preprocessing step to enhance object detection accuracy in a novel way. The integration of a targeted adversarial attack to guide the restoration process is both innovative and effective as supported by the experimental results.  Broad Applicability: The proposed pipeline is versatile applicable to any restoration method and compatible with both onestage and twostage detectors. This broad applicability enhances the methods value to the research and application communities.  Extensive Experimental Validation: The authors conduct a comprehensive set of experiments across different conditions and datasets providing strong empirical evidence of the methods effectiveness. Comparisons with existing approaches further highlight the proposed methods advantages. Weaknesses:  Restoration Model Limitations: While the adversarial attack boosts detection performance on test sets impressively the less significant improvement when learning the attacks pattern points to potential limitations in current restoration models ability to capture complex adversarial perturbations accurately.  Lack of Discussion on Negative Impacts: The paper does not discuss the potential negative societal impacts of its techniques such as misuse in surveillance or the spread of manipulated images which is critical given the increasing concern over ethical AI use.  Complexity and Efficiency: The added complexity of generating and employing adversarial attacks for finetuning restoration models might impact the methods efficiency. There is little discussion about the computational costs and practicality of deploying this method in realworld lowlatency applications.  3) Questions and Suggestions for the Authors  Question: How do the computational costs of the proposed method compare to traditional image restoration and object detection pipelines especially concerning realtime processing requirements  Suggestions:  It would be beneficial to include a discussion on the potential negative societal impacts of the presented technique considering the dualuse nature of AI technologies.  Exploring strategies to enhance restoration models capacity to learn from adversarially perturbed pseudo ground truths effectively could further improve the methods performance.  Limitations While the paper addresses the limitations of current image restoration methods in aiding object detection tasks it could benefit from a clearer articulation of its own limitations notably the efficiency and computational cost of integrating targeted adversarial attacks into the restoration process. Additionally discussing potential misuse and ethical considerations surrounding adversarial attacks would provide a more comprehensive view.  Constructive Suggestions for Improvement  Elaborate on the computational implications of the proposed method to better understand its applicability in timesensitive applications.  Investigate and discuss methods to improve the ability of restoration models to learn from adversarially perturbed inputs more effectively.  Address the ethical considerations and potential negative impacts associated with the use of adversarial attacks in image restoration providing guidelines or safeguards to mitigate these concerns.", "rG7HZZtIc-": " Review of \"Decoupled Dynamic Neural Radiance Field (DNeRF) for 3D Scene Decoupling and Reconstruction\"  1) Summary This paper proposes Decoupled Dynamic Neural Radiance Field (DNeRF) a novel selfsupervised methodology for 3D scene reconstruction from a monocular video. By leveraging neural radiance fields (NeRF) the method separates dynamic objects and their shadows from the static background. Distinctively DNeRF can achieve this separation without requiring additional data (e.g. multiview captures) or pretrained models for object detection or segmentation. The approach introduces a skewedentropy loss and a shadow field network to accurately decouple dynamic and static components and manage the modeling of moving shadows. Empirical analyses on a new dataset and comparisons with stateoftheart methods display DNeRFs superiority in synthesizing clean backgrounds and correctly segmenting dynamic objects and shadows.  2) Assessment Strengths:  Innovative Approach: The selfsupervised learning framework for dynamic and static scene decoupling without reliance on additional data or pretrained models is a substantial contribution.  Comprehensive Treatment of Shadows: By introducing a shadow field network this work encompasses a nuanced understanding of shadows impact on scene decoupling which is often overlooked in similar research areas.  Extensive Evaluations: Demonstrating superior performance on various metrics across multiple datasets including a newly introduced dataset provides strong evidence of the methods effectiveness.  Practical Toolset: Releasing code and datasets upon publication bolsters the researchs applicability and encourages further exploration within the scientific community. Weaknesses:  Dependence on Camera Registration: The reliance on precise camera registration is a potential bottleneck for realworld applications where achieving such accuracy can be complex.  Handling of Highly Reflective Surfaces: The methods difficulty in managing highfrequency viewdependent radiance changes might limit its applicability to scenes with reflective surfaces.  Ambiguity in Motion Clues: Struggles with textureless targets that exhibit minimal or repetitive motion suggest limitations in the current approach to leveraging motion as a key distinguishing factor.  Complexity in Loss Function: The introduction of a skewedentropy loss while innovative adds complexity to the model particularly in tuning hyperparameters for optimal performance.  3) Questions and Suggestions  Hyperparameter Sensitivity: The paper mentions that the optimal choice of hyperparameters is influenced by various factors like object motion and video length. Could the authors provide more insights into this sensitivity and potentially automate the tuning process  Extension to Reflective Surfaces: Given the noted difficulty with reflective surfaces can the authors speculate on adjustments or augmentations to DNeRF that might address this challenge  Impact of Imperfect Camera Registration: It would be beneficial if the paper could discuss the methods robustness to variations in camera registration accuracy possibly including an evaluation scenario with perturbed camera parameters.  4) Limitations and Societal Impact The paper acknowledges key limitations such as dependence on precise camera registration and challenges with reflective surfaces and subtle motions. A discussion on the potential societal impact both positive (e.g. advancements in robotics and autonomous driving) and negative (e.g. privacy concerns with detailed 3D reconstructions) would have made the exploration more comprehensive. As for enhancements incorporating mechanisms or preprocessing steps to handle imperfect camera registration could broaden the method\u2019s applicability. Additionally exploring techniques to better manage reflective and textureless surfaces could further improve performance.  Conclusion DNeRF introduces an innovative approach to dynamic and static scene decoupling with significant advancements in managing shadows and selfsupervised learning from monocular videos. Despite highlighted limitations and areas for further research this work establishes a strong foundation that contributes materially to the fields advancement.", "pp7onaiM4VB": " Review for \"Exploiting DomainSpecific Batch Normalization on SPD Manifolds for EEGBased Transfer Learning\"  Summary of the Paper The paper introduces a novel machine learning framework aimed at addressing the challenge of unsupervised domain adaptation (UDA) in electroencephalography (EEG) data analysis with particular focus on intersession and intersubject transfer learning. The core contribution is the development of a novel method named SPD domainspecific momentum batch normalization (SPDDSMBN) that facilitates learning domaininvariant tangent space mapping (TSM) models directly on the symmetric positive definite (SPD) manifold in an endtoend fashion. This approach leverages geometric deep learning to overcome domain shifts in EEG data. The proposed TSMNet architecture integrates the SPDDSMBN layer to achieve stateoftheart performance in transfer learning tasks across multiple EEG braincomputer interface (BCI) datasets. The work is thoroughly validated with extensive experiments demonstrating significant advancements over traditional methods.  Assessment of Strengths and Weaknesses Strengths: 1. Innovative Approach: The paper proposes an original technique to address domain adaptation in EEG analysis exploiting the Riemannian structure of SPD manifolds. The SPDDSMBN is a significant advancement in geometric deep learning providing a pathway to learn domaininvariant features effectively. 2. Strong Empirical Validation: The authors present comprehensive experiments across six diverse EEG datasets showing that TSMNet consistently outperforms or matches stateoftheart methods in transfer learning scenarios. This thorough validation strengthens the papers claims. 3. Interpretability: A key advantage of the proposed method is its intrinsic interpretability stemming from the TSM models nature. This feature is crucial in healthcare applications where understanding the decisionmaking process is vital. 4. Open Source: By providing the code and detailed experimental setup the authors ensure transparency and reproducibility which enhances the papers utility to the research community. Weaknesses: 1. Computational Complexity: The paper acknowledges a limitation regarding the computational complexity linked to eigen decomposition on highdimensional SPD features. This issue could hinder the frameworks applicability to datasets with fine spatial granularity such as fMRI connectivity matrices. 2. Scope of Validation: While the experimental validation is robust across selected BCI tasks the scope of these tasks is relatively narrow. Future studies could expand on the application domains exploring the frameworks utility in broader contexts within neuroscience and beyond. 3. Lack of Comparative Analysis on Failure Cases: The paper could have been strengthened by a more detailed analysis of scenarios where TSMNet underperforms or fails to close the gap with domainspecific methods offering insights into potential limitations or areas for improvement.  Questions and Suggestions for the Authors 1. Framework Generalization: Have the authors considered testing the frameworks applicability to other types of manifoldstructured data beyond EEG Insights into the method\u2019s generality would be valuable. 2. Realworld Usability: Considering the computational complexity can the authors comment on realworld applicability Particularly how do they envision the deployment of TSMNet in practical resourceconstrained healthcare settings 3. Improvement on High Dimensionality Issue: Could the authors propose potential strategies or future research directions to mitigate the computational complexity issue especially for highdimensional data  Addressing Limitations and Societal Impact The authors have acknowledged the frameworks limitations concerning computational complexity which is commendable. However a more indepth discussion on strategies to overcome this and other identified limitations would strengthen the paper further. Additionally while the authors briefly mention the absence of immediate negative societal impacts a more comprehensive analysis regarding privacy data security and ethical considerations in deploying such technologies in healthcare and consumer sectors could have enriched the discussion. Constructive suggestions include investigating lightweight models for resourceconstrained environments and ensuring robust privacypreserving approaches in handling sensitive EEG data.  Conclusion This paper represents a significant contribution to the field of EEGbased transfer learning introducing an innovative framework that addresses critical challenges in domain adaptation. Despite some limitations and areas for future exploration the work sets a new benchmark in the field and opens up exciting avenues for research in geometric deep learning applied to neurotechnology.", "mWaYC6CZf5": "Review for \"Alleviating Representation Collapse in Sparse MixtureofExperts via LowDimensional Hyperspherical Routing\" 1. Summary: This paper introduces a novel routing algorithm aimed at addressing the representation collapse issue observed in Sparse MixtureofExperts (SMoE) models. In traditional SMoE models the routing mechanism plays a crucial role by distributing input tokens to the most suitable experts based on tokenexpert similarity. However this tends to lead to token clustering around expert centroids reducing the diversity of token representations. To counteract this the authors propose routing token representations and expert embeddings within a lowerdimensional hypersphere coupled with L2 normalization and soft expert gating with learnable temperature. Their method was evaluated through crosslingual language model pretraining and finetuning on downstream tasks demonstrating consistent improvements over existing SMoE models across multiple multilingual benchmarks. 2. Assessment: Strengths:  The paper tackles the underexplored issue of representation collapse in SMoE models contributing significantly to the field of conditional computation and model efficiency.  The proposed method of hyperspherical routing is innovative and wellreasoned showing an understanding of the geometric implications in representation learning.  The comprehensive experiments across several benchmarks and extensive analysis on routing behaviors and representation properties validate the effectiveness of the proposed method.  Implementation details including the introduction of a soft expert gate with learnable temperature are thoughtfully presented which should facilitate replication and further research. Weaknesses:  While the paper presents positive results there is a lack of deep theoretical analysis explaining why hyperspherical routing precisely prevents representation collapse.  The evaluation focuses solely on crosslingual tasks and benchmarks. The generalizability of the proposed method to other domains or tasks (e.g. image processing or speech recognition) remains unclear.  The discussion on the computational efficiency gains is somewhat superficial. Quantitative comparisons on computational costs versus performance gains against baseline models would have been insightful. 3. Questions and Suggestions for the Authors:  Can the authors provide more theoretical insights or derivations that explain the mechanics by which hyperspherical routing prevents representation collapse  It would be beneficial to see the application of this routing method to domains outside of language processing such as vision or speech. Are there plans to explore these areas  Can the authors elaborate on the computational efficiency of their method Specifically how does the computational overhead of implementing hyperspherical routing compare with traditional methods at scale  A minor suggestion: The paper could benefit from a section discussing potential pitfalls or scenarios where the proposed method might not perform as expected. 4. Limitations and Societal Impact: The paper does not thoroughly address the limitations related to the scalability of the proposed method especially when applied to domains requiring even more computational resources such as highresolution image processing. Furthermore while the authors briefly mention the potential reduction in computational and environmental costs as an indirect societal benefit a deeper analysis of the ethical implications especially related to the data and model biases in multilingual applications would have strengthened the paper. Constructive suggestions include a more detailed discussion on the environmental impact of largescale model training and an evaluation of model fairness across languages.", "nJJjv0JDJju": " Peer Review of \"Enhancing Diffusion Models for Inverse Problems through Manifold Constraints\"  1. Summary This paper introduces a novel framework designed to improve the efficiency and accuracy of diffusion models applied to inverse problems by incorporating a manifold constraint. The authors focus on common issues encumbering existing diffusion model solvers notably their tendency to produce suboptimal results due to deviation from the data manifold during the iterative reverse diffusion and projectionbased measurement consistency phases. To tackle this a correction term inspired by manifold geometry is proposed dubbed the Manifold Constrained Gradient (MCG) which is simple to implement and significantly boosts performance across several tasks including image inpainting colorization and sparseview CT reconstruction. The contributions of this paper are multifaceted: theoretical underpinnings that justify the use of the manifold constraint extensive experimental results showcasing superior performance over existing methods and the proposition of a generally applicable framework that enhances diffusion models for a wide range of inverse problems.  2. Assessments of Strengths and Weaknesses  Strengths  Theoretical Foundation: The paper is grounded in rigorous theoretical analysis providing a solid foundation for the proposed MCG approach. The usage of Tweedie\u2019s formula and the newly introduced manifold constraint enrich the understanding of diffusion models behavior in the context of inverse problems.  Empirical Validation: The authors present a comprehensive set of experiments across various applications demonstrating the effectiveness of their method. The comparison with stateoftheart models and the ablation study provide convincing evidence of the superiority of the proposed method.  Practicality: Implementation of the MCG requires only a few lines of code making it an attractive solution for enhancing the performance of existing diffusion model solvers.  Weaknesses  Assumption Specificity: The strong manifold assumption proposing that the data manifold is locally linear might not be applicable to all data types and could limit the methods generalizability. It would be beneficial for the authors to discuss the implications of this assumption further.  Speed and Computational Efficiency: The authors acknowledge that diffusion models including their proposed method are slow to sample from which could limit practical applications requiring realtime or nearrealtime processing. More discussion on potential solutions or ongoing work to address this limitation would be valuable.  Potential for Bias: Like all generative models the proposed method is susceptible to biases present in the training data. While the authors briefly mention this in the limitations section a deeper exploration of how these biases might manifest specifically in the outputs of their model and potential mitigation strategies would be constructive.  3. Questions and Suggestions for the Authors  Manifold Assumption: Can the authors provide more insight or empirical evidence on the impact of the strong manifold assumption on the performance of the MCG across different types of data or inverse problems  Computational Efficiency: Are there ongoing efforts or ideas that the authors are exploring to significantly reduce the computational requirements of their method Insights into potential avenues to speed up the process would be beneficial to readers.  Bias Mitigation: The paper could benefit from a discussion on specific biases (e.g. in image datasets) that might affect the models output and how these biases could be actively mitigated during model training or through postprocessing techniques.  4. Limitations and Broader Impact The authors have suitably acknowledged the limitations and potential negative societal impacts of their work including the susceptibility to generate malicious content like deepfakes and the reinforcement of existing biases in the training data. To further strengthen this section the authors could consider providing more concrete suggestions on mitigating these risks such as implementing specific fairness and diversity checks in the model evaluation phase or recommending ethical guidelines for the use of their technology. Overall the paper contributes significantly to the field of diffusion models and inverse problems offering promising results and opening avenues for further research. With attention to the discussed weaknesses and suggestions the work has the potential to impact both academic research and practical applications positively.", "kY1RbKE7DWE": "Peer Review of \"AnchorChanging Regularized Natural Policy Gradient (ARNPG) for MultiObjective MDPs\" 1. Summary of the Paper The paper introduces the ARNPG framework which aims to address policy optimization in Markov decision processes (MDPs) with multiple reward value functions. These functions need to be optimized jointly according to specific criteria like proportional fairness hard constraints and maxmin tradeoffs. The key innovation is the integration of wellperforming firstorder methods into the policy gradientbased optimization for multiobjective MDPs. This approach achieves theoretical global convergence at a rate of \\xc3\\x95(1T) with exact gradients and demonstrates superior performance empirially over existing methods. The framework is extensively tested in various scenarios including tabular setups and actorcritic deep reinforcement learning (RL) environments. 2. Assessment of Strengths and Weaknesses Strengths:  Novel Framework: The ARNPG introduces a novel idea of integrating firstorder methods into policy gradient optimization which represents an original contribution to multiobjective MDPs and potentially opens avenues for further research.  Theoretical Rigor: The paper provides a solid theoretical foundation for the ARNPG framework including global convergence guarantees. The detailed theoretical backing helps in establishing the framework\u2019s reliability.  Experimental Validation: Comprehensive experiments validate the framework\u2019s effectiveness in improving performance across various settings including both exact gradient and samplebased tabular scenarios and more complex actorcritic deep RL scenarios. Weaknesses:  Generalization Concerns: Despite the promising results there is a lack of discussion on the scalability of the method to extremely large state or action spaces or highly complex multiobjective criteria.  Algorithm Complexity: The integration of firstorder methods could potentially increase the computational complexity especially in samplebased settings. More discussion on the computational tradeoffs and how they compare with baseline methods would be valuable.  Limited Benchmarks: The comparison to existing methods is somewhat narrow. Including a broader range of existing multiobjective optimization methods could have provided a clearer picture of the ARNPG\u2019s performance landscape. 3. Questions and Suggestions for the Authors  Extension to Larger Spaces: How does the ARNPG framework perform in environments with extremely large state or action spaces Would the method\u2019s performance degrade significantly and what modifications if any could mitigate this  Comparison to a broader range of existing methods: The current set of benchmarks seems limited. Could the authors evaluate the framework\u2019s performance against other stateoftheart multiobjective RL algorithms  Discussion on Computational Overhead: The integration of firstorder methods likely increases the computational complexity. Could the authors comment on the tradeoff between computational cost and performance improvement Additionally are there strategies to make the ARNPG framework more computationally efficient 4. Limitations and Societal Impact The discussion on limitations appears somewhat limited. Specifically the generalization to highly complex environments and the computational efficiency of the framework are areas that could be further expanded. Addressing these limitations constructively would strengthen the paper. As for societal impact the authors did not discuss any potential negative implications of their work. While multiobjective optimization is generally a positive contribution to decisionmaking systems discussing the ethical considerations such as biases in decisionmaking criteria or in the reward mechanisms would be prudent. Constructive suggestions include recommendations for ethical considerations in applying the ARNPG framework and ensuring transparency in how multiobjective criteria are weighed. Overall while the paper presents a promising approach with solid theoretical backing and empirical validation addressing the outlined suggestions and questions would substantially bolster the contribution and its broader impact on the field.", "wjClgX-muzB": " Peer Review for \"Support Decomposition Variational Inference for Probabilistic Programs with Stochastic Support\"  1. Summary of the Paper The paper introduces a novel variational inference (VI) method named Support Decomposition Variational Inference (SDVI) specifically designed for probabilistic programs with stochastic support. Traditional VI methods construct a single global guide for variational approximation which struggles with stochastic support due to the inherent discontinuities and complex dependencies. SDVI addresses these challenges by decomposing the original program into subprograms each with static support and constructing separate variational guides for each. This approach simplifies the guide construction process leading to better inference performance. Implemented in Pyro SDVI is empirically shown to outperform existing methods on various problems.  2. Assessment of the Paper Strengths:  Originality: The paper tackles the less explored problem of VI within probabilistic programs having stochastic support. The decomposition approach is innovative and addresses a critical limitation of existing VI methods.  Technical Soundness: The methodology is sound with a clear explanation of how programs are decomposed into straightline programs (SLPs) and how these are utilized within the SDVI framework. The mathematical foundation for constructing and training the variational guides for each SLP is rigorously detailed.  Empirical Validation: The paper provides a comprehensive evaluation of SDVI across various domains showing its superiority over existing approaches. The inclusion of stochastic and infinite Gaussian mixture models alongside a Gaussian Process kernel inference task underlines SDVIs versatility.  Implementation and Reproducibility: By integrating SDVI into Pyro and making the code available the authors facilitate reproducibility and allow for broader adoption within the community. Weaknesses:  Discussion on Limitations: While some limitations are acknowledged such as difficulties in learning effective variational approximations for an excessively large number of SLPs the paper could benefit from a deeper exploration of potential pitfalls or scenarios where SDVI might underperform.  Comparison with SamplingBased Methods: The paper primarily focuses on comparing SDVI with other VI approaches. Including comparisons with samplingbased inference methods particularly for probabilistic programs with stochastic support could provide a more comprehensive performance benchmark.  Resource Allocation Scheme Clarity: Although the Successive Halving algorithm used for allocating computational resources among SLPs is mentioned a more detailed discussion or illustration of its practical impact on inference performance would be beneficial.  Societal Impact Consideration: There is no explicit discussion on the potential negative societal impacts of the proposed method which could be pertinent given the broad applicability of probabilistic programming.  3. Questions and Suggestions for the Authors  Generalization to More Complex Stochastic Support Structures: How well does SDVI adapt to highly complex or deeply nested stochastic support structures found in some realworld applications  Handling of Discrete Variables: The approach for models branching on discrete variables is briefly outlined. Could the authors elaborate on potential challenges and considerations when dealing with a mix of continuous and discrete variables within the same SLP  Extended Comparison with NonVI Methods: Could the authors provide insights or potential future work directions on how SDVI compares to or could be integrated with nonVIbased inference methods especially advanced MCMC techniques tailored for stochastic support  4. Limitations and Potential Negative Societal Impact The authors have not thoroughly addressed the limitations and potential negative societal impacts of their work. While SDVI represents a significant advancement in VI for probabilistic programs with stochastic support an assessment of scenarios where it might be less effective or fail is missing. Additionally discussions around the potential misuse of such powerful inference tools especially in sensitive or regulated domains could foster a more responsible application. Suggestions for improvement include a dedicated section discussing these aspects along with possible mitigation strategies to address identified limitations and risks.  Conclusion SDVI represents a significant step forward in the domain of variational inference for probabilistic programs particularly addressing the challenging aspect of stochastic support. Its innovative approach rigorous methodology and broad applicability make it a valuable contribution to the field. However a more indepth discussion on its limitations broader comparison benchmarks and potential societal impacts could further strengthen the paper.", "uRTW_PgXvc7": " Peer Review for \"SpatioTemporal Adapter for ParameterEfficient ImagetoVideo Transfer Learning\"  1) Summary of the Paper This paper addresses the challenge of adapting large pretrained image models for video understanding tasks particularly focusing on action recognition. It introduces a novel SpatioTemporal Adapter (STAdapter) designed for parameterefficient finetuning enabling image models to reason about temporal video content efficiently. The STAdapter utilizes a compact and computationally efficient design to bridge the gap between image and video modalities allowing for significant reductions in parameter updates when adapting to video tasks. Extensive experiments demonstrate that the STAdapter achieves comparable or superior performance to both the traditional full finetuning approach and stateoftheart video models with the advantage of greatly reduced parameter overhead.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of a parameterefficient adapter for crossmodality transfer learning is innovative. The STAdapter concept addresses an underexplored area filling the gap between image and video understanding.  Efficiency: The proposed methods parameter efficiency is very impressive requiring approximately 20 times fewer parameters to be updated compared to existing methods. This efficiency does not come at the cost of performance which is a significant achievement.  Extensive Experiments: The authors have conducted a comprehensive set of experiments across different datasets and compared the proposed method against a wide range of existing solutions and configurations. This thorough evaluation adds credibility to the claims made.  Replicability: The provision of code and model details enhances the papers utility to the research community encouraging replication and further research based on this work. Weaknesses:  Limited Focus: The paper focuses predominantly on the action recognition task. While the results are impressive the generalizability of the STAdapter to other video understanding tasks (e.g. object tracking scene recognition) remains unclear.  Potential Bias towards Certain Datasets: The experiments mainly use datasets where visual features are crucial for action recognition. The performance and utility of STAdapter in contexts where temporal dynamics are more subtle or complex are not well explored.  Lack of Theoretical Insight: The paper could benefit from a deeper theoretical discussion on why the STAdapter works so effectively for the crossmodality transfer learning providing insights into the learning dynamics introduced by the adapter.  3) Questions and Suggestions for the Authors  Generalizability: Can the authors discuss or hypothesize the STAdapters applicability to other video understanding tasks beyond action recognition Including preliminary results on such tasks would be highly beneficial.  Comparison with TaskSpecific Architectures: While the STAdapter is compared with stateoftheart methods a more detailed analysis comparing its performance with recent taskspecific architectures designed explicitly for video understanding could enrich the discussion.  Experimental Design: Could the authors clarify the decisionmaking process behind choosing the specific datasets and baselines for the experiments Detailing any limitations encountered during experimentation would also be helpful for readers.  Impact on Training Time: While parameter efficiency is well discussed the paper should also comment on the training time and computational resources required compared to full model training and other efficient learning methods.  Limitations and Societal Impact The authors have outlined the limitations regarding focusing on a single task and the potential adaptability of the STAdapter for other video understanding tasks. However a broader discussion on the limitations related to the types of actions and the diversity of datasets used in the experiments would be beneficial. Additionally the paper lacks a discussion on potential negative societal impacts. While the work primarily contributes to the technical domain considering and discussing the implications of deploying such technologies in realworld applications\u2014for example surveillance or content moderation\u2014would provide a more rounded perspective. A suggestion for improvement could involve exploring the adaptability of the STAdapter in more diverse and challenging scenarios including poorly represented actions and realworld events in public datasets to ensure the technology benefits a broader spectrum of society without bias. In conclusion this paper presents an innovative and highly efficient approach for crossmodality transfer learning from image models to video understanding tasks. Despite some limitations the work significantly contributes to the field opening up new avenues for research in video understanding and efficient learning methods.", "x4JZ3xX5mtv": "Review of \"Mitigating the Seesaw Problem in SingleImage View Synthesis Using an Efficient NonAutoregressive Model\" 1) Summary The paper introduces an innovative framework for synthesizing novel views from a single image addressing the \"seesaw\" problem\u2014 the tradeoff between preserving reprojected content and filling in unseen outofview regions. This is a significant issue in current autoregressive models that require high computational costs. The proposed solution is a nonautoregressive model with dual renderers (explicit and implicit) and a specially designed loss function that facilitates the complementary strengths of both renderers achieving faster processing (\\xe2\\x89\\x88100 times faster) while maintaining image quality. The effectiveness of this framework is demonstrated through experiments with the RealEstate10K and ACID datasets showing superior performance over stateoftheart autoregressive methods in both efficiency and output image quality. 2) Assessment Strengths:  Innovation: The approach to combining explicit and implicit methods to tackle the seesaw problem is novel and well thought out.  Performance: The substantial speed improvement (\\xe2\\x89\\x88100x faster than autoregressive methods) without compromising the image quality is noteworthy. This could significantly impact applications requiring realtime processing.  Experimental Validation: The comprehensive evaluation on two challenging datasets and the positive comparison with existing stateoftheart methods convincingly validate the proposed models effectiveness. Weaknesses:  Generalization Ability: While the paper presents results on two datasets further investigation into the models generalizability across a broader set of scenarios and datasets would strengthen the findings.  Limitation Discussion: The discussion on limitations primarily focuses on potential advantages. An indepth analysis of scenarios where the proposed method might not perform as expected would provide a more balanced view.  Societal impact: While the potential privacy concerns are noted a more thorough examination of the negative societal impacts particularly in the context of creating realistic images could better inform future research directions and ethical considerations. 3) Questions and Suggestions  Dataset Diversity: Have the authors considered evaluating their model on outdoor scenes or datasets with more varied geometry This could help understand the models adaptability to different contexts.  Ablation Study on Renderer Dependency: An ablation study that individually assesses the contribution of the explicit and implicit renderers to the final performance might provide deeper insights into their roles and importance.  Future Work Directions: Exploring the integration of this framework into video synthesis or extending it for full 3D scene reconstruction could be interesting future work directions. Any preliminary thoughts or findings on these would be valuable. 4) Limitations and Potential Negative Societal Impact The authors have acknowledged the limitations of their work particularly focusing on the computational efficiency and effectiveness of the proposed model. However a more comprehensive analysis of scenarios where the models performance might degrade (e.g. complex geometries or rapid viewpoint changes) would provide a clearer picture of its applicability bounds. On societal impacts while privacy concerns with generating realistic images are mentioned further elaboration on potential misuse scenarios (e.g. creating fake images for malicious purposes) and mitigation strategies would be responsible. Suggestions for improvement could include incorporating mechanisms to detect and flag synthetically generated images or developing ethical guidelines for the use of such technologies.", "nyCr6-0hinG": "Summary of the Paper and its Contributions: This paper introduces MetaSchedule a domainspecific probabilistic programming language designed to create a rich adaptable search space for tensor program optimization. It allows domain experts to easily propose stochastic choices in program transformations thus facilitating the construction of a search space in a modular manner. MetaSchedule is part of an endtoend learningdriven framework that aims to optimize tensor programs by searching within this constructed search space. The system demonstrates its capability by matching or surpassing the performance of existing stateoftheart optimization frameworks across various hardware backends with notable improvements in deep learning workload optimizations achieving up to a 48 speedup. The primary contributions include the novel probabilistic language abstraction for search space construction the learningdriven framework to navigate the search space and the empirical validation of these innovations across multiple platforms and deep learning models. Assessment of Strengths and Weaknesses: Strengths: 1. Innovative Abstraction: The probabilistic programming model abstraction for constructing optimization search spaces is novel. It uniquely addresses the challenge of creating a flexible and rich search space that can evolve with new optimizations and hardware changes without needing significant modifications. 2. Modularity and Customization: The paper succeeds in demonstrating how MetaSchedule enables modularity and easy customization which is crucial for adapting to different hardware and optimization techniques over time. 3. Empirical Validation: The extensive experiments and comparisons with existing frameworks (like TVM and PyTorch) provide solid evidence of MetaSchedules effectiveness and efficiency in practical scenarios. Weaknesses: 1. Complexity for New Users: While the paper highlights modularity and ease of use for domain experts the initial learning curve and complexity of utilizing a new probabilistic programming language might be challenging for new users. 2. Limited Discussion on Scalability: There is minimal focus on the scalability of the proposed system especially in ultralargescale deep learning models or extremely complex search spaces which might be a concern for future extensions. 3. Potential Overhead Costs: The abstraction and modularity might introduce overheads in defining and navigating the search space. There is a lack of discussion regarding these potential costs and how they impact the overall optimization efficiency. Questions and Suggestions for the Authors: 1. Learning Curve and Usability: Can the authors provide insights or empirical data on the learning curve for new users or nondomain experts to use MetaSchedule effectively 2. Scalability Considerations: It would be beneficial to discuss how MetaSchedule scales with increasingly complex models and whether there are any mechanisms to handle potential exponential growth in search space size. 3. Overhead and Efficiency: Could the authors elaborate on any overheads introduced by the probabilistic programming abstraction and how these tradeoffs were managed or minimized during system design Limitations and Societal Impact: The paper does not extensively discuss the potential limitations related to scalability overhead costs and the barrier to entry for new users due to the complexity of a new programming model. Also there is no discussion about the potential negative societal impacts such as the environmental cost of more extensive search spaces leading to increased computational resources and energy consumption. As a constructive suggestion the authors could consider addressing these points by adding discussions on the scalability techniques optimizing the efficiency of the search process to reduce unnecessary computation and highlighting any energyefficient practices that could mitigate negative environmental impacts. Adding a section on potential mitigation strategies for new users such as communitydriven tutorials or simplified interfaces could also be beneficial.", "tmer8WAEzV": "Review of \"Efficient Sampling from HighDimensional Constrained Distributions via Constrained Riemannian Hamiltonian Monte Carlo\" 1. Summary of Contributions: The paper presents a novel algorithm for sampling from illconditioned nonsmooth constrained distributions in very highdimensional spaces specifically upwards of 100000 dimensions. The proposed method Constrained Riemannian Hamiltonian Monte Carlo (CRHMC) incorporates constraints into the Riemannian version of Hamiltonian Monte Carlo while maintaining sparsity thus achieving a mixing rate independent of condition numbers. The paper demonstrates through empirical results on benchmark datasets from systems biology and linear programming that CRHMC can significantly outperform existing sampling packages providing up to a 1000fold speedup on specific instances like the RECON3D human metabolic network. 2. Assessment of Strengths and Weaknesses:  Strengths: 1. The algorithm addresses a critical and challenging problem of efficiently sampling highdimensional constrained distributions which is a significant contribution to the field of sampling algorithms. 2. Demonstrating empirical success on practical realworld datasets particularly in systems biology adds substantial credibility and utility to the proposed method. 3. The methodology of incorporating constraints into the Riemannian Hamiltonian Monte Carlo and maintaining sparsity is novel and wellarticulated. 4. The comprehensive comparison of CRHMC with existing methods highlights its superior performance in terms of mixing rate and sampling time.  Weaknesses: 1. The theoretical underpinnings particularly the formal proof of convergence and how the mixing rate is independent of condition numbers are not thoroughly detailed within the main text. 2. Although the paper mentions that the package is available on GitHub a more detailed discussion on the ease of use compatibility and integration with existing systems would have been beneficial for potential users. 3. The examples and experiments are from specific fields (systems biology and linear programming). Expanding the empirical evaluation to include examples from other domains could provide a broader understanding of the algorithms applicability. 4. While the scalability of the algorithm is impressive there is little discussion on the computational resources required for such performance a critical factor for potential adopters of the technology. 3. Questions and Suggestions for the Authors:  Can the authors provide more theoretical details or direct references on how the algorithm guarantees a mixing rate independent of condition numbers  Given the algorithms performance on the RECON3D network how does CRHMC handle instances where the dimensionality goes beyond 100000 Is there a theoretical or practical limit to the algorithms scalability  Are there potential applications or domains outside of systems biology and linear programming where CRHMC could be particularly beneficial Any preliminary results or ongoing research in these areas would be insightful.  A more detailed discussion on the computational efficiency in terms of CPU time and memory requirements across various scales of problems would be valuable. Similarly insight into the parallelization capabilities of the algorithm and how it affects performance would be appreciated. 4. Limitations and Societal Impact:  The paper does not discuss the limitations related to the kinds of constraints or distributions for which CRHMC might be less effective.  While the potential negative societal impact of the work is not directly addressed it is crucial to consider ethical implications in the broader use cases of the algorithm particularly in sensitive or biased datasets within systems biology or humanrelated data.  Suggestion: Including a broader discussion on the range of constraints and distributions CRHMC is adapted to alongside any potential limitations would make the paper more comprehensive. Additionally a section dedicated to the ethical considerations and responsible use of CRHMC could enhance the societal relevance of the work. In conclusion the paper makes a significant contribution by presenting a highly efficient and practical algorithm for sampling from highdimensional constrained distributions demonstrated by substantial empirical success. Addressing the outlined questions and suggestions could further strengthen the paper\u2019s contributions and impact.", "wcBXsXIf-n9": "Review of \"A Unified Loss Function for Margin Maximization in Euclidean and Angular Spaces for Open Set Recognition and Deep Neural Networks\" 1) Summary of the Paper The paper introduces a novel loss function that aims to improve the performance of deep neural network classifiers by maximizing the margin in both Euclidean and angular spaces simultaneously. The proposed method achieves this by ensuring that class samples are tightly clustered around their respective class centers which are positioned on the boundary of a hypersphere resulting in equivalent pairwise distances between class centers. Unlike existing methods the proposed loss function does not require the setting of hyperparameters for classical classification problems simplifying its usage. The authors also address the challenge of open set recognition by showing that their classifier can effectively handle test samples from unknown classes not seen during the training phase. Experimental results demonstrate that the proposed method achieves stateoftheart accuracies on open set recognition tasks while maintaining simplicity. 2) Assessment of Strengths and Weaknesses Strengths:  Innovative Loss Function: The introduction of a loss function that concurrently maximizes the margin in both Euclidean and angular spaces is novel and potentially impactful for improving classification accuracies.  Simplicity and Usability: The absence of a need for hyperparameter tuning for classical classification problems is a significant advantage making the method accessible to a broader range of users and applications.  Open Set Recognition: The demonstrated utility of the classifier for open set recognition where it can discriminate against unknown class samples addresses an important and challenging area in classification problems. Weaknesses:  Dimensionality Limitation: The requirement that the CNN feature dimension must be larger than or equal to the total number of classes minus 1 may restrict the applicability of the proposed method in scenarios with a large number of classes.  Comparative Evaluation: While the paper provides experimental evidence of superior performance in open set recognition a more comprehensive comparison with existing methods on a wider variety of datasets including largescale datasets would strengthen the case for the proposed methods efficacy.  Dimension Augmentation Module (DAM): The introduction of DAM to overcome the dimensionality limitation is a thoughtful addition however its performance seems to partially solve the problem and further refinement is necessary to achieve stateoftheart results in largescale recognition tasks. 3) Questions and Suggestions for the Authors  Clarification on DAM: Could the authors provide more insights into the challenges faced with DAM and potential strategies for its improvement Specifically how do architectural and activation function changes contribute to enhancing DAMs effectiveness  Evaluation on Additional Datasets: To better understand the generalizability of the proposed method could the authors evaluate its performance on additional possibly larger datasets beyond those used in the current study  Impact of Hyperparameter Settings in Open Set Recognition: Although the method does not require hyperparameter tuning for classical classification problems two parameters must be chosen for open set recognition. Could the authors discuss the sensitivity of model performance to these hyperparameter settings and provide guidance for their selection 4) Limitations and Societal Impact The authors have addressed the limitation regarding the dimensionality of CNN features which is commendable. However further exploration of the effectiveness and optimization of the Dimension Augmentation Module (DAM) for largescale datasets is encouraged. Regarding the societal impact while the paper does not directly discuss potential negative impacts the advancement of open set recognition capabilities could have implications for privacy and surveillance. Constructive suggestions include a deeper examination of the proposed methods applicability across diverse domains and its ethical use alongside the continuous refinement of DAM to enhance performance across a broader array of tasks and datasets.", "q-LMlivZrV": "Summary and Contributions The paper introduces \"Contrastive Leave One Out Boost\" (CLOOB) a novel method combining modern Hopfield networks and the InfoLOOB objective to address the \"explaining away\" problem identified in CLIP a model renowned for its zeroshot transfer learning capabilities. CLIPs primary issue according to the authors lies in its focus on a limited set of features while neglecting others owing to insufficient exploitation of feature cooccurrences and covariance structures. The authors suggest that modern Hopfield networks can amplify these structures thereby enriching embeddings with more relevant features. However modern Hopfield networks intensify the InfoNCE objectives saturation effect which the proposed InfoLOOB objective successfully mitigates. CLOOB is tested against CLIP on zeroshot transfer learning tasks using the Conceptual Captions and YFCC datasets demonstrating consistent superiority across various architectures and datasets. Strengths and Weaknesses Strengths: 1. Innovative Approach: The integration of modern Hopfield networks for enhancing covariance structure and introducing the InfoLOOB objective to combat the saturation effect is novel and welljustified. This dual approach enables CLOOB to effectively mitigate the \"explaining away\" problem inherent in CLIP. 2. Comprehensive Evaluation: The paper provides extensive experimental results comparing CLOOB with CLIP across different architectures and datasets showcasing CLOOB\u2019s consistent improvement in zeroshot transfer learning performance. 3. Strong Theoretical Foundation: The theoretical underpinnings of why modern Hopfield networks enrich embeddings and how the InfoLOOB objective prevents saturation provide a solid foundation for the proposed method. Weaknesses: 1. Computational Efficiency: The paper does not address the computational implications of incorporating modern Hopfield networks into the training process. These networks might introduce significant overhead affecting the practical applicability of CLOOB especially when dealing with large datasets. 2. Generalization Beyond ZeroShot Learning: While CLOOB demonstrates improved performance in zeroshot transfer learning tasks the paper does not explore its efficacy in other learning paradigms such as fewshot learning or supervised tasks. This limitation restricts our understanding of CLOOB\u2019s versatility. 3. Dependency on HighQuality Data: The performance gains achieved by CLOOB might significantly depend on the quality of data used for pretraining. The paper should discuss how variations in data quality could affect the efficacy of CLOOB in realworld scenarios. Questions and Suggestions for the Authors 1. On Computational Efficiency: Can the authors provide insights into the computational costs associated with implementing modern Hopfield networks within CLOOB particularly in comparison to CLIP 2. Extending to Different Learning Paradigms: Have the authors considered evaluating CLOOB\u2019s performance beyond zeroshot transfer learning tasks such as fewshot learning or classical supervised learning It would be beneficial to understand the models versatility across various learning settings. 3. Impact of Data Quality: How does the quality of pretraining data impact CLOOBs performance It would be informative to include an analysis of CLOOBs sensitivity to data quality possibly through ablation studies using varied datasets. Limitations and Societal Impact The paper does not explicitly address potential negative societal impacts. Given the widespread application of such models there is a risk of reinforcing existing biases in the training data which can perpetuate harmful stereotypes if not adequately addressed. Moreover the increased computational requirements of modern Hopfield networks could contribute to larger carbon footprints raising environmental concerns. Suggestions for Improvement:  Conduct a thorough bias and fairness analysis to identify and mitigate potential biases in model predictions.  Explore more efficient implementations of modern Hopfield networks to reduce computational costs and environmental impact.  Assess the applicability of CLOOB across diverse datasets particularly those with varying qualities and biases to ensure broader applicability and resilience against data imperfections.", "xnI37HyfoP": " Review of \"An Algorithm for Nonnegative Tensor Completion with a New Norm\"  Summary of the Paper This paper introduces a novel algorithm for the completion of nonnegative tensors which is described as a significant challenge due to the computational and theoretical limitations associated with tensor completion problems. Unlike matrix completion tensor completion has not had an algorithm capable of achieving the informationtheoretic sample complexity rate. The authors propose a new norm defined by the gauge of a specific 01 polytope which serves as a convex surrogate for tensor rank while maintaining low statistical complexity. This norm helps in formulating a tensor completion problem that although NPhard allows for practical numerical solutions via a variant of the FrankWolfe algorithm. The algorithms performance is empirically validated through several computational experiments showing effective scalability and convergence properties on tensors with up to one hundred million entries.  Assessment of the Paper Strengths: 1. Novelty and Theoretical Contribution: The introduction of a new norm for nonnegative tensors represents a significant theoretical advancement. This new norms ability to act as a convex surrogate for tensor rank could influence further developments in tensor completion and optimization. 2. Algorithmic Development: The proposed algorithm for nonnegative tensor completion not only demonstrates practical feasibility but also meets the informationtheoretic sample complexity rate. This is particularly noteworthy given the background context of unresolved tensions between computational feasibility and theoretical optimality in the field. 3. Empirical Validation: The authors have carried out extensive computational experiments to validate the efficacy and scalability of their proposed algorithm. Using tensors with up to one hundred million entries for experimentation adds substantial credibility to the proposed methods practical relevance. Weaknesses: 1. NPhardness of the Problem: While the paper innovatively approaches the nonnegative tensor completion problem the NPhard nature of the proposed method poses significant challenges to its general applicability especially in scenarios where integer programming might become computationally infeasible. 2. Limited Scope of Application Demonstrated: Despite the impressive performance results shared the experiments focus primarily on the comparison with few existing methods and on tensors of specific types. The impact of the algorithm on a wider range of tensor completion applications is not thoroughly explored. 3. Potential for Overfitting: The paper would benefit from a more detailed discussion on the potential of the proposed model to overfit especially in the light of its performance on tensors with a large number of entries and the methodological complexity involved in its design.  Questions and Suggestions for the Authors 1. Generalization to Other Tensor Types: The authors mention future work aiming to generalize the approach to all tensor types. Could preliminary insights into potential challenges or modifications necessary for such generalization be shared 2. Computational Feasibility at Larger Scales: Given the NPhard nature of the problem how do the authors envisage the algorithms scalability and computational feasibility for even larger tensors or in resourceconstrained scenarios 3. Comparison with More Baselines: Future work could benefit from comparing the proposed method against a broader range of existing tensor completion algorithms including those based on different theoretical premises or optimization strategies.  Addressing Limitations and Societal Impact The paper provides a compelling contribution but falls short in discussing the potential limitations related to the NPhardness of the problem. Addressing computational limitations and potential mitigation strategies could enhance the papers impact. Additionally a discussion on the potential negative societal impacts especially concerning privacy and security in the application domains mentioned (e.g. healthcare) would provide a more holistic view of the methods implications. Constructive Suggestions: 1. Exploring Approximation Algorithms: Investigating and developing approximation algorithms for the proposed tensor completion problem could provide alternate pathways to circumvent the challenges posed by NPhardness in practical applications. 2. Broader Applications and Societal Impact Analysis: Future iterations of this work could benefit from a more expanded analysis of potential applications across various domains. Including a section discussing ethical considerations and potential negative societal impacts would enrich the papers contribution and address the growing demand for responsible AI research practices. Overall this paper presents a significant theoretical and algorithmic contribution to the field of tensor completion. Addressing the noted weaknesses and considering the suggestions for improvement could further enhance its value and impact.", "lHuPdoHBxbg": " Peer Review of \"CoarsetoFine Autoregressive Networks for Probabilistic Forecasting\" 1) Summary: The paper introduces CoarsetoFine Autoregressive Networks (C2FAR) a novel framework designed for probabilistic forecasting by modeling the distribution of univariate numeric random variables through a hierarchical autoregressive discretization process. This method generates progressively finer intervals from a sequence of binned distributions conditioned on previously generated coarser intervals overcoming the limitations of flat binned distributions by achieving exponentially higher precision with only a linear increase in complexity. C2FAR is versatile supporting both discrete and continuous data series and is applicable to various time series use cases such as anomaly detection interpolation and compression. Moreover the authors demonstrate that C2FAR outperforms stateoftheart methods on several benchmark forecasting datasets thereby showcasing its efficacy in probabilistic forecasting. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: C2FARs hierarchical discretization process represents a significant advance over flat binned distributions allowing for higher precision in modeling distributions without a corresponding explosion in complexity which is a notable conceptual innovation.  Versatility: The ability of C2FAR to handle both discrete and continuous data as well as series of arbitrary scale and distribution makes it a highly versatile tool for probabilistic forecasting across a broad range of applications.  Empirical Validation: The authors provide comprehensive experimental validation of C2FAR demonstrating its superior performance over existing methods on multiple realworld datasets thus providing strong empirical evidence of its effectiveness. Weaknesses:  Complexity in Implementation: While offering several advantages the hierarchical nature of C2FAR may introduce complexity in its implementation and tuning potentially making it less accessible for practitioners without significant expertise in deep learning.  Limitation on Univariate Series: The focus on univariate numeric random variables may limit C2FARs direct applicability to multivariate time series forecasting a common scenario in various domains.  Potential Overfitting: Given its capability to achieve high precision there might be a potential risk of overfitting especially in cases of limited data. The paper does not explicitly address how this risk is mitigated. 3) Questions and Suggestions for the Authors:  Generalization to Multivariate Series: Can C2FAR be easily extended to model multivariate time series data If so what modifications or additional considerations are necessary  Mechanisms to Prevent Overfitting: It would be beneficial if the authors could elaborate on any mechanisms or techniques embedded within C2FAR or used during the training process to mitigate the risk of overfitting.  Comparison with NonAutoregressive Models: How does C2FAR compare with recent advancements in nonautoregressive models for time series forecasting Including such comparisons could potentially highlight C2FARs unique advantages more clearly.  Explicit Discussion on Negative Societal Impact: The authors should consider adding a section discussing the potential negative societal impacts of their work including the environmental cost of training large models and the risk of misuse in sensitive applications. 4) Limitations and Potential Negative Societal Impact: The authors have not provided an explicit discussion on the potential limitations and negative societal impacts of their work. It would be constructive to include considerations regarding the environmental costs of training potentially large and complex models like C2FAR especially if deployed at scale. Furthermore discussion around the misuse of sophisticated forecasting technologies in finance surveillance or other areas could enrich the ethical considerations of the paper. To improve the authors could outline specific scenarios of potential misuse or harm and propose guidelines or safeguards to prevent such outcomes.", "xqyEG7EhTZ": " Peer Review Report  1) Brief Summary of the Paper and Its Contributions The paper introduces Tempo an innovative approach designed to reduce the memory footprint during the training of Transformerbased models thereby enabling higher batch sizes and increased training throughput. Tempo introduces three main optimizations: Inplace GELU Inplace LayerNorm and SubLayer Dropout Recomputation which allow certain activations to be discarded or recomputed rather than stored saving significant memory. The authors evaluate Tempo on BERTLARGE GPT2 and RoBERTa models demonstrating increased efficiency in terms of higher possible batch sizes and improved training throughput with minimal impact on model performance. The work claims novelty in its specific focus on memory footprint optimizations tailored for Transformerbased models presenting a solid argument for its efficiency improvements over the stateoftheart approaches.  2) Assessment of the Strengths and Weaknesses Strengths:  Highly Relevant Problem: The paper tackles the significant issue of memory limitations in training large Transformer models which is a pertinent challenge in the field.  Innovative Techniques: The proposed techniques (Inplace GELU Inplace LayerNorm and SubLayer Dropout Recomputation) are novel and cleverly address the crux of the problem.  Comprehensive Evaluation: The thorough evaluation across several models and configurations demonstrates the versatility and effectiveness of Tempo showcasing clear improvements over baseline methods.  Potential for Impact: Given the increasing size and complexity of Transformer models the optimizations could greatly benefit the broader ML community by reducing costs and environmental impact. Weaknesses:  Limited Discussion on Compatibility: The paper could benefit from a more detailed discussion on how Tempo interacts with or influences other optimization strategies and its compatibility with different hardware architectures.  Scarcity of Theoretical Analysis: While the empirical evidence is strong a deeper theoretical analysis or justification for why the proposed methods work so efficiently could strengthen the paper.  Underexplored Potential Limitations: The discussion on limitations especially nonmemory related ones (e.g. any tradeoffs related to computational efficiency precision loss in low memory scenarios) is not thoroughly explored.  Potential for Negative Societal Impact: The paper briefly mentions the reduced cost and environmental impact but does not delve into any potential negative impacts or ethical considerations of making larger models more feasible to train.  3) Questions and Suggestions for the Authors  Could the authors elaborate on how Tempo would integrate or potentially conflict with existing training optimizations such as mixedprecision training or gradient checkpointing  Is there an impact of the proposed techniques on the convergence speed of models Some insight into the effects on training dynamics would be valuable.  Could further details be provided regarding the practicality of implementing Tempo across different deep learning frameworks and hardware environments  It would be beneficial if the authors can explore and discuss any potential scalability issues or limitations when applying Tempo to even larger models or datasets.  A deeper investigation into the potential negative societal impacts such as the facilitation of even larger models which might exacerbate issues of computational inequality would be an essential addition.  4) Limitations: Addressing Limitations and Potential Negative Societal Impact The authors have made commendable efforts in reducing the memory footprint during the training of Transformerbased models which can have a positive impact by reducing computational resources and associated costs. However the paper lacks a thorough discussion on the limitations particularly concerning the scalability of these optimizations to future possibly more complex models and their interaction with other optimization strategies. Additionally while reducing the training cost of large models is beneficial a discussion on potential negative societal impacts such as the implications of enabling even larger models on carbon footprint and the broader environmental impact is missing. Its crucial to consider and balance these advancements with their ethical and societal implications. Constructive Suggestions for Improvement:  Expand the discussion on the compatibility and potential conflicts of Tempo with other optimization methods and hardware architectures.  Include a theoretical framework or analysis that supports why the proposed optimizations work adding depth to the empirical results.  Provide a more detailed examination of limitations particularly in terms of scalability and integration with other optimizations.  Discuss potential negative societal impacts more thoroughly offering insights into how these challenges could be mitigated or balanced with the methods benefits.", "nE8IJLT7nW-": "Summary: This paper introduces a biologically inspired approach for modeling peripheral vision within deep neural networks specifically for the task of visual recognition. The principal innovation is the incorporation of peripheral position encoding within the multihead selfattention layers of a Transformer model resulting in a new model architype dubbed Peripheral Vision Transformer (PerViT). The authors approach allows the network to learn diverse visual features by partitioning the visual field into multiple peripheral regions. Extensive evaluations on the ImageNet1K dataset demonstrate improvements in image classification performance across different model sizes highlighting the advantages of integrating peripheral inductive biases into deep learning models for visual recognition. Assessment: Strengths: 1. Novelty: The paper introduces an innovative approach by integrating humanlike peripheral vision into deep learning models which is a significant contribution toward narrowing the gap between human and machine vision. 2. Improved Performance: The PerViT model demonstrates superior performance in image classification tasks on the ImageNet1K dataset showcasing the potential of peripheral position encoding in enhancing the capabilities of visual recognition models. 3. Indepth Analysis: Authors have provided a comprehensive investigation into the inner workings of PerViT offering both qualitative and quantitative analyses to understand how the model perceives visual data compared to human vision. This level of detail enriches the papers contribution and provides valuable insights into the effectiveness of the proposed method. 4. Biological Relevance: The biologically inspired approach of mimicking human peripheral vision adds an intriguing perspective to the field of computer vision and could inspire future research in the area. Weaknesses: 1. Complexity and Scalability: The paper does not fully address the computational complexity introduced by the peripheral position encoding which could be a concern for scaling the model to higher resolution images or broader applications beyond image classification. 2. Generalization to Other Tasks: While the paper focuses on image classification it remains unclear how well the proposed PerViT model would generalize to other vision tasks such as object detection or segmentation. A broader evaluation across different tasks would strengthen the argument for the models versatility. 3. Comparative Baseline Selection: The paper includes performance comparisons with baseline models however it lacks a direct comparison with other biologically inspired approaches or models that specifically aim to mimic human visual processing mechanisms. 4. Impact on Model Interpretability: Although the introduction of peripheral position encoding is innovative the paper could further elaborate on how this approach affects the interpretability of the model decisions an increasingly important aspect as AI models become more integrated into societal applications. Questions and Suggestions for Authors: 1. Computational Complexity: Can the authors provide more information on the computational cost of incorporating peripheral position encoding and any strategies considered to mitigate potential impacts on model efficiency 2. Generalization Across Tasks: Would the authors speculate on or provide preliminary results on how the PerViT model performs on other computer vision tasks beyond image classification 3. Interpretability and DecisionMaking: How does the introduction of peripheral vision biases influence the interpretability of the models decisions Are there specific instances or classes where this approach leads to more intuitively understandable decisionmaking patterns 4. Robustness to Adversarial Attacks: Given the models biological inspiration have the authors assessed its robustness to adversarial attacks compared to traditional models This would be interesting given the unique way in which the model processes visual information. Limitations and Societal Impact: While the authors have made a novel contribution the limitations around computational scalability generalization to other tasks and detailed discussion on the societal impacts or ethical considerations are areas for improvement. To enhance the paper further I suggest the authors:  Discuss potential strategies for overcoming computational challenges to facilitate the application of PerViT to higher resolution images or realtime video streams.  Explore and present the model\u2019s performance on other vision tasks providing insights into its versatility and adaptability.  Dive deeper into the model\u2019s interpretability potentially opening avenues for safer and more transparent AI systems in critical applications such as healthcare or surveillance.  Comment on the potential for misuse or adversarial manipulation given the model\u2019s unique processing capabilities and suggest safeguards that could be implemented.", "ocg4JWjYZ96": " Review of \"Empirical Influence Function (EIF) for Debugging and Mitigating Generalization Errors in Deep Metric Learning Models\"  Summary: The paper presents an indepth investigation of generalization errors in Deep Metric Learning (DML) and proposes a novel technique the Empirical Influence Function (EIF) to address these issues. The EIF method is designed to efficiently identify and quantify the impact of training samples on generalization errors within DML models. Notably it can guide the relabeling of potentially noisy training samples to mitigate such errors. The authors conducted comprehensive experiments demonstrating that EIF outperforms existing baseline methods in locating relevant training samples more quickly and with statistical significance. Further a field study utilizing popular datasets (CUB200 CARS196 InShop) showcases EIFs ability to detect labeling mistakes hinting at broader implications for improving DML model performance. Importantly the authors share their code fostering transparency and facilitating future research.  Evaluation: Strengths: 1. Novelty and Relevance: The EIF presents a significant advancement in understanding and rectifying generalization errors in DML. Such an approach is timely given the marginal improvements by recent models and the pressing need for better debugging tools. 2. Comprehensive Evaluation: The authors thoroughly evaluate EIF across various dimensions including its effectiveness in sample identification relabeling recommendation and runtime performance. Such an extensive experimental setup provides robust evidence of EIFs utility. 3. Practical Impact: By identifying labeling mistakes in widelyused datasets EIF not only demonstrates immediate value in improving existing models but also influences future dataset preparation and model development practices within the DML community. Weaknesses: 1. Generalizability Concerns: While EIFs performance is impressive across the studied datasets the paper lacks discussion on its applicability to other domains or more complex data types (e.g. video text). Assessing EIFs versatility could enhance its adoption. 2. Limitations on Dataset Size and Noise Levels: The experiments focus on datasets and artificially induced noise levels that may not fully encapsulate realworld scenarios. A broader range of datasets and spontaneous noise types could provide deeper insights into EIFs robustness. 3. Baseline Comparisons: The selection of baseline methods primarily includes traditional influence functions. Including comparison with other contemporary debugging or explanation tools could further position EIF within the current stateoftheart.  Questions and Suggestions for Authors: 1. Generalizability to Other Data Types: Could the authors elaborate on EIFs applicability beyond image datasets particularly in domains where DML is rapidly evolving (e.g. text audio) 2. Impact of Realworld Noise: How does EIF perform under realworld noise conditions which might exhibit more complex patterns than artificially induced label noise 3. Scalability to Larger Datasets: The current experiments are constrained to relatively moderatesized datasets. Could the authors discuss EIFs scalability challenges or potential strategies when applied to largerscale datasets 4. Comparison with Contemporary Tools: Incorporating comparisons with more recent debugging or explanation tools could solidify EIFs place within the toolkit available to researchers. Are there plans to extend such analysis  Limitations and Societal Impact: The paper acknowledges some limitations particularly in terms of EIFs computational efficiency and the scope of datasets examined. However a more thorough discussion on these fronts including potential negative societal impacts (e.g. reinforcing biases during relabeling recommendations) would provide a more balanced view. Addressing these issues with concrete steps or future research directions could enhance the papers contribution. To improve the authors might consider experimenting with a wider array of datasets including those with inherent realworld noise and larger scales to test EIFs robustness and scalability. Moreover delving into possible biases introduced during the relabeling process and suggesting mechanisms to mitigate these could reinforce EIFs ethical application.  Conclusion: Overall this paper offers significant contributions to the field of Deep Metric Learning by introducing EIF a tool that not only aids in understanding but also in mitigating generalization errors. Despite its strengths expanding the evaluation framework and addressing the highlighted limitations could further enhance its impact. This work prompts an important step forward in debugging and explaining DML models essential for their future development and application.", "pfEIGgDstz0": " Peer Review for \"Neural Deformation Pyramid for NonRigid Point Cloud Registration\"  1. Briefly summarize the paper and its contributions This paper introduces the Neural Deformation Pyramid (NDP) a novel method for nonrigid point cloud registration leveraging hierarchical motion decomposition. The approach utilizes MultiLayer Perceptions (MLPs) at different levels of a pyramid architecture with each level encoding and processing 3D points using sinusoidally encoded frequencies to estimate motion increments. This hierarchical decomposition enables the model to differentiate between rigid and nonrigid motions effectively achieving significant speed improvements\u201450 times faster\u2014over existing MLPbased approaches. Furthermore NDP demonstrates stateoftheart performance on the challenging 4DMatch4DLoMatch benchmark for partialtopartial nonrigid registration under both nonlearned and supervised settings with applications extended to shape transfer. The paper provides a clear pathway through the complexities of nonrigid point cloud registration by decomposing the problem into manageable hierarchically structured tasks.  2. Provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Innovative Approach: The hierarchical decomposition strategy used in NDP is original and addresses the nonrigid point cloud registration problem effectively. The use of sinusoidally encoded inputs to control the MLPs capacity for capturing different motion scales is particularly elegant.  Significant Performance Improvements: Achieving a 50 times speedup over existing methods without sacrificing accuracy is commendable. Furthermore NDPs superior performance on the 4DMatch4DLoMatch benchmark underscores its practical utility.  WellStructured Experiments: The benchmarks ablation studies and application demonstrations (e.g. shape transfer) provide comprehensive evidence of NDPs capabilities and advantages. Weaknesses:  Limited in Handling Topological Changes: The methods reliance on 3D Euclidean spaces might restrict its applicability to scenarios involving significant topological alterations.  Lack of RealTime Performance: Despite the impressive speed improvement NDP does not achieve realtime processing which could limit its application in timesensitive contexts.  Potential for Improvement in Lowoverlap Cases: While NDP advances the handling of nonrigid point clouds there remains room for improvement in scenarios with low overlap as acknowledged by the authors.  3. Questions and Suggestions for the Authors  Handling Topological Changes: Have the authors considered extensions or modifications to NDP that could better accommodate significant topological changes in the point clouds  RealTime Performance: Could the authors elaborate on potential pathways or optimizations that might bring NDP closer to realtime performance  LowOverlap Scenarios: The paper mentions the challenge of lowoverlap cases. Could the authors discuss any preliminary ideas or future directions to enhance performance in these challenging scenarios  4. Limitations and Potential Negative Societal Impact The authors have addressed some limitations in their work including challenges with topological changes and realtime performance. However further detailing strategies to mitigate these limitations could strengthen the paper. For instance exploring adaptations of NDP to work within manifold space might offer a pathway to handling topological variations more robustly. Regarding potential negative societal impact the authors briefly note that their work as a foundational technology does not directly engender negative outcomes. While this is fundamentally true a deeper discussion on how nonrigid point cloud registration could be misused in surveillance or privacyinvading applications would be valuable. Providing guidelines or ethical considerations for the application of NDP could help address these concerns. Overall the paper presents a significant advancement in the field of nonrigid point cloud registration with compelling results and clear future research directions.", "u4ihlSG240n": "Review for \"OmniVL: A Unified Foundation Model for ImageLanguage and VideoLanguage Tasks\" 1. Summary of the Paper and Its Contributions: The paper introduces OmniVL a unified foundational model designed to concurrently support imagelanguage and videolanguage pretraining along with downstream tasks without the necessity for taskspecific adaptors. Leveraging a unique transformerbased visual encoder OmniVL processes both image and video inputs. The paper claims it\u2019s the first to show bidirectional benefits for image and video tasks through such a model. It incorporates a novel loss function the UniVLC loss to optimize the model by utilizing a mix of supervised and noisily supervised data across both visual and language inputs. The authors report stateoftheart or competitive performance across a suite of downstream tasks suggesting OmniVLs effectiveness in multimodal AI tasks. 2. Assessment of the Paper: Strengths:  Innovative Architecture: The unified transformerbased approach and decoupled pretraining methodology are significant innovations that streamline the training of models for both image and video tasks setting a new paradigm in visionlanguage pretraining.  Broad Scope: The ability of OmniVL to support a wide variety of tasks\u2014from visualonly tasks like image classification and video action recognition to multimodal tasks such as question answering and captioning\u2014is impressive and highlights its versatility and potential as a foundational model.  Extensive Evaluation: The comprehensive evaluation across numerous tasks and datasets provides strong evidence of the model\u2019s efficacy allowing for a robust understanding of its capabilities and limitations. Weaknesses:  Data and Compute Requirements: While the results are commendable the paper lacks explicit discussion on the computational resources and data requirements for pretraining and finetuning OmniVL. Given its performance its likely that substantial resources are necessary which could limit accessibility for some researchers and practitioners.  Lack of Comparison to Unimodal Tasks: Although the paper provides an extensive comparison for multimodal tasks comparisons detailing how OmniVL performs relative to stateoftheart models dedicated to unimodal tasks (visualonly or languageonly) are missing. This would help in understanding the tradeoffs in using a unified model versus taskspecific models.  Commonsense Reasoning  ZeroShot Capabilities: As acknowledged by the authors the model lacks in commonsense reasoning and zeroshot capabilities for certain tasks which are critical for practical utility and broader application of such models. 3. Questions and Suggestions for the Authors:  Could the authors clarify the compute resources required for pretraining and finetuning OmniVL Understanding its scalability would be beneficial for potential users.  How does OmniVL perform on unimodal tasks compared to the current stateoftheart models dedicated to those tasks Adding this comparison could provide deeper insights into the model\u2019s versatility.  Is there a potential roadmap for integrating commonsense reasoning and enhancing zeroshot capabilities within the OmniVL framework Insights into future directions could be valuable for the research community.  Given the models reliance on webcrawled data which may contain biases what steps or methodologies do the authors suggest to mitigate the potential propagation of these biases 4. Limitations and Societal Impact: While the authors briefly touch upon limitations related to commonsense reasoning and zeroshot capabilities a deeper exploration of the limitations concerning computational requirements and data biases would strengthen the paper. Addressing how the model could potentially amplify biases present in the training data especially given its broad applicability across tasks is crucial. Providing guidelines or methodologies for bias mitigation and responsible model deployment would be a constructive addition ensuring that the models adoption does not inadvertently exacerbate existing societal issues. Constructive Suggestions for Improvement:  Discussing the computational and data requirements in detail would provide a clearer picture of the models applicability and accessibility.  Including comparisons for unimodal tasks could offer a complete view of OmniVL\u2019s performance spectrum.  Outlining specific strategies for bias mitigation and ensuring responsible deployment would be valuable given the models potential societal impact.  Future work could concentrate on overcoming identified limitations particularly around commonsense reasoning and zeroshot learning capabilities to further enhance this promising foundation models utility.", "wmdbwZz65FM": " Peer Review of \"Adversarial Training Strategy for Improving Sequence VAEs via Targeted Dropout\"  1) Briefly summarize the paper and its contributions The paper addresses the challenge of posterior collapse in autoregressive variational autoencoders (VAEs) utilized for sequence data. It introduces an adversarial training strategy employing informationbased stochastic dropout to mitigate posterior collapse a common issue where the VAEs latent space becomes uninformative. The authors theoretically and empirically demonstrate that their approach effectively reduces posterior collapse by forcing the model to utilize the latent space enhancing both sequence modeling and the informativeness of the latent space. The methodology is evaluated on standard text benchmarks showing improvements over traditional dropout methods.  2) Please provide a thorough assessment of the strengths and weaknesses of the paper Strengths:  Theoretical Foundation: The paper provides a strong theoretical justification for the proposed adversarial method effectively addressing a known problem in the training of sequence VAEs.  Innovative Approach: The concept of using adversarial training to induce targeted dropout based on the information content of sequence elements is novel and intriguing.  Empirical Validation: Comprehensive experiments demonstrate the efficacy of the proposed method in improving the latent space informativeness without compromising sequence modeling performance. Weaknesses:  Model Complexity: The introduction of an adversarial component increases the models complexity potentially making it more challenging to train and finetune.  Hyperparameter Sensitivity: The reliance on the hyperparameter \u03bb to control the adversarial effect might make the model sensitive to its tuning affecting generalizability.  Generalization: While the method shows promise on the datasets investigated it remains unclear how well this approach generalizes to other types of sequential data such as speech or video.  3) Please list up and carefully describe any questions and suggestions for the authors Questions:  How does the adversarial training strategy scale with longer sequences or larger datasets  Could the authors elaborate on the potential effects of adversarial dropout on model convergence time and computational efficiency Suggestions:  Exploring Transformer Architectures: Given the dominance of transformers in sequence modeling investigating how the adversarial dropout technique applies to transformerbased VAEs could be highly beneficial.  Hyperparameter Analysis: A deeper analysis of the hyperparameter \u03bbs impact on various aspects of training (e.g. convergence speed model robustness) would provide more insights into the models behavior across different settings.  Generalization Study: Conducting experiments on a wider range of sequential data types could help ascertain the methods generalizability and effectiveness beyond textual data.  4) Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work The authors touch upon the potential positive and negative uses of their method but do not delve deeply into the ethical considerations or the broader societal impacts. While the improvement in controlled sequence generation is acknowledged a more thorough discussion on how these advancements might be misused such as in generating disinformation or deepfakes would be warranted. Further the paper could benefit from a discussion on mitigation strategies for potential negative uses and an exploration of responsible AI practices in the context of their work. Constructive Suggestions for Improvement:  Ethical Considerations: A dedicated section addressing ethical implications potential misuse and guidelines for responsible use would strengthen the paper.  Limitation Acknowledgment: Explicitly acknowledging the limitations related to model complexity computational demands and the hyperparameter sensitivity would provide a more balanced view.  Future Directions: Suggestions for future research could include exploring less parametersensitive methods of implementing adversarial dropout or extending the approach to multimodal data to address the generalization concern.", "rcrY85WLAKU": "This paper introduces a novel framework for tensor network embeddings that use Gaussian random matrices to efficiently perform dimensionality reduction on tensor network structured inputs such as those encountered in tensor decomposition and kernel regression. Unlike previous work which tailored embeddings for specific tensor structures like the Kronecker and KhatriRao products this work aims at a more general approach that can handle arbitrary tensor network structures. Through a systematic design the proposed embeddings achieve low sketching computational costs and small sketch sizes while ensuring the accuracy of the embeddings. The paper provides a detailed analysis of the efficiency of these embeddings presents an algorithm for their implementation and applies the method to CANDECOMPPARAFAC (CP) tensor decomposition and tensor train rounding showing improvements over existing techniques.  Strengths: 1. Generalizability: A significant contribution of this work is its ability to generalize the application of tensor network embeddings beyond specific structures addressing a broader range of tensor networks. 2. Theoretical Foundation: The paper presents a thorough theoretical foundation for the proposed embeddings. It provides mathematical proofs for the accuracy and efficiency of the embeddings which is a strong point of this work. 3. Algorithmic Efficiency: The introduction of an algorithm that achieves nearoptimal asymptotic cost for sketching inputs with tensor network structure is a practical contribution. This efficiency promises to advance the applications in CP decomposition and tensor train rounding significantly. 4. Applicability to Realworld Applications: Demonstrating the use of these embeddings in CP decomposition and tensor train rounding which are crucial in various applications showcases the practical relevance of this research.  Weaknesses: 1. Complexity and Accessibility: The paper is dense and loaded with mathematical formulations and theoretical proofs which might make it less accessible to readers not wellversed in tensor networks or those from a more applied background. 2. Limited Experimentation: While theoretical analysis is strong the paper could benefit from a broader range of empirical evaluations especially in scenarios that test the limits of the proposed embeddings in terms of scale noise and realworld data complexity. 3. Comparison with StateoftheArt: The paper could have provided a more extensive comparison with stateoftheart methods especially in the experimental section to highlight the improvements more clearly.  Questions and Suggestions: 1. Scalability and Performance: How does the performance of the proposed algorithm scale with the size and complexity of the tensor networks More insights into the scalability could enhance the paper. 2. Handling Noise and Imperfections in Data: It would be useful to discuss how the proposed embeddings handle noise and imperfections typically found in realworld data. Are there scenarios where the accuracy of the embeddings might significantly decrease 3. Extensions to Other Tensor Structures: The paper mentions future work on more general embeddings including those based on fast sketching techniques. Could the authors speculate on the potential challenges and benefits of extending their approach to these structures 4. Software Implementation: Providing a reference to an opensource implementation or discussing the practical aspects of implementing the proposed algorithm would significantly benefit readers interested in applying this work.  Limitations and Societal Impact: The authors have made an effort to discuss the limitations related to the complexity and the specific focus of their work. However they could further elaborate on the potential negative societal impacts especially if these tensor network embeddings were to be used in privacysensitive applications. Constructive suggestions include a discussion on ethical considerations the potential for misuse in surveillance or biased outcomes in data mining applications. Additionally future work could explore incorporating mechanisms to ensure privacy preservation and fairness when these embeddings are deployed in realworld scenarios.", "nE8_DvxAqAB": " Peer Review for \"Unlocking Egocentric VideoLanguage Pretraining with EgoClip EgoNCE and EgoMCQ\"  1) Summary of Contributions The paper introduces an innovative approach to VideoLanguage Pretraining (VLP) in the context of egocentric (firstperson) video data addressing the challenges of applying traditional VLP methods optimized for thirdperson videos. The authors have developed a comprehensive framework comprising three key contributions:  EgoClip Dataset: A largescale firstperson videotext pretraining dataset with 3.8M cliptext pairs derived from the Ego4D dataset covering diverse human activities. This dataset addresses the gap in suitable pretraining resources for egocentric video understanding.  EgoNCE Pretraining Objective: A novel adaptation of the videotext contrastive learning objective tailored for the nuances of egocentric video by intelligently mining positive and negative samples that are contextually relevant to firstperson perspectives.  EgoMCQ Benchmark: A development benchmark closely aligned with the EgoClip dataset designed to facilitate efficient validation and iteration of egocentric VLP models promoting rapid development and testing of new approaches. These contributions collectively advance the state of egocentric video understanding demonstrating superior performance on a variety of downstream tasks across multiple datasets.  2) Assessment of Strengths and Weaknesses Strengths:  Innovativeness: The paper addresses a significant gap in the VLP domain by focusing on egocentric videos offering welljustified and original solutions that are likely to initiate further research in this area.  Comprehensive Approach: The creation of a dataset (EgoClip) a novel training objective (EgoNCE) and a development benchmark (EgoMCQ) represents a holistic approach to solving the problem making this work a valuable resource for the community.  Strong Empirical Performance: The experiments conducted provide strong evidence for the effectiveness of the proposed methods showcasing stateoftheart performance on several egocentric video understanding tasks. Weaknesses:  LongTerm Temporal Dependencies: The approach notably lacks consideration for longterm temporal dependencies in videos which can be crucial for understanding more complex activities and events. Addressing this limitation could further improve performance.  Potential for Bias and Privacy Concerns: While acknowledging societal impacts the discussion on how biases present in the Ego4D dataset and privacy concerns are mitigated could be expanded. This is particularly relevant given the intimate nature of firstperson video data.  3) Questions and Suggestions for the Authors  LongTerm Temporal Information: Have the authors considered extending their framework to incorporate longterm temporal information If not could they speculate on potential methods and their impact on the models performance  Bias and Privacy Mitigation: Could the authors detail any specific measures taken during dataset creation or model training to address potential biases in the data and ensure privacy is respected  CrossDomain Applicability: How transferable do the authors expect the learned representations to be particularly for egocentric videos in vastly different domains from those covered in the Ego4D dataset  4) Limitations and Societal Impact The acknowledgment of the approachs limitation concerning longterm temporal dependencies is appreciated and the authors commitment to addressing this in future work shows a clear path forward. On the societal impact while the potential benefits in augmented reality and robotics are highlighted a more indepth discussion on privacy concerns and bias mitigation strategies would strengthen the paper. Constructively the authors could explore anonymization techniques for sensitive data and establish guidelines for ethically leveraging the insights gained from firstperson video analysis.", "wKd2XtSRsjl": "This paper introduces a new metric Mutual Information Divergence (MID) for evaluating multimodal generative models particularly in the context of texttoimage generation and image captioning tasks. MID is based on the novel application of negative Gaussian crossmutual information utilizing CLIP features to measure the alignment between generated content and conditional data. The authors propose MID as a unified metric that outperforms existing methods across several benchmarks demonstrating its consistency sample parsimony and robustness.  Strengths: 1. Originality: The introduction of the Mutual Information Divergence (MID) leveraging negative Gaussian crossmutual information is an original contribution that addresses the complexity of evaluating multimodal generative models. 2. Comprehensive Evaluation: The authors extensively compare MID with existing metrics across multiple benchmarks including generated and humanannotated judgments showing significant improvements in correlation with human judgment visual reasoning accuracy and robustness toward the exploited CLIP model. 3. Theoretical Foundations: The paper provides a solid theoretical analysis of MID including its relationship with outofdistribution detection bias and variance decomposition and the KullbackLeibler divergence underscoring the metrics theoretical soundness. 4. Publicly Available Code: By making the code available the authors encourage reproducibility and further research based on their work.  Weaknesses: 1. Dependence on CLIP Features: While the metrics performance is commendable its dependence on CLIP features might limit its applicability in scenarios where CLIPs biases or limitations are pronounced. 2. Evaluation on Diverse Datasets: Although the paper presents evaluations on several benchmarks further testing on a more diverse set of datasets could provide a better understanding of MIDs generalizability across different types of multimodal generative models. 3. Potential Overemphasis on Metrics: While improving evaluation metrics is crucial the work might inadvertently contribute to the ongoing race for metric optimization potentially overshadowing the importance of qualitative assessments and multimodal understanding.  Questions and Suggestions for the Authors: 1. Bias and Fairness Evaluation: Have you analyzed MIDs ability to detect or mitigate biases present in the CLIP model or generated content Including such an analysis could address potential concerns related to the societal impact of generative models. 2. Extension Beyond VisualTextual Modalities: Could MID be adapted for evaluating generative models in other modalities (e.g. audiotext) Expanding the metrics applicability could significantly increase its utility. 3. Comparison with Human Judgment: How does MID compare with direct human evaluation in finegrained aspects of generated content such as creativity or emotional impact Including such comparisons could further validate the metrics alignment with human perceptions.  Limitations and Societal Impact: The authors have not thoroughly addressed the limitations related to the dependence on CLIP features which may inherit biases from the pretrained model. They also do not provide an explicit discussion on the potential negative societal impact such as the misuse of generative models for creating misleading information. To improve the authors could consider adding:  An analysis of MIDs behavior in scenarios where CLIP is known to exhibit biases.  A discussion of ethical considerations and potential mitigations for the misuse of generative models emphasizing the importance of responsible use and further research on bias and fairness in multimodal evaluation metrics.", "nLGRGuzjtoR": " Review  1) Brief Summary The paper presents a comprehensive theoretical and empirical critique of existing methods for removing sensitive or spurious concepts from neural network representations specifically in text classification tasks. It contends that posthoc and adversarial removal methods which rely on an auxiliary (probing) classifier to identify and eliminate unwanted latent features are fundamentally flawed. The authors argue these methods can either fail to remove the target concept or worse eliminate useful taskrelevant information due to the inaccuracy of the probing classifiers in exclusively targeting conceptcausal features. Through both theoretical proofs and experiments across synthetic and realworld datasets (MultiNLI a sentiment analysis task tweetmention detection and Twitter datasets) the paper demonstrates the counterproductive effects of these removal techniques. It also proposes a spuriousness score as a metric to evaluate the quality of concept removal methods more reliably.  2) Assessment of Strengths and Weaknesses Strengths:  Rigorous Theoretical Framework: The paper provides a strong theoretical basis for the criticism of probingbased concept removal methods highlighting why these methods inherently risk failing due to the inability to isolate conceptcausal features accurately.  Comprehensive Empirical Validation: The experiments conducted across multiple datasets and scenarios offer solid evidence supporting the theoretical claims showcasing realworld implications.  Practical Contribution: The introduction of a spuriousness score is a significant practical contribution aiding in the evaluation of concept removal methods effectiveness beyond the mere accuracy of a probing classifier. Weaknesses:  Assumption of Binary Labels: The theoretical analysis primarily focuses on the scenario where both the main task and concept labels are binary. Although this simplification aids in clarity and tractability it might limit the generalizability of the findings to more complex multiclass scenarios.  Frozen Latent Representations: The paper assumes a nontrainable (frozen) encoder for parts of its analysis which might not correspond to how most realworld applications operate potentially limiting the applicability of the theoretical insights.  Lack of Alternative Solutions: While the paper effectively critiques current methodologies and provides a useful metric for assessment it stops short of proposing or evaluating alternative approach strategies for concept removal leaving readers without a clear path forward.  3) Questions and Suggestions for Authors  MultiClass Extension: Can the theoretical analysis be extended to cover multiclass scenarios and if so how might the conclusions differ  Alternative Methodologies: Given the identified shortcomings of probingbased methods what alternative strategies do the authors envision could more effectively remove unwanted concepts while preserving taskrelevant features  Impact of NonFrozen Encoders: How do the authors expect the dynamics and efficacy of concept removal methods to change when considering trainable encoders which are more common in practice  Broader Implications: Could the authors elaborate on the broader implications of their findings for the field of model fairness and interpretability especially in regards to developing models resilient to spurious correlations  4) Addressing Limitations and Societal Impact The paper adeptly identifies its methodological assumptions\u2014such as the use of binary labels and frozen encoders\u2014and their potential limitations. However it could benefit from a deeper exploration of alternative concept removal strategies that evade the pitfalls outlined. Additionally while the negative societal impacts of failing to remove biased or sensitive concepts from models are implicitly acknowledged a more explicit discussion on how these findings could guide more ethical AI development practices would enhance the papers contribution. Constructive suggestions include encouraging research into data augmentation strategies that generate counterfactual examples and advocating for a shift towards fairness constraints on model outputs rather than representations.", "vMQ1V_z0TxU": "Summary and Contributions: This paper addresses the challenge of unsupervised outofdistribution (OOD) detection a key aspect for ensuring the reliability of machine learning systems. The authors identify a significant limitation in hierarchical Variational Autoencoders (VAEs) known as \"posterior collapse\" which hinders their performance in OOD detection. To overcome this they propose an informative hierarchical VAE designed to strengthen the connections between data samples and their multilayer stochastic latent representations. Additionally they introduce a new score function named Adaptive Likelihood Ratio (LLRada) which leverages semantic divergences across the latent hierarchy to distinguish between indistribution and OOD samples effectively. The combined approach demonstrates superior performance in OOD detection over stateoftheart methods across various benchmarks. Strengths: 1. Novel Approach: The informative hierarchical VAE and the Adaptive Likelihood Ratio score function are innovative solutions to the \"posterior collapse\" problem addressing a gap in existing literature on unsupervised OOD detection. 2. Theoretical and Empirical Validation: The paper provides a detailed theoretical analysis of \"posterior collapse\" and demonstrates how their proposed method effectively mitigates this issue. The empirical results further validate their theoretical claims showcasing significant improvements in OOD detection performance. 3. Robust Evaluation: The authors conduct extensive comparisons with stateoftheart methods and across various datasets illustrating the generalizability and effectiveness of their approach. Weaknesses: 1. Complexity and Implementational Details: While the proposed method is innovative it introduces additional complexity particularly in the construction of the informative hierarchical VAE and the computation of the Adaptive Likelihood Ratio. More details on the practical implementation aspects would be beneficial. 2. Potential for Overfitting: Given the methods reliance on enhancing connections between data samples and latent representations there might be a risk of overfitting especially in scenarios with limited or highly specific indistribution data. This aspect could be further explored. Questions and Suggestions for Authors: 1. Implementation Details: Can the authors provide more insight into the implementation challenges if any associated with the informative hierarchical VAE and Adaptive Likelihood Ratio computation 2. Impact of Data Diversity: How does the proposed method perform across datasets with varying levels of complexity and diversity Are there certain types of datasets where the method performs better or worse 3. Comparison with Supervised Methods: While the focus is on unsupervised OOD detection it would be interesting to see how the proposed method compares against supervised OOD detection approaches especially in terms of scalability and adaptability to new datasets. Limitations and Societal Impact: The authors have not extensively discussed the potential limitations or negative societal impacts of their work. Considering the increasing deployment of machine learning systems in critical applications an inadequate understanding of the limitations could lead to overreliance on potentially flawed OOD detection mechanisms. To improve the authors could: 1. Explore the potential limitations regarding the diversity and representation of the datasets used for training and validate the method\u2019s performance across a wider range of realworld scenarios. 2. Discuss the computational efficiency of their method especially in resourceconstrained environments or realtime applications. 3. Highlight any potential ethical implications such as biases that might be amplified through the use of their method for OOD detection and suggest mitigation strategies. Overall this paper makes a significant contribution to the field of unsupervised OOD detection by addressing a critical limitation in hierarchical VAEs. With further exploration of the highlighted questions and suggestions the work could have a profound impact on the reliability and safety of machine learning systems across various applications.", "mMT8bhVBoUa": " Review of \"Gaussian Wasserstein Inference in Function Spaces (GWIFS)\"  1) Summary The paper presents a novel framework for generalized variational inference in infinitedimensional function spaces introducing Gaussian Wasserstein Inference in Function Spaces (GWIFS) or GWI. The primary innovation lies in utilizing the Wasserstein distance between Gaussian measures in Hilbert spaces to derive a variational posterior through a tractable optimization criterion. This method elegantly sidesteps issues confronted by traditional variational inference approaches particularly in function space inference. Moreover the paper demonstrates how deep neural networks (DNN) can be integrated within the GWI framework (termed GWInet) to enhance predictive performance while preserving rigorous uncertainty quantification akin to Gaussian processes. The authors validate the effectiveness of GWInet across various datasets showcasing its stateoftheart performance.  2) Strengths and Weaknesses Strengths:  Theoretical Novelty: The paper adeptly bridges gaps between Gaussian processes variational inference and Wasserstein distance to create a robust methodology for inference in function spaces.  Practical Utility: GWInets ability to meld the representational prowess of DNNs with principled uncertainty quantification is both novel and practically beneficial offering enhanced predictive performance.  Empirical Validation: The authors provide a comprehensive set of experiments across diverse datasets demonstrating the superior performance of GWInet over existing methods in terms of negative loglikelihood (NLL) and outofdistribution (OOD) detection. Weaknesses:  Computational Efficiency: While achieving impressive performance the paper does not thoroughly discuss the computational implications of GWInet especially when scaling to larger datasets or deeper DNN architectures.  Variational Approximation Limitation: The reliance on Gaussian measures for prior and posterior approximation restricts the potential to capture multimodal posterior distributions which could limit the methods applicability in some scenarios.  Kernel Selection Sensitivity: The methods success hinges on the appropriate selection of kernels and their spectral properties which might require careful tuning and potentially limit the methods generalizability.  3) Questions and Suggestions  Question: The paper mentions overcoming numerical instabilities associated with the inversion of the kernel matrix through smart initialization. Could you elaborate on these strategies and discuss their general applicability  Suggestion: It would be beneficial to incorporate a more detailed analysis regarding the computational complexity of GWInet possibly comparing it to both traditional variational inference methods and Gaussian processes in terms of computational time and resource requirements.  Suggestion: Exploring nonGaussian variational approximations within the GWI framework could potentially enhance its flexibility and applicability. Future work could investigate this direction possibly leveraging recent advancements in variational inference methodologies.  4) Limitations and Societal Impact The authors have adequately discussed some limitations such as potential numerical instabilities and the approximation quality relative to kernel properties. However an indepth computational complexity analysis and its implications for largerscale problems would enrich the discussion. Moreover while the paper briefly mentions the Gaussian nature of posterior approximation as a constraint a more thorough exploration of potential impacts or workarounds would be beneficial. Regarding societal impact while the paper primarily focuses on advancing inference methodologies within machine learning the implications of improved uncertainty quantification in sensitive applications (e.g. healthcare autonomous vehicles) bear consideration. Future iterations could benefit from a discussion on the ethical and societal implications of deploying such advanced inference systems in realworld scenarios especially considering the potential for misinterpretation of uncertainty estimates. Constructive Suggestions for Improvement:  Consider expanding the limitations section to address scalability and potential strategies for tackling nonGaussian posteriors.  Include a section or discussion on the potential societal impacts of deploying advanced inference systems utilizing GWInet in sensitive applications striving for responsible and transparent use of such technologies.", "zrAUoI2JA2": "The paper presents the formulation and evaluation of uHuBERT a novel selfsupervised pretraining framework designed for processing various speech modalities including both multimodal (audiovisual) and unimodal (audio or visual alone) data. The primary innovation of uHuBERT lies in its ability to learn modalityagnostic framelevel representations through a unified masked cluster prediction objective leveraging modality dropout during pretraining. This approach enables the model upon finetuning to perform admirably on speech processing tasks across different input modalities even achieving zeroshot modality generalization. Specifically when tested with audiovisual audio and visual inputs on LRS3 the model shows competitive or superior word error rates compared to existing modalityspecific models marking a significant step forward in the field of multimodal speech processing. Strengths: 1. Innovative Approach to Data Utilization: The paper addresses a crucial bottleneck in multimodal speech model development  the scarcity of labeled and unlabeled data across modalities. By employing a unified pretraining strategy that leverages both unimodal and multimodal data uHuBERT vastly expands the pool of usable data enabling more robust learning and generalization. 2. Zeroshot Modality Generalization: The ability of uHuBERT to perform zeroshot modality generalization significantly enhances its practical utility as it can effortlessly adapt to and operate on modalities unseen during finetuning. This flexibility is unprecedented and highly valuable for deploying models in dynamic realworld scenarios where data availability varies. 3. Comprehensive Evaluation: The paper provides exhaustive analysis and experimentation comparing uHuBERT with stateoftheart modalityspecific models across different speech processing tasks and modalities. This thorough evaluation not only establishes the model\u2019s efficacy but also underscores its potential as a versatile solution for multimodal speech processing. Weaknesses: 1. Computational Cost and Complexity: While the model achieves remarkable performance the paper does not thoroughly discuss the computational requirements or the practicality of deploying such a model in resourceconstrained environments. The complexity of training and finetuning a model of this nature particularly one that simultaneously handles various data modalities could be a limiting factor for adoption. 2. Generalization to Other Domains: Although uHuBERT demonstrates impressive zeroshot modality generalization within speech processing tasks it remains unclear how well this approach would generalize to other domains or tasks outside of speech recognition and translation. Explicit exploration or discussion in this area could broaden the model\u2019s applicability. Questions and Suggestions for the Authors: 1. Exploring Further Domains: Can uHuBERT be adapted or extended for tasks beyond speech processing such as multimodal sentiment analysis or crossmodal retrieval Insights into potential modifications or extensions for these applications could enhance the models utility. 2. Addressing Computational Efficiency: Could the authors provide more details on the models computational requirements and any observed or anticipated challenges in deploying uHuBERT in realworld possibly resourcelimited scenarios Suggestions for reducing computational burden without significantly compromising performance would be valuable. 3. Impact of Different Data Augmentation Techniques: The use of modality dropout is novel but how does its impact compare to other data augmentation or manipulation techniques in pretraining An analysis here could offer deeper insights into the key factors driving the model\u2019s robustness and versatility. Limitations and Societal Impact: While the paper touches upon the revolutionary potential of uHuBERT in enhancing multimodal speech processing it does not thoroughly delve into the limitations or the broader societal implications. For instance the adaptation of the model to languages or dialects with limited data availability remains unaddressed. Furthermore discussions on ethical considerations particularly concerning privacy and consent in the context of processing potentially sensitive audiovisual data are absent. To strengthen the paper the authors are encouraged to consider these dimensions critically. Addressing these aspects would not only provide a more comprehensive understanding of the models limitations but also guide future work towards responsible and inclusive research directions.", "p62j5eqi_g2": " Peer Review for \"Blackbox Adversarial Attacks against Deep Clustering Models\"  1. Summary This paper explores an underinvestigated area in the realm of deep learning robustness specifically targeting deep clustering models with adversarial attacks. Traditionally unsupervised machine learning models like clustering algorithms were not analyzed for vulnerability against adversarial examples to the same extent as supervised models. The authors propose a novel blackbox adversarial attack leveraging Generative Adversarial Networks (GANs) to generate minimally perturbed images that significantly degrade the performance of stateoftheart deep clustering models. Through extensive experiments on multiple realworld datasets (e.g. CIFAR10 CIFAR100 STL10) the authors demonstrate the effectiveness of their attacks not only in reducing the clustering performance but also in attacking a productionlevel face clustering API Face. The paper also explores existing defense strategies finding them largely ineffective against their proposed attack underscoring the need for more robust defense mechanisms in deep clustering.  2. Assessment Strengths:  Originality: The paper tackles an innovative and scarcely studied problem space\u2014the robustness of deep clustering models to adversarial attacks specifically in a blackbox scenario where the model details are unknown to the attacker.  Technically Sound Methodology: The proposed GANbased attack architecture is wellthoughtout and aligns with the goal of generating effective adversarial examples without explicit knowledge of the underlying model.  Solid Experimental Validation: The experimental setup is rigorous covering several stateoftheart deep clustering models and datasets. The inclusion of a productionlevel MLaaS API (Face) adds to the realworld relevance and the generality of their findings.  Comprehensive Analysis: Beyond proposing a novel attack the paper does a commendable job at analyzing current defense strategies adding depth to the discourse on making deep clustering models robust. Weaknesses:  Limited Exploration of Defense Mechanisms: While the paper provides insight into why current defenses fail it stops short of exploring or proposing any new defensive strategies tailored to deep clustering models.  Assumption of Dataset Knowledge: The attack model assumes knowledge of the training dataset which might not always be practical or possible in realworld scenarios. This could limit the applicability of the findings or the perceived severity of the threat.  3. Questions and Suggestions for the Authors  On Defense Mechanisms: Have the authors considered any novel defense mechanisms tailored for deep clustering models that could potentially mitigate the proposed attack Including even preliminary ideas or directions could be valuable for future research.  Transferability Across Datasets: It would be informative to understand whether adversarial examples generated for one dataset (e.g. CIFAR10) retain their potency when applied to a different dataset (e.g. CIFAR100) beyond just model transferability.  Impact of Dataset Knowledge: Could the authors elaborate on the degree to which the attacks effectiveness is reduced if the exact training dataset is unknown Exploring partial knowledge scenarios might broaden the scope of the findings.  Computational Overheads: Generating adversarial examples using GANs can be computationally intensive. Some discussion on the practicality of scaling this attack in constrained environments or suggestions on optimizing the training process would be beneficial.  4. Limitations and Societal Impact The authors make an effort to address the limitations and potential negative societal impacts of their work. The necessity of meeting all threat model conditions for the most effective attack and the assumption regarding knowledge of the training dataset are acknowledged limitations. Moreover while the paper mentions potential negative uses of their findings it firmly advocates for the work to serve as a catalyst for developing more robust deep clustering models. Suggestions for improvement include a deeper discussion on potential defense strategies and an extension of the threat model to account for scenarios with limited or no prior dataset knowledge. Additionally it would be beneficial to discuss further the ethical considerations of publishing such attack methodologies including any safeguards the authors might recommend to prevent misuse.", "ripJhpwlA2v": " Review of \"Group Invariant Learning with Emphasis on Falsity Exposure and Label Balance Criteria\"  Summary This paper tackles the problem of spurious correlations in machine learning specifically focusing on group invariant learning (groupIL) where environment labels are unknown. The authors highlight the limitations of current groupIL methods which fail to generalize well to outofdistribution (OOD) data due to reliance on spurious correlations. To address this the paper proposes two crucial criteria for effective groupIL: the Falsity Exposure criterion and the Label Balance criterion. The former ensures that variations in spurious features are sufficiently exposed across groups while the latter ensures that the label distribution is balanced across groups. The authors introduce SCILL (SpuriousCorrelationStrata Invariant Learning with Labelbalancing) a novel groupIL method designed to satisfy these criteria. Theoretical analysis and extensive experiments on synthetic and realworld datasets demonstrate SCILLs superiority over existing methods in handling spurious correlations.  Strengths 1. Theoretical Contribution: The introduction of the Falsity Exposure and Label Balance criteria is novel. These criteria provide a theoretical foundation that was previously lacking in group invariant learning research. The papers theoretical analysis deepens the understanding of why certain groupIL methods fail and establishes a clear pathway for designing effective groupIL models. 2. Practical Impact: The SCILL methodology is not only theoretically sound but also practically effective as demonstrated by its superior performance on both synthetic and realworld datasets. This makes the papers contribution highly relevant for various applications plagued by spurious correlations. 3. Comprehensive Evaluation: The thorough experimental setup including comparisons to stateoftheart methods ablation studies and robustness analysis convincingly validates the proposed criteria and the SCILL method. The inclusion of both synthetic and real data benchmarks provides a comprehensive picture of the methods effectiveness across different scenarios.  Weaknesses 1. Causal Assumption Generalizability: While the paper makes significant strides in addressing spurious correlations its theoretical foundation is built on specific causal assumptions. Specifically the assumption of conditional independence between invariant and spurious features given the label may not hold in all realworld scenarios. This limitation could affect the generalizability of the findings. 2. Complexity and Implementation Details: The paper introduces a complex method that might be challenging to replicate without detailed implementation insights. While code is provided a more thorough explanation of the algorithmic details hyperparameter tuning and practical considerations for applying SCILL in various contexts would enhance the papers accessibility and impact. 3. Discussion on Negative Societal Impacts: The manuscript briefly mentions addressing potential negative societal impacts but does not delve into specific scenarios or mitigation strategies. Given the emphasis on improving model generalization and fairness a more detailed discussion on this aspect could strengthen the papers contribution to ethical AI research.  Questions and Suggestions for the Authors 1. Causal Assumption Expansion: Can the authors discuss how SCILL might be adapted or extended to scenarios where the conditional independence assumption does not hold Are there practical steps researchers can take to apply SCILL under different causal structures 2. Implementation Details: Could the authors provide more insights into the challenges of implementing SCILL especially concerning the statistical split algorithm and the choice of hyperparameters This information would be invaluable for researchers and practitioners looking to apply SCILL to their problems. 3. Addressing Negative Societal Impacts: Could the authors expand on the potential negative societal impacts of their work including scenarios where SCILL might inadvertently exacerbate biases Additionally suggesting concrete strategies for mitigating such impacts would be beneficial.  Limitations and Societal Impact The paper rightly identifies the generalizability of its causal assumption as a limitation. Expanding the theoretical framework to account for more complex causal structures could further enhance the utility and applicability of SCILL. As for societal impacts while the method aims to mitigate spurious correlations thereby improving model fairness and robustness a deeper exploration into how SCILL could affect algorithmic bias and fairness across different domains would be a meaningful contribution.  Conclusion This paper makes a significant contribution to the field of machine learning by addressing the critical issue of spurious correlations through a wellfounded theoretical framework and a novel group invariant learning method SCILL. Despite its limitations the work sets a new direction for future research in handling spurious correlations and enhancing model generalization. With further development the SCILL method has the potential to impact various practical applications and contribute to the advancement of more reliable and fair AI systems.", "uyEYNg2HHFQ": " Peer Review Report  Summary of the Paper This paper presents a novel approach for learning and applying hyperrepresentations derived from a \"model zoo\" (a large collection of pretrained neural network models) to generate new model weights for neural networks. Its primary contributions include extending the use of hyperrepresentations to generative tasks introducing a layerwise loss normalization technique that enhances the quality of generated model weights and proposing various sampling methods based on the topology of hyperrepresentations. The authors demonstrate that models generated using these techniques are diverse performant and capable of outperforming strong baselines across several downstream tasks including model initialization ensemble sampling and transfer learning. The methodology is validated on multiple image datasets showcasing the potential of leveraging aggregated knowledge from existing models to create new models efficiently.  Assessment of Strengths and Weaknesses Strengths: 1. Novel Approach: The extension of hyperrepresentation learning to generative tasks is novel and innovative representing a significant contribution to the field of neural network initialization and model generation. 2. Comprehensive Evaluation: The paper provides a thorough empirical evaluation across various datasets and tasks convincingly demonstrating the efficacy of the proposed approach. 3. Practical Relevance: The ability to generate performant models without the need for original training data has significant practical implications especially in scenarios where data might be sensitive or not readily available. 4. Open Source Contribution: The availability of code and datasets enhances the reproducibility of the research and facilitates future work in this area. Weaknesses: 1. Lack of Theoretical Insights: The paper primarily focuses on empirical results with limited theoretical analysis on why the proposed methods work and under what conditions they may fail. 2. Scalability Concerns: Although the paper presents results on small to mediumsized architectures it does not thoroughly address scalability to larger more complex networks. 3. Limited Discussion on Limitations and Societal Impact: Theres a brief mention of potential negative societal impacts but a deeper exploration of these aspects including biases in model zoos and potential misuse would strengthen the paper.  Questions and Suggestions for the Authors 1. Theoretical Foundation: Can the authors provide any theoretical insights or intuitions why layerwise loss normalization significantly improves the generation of functional model weights 2. Scalability: How does the proposed method scale with the size and complexity of the neural networks in the model zoo Can the current approach be efficiently applied to very large models such as those used in natural language processing tasks 3. Limitations and Societal Impact: Could the authors elaborate on the potential limitations of their approach especially regarding the diversity of the model zoos and how it might affect the robustness of generated models Additionally discussing possible negative societal impacts such as propagating biases present in the model zoo would be valuable. 4. Comparison with StateoftheArt: While the paper demonstrates that the generated models outperform strong baselines a comparison with stateoftheart methods in model initialization and knowledge distillation might provide a clearer picture of the proposed methods advantages and positioning.  Limitations and Constructive Suggestions The paper could benefit from a deeper theoretical analysis of the proposed methods which could help in understanding the conditions under which they are expected to perform well or fail. Additionally addressing scalability issues more explicitly would enhance the papers relevance to a broader range of applications. To mitigate these limitations I suggest the following:  Explore theoretical aspects of layerwise loss normalization and its impact on the generative process.  Conduct experiments or provide analysis on the scalability of the proposed methods perhaps including case studies on more complex network architectures.  Enhance the discussion on limitations and potential negative societal impacts offering guidelines or remediations to mitigate these concerns. Overall this paper presents a promising approach towards leveraging existing models to generate new ones efficiently. Addressing the weaknesses and considering the suggested improvements would further strengthen this valuable contribution to the field.", "kQgLvIFLyIu": "Review of \"Coreset Construction for LineSets Clustering and its Generalizations\" 1) Summary and Contributions: The paper presents an innovative approach to the klinesets clustering problem where the goal is to find k centers in a multidimensional space that minimize the sum of squared distances from every set of lines to its nearest center. The authors generalize the classic kmeans problem and introduce a novel concept of \u03b5coresets for this problem\u2014a small weighted subset of linesets that approximates the original sets clustering cost within a (1 \u00b1 \u03b5) factor for any choice of k centers. The primary contributions of the paper include:  A proof that small \u03b5coresets exist for the linesets kmedian problem.  The first algorithm to construct such coresets efficiently with a runtime of O(n log n) for fixed dimensions number of centers k and approximation error \u03b5.  A reduction of the linesets clustering problem to a new problem of \"fair clustering\" involving sets of colored points and centers providing broader applicability and potential for future research in clustering diverse data forms. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: The paper addresses a unique and complex problem by extending the wellknown kmeans clustering to linesets offering a fresh perspective on clustering geometrical shapes.  Practicality: The introduction of \u03b5coresets for linesets clustering and its construction algorithm is highly practical and extends the toolbox available for handling data in various dimensions and structures including applications in computer vision and dealing with missing data.  Comprehensive Approach: In addition to theoretical proofs the authors provided an opensource implementation of their algorithms and conducted extensive experiments on both synthetic and real datasets lending credibility and practical value to their work. Weaknesses:  Clarity in Algorithm Presentation: The methods and algorithms particularly around the novel reduction process and coreset construction could benefit from more detailed explanations or stepbystep illustrative examples to aid readers\u2019 understanding.  Discussion on Limitations: While the paper mentions the construction time and the dependency on various parameters a deeper exploration into the practical limitations such as scenarios with extremely high dimensions (curse of dimensionality) or the handling of nonlinear shapes would be valuable.  Analysis of Societal Impact: The paper does not directly address potential negative societal impacts. Given the broad applicability of clustering including potentially sensitive applications a discussion on ethics privacy or misuse would have been appropriate. 3) Questions and Suggestions for the Authors:  Could the authors provide more intuitive insights or visualization into how the reduction to fair clustering works This complex process might be more accessible with additional diagrams or stepbystep examples.  The experiments mainly focus on the performance comparison between the proposed coreset construction and uniform sampling. Have there been considerations for comparing it against other stateoftheart clustering or coreset methods  Given the versatility of the proposed method could the authors discuss potential applications beyond the ones listed Specifically applications in network analysis or bioinformatics where the data can naturally form lines or require clustering with mixed data types might be particularly interesting.  A suggestion would be to include a discussion or a separate section on potential negative societal impacts and ethical considerations surrounding automated clustering methods. Since clustering is often applied to data involving individuals (e.g. in social networks customer data) addressing bias privacy and fairness explicitly would enrich the papers contribution. 4) Limitations and Societal Impact: The paper makes a significant methodological contribution but falls short in discussing the limitations related to highdimensional data and nonlinear shapes tractability. Moreover the absence of a direct examination of potential negative societal impacts or misuse in sensitive applications (e.g. surveillance data mining without consent) is a noticeable gap. To improve the authors could extend their experimental analysis to include highdimensional scenarios and discuss strategies to mitigate any identified limitations. Additionally incorporating a section on ethical considerations privacy preservation techniques and fairness in clustering could make the paper more rounded and socially responsible.", "uCXNOeL0TG": "Peer Review Report: 1) Summary of the Paper and its Contributions: The paper introduces the multiworker restless multiarmed bandit (MWRMAB) problem addressing the challenge of intervention planning under resource constraints and heterogeneity in intervention resources. This problem is crucial for applications in diverse fields such as project monitoring machine repair and antipoaching patrol scheduling. The authors recognize the limitations of existing restless multiarmed bandit (RMAB) frameworks that treat intervention resources as coming from a uniform pool thereby ignoring the realworld scenario where resources (workers) have varied costs budgets and effects. To address this gap the paper makes two significant contributions. Firstly it extends the Whittle index for the RMAB problem to accommodate multiple workers with heterogeneous intervention costs and budgets. Secondly it proposes a novel indexbased scheduling policy that ensures fairness among workers in terms of workload distribution alongside achieving efficient resource allocation. An empirical evaluation of their method across various cost structures confirms the superiority of their approach in terms of fairness and reward accumulation in comparison with other baseline methods. 2) Assessment of the Strengths and Weaknesses: Strengths:  Novelty: The paper addresses a gap in the RMAB literature by considering the heterogeneity of workers which is a significant step towards modeling more realistic intervention planning problems.  Practicality and Applicability: The proposed framework is motivated by and applicable to realworld problems in diverse domains highlighting its practical utility.  Theoretical and Empirical Validation: The authors provide a rigorous theoretical foundation for their approach accompanied by comprehensive empirical evaluation showcasing the efficiency and fairness of their proposed methods.  Methodological Rigor: The decomposition of the MWRMAB problem into subproblems and the subsequent computation of Whittle indices is elegantly tackled displaying methodological strength. Weaknesses:  Scalability Concerns: While the method is shown to outperform existing approaches in terms of fairness and efficiency the scalability of the algorithm in extremely largescale scenarios remains less explored.  Limited Addressing of Societal Impacts: While the paper briefly mentions potential negative societal impacts in the appendix a more thorough exploration of these aspects especially in sensitive applications like antipoaching would enhance its relevance. 3) Questions and Suggestions for Authors:  Scalability: Can the authors provide more insights on the scalability of their methodology in scenarios with a significantly larger number of arms and workers Any suggestions on how to mitigate potential computational bottlenecks would be valuable.  Diversity of Applications: The paper primarily focuses on applications like antipoaching patrols and machine repairs. Could the authors discuss the applicability and adjustments needed for their model in other domains for instance healthcare or disaster response  Ethical Considerations: Given the potential application in sensitive areas could the authors elaborate on ethical considerations specifically on how algorithmic decisions might affect the workers wellbeing and job satisfaction 4) Limitations and Suggestions for Improvement: The authors have adequately addressed many of the conceptual and technical limitations of their work. However the paper could be strengthened by a more detailed discussion on practical constraints such as the impact of incorrect model assumptions or data inaccuracy on the fairness and efficiency of the intervention planning. Moreover exploring the potential unintended consequences of implementing such a system in realworld settings such as job displacement or increased workload pressure on certain workers would make the work more comprehensive. In summary the paper presents a highly relevant and novel approach to a complex problem with solid theoretical backing and empirical validation. Addressing the identified weaknesses and considering the proposed suggestions would further enhance its contribution to the field.", "p9_Z4m2Vyvr": "Review of \"Clusterwise Amortized Mixing Coupling Processes for Efficient and Flexible Clustering\" 1) Summary: This paper introduces a novel clustering approach named Amortized Mixing Coupling Processes (AMCP) which offers an efficient and flexible mechanism for data clustering. The method leverages nonparametric Bayesian posterior for amortized clustering significantly reducing computational complexity from quadratic to linear in terms of the current number of clusters through intracluster mixing (IntraCM) and intercluster coupling (InterCC) strategies. These strategies effectively investigate and utilize relationships between data points and reference distributions to generate and adjust clusters dynamically thus addressing scalability issues associated with large datasets. Extensive experiments on both synthetic and realworld datasets demonstrate the superiority of AMCP over existing methods in terms of clustering performance and computational efficiency. 2) Assessment: Strengths:  The paper addresses the critical issue of scalability in clustering large datasets by proposing an innovative method that reduces computational complexity.  AMCPs ability to dynamically adjust the number of clusters and its parameters during the clustering process is a notable advancement over fixed cluster models making it highly adaptable to various datasets.  The use of optimal transport theory to relate data points within and across clusters is novel and demonstrates a strong theoretical foundation.  The extensive experimental evaluation including comparisons with stateoftheart methods on both synthetic and realworld datasets solidly validates the proposed approachs effectiveness and efficiency. Weaknesses:  While the paper presents a comprehensive set of experiments the analysis of the impact of hyperparameters (e.g. the number of EM steps) is somewhat limited and could be expanded.  The discussion on the potential implications of the proposed method on privacy or fairness when clustering sensitive data is missing. Given the growing concerns around ethical AI such a discussion could enhance the papers relevance.  A deeper exploration into the limitations of the AMCP especially in scenarios where the underlying assumptions may not hold (e.g. highly noisy datasets or datasets with nondistinct clusters) would provide a more balanced view of its applicability.  The paper could benefit from a more detailed discussion on the theoretical properties of AMCP such as convergence guarantees and robustness to initialization. 3) Questions and Suggestions:  Could the authors elaborate on how AMCP would perform in the presence of highdimensional data where the curse of dimensionality might affect the efficiency of clustering  It would be interesting to see a comparison or integration of AMCP with recent advances in deep clustering methods. Can the authors discuss potential synergies or conflicts  Considering the methods reliance on optimal transport how sensitive is AMCP to the choice of distance metrics and are there specific metrics that work best with this approach  A suggestion would be to explore and discuss potential extensions of AMCP to address hierarchical clustering or multiview clustering challenges which are common in complex realworld datasets. 4) Limitations and Societal Impact: The authors have not adequately addressed the potential limitations and negative societal impacts of their work. For constructive improvement it would be beneficial to include:  A discussion on the robustness of AMCP against adversarial attacks or data poisoning which could be critical if deployed in sensitive or securityfocused applications.  Considerations around the privacy implications of clustering especially when dealing with sensitive personal data and whether AMCP introduces any particular vulnerabilities or challenges in this regard.  An exploration of scenarios where AMCP might fail or perform suboptimally along with recommendations for practitioners on how to diagnose and mitigate such issues. Overall the paper presents a significant contribution to efficient clustering methodologies. Addressing the highlighted questions and suggestions would further strengthen the papers impact and applicability to a broader set of clustering problems.", "pz2UcXyX0Cj": " Peer Review for \"CausalityDriven Hierarchical Reinforcement Learning (CDHRL)\"  1. Brief Summary The paper introduces CDHRL a novel framework for Hierarchical Reinforcement Learning (HRL) that leverages causality among environment variables to autonomously discover highquality hierarchical structures aiming at improving exploration efficiency in complex environments with sparse rewards. Unlike previous methods that rely on randomnessdriven exploration CDHRL utilizes causality to guide the construction of subgoal hierarchies leading to a more efficient method for discovering and structuring subgoals in environments like 2DMinecraft and Eden. The framework consists of causality discovery and subgoal hierarchy construction processes which iteratively boost each other demonstrated through substantial performance improvements over stateoftheart HRL methods.  2. Assessment of Strengths and Weaknesses Strengths: 1. Innovative Approach: The causalitydriven paradigm to discover hierarchical structures is a significant departure from traditional randomnessdriven methods addressing the critical challenge of low exploration efficiency. 2. Iterative Boosting Framework: The synergistic iteration between causality discovery and subgoal hierarchy construction is a compelling methodology that ensures continuous improvement in the exploration strategy. 3. Performance Improvement: The empirical results present a considerable enhancement in learning speed and final performance in complex tasks compared to existing HRL methods demonstrating the efficacy of CDHRL. 4. Causality as a Natural Fit: By leveraging natural causalities among environment variables the paper convincingly argues that such a structure is inherently suitable for modeling and discovering subgoals which adds to the intuitive appeal of the approach. Weaknesses: 1. Limited Application Scope: The paper acknowledges that CDHRL is primarily applicable in environments with discrete environment variables and requires an extra disentangled encoder in purely imagebased environments potentially limiting its broader applicability. 2. AssumptionHeavy Methodology: The implementation of CDHRL relies heavily on the assumption that highquality environment variables (EVs) can be extracted and that these variables truly capture the underlying state of the environment in a meaningful way for causality mapping. 3. Lack of Theoretical Insights: While the empirical results are strong the paper could benefit from a deeper theoretical analysis of why the causalitydriven approach is more effective including potential caveats and limitations. 4. Complexity and Computation Cost: Given the iterative nature of the proposed method and the complexity surrounding the causal discovery process the computational cost and scalability of CDHRL remain unclear from the current presentation.  3. Questions and Suggestions for the Authors 1. Scalability and Computation Cost: Could the authors elaborate on the computational complexity of CDHRL especially in relation to the iterative boosting cycle between causality discovery and subgoal hierarchy construction 2. Robustness of Causality Discovery: How robust is the causality discovery process to errors or noise in the environment variables A discussion on this would help in understanding potential limitations. 3. Generalization across Environments: It would be beneficial if the authors could discuss or provide insights into how CDHRL performs or might be adapted across varied environments beyond 2DMinecraft and Eden including continuous action spaces. 4. Theoretical Underpinnings: Further theoretical discussion on why causalitydriven exploration outperforms randomnessdriven approaches could enrich the papers contributions and provide a stronger foundation. 5. Potential Negative Societal Impact: The paper briefly mentions limitations but does not discuss potential negative societal impacts. Considering the current focus on responsible AI adding a discussion about ethical considerations or misuse scenarios (if any) might be necessary.  4. Limitations and Societal Impact The authors have outlined the limitations concerning the application scope of CDHRL particularly in environments with discrete environment variables and the need for an extra encoder in imagebased environments. However a more indepth discussion on the computational costs potential scalability issues and the robustness of causality discovery in noisy environments would further strengthen the paper. Regarding potential negative societal impacts the paper lacks a discussion on this front. While the direct negative impacts might not be apparent given the technical nature of the work a reflective consideration of how such powerful exploration methods could be misused or lead to unintended consequences in realworld applications would demonstrate a responsible approach to AI research. Constructive suggestions include elaborating on these limitations and considering a section on ethical considerations and safeguarding against misuse to guide future responsible applications of CDHRL.", "nax3ATLrovW": "Peer Review of \"Circuit GNN: Towards Efficient and Effective Deep Learning for Electronic Design Automation\" 1) Summary of the Paper The paper introduces a novel method for integrating topological and geometrical information into a unified heterogeneous graph named Circuit Graph aiming to enhance the efficiency and effectiveness of deep learning technologies in Electronic Design Automation (EDA). By proposing a messagepassing paradigm Circuit GNN the authors address the existing gap in integrating multiple information sources (netlist and design layout) optimally. The approach notably improves upon stateoftheart performance for specific prediction tasks within the EDA domain such as logic synthesis and global placement chip design stages alongside offering significant speed improvements in congestion prediction tasks. 2) Assessment of Strengths and Weaknesses Strengths:  Novelty: The introduction of the Circuit Graph as a means to synergize both topological and geometrical information within EDA processes represents a significant leap forward in the field. Furthermore the development of Circuit GNN for messagepassing based on this heterogeneous graph is a unique approach that distinguishes this work from existing methodologies.  Performance: The experimental results demonstrate superior performance of the proposed method compared to stateoftheart techniques especially noting a substantial reduction in congestion prediction time which is critical for improving the speed of EDA processes.  Versatility: By designing a model adaptable to both logic synthesis and placement stages the paper addresses a critical need within the EDA community for more flexible tools capable of operating across different stages of the design process. Weaknesses:  Generalizability Concerns: While the paper presents a novel approach and demonstrates its effectiveness across specific EDA tasks there is limited exploration into the adaptability of this method to other EDA processes or its application in different contexts such as earlystage EDA representations.  Assumption of Information Availability: The models performance is heavily reliant on the integrated use of topological and geometrical information. In scenarios where one of these is absent or substantially lacks quality the effectiveness of the method could be constrained.  Complexity and Reproducibility: Given the innovativeness of the approach there is a considerable amount of complexity involved in understanding and implementing the proposed model. The paper could benefit from a more detailed exposition regarding the implementation details to aid replication and adoption by other researcherspractitioners. 3) Questions and Suggestions for the Authors  Scalability: How does Circuit GNN perform in terms of scalability especially as the circuit complexity and the size increase It would be beneficial if the authors could discuss any potential performance bottlenecks.  Broader Applicability: Are there considerations or planned extensions for applying Circuit GNN to earlystage EDA tasks where different graph representations (e.g. dataflow graph) are prevalent Further discussion on this topic would enrich the papers scope.  Repository and Implementation Detail: Providing a public repository with code and detailed implementation instructions could significantly enhance the papers impact by facilitating easier experimentation and extension by the community.  Limitations on Data Availability: In realworld settings the completeness and availability of geometrical and topological information may vary. It would be insightful if the paper could discuss strategies or adjustments in such cases to maintain performance levels. 4) Limitations and Societal Impact The authors briefly mention the potential application gap between their novel machine learning algorithms and their adoption in mainstream commercial EDA tools which is an important limitation to consider. Additionally they acknowledge the varying applicability of their method to different graph representations used in EDA tasks. However a more indepth discussion on these limitations including potential strategies for bridging these gaps would strengthen the paper. Regarding societal impact the authors rightly note the minimal direct societal implications of their work primarily focused on technological advances within the EDA sector. Nevertheless elaborating on indirect impacts such as the potential for reduced electronic device costs or improved computational efficiencies leading to energy savings could offer a broader perspective on the significance of this research. Constructive Suggestions for Improvement:  Expanding the discussion on the models scalability and proposing solutions for potential bottlenecks.  Exploring the applicability and necessary adjustments for the model to suit different stages of EDA or other computational tasks beyond the scope of the current study.  Enhancing the reproducibility of the study by providing more detailed implementation details or making the codebase publicly available.  Broadening the discussion on the societal impacts including potential indirect benefits of the proposed technology.", "rTvH1_SRyXs": "Review of \"A Unified Framework for Understanding Post hoc Explanation Methods through Local Function Approximation\" 1. Summary This paper addresses the fragmentation and lack of a unified conceptual goal in the field of post hoc explainability of machine learning models. The authors propose a mathematical framework that brings together eight popular explanation methods under the umbrella of local function approximation (LFA) showing that these methods essentially perform a similar function but differ in the specifics of neighborhood and loss function employed. Through this unification the paper contributes to the field by (1) introducing a no free lunch theorem for explanation methods emphasizing that no single method can optimally perform across all situations and (2) offering a principle to guide the selection of methods based on their faithfulness to the blackbox model. These theoretical contributions are empirically validated across diverse realworld datasets model classes and prediction tasks. 2. Assessment Strengths:  Novelty and Contribution: The paper makes a significant conceptual leap by demonstrating the commonalities across disparate explanation methods which previously seemed unrelated. This unification is a substantial contribution to the field of explainable AI (XAI) as it not only clarifies the landscape but potentially simplifies the selection process for practitioners.  Theoretical and Practical Insights: The introduction of a no free lunch theorem for explanation methods is an impactful theoretical contribution highlighting the inherent tradeoffs in the field. The guiding principle for method selection based on faithfulness is a practical contribution that directly addresses the challenges faced by practitioners in choosing among explanation methods.  Empirical Validation: The authors provide a thorough empirical validation of their theoretical claims across various datasets and model types enhancing the credibility and applicability of their work. Weaknesses:  Scope of Explanation Methods: While the unification of eight popular methods is impressive the field of XAI encompasses a broader array of techniques. The frameworks applicability to methods beyond those analyzed (e.g. counterfactual explanations) remains unclear.  Interpretability vs. Faithfulness: The focus on local function approximation prioritizes faithfulness over interpretability. While the authors recognize this distinction the implications for the interpretability of explanations particularly in complex realworld applications are not fully explored.  Potential for Bias and Ethical Considerations: While the paper briefly mentions fairness across subgroups in related work a more indepth analysis of potential biases introduced by different explanation methods would enhance the work. Ethical considerations particularly in highstakes applications would benefit from further elucidation. 3. Questions and Suggestions for Improvement  Can the authors extend the framework to include other forms of explanation methods like counterfactual explanations to assess its broader applicability  It would be beneficial to include a more indepth discussion on the balance between interpretability and faithfulness perhaps suggesting ways to achieve a better balance in practice.  Regarding the potential for bias in explanation methods can the authors propose mechanisms or guidelines to evaluate and mitigate bias within the unified framework 4. Limitations and Societal Impact The paper does an admirable job in advancing the theoretical understanding and practical application of post hoc explanation methods. However the authors could further address the limitations regarding the scope of explanation methods covered and discuss the tradeoff between faithfulness and interpretability in more detail. On societal impact a deeper exploration of how different explanation methods might perpetuate or mitigate biases in model explanations would add value. Suggestions include developing guidelines for selecting explanation methods that minimize potential negative societal impacts focusing on fairness transparency and accountability. Overall this work marks a significant step towards demystifying the landscape of post hoc explanation methods in machine learning offering both theoretical insights and practical guidance that can aid in the development of more understandable and trustworthy AI systems.", "zAuiZpZ478l": "Review of \"Hierarchical Lattice Layer for Partially Monotone Neural Networks\" 1) Summary of the Paper: This paper introduces the Hierarchical Lattice Layer (HLL) which addresses challenges faced by the TensorFlow Lattice librarys lattice layer in partially monotone regression. Partially monotone regression takes advantage of the knowledge that target values vary monotonically with specific input features. The proposed HLL allows for training with a standard stochastic gradient descent algorithm while adhering to monotonicity constraints and supports highdimensional input vectors. Importantly the paper demonstrates through experiments that HLL maintains prediction performance similar to its predecessor TL Lattice expanding utility without compromising accuracy. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovation: The hierarchical approach to designing a neural network layer that can handle highdimensional vectors and ease training constraints is novel.  Experimental Validation: Conducting experiments with real datasets to compare HLL with TL Lattice and other models furnishing evidence of HLLs effectiveness.  Technical Depth: The paper provides a thorough technical exposition of HLL including its theoretical foundation complexity analysis and how it enforces partial monotonicity. Weaknesses:  Experimental Scope: The paper limits its experimental analysis to singlelayer comparisons. A more comprehensive analysis involving multilayer configurations or various dataset complexities could provide a deeper understanding of HLLs potential and limitations.  Comparison with Baseline Models: While HLLs performance is compared with TL Lattice and a standard MLP model evaluating against a wider range of existing partially monotone regression models could have strengthened the case for HLLs relative advantage.  Discussion on Practicality and Scalability: Given the constraint of HLL not being applicable for cases with a large `m` a discussion on practical workarounds scalability challenges or potential modifications to address this limitation would have been beneficial. 3) Questions and Suggestions for the Authors:  Scalability: Could you elaborate on potential strategies to make HLL applicable for scenarios where `m` is large Are there optimization techniques or structural modifications to HLL that could facilitate this  Multilayer Configurations: Have you considered evaluating the performance of HLL in a multilayer neural network setup Such an investigation could reveal synergies or challenges in using HLL within more complex models.  Comparison with Other Techniques: It might be instructive to compare HLL with other techniques addressing partially monotone regressions such as those utilizing constraintbased regularization. Could such comparisons be part of future work  Realworld Applications: The paper could benefit from a discussion on potential realworld applications of HLL especially in sectors where partially monotone regression is critical but challenged by highdimensional data. 4) Limitations and Societal Impact: The paper briefly mentions the limitation of HLL in handling applications with a large number of monotone features (`m`) without delving into possible ways to overcome this challenge. Including potential strategies for addressing this or discussing how future work could tackle this limitation would enhance the papers depth. On the societal impact front while the paper presents a technical advancement a concise discussion on how HLL could influence fair machine learning practices and reduce biases by better incorporating domain knowledge through partial monotonicity would have added valuable context. Given the increasing importance of ethical AI suggesting ways to ensure HLL\u2019s use adheres to ethical standards would be beneficial.", "xbJAITw9Z6t": " Peer Review for \"Stacked Unsupervised Learning Algorithm for MNIST Digit Clustering\"  1) Briefly summarize the paper and its contributions This paper presents a novel algorithm for Stacked Unsupervised Learning (SUL) that effectively clusters MNIST digits with a remarkable accuracy of 98 coming close to supervised and selfsupervised methods without requiring labeled data or data augmentation. The algorithm utilizes multiple convolutional energy layers inspired by primary visual cortex models with kernels trained via a fast minibatch implementation of the KSubspaces algorithm. Crucially the success of the model hinges on preprocessing with a convolutional ZCA whitening layer less sparse representations during inference compared to learning and rescaling for gain control. Hyperparameters of the network architecture are optimized through supervised metalearning enhancing unsupervised learning performance. This work challenges the prevailing dominance of backpropagationbased methods in practical applications by demonstrating SULs potential in a biologically plausible learning paradigm.  2) Thorough assessment of the strengths and weaknesses of the paper Strengths:  Biological Plausibility: The approach is inspired by neuroscientific models potentially offering insights into how learning occurs in the brain.  Innovative Methodology: The integration of KSubspaces with convolutional energy layers and the novel application of supervised metalearning for hyperparameter optimization in an unsupervised learning context are significant contributions.  Significant Performance: Achieving high accuracy on the MNIST digit clustering task comparable to supervised methods without using labeled data is a strong testament to the algorithms efficacy.  Comprehensive Evaluation: The authors provide an extensive examination of their approach through ablation studies demonstrating the impact of various components and choices within the architecture. Weaknesses:  Complexity and Computational Resources: The reliance on metalearning for architecture optimization could imply significant computational resources potentially limiting the methods accessibility.  Generalizability Concerns: The focus on MNIST while providing a clear benchmark does not address the algorithms performance on more challenging or diverse datasets.  Lack of Theoretical Understanding: While the algorithm shows practical success the paper does not deeply explore the theoretical underpinnings of why certain methods (e.g. KSubspaces clustering ZCA whitening) work well together.  3) Questions and suggestions for the authors  On Generalizability: Can the authors provide insights into how their SUL algorithm might perform on more complex datasets such as CIFAR10 or in different domains altogether  Clarification on MetaLearning: It would be beneficial to have a more detailed explanation of how supervised metalearning influences the unsupervised learning process and whether its benefits justify the computational overhead.  On Biological Plausibility: How do the authors reconcile the use of significant computational resources and supervised metalearning which may not have direct analogs in biological systems with the aim of achieving biologically plausible learning  Suggestions for Improvement: Exploring theoretical justifications for the methods components and providing a more detailed discussion on potential limitations regarding scalability and applicability to a wider range of tasks could strengthen the paper.  4) Limitations and Societal Impact The authors have not adequately addressed the limitations related to the generalizability of their method to more complex or varied datasets. Furthermore the potential negative societal impacts of advanced unsupervised learning technologies including privacy concerns and malicious use are not discussed. It is suggested that future iterations of the work include a broader discussion on these aspects potentially exploring the adaptation of this method to other datasets and domains and elaborating on ethical considerations relevant to unsupervised learning technologies.", "rJjJda5q0E": "The paper introduces an innovative approach to analyzing the Bayesian regret of Thompson Sampling (TS) in the context of contextual bandits with binary losses especially under conditions where contexts are selected adversarially. The authors propose a novel concept called the \"lifted information ratio\" which focuses on the unknown model parameter rather than on the optimal action. This adjustment enables them to bound the regret in terms of the entropy of the prior distribution facilitating a relatively straightforward proof without the need for structural assumptions on the likelihood or the prior. This methodological shift is particularly potent in situations with infinite entropy priors requiring only a Lipschitz assumption on the loglikelihood. The paper showcases this methodologys utility by deriving regret bounds for logistic bandits which are shown to be independent of the smallest slope of the sigmoid link function\u2014a notable improvement over existing literature.  Strengths: 1. Novel Theoretical Framework: The papers main strength lies in its innovative use of the \"lifted information ratio\" for analyzing TS. This approach marks a significant departure from prior methods offering a more general framework applicable to a broader range of settings particularly in noni.i.d contextual environments. 2. Comprehensive Bound Analysis: The authors derive regret bounds under minimal assumptions addressing both finite and infinite entropy priors. The bounds for logistic bandits which do not depend on the sigmoids smallest slope represent a substantial theoretical advancement. 3. Solid Theoretical Contributions: By extending the analysis of TS to more complex and realistic settings without relying heavily on restrictive assumptions the paper makes a significant contribution to the field of sequential decisionmaking and online learning.  Weaknesses: 1. Assumption Clarity: While the paper makes an effort to generalize the settings under which Thompson Sampling can be analyzed the specifics of the assumptions (e.g. the Lipchitzness of the loglikelihood) could benefit from a more detailed exploration of their implications and limitations in practical scenarios. 2. Empirical Validation: The paper primarily focuses on the theoretical analysis with limited empirical demonstration of the derived bounds. Including simulations or realworld data examples that confirm the practical effectiveness of the proposed approach would significantly enhance the papers impact. 3. Complexity of Implementation: The paper does not address the computational aspects of implementing the proposed TS variant particularly the challenges in updating and sampling from the posterior in complex models. This is an essential consideration for practical adoption.  Questions and Suggestions for the Authors: 1. Empirical Evidence: Could the authors provide empirical evidence or simulations that demonstrate the practical utility of the proposed methodology particularly comparing it to existing approaches in diverse contextual bandit settings 2. Assumption Implications: A deeper discussion on the assumptions used especially on the Lipschitz condition of the loglikelihood could improve the paper. How do these assumptions compare to those commonly accepted in the literature and what are their practical implications 3. Computational Concerns: Addressing the computational challenges associated with the proposed approach would be beneficial. Are there efficient algorithms or approximations that can be employed to deal with complex models where exact posterior sampling is infeasible  Limitations and Societal Impact: The paper does a commendable job of advancing the theoretical understanding of Thompson Sampling in adversarially selected contexts. However it stops short of fully addressing the computational challenges inherent in applying the proposed methodology to practical problems which could hinder its adoption in realworld applications. Additionally while the manuscript mentions logistic bandits a discussion on the potential negative societal impacts of misapplying these theoretical advancements is absent. For example in sensitive applications like personalized medicine or financial products the limitations and assumptions should be carefully considered to avoid unintended consequences. Constructively future work could explore these computational challenges in more detail propose efficient approximation algorithms and discuss ethical considerations in the application of such advanced bandit solutions to ensure beneficial societal impact.", "z0M3qHDqH20": "Review of \"HUMUSNet: A Hybrid Unrolled MultiScale Network for Accelerated MRI Reconstruction\" 1. Summary: The paper introduces HUMUSNet an innovative hybrid architecture designed for accelerated MRI reconstruction combining the strengths of convolutional neural networks (CNNs) and Transformer models. By leveraging the implicit bias and efficiency of CNNs together with the powerful representational capabilities of Transformers to capture longrange dependencies the authors propose a solution to counter the limitations of conventional CNNbased approaches in MRI reconstruction tasks. HUMUSNet utilizes a novel multiscale feature extraction strategy that balances the computational and memory demands typically increased by high input image resolution and extensive patch tokenization. The architecture demonstrates superior performance on the fastMRI dataset outperforming existing stateoftheart models. Additionally the paper details the networks effectiveness on two other MRI datasets and underscores its versatility through extensive ablation studies. 2. Assessment: Strengths:  Innovative Approach: The hybrid use of CNNs and Transformers for extracting features at different scales is novel and addresses welldocumented limitations in current methods. The multiscale feature extraction effectively captures both high and lowlevel details which is critical for the complexity of MRI images.  StateoftheArt Performance: Achieving new stateoftheart results on the largest publicly available MRI dataset (fastMRI) is a significant accomplishment. This validates the efficacy of the proposed architecture in realworld settings and showcases the potential of hybrid models in medical imaging tasks.  Comprehensive Evaluation: The extensive experiments including ablation studies demonstrate the robustness of HUMUSNet across different datasets and configurations. This thorough validation strengthens the paper\u2019s claims and provides valuable insights into the architectures components and design choices. Weaknesses:  Compute and Memory Efficiency: While the authors address the high compute and memory costs associated with Transformer models there is limited discussion on the practical implications of deploying such a network in clinical settings where computational resources may be constrained. Further analysis comparing the resource efficiency of HUMUSNet with existing models could enhance the papers relevance for realworld applications.  Generalization to Other Imaging Modalities: The focus is predominantly on knee MRI reconstruction. Additional experiments on diverse anatomical regions or different imaging modalities could provide a more comprehensive understanding of the model\u2019s applicability and generalizability. 3. Questions and Suggestions for the Authors:  Could the authors elaborate on the potential impact of HUMUSNet in clinical workflows especially concerning its computational efficiency in a standard clinical setting  Considering the impressive performance on knee MRI images have the authors explored or plan to explore the application of HUMUSNet to other imaging modalities or anatomical regions Insights into such explorations would be invaluable.  It would be beneficial if the authors could discuss any observed tradeoffs when adjusting the multiscale feature extraction strategy specifically regarding the model\u2019s accuracy and computational demands. 4. Limitations and Societal Impact: The paper lacks an explicit discussion on the limitations related to fixedsize inputs which may restrict the models applicability to varying image dimensions commonly encountered in clinical practice. Addressing this limitation in future work will be crucial for wider adoption. Furthermore while the paper touches on the significant computational resources required by Transformer models a detailed analysis comparing the efficiency of HUMUSNet with existing methods is missing. This is particularly relevant given ongoing discussions in the AI field regarding the environmental impact of computationally intensive models. Regarding potential negative societal impacts the primary concern revolves around the equitable access to and deployment of such advanced technologies in resourcelimited settings. Ensuring that the benefits of advancements like HUMUSNet are universally accessible across diverse healthcare systems is a challenge that requires attention beyond the technical domain. Suggestions for Improvement:  A future revision could include a more detailed exploration of the models computational efficiency and potential environmental impact. This would provide a more rounded evaluation of its feasibility for widespread clinical implementation.  Addressing the fixedsize input limitation in the next iteration of the model could significantly enhance its practical applicability and flexibility.  The authors should consider expanding their research to include a wider variety of imaging modalities and anatomical regions to underscore the models versatility and applicability across medical imaging tasks.", "yjWir-w3gki": "Summary: The paper introduces a theory for modelbased reinforcement learning (RL) that is generalized to arbitrary discount functions diverging from the traditional exponential discounting approach to include hyperbolic discounting and thereby more accurately modeling human decisionmaking behaviors. The novelty of this work lies in its adaptation of the Hamilton\u2013Jacobi\u2013Bellman (HJB) equation to accommodate a general discount function employing a collocation method with deep learning for functional approximation. This approach not only enables the computation of an optimal policy but also facilitates the analysis of human discounting behavior through the inverse reinforcement learning (IRL) framework to infer discount function parameters from decision data. The methodology is validated through simulations on two problem scenarios and the findings suggest its potential for broader applications in understanding and modeling sequential decisionmaking tasks as encountered by humans. Strengths: 1. Theoretical Contribution: By extending the formulation of the HJB equation to consider arbitrary discount functions the paper bridges a critical gap between RL and behavioral economicspsychology in modeling decisionmaking under uncertainty. 2. Technical Robustness: The application of a collocation method coupled with deep learning for solving the HJB equation is innovative demonstrating a sophisticated approach to handling the computational complexity inherent in nonexponential discounting. 3. Empirical Validation: The use of simulated problem scenarios for validation is a strong point as it showcases the methods applicability and its effectiveness in modeling timedependent decisions and preference reversals phenomena commonly observed in human behavior. 4. Broader Implications: The paper not only contributes to the RL field by offering a new modeling framework but also opens avenues for empirical research in psychology and economics by providing a tool for investigating individual and collective decisionmaking processes. Weaknesses: 1. Assumption of Known Model: One significant limitation is the assumption that the model including the reward function and dynamics is known a priori. This assumption may not hold in many realworld scenarios limiting the methods applicability outside controlled simulations. 2. Finite Action Space: The current formulation is confined to scenarios with a finite set of actions which may not capture the continuum of possible decisions available in complex decisionmaking situations. 3. Scalability Concerns: The scalability of the proposed collocation method to larger state spaces or more complex models is uncertain potentially hindering its application to highdimensional problems. 4. Potential for Negative Societal Impact: The authors briefly touch on the societal implications of their work but do not provide a detailed analysis of potential negative impacts such as privacy concerns or misuse of personalized decision models. Questions and Suggestions for Authors: 1. Model Assumption Relaxation: Could the authors discuss potential extensions of their framework to scenarios where the model is partially known or must be inferred from data 2. Handling Continuous Action Spaces: Are there planned extensions of the methodology to accommodate continuous action spaces and if so what approaches are being considered 3. Scalability Enhancement: It would be beneficial if the authors could explore and discuss potential methods for improving the scalability of their solution technique to more complex or higherdimensional problems. 4. Detailed Societal Impact Discussion: I recommend providing a more indepth discussion of the potential negative societal impacts including how risks such as data privacy and model misuse might be mitigated. Limitations and Suggestions for Improvement: While the authors address some limitations (e.g. finite action space and known model assumption) the paper could benefit from a more comprehensive discussion on scalability and the practical feasibility of applying this method to realworld problems. It would be advantageous to explore adaptive or samplingbased methods to extend applicability to larger problem instances. Additionally a more thorough examination of the ethical and societal implications paired with concrete suggestions for responsible use would strengthen the paper.", "mrt90D00aQX": " Review of \"FedSR: Federated Learning with Simple Representation for Enhanced Domain Generalization\"  1. Summary The paper introduces FedSR a novel framework that integrates Domain Generalization (DG) into Federated Learning (FL) to address the challenge of model generalization to unseen target data while still adhering to the decentralized and privacypreserving principles of FL. The authors propose employing an L2norm and a conditional mutual information regularizer on the data representation within a FL setting to encourage the learning of essential information and disregard spurious correlations. Theoretical connections to representation alignment in DG are provided and extensive experiments demonstrate that FedSR outperforms both FL baselines and offers competitive performance against stateoftheart centralized DG methods.  2. Strengths and Weaknesses Strengths:  Novel Approach: The paper tackles the significant challenge of achieving domain generalization in a decentralized and privacypreserving manner. The integration of DG into FL without requiring data sharing or complex implementation is innovative.  Theoretical Foundation: The authors not only propose a novel method but also provide a solid theoretical framework connecting their approach to principles of domaininvariant representation learning. This theoretical backing enhances the credibility and understanding of the proposed method.  Extensive Evaluation: The method is thoroughly evaluated across various datasets demonstrating its efficacy and versatility. The comparison with both FL and centralized DG baselines provides a comprehensive overview of its performance.  Practical Implications: The approach has significant implications for realworld applications especially in scenarios where data privacy is paramount and robust model generalization is required across diverse domains. Weaknesses:  Generalization to Extremely Diverse Unseen Domains: The paper mentions that enforcing invariance among source domains might not generalize well to arbitrary unseen target domains. This limitation is critical and might affect the applicability of FedSR in highly dynamic environments where the target domain deviates significantly from the source domains.  Complexity and Hyperparameter Sensitivity: While the authors mention that their method is not overly sensitive to the number of local optimization iterations the paper does not provide a detailed discussion on the sensitivity to the regularization hyperparameters. Given the reliance on regularization techniques understanding the impact of these parameters and how to tune them effectively is essential for practical deployments.  Comparison to Recent FL Methods: The paper could benefit from including comparisons to more recent FL methods that might have proposed different strategies for handling domain shifts even though not directly focused on DG.  3. Questions and Suggestions  Question on Adaptive Regularization: Have the authors considered adaptive strategies for tuning the regularization strengths during training to better accommodate varying degrees of domain shift across clients  Suggestion for Hyperparameter Discussion: It would be beneficial to include a discussion or supplementary section detailing the sensitivity analysis of the regularization hyperparameters (\u03b1L2R and \u03b1CMI). This analysis could guide practitioners in applying FedSR to new datasets and environments.  Exploration of Complex Target Domains: The authors might consider extending their experiments or future work to include scenarios where the unseen target domain is drastically different from the source domains. This would provide insights into the limits of domain invariance enforced by FedSR.  Comparison with Recent Advances: Including comparisons with the latest FL techniques addressing domain shifts even if not specifically through a DG lens could strengthen the argument for the novelty and effectiveness of FedSR.  4. Limitations and Societal Impact The authors acknowledge a key limitation regarding the potential mismatch between the enforced domain invariance among source domains and an arbitrary unseen target domain. This limitation is crucial for practical applications where the target domain is highly unpredictable. A constructive suggestion would be to explore dynamic or adaptive regularization mechanisms that adjust based on the perceived domain shift or to investigate methodologies for incorporating a broader range of domain variances during training. On the societal impact front while the paper emphasizes the privacypreserving nature of FedSR a discussion on the potential for biases induced by the regularizers or the aggregation method is lacking. For instance how does the method ensure that the model does not inadvertently learn biased representations due to imbalances in domainspecific data Addressing these concerns with clear strategies would further strengthen the paper.  Conclusion Overall the paper presents a compelling and wellvalidated approach to integrating domain generalization into federated learning. Despite the highlighted limitations and areas for further exploration FedSR represents a significant step forward in the quest for privacypreserving domainagnostic learning models. The theoretical contributions paired with extensive experimental validation provide a strong foundation for future research in this critical area of machine learning.", "msBC-W9Elaa": " Review of \"DimensionIndependent Generalization Bounds for SGD with Localized Covering\"  Summary: The paper addresses the fundamental issue of deriving dimensionindependent generalization bounds in the context of stochastic gradient descent (SGD). It introduces a novel localized covering technique tailored to the trajectories of SGD departing from the classical uniform covering approaches that suffer from exponential dependency on dimensionality. This enables the authors to establish generalization error bounds that are notably independent of the ambient dimension for a broad class of nonconvex loss functions. Specifically it shows that if the objective function is a finite perturbation of a piecewise strongly convex and smooth function with \\(P\\) pieces the generalization error can be upper bounded by \\(O\\left( \\sqrt\\log n \\log(nP)n \\right)\\) where \\(n\\) is the sample size. Utilizing this framework the paper improves upon existing generalization bounds for several machine learning models including multiindex linear models multiclass support vector machines and Kmeans clustering under both hard and soft label setups.  Strengths: 1. Novelty and Technical Depth: The approach of employing a localized covering technique is innovative and represents a significant technical contribution. The paper adeptly moves beyond the limitations of classical uniform covering methods and algorithmic stabilitybased bounds offering a fresh perspective on controlling generalization error in high dimensions. 2. Broad Applicability: The theoretical results are applied to various statistical models showcasing the utility of the proposed method. This wide applicability demonstrates the potential impact of the study on different areas within machine learning. 3. Clear Improvement Over StateoftheArt: The paper clearly outlines how it advances the current knowledge by providing dimensionindependent generalization bounds that do not require assumptions like early stopping or decaying step size which are often needed in other approaches.  Weaknesses: 1. Realworld Applicability: While the theoretical contributions are substantial the paper could benefit from a more detailed discussion on the practical implications of the findings such as how these bounds might influence actual algorithm design or empirical performance on benchmark datasets. 2. Assumption Transparency: The assumptions required for the main results such as weak Lipschitz continuity and bounded deviation are somewhat buried in the text. A clearer upfront summary of these assumptions and their implications could aid readability and the subsequent application of these results. 3. Limited Discussion on Limitations and Societal Impact: The paper briefly mentions limitations and potential extensions however a more thorough examination of the limitations and any possible negative societal impacts especially in the context of deploying these models in realworld scenarios would have been valuable.  Questions and Suggestions for the Authors: 1. Empirical Validation: Can the authors provide any empirical studies or simulations to corroborate the theoretical improvements suggested by their generalized bounds Such evidence could strengthen the papers impact. 2. Clarity on Assumptions: Could the authors elaborate on the necessity and implications of the assumptions made particularly the weak Lipschitz continuity and bounded deviation How restrictive are these assumptions in practical scenarios 3. Societal Impact Discussion: The authors could expand the discussion on the limitations and societal impact particularly in relation to model deployment. Are there specific areas or applications where the proposed methods limitations could pose challenges 4. Extension to Varying Step Size Schedules: Given the practical importance of varying step sizes in SGD implementations any insights or preliminary results on extending the current framework to accommodate such variations would be invaluable.  Limitations and Societal Impact: The authors have outlined some limitations and areas for future work including the extension to varying step size schedules and the exploration of the practical applicability of the results. Nonetheless a more nuanced discussion on these limitations especially in terms of practical deployment challenges and any potential for misuse or unintended consequences in sensitive applications would enhance the paper. Constructive suggestions might include recommendations for empirical validation strategies as well as caution regarding the interpretation and application of the theoretical bounds in settings with ethical or societal implications.", "lKULHf7oFDo": "Summary of the paper: The paper addresses fairness in federated learning (FL) particularly focusing on challenges posed by nonIID (independently and identically distributed) data across local agents. Leveraging concepts from cooperative game theory and social choice theory the authors propose a fairness criterion termed \"corestable fairness.\" This criterion ensures that no subset of agents can significantly benefit by forming a coalition thus encouraging collective optimization without the detriment to individual agents due to poor data quality in others. To achieve this fairness the paper introduces an efficient federated learning protocol CoreFed which can determine corestable predictors for convex loss functions and approximate corestable predictors for nonconvex scenarios like deep neural networks. The efficacy of CoreFed and its fairness measures were empirically evaluated on two realworld datasets showing it achieves better corestable fairness than the FedAvg protocol while maintaining comparable accuracy. Assessment of Strengths: 1. Innovative Approach to Fairness: The paper introduces a novel approach to ensuring fairness in federated learning by integrating principles from cooperative game theory and social choice theory which is a significant theoretical contribution. 2. Comprehensiveness: The authors provide a thorough theoretical foundation for corestable fairness including existence proofs and a demonstration of how core stability can be achieved or approximated in practical settings. 3. Empirical Validation: The effectiveness of the CoreFed protocol is empirically validated on realworld datasets. This practical validation is essential for establishing the feasibility of the theoretical contributions. Weaknesses: 1. Scalability Concerns: While the paper provides a strong theoretical foundation and initial empirical validation it does not extensively discuss the scalability of CoreFed especially in very large federated networks with complex models. 2. Limitations on NonConvex Functions: The paper acknowledges challenges in achieving core stability with nonconvex loss functions relying on approximations for scenarios like deep neural networks. This limitation might constrain the applicability of CoreFed in some advanced ML tasks. 3. Analysis of Communication Overheads: Federated learning protocols can be significantly impacted by communication costs but the paper does not thoroughly analyze the communication overhead associated with CoreFed which could be a potential barrier to its adoption. Questions and Suggestions for Authors: 1. Scalability Analyzes: Can the authors provide more insights into the scalability of CoreFed perhaps through simulations or additional experiments involving larger networks and datasets 2. Handling NonConvex Functions: While the paper discusses approximate corestable predictors for nonconvex loss functions further details on the limitations and potential impacts on fairness would be beneficial. Could more sophisticated approximation methods improve outcomes here 3. Communication Efficiency: Considering the often resourceconstrained nature of federated learning environments can the authors elaborate on the communication efficiency of CoreFed Exploring adaptive communication strategies could be a valuable area for further research. 4. Comparison with Other Fairness Approaches: The paper could benefit from a deeper comparative analysis with other fairness approaches in federated learning providing a clearer picture of the tradeoffs involved. Limitations and Negative Societal Impact: The paper points to the limitations associated with nonconvex functions and briefly mentions scalability. However a thorough discussion on the limitations regarding broader applicability across various federated learning settings and potential negative societal impacts is missing. Given that fairness is a multifaceted concept varying interpretations and implementations could inadvertently introduce biases or unfair outcomes in certain scenarios. It would be constructive for future work to explore these aspects along with strategies for mitigating any potential negative societal impacts such as reinforcing existing inequalities.", "mjVmifxpKqS": " Peer Review  1) Summary of the Paper This paper addresses the challenge of episodic reinforcement learning (RL) under the constraints of finitehorizon Markov Decision Processes (MDPs) with a limited number of batch updates. The authors propose a novel multibatch reinforcement learning framework designed to minimize the frequency of policy updates a critical concern in scenarios demanding high parallelism or where policy updates are costly. The main contribution is a computationally efficient algorithm that achieves an O\\xcc\\x83(SAH3K) regret bound with O(H  log2 log2(K)) batch complexity where S is the state space size A is the action space size H is the planning horizon and K is the number of episodes. This is claimed to be the first such regret bound achieved with this level of batch complexity. The authors prove both upper and lower bounds for batch complexity showing that their results are nearly optimal up to logarithmic terms.  2) Assessment of Strengths and Weaknesses Strengths:  Theoretical Contribution: The paper introduces significant advancements in the multibatch RL domain by providing the first algorithm to achieve a nearoptimal regret bound with remarkably low batch complexity. The theoretical analysis is thorough and grounded in a solid mathematical framework.  Practical Relevance: The motivation for reducing the number of policy updates is compelling especially for applications where updates are expensive or require high levels of parallelism. This work has the potential to impact various fields including medicine hardware placement and database optimization.  Technical Depth: The technical innovations including a nearoptimal design scheme and an efficient computational method for policy exploration demonstrate deep insights into the challenges of episodic RL. Weaknesses:  Empirical Validation: The paper is primarily theoretical and does not include empirical results to validate the performance of the proposed algorithm in realworld scenarios or benchmarks.  Comparison with Existing Methods: While the theoretical improvements are clear the paper lacks a direct comparison of its approach with existing methods in terms of practical performance metrics beyond the regret and batch complexity.  Broader Impact Discussion: The authors briefly mention that the broader impact is not applicable which is a missed opportunity to explore potential negative consequences or ethical considerations in applications of their work.  3) Questions and Suggestions for the Authors  Empirical Evaluation: Can the authors provide empirical evidence or simulations to support the theoretical advantages of their algorithm Additionally it would be beneficial to see comparisons with stateoftheart RL algorithms in terms of both performance and practical implementation issues.  Implementation Details: More guidance on the practical implementation of the proposed algorithm including parameter selection and computational requirements would increase the accessibility of this work for practitioners.  Extension to Function Approximation: The authors mention the potential extension to reinforcement learning with function approximation as an interesting future direction. Can they provide preliminary insights or hypotheses on how the current approach might adapt to or perform in such settings  4) Limitations and Potential Negative Societal Impact The authors have not fully addressed the potential limitations and societal impacts of their work. While the research is primarily theoretical its crucial to acknowledge that algorithms influencing decisionmaking processes particularly in sensitive areas like medical or legal systems may have significant consequences. Suggestions for Improvement:  Discuss Ethical Implications: A more detailed discussion on the ethical implications of deploying such RL algorithms considering the possibility of unintended bias or errors especially in highstakes environments.  Limitations in Practical Deployment: The authors should elucidate on potential limitations in realworld applications such as the robustness of the algorithm to model misspecifications or the challenges in hyperparameter tuning. Overall the paper presents a significant theoretical advancement in multibatch reinforcement learning with applications across various domains. However empirical evidence demonstrating these theoretical strengths and a thorough discussion of potential broader impacts would make this work more comprehensive and beneficial to the field.", "xnuN2vGmZA0": "Peer Review of \"Video Instance Segmentation via Object Token Association (VITA)\" 1) Brief Summary: The paper introduces a novel approach named VITA for offline Video Instance Segmentation (VIS) where the main contribution is leveraging explicit objectoriented information for improved contextual understanding of video sequences. Unlike previous methods that heavily rely on dense spatiotemporal features VITA builds video context through associations of framelevel object tokens generated by an image object detector. The method circumvents the need for heavy computation by avoiding spatiotemporal backbone features for object tracking across frames thus enabling processing of long highresolution videos on standard hardware. The authors present stateoftheart results on benchmark datasets such as YouTubeVIS 2019  2021 and OVIS showcasing the effectiveness of their approach. Moreover VITA\u2019s design provides several practical benefits including handling extended video lengths and maintaining performance with a frozen framelevel detector. The paper also includes comprehensive ablation studies to validate the effectiveness of the proposed components of VITA. 2) Assessment: Strengths: 1. Novel Approach: VITA introduces a refreshing perspective by utilizing object tokens for video instance segmentation focusing on objectcentered contexts rather than dense spatiotemporal features. This methodology not only improves accuracy but also computational efficiency for longer sequences. 2. StateoftheArt Performance: The authors demonstrate outstanding results across multiple benchmarks outperforming existing methods significantly. The inclusion of challenging datasets and validation through extensive experiments add to the credibility of the proposed method. 3. Practical Advantages: Addressing a crucial gap in offline VIS VITA shows capabilities for processing long videos overcoming memory limitations encountered by previous methods. The feasibility of using a frozen framelevel detector without substantial performance loss is particularly noteworthy for applications with resource constraints. 4. Comprehensive Validation: The paper provides a detailed experimental section including ablation studies that validate the contribution of individual components. Such thorough examination is commendable and strengthens the papers claims. Weaknesses: 1. Complexity and Generalizability: While VITA shows superior performance on benchmark datasets the complexity of the proposed method might pose challenges in generalizing to a broader range of realworld scenarios. The dependence on pretrained object detectors and the methods performance with unconventional or rare objects remain to be explored. 2. Limited Exploration of Temporal Dynamics: The paper primarily enhances spatial understanding through object tokens. However temporal dynamics especially for detecting complex behaviors in videos seem underexplored. A more explicit focus on temporal information could potentially improve performance further. 3. Resource Efficiency: Although VITA offers practical benefits for handling longer sequences the resource efficiency (in terms of computational cost and memory usage) comparison against existing methods could be elaborated further. The tradeoff between efficiency and accuracy especially when scaling to very highresolution videos warrants more discussion. 3) Questions and Suggestions for the Authors: 1. Generalization Capability: Can the authors comment on VITA\u2019s performance and robustness in scenarios with rare or unconventional objects not covered by the pretrained image object detectors 2. Temporal Dynamics Handling: Would the incorporation of explicit temporal features enhance model performance especially for tracking objects with complex behaviors Is there any plan to explore this direction 3. Resource Efficiency Metrics: It would be beneficial to include more detailed comparisons regarding computational cost and memory usage against benchmark methods particularly when processing highresolution videos. 4) Limitations and Societal Impact: The authors acknowledge limitations in terms of processing capacity for an infinite number of frames and handling complex behaviors over long sequences. Addressing these will be critical for advancing VIS research and applications. Though not extensively discussed the societal impact of enabling more efficient and accurate video understanding tools could be profound especially in areas like surveillance content moderation and autonomous navigation. However these applications also underscore the importance of considering privacy and ethical use as part of future work. Constructive steps towards mitigating potential negative impacts would further strengthen the paper.", "xNeAhc2CNAl": "Review of the Paper \"Extreme Compression of BERT Models via UltraLow Bit Precision Quantization\" Summary: The paper addresses the challenge of deploying large NLP models particularly Transformerbased architectures like BERT on resourceconstrained devices. The authors focus on extreme compression methods emphasizing ultralow bit precision (binaryternary) quantization. Conventional approaches to such aggressive compression have involved complex pipelines featuring multistage knowledge distillation and hyperparameter tuning often neglecting smaller models already compressed through distillation. The paper contributes by systematically evaluating the impact of various hyperparameters and training strategies on extreme quantization efficacy. The authors highlight that previous studies have undertrained such quantized models leading to unnecessary complexity. Through comprehensive testing they propose a simpler more effective compression method XTC which combines extreme quantization and layer reduction to achieve significant size reduction (by 50x) and superior performance on GLUE tasks compared to existing methods. Assessment: Strengths: 1. Systematic Evaluation: The paper stands out for its systematic and comprehensive analysis of previous extreme compression methods revealing that prior approaches were significantly undertrained. 2. Innovative Compression Pipeline (XTC): The introduction of XTC is noteworthy. It simplifies the compression process by eliminating the computationally expensive pretraining knowledge distillation while maintaining or even improving model performance. 3. Empirical Results: The empirical results demonstrating superior performance on GLUE benchmarks with 50x reduction in model size present a strong case for the methods effectiveness. Weaknesses: 1. Generalization Beyond BERT: The study is focused exclusively on BERT models. It remains unclear how the proposed method would perform on other large Transformer models such as GPT3. 2. Potential Overfitting Concerns: With extreme distillation and quantization there is a potential risk of overfitting the model to specific tasks or benchmarks. The paper could benefit from a discussion on how robust XTC is across diverse and unseen datasets. 3. Hardware Compatibility and Inference Time: While the paper highlights the compression effectiveness and accuracy improvements theres limited discussion on the compatibility with various hardware platforms and the impact on inference time which are critical for resourceconstrained devices. Questions and Suggestions for the Authors: 1. Extending Beyond BERT: Can the authors discuss or speculate on the applicability of XTC to other Transformer models like GPT series or RoBERTa 2. Robustness Analysis: It would be beneficial to include an analysis of the models performance on outofdistribution data or unseen tasks to assess the robustness of the compressed models. 3. Hardware Compatibility: Can the authors provide more insights into the compatibility of the compressed models with different hardware platforms especially lowpower devices and the implications for inference time and energy efficiency Limitations and Societal Impact: The paper briefly mentions potential limitations in the generalization of findings beyond BERT models and highlights the methodological innovation in addressing extreme model compression. However a more detailed discussion on the limitations related to hardware compatibility potential biases in heavily compressed models and the broader societal implications of deploying such models in realworld applications would enhance the paper. A constructive suggestion would be to explore the energy efficiency and environmental impact of deploying these compressed models considering the growing concerns around the carbon footprint of training and deploying largescale AI models. In conclusion the paper represents a significant step forward in the field of model compression for NLP providing a novel method that simplifies the quantization process and achieves remarkable performance gains. Addressing the highlighted weaknesses and considering the suggested improvements would further strengthen the work.", "xijYyYFlRIf": "Review of \"GAUDI: Generative Modeling of 3D Scenes with Disentangled Radiance Fields and Camera Poses\" 1. Summary This paper introduces GAUDI a sophisticated generative model designed for the complex task of creating and rendering highly realistic 3D scenes from diverse camera perspectives. The approach is distinguished by its focus on scalability and its ability to disentangle the modeling of radiance fields (essentially the visual content of a scene) from camera poses (the viewing angles from which the scene is observed). The authors present a twostage process wherein they first optimize for latent representations of these elements using a novel denoising optimization objective. Subsequently they train a generative model on these latent representations to produce new scenes either conditionally\u2014based on sparse images or textual descriptions\u2014or unconditionally. The methodology diverges from previous work by providing a solution that avoids common training issues like mode collapse and it moves beyond the limitation of assuming shared camera pose distributions across different scenes. The validation of GAUDI across multiple datasets showcases its ability to outperform existing models in generating 3D scenes with high fidelity. 2. Assessment of Strengths and Weaknesses Strengths: a. Innovation in Disentanglement: GAUDI\u2019s approach to disentangle radiance fields and camera poses in the latent space is both novel and impactful addressing a crucial complexity in 3D scene rendering that many previous works have not. b. Scalability and Performance: The model demonstrates exceptional scalability being capable of handling thousands of indoor scenes and generalizing across diverse datasets. The achieved stateoftheart performance in both unconditional and conditional generation settings is commendable. c. Versatility: The ability of GAUDI to operate under conditional settings guided by sparse images or textual descriptions opens up new avenues for 3D content creation and machine learning applications requiring scene understanding and generation. Weaknesses: a. Computational Requirements: While scalability is a strength the paper does not fully address the computational demands of the technique particularly in terms of memory and processing power which could limit accessibility. b. Limited Evaluation on Societal Impacts: The paper could benefit from a more thorough discussion on potential negative societal impacts especially concerning the generation of hyperrealistic scenes that could be misused in misinformation campaigns. c. Transparency in Model Limitations: Though GAUDI shows promising results the paper could be improved by discussing more openly the limitations of the model for instance in scenarios where available conditioning information is extremely sparse or ambiguous. 3. Questions and Suggestions for the Authors a. Generalization: Could the authors elaborate on how GAUDI might perform on outdoor scenes or scenes with significant variation in lighting and weather conditions b. Realism in Conditional Generation: In conditional generation especially with textual descriptions how does GAUDI handle abstract or nonvisual text cues A discussion on this would be insightful. c. Adaptability: Is there flexibility in GAUDI to incorporate future advancements in 3D scene understanding and representation such as dynamic elements or increasingly complex interactions within the rendered environment 4. Limitations and Societal Impact The authors have presented a detailed and technically sound paper with a clear outline of its contributions. However while the work is pioneering a deeper engagement with the potential for negative societal impacts such as misuse in generating deceptive visual content would enhance the paper. Furthermore considerations around the diversity and ethical sourcing of the training data could ensure the responsible development of similar technologies in the future. Constructive Suggestions for Improvement:  The authors could expand the discussion on computational efficiency and potential strategies for reducing the resource demands of the GAUDI model.  Exploring and elucidating the limitations of the model in handling extremely varied or complex 3D scenes would provide a more balanced view.  A dedicated section on ethical considerations and mitigation strategies for potential misuse emphasizing responsible use and development would be valuable. In conclusion GAUDI represents a significant step forward in the generative modeling of 3D scenes. Addressing the highlighted concerns and suggestions could further solidify its contributions to the field.", "lzZstLVGVGW": " Review of \"Earthformer: A SpaceTime Transformer for Earth System Forecasting\"  1) Summary of the Paper This paper introduces Earthformer a Transformerbased model aimed at forecasting Earth system variabilities such as weather and climaterelated phenomena. Traditional approaches rely heavily on numerical simulation models which are computationally expensive and require substantial domain expertise. Earthformer leverages a deep learning framework more specifically a spacetime Transformer architecture incorporating a novel Cuboid Attention mechanism for efficient processing of highdimensional spatiotemporal Earth observation data. The model decomposes the input data into cuboids and applies selfattention in parallel enhancing computational efficiency. Global vectors are introduced to facilitate information sharing among cuboids capturing the overall system dynamics. The paper evaluates Earthformer on both synthetic (MovingMNIST and Nbody MNIST) and realworld datasets (SEVIR for precipitation nowcasting and ICARENSO for sea surface temperature anomalies forecasting) demonstrating stateoftheart performance in Earth system forecasting tasks.  2) Assessment of Strengths and Weaknesses Strengths:  Novelty: Earthformer introduces an innovative attention mechanism Cuboid Attention alongside global vectors to efficiently handle spatiotemporal sequences which is a significant advancement for deep learning applications in Earth system forecasting.  Performance: The model outperforms existing stateoftheart methods on both synthetic and realworld benchmarks demonstrating its effectiveness in forecasting complex Earth system phenomena.  Flexibility: The adoption of a hierarchical encoderdecoder architecture and the exploration of different cuboid attention patterns reflect the models flexibility which can cater to various forecasting tasks with different spatiotemporal dynamics. Weaknesses:  Deterministic Nature: The model is deterministic and does not account for the inherent uncertainty in Earth system forecasting. This limitation can lead to predictions that lack smallscale details and may affect decisionmaking processes based on the forecasts.  Lack of Physical Constraints: Being purely datadriven Earthformer does not incorporate physical knowledge of the Earth system potentially missing out on valuable insights that could improve forecasting accuracy and reliability.  Computational Resource Requirements: Despite the efficiency improvements with Cuboid Attention the training and operation of such a complex Transformerbased model may still demand significant computational resources potentially limiting its accessibility and scalability.  3) Questions and Suggestions for the Authors  Modeling Uncertainty: Can the authors discuss potential approaches or modifications to Earthformer that could model uncertainty Including probabilistic forecasting methods might improve the models utility for realworld decisionmaking.  Incorporating Physical Knowledge: How might Earthformer be adapted to incorporate physical laws or constraints directly into its architecture Exploring the integration of datadriven and physicsbased models could enhance forecasting performance and interpretability.  Resource Efficiency: Are there further optimizations possible to reduce the computational demands of Earthformer particularly for deployment in resourceconstrained environments  Longterm Forecasting: How does Earthformer perform on longterm forecasting tasks beyond the temporal scope of the datasets used in the experiments Insights into its scalability in time would be valuable for applications requiring extended forecasts.  4) Limitations and Potential Negative Societal Impact The authors have acknowledged the limitations related to the deterministic modeling approach and the lack of physical constraints in Earthformer. Another limitation is the potential computational expense of deploying such models which could limit usage in areas with limited resources. To mitigate these limitations the authors could explore introduction of probabilistic elements into Earthformer to capture uncertainty better and research ways to integrate physical knowledge for more grounded forecasts. Additionally optimizing the model to reduce computational requirements can make it more accessible. The potential negative societal impact could arise from reliance on inaccurate forecasts for critical decisionmaking especially in disaster management and agriculture. Future work should aim at improving the models robustness and reliability to minimize such risks. An ethical consideration is also ensuring equitable access to the technology across different regions preventing exacerbation of the digital divide.", "tX_dIvk4j-s": "Peer Review: 1) Summary: This paper introduces VisCo Grids a novel approach for surface reconstruction from sparse point clouds which leverages gridbased implicit representations enhanced by two innovative geometric priors namely Viscosity and Coarea. Unlike Implicit Neural Representations (INRs) which utilize neural networks for surface reconstruction and suffer from issues such as uncontrolled inductive biases costly inference and slow training VisCo Grids employs simple grid functions that promise instant inference improved training speed and comparable accuracy. The viscosity prior aims at replacing the smoothness bias in INRs by ensuring smooth Signed Distance Function (SDF) solutions while the coarea prior focuses on reducing the surface area of the reconstruction encouraging more efficient and minimal surface solutions. The paper presents extensive experimental validation demonstrating that VisCo Grids achieve comparable or superior performance to stateoftheart INR methods on standard surface reconstruction benchmarks. 2) Strengths and Weaknesses: Strengths: a) Novelty: The introduction of gridbased surface reconstruction using the Viscosity and Coarea priors is a novel contribution that addresses significant shortcomings in the current INRbased methods. b) Performance: The experiments demonstrate that VisCo Grids can achieve comparable results to INR methods but with the advantage of instant inference and faster training which is quite significant. c) Broad Evaluation: The paper conducts a comprehensive evaluation including ablation studies that effectively demonstrate the impact of each proposed component. Weaknesses: a) Scalability: The main limitation associated with grid methods including VisCo Grids is their scalability concerning resolution. As the resolution increases the computational cost and memory requirements could potentially become prohibitive which is not adequately discussed in the paper. b) Lack of Theoretical Insight: While the empirical results are strong the paper lacks a theoretical analysis of the proposed geometric priors. Insights into how and why these priors work could enhance the understanding and future development of such techniques. c) Comparison with Advanced INR Methods: The comparisons are robust but including more recent and possibly more advanced INR methods could strengthen the claim of VisCo Grids\u2019 performance superiority. 3) Questions and Suggestions: a) How do VisCo Grids scale with increasing resolution It would be beneficial if the authors could discuss or provide some insights into how their method scales especially since gridbased methods are known to face scalability issues. b) Can the authors provide more theoretical insights or intuitions on why the viscosity and coarea priors effectively produce highquality surface reconstructions Adding this could enrich the papers contribution. c) Have the authors considered integrating VisCo Grids with neural network components to potentially overcome the limitations of purely gridbased methods This hybrid approach could address scalability issues and allow for more flexibility in surface reconstruction. d) Can the method handle topological changes smoothly and if so how does the performance compare to INR methods in such scenarios Examples or discussion on this topic would be valuable. 4) Limitations and Societal Impact: The paper briefly mentions that all grid methods including VisCo Grids inherently have a limitation in terms of predefined degrees of freedom which could restrict the methods adaptability and detail capture ability compared to neural networkbased methods. However this limitation is not deeply explored especially in the context of scalability and high resolution. The authors are encouraged to discuss potential approaches or future work directions that could help mitigate this limitation. Regarding societal impact the authors state that they do not foresee immediate negative implications mentioning that any highquality 3D reconstruction technology could be misused in malicious settings. It would strengthen the paper if the authors could elaborate on specific misuse scenarios and suggest guidelines or measures that could help mitigate such risks ensuring responsible use of their technology. Overall the paper presents a significant contribution to the field of surface reconstruction offering an alternative to INRbased methods. However addressing the mentioned weaknesses and questions could further elevate the works impact and applicability.", "oDRQGo8I7P": " Review of \"Riemannian ScoreBased Generative Models (RSGMs)\"  1) Brief Summary The paper introduces Riemannian ScoreBased Generative Models (RSGMs) which extend scorebased generative modeling from Euclidean spaces to Riemannian manifolds. The work adapts the \"noising\" and \"denoising\" processes of traditional SGMs to the geometry of manifolds leveraging Geodesic Random Walks for sampling and proposing new techniques for score estimation. The authors provide theoretical convergence bounds for their approach on compact manifolds and demonstrate its efficacy on various datasets including those related to earth and climate sciences showcasing improvements over recent baselines and scalability to highdimensional manifolds.  2) Assessment of Strengths and Weaknesses Strengths:  The work addresses a significant gap in generative modeling by extending SGMs to Riemannian manifolds which is pertinent for data naturally lying on nonEuclidean geometries.  Theoretical contributions including the derivation of a timereversal formula on manifolds and convergence analysis for compact manifolds provide a solid mathematical foundation for RSGMs.  The empirical evaluation is thorough covering a variety of manifolds and showcasing the models versatility across different types of data and its superior performance over several baselines.  Introduction of Geodesic Random Walks for sampling and novel score estimation techniques are significant technical contributions that could influence future research in manifoldbased generative modeling. Weaknesses:  The discussion on the limitations and potential negative societal impacts of the work is absent leaving these critical aspects unaddressed.  Some technical details notably around the approximation techniques for highdimensional manifolds and the handling of manifolds with boundaries could be further elaborated.  While the paper demonstrates improvements over baselines and scalability the computational efficiency particularly in terms of time complexity and resource requirements is not discussed in depth.  3) Questions and Suggestions for Authors  Question 1: Can the authors provide more insights into how RSGMs can be adapted or extended to handle manifolds with boundaries or noncompact manifolds in more detail  Question 2: Was computational efficiency (e.g. training time and resource usage) compared to baselines and if so how do RSGMs perform in this aspect  Suggestion 1: It would be beneficial if the authors could discuss potential negative societal impacts considering the increasing importance of ethical AI research.  Suggestion 2: Expanding on the technical details and challenges encountered when approximating score functions on highdimensional manifolds could enrich the paper.  4) Limitations and Societal Impact The paper does not discuss its limitations and potential negative societal impacts. While theoretical and empirical contributions are solid incorporating discussions on computational efficiency adaptability to manifolds with complex geometries (e.g. with boundaries) and potential misuse scenarios would provide a more holistic view. I suggest the authors add a dedicated section discussing these aspects offering a balance between the advancements introduced and the challenges or risks they might entail. Overall the paper represents a significant step forward in the field of generative modeling opening up new avenues for research and application in domains requiring the modeling of complex manifoldvalued data.", "zz0FC7qBpkh": "Peer Review for \"Mirror Reflected Invariant Risk Minimization (MRI): A New Approach for Learning Invariant Representations\" 1) Summary of the Paper: This paper critically examines the limitations of the Invariant Risk Minimization (IRM) technique for learning invariant features across different environments particularly its failure in certain scenarios due to a fundamental design flaw in its formulation. The authors introduce an alternative approach termed Mirror Reflected Invariant Risk Minimization (MRI) which addresses this flaw by conserving the labelconditioned feature expectation across environments. They develop a practical version MRIv1 and provide theoretical proofs showing that MRIv1 can guarantee invariant predictors for general linear problems given sufficient environments. They also empirically demonstrate MRIv1s superiority over IRMv1 through superior outofdistribution (OOD) generalization performance in both linear and nonlinear imagebased problem settings. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Concept: The introduction of MRI as a remedy to IRMs fundamental design flaw is novel and addresses a significant gap in learning invariant representations across varying environments.  Comprehensive Theoretical Analysis: The paper provides rigorous theoretical proofs showing that MRIv1 can successfully eliminate spurious correlations in linear problems given a sufficient number of environments.  Empirical Validation: Extensive empirical evaluations showcase MRIv1s outperformance of IRMv1 in achieving nearoptimal OOD generalization in various domains demonstrating the practical applicability of the proposed method.  Potential for Wide Impact: This research has the potential to significantly impact the design of machine learning models aimed at domain generalization making it relevant for a wide range of applications. Weaknesses:  Limited Evaluation on Benchmark Datasets: While the authors provide empirical evidence of MRIv1s effectiveness evaluations on more diverse and challenging benchmark datasets could further validate its generalizability.  Assumption on Environment Overlap: The assumption that significant support overlap is needed across domains for MRI and IRMs effectiveness may limit their applicability in realworld scenarios where such overlap is minimal or nonexistent.  Technical Accessibility: The high technical depth while necessary for rigor might make the paper less accessible to a broader audience or practitioners not deeply versed in the theoretical aspects of domain generalization. 3) Questions and Suggestions for the Authors:  Clarification on Environment Assumptions: Could the authors elaborate on the feasibility of achieving significant support overlap across domains in practical scenarios Are there known strategies or preprocessing steps that can ensure this condition is met  Evaluation on Diverse Datasets: It would be compelling to see additional evaluations on a wider array of datasets including realworld datasets that exhibit high variability and fewer environmental overlaps to further assess MRIv1s robustness and practical utility.  Accessibility Improvements: Consider including a more detailed explanation or a simplified overview of the key theoretical concepts for readers less familiar with invariant risk minimization or the specific mathematical formulations. This could enhance the papers accessibility and its impact on the broader machine learning community. 4) Limitations and Societal Impacts: The paper briefly mentions the requirement for significant support overlap across domains as a common limitation of both MRI and IRM. While this is an important start a more thorough discussion on this limitations implications on practical applicability would be beneficial. Additionally the potential negative societal impacts of the proposed model especially in failure modes or biases introduced through spurious correlations in realworld applications are not discussed. A section dedicated to exploring these aspects including possible mitigation strategies would not only strengthen the paper but also contribute to the responsible advancement of machine learning techniques. Constructive suggestions include incorporating fairness and bias evaluations in the empirical studies and exploring the adaptability of MRI in scenarios with minimal crossdomain overlap or high environmental variability.", "m_JSC3r9td7": "The paper introduces DeepMed a novel method utilizing deep neural networks (DNNs) for causal mediation analysis specifically targeting the estimation of Natural Direct and Indirect Effects (NDENIE). By leveraging the multiplyrobust property of the efficient influence function within a semiparametric framework DeepMed addresses the estimation of NDENIE without necessitating sparsity constraints on the DNN architecture. The authors provide both theoretical advancements and empirical evidence to demonstrate the efficacy of DeepMed conducting synthetic experiments and analyzing real datasets concerning machine learning fairness to show its practical applicability. Additionally the paper contributes an R package to facilitate the replication and further application of their method. Strengths: 1. Theoretical Contributions: The paper presents novel theoretical results asserting that DeepMed can achieve the semiparametric efficiency bound in estimating NDENIE without relying on sparse DNN architectures and it can adapt to lowdimensional structures in the nuisance functions. These are significant advancements over the current literature highlighting DeepMeds ability to avoid the curseofdimensionality which is common in nonsparse DNNbased causal inference models. 2. Empirical Validation: Through sophisticated synthetic experiments simulating nearly infinitedimensional functions and the reanalysis of two realworld datasets on algorithmic fairness the paper thoroughly demonstrates the practical effectiveness and versatility of DeepMed. The experimental design and its results are compelling establishing DeepMeds advantages over existing methods in terms of bias reduction and handling complex data structures. 3. Reproducibility and Accessibility: By providing a userfriendly R package the authors significantly enhance the accessibility and reproducibility of their method aligning with current scientific norms for transparency and open science. Weaknesses: 1. Assumption Transparency and Robustness: While the paper outlines the assumptions necessary for DeepMeds theoretical properties there is a lack of emphasis on how sensitive DeepMeds performance is to violations of these assumptions. Additionally the paper could benefit from a more thorough discussion on the practical implications and identifiability concerns arising from these assumptions in realworld applications. 2. Comparison with StateoftheArt Methods: The empirical analysis primarily demonstrates DeepMeds superiority over traditional methods. However a detailed comparison with the latest stateoftheart methods in causal inference possibly incorporating other advanced machine learning techniques could provide a clearer picture of DeepMeds standing in the current research landscape. 3. Handling of Unmeasured Confounding: The discussion on unmeasured confounding is deferred to the supplementary material which may understate the importance of this issue in causal inference. Including strategies within DeepMed or recommendations for addressing unmeasured confounding in the main text could improve the papers practical relevance. Questions and Suggestions for the Authors: 1. Sensitivity Analysis: Could the authors provide a sensitivity analysis regarding the robustness of DeepMeds performance to violations of the assumed conditions especially the ignorable treatment assignment 2. Broader Comparison: How does DeepMed compare with the latest methods in causal inference that might use similar or different advanced statistical or machine learning techniques Including such a comparison could highlight DeepMeds unique contributions more vividly. 3. Extension to Unmeasured Confounding: While acknowledging that handling unmeasured confounding is complex could the authors suggest extensions of DeepMed or auxiliary methods that practitioners could use to tackle this challenge Limitations and Societal Impact: While the authors acknowledge the potential negative societal impact of misapplied causal inference methods and call for careful assumption checking a more detailed discussion on specific scenarios where DeepMed might inadvertently contribute to bias or unfairness would be beneficial. Additionally suggesting best practices for mitigating such risks especially in sensitive applications like algorithmic fairness would strengthen the papers contribution to responsible AI research. Overall incorporating some of these suggestions and clarifications could significantly enhance the papers value to both the theoretical and applied research communities.", "wiGXs_kS_X": " Review of \"Logical Credal Networks: Bridging Logic and Probability for Uncertainty Modelling\"  Summary: This paper introduces Logical Credal Networks (LCNs) a novel probabilistic logic framework that allows for expressing probability bounds and conditional probability bounds on both propositional and firstorder logic formulas without significant restrictions such as acyclicity. LCNs aim to overcome the limitations of existing formalisms by incorporating a generalized Markov condition which facilitates identifying implicit independence relations among atomic formulas thereby enhancing inference capabilities. The authors demonstrate LCNs\u2019 superiority over existing approaches through a series of experiments involving random networks Mastermind games with uncertainty and a credit card fraud detection application showcasing their ability to effectively handle and aggregate multiple sources of imprecise information.  Strengths: 1. Clarity and Scope: The paper is wellorganized and clearly articulates the motivation behind LCNs their formal definition and practical implications. It comprehensively covers related work systematically distinguishes LCNs from existing formalisms and explains their advantages. 2. Generalization: LCNs represent a significant advancement in combining probabilistic logic and graphical models generalizing over both in a novel way that effectively deals with imprecise information through probability bounds. 3. Empirical Evaluation: The experiments are welldesigned and demonstrate the practical benefits of LCNs in handling realworld problems involving uncertainty. The comparison with existing approaches is particularly compelling substantiating the papers claims about LCNs\u2019 superior performance.  Weaknesses: 1. Complexity and Scalability: While the paper briefly mentions the NPhard nature of inference in LCNs and proposes future work towards scalability a deeper discussion on the current limitations regarding the complexity and potential scalability solutions would have been beneficial. The exponential growth in runtime with the problem size raises significant concerns about the practical applicability of LCNs to largescale problems. 2. RealWorld Application and Validation: Although the paper presents practical applications the examples are somewhat contrived. Additional realworld applications especially in domains critical to probabilistic reasoning (e.g. healthcare security) could further validate the utility and versatility of LCNs. 3. Discussion on Limitations and Societal Impact: The paper lacks a thorough discussion on the potential negative societal implications of LCNs especially considering their application in sensitive areas like fraud detection. An analysis of ethical considerations potential biases and misuse would add value to the manuscript.  Questions and Suggestions for Authors: 1. Could you elaborate on potential approaches or ongoing work to address the complexity and scalability challenges associated with LCNs Any benchmarks on larger datasets or more complex domains could be enlightening. 2. How might LCNs be extended or adapted to explicitly handle temporal or sequential data which is common in many realworld applications of probabilistic logic 3. Considering the use of LCNs in applications like credit card fraud detection have the authors considered the implications of false positivesnegatives on individuals How might bias in the underlying data or probabilistic rules be mitigated 4. It would be beneficial if the authors could provide more insight into the theoretical underpinnings of the generalized Markov condition perhaps including a discussion on situations where this condition might lead to suboptimal inferences or where it might fail to capture crucial dependencies.  Limitations and Potential Negative Societal Impact: The paper does not sufficiently address the potential negative societal impacts of its proposed methodology. False positives in fraud detection for example can have severe consequences for individuals mistakenly identified as fraudsters. Moreover the authors do not discuss how biases in input data could propagate through LCNs or how such biases might be mitigated. Constructively the authors could incorporate fairness and bias assessment metrics into their evaluation framework and explore mechanisms to ensure that the inferences made by LCNs do not inadvertently perpetuate or amplify biases present in the training data.", "x56v-UN7BjD": "Review of \"Improved Algorithms for PrivacyPreserving NonConvex Empirical Risk Minimization\" 1) Brief Summary: This paper introduces innovative algorithms designed for privacypreserving empirical risk minimization (ERM) in nonconvex settings a vital area of research given the increasing use of statistical machine learning models with sensitive data in various domains. The authors develop an enhanced sensitivity analysis for stochastic gradient descent (SGD) in nonconvex settings employing a novel combination of treeaggregation techniques for momentum computation and analysis of momentum with private aggregation. The primary contribution is the presentation of two algorithms the first implements a Differentially Private Normalized SGD with momentum (DPNSGD) and the second focuses on Sensitivity Reduced Normalized SGD. These algorithms notably improve the gradient norm bounds while ensuring (\u03b5 \u03b4)differential privacy achieving an error rate of \\(\\tildeO(d13N23)\\) and reducing gradient evaluation complexity compared to previous work. 2) Assessment of Strengths and Weaknesses: Strengths:  Novelty: The paper advances the domain of privacypreserving nonconvex ERM by introducing new approaches to sensitivity analysis and private momentum computation.  Significant Improvement Over Previous Works: It substantially improves upon the gradient norm bounds and computation complexity which are crucial metrics for the effectiveness and efficiency of privacypreserving algorithms.  Theoretical Contributions: The authors provide rigorous theoretical guarantees for their algorithms with detailed proofs sketched in the paper and fully elaborated in the appendices. These guarantees are vital for the adoption of these algorithms in practice. Weaknesses:  Practical Evaluation: The paper primarily focuses on theoretical contributions without offering empirical evaluations. Including experimental results comparing these new algorithms to existing methods on benchmark datasets could strengthen the paper by validating theoretical improvements in practice.  Complexity and Implementational Details: The paper is heavily theoretical and the complexity of the algorithms might pose challenges for practitioners interested in implementation. A supplementary section discussing practical implementation aspects optimization tips or software availability could enhance accessibility and applicability.  Discussion on RealWorld Implications: While the paper touches upon the potential for improved privacy in various domains a more detailed exploration of specific realworld applications and how these algorithms could be integrated could provide valuable insights for readers. 3) Questions and Suggestions for the Authors:  Empirical Validation: Can the authors provide empirical validation of their algorithms against existing methods This could involve comparisons on benchmark datasets to demonstrate practical improvements in terms of privacy accuracy and computation time.  Implementation Guidance: Could the authors discuss potential challenges in implementing these algorithms in realworld settings Are there particular computational resources or software libraries recommended for practitioners  Future Research Directions: The paper hints at incorporating privacy amplification by shuffling as a future direction. Could the authors elaborate on potential approaches or challenges in integrating this with the proposed algorithms  Impact on Smaller Datasets: How do the proposed algorithms perform with smaller datasets where privacy concerns are significant but the volume of data might not be sufficient to leverage the benefits of the techniques fully 4) Limitations and Societal Impact: The authors discuss several promising directions for further improving their results particularly around incorporating privacy amplification techniques. However the practical implications especially in the context of deploying these algorithms in sensitive applications are not thoroughly addressed. Considering the potential negative societal impact such as misuse of differential privacy to obfuscate algorithmic biases offering a stronger analysis of these aspects could make the paper more comprehensive. Providing guidelines or considerations for ethical use and implementation of these privacypreserving algorithms would be a valuable addition.", "mE1QoOe5juz": "Review of \"Leveraging Tiered Structure for RiskDiversified Policy Learning in UserInteraction Applications\" 1) Summary and Contributions: This paper introduces a novel learning framework designed to enhance user interaction applications by incorporating a tiered user structure based on their tolerance towards exploration risks. The proposed framework divides users into two groups: risktolerant (GO) and riskaverse (GE). For each group a distinct policy  \u03c0O for online interactions with GO and \u03c0E for exploiting prior data with GE  is applied aiming to balance exploration and exploitation accordingly. The paper thoroughly examines whether separate handling of these groups leads to any advantages particularly for riskaverse users across both gapindependent and gapdependent settings. The main findings include:  In gapindependent settings no significant benefit is detected from adopting the tiered approach.  In gapdependent settings the employment of Pessimistic Value Iteration (PVI) for \u03c0E significantly reduces the regret for riskaverse users to a constant level independent of the number of episodes while \u03c0O retains nearoptimal online regret. The contributions of the study are threefold: a novel problem formulation that conceptualizes the tiered structure in user interaction scenarios theoretical findings that challenge the efficacy of tiered strategies in gapindependent settings and crucially the successful application of PVI in gapdependent settings that leads to constant regret for GE without sacrificing the performance of GO. 2) Assessment of Strengths and Weaknesses: Strengths:  Originality: The paper addresses an underexplored problem by introducing a tiered framework that distinctly categorizes users based on their risk tolerance. This is particularly innovative in the context of reinforcement learning (RL) and userinteraction applications.  Theoretical Depth: The analysis covering both gapindependent and gapdependent scenarios is thorough with comprehensive proofs supporting the conclusions.  Practical Relevance: The frameworks focus on user interaction applications such as recommendation systems and medical treatment plans underscores its potential for realworld impact. Weaknesses:  Experimental Validation: The paper primarily focuses on theoretical results with experimental demonstrations relegated to the appendix. Comprehensive empirical studies on realworld datasets could strengthen the validation of the tiered learning frameworks efficacy.  Assumption on User Division: The framework assumes prior knowledge about user tolerance towards exploration risks which may not always be practical or accurately determinable in realworld applications.  Scalability Concerns: The proposed approachs scalability especially in environments with large user bases and diverse risk tolerance levels is not explicitly discussed. 3) Questions and Suggestions for the Authors:  Experimental Setup: Could the authors provide more details on the experimental settings outlined in Appendix H It would be particularly beneficial to understand how the tiered structure was simulated and if real user data could further validate the framework.  Impact of Incorrect Tier Assignment: Have the authors considered the impact of incorrect tier assignments on the overall system performance  Extension to Multiple Tiers: The authors briefly touch upon the possibility of extending the framework to accommodate multiple tiers. Could they elaborate on potential methods or challenges in implementing and analyzing such an extension 4) Limitations and Societal Impact: The authors briefly address limitations in their discussion particularly noting the assumption of homogeneous dynamics within user groups and the consideration of only two tiers. A more thorough examination of the implications of these limitations in varied application settings would strengthen the paper. Additionally while the paper does not directly address potential negative societal impacts the tiered approachs implications on fairness and equity deserve attention. Users categorized in a lower tier may perpetually receive less optimized interactions inadvertently exacerbating inequality. Suggestions for improvement include exploring adaptive mechanisms for tier reassignment based on user feedback and ensuring transparent communication about how different tiers affect user experience.", "tjFaqsSK2I3": "Review of \"Unifying Vision Tasks through a Shared PixeltoSequence Interface\" 1) Summary of the Paper: The paper presents an innovative approach to unify a range of core computer vision tasks (specifically object detection instance segmentation keypoint detection and image captioning) into a single modeling framework using a pixeltosequence interface. By treating the outputs of these tasks as sequences of discrete tokens and employing a neural network with a shared architecture and loss function the authors demonstrate that its possible to efficiently train a single model to perform these tasks without the need for taskspecific customizations. The model utilizes prompts to generate taskspecific outputs and achieves competitive performance across all tasks on the COCO dataset. This work is a step towards simplifying the design and training of vision systems capable of performing multiple tasks moving closer to the goal of artificial general intelligence. 2) Strengths and Weaknesses:  Strengths: 1. Innovation: The paper introduces a novel approach to unify different vision tasks into a single framework which is a significant departure from the taskspecific architectures and loss functions prevalent in the field. 2. Simplicity and Efficiency: By using a unified interface and shared model architecture the approach simplifies the model design and training process making multitask learning more scalable. 3. Competitive Performance: The paper demonstrates that the model achieves competitive performance on the COCO dataset across all four tasks despite the absence of taskspecific customizations. 4. Proof of Concept: This work serves as a strong proof of concept that diverse vision tasks can be effectively addressed through a unified modeling approach.  Weaknesses: 1. Potential Limitations in Scaling: While the paper presents encouraging results its uncertain how well this approach will scale to a broader range of tasks beyond the four considered. The paper mentions potential benefits from scaling up but does not provide evidence or experiments to support this. 2. Inference Efficiency: The paper acknowledges that inference speed might be slower due to the autoregressive nature of the model which could be a significant limitation in realworld applications. 3. Further Improvements Needed: The authors mention that architectures and training techniques can be further improved to challenge specialized systems. This indicates that the current approach while promising is not yet fully optimized. 3) Questions and Suggestions for the Authors:  Questions: 1. Have the authors considered including more diverse vision tasks to test the scalability and generality of the proposed approach 2. Could the authors elaborate on the potential strategies to improve inference efficiency and whether any preliminary experiments have been conducted in this direction  Suggestions: 1. Future work could benefit from exploring nonautoregressive sequence modeling to address the mentioned inference speed limitations. 2. It would be valuable to conduct experiments that specifically investigate the benefits of scaling up the model and pretraining on larger datasets to establish the approachs effectiveness across a wider range of tasks and conditions. 3. An indepth comparison with specialized systems for each task might help better understand the tradeoffs involved in adopting a unified approach over taskspecific models. 4) Limitations and Societal Impact: The authors acknowledge certain limitations including potential inference speed issues and the need for further improvements to match the stateoftheart specialized systems. However the paper does not explicitly address the potential negative societal impact of this technology such as privacy concerns or misuse in surveillance applications. Constructive suggestions for improvement include:  Ethical Considerations: Future iterations of this work could benefit from including a discussion on the ethical implications of widespread deployment of such technology particularly in sensitive or regulated domains.  Bias and Fairness: Given the models dependence on training data its crucial to ensure diverse and representative datasets to minimize biases. The authors could explore mechanisms to evaluate and mitigate dataset biases that might propagate through the models predictions across different tasks.  Transparency and Explainability: As models become more generalized and capable of performing multiple tasks ensuring their interpretability becomes increasingly challenging but essential especially for applications requiring trust and reliability. Future research could focus on developing tools and methods to enhance the transparency and explainability of such models.", "qwjrO7Rewqy": " Review of \"Efficient Graph Neural Networks for Extended Persistence Diagram Approximation\"  1) Summary of the Paper This paper addresses the computational bottleneck of computing extended persistence diagrams (EPDs) for large and dense graphs by proposing a novel graph neural network (GNN) framework for approximating EPDs effectively. The novel approach benefits from insights into the EPD computation algorithm leading to a method that is not only efficient but also provides better supervision through an edgewise prediction strategy. By approximating the UnionFind algorithm through a customized GNN architecture the authors manage to achieve significant speedups (nearly 100 times faster) in computing EPDs for large graphs demonstrating their methods effectiveness in downstream graph representation learning tasks such as node classification and link prediction. Additionally their approach exhibits potential for transferability to unseen graphs opening new doors for the application of topological features in analyzing complex realworld networks.  2) Strengths and Weaknesses Strengths:  Innovation: The paper introduces an innovative approach to approximating EPDs by leveraging neural algorithmic reasoning addressing a significant computational challenge in graph learning.  Empirical Validation: Comprehensive experiments validate the effectiveness of the proposed method in both approximating EPDs and enhancing downstream learning tasks.  Efficiency: Demonstrated speedup on large and dense graphs underscores the practical value of the proposed method in scenarios where traditional computations are prohibitive.  Transferability: The methods potential for transferability to unseen graphs is a significant advantage for applying topological features across various domains. Weaknesses:  Generalization across Graph Domains: While the paper hints at transferability it does not deeply explore the methods generalization capabilities across different graph domains or detail the conditions under which transferability holds.  Comparison with StateoftheArt Methods: The paper could benefit from a more extensive comparison with other stateoftheart methods in both EPD approximation and downstream tasks including those that do not rely on GNNs.  3) Questions and Suggestions for the Authors Questions:  Can the authors elaborate on how the model performs on graphs with different structural properties (e.g. graphs with varying degrees of sparsity)  How does the choice of filter functions impact the models performance and is there a way to automate this choice based on the input graphs characteristics Suggestions:  Broadening the Scope of Comparisons: Including a wider range of baselines especially nonGNNbased approaches might provide a clearer picture of the proposed methods standing in the current landscape.  Exploring Transferability in Depth: Additional experiments or theoretical insights into the conditions and limitations of model transferability across different graph domains could enrich the papers contributions.  Addressing Potential Overfitting: It might be beneficial to discuss the potential for overfitting especially in transfer scenarios and propose strategies (e.g. regularization techniques) to mitigate such risks.  4) Limitations and Societal Impact The authors have demonstrated the methods efficacy and discussed its efficiency and potential for generalizing to unseen graphs which are crucial aspects. However a deeper analysis of the limitations regarding the models scalability sensitivity to hyperparameters and performance across a diverse set of graph types would enhance the paper. Regarding societal impact while the authors do not explicitly discuss potential negative impacts the deployment of such technologies in sensitive areas (e.g. social network analysis bioinformatics) necessitates awareness of privacy and ethical considerations. Constructively further research could investigate ensuring that the approximation of topological features does not inadvertently reveal sensitive information about the underlying graph data. Overall this paper presents a significant advancement in the approximation of EPDs for graph neural networks showcasing both technical novelty and practical utility. With further exploration of identified areas for improvement this work could pave the way for wider adoption and application of topological methods in graph learning.", "pkzwYftNcqY": "PeerReview of the Paper on \"Computationally Efficient Nonparametric Testing with Aggregated Kernel UStatistics\" 1. Summary This paper introduces a set of computationally efficient nonparametric tests aimed at addressing three classical problems in statistics: twosample testing independence testing and goodnessoffit testing. The proposed methods leverage aggregated kernel Ustatistics which are known for their powerful testing abilities but suffer from high computational costs. By employing incomplete Ustatistics and a novel aggregation scheme across multiple kernel bandwidths the authors manage to significantly reduce the computational expense without compromising the tests power. The quantile estimation for determining test thresholds is innovatively handled using a wild bootstrap method providing a solution to the traditional kernel selection dilemma. Rigorous theoretical analysis including nonasymptotic uniform separation rates and variance and quantile bounds for the introduced statistics underpin the work. Empirical validation confirms the superiority or competitiveness of the proposed methods against existing stateoftheart tests in terms of both power and computational efficiency. 2. Strengths and Weaknesses Strengths:  The paper addresses an important and challenging problem in statistics and machine learning: reducing the computational cost of kernelbased nonparametric tests without sacrificing test power.  Theoretical contributions are significant providing nonasymptotic uniform separation rates for two of the proposed tests and demonstrating the tests minimax optimality under certain conditions.  The wild bootstrap approach for quantile estimation is innovative and addresses the kernel selection problem effectively.  Empirical results strongly support the theoretical claims showing that the proposed tests either outperform or are competitive with existing lineartime tests while being computationally more efficient. Weaknesses:  While the paper provides extensive theoretical analysis it appears that the discussion on the practical implementation details (e.g. choice of kernels selection of hyperparameters for the tests) is somewhat limited. This could potentially hinder the reproducibility and application of the proposed methods in practical settings.  The limitations and potential negative societal impacts of the work are not thoroughly discussed. The paper could benefit from a more detailed examination of scenarios where the proposed methods might fail or yield misleading results. 3. Questions and Suggestions for the Authors  Could the authors provide more details on the choice of kernels and the selection of hyperparameters in their experiments How sensitive are the proposed tests to these choices  Are there specific types of data distributions or problem domains where the proposed tests might not perform as well How can practitioners identify such scenarios  It would be beneficial if the authors could discuss any potential negative societal impacts of their work. Are there applications or contexts where the misuse of these tests could lead to unintended consequences 4. Limitations and Societal Impact  The authors have not fully addressed the limitations of their approach. Specifically a discussion on the scenarios where the proposed methods may underperform or the conditions under which the theoretical guarantees do not hold would be valuable.  The potential negative societal impacts of the work are not discussed. Given the broad applicability of statistical testing reflecting on how inaccuracies or misinterpretations of test results could affect decisionmaking in critical domains would enhance the papers contributions. Constructive Suggestions:  To address the limitation related to the practical implementation the authors could provide an additional section or appendix detailing the experimental setup including kernel selection and hyperparameter tuning processes.  A subsection dedicated to discussing the limitations of the proposed methods accompanied by practical guidelines for their application would be highly beneficial.  The authors are encouraged to include a discussion on the ethical considerations and potential negative societal impacts of their work highlighting areas where careful application is warranted.", "v6NNlubbSQ": "The manuscript proposes a sophisticated method for quantifying uncertainty in machine learning model predictions termed Nonparametric Uncertainty Quantification (NUQ). By leveraging the NadarayaWatson kernelbased estimator and considering the asymptotic Gaussian approximation the authors manage to explicitly disentangle aleatoric (inherent data noise) and epistemic (model uncertainty) uncertainties. This approach does not necessitate significant modifications to existing neural network architectures or training regimes making it broadly applicable and computationally efficient. The authors show that NUQ outperforms existing uncertainty estimation methods across various tests including text and image classification tasks on several wellknown datasets.  Strengths: 1. Novelty and Theoretical Rigor: The approach is theoretically wellfounded with the paper providing formal proofs of the NUQbased estimators consistency. This approach adds a principled method to the toolkit available for uncertainty quantification in machine learning. 2. Practical Applicability: NUQ is universally applicable to any deterministic neural network model without necessitating architectural or procedural adjustments. This plugandplay feature enhances its practical utility significantly. 3. Performance: According to experimental evaluations NUQ outperforms several existing methods for uncertainty quantification across various tasks and datasets demonstrating its effectiveness and robustness particularly in lowresource settings and outofdistribution detection. 4. Efficiency: The method is computationally efficient leveraging techniques such as the use of hierarchical navigable small world graphs for fast nearest neighbor searches making it scalable to large datasets as demonstrated on ImageNet.  Weaknesses: 1. Assumption on Distributional Approximation: The approach relies on the assumption of asymptotic Gaussian distribution for the difference between the estimated and true conditional label distributions. While this is common and justified in theory the realworld applicability could vary across different data distributions potentially limiting the methods effectiveness in some scenarios. 2. Scalability Concerns: Despite the notable efficiency and scalability demonstrated the requirement of storing embeddings for the entire dataset and computing distances could pose limitations when dealing with extremely large datasets or in constrained computational environments. 3. Comparison with Recent Approaches: The paper could benefit from a broader comparison with the most recent uncertainty quantification methods especially those proposed concurrently or following the heuristic approaches that have shown promising results in specific applications.  Questions and Suggestions for the Authors: 1. Handling NonGaussian Distributions: Could the authors discuss or propose extensions of NUQ to accommodate cases where the Gaussian approximation does not hold well for the conditional label distributions 2. Adaptation to TimeSeries Data: How could NUQ be adapted for or applied to uncertainty quantification in timeseries predictions where data points are not independently distributed 3. Quantitative Impact of Hierarchical Navigable Small World Graphs: It would be enriching to quantify the computational efficiency gained by employing hierarchical navigable small world graphs for nearest neighbor searches compared to other approaches.  Limitations and Future Work: The authors have to a reasonable extent acknowledged the limitations of their approach including the reliance on asymptotic distributional approximations. However a more detailed discussion on the potential impacts of these limitations on specific types of data (e.g. highly imbalanced datasets or those with complex distributions) would be valuable. Additionally exploring the adaptation of NUQ to a wider range of learning tasks including unsupervised and semisupervised scenarios could significantly broaden its applicability. Regarding societal impact discussing the potential consequences of incorrect uncertainty estimation in critical applications (e.g. healthcare autonomous driving) and suggesting mitigation strategies would enhance the papers contributions.  Overall Assessment: This paper presents a significant contribution to the field of uncertainty quantification in machine learning offering a theoretically grounded and practically efficient method. While there are areas for further exploration and clarification the proposed NUQ method stands as a promising advancement. Following the authors responses to the questions and suggestions and with minor revisions particularly concerning the broader implications and limitations of their work this paper would merit publication.", "w6tBOjPCrIO": "Review of \"Leveraging Locally Factored Dynamics for ModelBased Counterfactual Data Augmentation in Reinforcement Learning\" 1. Summary of the Paper and Its Contributions The paper introduces a novel framework named Modelbased Counterfactual Data Augmentation (MOCODA) aimed at enhancing the capability of reinforcement learning (RL) agents to generalize to unseen states and actions. The core insight is based on leveraging known or learned local factorizations in environment transitions which can significantly reduce the sample complexity for training a dynamics model and enable this model to generalize to outofdistribution (OOD) states and actions efficiently. The MOCODA framework employs a locally factored dynamics model to create an augmented distribution of states and actions from which counterfactual transitions are generated for RL training. The papers key contributions include a demonstration of exponential reduction in sample complexity through known local structure the introduction of the MOCODA framework that outperforms previous methods in handling a broader set of local structures and empirical evidence showing enhanced performance in a robotics manipulation task indicative of the frameworks ability to enable \"zeroshot\" generalization to unseen states and actions. 2. Assessment of Strengths and Weaknesses Strengths:  Novel Framework: MOCODA introduces a significant innovation in utilizing locally factored dynamics for data augmentation facilitating learning in complex multiobject domains where generalization to unseen states and actions is critical.  Theoretical Justification: The paper provides a solid theoretical foundation for the proposed method including an adaptation of modelbased RL to the locally factored setting which convincingly shows the potential for exponential gains in sample complexity.  Empirical Validation: The authors present compelling empirical evidence of MOCODAs effectiveness in improving outofdistribution generalization particularly through the robotics manipulation task underscoring the practical applicability of their approach. Weaknesses:  Complexity of Implementation: The implementation of MOCODA while effective appears complex potentially hindering its adoption in broader RL applications or by practitioners without deep technical expertise in modelbased RL.  Limited Discussion on Scalability: Although the paper indicates that MOCODA works with highdimensional inputs detailed insights or empirical results demonstrating scalability especially in resourceconstrained environments are lacking. 3. Questions and Suggestions for the Authors  Scalability and Efficiency: Can the authors elaborate on the computational efficiency of MOCODA especially in terms of scalability to higherdimensional state spaces or more complex environments Are there strategies to mitigate potential computational bottlenecks  Robustness to Model Misspecifications: How does MOCODA perform when the underlying assumptions about local factorization are not perfectly met or when there are errors in the learned factorization Are there mechanisms within the framework to detect and correct for such discrepancies  Extension to MultiAgent Scenarios: Could MOCODA be adapted for multiagent reinforcement learning settings where the dynamics involve interactions among multiple agents What modifications would be necessary to accommodate such scenarios 4. Limitations and Societal Impact The authors have not thoroughly addressed the potential limitations and negative societal impacts of their work. While the theoretical and empirical contributions are substantial it would be beneficial for the authors to discuss scenarios where MOCODA might fail or yield suboptimal results such as in environments with extremely sparse rewards or highly dynamic changes. Additionally considering the broader applicability of RL in societal contexts a discussion on the ethical considerations especially around fairness and accountability when deploying RL systems trained with MOCODA in realworld applications would enrich the paper. Suggestions for Improvement:  Provide a detailed analysis or discussion on computational efficiency and strategies to enhance scalability.  Include experiments or theoretical analysis that probe the robustness of MOCODA under model misspecifications or errors in factorization learning.  Expand the discussion on potential ethical considerations and societal impacts particularly in the context of deploying systems trained with MOCODA in sensitive or impactful applications.", "o8nYuR8ekFm": "Peer Review of \"Exploring Generalization in Deep Learning via Compressed Neural Networks\" 1) Summary: This paper addresses a significant challenge in the field of deep learning pertaining to the development of nonvacuous generalization bounds for deep neural networks. The key contribution of this work is the introduction of a novel compression approach that leverages quantizing neural network parameters within a linear subspace to achieve stateoftheart generalization bounds across various tasks including transfer learning. The authors highlight the profound ability of large models to be compressed significantly more than previously understood aligning with the principle of Occams razor. They also explore the role of model size equivariance and the implicit biases of optimization for generalization in deep learning. Through this exploration the paper sheds light on why neural networks generalize well on structured datasets and less so on unstructured ones. 2) Assessment: Strengths:  Novelty in Approach: The paper introduces a compression method that is distinct from previous works demonstrating significant advancements in achieving tighter generalization bounds. This methods adaptability to the problem complexity and its incorporation of learned quantization are particularly innovative.  Comprehensive Analysis: The thorough examination of how the proposed compression scheme impacts generalization across different datasets and in the context of transfer learning is commendable. This holistic approach provides a deeper understanding of the dynamics at play in neural network generalization.  Strong Empirical Results: The authors present compelling empirical evidence to support their claims achieving stateoftheart generalization bounds on benchmarks like CIFAR10 and ImageNet. This empirical validation significantly strengthens the papers contributions. Weaknesses:  Model and Method Generality: While the paper presents impressive results on image datasets the generality of the compression approach to other domains or tasks (e.g. NLP time series analysis) remains uncertain. Future work could benefit from exploring these avenues.  Theoretical Insights on Compression: The paper could benefit from deeper theoretical insights into why their particular method of compression (quantization within a linear subspace) leads to better generalization bounds. Some discussion is provided but a more rigorous theoretical framework could add value.  Discussion on Practical Implications: While the focus on generalization bounds is clear and well executed the practical implications of the proposed method could be better highlighted. For instance how does this compression approach impact inference time memory usage or deployment on resourceconstrained devices 3) Questions and Suggestions for the Authors:  Could the authors provide additional insight into how the proposed compression method might generalize to tasks beyond image classification such as sequential data processing or natural language tasks  It would be beneficial if the paper could include a more detailed theoretical discussion on why quantization within a linear subspace specifically results in improved generalization bounds.  Considering the practical applications of neural network compression could the authors discuss how their proposed method impacts other aspects of model deployment such as efficiency or resource requirements 4) Limitations and Societal Impact: The authors have managed to highlight their works positive implications particularly in understanding neural network generalization better. However the discussion on limitations is somewhat lacking. For example the potential for the methods applicability beyond imagebased tasks is not addressed. Additionally while the potential negative societal impacts of their work seem minimal a discussion on this aspect (e.g. biases in transfer learning from pretrained models) would make the paper more comprehensive. Suggestions for improvement include expanding the evaluation to other task domains and providing a more thorough discussion on both the limitation of the methods generality and any possible negative societal impacts.", "wuunqp9KVw": "Review of \"An EndtoEnd Probabilistic Method for Pluralistic Image Completion\" 1. Brief Summary This paper addresses the challenge of pluralistic image completion a task that aims to fill in the missing regions of an image to produce multiple visually realistic and diverse completion results. The authors identify that previous methods suffer from noninterpretability and dissatisfaction concerning the constraints used for enforcing visual realism and diversity in the generated images. To overcome these limitations the authors propose an endtoend probabilistic method that utilizes a unified probabilistic graph model (PGM) to represent the complex interactions in image completion tasks more effectively. The method employs a Gaussian mixture model (GMM) to diversify the output with taskrelated parameters optimized adaptively during training. The results reported demonstrate the superiority of the proposed method over stateoftheart techniques in producing visually realistic and diverse image completions. 2. Assessment of Strengths and Weaknesses Strengths:  Innovative Approach: The methods use of a probabilistic graph model for pluralistic image completion is novel and justifies the addition of constraints for visual reality and diversity in a coherent manner.  Improved Diversity: Employing GMM to increase the diversity of results seems promising as it allows for taskrelated parameters to be optimized leading to more interpretable and satisfactory results.  Comprehensive Evaluation: The paper conducts extensive experiments across several datasets showing qualitative and quantitative improvements over stateoftheart methods. The inclusion of both common metrics and diversity metrics strengthens their claims. Weaknesses:  Complexity and Interpretability: While the proposed method is reported to have superior performance the introduction of a more complex probabilistic graph model might increase the difficulty of understanding and implementing the method for practical applications.  Generalization Ability: The paper lacks a thorough examination of the method\u2019s ability to generalize to unseen data or its application potential in other relevant domains beyond the datasets tested.  Computational Efficiency: Given the more complex approach involving GMM and the probabilistic graph model there are potential concerns about the computational efficiency and scalability of the proposed method which the paper does not fully address. 3. Questions and Suggestions for the Authors  Model Complexity: Can the authors comment on the computational complexity of their method compared to other stateoftheart methods and its impact on practical applications  Generalization: It would be beneficial to see how this method performs on completely unseen datasets or in other imagerelated tasks to better understand its generalization capabilities.  Experimental Design: While the results are impressive providing more insight into the failure cases of your method could help understand its limitations better. Could the authors provide examples where their method does not perform as expected  Societal Impact Discussion: The paper briefly touches upon potential applications like photo restoration and object removal. However discussing the ethical considerations such as potential misuse for creating deepfakes would be valuable. 4. Limitations and Societal Impact The authors have made an attempt to address the limitations by discussing the potential reduction in diversity for deterministic inferences. However a deeper analysis of the computational efficiency and potential scalability issues would enhance the paper. Moreover while the paper briefly mentions applications a more detailed discussion on the ethical implications and negative societal impacts particularly concerning creating misleading content or deepfakes is warranted. Suggestions for mitigations such as watermarking synthesized images or developing detection mechanisms should be considered to encourage responsible use of the technology.", "u6OfmaGIya1": "Review of \"SECOND THOUGHTS: Enabling Language Models to Realign with Human Values through EditBased Learning\" 1) Brief Summary This paper introduces SECOND THOUGHTS a novel learning paradigm aimed at addressing the issue of value alignment in language models (LMs). The technique models a chainofedits process which allows the model to transition from valueunaligned to valuealigned text. By employing a combination of finetuning on these edit chains and enhancement through reinforcement learning the approach claims to generate text that is not only more aligned with human values but also interpretable and amendable to interactive error correction. The authors tested their approach on three benchmark datasets and demonstrated its efficacy through extensive human evaluation. The paper makes significant contributions by pinpointing the limitations of current largescale LMs in aligning with human values and offering a robust methodology that substantially improves value alignment and transfer learning capabilities in fewshot scenarios. 2) Assessment of Strengths and Weaknesses Strengths: a. Novelty and Relevance: The approach is innovative addressing a critical and timely challenge in the AI field \u2013 ensuring language models align with human values. The solution is practically relevant given the increasing integration of LMs in systems engaging with humans. b. Technical Soundness: The methodology combining dynamic programming for edit inference and reinforcement learning for refinement is technically solid. The detailed explanation of the chainofedits modeling the augmented edits modeling (AEM) and the reinforcement learning refinement stages offer transparency into the model workings. c. Extensive Evaluation: The models performance has been rigorously assessed through comparison with leading alternatives on benchmark datasets and its superiority is further validated through human evaluations. This thorough evaluation strengthens the papers claims. d. Impact and Interpretability: By demonstrating an ability to generate valuealigned text especially in poisoned contexts and providing a mechanism for interactive error correction the paper ensures practical impact. The model\u2019s interpretability through the chainofedits mechanism is a significant advantage for realworld applications. Weaknesses: a. Potential Biases: While the approach depends on human edits and evaluations theres an implicit risk of introducing biases especially given the subjective nature of value alignment. The diversity in human values and the potential for these to change over time raises questions about the models longterm efficacy and fairness. b. Scalability and Generalizability Concerns: The paper demonstrates success in fewshot scenarios but its unclear how scalable the approach is when dealing with the vast and varied realworld data LMs are expected to handle. Additionally the generalizability of the model across languages cultures and differing sets of values remains to be seen. c. Dependency on Source Quality: The success of the SECOND THOUGHTS paradigm seemingly rests on the quality of humangenerated edits. This reliance could limit the models ability to autonomously improve and adapt especially in environments where highquality edits are scarce. Questions and Suggestions for the Authors: 1. Bias Mitigation: Could the authors elaborate on measures taken or proposed to mitigate biases introduced through human edits and evaluations 2. Scalability and Multilinguality: How do the authors foresee the model adapting to the scalability requirements of realworld applications and extending to multilingual and multicultural contexts 3. Autonomy in Learning: Are there plans to reduce the models reliance on humangenerated edits perhaps through unsupervised learning techniques that might enable the model to generate its chainofedits 3) Limitations and Societal Impact The paper briefly touches upon potential negative societal impacts through its reliance on manually annotated datasets that may introduce biases. However it could benefit from a deeper exploration of the limitations regarding scalability generalizability across cultures and dependence on quality source edits. A constructive suggestion would be to incorporate an iterative feedback loop where the model\u2019s outputs are continuously evaluated and corrected by a diverse and global user base ensuring the models learning process remains dynamic and inclusive. Additionally discussing strategies to evaluate and enhance the models performance autonomously without heavy dependence on human input would greatly add to the works robustness and applicability in varied societal contexts.", "yoBaCtx_a3": "This paper introduces a novel algorithm designed to address the complex problem of learning switched linear dynamical systems from noisy observational data of a systems state or output. Switched linear systems characterized by their utilization of multiple linear dynamical modes for data fitting within specified tolerances find broad applicability across robotics cyberphysical systems and various other domains. Traditionally solving this class of problems especially due to the inherent NPhardness and the challenge of fitting multiple linear models (k  1) to data has resulted in exponential time complexities with respect to the number of data points involved. By ingeniously reformulating the problem to exploit a \"gap\" within the error tolerance parameters and employing a mix of the ellipsoidal method and separation oracle themes known from convex and nonsmooth optimization literature the authors present an algorithm boasting linear complexity in terms of data points while maintaining exponential complexity relative only to the number of state variables and modes desired. Empirical validation across both synthetic microbenchmarks and realworld problems attests to the practical performance gains achievable against traditional mixedinteger linear programming solvers even those highly optimized like Gurobi. Strengths: 1. Innovative Problem Reformulation: By introducing a reformulation that incorporates a tolerance gap the paper significantly improves computational efficiency an advancement over the direct MILP approaches that suffer from poor scalability due to exponential time complexity with the number of data points. 2. Theoretical and Practical Contributions: The authors provide a solid theoretical foundation for their algorithm and demonstrate its efficacy through comprehensive experimentation. Practical applications such as modeling human handwriting and mechanical systems with contact forces illustrate the algorithm\u2019s utility in realworld scenarios. 3. Scalability: The empirical results showcase exceptional scalability of the proposed algorithm especially as it pertains to the amount of data which is critical for applications involving large datasets commonly encountered in robotics and cyberphysical systems. Weaknesses: 1. Exponential Complexity in Modes and State Dimensions: While the reformulation leads to linear complexity in the number of data points the exponential dependence on the number of modes and state variables remains a challenge potentially limiting applicability in scenarios with high dimensionality or requiring numerous modes. 2. Lack of Markov Model Generation: The algorithm focuses on fitting dynamics to data points without inferring the generator for mode switches which could limit its applicability in scenarios where understanding transition logic between modes is crucial. Questions and Suggestions: 1. Extension to High Dimensional Systems: Have the authors considered approaches to mitigate the exponential complexity related to the number of modes and state variables Any insights or ongoing work in this direction could be valuable. 2. Addressing Mode Transition Logic: While the methodology excellently fits multiple modes to the data integrating the learning of transition logic between modes could enhance the systems utility. Could a hybrid algorithm that combines the current approach with a method for learning mode transition logic be feasible 3. Consideration of Computational Efficiency: Given the reliance on separation oracles and the ellipsoidal method what are the practical computational resource requirements and how do they scale with problem size Insights into computational overheads might help assess the algorithm\u2019s realworld applicability more concretely. Limitations and Societal Impact: The paper partially addresses the limitations inherent in its approach notably the exponential complexity with respect to the number of modes and dimensionality of the state space. A further examination into the practical limits of system size and complexity that the proposed algorithm can handle efficiently would be beneficial. Moreover addressing the algorithms inability to learn the logical structure governing mode transitions could significantly enhance its practical applicability. From a societal impact perspective while the paper mentions potential beneficial applications including improved medical devices and humanrobot interaction a deeper exploration of the negative implications particularly in relation to surveillance and privacy concerns is warranted. Constructive suggestions include incorporating ethical guidelines for the use of such technology and transparently communicating the potential for misuse in surveillance applications to foster responsible innovation and application in this field.", "nSe94hrIWhb": " Review of \"Efficient Topological Data Analysis of Large Networks\"  1) Summary The paper presents an innovative approach to address the computational challenges associated with applying Topological Data Analysis (TDA) to large networks. The authors introduce two algorithms CoralTDA and PrunIT designed to compute persistence diagrams of large graphs effectively. The CoralTDA algorithm leverages the (k1)core of a graph to compute its kth persistence diagram significantly reducing the graph size needed for computations without loss of information. The PrunIT algorithm further optimizes this process by pruning dominated vertices leading to substantial reductions in vertex count and computational time. Through a series of experiments on various large realworld networks the authors demonstrate the efficacy of their approach showing up to 95 reduction in graph size and thus making the application of TDA methods to large networks feasible.  2) Assessment Strengths:  Innovative Approach: The paper addresses a significant bottleneck in the application of TDA to large networks by introducing two novel algorithms that drastically reduce computation times and resources needed.  Significant Reductions: The demonstrated reductions in graph sizes and computational costs are impressive potentially allowing for the broader application of TDA in fields where it was previously infeasible.  Theoretical Foundation: The authors provide solid theoretical underpinnings for their algorithms including proofs and a detailed explanation of the methodology which adds credibility to their claims.  Broad Applicability: The algorithms are tested on a diverse set of realworld networks showcasing the versatility and broad applicability of the proposed method.  Opensource Implementation: By making their implementation available on GitHub the authors encourage replication independent verification and further innovation on their work. Weaknesses:  Lack of Comparative Analysis: The paper could be strengthened by including a more comprehensive comparative analysis with existing methods particularly in terms of computational efficiency and accuracy of the persistence diagrams generated.  Potential Overfit to Specific Types of Networks: While the results are impressive there is a concern that the performance gains could be specific to the network types tested. Additional testing on a wider array of network structures might be necessary to validate the general applicability of the algorithms.  Discussion on Limitations and Negative Impact: The paper does not thoroughly discuss the limitations of the proposed methods or their potential negative societal impacts.  3) Questions and Suggestions  Can the authors provide more comparative data against existing methods especially in situations where those methods might perform better  Is there a potential for the algorithms to miss or inaccurately represent topological features in certain types of graphs or under specific conditions  Could the authors elaborate on any limitations they foresee in the application of their algorithms to different types of large networks not covered in the experiments  It would be beneficial if the authors could discuss any potential negative societal impacts their work could have particularly in the context of privacy and security in networks analysis.  4) Limitations and Potential Negative Societal Impact The authors have not adequately discussed the limitations of their work or its potential negative societal impacts. While the computational advancements are clear there is little discussion on situations where the algorithms may not perform as expected or may be inappropriate to apply. Additionally with the increasing use of network analysis in areas like social media finance and bioinformatics there is a risk that such powerful tools could be misused for instance in enhancing surveillance capabilities or invading privacy. Suggestions for Improvement:  A deeper exploration of the limitations especially concerning the types of networks or data distributions where the algorithms might underperform would be valuable.  Including a section on potential negative societal impacts such as privacy concerns or the misuse of network analysis tools would provide a more balanced view of the work.", "pV7f1Rq71I5": " PeerReview of the Manuscript  1) Summary of the Paper and its Contributions The paper proposes a novel scheme for estimating the entropy of an unknown distribution D over 1 ... k with a significant reduction in sample complexity leveraging a constant memory streaming algorithm. Building on the work of Acharya et al. (NeurIPS 2019) the authors introduce an improved estimation technique and an \"intervalbased\" algorithm which together reduce the required sample complexity from (k\u03b53) \u00b7 polylog(1\u03b5) to (k\u03b52) \u00b7 polylog(1\u03b5). This contribution is significant since it addresses the question of whether the tradeoff between sample complexity and memory in previous work was necessary and provides a substantial improvement. The paper combines theoretical insights and detailed mathematical proofs to present its contributions clearly.  2) Assessment of the Strengths and Weaknesses Strengths:  Novelty: The paper provides a novel approach to entropy estimation in lowmemory streaming algorithms significantly improving over previous work in terms of sample complexity.  Technical Depth: The mathematical rigor and thorough analysis demonstrate the correctness and efficiency of the proposed methods. The authors clearly outline the theoretical foundations estimators and algorithms contributing valuable knowledge to the field.  Practical Relevance: By reducing the sample complexity and maintaining constant memory the proposed scheme is advantageous for practical applications especially in resourceconstrained environments.  WellStructured: The paper is wellorganized providing a clear introduction to the problem a detailed description of the methodology a rigorous mathematical analysis and a discussion on the implications and future work. Weaknesses:  Experimental Evaluation: The paper lacks empirical evaluation of the proposed scheme. Including experiments demonstrating the practical effectiveness of the algorithm and comparing its performance with existing methods could strengthen the paper.  Discussion on Limitations and Negative Societal Impact: While the paper focuses on theoretical aspects it does not adequately discuss potential limitations or negative societal implications of the proposed methods.  Accessibility: Given the heavy reliance on mathematical theory and notation the paper may be less accessible to readers not familiar with the specific area of streaming algorithms and information theory.  3) Questions and Suggestions for the Authors  Could the authors provide practical examples or applications where the proposed entropy estimation scheme would be particularly beneficial  It would be valuable to include experimental results comparing the proposed scheme with previous works especially in terms of sample complexity and memory usage in practical settings.  Can the authors discuss any potential limitations or scenarios where the proposed scheme might not perform as expected  Given the significant improvement in sample complexity what are the possible negative societal impacts especially concerning privacy and data security when applying this technique in realworld scenarios  4) Limitations and Potential Negative Societal Impact The authors have not adequately addressed the limitations of their work. For example the performance of the proposed scheme under different data distributions or in the presence of noisy data is not discussed. Furthermore the paper does not consider the potential negative societal impacts such as privacy concerns that might arise due to the effective entropy estimation from limited samples. Constructive Suggestions for Improvement:  The authors should consider discussing the robustness of the proposed algorithm against variations in data distributions and the presence of noise.  It would be beneficial to include a section discussing potential limitations and how they might be addressed in future work.  A thoughtful discussion on the potential adverse societal impacts such as privacy concerns and misuse of the technology should be included to give a comprehensive view of the implications of this work.", "swIARHfCaUB": " Peer Review  1) Brief summary of the paper and its contributions The paper presents an extension of the Online FrankWolfe (OFW) method for online convex optimization to handle scenarios where gradient information experiences arbitrary delays. The original OFW method is advantageous due to its projectionfree nature allowing for efficient optimization over complex decision sets achieving an O(T34) regret bound for convex losses and an O(T23) for strongly convex losses under immediate gradient feedback. The proposed delayed OFW (dOFW) adapts OFW to operate in environments where gradients are delayed by arbitrary rounds achieving O(T34  dT14) for convex losses and O(T23  dlogT) for strongly convex losses where d represents the maximum delay. These bounds are shown to be competitive with the original OFW under significant delays. The paper validates the theoretical results through simulation experiments.  2) Thorough assessment of strengths and weaknesses Strengths:  Originality: The paper addresses an important practical limitation in the OFW method by extending it to environments with delayed gradient feedback which is a nontrivial improvement.  Theoretical Contributions: The derived regret bounds for both convex and strongly convex losses under arbitrary delays strengthen the theoretical understanding of OFWs performance. Importantly these bounds match those of the original OFW under specific delay conditions showcasing the efficacy of dOFW.  Practical Relevance: Delayed feedback is common in realworld applications. Therefore the proposed method has significant practical implications for improving online learning in such scenarios.  Experimental Validation: The simulation experiments provide empirical evidence supporting the theoretical claims and demonstrate the performance improvements over applying a blackbox technique to OFW. Weaknesses:  Assumptions: There are several underlying assumptions such as bounded gradients and decision set diameter. While common in the literature these assumptions may limit the applicability of the results to more general settings.  Comparison Baseline: Comparisons are mainly conducted against a combination of a blackbox technique with OFW. It would be beneficial to include comparisons with other stateoftheart methods for delayed feedback OCO to better position dOFWs performance.  Scalability: While the algorithm shows promising results the paper does not deeply explore scalability aspects especially for very largescale problems or extremely delayed settings which are critical for some practical applications.  3) Questions and Suggestions for the Authors  Comparison with Alternative Methods: Besides the combination of a blackbox technique with OFW are there other recent methods addressing delayed feedback in online convex optimization that could serve as comparators Including such comparisons could strengthen the impact of dOFW.  Applicability to RealWorld Problems: Can the authors provide insights or examples of realworld problems where dOFW could be particularly beneficial Specific sectors or applications where this is most relevant would be informative.  Scalability and Efficiency: Given that online learning often deals with highdimensional data how does dOFW scale with the dimensionality of the problem and the length of the delay Are there optimization techniques or approximations that could be applied to improve its efficiency  Impact of Assumptions: The paper relies on common assumptions in the OCO literature. Could relaxing these assumptions significantly affect the regret bounds and are there plans to explore such directions  4) Limitations and Potential Negative Societal Impact The authors have typically addressed the theoretical limitations through their regret bounds under assumptions common in the OCO literature. However a deeper discussion on practical limitations regarding scalability and computational efficiency especially in realtime applications would enhance the paper. Regarding societal impact while the paper does not explicitly discuss negative societal implications it is always prudent for work in optimization and machine learning to consider potential misuse especially in sensitive applications like automated trading surveillance and personal data analysis. Suggestions for improvement include:  Ethical Considerations: Briefly discuss potential ethical considerations in applying dOFW to ensure that future applications consider fairness privacy and transparency.  Robustness to Adversarial Settings: Given the delayed feedback setting further exploration of the robustness of dOFW in adversarial or noisy environments would be beneficial to ensure that it performs reliably in realworld scenarios. In summary this paper makes significant contributions to online convex optimization under delayed feedback with promising theoretical and empirical results. Addressing the highlighted questions and suggestions could further strengthen the paper.", "v9Wjc2OWjz": " Peer Review of \"On the Performance of Mismatched Estimation in Structured Noise\"  1) Summary of the Paper and Its Contributions This paper addresses the challenge of estimating a rank1 signal within a noise environment that is rotationally invariant but nonGaussian where the actual noise statistics are unknown and Gaussian noise is naively assumed. This scenario is relevant as obtaining accurate noise statistics can be impractical in many largescale data applications. The authors explore the performance implications of this mismatch by analyzing a Bayes estimator and an Approximate Message Passing (AMP) algorithm both premised on the incorrect Gaussian noise assumption. Through detailed theoretical examinations leveraging the theory of spherical integrals and matrix perturbations for the Bayes estimator and a novel artificial AMP to correct the mismatch they provide sharp asymptotic characterizations of meansquare error (MSE) and overlap (a measure of signal recovery quality).  2) Assessment of Strengths and Weaknesses  Strengths:  Originality and Relevance: The problem tackled is both original and highly relevant addressing a practical challenge in data analysis where noise characteristics may not be fully known. The focus on structured rotationally invariant noise is particularly pertinent given its occurrence in various realworld applications.  Rigorous Analysis: The paper introduces rigorous theoretical frameworks and analyzes the performance of mismatched estimation strategies comprehensively. The utilization of spherical integrals matrix perturbation theory and the development of an artificial AMP algorithm highlight a deep technical contribution.  Practical Insights: Despite its theoretical orientation the paper uncovers practical insights into the limitations of assuming Gaussian noise and the potential performance improvements that can be realized by adapting algorithms to more accurately reflect noise characterizations.  Novel Findings: The discovery that AMP algorithms do not necessarily compute the Bayes estimator efficiently under mismatched assumptions especially in terms of MSE adds valuable knowledge to the understanding of AMPs applicability and limitations.  Weaknesses:  Complexity and Accessibility: The technical depth while a strength also makes the paper less accessible to readers not wellversed in the mathematical foundations of signal processing and statistical inference. Simplification where possible or a more extensive discussion section might enhance accessibility.  Empirical Evaluations: While the paper is theoretically solid the inclusion of more extensive empirical evaluations across diverse scenarios could further substantiate the theoretical findings and illuminate the practical implications in a broader range of applications.  Discussion on Practical Implementation: There is an opportunity to expand the discussion around practical implementation challenges and potential solutions particularly in interpreting and applying theoretical insights to realworld data analysis tasks where noise characteristics may vary significantly.  3) Questions and Suggestions for the Authors  Empirical Evaluation on Real Datasets: Can the authors provide empirical evaluations of the discussed estimation strategies on realworld datasets where the noise characteristics are empirically determined but not strictly Gaussian How does the performance compare in such realistically complex scenarios  Extension to HigherRank Signals: While the focus on rank1 signals is clear how might the analysis extend to higherrank signal estimation Are there additional complexities or considerations that would fundamentally alter the findings  Impact of Assumption Variances: The paper assumes uniform spherical distribution for theoretical analysis. How sensitive are the findings to variations in these assumptions about the signal distribution  Algorithmic Complexity and Scalability: Could the authors discuss the computational complexity and scalability of the proposed AMP correction methodology Given largescale datasets what considerations should be made to efficiently implement these techniques  4) Limitations and Suggestions for Improvement The authors have done a commendable job addressing the limitations and potential negative societal impact of their work particularly in emphasizing the importance of correctly identifying noise characteristics for accurate data analysis. However a detailed discussion on the ethical considerations of misestimation in sensitive applications (e.g. medical diagnostics or financial modeling) where inaccurate assumptions may have significant consequences would enhance the paper. Additionally the authors could elaborate on strategies for identifying or mitigating potential biases introduced by mismatched noise assumptions in practical applications. Overall this paper makes significant contributions to the understanding of mismatched noise estimation in structured noise environments providing both theoretical insights and foundational methodologies for improving estimation accuracy under practical constraints.", "pnSyqRXx73": "This paper presents a thorough analysis of the asymptotic behavior of stochastic gradient descent (SGD) algorithms with a focus on the impact of different stochastic input sequences including i.i.d. noise random walk on arbitrary graphs and nonMarkovian processes. By employing the concept of efficiency ordering\u2014commonly used to compare the performance of Markov Chain Monte Carlo (MCMC) samplers\u2014the authors analyze SGD algorithms based on the Loewner ordering of covariance matrices associated with scaled iterate errors in the long term. The paper demonstrates that input sequences that are more efficient for MCMC sampling also result in smaller covariance of errors for SGD algorithms in the limit. This finding has practical implications for decentralized optimization and swarm learning where SGD is implemented in a random walk fashion for cost efficiency and data privacy concerns. The paper also compares different sampling methods within the SGD framework and demonstrates the utility of their method by applying it to variants such as accelerated SGD and Adam.  Strengths 1. Novelty and Generalization: The paper introduces a novel approach by connecting the concept of efficiency ordering from MCMC literature with the performance of SGD algorithms. This connection allows for a more general framework that could encompass a broader family of stochastic optimization algorithms. 2. Theoretical Depth and Rigor: The theoretical analysis is comprehensive and rigorous providing a solid foundation for the claims made. The use of Loewner ordering to compare the covariance matrices of iterate errors is both innovative and fitting. 3. Practical Implications: The findings have significant implications for decentralized optimization and privacypreserving machine learning methods where the choice of stochastic input sequence can greatly affect algorithm efficiency and data privacy. 4. Empirical Validation: The empirical observations supporting the theory for various SGD variants such as accelerated SGD and Adam provide credibility and showcase the potential for practical application of the proposed method.  Weaknesses 1. Complexity and Accessibility: The paper is theoretically dense which might limit its accessibility to readers not wellversed in the detailed mathematics of SGD and MCMC literature. A more intuitive explanation or a simplified summary of the key results could enhance its readability. 2. Limited Empirical Validation: While the paper presents numerical simulations to support its theoretical findings the empirical analysis could be expanded to include more diverse datasets and settings particularly in realworld decentralized optimization scenarios. 3. Discussion on Limitations and Societal Impact: The paper briefly touches on potential applications but does not thoroughly discuss the limitations or the negative societal impacts of the proposed method. Given the relevance to data privacy a more detailed examination of potential misuse or ethical considerations would be valuable.  Questions and Suggestions 1. Extension to NonConvex Settings: Could the authors discuss how their findings might extend to nonconvex optimization problems Given the prevalence of nonconvex objectives in machine learning insights into this aspect would be beneficial. 2. Comparison with Other Efficiency Metrics: How does efficiency ordering as proposed compare with other popular efficiency metrics for SGD (e.g. convergence rate robustness to hyperparameters) A discussion on this would provide a clearer picture of the tradeoffs involved. 3. Practical Implementation Aspects: Are there specific recommendations or guidelines for practitioners looking to apply these findings to decentralized optimization problems especially in terms of selecting or designing the stochastic input sequence  Limitations and Suggestions for Improvement While the authors have provided a foundational theoretical contribution the paper could benefit from a more comprehensive discussion on the limitations of their approach. Specifically acknowledging the complexity and potential computational overhead associated with implementing their method in practical largescale problems would provide a more balanced view. Furthermore addressing potential negative societal impacts especially those related to data privacy and the use of decentralized optimization in sensitive applications is crucial. Constructive suggestions include proposing mechanisms to safeguard against possible misuse and exploring the ethical implications of efficiency improvements in stochastic algorithms.", "yoLGaLPEPo_": "Review of \"Hyperbolic Feature Augmentation for Overcoming Overfitting in Hyperbolic Learning with Limited Data\" 1. Summary The paper introduces a novel approach for feature augmentation in hyperbolic spaces termed Hyperbolic Feature Augmentation (HFA) to address overfitting challenges in scenarios characterized by data scarcity. The motivation is drawn from the hierarchical structure inherent in many forms of realworld data (e.g. images graphs and languages) which hyperbolic spaces can encapsulate more effectively compared to Euclidean spaces. Specifically the paper proposes using a wrapped hyperbolic normal distribution coupled with a neural ordinary differential equations (ODE) approach that incorporates metalearning for feature augmentation. An upper bound for augmentation loss is derived allowing for theoretically infinite augmentations and hence improving model performance notably in fewshot and continual learning tasks without explicit hyperbolic operations. 2. Assessment Strengths:  Originality: The proposal to carry out feature augmentation directly in hyperbolic space is novel addressing the limitations of current augmentation methods confined to Euclidean space especially under the challenge of limited data.  Methodological Rigor: The introduction of a metaneuralODE scheme for distribution estimation and the derivation of an upper bound for augmentation loss are theoretically solid contributions enhancing the practicability and efficiency of the proposed method.  Practical Impact: Demonstrating significant performance improvements in fewshot and continual learning tasks illustrates the method\u2019s potential to enhance hyperbolic learning frameworks particularly beneficial for applications dealing with hierarchical data structures. Weaknesses:  Generalization across Data Types: The papers focus is somewhat limited to imagery and possibly graph data. It lacks explicit exploration into how the proposed method performs across a broader spectrum of data types leaving its versatility and adaptability to various realworld applications a bit unclear.  Exploration of Hyperparameter Sensitivity: There\u2019s scarce discussion on how sensitive the model performance is to the choice of hyperparameters including the configuration of the neural ODEs and metalearning components which might impact the ease of adoption and optimization in practical scenarios.  Comparison with StateoftheArt: While the experiments illustrate advancements over traditional hyperbolic and Euclidean augmentation methods a more exhaustive comparison including linear and nonlinear baseline models and recent advancements in nonEuclidean geometry learning would enhance validation. 3. Questions and Suggestions for the Authors  Generalization Capability: Can the authors provide additional insights or experimental results on how HFA performs across other hierarchical data types such as textual data or complex networks  Hyperparameter Sensitivity: It would be beneficial if the paper included a discussion or analysis on the sensitivity of the model performance to various hyperparameters potentially offering guidelines for practitioners.  Comparative Analysis: More comprehensive comparative analysis involving a broader set of benchmarks including nonhyperbolic methods could offer a clearer picture of where HFA stands in relation to the stateoftheart across different tasks and data regimes. 4. Limitations and Societal Impact The authors have touched upon the mathematical limitations and have described the computational efficiency of their method. However a deeper discussion on potential limitations related to the adaptability of the model to varied data complexities and structures would be informative. Moreover the societal impact particularly any potentially negative implications of misapplication or biases inherent in limited data scenarios is not addressed. Suggestions for Improvement:  Broader Data Consideration: Expand experiments or provide theoretical discussion on the methods adaptability to different data types and structures.  Limitation Discussion: A more elaborate discussion on potential limitations including scenarios where HFA might not perform as expected would offer a balanced view.  Societal Impact: A section discussing the potential societal impact including ways to mitigate negative implications such as enforcing data diversity in training to prevent bias would be a valuable addition.", "zvNMzjOizmn": "Review of Amortized Langevin Dynamics for Efficient Posterior Sampling in Deep Latent Variable Models 1. Brief Summary This paper introduces the Amortized Langevin Dynamics (ALD) a novel approach to improve the efficiency of Markov chain Monte Carlo (MCMC) for posterior sampling in deep latent variable models (DLVMs). Traditional MCMC methods including Langevin Dynamics (LD) suffer from slow convergence and the computational cost of datapointwise iterations. The authors propose replacing these iterations with updates of an encoder that directly maps observations to latent variables thus amortizing the cost over all datapoints. They provide theoretical proof that ALD maintains the validity of MCMC sampling under mild assumptions. Furthermore leveraging ALD the paper presents the Langevin Autoencoder (LAE) which improves upon traditional autoencoders and variational autoencoders (VAEs) in terms of test likelihood for the image generation task across several benchmarks. 2. Assessment of Strengths and Weaknesses Strengths:  Innovation: The concept of ALD that amalgamates amortization with Langevin dynamics to expedite and economize the posterior sampling process is innovative and addresses a wellknown limitation in DLVM literature.  Theoretical Justification: The paper does not simply propose a new method but supports it with solid theoretical backing (Theorem 1) proving that ALD can achieve the true posterior distribution as its stationary distribution under specific conditions.  Experimental Validation: Comprehensive experiments on toy datasets and challenging image datasets (MNIST SVHN CIFAR10 CelebA) convincingly demonstrate the efficacy of ALD and LAE over existing methods like VAEs and other MCMCbased approaches.  Simplicity of Implementation: The LAE derived from ALD can be implemented with modifications to traditional autoencoders suggesting ease of adoption in practical applications. Weaknesses:  Constraint on Encoder Structure: The authors mention a constraint on the structure of the encoder to meet the conditions for Theorem 1. This limitation may hinder the applicability of ALD in more complex DLVMs that involve hierarchical structures in latent variables.  Lack of Discussion on Computational Efficiency: While the amortization promises efficiency a detailed analysis comparing the computational cost in terms of both time and resources between ALD and classical MCMC methods is missing.  Potential Overfitting Concerns: The paper does not discuss how the proposed method might guard against overfitting especially in scenarios with highly flexible encoder networks.  Limited Discussion on Societal Impact: Though briefly mentioned the potential negative societal impacts specifically the misuse of deep generative models are largely unexplored. 3. Questions and Suggestions for the Authors  Encoder Architecture Details: Could the authors elaborate on the architectural constraints of the encoder mentioned and how they might affect the application of ALD to complex models with hierarchical latent structures  Comparison of Efficiency: It would be beneficial to include a quantitative comparison of computational efficiency (in terms of both processing time and memory usage) between ALDLAE and traditional MCMC and VAE methods.  Overfitting Mitigation Strategies: Are there specific strategies or regularization techniques recommended when employing ALD to prevent overfitting considering the flexibility of the encoder  Expanded Discussion on Societal Impact: Could the authors expand on the potential negative societal impacts of their work particularly concerning deepfake technology and suggest measures to mitigate these risks 4. Limitations and Societal Impact The authors have made an attempt to address the limitations and potential negative societal impacts of their work. However the discussion remains somewhat superficial. Specifically the constraint on the encoders structure which could limit the applicability of ALDLAE in advanced DLVM architectures demands a more thorough analysis. On the societal front while acknowledging the risks associated with more realistic image generation capabilities (e.g. deepfakes) the paper stops short of proposing concrete steps to mitigate these concerns. To strengthen this part the authors could explore algorithmic safeguards that could be baked into LAE to hinder malicious use or suggest policy and ethical guidelines for responsible usage.", "yCJVkELVT9d": "PeerReview: 1) Summary of the Paper: The paper conducts a comprehensive analysis of the robustness of several popular Graph Neural Network (GNN) defenses against adaptive attacks. The authors evaluated seven different defenses which are aimed at improving various aspects of GNNs such as the graph structure model architecture and training procedures. Their findings reveal that the majority of these defenses do not significantly improve the adversarial robustness of GNNs when compared to an undefended baseline. The work highlights the importance of evaluating GNN defenses against adaptive attacks to obtain more realistic estimates of their robustness. Furthermore the paper presents a novel collection of perturbed graphs that can be used as a robustness unit test for GNN models and outlines a systematic methodology for designing strong adaptive attacks. 2) Assessment of Strengths and Weaknesses: Strengths:  Comprehensive Evaluation: The thorough analysis covering a wide spectrum of defense strategies provides a valuable overview of the state of adversarial robustness in GNNs.  Methodological Rigor: The detailed methodology for designing adaptive attacks is a significant contribution offering clear guidelines for researchers to test the robustness of their defenses.  Practical Implications: The perturbed graphs dataset introduced as a robustness unit test is a practical tool for preliminary evaluation of new defenses potentially speeding up the research cycle.  Relevance: Given the increasing application of GNNs in critical domains the discussed topic of adversarial robustness is highly relevant and timely. Weaknesses:  Dataset Limitations: The evaluations primarily rely on small datasets (Cora ML and Citeseer) raising questions about scalability and applicability to larger more complex graphs.  Defensive Coverage: While the paper covers a spectrum of defenses it evaluates only seven in detail. This selection might not fully represent the diversity of strategies in the field.  Potential for Bias: The process of selecting defenses based on citation count and availability of code could introduce a bias towards certain types of defenses or those from more prominent research groups. 3) Questions and Suggestions for the Authors:  Dataset Scalability: Have the authors considered evaluating their methodology on larger more complex graph datasets Doing so could significantly enhance the generalizability of their findings.  Broader Defense Coverage: Could the authors provide some insights into how their adaptive attack methodology might be applied to other defense strategies not covered in this paper  Impact of Bias: How might the selection process for the defenses evaluated in this study influence the overall conclusions A discussion on this could enrich the paper\u2019s narrative. 4) Limitations and Societal Impact: The authors have made commendable efforts to elucidate the limitations concerning dataset scalability and the selectivity of defenses for evaluation. However a more extensive discourse on the broader implications of their findings particularly in terms of realworld applications of GNNs could be beneficial. Addressing the potential negative societal impacts especially in scenarios where overestimation of model robustness could lead to vulnerabilities in critical systems is necessary. Suggestions include integrating ethical considerations into the design of adaptive attacks and promoting transparency in reporting the adversarial robustness of GNN models to mitigate overconfidence in their security.", "kcQiIrvA_nz": "The paper under review addresses the topical issue of protecting the copyrights of opensourced datasets used to train Deep Neural Networks (DNNs). The authors critically analyze the limitations of existing dataset ownership verification methods which principally rely on poisononly backdoor attacks. They propose an innovative approach termed untargeted backdoor watermarking (UBW) which is designed to safeguard datasets without the determinism associated with previous methods. This method is explored under both poisonedlabel and cleanlabel settings. Through extensive experimentation on benchmark datasets the paper argues for the effectiveness of UBW in terms of implementation and resilience against backdoor defenses. Strengths: 1. Novel Approach: The paper introduces a novel concept of untargeted backdoor watermarking (UBW) for dataset copyright protection which is a significant departure from the deterministic nature of existing backdoor watermark methods. 2. Comprehensive Validation: The authors provide extensive experimental validation across benchmark datasets which underscores the feasibility and robustness of the proposed UBW method. The opensource availability of the codes further aids in replicability and scrutiny by the academic community. 3. Theoretical Contribution: Beyond empirical results the paper also delves into theoretical underpinnings with proofs on the correlation between averaged samplewise and averaged classwise dispersibility. This adds depth to the methodological rigor. 4. Practical Relevance: Given the increasing concerns over dataset copyright issues the paper addresses a highly relevant problem. Its solutions could have practical implications in safeguarding intellectual property in the era of opensource datasets. Weaknesses: 1. Limitation in Scope: The focus is primarily on image classification tasks. The generalizability of UBW to other types of data or machine learning tasks (e.g. natural language processing time series analysis) remains unexplored. 2. Potential for Evasion: Though the paper presents UBW as resilient against known backdoor defenses theres limited discussion on the capability of adaptive adversaries in circumventing UBW especially with advancements in attack methodologies. 3. Complexity in Implementation: The implementation of UBW especially under the cleanlabel setting involving bilevel optimization might present practical challenges in terms of computational resources and expertise required. 4. Underaddressed Ethical Considerations: While the paper mentions the potential negative societal impacts briefly theres a lack of a thorough discussion on ethical implications especially concerning the misuse of untargeted backdoors for malicious purposes. Questions and Suggestions for Authors: 1. Extension to Other Domains: Can the authors elaborate on potential adaptations of UBW to other machine learning tasks beyond image classification Are there inherent limitations in the approach that might restrict its applicability 2. Resistance to Adaptive Adversaries: How might UBW fare against adversaries who specifically tailor their attacks to bypass untargeted backdoor watermarks Any thoughts or preliminary findings on this would be valuable. 3. Implementation Details: Could the authors provide more insights on the computational overhead introduced by implementing UBW particularly under the cleanlabel setting Suggestions on making the approach more accessible to practitioners with limited resources would be helpful. 4. Ethical Safeguards: Given the resistance of UBW to existing backdoor defenses what measures do the authors suggest to prevent its misuse A detailed ethical discussion on this aspect would enrich the paper. Limitations and Potential Negative Societal Impact: The authors have made a commendable effort in addressing the limitations and potential negative societal impacts of their work. However the discussion could be deepened to include measures for preventing the potential misuse of UBW technology especially by adversaries for illicit purposes. Constructive suggestions for such preventive measures would enhance the papers contribution to ethical AI research and practice. Additionally a more detailed exploration of the methods applicability beyond image classification and consideration of the computational barriers to implementation would provide a clearer picture of the UBWs broader impact and limitations.", "uloenYmLCAo": "The paper introduces the BlockRecurrent Transformer a novel architecture designed to address the challenge of processing long sequences in language modeling tasks. This approach maintains the advantages of transformers such as parallel processing during training while integrating a recurrent cell that operates on blocks of tokens to efficiently manage sequence length. The key contributions include demonstrating the capability to process long sequences with linear complexity providing an innovative recurrent cell design that operates in blocks and empirically showing significant improvements in language modeling perplexity across three datasets: PG19 arXiv and GitHub source code.  Strengths: 1. Innovative Design: The BlockRecurrent Transformer innovates by combining the strengths of attention mechanisms with recurrence effectively addressing the transformers limitation with long sequences. This innovation allows for efficient parallel computations within blocks and maintains a large set of state vectors for capturing context. 2. Empirical Validation: The models effectiveness is empirically validated across diverse complex datasets including books academic papers and source code. This widespread testing demonstrates the models robustness and versatility. 3. Linear Complexity: The approach achieves linear complexity with respect to sequence length a significant improvement over traditional transformer models offering both faster training times and improved perplexity. 4. Open Source Contribution: Releasing the implementation as open source allows for broad accessibility and encourages further research and development on the foundation laid by this work.  Weaknesses: 1. Complexity and Intuitiveness: While achieving significant improvements the recurrent cells design adds complexity to the model architecture which could challenge understanding and further innovation without deep technical knowledge. 2. Limited Evaluation on Downstream Tasks: The paper focuses on language modeling perplexity with less emphasis on how the improvements translate to performance on downstream tasks that require longrange context understanding. 3. Model Initialization and Training Stability: The model requires careful initialization and gate configuration to avoid common failure modes indicating potential fragility in the training process that may require additional tuning and optimization efforts.  Questions and Suggestions for Authors: 1. Downstream Task Performance: Could the authors expand on the potential performance of the BlockRecurrent Transformer in downstream tasks especially those requiring the understanding of longrange contexts 2. Comparative Analysis: How does the models performance and efficiency compare to other recent advances aiming to address the transformers limitations with long sequences such as Performers or Linformers 3. Scalability: The paper demonstrates improved performance with increasing model size. Could the authors comment on the implications for scaling this model further 4. Training Stability: Could the authors provide more insights into the initialization techniques and training strategies that help ensure model stability particularly for different datasets or tasks  Limitations and Societal Impact: The authors briefly touch on the potential negative impacts of their work acknowledging that advancements in language modeling could be misused for creating disinformation spam or malicious content. However a more detailed discussion on mitigating these risks perhaps by incorporating mechanisms for identifying and flagging harmful content would strengthen the paper. Additionally exploring the environmental impact of training increasingly large models especially with respect to energy consumption and carbon footprint would provide a more comprehensive view of the works societal implications. Creative solutions or guidelines for balancing performance improvements with ethical and environmental considerations would be valuable contributions for future research directions."}