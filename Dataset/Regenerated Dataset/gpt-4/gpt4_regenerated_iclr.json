{"gFDFKC4gHL4": " Peer Review:  1) Summary of the Paper: This paper addresses the challenge of continuously changing Machine Learning (ML) prediction APIs which can alter their performance over time due to model updates by the service providers. The focus is on a methodical assessment of these changes through a quantifiable measure\u2014differences in the confusion matrix over time. To efficiently estimate these shifts within a constrained sample budget the authors introduce MASA (ML API Shift Assessment) an algorithmic framework that employs adaptive sampling strategies. Through empirical analysis on realworld cases MASA demonstrates superior efficiency in estimating API performance shifts notably improving the accuracy of the estimation with fewer samples compared to standard sampling approaches. The study substantiates the prevalence of significant performance shifts in commercial ML APIs across various applications underlining the necessity for systematic shift assessment. Additionally the authors aim to foster further research by releasing a substantial dataset annotated with ML API predictions.  2) Strong and Weak Points Questions and Feedback: Strong Points: a. Practical Relevance: The topic of assessing performance shifts in ML prediction APIs is highly relevant in todays AIdriven applications. The paper taps into a critical challenge of maintaining application reliability amidst silent API updates. b. MASA Framework: The introduction of MASA as a structured approach to estimate changes in APIs confusion matrices is commendable. Its capacity to reduce the required sample size for accurate shift estimation is a substantial contribution to efficient resource utilization. c. Empirical Validation: The paper does not just propose a theoretical model but also validates it with extensive experiments on realworld data showing MASAs effectiveness over traditional sampling methods. Weak Points: a. Generalizability: The application of MASA primarily focuses on classification tasks. The paper acknowledges this limitation but it leaves a gap in understanding how the proposed method could be adapted or extended to other types of ML tasks e.g. regression or sequence prediction tasks. b. Sensitivity Analysis: While the paper thoroughly discusses the algorithm\u2019s performance it lacks a detailed sensitivity analysis on how hyperparameters (like sample size) and model changes impact the estimation accuracy. c. Comparison with Existing Methods: The paper could benefit from a more explicit comparison with existing methods (beyond uniform and stratified sampling) for assessing API shifts particularly those that might employ different models for change detection. Questions and Feedback: 1. Can the authors elaborate on the potential extension of MASA to nonclassification tasks 2. How sensitive is MASA to the choice of hyperparameters and what guidelines can be provided for their selection 3. It would be enriching to include a discussion on how MASA copes with cases of extreme shifts in API performance especially when the shifts might render previously reliable partitions irrelevant.  3) Review Summary and Recommendation: The paper addresses an underexplored yet critical aspect of ML deployment\u2014assessing the performance shifts in ML APIs over time. By proposing MASA a novel and principled algorithmic framework for efficient shift assessment the authors make a significant contribution towards understanding and mitigating the impacts of these shifts on downstream applications. The empirical validation presents a strong case for MASAs effectiveness and efficiency highlighting its practical utility. Nevertheless the paper could be strengthened by expanding the scope beyond classification tasks deeper analysis of the methods sensitivity to various factors and comparison with a broader spec of existing methodologies. Given the papers relevance the novelty of MASA and the substantiated empirical findings I recommend acceptance with minor revisions. The suggested enhancements focused on generalizability sensitivity analysis and expanded comparison with existing methods would further solidify the paper\u2019s contributions and utility to the field.", "tgcAoUVHRIB": " Peer Review of \"Neural NetworkBased Framework for Logical Reasoning over Knowledge Graphs\"  1. Summary of the Paper The authors of this paper propose a novel framework for answering multihop logical queries on Knowledge Graphs (KGs) using neural networkbased embeddings. The work addresses the challenge of reasoning over large and incomplete knowledge graphs by introducing models that utilize onepoint vector embeddings to represent both the queries and the entities in the KGs. The framework is versatile enough to handle FirstOrder Logical (FOL) queries including those that involve negative queries by employing neural networks to implement logical operations such as conjunction disjunction and negation. The authors demonstrate through experiments on benchmark datasets (FB15k FB15k237 NELL) that their models not only achieve superior performance compared to existing stateoftheart methods but also significantly improve upon the previous approach that used similar singlepoint vector embeddings. The main contribution of this work lies in the development of new embeddingbased methods for logical reasoning over KGs and handling negative queries effectively.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Innovative Approach: The use of neural networks to create a framework capable of processing full FOL queries including negative queries is novel and innovative. It advances the field of KG reasoning by providing a more flexible solution compared to previous methods limited by the capabilities of specific logical operations. b. Significant Performance Improvement: The reported performance improvement over stateoftheart methods is impressive. Achieving a 10 relative increase over the bestperforming method and more than a 30 improvement over the original method based on singlepoint vector embeddings substantiates the effectiveness of the proposed models. c. Robust Experimentation: The authors have conducted extensive experimentation using wellknown datasets which adds credibility to their claims. The methodology for experimentation appears to be solid and well thought out providing a strong foundation for the results presented. Weak Points: a. Complex Implementation: While the paper describes the models and their theoretical foundations it does not provide detailed insights into the practical challenges of implementing the proposed framework. Future users and researchers might benefit from a discussion on implementation details complexities and potential pitfalls. b. Limited Discussion on Scalability: The paper lacks a detailed exploration of the scalability of the proposed models. Given the complexity and size of realworld KGs it would be beneficial to understand how the models perform in highly scalable environments and what limitations might exist. c. Absence of Comparative Analysis: While the paper reports improved performance over existing methods it lacks a deeper comparative analysis explaining why the proposed models outperform others. Understanding the specific features or characteristics that contribute to this superior performance could provide valuable insights for further research. Questions and Feedback: 1. Could the authors provide more details on the practical challenges encountered when implementing the proposed models Any insights on overcoming these challenges would be valuable. 2. It would be beneficial if the authors could elaborate on the scalability of their models. How do the models perform as the size of the knowledge graphs increases and are there any observed limitations 3. A deeper comparative analysis of the proposed models against existing methods would enrich the paper. What specific features of the neural networkbased implementation contribute to the observed performance improvement  3. Review Summary The paper presents a significant advancement in the field of artificial intelligence specifically in reasoning over knowledge graphs using neural networkbased embeddings. The proposed framework stands out for its ability to process full FOL queries including handling negative queries and demonstrates a notable improvement in performance over existing stateoftheart methods. The experimental validation of the models using benchmark datasets further substantiates the effectiveness of the proposed approach. However for the paper to provide a more comprehensive contribution it would benefit from addressing the practical aspects of model implementation discussing scalability in more detail and providing a deeper comparative analysis with existing methods. Despite these areas for enhancement the papers innovative approach and substantial experimental evidence strongly support its significance in advancing logical reasoning over knowledge graphs. The potential impact on questionanswering systems search engines and recommender systems among others is considerable making this work a valuable addition to the body of research in artificial intelligence and knowledge graph reasoning.", "nCw4talHmo5": " Review of \"Parallelly Distributable Slimmable Neural Networks (ParaDiS)\"  1. Summary of the Paper This paper introduces a novel framework named parallelly distributable slimmable (ParaDiS) neural networks aimed at efficiently distributing neural network inference across multiple devices with varying capacities without the need for retraining. The key innovation of ParaDiS lies in its ability to split a neural network into several subnetworks (or switches) that can be parallelly distributed across different devices which then jointly contribute to the final task outcome by fusing their results. This approach builds on the concept of slimmable networks but extends it to enable parameter sharing across different network configurations distributable on multiple devices. The paper evaluates the ParaDiS framework using MobileNet v1 and ResNet50 architectures for the ImageNet classification task and WDSR architecture for image superresolution demonstrating comparable or superior performance to individually trained distributed configurations and universally slimmable network models.  2. Strengths and Weaknesses Strengths:  Innovative Approach: The concept of ParaDiS networks addresses a critical problem in deploying neural networks on edge devices with varying computational capabilities offering a practical solution that requires no retraining.  Comprehensive Evaluation: The paper provides a detailed experimental analysis comparing the ParaDiS framework to existing approaches across different models and tasks showcasing its effectiveness.  Methodological Rigor: The training procedure for ParaDiS networks introduces novel modifications to the knowledge distillation process which appear to be key contributors to the performance gains observed. Weaknesses:  Limited Scalability Discussion: While the paper touches on scalability towards the end it does not fully explore the potential limitations or challenges in scaling ParaDiS networks to a large number of devices or switches.  Potential Device Heterogeneity Overlooked: The practical deployment scenarios might involve not just varying computational capacities but also different types of devices with varied hardware architectures. The impact of these factors on the performance of ParaDiS networks isnt discussed. Questions and Feedback to the Author: 1. Scalability Beyond Four Devices: The paper mentions the potential for scaling ParaDiS networks beyond the four switches used in the experiments but doesnt dive deeply into this aspect. How do the authors envisage the scalability of this framework in scenarios with a considerable number of heterogeneous devices 2. Impact of Device Heterogeneity: Could the authors comment on how different hardware architectures across devices might impact the performance and efficiency of the ParaDiS framework 3. Energy Efficiency Consideration: While processing latency and communication load are considered theres no mention of energy consumption. Can the authors provide insights on how ParaDiS networks compare to other approaches in terms of energy efficiency especially when deployed on batterypowered edge devices  3. Review Summary The paper \"Parallelly Distributable Slimmable Neural Networks\" presents a valuable advancement in the deployment of neural networks on devices with limited and varying computational resources. The ParaDiS framework is both innovative and practical addressing a genuine need for flexibility in edge computing scenarios. The experimental results convincingly demonstrate the frameworks capability to achieve high performance without retraining for different device configurations. However the paper could benefit from a more thorough discussion on scalability and the practical implications of device heterogeneity. Moreover incorporating considerations regarding energy efficiency could significantly enhance the frameworks applicability in realworld scenarios especially for batteryoperated IoT devices. Given the quality of the research the relevance of the problem addressed and the potential impact of this work I recommend the paper for publication contingent upon addressing the discussed limitations particularly around scalability and device heterogeneity. These improvements could broaden the understanding and applicability of the ParaDiS framework making it a cornerstone in research on neural network deployment on edge devices.", "tzO3RXxzuM": " Summary The paper presents a comprehensive study on generalizing noisy stochastic minibatch based iterative algorithms focusing on stabilitybased generalization bounds. The authors significantly extend the understanding and applicability of these bounds by introducing three technical advances: 1) offering generalization error bounds for noisy stochastic iterative algorithms that depend on expected stability which itself is bounded by expected Le Cam Style Divergence (LSD) resulting in an improved sample dependence 2) the introduction of Exponential Family Langevin Dynamics (EFLD) which generalizes stochastic gradient Langevin dynamics (SGLD) by incorporating exponential family noise and 3) providing datadependent expected stabilitybased generalization bounds for EFLD and establishing optimization guarantees for noisy signSGD a variant extended with Bernoulli noise. The presented empirical results on benchmark datasets underline that the proposed bounds are quantitatively sharper and nonvacuous compared to existing bounds.  Strong Points 1. Novel Framework and Generalization of SGLD: The introduction of Exponential Family Langevin Dynamics (EFLD) and the general framework for developing stabilitybased generalization bounds are significant contributions. They provide flexibility in incorporating various noise types into stochastic gradient descent enhancing the practicality and applicability of the algorithms in different settings. 2. Improved Sample Dependence: The shift from a common O(1\\xe2\\x88\\x9an) to a more favorable O(1n) sample dependence for the generalization error bounds is a notable improvement. This theoretical advancement potentially translates to enhanced performance in realworld scenarios where sample sizes can be a limiting factor. 3. Empirical Validation: The empirical experiments conducted offer valuable insights into the practical effectiveness of the proposed theoretical contributions. Demonstrating that the bounds are nonvacuous and sharper than existing ones through empirical evidence significantly strengthens the papers claims.  Weak Points 1. Comparative Analysis: While the paper presents a comparison of its results with existing bounds a more detailed comparative analysis including a broader range of datasets and settings could further validate the proposed methods superiority and applicability. 2. Implementation Details: The paper could benefit from a more detailed description of the implementation details for the EFLD and noisy signSGD algorithms. This would aid in reproducibility and allow for a better assessment of the practical difficulties and complexities involved in applying the proposed approaches. 3. Theoretical Assumptions: Some of the theoretical advances rely on assumptions (e.g. datadependent scaling for the EFLD) that may not always be easy to satisfy in practice. A more indepth discussion on the implications and feasibility of these assumptions in realworld applications would be beneficial.  Questions and Feedback to the Author 1. Broader Applicability: Can the authors elaborate on the potential applicability of their proposed framework and bounds to other types of learning algorithms beyond the ones discussed in the paper 2. Assumptions and Limitations: Could the authors discuss the limitations imposed by the assumptions made in their theoretical analysis and how these might affect the practical implementation of their algorithms 3. Scalability: How do the proposed EFLD and noisy signSGD algorithms scale with the dimensionality of the data and the complexity of the model Are there scenarios where the proposed approach may not be as effective  Review Summary This paper makes several significant contributions to the understanding and application of stabilitybased generalization bounds for noisy stochastic iterative learning algorithms. The introduction of EFLD and the advancement in sample dependence of the generalization error bounds are both noteworthy and represent important steps forward in the field. The empirical results strongly support the theoretical claims showing that the proposed bounds are indeed nonvacuous and sharper than existing bounds. However the paper could be strengthened by addressing the weak points mentioned especially by providing a more detailed comparative analysis elaborating on implementation details and discussing the practical implications of the theoretical assumptions. Overall the paper is a valuable contribution to the literature offering new directions for future research and potential improvements in the performance of stochastic iterative learning algorithms.", "j3krplz_4w6": " Review of \"TEXTEXPLANATIONFOOLER: Assessing the Robustness of Text Classification Explanations\"  1. Summary of the Paper This paper introduces TEXTEXPLANATIONFOOLER (TEF) a novel adversarial attack algorithm aimed at exploring the robustness of explanation methods in text classification models. The authors argue that similar to vision applications text classification models are susceptible to perturbations that significantly alter model explanations without affecting the predicted outcomes. TEF operates by making imperceptible alterations to text inputs deceiving widelyused explanation methods while keeping the classifiers predictions unchanged. The methodologys effectiveness is evaluated across five text classification datasets various deep neural network (DNN) architectures including three transformer models and three explanation methods. The authors demonstrate that all tested models and explanations are vulnerable to TEF perturbations. Additionally they explore the transferability of these perturbations across different models and explanation methods and introduce a semiuniversal attack that requires no knowledge of the classifier or explanation method. The findings underscore the fragility of explanations in text classification and stress the need for robustness in explanation methods before deployment in critical applications.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The paper addresses an understudied but critical aspect of machine learning interpretability  the robustness of explanation methods in text classification. By introducing TEF the paper fills a significant gap in assessing explanation methods vulnerability to adversarial attacks in text domains.  Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed methodology across multiple datasets model architectures and explanation methods strengthening the validity of the findings.  Practical Relevance: Given the increasing reliance on machine learning models especially in critical domains like healthcare the robustness of explanation methods is vital. This paper brings attention to this issue emphasizing the importance of reliable interpretative methods for realworld applications. Weak Points:  Lack of Solution or Mitigation Strategies: While the paper successfully identifies and demonstrates the vulnerability of explanation methods to adversarial perturbations it stops short of suggesting or evaluating potential solutions or mitigation strategies.  Potential for Dataset Bias: The evaluation is confined to five text classification datasets. Although these datasets are widely used the results might not generalize across other datasets particularly those with different characteristics or in different languages.  Complexity and Accessibility of Proposed Attack: The complexity of the TEF attack might pose challenges for practitioners in understanding and assessing the robustness of explanation methods. Simplifying the approach or providing tools for easier assessment could increase the papers practical impact. Questions and Feedback:  Mitigation Strategies: Could the authors suggest potential strategies or directions for developing more robust explanation methods capable of resisting such adversarial perturbations  Impact on Model Performance: Is there any observation or insight regarding the impact of TEF perturbations on the overall performance and reliability of the text classification models beyond the explanation methods  Extension to Other Domains: Do the authors anticipate that the TEF methodology could be adapted or applied to other domains outside text classification such as time series or audio signal processing  3. Review Summary This paper makes a significant contribution to the field of machine learning interpretability by highlighting the vulnerability of explanation methods in text classification models to adversarial perturbations. The introduction of TEXTEXPLANATIONFOOLER is both innovative and timely given the increasing deployment of AI systems in critical domains. The comprehensive evaluation across multiple datasets model architectures and explanation methods substantially supports the authors claims about the fragility of current explanation techniques. However the paper could be strengthened by addressing the lack of proposed solutions or mitigation strategies for enhancing the robustness of explanation methods. Additionally expanding the evaluation to a more diverse set of datasets and simplifying the TEF methodology could further augment its relevance and accessibility to practitioners. In summary this paper successfully brings to light an important yet often overlooked aspect of AI interpretability and sets the stage for future research aimed at developing more resilient explanation methods. I recommend this paper for publication provided the authors can address the current limitations around proposing mitigation strategies and potentially simplifying the methodology for broader adoption.", "q7n2RngwOM": "Review of the Paper on Treatment Effects Estimation under Limited Overlap Using \u03b2IntactVAE 1. Summary The paper addresses the issue of estimating individualized treatment effects (TEs) when there is limited overlap in covariates across treatment groups. This situation poses a challenge for causal inference as certain feature combinations are only present in one of the treatment groups making traditional estimation techniques less reliable. The solution proposed in this work centers around the use of a prognostic score modeled as a latent variable to enable the estimation of TEs even in the presence of limited covariate overlap. The authors introduce a new variational autoencoder (VAE) architecture termed \u03b2IntactVAE capable of recovering prognostic scores and ensuring balanced representation for TE estimation. Through theoretical analysis and empirical comparisons with stateoftheart methods on (semi)synthetic datasets the paper demonstrates the efficacy of the proposed approach in identifying and estimating TEs under the challenging condition of limited overlap. 2. Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Foundation: The paper is theoretically solid providing clear definitions and propositions that underpin the methodological contributions. The formalization of identification under the generative prognostic model and the recovery of prognostic scores are particularly compelling.  Innovativeness: Introducing \u03b2IntactVAE which combines identifiable representation learning and causal inference presents a novel approach to address the issue of limited overlap in TE estimation.  Empirical Evaluation: The comprehensive experimental setup and analysis especially the comparison with stateoftheart methods effectively demonstrate the methods superiority in handling limited overlap conditions. Weak Points:  Complexity and Scalability: The paper does not thoroughly discuss the computational complexity and scalability of \u03b2IntactVAE. Given the sophisticated architecture and the potential for highdimensional data in causal inference insights into these aspects would be valuable.  Generalizability: It is unclear how generalizable the proposed approach is across different domains or types of observational data that may have varied forms of confounding and biases.  Limited Discussion on Practical Implementation: Despite providing code in Supplementary Material the paper lacks discussion on practical challenges that practitioners might face when implementing the proposed method such as sensitivity to hyperparameter settings and data preprocessing requirements. Questions and Feedback: 1. Can the authors comment on the computational complexity of \u03b2IntactVAE and its scalability to highdimensional datasets 2. How robust is the proposed method to deviations from the assumptions made in the paper particularly regarding the datagenerating process and noise models 3. The paper would benefit from a more detailed discussion about the selection of hyperparameter \u03b2 including strategies for tuning and its impact on performance across different scenarios. 3. Review Summary The paper introduces a novel method for estimating treatment effects under the challenging condition of limited overlap in covariates using a new VAE architecture \u03b2IntactVAE. Theoretical analyses underline the methods ability to recover prognostic scores essential for identifying individualized TEs and empirical results demonstrate its effectiveness compared to existing methods. While the paper stands out with its theoretical rigour and innovative approach it could be strengthened by addressing computational complexity scalability issues and providing more guidance on practical implementation aspects. Overall the work makes a significant contribution to the causal inference literature particularly in scenarios where traditional methods face limitations due to covariate imbalance.", "ieNJYujcGDO": " Peer Review on \"Exploring the Impact of Mixup Training on Classification Models\"  1. Summary of the Paper The presented paper delves into the theoretical underpinnings of Mixup training\u2014a technique where models are trained on convex combinations of data points and their labels\u2014to uncover why and when it enhances generalization and robustness in classification tasks. Specifically the authors investigate the conditions under which Mixup training can minimally affect the original empirical risk and how it can lead to classifiers that are either superior or equivalent in terms of generalization when compared to models trained via conventional empirical risk minimization (ERM). Through mathematical derivations they identify scenarios where Mixup can fail to minimize the empirical risk and propose sufficient conditions under which it succeeds. They also explore the margin benefits of Mixup classifiers providing insights into their superior generalization capabilities. Their findings are substantiated with theoretical proofs and empirical experiments contributing significantly to the understanding of Mixups role in model training dynamics.  2. Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Depth: The paper presents an indepth theoretical analysis of Mixup training contributing novel insights into its effects on classification models. By offering a mathematical framework to understand when Mixup training behaves similarly to or diverges from ERM the work significantly advances the theoretical knowledge in this area.  Practical Relevance: The exploration of conditions under which Mixup can fail or succeed is highly relevant for practitioners looking to apply this method. It aids in more informed decisionmaking regarding when to employ Mixup training for optimal model performance.  Comprehensiveness: The coverage of both failure and success scenarios along with margin analysis and conditions for generalization equivalence provides a wellrounded perspective on Mixup training making the paper valuable for both theoreticians and practitioners. Weak Points:  Empirical Validation: While the paper includes experiments to validate certain theoretical predictions the empirical studies could be expanded to include a broader range of datasets and model architectures. Doing so would strengthen the claims about Mixups robustness and generalization benefits.  Complexity and Accessibility: The heavy reliance on mathematical derivations and theoretical language might limit the papers accessibility to a broader audience. Simplifying the presentation or providing more intuitive explanations could make the powerful insights more accessible to nontheoretical researchers.  Discussion on Practical Implications: The paper could benefit from a more detailed discussion on the practical implications of the findings specifically how practitioners can leverage the insights to improve model training in realworld scenarios. Questions and Feedback:  Could the authors elaborate on how model architectures might influence the effectiveness of Mixup training Are there characteristics of models that make them more amenable to the benefits of Mixup  In the experiments conducted was the impact of different mixing distributions (beyond Beta distribution) on the findings explored If not do the authors believe different distributions could present diverging outcomes  The paper briefly mentions the potential conflict between Mixup training points and original data points at high values of \u03b1. Could this be explored in further depth to provide more granular guidance on selecting \u03b1 values  3. Review Summary The paper provides a comprehensive theoretical analysis of Mixup training identifying its potential to improve or replicate the generalization capabilities of empirical risk minimization. Theoretical findings are well supported through mathematical derivations and complemented by empirical evidence making a significant contribution to the understanding of Mixup training in classification tasks. However expanding the empirical validation across more datasets and models simplifying the presentation for broader accessibility and elaborating on practical implications could enhance the papers impact. Overall the strengths of the paper lie in its detailed theoretical exploration of Mixup training offering valuable insights for both researchers and practitioners in machine learning. My recommendation would lean towards acceptance given the paper\u2019s substantial contributions to the theoretical understanding of Mixup training provided that the authors address the identified opportunities for improvement.", "vUH85MOXO7h": " Review of \"Tree Neural Tangent Kernel: Understanding the Behavior of Infinite Ensembles of Soft Trees\"  Summary The paper presents a comprehensive theoretical framework named Tree Neural Tangent Kernel (TNTK) to analyze and understand the behavior of infinite ensembles of soft trees. Soft trees as the authors describe are a kind of decision tree trained using a gradient method where splitting rules are formulated in a differentiable form. This work bridges the gap in theoretical understanding of soft tree ensembles by extending the Neural Tangent Kernel (NTK) concept heretofore predominantly applied to neural networks to tree ensembles. The authors posit that by conceptualizing an ensemble of infinitely many soft trees the TNTK can reveal several nontrivial properties of such ensembles including global convergence the equivalence of oblivious tree structures and the degeneracy resulting from deepened trees. Through rigorous theoretical analysis coupled with numerical experiments on realworld datasets the paper demonstrates these properties and compares the performance of the TNTK with the NTK induced by multilayer perceptrons (MLPs) highlighting situations where TNTK outperforms or is equivalent to the latter.  Strong Points 1. Novelty and Theoretical Contribution: The extension of the NTK concept to tree ensemble models is novel and represents a significant theoretical contribution. This work provides an analytical foundation to understand the behavior of soft tree ensembles a domain previously lacking in theoretical analysis. 2. Comprehensive Analysis: The authors present a thorough theoretical analysis supported by mathematical proofs of the TNTKs properties. Each theorem is wellexplained providing clear insights into the implications for soft tree ensembles. 3. Practical Relevance: The equivalence of oblivious tree models in the limit of infinite trees provides theoretical backing for the empirical success of such structures. Additionally the study touches on the practical issue of tree depth showing how deeper trees lead to degeneracy which aligns with observations in realworld applications. 4. Numerical Experiments: The experimental validation using realworld datasets adds empirical weight to the theoretical findings. The comparison between TNTK and MLPinduced NTK showcases the practical implications and potential computational benefits of the TNTK.  Weak Points 1. Exploration of Model Limitations: While the paper effectively demonstrates the advantages and theoretical properties of the TNTK it could benefit from a deeper discussion on the limitations or potential drawbacks of using the TNTK especially in comparison with other models. 2. Broader Applicability: The focus is primarily on theoretical contributions and validation through numerical experiments limiting discussions on how these findings could be applied or extended to practical machine learning challenges beyond those directly related to decision trees or neural networks. 3. Clarity and Accessibility: Some sections especially those involving complex mathematical derivations might benefit from further simplification or additional explanatory notes to make the content more accessible to readers not deeply familiar with the theoretical underpinnings of the NTK or soft tree models.  Questions and Feedback to the Author 1. Could the authors elaborate on potential limitations of the TNTK approach Are there specific scenarios or dataset characteristics where TNTK might not offer significant advantages or could even perform poorly 2. How might the insights derived from the TNTK be used to improve the practical deployment of soft tree ensembles Are there specific guidelines or considerations that practitioners should keep in mind when utilizing these models 3. The discussion on degeneracy induced by deep trees is intriguing. Could the authors provide more insights into potential strategies to mitigate this degradation in performance particularly in scenarios where depth cannot be easily compromised  Review Summary This paper makes a notable theoretical advancement in the understanding of soft tree ensembles by introducing the Tree Neural Tangent Kernel (TNTK). It elegantly bridges a significant gap in the literature providing a robust framework to analyze the properties of infinite ensembles of soft trees. The theoretical contributions are wellsupported by mathematical proofs and numerical experiments underscoring the TNTKs practical implications and computational superiority in certain contexts over the MLPinduced NTK. Despite the strengths of the work further discussions on the limitations of the TNTK and its broader applicability to practical machine learning tasks would greatly enhance the papers impact. Additionally making the complex theoretical content more accessible could broaden the audience reach. Overall this paper represents a significant step forward in the understanding of tree ensemble models and opens new avenues for further research in this important area.", "gccdzDu5Ur": " Review of \"Leveraging Feature Priors for Model Generalization\"  Summary of the Paper This paper delves into the methodology of improving model generalization by constraining the features models rely on employing feature priors as a vehicle for this purpose. Recognizing that feature diversity among models can yield less overlapping failure modes the authors propose a framework for combining such models to enhance generalization especially in scenarios plagued with unreliable data such as those containing spurious correlations. The paper is articulate in demonstrating how models trained on distinct feature priors\u2014specifically shape and texture in image classification\u2014can benefit from each others insights through a cotraining approach on unlabeled data. This method not only corrects individual model inaccuracies but also steers models away from relying on spurious correlations thereby aligning with the true distribution of interest.  Strengths and Weaknesses Strong Points: 1. Novel Framework: The paper introduces an innovative approach combining different feature priors through cotraining presenting a compelling argument for its efficacy in enhancing model generalization and reducing dependency on spurious correlations. 2. Comprehensive Evaluation: The experiments are thorough and welldocumented spanning different datasets (CIFAR10 STL10 CelebA) and settings (synthetic and realworld) providing a robust base for the papers claims. 3. Valuable Insights into Feature Diversity: By fostering diversity among model features the paper offers significant insights into how models can complement each others strengths a concept pivotal for developing robust AI systems. Weak Points: 1. Limited Prior Types: The paper primarily investigates shape and texture priors which while providing clear insights leaves the exploration of other types of feature priors (e.g. color semantic content) less addressed. 2. Potential Scalability Issues: Though cotraining displays benefits in controlled settings the paper could elaborate more on the scalability of this approach particularly in larger datasets or more complex models beyond image classification. 3. Generalization Beyond Image Classification: The focus is predominantly on image classification tasks. Expanding the scope to include other types of tasks (e.g. text audio) could vastly enrich the papers impact and applicability. Questions and Feedback for the Author: 1. Expansion to Other Feature Priors: Could the authors speculate or provide preliminary insights into how other forms of feature priors might interact within the proposed framework 2. Scalability Considerations: Have any scalability tests been performed particularly for datasets of larger scale or complexity If so it would be beneficial to include these insights. 3. Extension to Other Domains: How do the authors envision applying their methodology to domains outside image classification Are there specific challenges or modifications anticipated  Review Summary This paper presents a commendable step forward in leveraging feature priors to improve model generalization. Its innovative approach through combining diverse feature priors via cotraining holds significant promise especially in mitigating the detrimental effects of spurious correlations. The findings are supported by rigorous experiments and provide a strong foundation for future work in this domain. However the investigation into types of feature priors is somewhat narrow focusing mainly on shape and texture. Expanding this spectrum could unveil further nuances in how different feature priors can be optimized. Additionally considerations around scalability and applicability across other domains remain areas ripe for exploration. In essence this paper contributes valuable insights into model generalization strategies emphasizing the importance of feature diversity. With further expansion in its scope and application the proposed framework has the potential to significantly influence how we approach the design and training of generalized models across various domains of AI.", "xKZ4K0lTj_": " Peer Review of \"Fewshot Imitation with Skill Transition Models (FIST): A semiparametric approach to solve unseen longhorizon tasks\"  1. Summary The paper introduces Fewshot Imitation with Skill Transition Models (FIST) an innovative algorithm aimed at bridging the gap in generalizing autonomous agents to execute unseen longhorizon tasks through imitation learning. By leveraging an offline dataset of diverse task demonstrations FIST extracts behavioral skills and utilizes these to adapt to new tasks with minimal demonstrations. It incorporates an inverse skill dynamics model to select appropriate skills based on both current and future state predictions coupled with a distance function to prevent deviation from the demonstrated task trajectory. The algorithms effectiveness is validated through experiments in simulated navigation and robotic manipulation environments where it shows significant improvements over existing baselines in generalizing to unseen tasks.  2. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Methodology: FIST introduces a novel integration of skill extraction with fewshot imitation learning attempting to address the generalizability of robotic tasks in a more efficient manner. The semiparametric approach to evaluate skills in fewshot imitation is particularly commendable. 2. Comprehensive Evaluation: The paper presents a thorough evaluation across navigation and manipulation tasks demonstrating FISTs ability to generalize to new tasks effectively. The comparison with existing methods (BCFT and SPiRL) and the inclusion of ablation studies strengthen the validity of the claimed improvements. 3. Clarity and Structure: The paper is wellorganized with a clear progression from problem formulation to methodological development experimental validation and conclusion. It articulates the novelty and significance of FIST coherently enhancing reader comprehension. Weak Points: 1. Limited Discussion on Limitations: While the algorithm excels in the tested environments the paper could benefit from a more indepth discussion on potential limitations such as the quality of offline datasets required and scalability to more complex realworld tasks. 2. Dependence on Quality of Demonstrations: The performance of FIST strongly hinges on the quality and diversity of the offline dataset. A more detailed exploration of how performance varies with dataset quality and strategies to mitigate issues arising from poorquality demonstrations would be beneficial. 3. Generalization to Realworld Tasks: While promising results are shown in simulated environments the paper lacks discussion on the applicability and potential challenges of translating these results to realworld robotic tasks. Questions and Feedback: 1. Could the authors elaborate on any attempts or future plans to validate FISTs effectiveness in realworld robotic tasks 2. The paper mentions the use of a contrastive distance function how sensitive is FISTs performance to the choice of distance metrics and were alternative metrics considered 3. Regarding the algorithms reliance on the quality of the offline dataset could the authors discuss potential approaches or modifications to FIST that could mitigate issues arising from less diverse or poorquality datasets  3. Review Summary This paper presents FIST a promising algorithm that significantly advances the field of fewshot imitation learning for longhorizon tasks. The methodology is novel blending skill extraction from offline data with semiparametric imitation learning to enable generalization to unseen tasks. The experiments demonstrate clear improvements over existing methods substantiating the algorithms potential. However further exploration into its limitations sensitivity to dataset quality and adaptation strategies for poorquality datasets would enhance its applicability. Additionally extending the discussion on realworld applicability and addressing potential scalability challenges could provide a more comprehensive view of FISTs practical utility. Overall given the strong experimental results and the clear presentation of the proposed approach this paper makes a significant contribution to the field. I recommend its acceptance contingent upon addressing the discussed limitations and questions to strengthen the paper further.", "yeP_zx9vqNm": "Peer Review of \"Peripheral Computation in Adversarially Robust Neural Networks: A Psychophysics Study\" 1) Summarize the paper: The paper explores the similarity between adversarially robust deep neural network (DNN) representations and human peripheral visual computation through psychophysical experiments using metamerism. Advancements in DNNs have shown that adversarial robustness can lead to representations that are perceptually more aligned with human vision but its unclear how these align with the biological constraints of human vision especially in peripheral processing. The study compares the discriminability of images synthesized to match adversarially robust and nonrobust DNN representations and a texturebased model of peripheral vision (Texforms) across different retinal eccentricities. Results indicate that adversarially robust representations mimic human peripheral visual computation more closely than nonrobust representations suggesting that localized texture summary statistics representations might play a role in human invariance to adversarial perturbations. This discovery points towards the potential for implementing similar representations in DNNs to achieve adversarial robustness. 2) Strong and Weak Points Questions and Feedback: Strong Points:  The interdisciplinary approach that combines computer science and cognitive psychology to investigate DNNs robustness and human vision.  Rigorous methodology involving psychophysical experiments to compare human and DNN representation alignment.  The use of various metrics for comparing the discriminability of stimuli across different models showcases thorough analysis.  Provision of codedata for reproducibility and transparency. Weak Points:  Limited diversity in the human subject sample. The discussion doesnt significantly address how variability in human visual processing across different demographics might affect the outcomes.  Exclusively focusing on peripheral visual computation might overlook how central vision interactions contribute to recognizing adversarially perturbed images.  The paper briefly mentions foveal computation discrepancies but doesnt delve into potential implications or solutions for these differences. Questions and Feedback:  Could the authors clarify how the selected range of retinal eccentricities was determined and whether extending this range could impact the findings  It might be beneficial to include a discussion on the potential limitations of applying these findings in practical adversarial training scenarios given the difference between biological and computerbased visual systems.  Feedback for future work could include exploring the interplay between peripheral and foveal vision in adversarial robustness and investigating how demographic differences in human subjects influence the perceptual discriminability of adversarially robust representations. 3) Review Summary (Supporting Arguments for Recommendation): The paper provides a compelling insight into how adversarially robust DNNs mimic aspects of human peripheral visual computation offering potential pathways towards making DNNs more adversarially robust by emulating biological visual processing strategies. This exploration stands out for its innovative approach to bridging the gap between human cognitive processes and artificial intelligence. However the studys focus on a relatively homogenous group of human subjects limits its ability to account for variability in human visual processing. Additionally while the experiments meticulously investigate peripheral visual computation the interaction between peripheral and central vision in adversarial robustness remains unexplored. Addressing these gaps could significantly enhance the papers impact and application. Despite these limitations the rigorous methodology along with the critical insights offered underscores the papers contributions to both cognitive psychology and computer vision. It successfully lays groundwork for further interdisciplinary research that could lead to more robust artificial vision systems by incorporating biologically inspired mechanisms. Therefore I recommend this paper for publication provided the authors consider the feedback to broaden the discussion on potential limitations and future directions.", "xy_2w3J3kH": " Review of \"Decentralized Cooperative MARL with Homogeneous Agents\"  1. Summary The paper addresses critical challenges in Cooperative MultiAgent Reinforcement Learning (MARL) focusing on the Centralized Training and Decentralized Execution (CTDE) paradigms inefficiencies. It introduces a novel framework for decentralized cooperative MARL particularly for homogeneous Markov games (MGs) where agents share identical action spaces and the systems dynamics are permutation invariant. The authors present a theoretical foundation justifying decentralized policy parameter sharing without incurring suboptimality in such homogeneous MGs. They then propose a decentralized actorcritic method with consensusbased updates for both actors and critics ensuring convergence under certain conditions. Additionally practical algorithms are developed including a communicationefficient mechanism that adapts to partial observability and dynamically adjusts communication among agents. Empirical studies demonstrate the effectiveness of these algorithms compared to stateoftheart CTDE methods achieving comparable policy performance with significantly reduced communication costs.  2. Strong and Weak Points Question and Feedback to the Author Strong Points:  Theoretical Contribution: The paper provides a solid theoretical foundation for policy parameter sharing in homogeneous MGs a significant contribution that fills a gap in cooperative MARL literature.  Algorithmic Innovation: The decentralized actorcritic method with consensus update is innovative offering a promising direction for efficient learning in cooperative MARL settings.  Practical Relevance: The development of practical algorithms that consider realworld constraints (e.g. partial observability and dynamic communication networks) significantly enhances the relevance of the proposed methods.  Empirical Validation: The comprehensive experimental evaluation demonstrates the efficacy of the proposed algorithms across multiple settings substantiating the claims with empirical evidence. Weak Points:  Scalability and Generalizability: The paper mainly discusses homogeneous MGs and it remains unclear how the proposed methods would perform in heterogeneous settings or scale to more complex environments.  FiniteTime Analysis Absence: While asymptotic convergence is proven finitetime convergence rates are not provided which are crucial for understanding the practical implications of the algorithms.  Benchmarking: The comparison with stateoftheart CTDE algorithms is beneficial however a direct comparison with other decentralized MARL methods could further validate the proposed methods advantages. Questions and Feedback:  Could the authors comment on the potential strategies to extend the proposed decentralized actorcritic method to heterogeneous agent settings  It would be beneficial to discuss the limitations of the proposed method in dealing with nonhomogeneous action spaces and provide insights or future directions to address these issues.  Providing finitetime convergence analyses could enhance the theoretical framework. Are there ongoing efforts or challenges the authors have identified in deriving such results  The paper would benefit from a more detailed discussion on how the proposed methods can be integrated with existing cooperative strategies that involve heterogeneous agents or complex interaction models.  3. Review Summary The paper makes significant contributions to the cooperative MARL field addressing the inefficiencies of the CTDE paradigm through a decentralized approach tailored for homogeneous MGs. The theoretical justification for policy parameter sharing without losing optimality represents a notable advance. Moreover the proposed decentralized actorcritic method and its practical variations offer effective solutions for communicationefficient learning in cooperative settings. Despite the focus on homogeneous MGs and the absence of finitetime convergence analyses the strengths of the paper including theoretical contributions algorithmic innovation and thorough empirical validation provide valuable insights for future research in cooperative MARL. The practical relevance of the proposed methods and the potential for extensions to more general settings further underscore the papers significance.", "n0OeTdNRG0Q": " Peer Review of \"Efficient Sharpness Aware Minimizer (ESAM) for Deep Neural Networks\"  Summary The paper introduces the Efficient Sharpness Aware Minimizer (ESAM) a novel optimization technique designed to improve the efficiency of the Sharpness Aware Minimizer (SAM) without compromising on generalization performance. It tackles the crucial problem SAM poses \u2014 doubling the computational overhead compared to conventional optimizers like SGD. ESAM incorporates two innovative strategies: Stochastic Weight Perturbation (SWP) and SharpnessSensitive Data Selection (SDS). SWP reduces computational load by perturbing a stochastically selected subset of weights whereas SDS optimizes SAMs loss with a carefully picked subset of data more sensitive to sharpness. The authors theoretically justify why these strategies maintain generalization performance and demonstrate ESAMs effectiveness across CIFAR and ImageNet datasets with various DNN architectures. The paper claims ESAM achieves a significant reduction in computational overhead (from 100 extra to 40) while preserving or enhancing model accuracy.  Strong Points 1. Innovative Approach: ESAMs introduction of SWP and SDS addresses a significant limitation in SAM providing a practical solution for enhancing efficiency without loss of performance. 2. Theoretical Justification: The paper offers a solid theoretical foundation for both proposed strategies (SWP and SDS) adding credibility to their efficacy in maintaining generalization capability while reducing computational demands. 3. Extensive Experimental Validation: The comprehensive experiments across multiple datasets and neural network architectures thoroughly validate ESAMs efficiency and effectiveness claims. 4. Public Availability of Source Code: Sharing the implementation details and code at a public repository encourages reproducibility and further research in the field.  Weak Points 1. Lack of Comparison with Other Efficient Optimizers: While ESAM is compared against SAM and base optimizers like SGD the comparison against other stateoftheart efficient optimization algorithms is missing which could have highlighted ESAMs competitive advantages more distinctly. 2. Potential Overhead from Hyperparameter Tuning: The introduction of new hyperparameters (\u03b2 for SWP and \u03b3 for SDS) may pose additional overhead in terms of tuning for optimal performance across different tasks. The efficiency of this process and its impact on ESAM\u2019s overall computational savings was not thoroughly addressed. 3. Limited Discussion on Failure Cases or Limitations: The paper could benefit from a discussion on potential limitations or scenarios where ESAM might not perform as expected. Understanding these scenarios could guide future improvements and applications.  Questions and Feedback to the Author 1. Comparison with Other Optimizers: Have ESAM\u2019s efficiency and effectiveness been tested against other recent efficiencyimproved optimization techniques If so what were the findings 2. Hyperparameter Sensitivity Analysis: Could you provide more insight into how sensitive ESAMs performance is to the hyperparameters \u03b2 and \u03b3 This analysis could help practitioners better understand the robustness of ESAM across various settings. 3. Limitations of ESAM: Can the authors elaborate on any known limitations or cases where ESAM may not be the best choice Acknowledging these could help target ESAM\u2019s application more effectively.  Review Summary The proposed Efficient Sharpness Aware Minimizer (ESAM) represents a significant advancement in optimizing deep neural networks by addressing the computational inefficiency of SAM without compromising generalization capabilities. The innovative strategies of SWP and SDS backed by solid theoretical justification and extensive experimental validation showcase the potential of ESAM to significantly reduce computational overhead while maintaining or improving model accuracy. However the paper could benefit from a broader comparison with other optimization techniques a detailed analysis on hyperparameter sensitivity and a discussion on limitations or less optimal scenarios for ESAM application. Considering the strengths outweigh the identified weaknesses and the valuable contributions to the field I recommend this paper for publication. The inclusion of suggested improvements could further enhance its impact and applicability in advancing efficient deep learning research and practices.", "nzvbBD_3J-g": " Review of \"Intermediary Latent Space VAEs (InteLVAEs)\"  Summary of the Paper This paper argues against the conventional approach of using nonGaussian priors in Variational Autoencoders (VAEs) to incorporate inductive biases and propose a novel alternative: the Intermediary Latent Space VAEs (InteLVAEs). InteLVAEs introduce an intermediary set of latent variables with a standard Gaussian prior which is then mapped through a parametric function to the final latent representation. This parametric function allows the incorporation of desired inductive biases like sparsity or clustering directly into the latent representations. The authors demonstrate that InteLVAEs effectively enforce these characteristics avoid computational issues related to nonGaussian priors and outperform baseline methods in terms of generative and representational quality across various tasks.  Strong Points 1. Novelty and Theoretical Soundness: The paper presents a unique approach to incorporating inductive biases into VAEs addressing the limitations of altering the prior directly. The theoretical foundation is robust with clear explanations and relevant mathematical formulation. 2. Experimental Validation: The experiments are thorough and welldesigned covering a variety of inductive biases and comparing InteLVAEs with baseline methods. The use of both synthetic and realworld datasets enhances the credibility of the results. 3. Quality of Representations and Generative Models: InteLVAEs show considerable improvements over baselines in terms of the quality of generative models and the learned representations particularly for challenging properties like sparsity and multimodality. 4. Reproducibility and Clarity: Providing accompanying code and an extensive appendix with experiment details model structures and settings ensures that the results are reproducible. The paper is wellstructured offering readers a clear understanding of the methodology and findings.  Weak Points 1. Comparison with StateoftheArt Models: While baseline comparisons are provided the paper could further strengthen its argument by comparing InteLVAEs with stateoftheart models that are specifically designed for handling similar inductive biases. 2. Broader Impact and Limitations: The discussion on the ethical considerations and broader impact of the method is brief. A more thorough exploration of potential limitations and the societal implications of deploying InteLVAEs would be beneficial. 3. Analysis of Failure Modes: The paper could benefit from an analysis of scenarios where InteLVAEs might underperform or fail helping to set realistic expectations and guide future research efforts.  Questions and Feedback to the Author 1. How do InteLVAEs compare with the latest models that incorporate inductive biases or alternative methods like normalizing flows 2. Could the authors elaborate on any observed failure modes of InteLVAEs or situations where the models performance significantly decreases 3. Are there any scalability concerns with InteLVAEs especially pertaining to handling very highdimensional data or extremely complex data distributions  Review Summary This paper introduces a significant advancement in the field of variational autoencoders through the proposal of InteLVAEs. By effectively incorporating inductive biases directly into the latent representations and overcoming the limitations associated with nonGaussian priors the authors present a compelling alternative to conventional approaches. The experimental validation is comprehensive showing InteLVAEs superiority over baseline methods across multiple tasks and datasets. However the paper would benefit from a broader comparison with stateoftheart models a deeper exploration of potential limitations and an expanded discussion on ethical considerations. Overall the methodology is sound and the results presented are convincing marking a valuable contribution to the ongoing research on deep generative models and inductive bias incorporation.", "jE_ipyh20rb": "Peer Review of \"FEDPROF: A Federated Learning Approach Optimized for Data Representation Profiling and Matching\" 1) Summary The paper introduces FEDPROF a novel federated learning (FL) algorithm designed to tackle the challenges posed by lowquality biased or irrelevant local data in decentralized learning environments. Recognizing the diverse quality of data across clients and its impact on the efficiency and efficacy of FL the authors propose a solution that dynamically adjusts clients participation probability based on their datas representation profile dissimilarity. The approach leverages the observation that data representations in neural networks tend to follow a Gaussian distribution enabling a robust method of profiling and matching data representations. Comprehensive experiments demonstrate that FEDPROF not only accelerates the convergence of the global model but also achieves higher accuracy with significantly reduced communication rounds compared to existing FL algorithms thus enhancing both the efficiency and effectiveness of federated learning in the presence of data heterogeneity. 2) Strong and Weak Points Questions and Feedback Strong Points: a) Innovative Approach: The paper addresses a critical issue in FL i.e. the impact of heterogeneous and lowquality data by introducing a unique data representation profiling and matching scheme. This approach is both practical and scalable. b) Theoretical and Experimental Validation: The authors provide a solid theoretical foundation for their approach including proofs that underpin the Gaussian distribution of data representations. Comprehensive experiments offer strong empirical support for the algorithms effectiveness. c) Significant Performance Improvements: The experimental results showing up to a 4.5x speedup in convergence time and up to 2.5 accuracy gain are impressive. These improvements are critical for realworld FL applications especially in resourceconstrained environments. Weak Points: a) Assumption of Gaussian Distribution: While the paper provides theoretical support for the Gaussian distribution of data representations this assumption may not hold across all types of data and neural network architectures. It could limit the generalizability of the approach. b) Fairness Concerns: The selective participation strategy could potentially introduce biases where certain clients are consistently privileged over others raising fairness issues that are not sufficiently addressed. c) Computational Overhead: Although the computational efficiency of the algorithm in terms of communication rounds is highlighted the paper does not comprehensively assess the computation and memory overhead on the clientside which are crucial for FLs practicality. Questions and Feedback: 1. How does FEDPROF perform in nonideal conditions where data representation may not strictly follow a Gaussian distribution 2. Could the authors elaborate on potential fairness implications of the selective participation strategy and suggest ways to mitigate any adverse effects 3. An assessment of computational and memory overhead on clients when implementing FEDPROF would be valuable. Could the authors provide insights into the practical implications of their approach in terms of clientside resource utilization 3) Review Summary The paper makes a significant contribution to the area of federated learning by addressing the challenge of learning from decentralized data of varying quality. FEDPROF introduces an innovative approach to adaptively manage client participation based on data representation profiling and matching leading to notable improvements in learning efficiency and model performance. The theoretical and experimental validations strengthen the papers claims and offer a robust foundation for the proposed algorithm. However there are concerns regarding the generalizability of the Gaussian distribution assumption potential fairness issues and the lack of a detailed assessment of clientside computational overhead. Addressing these points could further enhance the contribution and applicability of this work in diverse federated learning scenarios. Overall the innovative methodology combined with substantial performance improvements makes FEDPROF a noteworthy contribution to the field. Future work focusing on broader applicability fairness considerations and practical resource constraints will be crucial for realizing the full potential of this approach in federated learning.", "qqdXHUGec9h": " Peer Review  1) Summary of the Paper The paper addresses partiallabel learning (PLL) where each training example is associated with a candidate label set containing the true label. The authors propose a novel method leveraging the class activation map (CAM) technique traditionally used in convolutional neural networks (CNNs) for visual tasks to select the true label from candidate sets without relying on data assumptions. However recognizing CAMs limitations to image inputs and CNNs the authors introduce the Class Activation Value (CAV) a more general approach that retains CAMs properties while being applicable to various input types and models. The CAV Learning (CAVL) method guides training by selecting the true label based on the highest CAV showing stateoftheart performance across multiple datasets and advancing PLL beyond the constraints of existing strategies.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Contribution: The introduction of CAV as an extension and generalization of CAM for PLL across input types and models is innovative addressing the limitation of prior CAM applications.  Empirical Evidence: Extensive experiments demonstrate the effectiveness of CAVL including improvements over stateoftheart methods in PLL. The use of both benchmark and realworld datasets underscores the approachs robustness and general applicability.  Addressing the Limitations of Previous Methods: The paper effectively critiques existing PLL strategies that depend on assumptions about data collection offering a more flexible solution that bypasses these shortcomings. Weak Points:  Explanation of CAVs Generalizability: While the authors claim that CAV is adaptable to various inputs and models a deeper explanation or theoretical foundation on why CAV is so versatile would strengthen the argument.  Comparison with Related Works: While the paper positions CAVL against stateoftheart methods a more detailed comparison including a discussion on potential tradeoffs or scenarios where CAVL might underperform would provide a more balanced view.  Implementation Details: Although the paper provides an algorithm for CAVL additional details on implementation such as pseudocode or considerations for practical application would be beneficial for reproducibility. Questions and Feedback:  How does the computational complexity of the proposed CAVL method compare with existing PLL strategies especially when scaling to large datasets  The paper focuses on the advantages of CAV over CAM for nonimage data. Could the authors discuss any potential limitations or challenges when applying CAV to very highdimensional data or complex model architectures  It would be valuable if the authors could provide more insights or potential future directions on refining CAVs calculation or exploring its applications beyond PLL such as in other weakly supervised learning scenarios.  3) Review Summary This paper makes significant contributions to the field of partiallabel learning by introducing Class Activation Values extending the idea of Class Activation Maps to a broader range of inputs and models. The extensive experimental validation presents a compelling case for CAVLs effectiveness positioning it as a new stateoftheart method for PLL. Despite its strengths the paper would benefit from a more indepth theoretical discussion on CAVs generalizability a detailed comparison with related works including potential limitations and further implementation details for practical application. Addressing these points could enhance the papers impact and provide clearer guidance for future research and applications of CAV in PLL and beyond.", "vnF5gDNvcKX": " Peer Review  Summarize the paper This paper addresses the challenge of high variance in policy gradient estimators due to the introduction of domain randomization (DR) in deep reinforcement learning (DRL) for better policy generalization in simulated environments. The authors propose a Variance Reduced Domain Randomization (VRDR) approach that efficiently reduces gradient estimator variance by introducing an optimal stateenvironmentdependent baseline. This baseline is shown to outperform both constant and statedependent baselines traditionally used in DR settings. The VRDR method divides the environment parameter space into subspaces approximating the optimal baseline for state and subspace pairs thereby reducing computational complexity. Empirical evaluations on six robot control tasks demonstrate that VRDR leads to faster convergence and in some cases higher rewards compared to baseline methods.  Strong and Weak Points Strong Points: 1. Theoretical Foundation: The paper provides a solid theoretical derivation of the optimal stateenvironmentdependent baseline including proofs of its variance reduction capability over existing baselines. This thorough theoretical groundwork ensures the methods reliability. 2. Empirical Validation: Through extensive experiments on six robotic control tasks the authors demonstrate the practical effectiveness of VRDR presenting a convincing case that the approach can speed up convergence and improve rewards in varying tasks. 3. Practical Approach to Computational Complexity: The proposal to divide the environment parameter space into subspaces to mitigate computational complexity is pragmatic. It shows the authors\u2019 consideration of making the VRDR approach feasible for realworld applications. Weak Points: 1. Clarity on Implementation Details: While the paper introduces an algorithm and discusses the methodology behind VRDR it could benefit from more detailed discussions on implementation challenges especially regarding tuning hyperparameters like the number of subspaces and clustering interval. 2. Comparison with StateoftheArt Methods: The paper compares VRDR with uniform domain randomization and multiple value network methods but lacks comparison with other stateoftheart domain randomization techniques or methods addressing high variance in policy gradients. 3. Discussion on Limitations: The paper could improve by providing a more detailed discussion of the limitations of VRDR potentially including cases where VRDR may not bring substantial benefits or might be less applicable. Questions and Feedback to the Author: 1. How did the authors choose the specific hyperparameters for VRDR such as the number of environment subspaces and the clustering interval More insights into these choices would help understand the tradeoffs involved. 2. Can the authors discuss any implementation challenges they faced especially regarding the computation and maintenance of multiple baselines 3. It would be beneficial if the authors could compare VRDR with more recent or alternative approaches addressing the high variance issue in policy gradients. How does VRDR stand in the broader context of variance reduction techniques in DRL 4. The generalization performance on unseen environments is crucial for the realworld applicability of DRL policies. Can the authors provide more details on how VRDR performs in environments significantly different from those seen during training  Review Summary The paper makes a significant contribution to the field of reinforcement learning by addressing the critical challenge of high variance in policy gradients due to domain randomization. The proposed VRDR approach backed by solid theoretical derivations and empirical validations successfully demonstrates faster convergence and improved performance in several robotic control tasks. While the method is promising the paper could be strengthened by a deeper discussion on implementation details broader comparisons with stateoftheart methods and a more extensive discussion of VRDRs limitations and applicability scope. Enhancing these aspects would provide a more comprehensive view of the VRDRs potential and facilitate its adoption in a wider range of DRL applications.", "yBYVUDj7yF": "Summarize the Paper: This paper focuses on analyzing contrastive learning an approach in selfsupervised learning and comparing it with autoencoders within a linear representation setting under the spiked covariance model. The authors theoretically demonstrate that contrastive learning surpasses autoencoders in terms of feature recovery and performance on downstream tasks. They further explore the impact of incorporating labeled data into the contrastive learning framework showing that while it enhances indomain classification accuracy it can detrimentally affect transfer learning capabilities. These theoretical insights are supported by numerical experiments. Strong Points: 1. Theoretical Depth and Novelty: The paper provides a rigorous theoretical framework comparing contrastive learning with autoencoders a novel contribution that helps bridge the gap in understanding why contrastive learning outperforms other methodologies in certain contexts. The mathematical formalization under the linear representation and spiked covariance model is both comprehensive and insightful. 2. Relevance to Current Research Themes: By focusing on contrastive learning a highly relevant topic in machine learning due to its successes in various domains the paper taps into a vibrant research area. Furthermore the exploration of supervised contrastive learning and its tradeoffs in transfer learning is timely and contributes to ongoing discussions around selfsupervised learning strategies. 3. Comprehensive Evaluation: The authors do not merely rest on theoretical contributions they also implement numerical experiments to validate their theories. This approach strengthens their claims and provides a practical dimension to their findings. Weak Points: 1. Limitation to Linear Representations: The papers focus on linear representations might limit the generalizability of its findings to more complex realworld applications which often require nonlinear methodologies. It would be beneficial to see how these insights extend beyond linear models. 2. Need for a Broader Range of Experiments: While numerical experiments support the theoretical findings adding experiments with different datasets especially realworld datasets could further validate the robustness and applicability of the proposed ideas. 3. Comparison with a Narrow Set of Models: The paper primarily compares contrastive learning with autoencoders. Including comparisons with other prominent unsupervised and selfsupervised learning methods could provide a more comprehensive picture of where contrastive learning stands in the wider landscape. Question and Feedback to the Author: It would be enlightening to see how the theoretical findings presented translate to complex nonlinear models commonly used in practice. Could future work extend these insights beyond linear representations possibly using approximation techniques to handle nonlinearities Additionally expanding the scope of numerical experiments to include diverse datasets and comparing performance across a wider array of selfsupervised learning methods would be invaluable. This could potentially illuminate other dimensions of contrastive learnings efficacy and areas for improvement. Review Summary: This paper effectively bridges a significant gap in understanding the effectiveness of contrastive learning compared to other methods notably autoencoders within a welldefined theoretical framework. It judiciously combines theory with numerical experiments to corroborate its claims marking a strong contribution to the literature on selfsupervised learning. However the focus on linear representations might limit the broader applicability of these insights and the range of experiments could be broadened to further validate the findings across various scenarios. Encouraging the authors to extend their analysis to nonlinear settings and to test their theories against a more extensive set of benchmarks would significantly enhance the impact of their work. Overall this study is a valuable addition to our theoretical understanding of contrastive learnings strengths particularly in highlighting the nuanced role of labeled data in such frameworks.", "yjxVspo7gXt": " Review of \"Scaling Bias Mitigation Techniques to Intersectional Groups in LargeScale Datasets\"  Summary This paper addresses the significant gap in existing bias mitigation algorithms scalability when dealing with intersectional protected groups arising from multiple attributes. Unlike previous works that primarily focus on single protected attributes this study undertakes an expansive empirical analysis on largescale datasets (ImageNet People Subtree and CelebA) to explore how these algorithms perform when scaled to settings with multiple protected attributes creating hundreds of intersectional groups. The authors find that leveraging protected attribute labels during training becomes increasingly crucial as the number of protected attributes grows. They introduce a novel regularization method called Knowledge Distillation of Independent Models as Regularization (DIR) to efficiently scale bias mitigation to multiple intersectional groups. DIR uses groupspecific teacher classifiers to train a single student classifier that mimics the output of the teacher classifier corresponding to the datapoint\u2019s ground truth group. The findings indicate that existing bias mitigation methods can significantly reduce bias amplification by 9 against empirical risk minimization (ERM) on ImageNet\u2019s People Subtree and DIR can further reduce bias amplification by an additional 15.  Strong Points 1. Innovative Approach: The introduction of the DIR method as a solution to scale bias mitigation techniques to settings with multiple intersectional groups is both innovative and effective. It addresses the core issue of model complexity and overfitting associated with increasing numbers of protected groups. 2. Comprehensive Empirical Analysis: The thorough evaluation of existing bias mitigation algorithms across large datasets and the first study of its kind on the ImageNet People Subtree are commendable. This systematic approach provides a solid foundation for understanding scaling challenges in realworld applications. 3. Significant Findings: The paper significantly contributes to the field by documenting the importance of leveraging protected attribute labels and demonstrating that knowledge distillation can be effectively used to scale bias mitigation methods. The reduction in bias amplification is a notable achievement.  Weak Points 1. Limited Discussion on Ethical Implications: While the paper mentions potential ethical concerns associated with the use of protected attribute labels briefly a more detailed discussion on the ethical implications especially the risk of reinforcing stereotypes or privacy concerns would have strengthened the paper. 2. Generalization of Results: The paper could benefit from discussing the generalizability of the DIR method across other domains or datasets beyond ImageNet and CelebA. Additionally insights into how DIR could be adapted or extended to other types of bias mitigation tasks would be valuable. 3. Exploration of Unlabeled Methods: Given the importance of protected attribute labels highlighted in the findings an indepth analysis or discussion on improving the performance of unlabeled methods in multiattribute settings could have been useful considering scenarios where attribute labels are scarce or unavailable.  Questions and Feedback to the Author 1. Ethical Considerations: Could the authors expand on the ethical considerations related to the labeling of protected attributes especially in terms of reinforcing stereotypes or privacy concerns 2. Generalization and Adaptability: How generalizable do the authors believe the DIR method is across different domains or datasets Are there specific types or categories of datasets where DIR might be less effective 3. Improving Unlabeled Methods: In light of the findings do the authors have suggestions or insights on how the performance of unlabeled methods could be improved in multiattribute settings where protected attribute labels are scarce  Review Summary The paper makes a significant contribution to the area of algorithmic fairness by addressing the scalability challenge of bias mitigation techniques in settings with multiple intersectional protected groups. The introduction of the DIR method as a novel regularization strategy to efficiently scale these methods is commendable and represents a notable advancement in the field. However the paper could be strengthened with a more detailed discussion on the ethical implications of using protected attribute labels and the generalizability of the DIR method across different domains. Additionally exploring ways to enhance the performance of unlabeled methods in multiattribute settings would add valuable insights. Overall the findings are robust and the methodology is sound making a strong case for the adoption of DIR in bias mitigation contexts involving intersectional protected groups.", "youe3QQepVB": " Review of \"MultiTask Oriented Generative Modeling for Visual Perception Tasks\"  1. Summary The paper introduces a novel approach referred to as MultiTask Oriented Generative Modeling (MGM) aimed at leveraging generative models to enhance performance across various visual perception tasks. By coupling a discriminative multitask network with a generative model the authors address the challenge of synthesizing useful data for multitask learning scenarios. The proposed MGM framework takes advantage of synthesized images coupled with weak annotations such as imagelevel scene labels to improve task performance in settings with limited data. The underlying hypothesis is that through joint training the discriminative model can guide the process of image synthesis leading to the generation of images that while not necessarily visually realistic are highly useful for improving taskspecific models. The method was evaluated on the NYUv2 and Taskonomy benchmarks focusing on semantic segmentation depth estimation and surface normal prediction and showed significant improvements over stateoftheart multitask approaches.  2. Strong and Weak Points Strong Points:  Innovative Approach: The paper addresses a relatively unexplored problem of using generative models to aid in multitask learning. It is a significant deviation from traditional methods that focus on visual realism towards generating data that is specifically useful for various perception tasks.  Comprehensive Framework: The MGM framework is wellthoughtout integrating a multitask discriminative model with a generative model through a refinement network and selfsupervision network. This includes thoughtful considerations on how to use weak annotations and synthesize images effectively to improve multiple visual tasks.  Empirical Validation: The evaluation on standard multitask benchmarks (NYUv2 and Taskonomy) is thorough and the reported improvements over stateoftheart multitask models are compelling. The experiments are designed to validate the frameworks effectiveness in lowdata regimes a critical aspect of the proposed benefits. Weak Points:  Limited Discussion on Scalability: While the paper demonstrates improvements on three visual tasks there is limited discussion on the frameworks scalability and adaptability to a broader range of tasks or more complex multitask learning scenarios.  Model Complexity and Efficiency: The introduction of multiple components (generative model refinement network selfsupervision network) could add to the computational complexity. Discussion on the tradeoffs between performance improvement and computational overhead or efficiency is lacking.  Generalizability of Generative Model Choice: Although the paper mentions the frameworks agnosticism to the choice of generative models there is insufficient evidence or discussion on how different generative models might impact the overall performance and effectiveness of the MGM approach. Questions and Feedback:  Can the authors elaborate on the potential scalability of the MGM framework to include more diverse or complex visual tasks beyond the three evaluated tasks  How does the computational complexity of the MGM framework compare to traditional multitask learning or singletask approaches and what are the implications for realworld applications  Further insight into how the choice of different generative models impacts the MGMs performance would be beneficial. Are there specific characteristics of generative models that make them more suitable for this framework  3. Review Summary The paper introduces a novel and promising approach to leverage generative modeling for enhancing multitask visual learning. The MGM frameworks ability to improve task performance especially in lowdata regimes through synthesized images paired with weak annotations is a significant contribution to the field. The comprehensive evaluation demonstrates the methods effectiveness over stateoftheart approaches highlighting its potential for wide application. However discussions on scalability computational efficiency and the impact of different generative models on performance are areas that require further exploration. Addressing these points would significantly strengthen the paper and offer a clearer understanding of the frameworks applicability and limitations. In conclusion the innovative ideas and positive initial results presented make a compelling case for the usefulness of generative models in multitask learning scenarios. With further refinement and exploration the MGM framework could pave the way for new approaches in visual perception tasks.", "vDa28vlSBCP": "Summarize the paper: The paper presents ProtoTrex a novel approach to interpreting transformer language models (LMs) through the use of prototype networks. Unlike traditional post hoc interpretability methods that analyze inputoutput correlations after the fact ProtoTrex incorporates casebased reasoning directly into the transformer architecture offering realtime examplebased explanations for model decisions. The explanations are provided as prototypical examples that resemble the input being classified aiming to mimic human reasoning processes and enhance user trust and model transparency. Additionally the paper introduces an interactive prototype learning setup (iProtoTrex) that enables users to interact with and improve the model by providing feedback on the explanations thus merging human insights with datadriven learning. Experimental results show that ProtoTrex performs comparably to standard transformer LMs on classification tasks while significantly improving interpretability. Furthermore iProtoTrex demonstrates the potential for user interactions to refine model explanations and incorporate diverse knowledge beyond the dataset. Strong and Weak Points Questions and Feedback: Strong Points: 1. Innovative Use of Prototypes: The paper creatively applies prototype networks to transformer LMs offering a unique way to tackle the opacity problem in NLP tasks with a casebased reasoning method that is intuitive and aligns well with human cognitive processes. 2. Interactive Learning: iProtoTrex is a significant contribution enhancing the prototype approach by incorporating user feedback which allows continuous improvement of model explanations and adaptability to unseen inputs or concepts beyond the original training data. 3. Comprehensive Evaluation: The paper provides a thorough experimental evaluation comparing ProtoTrex against several baseline LMs across different datasets and metrics showing competitive performance alongside improvements in interpretability and user trust. Weak Points: 1. Scalability Concerns: While the prototype approach is innovative the scalability of maintaining and updating prototypes as the model or data scales isnt thoroughly discussed. This could be a limitation in very large or evolving datasets. 2. Limited Detail on User Interaction Process: The paper could have elaborated more on the methods of collecting and implementing user feedback especially considering the varying levels of expertise among potential users and how these differences impact the effectiveness and accuracy of the interactions. 3. Evaluation of Interpretability: While the paper presents promising results on the interpretability and user trust aspects the evaluation of these qualitative factors could be more systematic. Including user studies or more detailed metrics on interpretability would provide stronger evidence of the models benefits. Questions and Feedback: 1. Can you elaborate on how the prototype updating process scales with increasingly large datasets or rapidly changing data environments 2. How do you ensure the quality of user feedback especially when users with varying levels of expertise are interacting with the model Is there a process for validating the feedback before its used to update the prototype network 3. A more detailed evaluation of interpretability through user studies or surveys focusing on the clarity relevance and actionability of the provided prototypes would strengthen the papers claims. Have you considered such evaluations Review Summary: The paper introduces a novel approach ProtoTrex to enhance the interpretability of transformer LMs through the use of prototype networks and an interactive learning setup. This method represents a significant step forward in making machine learning models more understandable and trustworthy to users by providing realtime casebased reasoning explanations. The inclusion of user feedback as a mechanism to refine and improve the model is particularly noteworthy presenting a promising direction for blending human insights with automated decisionmaking processes. However the paper could benefit from a more detailed discussion on scalability methods to ensure the validity of user feedback and a more systematic evaluation of interpretability to substantiate its claims further. Encouragingly the positive experimental results suggest that with refinement ProtoTrex could become a valuable tool for developing transparent and interpretable AI systems in NLP and beyond.", "fTYeefgXReA": "PeerReview 1. Summarize the paper The paper introduces a novel approach to employing neural networks directly on heterogeneous graphs without reducing them to homogeneous graphs thus preserving the unique information that heterogeneous structures offer. The authors develop graph neural networks (GNNs) composed of linear layers designed to be maximally expressive while maintaining equivariance to permutations of nodes within each distinct type. This innovation addresses the limitations of current GNNs that either overlook the heterogeneous nature of many realworld graphs or do not fully exploit the structural and semantic diversity of nodes and edges. Through the construction of two different architectures the paper showcases the applicability and effectiveness of their model on benchmarks for node classification and link prediction tasks extending the idea to more complex scenarios involving hyperedges and hypergraphs. 2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The approach to preserve the heterogeneous nature of graphs and directly operate on them using linear layers designed for equivariance within node types is innovative and addresses a significant limitation in current GNN models.  Theoretical Foundation: The paper provides a robust theoretical basis for the construction of the equivariant linear operations ensuring that the models do not lose information critical for capturing the full complexity of heterogeneous structures.  Performance: The evaluation shows that the proposed method not only competes well with existing methods but also outperforms across several benchmarks especially on link prediction tasks demonstrating the practical effectiveness of the approach. Weak Points:  Complexity and Scalability: The paper could better address the computational complexity and scalability of the proposed method especially in comparison to existing heterogeneous graph neural network approaches.  Generalization Beyond Benchmarks: While performance on the Heterogeneous Graph Benchmark is strong the paper could further discuss the models generalizability to a wider array of realworld problems beyond the evaluated tasks.  Hyperparameter Sensitivity: More insight into the sensitivity of the model performance to hyperparameter settings and the process of hyperparameter optimization could enhance the reproducibility and applicability of the work. Questions and Feedback:  Could the authors comment on the models performance in dynamically changing graphs where nodes or edges might be added or removed over time  How does the model cope with extremely large graphs particularly in terms of memory usage and computational efficiency  It would be beneficial to have more details on the limitations of the approach and potential strategies to overcome these enhancing the models applicability to a broader range of problems. 3. Review Summary The paper presents a significant advancement in the application of neural networks to heterogeneous graphs providing a method that maintains the richness of the graphs structure and semantics. The theoretical underpinnings are wellformulated and the empirical evaluation demonstrates the methods promise particularly in link prediction tasks. To enhance the contribution the authors should address questions related to the scalability complexity and generalization of the model to various realworld scenarios. While the paper does incorporate an extensive evaluation a deeper discussion on these aspects would be valuable. Nonetheless the work marks a noteworthy step towards more effectively leveraging the complexity inherent in heterogeneous graphs offering substantial potential for future research and application in this domain.", "oZe7Zdia1H5": " Peer Review of \"Existence of Structurally Sparse Winning Tickets in Deep Neural Networks\"  1. Summarize the paper This paper explores the existence of structurally sparse subnetworks or \"winning tickets\" within deep neural networks (DNNs) that can achieve similar accuracy to their dense counterparts while offering substantial reductions in inference time. The concept emerges from the Lottery Ticket Hypothesis (LTH) which until now predominantly focused on unstructured sparsity. The authors argue that unstructured sparsity is less practical due to its irregular patterns that are hard to accelerate on hardware. They introduce a novel methodology involving \"postprocessing techniques\" postiterative magnitude pruning (IMP) to enforce structural sparsity\u2014first by refilling important channels (\"refilling\") followed by regrouping nonzero elements into dense blocks (\"regrouping\"). This approach successfully demonstrates for the first time that it is possible to identify structurally sparse winning tickets with substantial inference speedups on real hardware without compromising accuracy. The paper validates these findings through extensive experiments across multiple datasets (CIFAR TinyImageNet ImageNet) and network architectures.  2. List strong and weak points of the paper Question and Feedback to the author Strong Points:  Novel Contribution: The exploration of structurally sparse winning tickets in DNNs provides a significant advancement in the practical application of the LTH marking an essential bridge between theory and realworld applicability.  Comprehensive Experiments: The paper boasts a wide array of experiments across various datasets and network architectures providing robust evidence for the claims made about the identification of structurally sparse winning tickets.  Impactful for RealWorld Applications: By addressing the hardware acceleration roadblock of LTH the findings directly contribute to the deployment of more efficient DNNs in resourceconstrained environments. Weak Points:  Lack of Comparative Baseline: While the paper introduces new methods for achieving structural sparsity it lacks a comparison with existing alternative methods outside the LTH framework that also aim to achieve hardwareefficient structural sparsity.  Potential Overemphasis on SparsityAccuracy Tradeoff: The focus is heavily on maintaining accuracy while achieving sparsity possibly at the expense of overlooking other important aspects such as the interpretability of the neural network or the robustness of the sparse networks.  Generalization Beyond Convolutional Networks: The paper could extend its examination to other types of networks such as recurrent neural networks (RNNs) or transformers to demonstrate the universality of the findings. Questions and Feedback:  Could the authors elaborate on how their methodology would adapt or perform with emerging DNN architectures not covered in the experiments  It would be beneficial for the readers if the paper included a discussion on the potential limitations of the proposed method. Are there scenarios or specific network architectures where the method might not yield the anticipated benefits  On the practical implementation side could the authors discuss the computational costs associated with the \"postprocessing techniques\" and how these costs compare to the benefits of inference speedup  3. Review Summary (Supporting Arguments for Recommendation) The paper presents a compelling advancement in the quest for practical implementations of the Lottery Ticket Hypothesis. By demonstrating experimentally that structurally sparse winning tickets exist and can be efficiently identified the authors tackle one of the significant challenges limiting the appeal of winning tickets in practice. The innovative approach of coupling refilling and regrouping postIMP is welljustified brilliantly executed and thoughtfully validated through extensive experiments. However for a more rounded presentation future versions of this work could benefit from a broader discussion on comparisons with nonLTH based methods aiming for structured sparsity potential limitations and applicability to a wider range of network architectures. Despite these minor gaps the soundness of the methodology coupled with the depth and breadth of experimental validation makes a strong case for the significance of the findings. Therefore I recommend this paper for publication acknowledging its potential to significantly influence both theoretical and practical aspects of network pruning and the deployment of deep learning models in resourceconstrained environments.", "ue4CArRAsct": " Peer Review  1) Summary of the Paper The paper introduces a novel autoencoder architecture\u2014 the Structural Autoencoder (SAE)\u2014for selfsupervised structured representation learning. This model is designed to learn a hierarchical representation of latent variables that mimics the structure of a general acyclic structural causal model (SCM). Differing from previous autoencoderbased methods such as Variational Autoencoders (VAEs) and their derivations that compromise between reconstruction quality and generative performance due to reliance on matching a prior latent distribution with regularization SAE uses a sampling technique based on the independence of latent variables thereby eschewing traditional regularization approaches. The paper argues that its architecture allows for improved performance on downstream tasks notably in generative modeling disentanglement and extrapolation capabilities across various datasets. The SAE achieves these feats without the usual tradeoffs by employing a structural decoder that orders information hierarchically and uses hybrid sampling to maintain independence between latent variables while avoiding the information bottleneck and posterior collapse seen in other autoencoder models.  2) Strengths and Weaknesses Strengths:  Innovative Approach: The paper introduces a significant architectural advancement with its SAE distinctly focusing on ordering latent variables hierarchically in line with SCMs and avoiding conventional regularization and its associated tradeoffs.  Empirical Validation: It extensively evaluates the models across several datasets demonstrating the utility of the SAE in varying contexts including the improvement in disentanglement and generative capabilities.  Hybrid Sampling Technique: The proposed sampling method though simple effectively addresses and mitigates common issues like the information bottleneck and latent space discontinuities without needing prior latent distribution matching. Weaknesses:  Lack of Theoretical Underpinning: The paper could benefit from a more detailed theoretical analysis explaining why the SAE structure leads to better performance particularly around the independence of latent variables and the implied disentanglement.  Generalizability Concerns: While experiments cover various datasets theres a potential question of how well the method generalizes to datasets with more complex or less clearly independent underlying factors of variation.  Indepth Comparison Lacking: More detailed comparisons with contemporary methods especially newer deep learning models that attempt unsupervised learning and disentanglement could better contextualize the SAEs performance and uniqueness. Questions and Feedback to the Author:  Could the authors elaborate on the theoretical basis for the independence of latent variables in the SAE architecture leading to improved extrapolation and disentanglement  How does the SAE model perform in scenarios where the assumption of independent underlying factors is less valid and does this affect the performance of downstream tasks  Is there scope for further exploration or adaptation of the hybrid sampling method to enhance fidelity in generative tasks further or to other model architectures beyond autoencoders  3) Review Summary and Recommendation The paper presents a compelling new autoencoder architecture that innovatively leverages a structured approach to latent variable modeling drawing on causal inference principles to enhance disentanglement and generative performance. By introducing a structured decoder and a pioneering hybrid sampling method the SAE model showcased notable improvements on several fronts without succumbing to the usual drawbacks of regularizationbased approaches. The empirical demonstrations across various datasets underline the models effectiveness and its potential applicability in different contexts. However to strengthen the paper a more rigorous theoretical grounding for the choice of architecture and its intrinsic benefits alongside an analysis of performance in less ideal conditions (where assumptions about data may not hold as firmly) would be beneficial. Furthermore positioning this work within the broader spectrum of current research in unsupervised learning and disentanglement would provide vital context for its innovations and implications. Given its significant contributions potential for wide application and the novelty of its approach I recommend this paper for publication. Still I encourage the authors to address the outlined questions and feedback to enrich the manuscript further.", "wzJnpBhRILm": " Summarize the Paper The paper presents an indepth exploration of batch normalization (BN) a technique commonly used to enhance the training speed of neural networks by normalizing the data across each minibatch. Despite its widespread use the theoretical underpinnings of BN have remained elusive and it is known to introduce unexpected artifacts into the training process. The authors aim to reinterpret BN as an attempt at approximating population normalization where data normalization is performed across the entire dataset. They investigate alternative methods for approximating the gradient derived from this population normalization stance introducing an approximation that allows for memoryefficient training by only requiring one example in memory at a time. This comes with a tradeoff of a slight increase in computation per training step. Additionally a perexample training procedure is introduced which further reduces computational complexity at the expense of a minor accuracy decrease. The paper also proposes improvements to batch renormalization for handling very small minibatches effectively. Notably the proposed normalization techniques do not alter the function class of the inference model and showcase robust performance even in the absence of identity shortcuts.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Approximation Method: The introduction of an approximation methodology that does not require keeping more than one example in memory is innovative and valuable for memoryconstrained training environments. 2. Indepth Theoretical Analysis: The paper provides a comprehensive theoretical framework to understand batch normalization and its alternatives better contributing significantly to the fields body of knowledge. 3. Extensive Experiments: The experimental evaluation is thorough with detailed comparisons against baseline methods and thoughtful exploration of the effects of various modifications such as gradient shrinkage and the introduction of Hadamard rotations. Weak Points: 1. Complexity for Practitioners: The proposed methods while intriguing add a layer of complexity that might deter practitioners from implementing them especially the twostage backpropagation and twotower model. 2. Limited Architecture Evaluation: The evaluation primarily focuses on Inceptionv3 and ResNet50 models. Exploring a broader range of architectures especially newer or more diverse ones could provide insights into the generalizability of the findings. 3. Analysis of Computational Overheads: The discussion on the computational overhead is somewhat limited. A more detailed breakdown of the computational costs and efficiency tradeoffs especially in comparison with traditional batch normalization would be beneficial. Questions and Feedback: 1. How do the proposed methods affect the training time compared to traditional BN considering the computational overhead introduced by the twostage backpropagation and the necessity to process each example separately 2. Could the authors provide insights on how the proposed normalization techniques perform on more recent or diverse neural network architectures beyond Inceptionv3 and ResNet50 3. It would be interesting to see an analysis of the tradeoffs between computational overhead memory usage and model accuracy in more detail. Can the authors expand on this possibly with a broader set of benchmarks  Review Summary Overall the paper makes significant contributions to understanding and improving batch normalization techniques. It introduces novel approximations for gradient computation that minimize memory requirements and computational complexity providing a valuable alternative for scenarios where memory is a constraint. The exploration of perexample normalization and the detailed experimental evaluation are particularly commendable. However the practicality of these techniques for broader use remains a concern primarily due to the added complexity and the limited scope of architecture evaluations. Further analyses regarding the computational efficiency and expanded evaluations across a wider range of models could enhance the papers impact and facilitate the adoption of the proposed methods in practice. Based on the strengths of the theoretical contributions and experimental validations and considering the areas for improvement identified I recommend this paper for publication with minor revisions. The suggested areas of focus would significantly strengthen the paper broadening its appeal and utility to both the research community and practitioners in the field.", "mwdfai8NBrJ": "Draft Peer Review 1) Summary: The paper presents a groundbreaking study on achieving provable adversarial robustness in reinforcement learning (RL) environments by introducing a novel method known as Policy Smoothing. Unlike traditional approaches that attempt to certify the behavior of a policy at every time step this work proposes a mechanism that certifies the total reward over an episode thereby allowing some flexibility in intermediate steps while still ensuring overall robustness against normbounded adversarial perturbations. The foundation of this method is an adaptive version of the NeymanPearson Lemma which has been developed to account for the dynamic nature of RL adversaries that adapt their strategies based on observed states and actions. The authors empirically validate their theoretical contributions by testing the proposed method on several benchmark environments including Cartpole Pong Freeway and Mountain Car demonstrating that the Policy Smoothing technique can indeed provide meaningful robustness guarantees in practice. 2) Strong and Weak Points Questions and Feedback: Strong Points:  Theoretical Contribution: The adaptive version of the NeymanPearson Lemma is a significant theoretical advancement that allows for the practical application of randomized smoothing in dynamic settings such as RL.  Practical Relevance: The method addresses a critical vulnerability in RL systems making it highly relevant for realworld applications where RL agents interact with potentially adversarial environments.  Comprehensive Empirical Evaluation: The paper includes a thorough evaluation across different environments which helps in understanding the practical implications and limitations of the proposed method. Weak Points:  Scalability Concerns: While the paper presents results on some benchmark environments it remains unclear how scalable the proposed method is especially in more complex or highdimensional RL settings.  Potential for Practical Attacks: The paper demonstrates the efficacy of Policy Smoothing against normbounded attacks but it would be beneficial to explore its robustness against more practical or sophisticated adversarial strategies that may not adhere strictly to the considered threat model.  Lack of Comparison with Other Approaches: The paper could be improved by including comparisons with existing adversarial defense methods in RL to contextualize the performance and unique advantages of Policy Smoothing. Questions and Feedback:  Could the authors elaborate on the computational overhead introduced by Policy Smoothing especially in terms of training time and inference latency  How does the proposed method perform under adaptive attacks specifically designed to target the smoothing mechanism  The paper mentions the potential application in dynamic realworld environments. Could the authors provide some hypothetical examples or scenarios where Policy Smoothing could be particularly beneficial 3) Review Summary: Overall the paper makes a substantial contribution to the field of provable adversarial robustness in reinforcement learning by introducing Policy Smoothing a novel method that leverages an adaptive version of the NeymanPearson Lemma. The theoretical foundations are solid and wellsupported by empirical evidence demonstrating the methods effectiveness in several benchmark environments. However the paper could be strengthened further by addressing scalability concerns exploring robustness against more practical adversarial strategies and providing direct comparisons with existing adversarial defense methods in RL. Given the significance of the theoretical contributions and the practical relevance of the work I recommend acceptance contingent on the authors considering the feedback and questions raised to enhance the papers clarity and comprehensiveness.", "kOtkgUGAVTX": " Review of \"Contrastive Intrinsic Control (CIC): A Novel Algorithm for Unsupervised Skill Discovery in Reinforcement Learning\"  1. Summary The paper proposes Contrastive Intrinsic Control (CIC) an innovative algorithm designed for unsupervised skill discovery in reinforcement learning (RL). CIC stands out by maximizing mutual information between skills and state transitions through a novel decomposition approach that prioritizes diverse behavior by maximizing state entropy. It introduces a unique lower bound estimate for mutual information incorporating a particle estimator for state entropy to promote diverse behaviors and contrastive learning for distinguishing between highdimensional continuous skills. The effectiveness of CIC is validated on the Unsupervised Reinforcement Learning Benchmark showing substantial improvements over previous methods in unsupervised skill discovery and overall exploration algorithms.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Novelty of the Lower Bound Estimate: The innovative approach of combining a particle estimator with noise contrastive estimation for mutual information provides a fresh perspective on encouraging exploration and skill discrimination. b. Empirical Evidence and Benchmarking: The comprehensive evaluation against established baselines and across multiple domains lends credibility to the claims of superior performance especially a significant improvement over previous competencebased methods. c. Detailing of Methodology: The clear exposition of CICs theoretical foundations along with thorough discussion on practical implementation challenges and how they were addressed offers a solid understanding of the algorithms workings. Weak Points: a. Generalization Across Different Environments: While the paper presents positive results on the Unsupervised Reinforcement Learning Benchmark it might benefit from exploration into how the algorithm performs in environments not covered by the benchmark. b. Complexity and Computational Demands: The introduction of particle estimation and noise contrastive estimation could potentially make CIC computationally more intensive than its predecessors yet discussion around computational costs or efficiency optimizations is lacking. c. Analysis of Failed Instances: There is little mention of scenarios where CIC might not perform as expected or how it compares in less favorable situations. An analysis of such cases could provide deeper insights into the algorithms robustness. Questions and Feedback: 1. Can the authors comment on the scalability of CIC in even more complex or higherdimensional environments than those tested 2. It would be beneficial if the authors could discuss the computational efficiency of CIC compared to other methods providing insights into its practical implementation challenges. 3. A deeper dive into instances where CIC might not perform optimally or comparison with environments where other algorithms have an edge could offer a broader perspective on its applicability.  3. Review Summary The paper presents an impressive advancement in the field of unsupervised skill discovery in reinforcement learning through Contrastive Intrinsic Control (CIC). The novel approach for mutual information estimation and skill discrimination stands as a significant contribution substantiated by thorough empirical evaluation. However the exploration lacks in areas concerning the algorithms computational efficiency wider applicability outside benchmark environments and handling of less favorable scenarios. Questions aimed at these aspects along with suggested improvements could enhance understanding and applicability of CIC. Overall the paper provides strong evidence supporting its claims making a compelling case for CICs potential to further research and applications in unsupervised reinforcement learning.", "nrGGfMbY_qK": "Summarize the Paper This paper introduces a new continual learning setup named \u201ciBlurry\u201d that aims to address realworld scenarios of online learning more effectively than existing setups by allowing for anytime inference incorporating both classincremental learning and blurry task boundaries. It challenges the traditional disjoint and blurry class distribution assumptions by proposing a more realistic scenario where new classes can appear at any time and class overlaps across tasks are possible. To evaluate continual learning models in this setup the authors introduce a new metric Area Under the Curve of Accuracy (AAUC) which measures a model\u2019s accuracy throughout training thus providing a better assessment of its ability for anytime inference. The authors also propose a new method that includes a novel memory management technique exclusive memorybased training and adaptive learning rate scheduling to address the challenges of the iBlurry setup. Their method demonstrated through empirical evaluations on CIFAR10 CIFAR100 TinyImageNet and ImageNet datasets significantly outperforms existing continual learning methods. Strong Points: 1. Novelty of the iBlurry Setup: The paper proposes a novel and realistic continual learning setup that is comprehensive encompassing both online and taskfree learning classincremental learning with blurry task boundaries and anytime inference. This setup addresses significant gaps in existing setups and aligns well with realworld learning scenarios. 2. Introduction of the AAUC Metric: The AAUC metric is a significant contribution as it provides a more nuanced measurement of a models performance across the entire learning process focusing on anytime inference ability. This contrasts with the traditional taskbased accuracy metrics and is more aligned with practical applications. 3. Effectiveness of the Proposed Method: The proposed method including innovative approaches to memory management memoryonly training and adaptive learning rate scheduling not only directly addresses the key challenges of the iBlurry setup but also significantly outperforms existing stateoftheart methods across multiple datasets. Weak Points: 1. Lack of Detailed Comparison with TaskBased Methods: While the paper mentions existing taskbased and classincremental learning setups it lacks a detailed direct comparison between their proposed method and these setups. Understanding how their method performs in traditional setups could provide more insights into its versatility and scope of applicability. 2. Complexity and Implementation Details: The paper could benefit from a more detailed discussion on the computational complexity and practical details of implementing the proposed memory management scheme memoryonly training strategy and adaptive learning rate scheduling. This could include considerations for hyperparameter tuning scalability and adaptability to different types of data streams. Questions and Feedback: 1. Robustness Across Varied Data Streams: How robust is the proposed method across highly variable data streams particularly in scenarios with extreme class imbalance or rapid changes in data distribution 2. Impact of Memory Size: The study uses fixed memory sizes for the experiments. How sensitive is the proposed method to changes in memory size and how does memory size affect the tradeoff between performance and computational resources 3. Generalization to Other Domains: Could the authors elaborate on the potential generalization of the iBlurry setup and the proposed method to other domains such as natural language processing or reinforcement learning Review Summary: The paper introduces a novel and potentially impactful approach to continual learning addressing key limitations of existing setups by proposing a new learning setup\u2014iBlurry\u2014that combines online taskfree classincremental learning with blurry task boundaries and anytime inference. The introduction of the AAUC metric represents a significant advancement for evaluating continual learning models in a more nuanced and practically relevant manner. The proposed method demonstrates superior performance over existing models providing a promising direction for future research in continual learning. Despite its strengths the paper could be improved by providing more detailed comparisons with taskbased methods and offering deeper insights into the robustness of the proposed method across varied data streams and different domains. Additionally discussing the computational complexity and practical implementation details would enhance its contribution. Overall the paper presents a significant step forward in the continual learning field with its novel setup method and evaluation metric opening new avenues for research and practical applications in this area.", "hzmQ4wOnSb": " Peer Review for the Manuscript on Question Answering Systems using GNNs  1) Summary of the Paper This manuscript delves into the realm of Question Answering (QA) systems particularly focusing on the utilization of Graph Neural Networks (GNNs) to enhance reasoning capabilities. The study scrutinizes the existing stateoftheart GNN modules in QA systems questioning their complexity and efficacy in performing reasoning tasks. It introduces a novel approach through the design of a Graph Soft Counter (GSC) a simplified yet remarkably efficient GNN module. By comparing its performance on two benchmark QA datasets CommonsenseQA and OpenBookQA the study highlights the potential overparameterization of existing GNN modules and underscores the significance of edge counting in knowledgeaware reasoning. Employing Sparse Variational Dropout (SparseVD) for model dissection this work sheds light on the dispensability of certain components within GNN architectures and proposes a more streamlined model that outperforms its predecessors with a fraction of the trainable parameters.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Methodology: The introduction of the Graph Soft Counter (GSC) as a simpler alternative to complex GNN modules is both innovative and impactful. Demonstrating superior performance with significantly fewer parameters is a testament to the effectiveness of this approach.  Comprehensive Analysis: The use of SparseVD for dissecting GNN modules provides an insightful analysis into the overparameterization issue and the relevance of different components in knowledge reasoning. The empirical observations are wellsupported by the data.  Interpretability and Efficiency: Emphasizing edge counting and introducing a parameterfree GSC module improves both interpretability and computational efficiency addressing two crucial aspects often criticized in modern AI systems. Weak Points:  Limited Scope of Investigation: The focus is primarily on the aspect of counting within GNNs potentially overlooking other reasoning capabilities or dimensions where existing modules may excel. How GSC performs in datasets or tasks requiring more complex reasoning beyond counting is not discussed.  Generalization: While the results on CommonsenseQA and OpenBookQA are impressive the generalizability of GSC to other knowledgeintensive domains or different types of KGs remains unclear.  Comparison with NonGNN Baselines: The paper effectively compares GSC with stateoftheart GNNbased QA systems but does not provide a direct comparison with nonGNN approaches which could have served as an additional baseline to measure the specific advantages of GSC. Questions and Feedback: 1. Complex Reasoning beyond Counting: Can the authors comment on the potential or adaptability of GSC in handling QA tasks that require reasoning beyond simple counting such as inference based on causality or contradictions 2. Application on Diverse KGs: Given the studys reliance on specific KGs how does GSC perform on more heterogeneous or less structured knowledge graphs Is there a preprocessing requirement for the KGs before applying GSC 3. Comparative Analysis Suggestion: A deeper comparative analysis with nonGNN QA systems could further highlight the unique strengths or shortcomings of the GSC approach. Incorporating such a comparison could provide a clearer picture of where GSC stands in the landscape of QA systems.  3) Review Summary and Recommendation This manuscript presents a compelling investigation into the reasoning capabilities of GNNs within the context of QA systems challenging the conventional complexity of these models and proposing an elegantly simplified alternative. The introduction of the Graph Soft Counter (GSC) marks a significant stride toward efficiency and interpretability in AI reasoning systems backed by convincing empirical evidence. Despite the notable contributions some areas such as the adaptability of GSC to complex reasoning tasks and its generalizability across diverse knowledge domains warrant further exploration. The authors are encouraged to address these points particularly by expanding on GSCs operational scope and providing a broader comparative analysis against both GNN and nonGNN QA methodologies. Accommodating these adjustments could significantly enhance the papers impact positioning it as a pivotal reference in the field. In conclusion considering the manuscripts strengths the novel insights provided and its potential to stimulate further research I recommend this paper for publication after the aforementioned suggestions are considered and incorporated ensuring a more comprehensive evaluation of GSCs capabilities and positioning within the broader spectrum of QA systems.", "kNKFOXleuC": " Summarize the paper The paper presents an innovative technique referred to as Anytime Dense Prediction with Confidence (ADPC) which is designed for anytime dense prediction tasks like semantic segmentation and human pose estimation. Anytime inference in contrast to traditional methods allows a model to provide a progression of predictions that can be halted at any moment providing flexibility for various computational constraints. The ADPC framework enhances this by attaching cascades of exits to a base model enabling multiple predictions at various stages of computation. Unique to this approach the predictions at each exit account for differences in depth and spatial dimensions by redesigning the exits with encoderdecoder architectures. Furthermore the proposed method introduces a novel spatially adaptive technique that utilizes confidence levels of predictions to reduce unnecessary computations on areas where early predictions are already sufficiently confident. This results in a significant reduction of total computation (44.4 and 59.1 for the evaluated tasks) while maintaining the same level of accuracy as baseline models. The methodology is validated through extensive experiments on Cityscapes semantic segmentation and MPII human pose estimation tasks showing superior performance over equivalent anytime inference approaches and achieving economical computational cost without sacrificing accuracy.  Strong and Weak Points Questions and Feedback  Strong Points: 1. Innovation: The paper introduces the first unified and endtoend approach for anytime dense prediction which is a significant contribution to the field of computer vision and machine learning. The integration of cascades of exits and tailored exit designs for handling dense prediction tasks is notably innovative. 2. Efficiency Improvement: Demonstrating a 4060 reduction in total FLOPs while maintaining baseline accuracy levels is a compelling achievement. This efficiency is crucial for deploying deep learning models on devices with limited computational resources. 3. Comprehensive Evaluation: The experiments conducted are thorough with extensive comparisons against stateoftheart models and methods. The use of two distinct dense prediction tasks for evaluation underscores the versatility and general applicability of the proposed approach.  Weak Points: 1. Lack of RealWorld Application Analysis: While the paper convincingly demonstrates computational efficiency and accuracy it does not provide direct insights into realworld applications such as deployment on mobile or embedded systems where anytime inference could be most beneficial. 2. Hardware Dependency Discussion Lacking: The paper mentions the lack of softwarehardware support for sparse convolution which suggests a significant dependency on future developments. A deeper discussion on how current hardware limitations might impact the practical deployment of ADPC would be beneficial. 3. Potential Bias in Confidence Measure: The confidence adaptivity approach relies heavily on the confidence measure used. There is a potential risk that this measure could introduce bias toward certain classes or features impacting the model\u2019s fairness and generalization.  Questions and Feedback:  Generalization Across Datasets: How well does the proposed ADPC generalize to other datasets beyond Cityscapes and MPII Additional validation on diverse datasets could solidify its applicability.  Impact on Different Architectures: The paper primarily focuses on HRNet architecture. How does ADPC perform with other network architectures especially newer or less common ones  Handling Hardware Limitations: Can the authors suggest potential pathways or current research directions that could overcome the mentioned hardware limitations particularly for enabling efficient sparse convolution  Review Summary The paper presents an important and novel contribution towards anytime dense prediction addressing the computational inefficiency problem in deep learning for dense prediction tasks. The introduction of ADPC with its unique exit strategies and spatially adaptive confidence measures substantially reduces the computational demand while retaining accuracy which is a remarkable achievement demonstrated through rigorous experimentation. However for a comprehensive evaluation further analysis on the applicability of ADPC in realworld scenarios especially considering current hardware limitations would be beneficial. Additionally expanding the validation to more diverse datasets and network architectures could provide deeper insights into the method\u2019s robustness and versatility. In conclusion the paper provides compelling evidence supporting the effectiveness and efficiency of ADPC in anytime dense prediction tasks. With future work addressing the outlined limitations and questions ADPC has the potential to significantly influence the deployment of deep learning models in resourceconstrained environments. Based on the provided evidence and discussions I recommend the acceptance of this paper with a suggestion for minor revisions to address the discussed weak points and questions.", "gD0KBsQcGKg": " 1) Summary of the Paper The paper introduces a novel approach to estimate prediction intervals (PIs) in regression problems by proposing disjoint PIs to account for multimodal distributions of data. Traditional PI methods focus on generating a single continuous interval which may not accurately capture the underlying uncertainty when the conditional density function exhibits multiple modes. The authors argue that considering multimodality is crucial for reliable regression uncertainty estimation. To address this they develop a new method for generating a union of disjoint PIs through a differentiable objective function and a unique neural network architecture enhanced by an ensemble method for improved performance. The proposed method evaluated on UCI benchmark datasets shows significant improvement over existing stateoftheart methods by reducing the average PI width by over 27. The paper also provides insights into the multimodal nature of realworld datasets and how the novel method effectively generates highquality PIs.  2) Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty and Practical Relevance: The concept of employing disjoint PIs to handle multimodality in predictive uncertainty is novel and practically relevant. It addresses a significant limitation in current regression models offering a more accurate estimation of uncertainty. 2. Empirical Results: The paper presents compelling experimental results demonstrating a clear improvement over existing methods. The reduction in PI width by 27 on average without sacrificing coverage probability is notable. 3. RealWorld Applicability: The methods ability to provide a coverage probability for each disjoint PI is particularly valuable offering intuitive and actionable insights into the predictive uncertainty which is of great importance in critical applications like autonomous driving and energy forecasting. Weak Points: 1. Complexity and Implementation Challenges: The increased complexity of the proposed DDD method both in terms of the objective function and the neural network architecture might be a hindrance to widespread adoption. The paper could benefit from a clearer discussion on the implementation challenges and potential solutions. 2. Generalization across Different Domains: While the method shows promise in UCI benchmark datasets its effectiveness across varied domains and with notably different data distributions needs further examination. For instance how the method performs in highly dynamic systems with rapidly changing data distributions remains unanswered. 3. Comparison with Bayesian Methods: Given the significant difference in computational demands between Bayesian and nonBayesian approaches a more detailed comparison in the context of generating disjoint PIs would be insightful. Questions and Feedback: 1. Scalability: How does the DDD method scale with extremely large datasets or in realtime prediction scenarios Can you discuss the computational requirements in more detail 2. Hyperparameter Tuning: Given the novel architecture and loss function how sensitive is the model to hyperparameter settings and what strategies did you employ to tune them effectively 3. Extension to Other Forms of Uncertainty: Could the proposed method be adapted or extended to quantify other forms of uncertainty such as aleatoric or epistemic uncertainty in predictive models 4. Support for Interpretability: Can the authors elaborate on how their method supports interpretability especially in understanding the sources or reasons behind the predictive uncertainty  3) Review Summary The paper makes a significant contribution to the field of regression uncertainty estimation by addressing the limitation of single continuous PIs and introducing a method for generating disjoint PIs to better capture multimodal data distributions. The authors provide a compelling argument supported by robust experimental evidence demonstrating the superiority of their approach over existing methods. The enhancements in uncertainty quantification could have profound implications for various realworld applications where reliable uncertainty estimates are critical. However the paper would benefit from a deeper exploration of the models scalability its performance across a broader range of domains and a more detailed comparison with Bayesian approaches which are traditionally strong in uncertainty estimation. Furthermore additional discussion on the challenges of implementing the proposed method and potential strategies for mitigating these challenges would enhance its practical value. Overall the paper presents a valuable advance in predictive modeling with clear demonstrations of its improvements over stateoftheart methods. With further exploration and refinement the proposed DDD method has the potential to set a new standard for uncertainty quantification in regression problems.", "lEB5Dnz_MmH": "Review of \"Collaborative Attention Adaptive Transformer for Financial Market Forecasting Using Social Media and Market Data\" 1. Summary The paper introduces a novel approach named Collaborative Attention Adaptive Transformer for Financial market Forecasting (CAFF) that aims to improve the accuracy of financial market predictions by deeply fusing realmarket price data with tweets from social media. The model employs parallel extraction of features from both data sources uses a parameterlevel fusion for integrating these features and applies a joint feature processing module for accurate market trend predictions. Experiments conducted demonstrate the superiority of CAFF over current stateoftheart methods in terms of prediction accuracy. Additionally the paper illustrates through stock trading simulations how trading strategies based on CAFF predictions can potentially increase profits underscoring the practical application value of the model. 2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The CAFF approach is innovative particularly in its use of the collaborative attention mechanism adapted from the field of multimodal fusion in computer vision. This marks a significant advancement in financial market forecasting.  Comprehensive Methodology: The methodology including feature extraction parameterlevel fusion and joint feature processing is wellthoughtout and detailed. It addresses the critical challenge of fusing heterogeneous data sources for financial forecasting.  Empirical Validation: The paper provides extensive experimentation and comparative analysis with stateoftheart methods. The inclusion of actual trading simulations to showcase the practical applicability of the model is a commendable aspect. Weak Points:  Explanatory Depth: While the technical aspects of the CAFF framework are well articulated the paper could further benefit from an indepth discussion on why tweets play a more critical role in the CAFF framework supported by more qualitative examples or case studies.  Generalizability Concerns: The paper primarily focuses on stock market forecasting. It would be beneficial to discuss the potential applicability and modifications needed for CAFF to be useful in other financial markets or scenarios.  Reproducibility: Although the dataset is shared more detailed specifications about the computational environment hyperparameter settings and any preprocessing steps would enhance the reproducibility of the results. Questions and Feedback:  How does CAFF handle cases of misinformation or highly polarized sentiments within the tweet data which could potentially skew predictions  Could the authors elaborate on the adaptability of CAFF to rapidly changing market conditions For instance during high volatility periods do certain model components require adjustments for sustained prediction accuracy  It would be informative if the authors could share insights or preliminary results on the scalability of CAFF concerning the volume of data processed given the continuous growth of social media data. 3. Review Summary and Recommendation The paper presents a significant contribution to the field of financial market forecasting by proposing a sophisticated deep learning framework that adeptly fuses realmarket prices with social media data. The introduction of a collaborative attention mechanism for data integration represents a novel advancement that potentially sets a new benchmark for research in this area. The empirical validation backed by comprehensive experimentation and realworld trading simulations convincingly demonstrates the models effectiveness and practical value. However to further strengthen the manuscript the authors should address the outlined weak points. In particular enriching the discussion around the implications of the critical role of tweet data exploring the models generalizability and enhancing the reproducibility of the findings would be highly valuable. Given the innovative approach solid methodology and promising results presented I recommend accepting this paper contingent on the authors considering the feedback provided to refine and enrich the manuscript further. The CAFF model has the potential to influence both academic research and practical applications in financial market forecasting significantly.", "oAy7yPmdNz": " Review of \"CoordX: Accelerating Implicit Neural Representations with MultiLayer Perceptrons\"  Summary The paper presents CoordX a novel architecture designed to accelerate both training and inference phases of implicit neural representations using multilayer perceptrons (MLPs). Implicit neural representations have become a popular approach for modeling a variety of signals including images videos and 3D objects leveraging MLPs to map input coordinates to output signals. However these approaches often suffer from high computational demands. CoordX addresses this issue by proposing an architecture that decomposes the input coordinates along their dimensions before processing them through split parallel branches of the MLP. This method not only significantly reduces computational requirements but also retains accuracy comparable to traditional undecomposed MLPs across a range of tasks including image and video representation and 3D shape rendering. The paper demonstrates the effectiveness of CoordX through extensive experiments showing considerable speedups in both training and inference times without sacrificing output quality.  Strong Points 1. Innovative Approach: The method of decomposing input coordinates and processing them through split MLP branches is both novel and inventive. This approach addresses a crucial bottleneck in the application of implicit neural representations by significantly reducing computational requirements. 2. Broad Applicability: CoordX is shown to be applicable across a variety of tasks including image video and 3D object representations. Its compatibility with existing techniques and potential for further application in dynamic scene synthesis and data compression is compelling. 3. Extensive Evaluation: The experimental evaluation is thorough comparing CoordX against baseline models across multiple dimensions including speed accuracy and computational efficiency. The use of different signals and tasks adds to the paper\u2019s robustness.  Weak Points 1. Limitations on Unidimensional Signals: The paper mentions that CoordX does not improve performance for unidimensional signals such as audio. It would be beneficial to understand more about why this limitation exists and if there are potential modifications that could overcome it. 2. Accuracy Tradeoffs: While the paper notes that CoordX retains similar accuracy levels compared to baseline models there is a noted drop in accuracy for certain tasks (e.g. a 3 dB drop in PSNR for image representation). Further exploration into methods to mitigate these drops in accuracy would strengthen the contribution. 3. Complexity and Implementation Details: The paper could benefit from a more detailed discussion on the complexity of implementing CoordX particularly how the fusion operation is executed and optimized. Additionally further clarification on how the split architecture interacts with varying input signal characteristics could enhance the papers comprehensibility.  Questions and Feedback 1. Could the authors elaborate on the limitations of CoordX with unidimensional signals and potential avenues for extending its applicability to such signals 2. Are there specific characteristics of input signals (e.g. sparsity frequency components) that could affect the efficiency and efficacy of CoordX 3. The fusion operation is a critical component of the CoordX architecture. Could more details be provided on its computational complexity and any potential bottlenecks 4. Regarding the noted drop in accuracy for image representation tasks are there ongoing or future strategies the authors are considering to address this issue without significantly impacting the computational gains  Review Summary The paper introduces CoordX a noteworthy architecture that significantly accelerates the training and inference of implicit neural representations by decomposing input coordinates. This innovation addresses a critical challenge in the field offering considerable computational savings while largely preserving output quality across multiple tasks. Despite its strong contributions the paper could be enhanced by addressing its limitations on unidimensional signals and providing deeper insights into the complexity and implementation of its fusion operation. Moreover strategies to counteract the minor drops in accuracy especially for image representation tasks would make the paper even more compelling. Overall CoordX represents a significant advance in the efficient deployment of implicit neural representations with broad applicability and potential for future exploration.", "oapKSVM2bcj": "Summarize the Paper: The paper presents EinsteinInspired Notation for Operations (einops) a new approach for manipulating tensor structures in deep learning and scientific computing. This approach aims to address the lack of readability reliability and expressiveness in current tensor operations by introducing a uniform notation for tensor manipulation. Einops focuses on making the input and output tensor structures explicit improving code readability and flexibility. It is implemented in a Python package that supports multiple frameworks (e.g. PyTorch TensorFlow etc.) and eliminates API discrepancies thereby facilitating code transfer between frameworks. The paper discusses the limitations of mainstream approaches for tensor operations reviews related work on labeled tensors and einsum and details the einops notation through comprehensive examples and case studies. The ability of einops to clearly describe and transform tensor structures without breaking tensor semantics is highlighted as a key advantage over conventional methods. Strong Points: 1. Addressing a Common Pain Point: The paper effectively identifies and addresses a significant problem in the field of deep learning  the complexity and lack of readability in tensor manipulation which impacts code reliability and development efficiency. 2. Comprehensive Solution: Einops is presented as a comprehensive solution with a focus on improving code readability and reducing errors in tensor operations. Its ability to explicitly define input and output tensor structures is a significant improvement over existing approaches. 3. Wide Framework Support: The implementation of einops across multiple frameworks addresses a critical barrier in tensor manipulation  API discrepancies. This broad compatibility is a key strength enhancing its utility for a wide user base. 4. Extensive Documentation and Examples: The paper provides a wealth of examples and case studies that demonstrate the versatility and effectiveness of einops in simplifying complex tensor operations which greatly aids understanding and adoption. Weak Points: 1. Potential Learning Curve: The introduction of a new notation system may present a learning curve for practitioners already familiar with existing tensor operation conventions. The paper could discuss strategies for easing this transition. 2. Performance Considerations: While the paper mentions that einops adds marginal overhead a more detailed discussion on performance implications especially in computationally intensive tasks would be beneficial. 3. Comparison with Other Approaches: The paper reviews related work but lacks a direct detailed comparison of einops with other specific approaches (e.g. named tensors einsum) in terms of functionality performance and ease of use. Questions and Feedback to the Author: 1. Can you elaborate on any strategies or resources available to help decrease the learning curve associated with adopting einops especially for teams heavily invested in the current paradigms 2. Could the paper provide a more detailed analysis of the performance implications of using einops particularly in scenarios with largescale tensor operations 3. A direct comparative analysis with other tensor manipulation methods such as named tensors or einsum could strengthen the argument for einops superiority. Can such a comparison be included highlighting the unique benefits and drawbacks Review Summary: The paper introduces einops an innovative approach for tensor manipulation addressing significant challenges in code readability reliability and the expressiveness of tensor operations. Its broad compatibility across frameworks and focus on clearly defining tensor structures represent substantial improvements over existing methods. However the introduction of a new notation may require some acclimatization for current practitioners and a deeper analysis of performance implications would enhance the papers comprehensiveness. Overall einops presents a convincing solution to a pervasive issue in the field and its implementation can significantly benefit researchers and engineers by making tensor manipulations more intuitive and errorresistant. Additional comparisons with existing approaches could further strengthen the case for einops as the preferred methodology for tensor operations.", "rwE8SshAlxw": " Peer Review Draft  Summary The paper introduces an innovative approach called Unsupervised Discovery of Object Radiance Fields (uORF) which focuses on inferring an objectcentric scene representation from a single image without supervision. The authors address the gap in existing methods of scene decomposition which often lack unsupervised learning do not fully capture the image formation process or fail to incorporate the scenes 3D nature. uORF leverages recent advancements in neural 3D scene representations and rendering integrated with deep inference networks for unsupervised 3D scene decomposition. The proposed method is tested across three different datasets demonstrating its effectiveness in scene segmentation editing in 3D and novel view synthesis without the need for explicit supervision of 3D geometry or object segmentation.  Strong Points: 1. Innovative Approach: The paper presents a novel method that effectively combines neural radiance fields with unsupervised learning for scene decomposition addressing a significant gap in the literature. 2. Comprehensive Evaluation: The authors thoroughly evaluate uORF on various tasks (segmentation novel view synthesis and scene editing) and datasets showing its versatility and robustness in handling complex scenes. 3. Practical Implications: By enabling unsupervised factorized generative and 3Daware scene representation from a single image uORF has the potential for significant impact in areas such as augmented reality robotics and computer graphics. Weak Points: 1. Computational Efficiency: While the authors mention tackling computational inefficiency through a novel progressive coarsetofine training detailed comparisons against existing solutions in terms of computational resources and time arent provided. 2. Generalization Capability: The paper shows promising results on synthetic datasets. However the generalization of uORF to realworld images and more diverse scene complexities needs further exploration. 3. Transparency on Limitations: The paper could benefit from a more detailed discussion on the limitations of uORF including conditions under which it may fail or produce suboptimal results. Questions and Feedback to the Author: 1. Generalization to RealWorld Images: Can the authors elaborate on how uORF performs on realworld datasets if tests were conducted 2. Comparison in Computational Resources: It would be beneficial to have a comparison in terms of computational time and resources required for uORF against some of the existing methods. Can the authors provide some insights into this 3. Limitations and Potential Biases: Could the authors discuss any recognized limitations of their proposed method or potential biases in how scenes are reconstructed or interpreted by uORF 4. Future Directions: What are the authors views on extending uORFs application beyond static scenes to more dynamic environments where objects and lighting conditions may change over time Review Summary: The paper presents a compelling and innovative approach to unsupervised scene decomposition using Object Radiance Fields (uORF). Its ability to infer detailed 3D scene representations from a single image without explicit supervision marks a significant advancement in the field. The comprehensive evaluation across different tasks and datasets demonstrates the methods robustness and potential applicability in various domains. However the paper would benefit from a deeper exploration of uORFs computational efficiency generalization to realworld scenes and a more thorough discussion on its limitations and potential biases. Further research into these areas could enhance the understanding and applicability of uORF in practical dynamic environments. Overall the contributions of this work are notable and it opens up promising directions for future research in neural rendering and scene understanding.", "hjd-kcpDpf2": " Peer Review for \"Maximize Ensemble Diversity in Reinforcement Learning (MEDRL)\"  Summarize the Paper This paper introduces Maximize Ensemble Diversity in Reinforcement Learning (MEDRL) a novel suite of regularization methods inspired by economics and consensus optimization aimed at improving the diversity within ensemblebased deep reinforcement learning (DRL) methods. Acknowledging the limitation of ensemble members converging to similar states thereby losing potential benefits MEDRL promotes diversity amongst the neural networks in an ensemble during training. The approach was evaluated on a variety of algorithms and environments demonstrating significant performance improvements including up to 300 in some cases and approximately 75 improvement in sample efficiency.  Strong Points 1. Innovative Approach: The paper addresses a critical but understudied issue in DRL ensembles\u2014lack of diversity. The innovative use of economic inequality measures to promote diversity is both novel and appropriately justified. 2. Comprehensive Evaluation: The empirical evaluation thoroughly examines MEDRL across multiple standard DRL algorithms and environments. The reported improvements in performance and sample efficiency are substantial lending strong support to the proposed approach. 3. Relevance: The problem tackled is highly relevant to ongoing research and applications in deep reinforcement learning especially where ensemble methods are employed to enhance performance and reliability.  Weak Points 1. Lack of Comparison with Related Works: While the paper briefly reviews related work it does not directly compare MEDRL with other specific approaches that might address ensemble diversity or representation similarity in reinforcement learning. 2. Implementation Details: The paper could provide more detailed insights into the practical implementation of MEDRL such as the choice of regularization weights or how these might need to be adjusted based on the specific DRL algorithm or problem domain. 3. Discussion on Limitations: Though the paper presents impressive results there is a lack of discussion regarding any limitations or potential downsides of applying MEDRL such as increased computational demands or scenarios where MEDRL might not provide as pronounced benefits.  Questions and Feedback to the Author 1. Can the authors provide more direct comparisons with existing methods aimed at increasing ensemble diversity or reducing representation similarity in DRL 2. It would be beneficial if the paper could offer guidance on selecting and tuning the regularization weight (\u03bb) based on the characteristics of the DRL task or the specific ensemble algorithm. 3. Could the authors discuss potential limitations or considerations when implementing MEDRL especially in terms of computational complexity and scenarios less suited to this approach  Review Summary The paper presents a wellmotivated and methodologically sound study introducing MEDRL a regularization approach encouraging diversity within ensembles in deep reinforcement learning. The novel application of economic inequality measures to this domain is both creative and effective as demonstrated by the significant performance gains reported across various algorithms and tasks. However the paper could be strengthened by more thoroughly situating MEDRL amidst related work on ensemble diversity in DRL providing deeper implementation insights and discussing potential limitations or tradeoffs. Overall the paper makes a substantial contribution to the field addressing an important problem and offering a wellevaluated solution that significantly enhances the performance and efficiency of ensemblebased DRL algorithms.", "qWhajfmKEUt": " Summarize the paper The paper presents a novel approach to improving adversarial robustness in deep neural networks (DNNs) through Feature Spectral Regularization (FSR). By conducting spectral analysis in feature space the authors discover that the dominance of top eigenvalues in the feature covariance matrix makes DNNs vulnerable to adversarial attacks. They propose penalizing the largest eigenvalue to distribute focus more evenly across the spectrum hypothesizing that this would enhance adversarial robustness. To test their hypothesis they conduct extensive experiments across various datasets and demonstrate that FSR indeed improves adversarial robustness significantly when combined with adversarial training and other defense methods.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The introduction of FSR as a method to regulate spectral signatures in feature space is novel and tackles the problem of adversarial vulnerability from a fresh perspective. 2. Comprehensive Experiments: The paper thoroughly investigates the effectiveness of FSR across multiple datasets and in combination with existing defense methods providing solid empirical evidence of its benefits. 3. Theoretical Insight: Alongside empirical validation the paper offers a theoretical explanation based on robust linear regression which enhances the credibility and understanding of FSR\u2019s mechanism. Weak Points: 1. Complex Implementation Details: While the paper asserts that FSR is simple to implement the actual integration with existing neural network architectures especially complex ones is not clearly addressed. 2. Potential Scalability Issues: The paper focuses on ResNet18 and WideResNet3410 architectures which may limit insights into FSR\u2019s scalability and effectiveness across a broader range of models or larger datasets. 3. Dependency on Hyperparameters: Although the paper discusses the role of hyperparameters like \u03b2FSR it does not thoroughly explore how sensitive FSR\u2019s performance is to these choices or offer detailed guidance on selecting them. Questions and Feedback:  Could the authors provide more insight into how FSR would perform on more complex or deeper neural network architectures not covered in this study  It would be beneficial if the paper could include case studies or examples where FSR did not perform as expected or had negligible impacts discussing potential reasons and how such scenarios could be addressed.  Further exploration into the optimization of FSR\u2019s hyperparameters and their impact on model performance and training stability would enhance the practical applicability of this approach.  Review Summary The paper makes a significant contribution to the field of adversarial defense in DNNs by introducing Feature Spectral Regularization (FSR). FSR is a genuinely novel approach that directly impacts adversarial robustness by altering the spectral properties of feature representations. The extensive experiments conducted provide strong empirical evidence supporting FSRs effectiveness across multiple datasets and in concert with existing defense strategies. However the paper could be improved by detailing the integration process of FSR into broader neural network architectures and exploring its scalability. Additionally addressing the concerns regarding the optimization of hyperparameters and potentially adverse scenarios where FSR may have minimal effects would provide a more comprehensive understanding of this method\u2019s applicability and limitations. Despite these minor critiques the paper\u2019s foundation is solid and its implications for advancing adversarial defense mechanisms in DNNs are substantial. It opens up new avenues for research in spectral analysis within neural networks and contributes valuable insights into enhancing model robustness against adversarial attacks.", "p0rCmDEN_-": " Review of \"Enhancing Visual Acuity in LowResolution Dynamical Systems through Recurrent Computations\"  Summary The paper investigates the role of recurrent neural computations and fixational drift in enhancing visual acuity in lowresolution settings an area not thoroughly addressed in both traditional computer vision and biological vision models. By emulating the small involuntary eye movements known as fixational drifts in a dynamical sensor system the authors propose a novel architecture for image classification. This system utilizes a convolutional neural network (CNN) with recurrent connectivity added to its earlier layers designed to improve classification accuracy of lowresolution images by mimicking these biological phenomena. Experimentation with downsampled CiFAR10 and CiFAR100 datasets demonstrated that the proposed Dynamical Recurrent Classifier (DRC) not only restores classification accuracy to baseline levels of a highresolution setup but also develops a diverse set of spatiotemporal selectivity patterns amongst neurons in early layers suggesting a complex processing capability for temporal and spatial image features. Moreover the study finds curved sensor trajectories to be more effective than straight ones aligning with recent findings from eyetracking studies in humans under challenging visual conditions.  Strong Points 1. Novel Approach: The paper introduces an innovative perspective by combining insights from biological visual systems with recurrent neural networks addressing the limitations of static image processing in lowresolution conditions. 2. Comprehensive Evaluation: The experimentation is thorough with extensive comparisons to baseline models demonstrating the effectiveness of the proposed DRC model across different datasets and conditions. 3. Biological Plausibility: The work is grounded in biological phenomena providing a valuable bridge between computational neuroscience and computer vision thereby opening avenues for interdisciplinary research. 4. Detailed Analysis of Neuronal Selectivity: The studys exploration into the varied spatiotemporal selectivity patterns in the early layers of the network offers deep insights into how such models could be mimicking the early visual areas of the brain.  Weak Points 1. Lack of RealWorld Applicability Discussion: While the paper excellently bridges the gap between theory and experimentation it falls short in discussing the realworld applicability of the proposed model especially in practical vision systems like surveillance or autonomous vehicles. 2. Computational Efficiency: The paper does not address the computational efficiency and scalability of the DRC model crucial for realtime applications. 3. Limited Dataset Scope: The experiments are limited to the CiFAR10 and CiFAR100 datasets. Including more varied datasets especially realworld lowresolution datasets would have strengthened the findings.  Questions and Feedback 1. How does the computational efficiency of DRC compare with traditional CNNs and RNNs especially in realtime processing scenarios 2. Could the DRC model be adapted or extended to solve problems beyond image classification such as object detection or semantic segmentation in lowresolution settings 3. The paper could benefit from a discussion on the implications of these findings for building more biologically plausible vision systems. What are the potential limitations and benefits in practical applications 4. Given the variance in neuronal selectivity patterns observed how robust is the DRC model to variations in image quality noise and other realworld visual disturbances  Review Summary This paper presents a significant contribution to the field of computer vision by highlighting the importance of dynamical processes like fixational drift and recurrent neuronal activity in enhancing visual acuity. The proposed DRC model not only revives the classification accuracy for downsampled images but also possesses the ability to develop complex spatiotemporal processing patterns mimicking early visual areas of the brain. Such an approach is grounded in biological plausibility providing an exciting intersection between computational neuroscience and artificial intelligence. However the study could be enhanced further by addressing practical considerations related to computational efficiency scalability and broader applicability beyond image classification tasks. Additionally expanding the datasets for experimentation and delving deeper into the models realworld implications would provide a more comprehensive understanding of its potential. Nonetheless this research opens new avenues for developing sophisticated vision systems inspired by biological processes thereby contributing significantly to the advancements in artificial vision and computational neuroscience.", "u7UxOTefG2": " Review of \"On the Misconceptions of OutofDistribution Detection with Bayesian Neural Networks\"  1) Summary This paper critically examines the widely held belief that Bayesian Neural Networks (BNNs) are inherently capable of outofdistribution (OOD) detection due to their ability to quantify epistemic uncertainty. The authors challenge this assumption by analyzing both the infinitewidth scenario where BNNs correspond to Gaussian Processes (GPs) and the practical finitewidth case. Their findings reveal that common architectural choices and priors do not necessarily lead to satisfactory OOD detection capabilities. Specifically they demonstrate empirically and theoretically that the induced kernels and weight space priors often fail to model predictive uncertainties that reflect the data generating process thereby questioning the intrinsic OOD detection capability attributed to BNNs. The paper emphasizes the importance of architectural and prior choices in determining OOD behavior and illustrates that a tradeoff may exist between generalization and effective OOD detection in BNNs. The authors argue for a more nuanced understanding of BNNs capabilities and limitations in detecting OOD samples highlighting the need for further research in this area.  2) Strong and Weak Points of the Paper Strong Points:  Novel Insight on BNNs and OOD Detection: The paper challenges a common belief in the ML community regarding BNNs capabilities shedding light on fundamental issues that could steer future research in new directions.  Comprehensive Approach: The authors provide a thorough investigation that spans theoretical underpinnings empirical demonstrations and the implications of different architectural choices and priors.  Relevance: With the increasing deployment of ML models in safetycritical applications the topic of reliable OOD detection cannot be overstated. This paper contributes significantly to understanding the limitations of current strategies.  Detailed Experiments: The paper supports its claims with extensive experiments including comparisons between infinitewidth and finitewidth BNNs and the analysis of different kernels and priors. Weak Points:  Limited Applicability to RealWorld Problems: The papers scope is constrained to lowdimensional problems limiting its direct applicability to complex realworld tasks.  Somewhat Constrained Discussion of Alternatives: While the focus on BNNs is understandable a discussion on how alternative models or approaches compare in the context of OOD detection would have been beneficial.  Reproducibility Concern: The link to the source code is provided as an anonymous link. While this is likely due to the review process ensuring easy access to the code upon publication is crucial for verifying results and fostering further research. Questions and Feedback to the Author:  How do the findings extend to more complex highdimensional data typical of realworld applications  Could the authors elaborate on possible modifications or entirely different approaches that might overcome the identified limitations of BNNs for OOD detection  Is there an intuitive explanation or speculation on why certain kernels do not perform well for OOD detection when induced by common architectural choices  3) Review Summary The paper provides a critical examination of the assumption that Bayesian Neural Networks inherently possess outofdistribution detection capabilities due to their quantification of epistemic uncertainty. By exploring both theoretical aspects and practical implications through a comprehensive study involving various kernels priors and architectural choices the authors convincingly argue that BNNs do not inherently excel at OOD detection without careful consideration of these factors. This work is an important contribution to the field highlighting the complexity of OOD detection and challenging prevailing misconceptions about the efficacy of BNNs in this context. However the papers focus on lowdimensional problems limits its immediate applicability to more complex reallife tasks a point that future research inspired by this work might explore. Additionally while the lack of comparison with alternative models or approaches is noted it does not detract significantly from the papers primary contributions. Based on its strengths and the important issues it raises this paper should be considered a valuable addition to the literature prompting a reevaluation of BNNs role in OOD detection. Future work motivated by the insights provided could potentially lead to more effective methods for OOD detection in practical highdimensional settings.", "ibqTBNfJmi": " Peer Review on \"(Counterbased) Frequencyaware SGD for Embedding Learning\"  1) Summarize the paper The paper introduces two variations of Stochastic Gradient Descent (SGD) namely Frequencyaware Stochastic Gradient Descent (FASGD) and Counterbased Frequencyaware Stochastic Gradient Descent (CFSGD) tailored specifically for embedding learning problems. These variations modify the standard SGD by incorporating token frequencydependent learning rates. The authors argue that this adaptation significantly improves learning efficiency especially in scenarios with imbalanced token distributions a common situation in embedding learning domains such as recommendation systems and natural language processing. The theoretical contributions of this paper include proofs that FASGD and CFSGD outperform traditional SGD in the context of nonconvex embedding learning problems by exploiting token frequency information. Empirically the authors demonstrate that these proposed methods match or surpass the performance of existing adaptive learning rate algorithms on standard benchmark datasets and a largescale industrial recommendation system all while requiring less memory.  2) Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Contribution: The paper fills a significant gap in the literature by providing a theoretical foundation that explains why tokendependent learning rates improve the efficiency of embedding learning specifically in the presence of nonconvex optimization landscapes.  Practical applicability: The authors successfully translate their theoretical insights into practical algorithms (FASGD and CFSGD) that can be readily incorporated into existing systems.  Empirical Validation: The comprehensive experimental setup including benchmarks and a realworld industrial application substantiates the claim that the proposed methods can improve upon both standard SGD and other adaptive algorithms in efficiency and memory usage. Weak Points:  Algorithm Complexity: While the paper provides a detailed explanation of the proposed methods the complexity and practical challenges in implementing these frequencyaware approaches (especially CFSGD) might not be fully addressed.  Generalizability: The papers focus is somewhat narrow targeting embedding learning problems with imbalanced token distributions. It could benefit from a discussion on the generalizability of the proposed approach to other types of machine learning tasks.  Memory Efficiency Claims: While the authors argue that CFSGD is more memoryefficient than other adaptive algorithms further details on the environmental and engineering implications of these memory reductions could strengthen the argument. Questions and Feedback to the author:  Could the authors elaborate on how the proposed algorithms might be adjusted or extended to other machine learning contexts beyond embedding learning  The paper could benefit from a more detailed discussion on the practical aspects of implementing CFSGD specifically how the dynamic estimation of token frequencies might impact the overall system performance in realtime scenarios.  Additional insights into the tradeoffs between the memory efficiency gains and computational overhead introduced by token frequency tracking in CFSGD would be beneficial.  3) Review Summary This paper introduces and theoretically validates Frequencyaware SGD (FASGD) and its online variant CFSGD as efficient optimization methods for embedding learning problems with imbalanced token distributions. The key contribution lies in demonstrating that token frequency information can significantly enhance learning efficiency providing a solid theoretical framework and empirical evidence to support this claim. While the proposed approaches are innovative and backed by solid theoretical and empirical evidence there are areas for improvement. Specifically the paper could benefit from deeper discussions on potential challenges and solutions in practical implementations the general applicability of the methods beyond the domain of embedding learning and a more nuanced exploration of the environmental and engineering implications of the discussed memory efficiency gains. In summary the paper presents a significant advancement in the field of machine learning optimization for embedding learning offering valuable insights novel methodologies and empirical validations. With some enhancements especially regarding the broader applicability and practical implementation nuances this work has the potential to make a lasting impact on the machine learning community and beyond.", "rMbLORc8oS": " Review of \"SemiRetro: A Novel Framework for Retrosynthesis Prediction with SemiTemplates and Directed Relational Graph Attention Layers\"  1. Summary of the Paper The paper presents SemiRetro a novel framework aimed at improving retrosynthesis prediction by combining the strengths of templatebased (TB) and templatefree (TF) approaches. Retrosynthesis is crucial in synthetic planning and drug discovery involving the inference of potential reactants from a given product. The proposed solution SemiRetro seeks to enhance accuracy and scalability by dividing full templates into several semitemplates embedded into a twostep TF framework. This adjustment not only reduces template redundancy but also maintains essential chemical knowledge for synthon completion. Furthermore the paper introduces a Directed Relational Graph Attention (DRGAT) layer for better feature extraction thereby improving center identification accuracy. The performance of SemiRetro is evaluated against recent stateoftheart TB and TF methods on the USPTO50k benchmark dataset. The results show SemiRetro outperforming existing methods in terms of scalability top1 accuracy interpretability and training efficiency.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Combination: The hybrid approach of merging TB and TF methods to harness their respective advantages in accuracy and scalability is innovative. This addresses a significant challenge in the domain.  Reduction in Template Redundancy: The concept of semitemplates significantly reduces the need for a vast number of templates addressing previous scalability and computational efficiency issues.  Directed Relational Graph Attention Layer: The introduction of DRGAT layers for enhanced feature extraction is commendable. It seems to offer better performance especially in cases where reaction classes are unknown.  Comprehensive Evaluation: The detailed experimental analysis comparing SemiRetro with several stateoftheart methods using the USPTO50k benchmark dataset thoroughly demonstrates the effectiveness of the proposed method. Weak Points:  Generalization Concerns: While SemiRetro shows promising results on the USPTO50k dataset it remains unclear how well it generalizes to other datasets or more complex multistep synthesis predictions.  Interpretability Discussion Lacks Depth: Although the paper claims that SemiRetro offers better interpretability the discussion on how interpretability is measured or experienced by users (e.g. chemists) is somewhat lacking.  SemiTemplate Extraction and Maintenance: The process of creating and updating semitemplates is not elaborated upon. It would be beneficial to understand how new chemical reactions are incorporated. Questions and Feedback:  How does SemiRetro perform on datasets aside from USPTO50k especially in multistep retrosynthesis scenarios  It would be insightful if the authors could elaborate on how SemiRetro\u2019s interpretability is evaluated in practice. Is there feedback from domain experts such as chemists on its utility  Can the authors discuss the process of maintaining and updating the semitemplate repository Is there an automated process for incorporating new chemical reactions and updating templates accordingly  3. Review Summary The paper introduces SemiRetro a novel framework for retrosynthesis prediction effectively merging the merits of TB and TF approaches to achieve impressive results in accuracy scalability interpretability and training efficiency. The approach of breaking down fulltemplates into semitemplates and the implementation of the DRGAT layer are particularly commendable innovations that contribute significantly to the field of computational chemistry and drug discovery. Nonetheless further discussion on the generalization capability of SemiRetro across diverse datasets and more complex retrosynthesis scenarios would enrich the paper. Additionally delving deeper into the practical aspects of interpretability and the maintenance of semitemplates would provide a more comprehensive understanding of the frameworks utility in realworld applications. In conclusion SemiRetro represents a significant step forward in the domain of retrosynthesis prediction. The frameworks innovations not only enhance current methods but also pave the way for future research in this area. With slight improvements in addressing the highlighted concerns this work has the potential to make a lasting impact on how synthetic pathways are predicted and analyzed within the scientific community.", "givsRXsOt9r": " Summarize the Paper This paper introduces a novel approach to learning representations of 3D molecular graphs emphasizing the utilisation of spatial positions of atoms in three dimensions. Building upon the concept that conventional message passing schemes inadequately represent the complete structure of 3D molecules due to their disregard for full spatial information the authors propose the Spherical Message Passing (SMP) method. SMP leverages the spherical coordinate system to uniquely identify molecular structures using three geometrical measures: distance angle and torsion leading to the development of the SphereNet framework for 3D molecular learning. SphereNet architectured around the SMP scheme incorporates physically meaningful representations of molecular 3D information to improve performance in various prediction tasks. Unlike existing models that suffer from high computational complexity especially when scaling to large molecular graphs SMP and consequently SphereNet promise efficient learning with less computational demand. The paper boasts of superior experimental results on datasets like OC20 QM9 and MD17 showing significant advancements over baseline models in terms of capability efficiency and scalability. The work is grounded in an evident gap for a principled framework in handling 3D molecular graphs moving beyond the limitations of existing graph neural networks (GNNs) by offering a reduction in training complexity without sacrificing representational completeness.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty in Approach: The paper successfully addresses a clear gap in the field by proposing SMP a novel message passing scheme that efficiently utilizes 3D information in molecular graphs which is a significant step forward from existing methods. 2. Comprehensive Experimental Validation: The suite of experiments across multiple datasets (OC20 QM9 MD17) with comparisons to a range of baseline methods demonstrates the methods effectiveness and applicability in realworld scenarios. 3. Efficiency and Scalability: The reduction in training complexity without sacrificing the ability to represent molecular structures especially for largescale molecules is a critical advancement. Weak Points: 1. Lack of Theoretical Justification: While the SMP approach is intriguing the paper could benefit from deeper theoretical insights into why this particular method outperforms existing approaches in 3D molecular graph representation. 2. Generalizability Concerns: The specific focus on molecular structures might limit understanding of the applicability of SMP and SphereNet to other 3D graphs not based on chemical properties or structures. 3. Failure Cases Not Explored: The paper mentions uncovered cases that the SMP approach may not represent assumed not to exist in practice. A more detailed exploration or discussion of these potential failure scenarios would strengthen the paper. Questions and Feedback: 1. Could the authors provide more theoretical background or justification for the design choices in SMP Specifically why are the chosen geometrical measures (distance angle torsion) sufficient for a comprehensive representation 2. Are there potential applications or extensions of SMP and SphereNet beyond molecular structures If so could the authors elaborate on the adaptability of their approach to other domains 3. It would be beneficial if the authors could discuss or provide insights into the specific cases where SMP might fail or not accurately represent the 3D structure. Are there theoretical limitations based on the chosen spherical coordinate system  Review Summary The paper represents a significant contribution to the field of representation learning for 3D molecular graphs through the introduction of SMP and the development of SphereNet. The novelty of the approach and its demonstrated efficacy across multiple datasets underscore the importance and timeliness of this work. Despite the strong experimental validation the paper would benefit from a deeper theoretical exploration of SMPs effectiveness and a discussion on the generalizability of the method to other domains or types of 3D graphs. Furthermore addressing potential failure cases in more detail would provide a more comprehensive understanding of the methods limitations. Overall the paper stands out for its innovative approach efficiency and scalability in 3D molecular learning marking a noteworthy advancement in the field.", "jWaLuyg6OEw": "Summarize the paper: The paper investigates the performance of two firstorder optimization algorithms developed from the forward Euler discretization of finitetime optimization flows: the rescaledgradient flow (RGF) and the signedgradient flow (SGF). These flows which include nonLipschitz or discontinuous dynamical systems locally converge to the minima of gradientdominated functions in finite time. The authors present an Euler discretization for these flows and offer convergence guarantees in both deterministic and stochastic settings. The paper tests the proposed algorithms on synthetic examples and realworld data specifically deep neural network (DNN) training on the SVHN dataset and demonstrates faster convergence in comparison to standard optimization algorithms. List strong and weak points of the paper Questions and Feedback to the author: Strong Points: 1. Theoretical Contribution: The paper extends finitetime convergence analysis to discretized versions of RGF and SGF enriching the theoretical understanding of these optimization algorithms. 2. Broad Applicability: By providing convergence guarantees and testing on both synthetic examples and realworld deep learning problems the paper showcases the broad applicability of the proposed algorithms. 3. Empirical Validation: The rigorous empirical testing on the SVHN dataset and comparison with standard optimization methods (including SGD and Adam) validate the proposed schemes effectiveness in realworld scenarios. Weak Points: 1. Assumptions and Generalizability: The paper relies on gradient dominance and Lipschitz smoothness assumptions. It would be helpful to understand better the algorithms performance outside these conditions or when these assumptions are only weakly met. 2. Comparison with Recent Optimizers: The field of optimization algorithms especially for deep learning is rapidly evolving. A more extensive comparison with a broader set of stateoftheart optimizers could provide more context on the proposed algorithms relative performance. 3. Implementation Details and Reproducibility: More detailed discussion on the practical implementation of the algorithms including sensitivity to initialization and hyperparameter tuning would aid in reproducibility and practical application by other researchers or practitioners. Questions and Feedback: 1. Scalability: How do the proposed algorithms scale with the dimensionality of the problem or the size of the neural network Additional insights into computational complexity would be valuable. 2. Robustness to Noise: In stochastic settings how robust are RGF and SGF to noise in gradient estimates and do the convergence properties hold under lessthanideal conditions 3. Hyperparameter Tuning: The paper mentions the selection of best step size through iterations. Can the authors shed more light on the sensitivity of the proposed algorithms to hyperparameter choices and any guidelines for their selection Review Summary: The paper makes a significant theoretical contribution by extending the finitetime convergence analysis to the discretized versions of RGF and SGF with practical implications for optimization in machine learning. The empirical evidence provided through testing on synthetic and realworld datasets especially on DNN training underscores the proposed algorithms effectiveness over traditional optimization methods. However the paper would benefit from a deeper exploration of the algorithms under varied assumptions an expanded comparison with a broader set of contemporary optimizers and more detailed practical implementation guidance. Addressing these aspects would enhance the papers impact and applicability in optimization and machine learning domains. Overall the theoretical insights and empirical results present a compelling case for the utility of the proposed discretization schemes in accelerating convergence in optimization problems.", "q79uMSC6ZBT": " Review of \"GRAMMFORMER: Generating Code Completions with Holes\"  Summary This paper introduces GRAMMFORMER a novel Transformerbased model designed to improve code completion tasks by generating code snippets (\"sketches\") with \"holes\" where the model is uncertain. The approach diverges from traditional lefttoright code generation by using programming language grammars to guide the generative process thus acknowledging and accommodating uncertainty in code prediction tasks. This method contrasts with standard language models (LMs) that tend to produce full sequences without consideration for potential inaccuracies. GRAMMFORMER is evaluated against standard sequence models on C and Python datasets utilizing both ROUGE and a newly proposed metric REGEXACC to measure the accuracy and precision of the generated code completions. The results indicate significant improvements over traditional models showcasing the potential of grammarguided code generation in producing more accurate and helpful code completions.  Strong Points 1. Innovative Approach: The papers concept of generating code sketches with holes represents a significant shift from conventional code completion strategies offering a more pragmatic solution that mirrors the uncertainties inherent in coding tasks. 2. GrammarGuided Generation: By leveraging programming language grammars GRAMMFORMER can generate more coherent and contextually appropriate code completions which is a clear advancement over purely probabilistic language models. 3. REGEXACC Metric: The introduction of a new evaluation metric tailored to the specific task of assessing code completions with holes is both novel and necessary providing a more relevant measure of performance for this research area. 4. Comprehensive Evaluation: The thorough comparison of GRAMMFORMER with baseline models across two programming languages using both ROUGE and REGEXACC metrics provides a solid foundation for the claimed improvements.  Weak Points 1. Computational Cost: The increased computational demands of GRAMMFORMER due to its grammarguided generation process are mentioned but not thoroughly quantified or addressed as a potential limitation. 2. Human Evaluation Lacking: While the paper introduces new metrics for evaluation there is no human evaluation of the code completions generated which could provide insights into the practical usefulness and acceptability of the suggested completions. 3. Limited Language Support: The study focuses on C and Python and its unclear how easily GRAMMFORMER can be adapted to other programming languages or how its performance might vary across languages with significantly different grammatical structures. 4. Data Source and Diversity: The paper does not discuss the diversity or representativeness of the GitHub repositories used for training and evaluation which could impact the models generalizability and performance across different coding styles and domains.  Questions and Feedback 1. Could the authors provide more details on the computational resources and time required for training and running GRAMMFORMER compared to traditional models 2. How might GRAMMFORMER be adapted to support a wider range of programming languages especially those with less formal or more complex grammatical structures 3. Is there a plan to include human evaluation in future work to assess the practical applicability and user satisfaction with the generated code completions 4. Could the authors comment on the potential impact of the datasets diversity on the models performance and how they plan to address this in future work  Review Summary The paper presents GRAMMFORMER a promising approach to code completion that acknowledges the inherent uncertainty in programming tasks by generating code sketches with holes. This innovative strategy guided by programming language grammar represents a significant improvement over traditional language models as evidenced by the performance on C and Python datasets. The introduction of the REGEXACC metric is a notable contribution providing a bespoke measure of effectiveness for code completion tasks. However the paper could be strengthened by addressing the computational costs expanding the language support including human evaluation and considering the datasets diversity in more detail. Overall the work is a valuable step forward in the field of code generation offering a more realistic and potentially more userfriendly approach to code completion.", "sPfB2PI87BZ": " Review Summary This paper presents OSTAR (Optimal Sample Transformation And Reweight) a novel approach for Unsupervised Domain Adaptation (UDA) under conditions of Generalized Target Shift (GeTarS). The method focuses on aligning pretrained representations by learning an Optimal Transport (OT) map implemented as a neural network which adjusts source domain representations to match those of the target domain. OSTAR stands out by circumventing the limitations of existing approaches that rely on domaininvariant representations which often deteriorate target discriminativity. The paper is wellorganized detailing the problem setup proposing a solution providing theoretical guarantees under mild assumptions and validating the approach through extensive experiments.  Strong Points: 1. Novelty and Significance: OSTAR addresses the underexplored problem of UDA under GeTarS with a unique approach that maintains target discriminativity while achieving alignment through an optimal transport map. This is an important contribution offering an alternative to domaininvariant representation learning. 2. Theoretical Justification: The paper provides a strong theoretical foundation for OSTAR including unicity of the solution and the ability to minimize target risk explicitly. The assumptions made are less restrictive compared to related works which increases the applicability of the approach. 3. Experimental Validation: The authors conduct comprehensive experiments across several datasets comparing OSTAR with stateoftheart approaches and demonstrating its competitiveness or superiority in most cases. The inclusion of Information Maximization (IM) as an extension further improves performance illustrating the methods flexibility. 4. Reproducibility and Code Availability: The paper includes a detailed reproducibility statement and the authors provide the source code ensuring that the research community can validate and build upon this work.  Weak Points: 1. Comparison with Domaininvariant Methods: While the paper shows OSTARs advantages over domaininvariant methods it could benefit from a deeper discussion on situations where one approach may be preferable over the other considering the tradeoffs involved. 2. Assumptions Validation: The authors make several assumptions to establish theoretical guarantees for OSTAR. It would be insightful to see an empirical validation or discussion of these assumptions in realworld datasets. 3. Impact on Discriminativity: The claim that domaininvariant methods may deteriorate target discriminativity is central to motivating OSTAR. More explicit evidence or examples where this phenomenon occurs would strengthen the argument. 4. Scalability and Complexity Analysis: While the paper mentions that OSTAR is scalable detailed analysis comparing its computational complexity and runtime against other UDA methods would be valuable for readers especially in practical applications.  Questions and Feedback: 1. Could the authors provide more insights into scenarios where OSTAR might not perform as well as expected such as specific characteristics of the source and target domains 2. How does the choice of neural network architecture for the OT map impact the performance of OSTAR and were other architectures besides ResNet considered 3. Can the authors discuss the implications of their approach for domains other than those experimented on such as natural language processing or speech recognition  Review Summary: OSTAR represents a significant advancement in the field of Unsupervised Domain Adaptation under Generalized Target Shift. Its novel approach of using Optimal Transport for aligning pretrained representations offers a promising alternative to existing domaininvariant methods addressing key limitations regarding target discriminativity. The thorough theoretical groundwork and comprehensive experimental validation underscore the methods effectiveness and potential. However further exploration into the robustness of the assumptions scalability and comparative analysis with domaininvariant approaches could enhance the papers impact. Nonetheless OSTARs flexibility theoretical guarantees and strong empirical performance make it a valuable contribution to the field.", "nhnJ3oo6AB": " Summarize the paper The paper presents a novel method LocoTransformer for enhancing quadrupedal robot locomotion through the combination of proprioceptive states and visual observations using a Transformerbased model. Leveraging both forms of input allows the robots to anticipate and navigate obstacles and uneven terrains more effectively than previous methods relying solely on proprioceptive data. By incorporating visual information the robot can proactively plan its movements leading to significant improvements in navigating complex environments. The authors evaluate their method in both simulated and challenging realworld scenarios showing superior performance and better generalization abilities compared to baselines. The project integrates these advances without resorting to hierarchical reinforcement learning or predefined controllers making it a pioneering step towards more adaptively navigating robots in dynamic environments.  Strong and weak points Questions and Feedback Strong Points: 1. Innovative Approach: The paper introduces a novel integration of proprioceptive states and visual inputs using a Transformer model for robot locomotion a significant advance over relying solely on proprioceptive data. 2. Comprehensive Evaluation: The extensive experiments across simulated and realworld environments validate the effectiveness and generalization capability of the proposed LocoTransformer model. 3. RealWorld Applicability: Demonstrating the methods performance in realworld scenarios especially in environments with unseen obstacles and terrains underscores its practical value. 4. Attention Mechanism Insight: Visualization of attention maps offers interesting insights into the models decisionmaking process highlighting how it prioritizes different environmental features. Weak Points: 1. Complexity and Computational Requirements: The paper does not thoroughly discuss the computational demands or the complexity of implementing the proposed Transformer model in realtime applications. 2. Limited Exploration of Failure Cases: There is a lack of detailed discussion on scenarios where LocoTransformer might fail or its limitations which is crucial for understanding the models applicability bounds. 3. Comparison With Latest Techniques: While the paper provides comparisons with several baselines it does not benchmark against the latest advancements in visionbased or multimodal RL for robot locomotion. Questions and Feedback: 1. Can the authors provide more details on the computational requirements of LocoTransformer especially in relation to realtime applicability on different robotic platforms 2. It would be beneficial if the authors could discuss potential failure scenarios or limitations observed during experiments to better understand the models robustness. 3. Including comparisons with recent advancements in the field could better position the proposed method within the current stateoftheart. Are there plans to benchmark against newer models  Review Summary The paper by the authors represents a significant advancement in the field of quadrupedal robot locomotion proposing a novel method LocoTransformer that effectively combines proprioceptive states and visual data using a Transformerbased model. This innovation enables robots to better anticipate and navigate complex environments demonstrating superior performance and generalization abilities over traditional approaches relying solely on proprioceptive information. Though the article is strong in its innovative approach extensive evaluation and practical applicability it does leave some areas unexplored such as detailed computational requirements discussion on limitations or failure scenarios and comparisons with the latest techniques in the field. Addressing these could strengthen the papers impact and applicability further. Despite these minor shortcomings the contributions made by LocoTransformer in enhancing the adaptability and efficiency of quadrupedal robots in dynamic environments are undeniably valuable. It opens new avenues for research and development in robotic autonomy and multimodal reinforcement learning making a compelling case for its adoption in future robotics research and applications.", "rvost-n5X4G": " Review of \"State Planning Policy Reinforcement Learning (SPPRL): A Novel Approach Towards Efficient and Interpretable RL Agents\"  1. Summary of the Paper This paper introduces a novel algorithm in the realm of reinforcement learning (RL) called State Planning Policy Reinforcement Learning (SPPRL). Unlike traditional RL algorithms that operate on the principle of selecting actions based on the current state (stateaction mappings) SPPRL focuses on planning the desired next state given the current state (statestate mappings). It uses an inverse dynamics control model (IDM) trained via supervised learning to translate the policys target states to actions. The authors employ offpolicy RL algorithms such as DDPG TD3 and SAC for training the RL agent within this framework. A significant innovation in SPPRL is formulating the learning process as a constrained optimization problem ensuring that the planned target states are physically relevant. This approach is theoretically grounded and benchmarked across various continuous control environments including MuJoCo tasks and SafetyGym levels where it demonstrates superior performance over traditional RL algorithms.  2. Strengths and Weaknesses Strengths:  Innovative State Planning Approach: The novel conceptualization of transitioning from stateaction to statestate mappings promises a profound impact on how policies are designed potentially enhancing interpretability and efficiency in RL.  Robust Methodological Framework: The use of constrained optimization coupled with Lagrange multipliers to ensure physical relevancy of target states is theoretically sound and innovative within the RL domain.  Comprehensive Benchmarking: The paper presents extensive experiments across diverse environments effectively comparing the performance of SPPRL with vanilla RL algorithms thereby validating the proposed approachs effectiveness.  OpenSource Contribution: By making their implementation available as an opensource PyTorch library the authors significantly contribute to reproducibility and further research in the field. Weaknesses:  Complexity and Computational Demand: The paper suggests that the SPP approach while effective might introduce considerable complexity and computational overhead due to the additional training of the IDM and the constrained optimization process.  Limited Scope on Safety Aspects: Despite showcasing experiments in SafetyGym environments the paper doesnt thoroughly explore or quantify the safety improvements or implications of using SPPRL over traditional methods.  Underexplored Longterm Policy Planning: The potential for integrating SPPRL with longterm policy planning and its implications on performance and safety are mentioned but not explored in depth. Questions and Feedback to the Author:  Could you elaborate on the computational efficiency of SPPRL in comparison to traditional RL algorithms especially in terms of training time and resource requirements  How does the SPPRL framework integrate with or improve upon safetycritical applications given its initial promising results in SafetyGym environments  Is there potential for this approach to be adapted or extended to discrete action spaces or is its application limited to continuous control tasks  3. Review Summary The paper presents a compelling novel approach to reinforcement learning through the introduction of State Planning Policy Reinforcement Learning (SPPRL). This methodology offers a departure from traditional stateaction mappings promising advancements in policy efficiency and interpretability. The rigorous methodological framework extensive benchmarking and opensource contributions significantly bolster the papers credibility and impact potential. However the additional complexity and computational demands of the SPPRL approach pose challenges that need to be addressed. Furthermore while the paper touches upon the relevance of SPPRL to safetycritical applications a more indepth analysis in this area could enhance its applicability and appeal. It is also essential to explore the possibility and implications of extending this approach to discrete action spaces or more complex environments. In conclusion the paper represents a significant step forward in the pursuit of more efficient and interpretable RL algorithms. With further research and exploration of outlined limitations SPPRL has the potential to become a foundational approach in the RL landscape.", "mNLLDtkAy4X": "Summarize the paper The paper introduces Aleatoric Mapping Agents (AMAs) a novelty in the field of artificial intelligence and reinforcement learning inspired by the cholinergic system in the mammalian brain. These agents are designed to enhance exploration in environments with sparse extrinsic rewards by using a form of curiosity based on intrinsic rewards. Unlike traditional curiositydriven agents that may get trapped by unpredictable actiondependent noise sources (termed as stochastic traps or the noisy TV problem) AMAs can differentiate between predictable and unpredictable dynamics in the environment. They achieve this by generating separate predictions for the mean state and the aleatoric (inherent) uncertainty of future states allowing them to reduce intrinsic rewards for transitions that are deemed unpredictable. The paper demonstrates that AMAs can successfully navigate environments that would typically ensnare curiositydriven agents providing both a theoretical and empirical advancement in the field. Additionally the work suggests a linkage between their model and biological processes offering a theoretical prediction for the role of acetylcholine in signaling uncertainties and proposing an experimental design to test this hypothesis in a realworld setting. Strong points 1. Novel Approach: The introduction of AMAs presents a significant advancement in dealing with the noisy TV problem in reinforcement learning addressing a notable gap in the literature where traditional curiosity models fail. 2. Neuroscience Inspiration: The novel form of curiosity inspired by mammalian brain processes particularly the role of acetylcholine in signaling uncertainty offers a fascinating interdisciplinary bridge between neuroscience and artificial intelligence. 3. Robust Testing: The paper conducts thorough experiments across various environments (e.g. Noisy MNIST Deep Learning MiniGrid Mario Space Invaders and Bank Heist) to demonstrate the efficacy of AMAs under different conditions providing extensive empirical support for their model. 4. Theoretical and Practical Contributions: By presenting a mechanism for avoiding stochastic traps and proposing an experimental design to test a neuroscience hypothesis the paper makes both theoretical and practical contributions to the field. Weak points 1. Complexity and Generalizability: While AMAs show promising results there is a concern about the complexity of implementing such agents in highly variable or realworld environments outside of the tested settings. 2. Limitations on Aleatoric Uncertainty Estimation: The paper acknowledges that aleatoric uncertainty estimates might not be reliable for outofdistribution data which could limit the scalability and effectiveness of AMAs in novel or highly dynamic environments. 3. NonStationary Rewards: As with other curiositydriven approaches the generation of nonstationary rewards might make learning more challenging for RL agents. Theres limited discussion on overcoming this inherent drawback in the long term. Questions and Feedback 1. Can the authors elaborate on how AMAs might be adapted or extended to handle outofdistribution data more reliably 2. Given the acknowledgment of nonstationary rewards as a challenge could future work explore mechanisms to stabilize learning in the presence of such rewards 3. How might the complexity of implementing AMAs in more variable environments be addressed to ensure broader generalizability Review Summary The paper presents a significant advancement in reinforcement learning by introducing Aleatoric Mapping Agents (AMAs) which effectively address the challenge of actiondependent stochastic traps that limit the exploration capabilities of curiositydriven agents. The approach is notably innovative drawing inspiration from the mammalian cholinergic system and is supported by robust empirical evidence across a range of test environments. However concerns regarding the complexity of the model the reliability of aleatoric uncertainty estimates for outofdistribution data and the inherent challenges posed by nonstationary rewards warrant further attention. Questions about extending the models reliability in more diverse environments and addressing inherent limitations in curiositydriven learning models are crucial for future research. Nonetheless the theoretical underpinnings and the proposed experimental design to probe the role of acetylcholine in uncertainty signaling represent a commendable interdisciplinary effort that could inspire new lines of inquiry in both neuroscience and artificial intelligence.", "wQfgfb8VKTn": " Review of \"ContextAware Sparse Coordination Graphs for Cooperative Multiagent Learning\"  Abstract Summary: This paper addresses the challenge of constructing dynamic sparse coordination graphs for cooperative multiagent reinforcement learning (MARL). The authors present a novel methodology that leverages the variance in payoff functions as a metric for adaptive construction of sparse coordination graphs termed ContextAware SparsE Coordination graphs (CASEC). The method is theoretically grounded with proofs provided to support the approach. An action representation learning component is introduced to enhance the stability and efficiency of learning. The effectiveness of the methodology is demonstrated through the MultiAgent COordination (MACO) benchmark and the StarCraft II micromanagement benchmark. The MACO benchmark crafted for this study aggregates classic coordination challenges categorizing and escalating their complexity.  Strong Points: 1. Innovative Approach: The paper introduces an original and theoretically supported method for generating sparse coordination graphs in MARL environments. The use of variance in payoff functions for this purpose is both innovative and logically sound. 2. Theoretical Justification: Providing a theoretical basis for why lower variance in payoff functions means less impact on greedy action selection upon the removal of graph edges strengthens the credibility of the proposed method. 3. Empirical Validation: The extensive empirical evaluation across the customdesigned MACO benchmark and the established StarCraft II benchmark offers strong evidence of the methods effectiveness and generality across different settings. 4. Action Representation Learning: Incorporating action representations to mitigate the effects of payoff function estimation errors is a clever solution that addresses a common challenge in sparse graph construction for MARL. 5. Communication Efficiency: Showing that sparse graphs can lead to substantial reductions in communication overhead without sacrificing performance is of practical significance for realworld applications.  Weak Points: 1. Scalability Concerns: While the approach is shown to be effective on the MACO benchmark and StarCraft II micromanagement tasks the scalability of the algorithm to even larger and more complex multiagent settings was not directly addressed. 2. Cycle Handling in Graphs: The paper acknowledges that the learned sparse coordination graphs are not always cyclefree which may lead to suboptimal action selection. Although mentioned the depth of treatment on how to effectively tackle this limitation seems insufficient. 3. Adaptive Threshold Selection: The methodology fixes the sparse graph construction threshold which may not be optimal across different stages of learning or different environments. The discussion on adaptively selecting this threshold is relegated to the appendix indicating a potential area for more indepth exploration in the main sections. 4. Comparison with Baseline Methods: While the paper compares the proposed method with several stateoftheart approaches a more detailed discussion on the relative advantages and limitations compared to each would enrich the analysis.  Questions and Feedback:  Could the authors elaborate on potential strategies to ensure the scalability of the proposed method to even larger more complex multiagent systems  How might the proposed method be adapted or extended to handle cycles in the coordination graphs more effectively  Would the authors consider exploring adaptive methods for selecting the communication threshold within the learning process potentially enhancing performance further  A comparison between the computational overhead of CASEC and other baseline methods would be insightful. Could the authors provide such an analysis to highlight the efficiency of the proposed method  Review Summary: The paper introduces a significant contribution to cooperative MARL through the development of a novel methodology for generating dynamic sparse coordination graphs. The approach is thoughtfully designed with theoretical and empirical validations lending it credence. The proposed method not only addresses the challenge of constructing efficient coordination graphs but also mitigates common issues such as instability in learning through the innovative use of action representations. While the method shows promise there are areas such as scalability and cycle handling in graphs where further development could enhance its applicability and effectiveness. Overall the paper presents an impactful advance in MARL deserving serious consideration for publication provided the authors can clarify and potentially expand on the points of concern raised.", "gI7feJ9yXPz": " Summary This paper addresses the significance and increasing application of minimax problems across a variety of machine learning tasks such as reinforcement learning robust optimization adversarial learning and distributed computing among others. Despite the focus on convergence behavior in current studies theres a notable gap in examining the generalization performance of minimax problems. The work stands out by offering improved generalization analyses and establishing sharper high probability generalization bounds for a spectrum of generalization measures within minimax problems. It pioneers in utilizing improved learning bounds to provide high probability generalization bounds with fast rates for classical empirical saddle point (ESP) solutions and several gradientbased optimization algorithms including gradient descent ascent (GDA) stochastic gradient descent ascent (SGDA) proximal point method (PPM) extragradient (EG) and optimistic gradient descent ascent (OGDA). A systematic analysis of sharper generalization bounds in minimax problems is presented contributing significantly to the understanding and application of minimax learning problems.  Strong Points 1. Comprehensiveness: The paper offers a detailed and systematic analysis of generalization bounds within minimax problems covering a broad range of algorithms and conditions. This comprehensive approach significantly advances the understanding of minimax problem generalization. 2. Novelty of High Probability Bounds: The introduction of sharper high probability generalization bounds provides a novel contribution to the field. The move from expected bounds to high probability bounds represents a significant shift toward understanding algorithm robustness better. 3. Practical Implications: By applying the theoretical findings to popular gradientbased optimization algorithms the paper bridges the gap between theoretical machine learning research and practical applications particularly in areas like GANs training.  Weak Points 1. Assumption Clarity: While the paper elaborates on various assumptions required for the theorems it might benefit from a clearer discussion on the practical feasibility of these assumptions in realworld scenarios. 2. Experimental Validation: The paper lacks empirical validation of the theoretical findings. Including experiments or case studies could substantially strengthen the claims and demonstrate the bounds practical significance. 3. Comparison to Related Work: While the paper does a commendable job discussing related work a more detailed quantifiable comparison with previous generalization bounds would help emphasize the improvements made.  Questions and Feedback 1. Assumption Feasibility: Can the authors provide more insights into the feasibility of the assumptions made in practical scenarios Are there known limitations to these assumptions in certain machine learning tasks 2. Empirical Evidence: Are there plans to empirically validate the theoretical findings Such validation could significantly enhance the papers contribution by providing practical evidence of the improvements in generalization bounds. 3. Impact on Training Efficiency: How do the improved generalization bounds affect the efficiency of training algorithms in practice Is there a tradeoff between achieving sharper bounds and computational complexity  Review Summary The paper presents a valuable contribution to the study of generalization performance in minimax problems by establishing improved and sharper high probability generalization bounds. Its comprehensive and systematic analysis across a variety of algorithms and conditions is commendable. However the paper would benefit from clearer discussions on the practicality of its assumptions inclusion of empirical evidence to validate theoretical findings and a more detailed comparison with existing works. Addressing these aspects could significantly bolster the papers impact and applicability to broader machine learning tasks. Despite these limitations the papers novel approach to high probability bounds and its implications for practical applications represent a noteworthy advancement in the field of machine learning research.", "obi9EkyVeED": " Review of \"FedDrop: Accelerating Federated Learning with ChannelWise Weighted Dropout\"  1) Summarize the paper. The paper introduces FedDrop a novel approach designed to enhance Federated Learning (FL) by focusing on reducing the computational cost without significantly hampering communication efficiency. It leverages the diversity in data distribution across clients by introducing channelwise weighted dropout layers between convolutional layers termed SyncDrop layers. These layers dynamically adjust the dropout keep probabilities for each neuron based on interclient update trajectories. The objective is to focus training effort on neurons relevant to the local data distribution thus reducing the FLOPs required for training. The authors present findings from comprehensive experiments across various datasets demonstrating that FedDrop improves the communicationcomputation tradeoff significantly compared to existing FL approaches.  2) Strong and Weak Points Questions and Feedback to the Author. Strong Points:  Innovation: The introduction of SyncDrop layers and the notion of adjusting dropout probabilities dynamically based on interclient update trajectories are novel and theoretically sound.  Empirical Evidence: The paper provides thorough experimental evaluation and comparison with existing methods across multiple datasets showing substantial improvements in the communicationcomputation tradeoff.  Theoretical Framework: The derivation of the FedDrop method including the introduction of FLOPs constraints and the optimization of dropout keep probabilities is wellfounded and contributes to the robustness of the approach. Weak Points:  Generalization Concerns: The paper primarily focuses on convolutional neural networks (CNNs) and does not clearly address how the FedDrop method would perform with nonCNN architectures.  Complexity and Scalability: While the paper addresses the computational and communication tradeoffs the added complexity due to the introduction of SyncDrop layers parameter initialization and optimization stages might pose scalability issues in FL setups with a very large number of edge devices.  Lack of RealWorld Validation: The experiments are conducted in simulated environments. The impact of realworld factors such as device heterogeneity and intermittent connectivity on the performance of FedDrop is not discussed. Questions and Feedback:  How does FedDrop adapt to highly dynamic environments where data distributions may change frequently over time  The paper could benefit from a clearer discussion on the scalability of FedDrop regarding the number of clients and the heterogeneity of computational resources.  It would be interesting to see a validation of the FedDrop approach in realworld scenarios possibly through partnerships with organizations deploying federated learning in practice.  3) Review Summary FedDrop introduces an innovative and theoretically grounded methodology to tackle the computational cost challenges inherent in federated learning especially in the context of nonIID data distributions. By dynamically adjusting dropout probabilities through the SyncDrop layers the paper demonstrates a promising avenue towards more efficient federated learning concerning the communicationcomputation tradeoff. The experimental results strongly support the superiority of FedDrop over existing methods. However the approachs adaptability to varying model architectures and scalability in truly decentralized environments with potentially thousands of heterogeneous edge devices remains to be thoroughly examined. Furthermore realworld applicability and resilience to changing data distributions and network conditions would significantly enhance the papers contribution to the FL field. Given the strengths of the proposed method and the promising experimental results I recommend this paper for publication with minor revisions particularly focusing on addressing the scalability and realworld applicability concerns.", "fQTlgI2qZqE": " Peer Review for \"Discovering Feature Interactions in Blackbox Models Using MultiArm Bandits and ParaACE\"  1. Summary This paper introduces a novel method for detecting feature interactions within blackbox models aiming to enhance explainable deep learning. The authors approach the problem by formulating interaction detection as a multiarm bandits problem which is solved using the Upper Confidence Bound (UCB) algorithm. This method is reported to detect interactions accurately and rapidly without relying on adhoc assumptions making it a leading solution in terms of detection stability and accuracy. Subsequently the detected interactions are used to construct a new model ParaACE which is lightweight interpretable and outperforms its predecessor (the Teacher model) in prediction accuracy by 26 while being significantly smaller in size. The paper highlights the potential of their method in aiding scientific discoveries in economics and smart medicine by presenting interpretations of real datasets. The approach is modelagnostic flexible for detecting multiway interactions and is accompanied by theoretical analysis performance evaluations and evidence of applicability across different domains.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Novelty and Contribution: The paper presents a unique method for feature interaction detection that fills a gap in explainable machine learning by providing a modelagnostic and efficient solution.  Theory and Algorithm Design: The thorough theoretical background including the reliance on wellestablished mathematical frameworks such as the multiarm bandits problem and the UCB algorithm enhances the credibility of the proposed method.  Empirical Validation: The comprehensive experimental setup including synthetic and real datasets from varied domains convincingly demonstrates the effectiveness of the proposed method.  Weak Points:  Generalization Across Different Models: While the paper claims modelagnostic capabilities the experiments mainly focus on deep learning models. It is not clear how well the method performs on drastically different models like simplistic linear models or highly complex ensembles beyond trees and neural networks.  Scalability Concerns: Although the method is presented as computationally efficient there is limited discussion on its scalability especially concerning very highdimensional data or extremely large datasets.  Interpretability of Multiway Interactions: The paper briefly addresses multiway interactions but does not deeply explore the interpretability of these interactions which are crucial for domains like genomics or complex systems analysis.  Questions and Feedback:  Can the authors provide more details on the scalability of their method in the context of highdimensional datasets or when applied to models trained on massive datasets  How does the proposed interaction detection method perform when applied to simpler models such as linear regression or SVMs compared to more complex models like deep neural networks  The paper would benefit from a deeper discussion on the interpretability of multiway interactions including potential limitations and how they might be addressed in future work.  3. Review Summary The paper introduces an innovative and theoretically sound approach for detecting feature interactions in blackbox models proposing the ParaACE model for improved prediction performance and interpretability. The methodology is robust supported by comprehensive theoretic analysis algorithm design and extensive experiments showcasing significant improvements over existing methods. However the generalization of the proposed method across a broad spectrum of model types and its scalability remains unclear. Moreover while the paper touches upon multiway interactions this aspect warrants further exploration to enhance the methods applicability in complex domains. Given these considerations I recommend acceptance contingent on addressing the mentioned limitations particularly by clarifying the methods performance on various model types and its scalability. Further elaboration on the treatment of multiway interactions would also make a substantial contribution to the papers impact and applicability.", "nT0GS37Clr": " Peer Review  Summary of the Paper The paper introduces Federated Supermask Learning (FSL) a novel technique aimed at improving federated learning (FL) by addressing two critical challenges: robustness against poisoning attacks and communication efficiency. Instead of training a neural network in the traditional sense FSL leverages a randomly initialized neural network (supernetwork) and collaboratively trains to find a subnetwork or supermask that achieves comparable performance to fully trained models. This is achieved by each client computing the importance (ranking) of each edge in the supernetwork based on their local data. These rankings are then aggregated by the server through a voting mechanism to form a global supermask. The approach is claimed to offer robustness by design against poisoning attacks by limiting the manipulation space for attackers and significantly reducing communication costs by sharing integer rankings instead of floatingpoint weights. Theoretical analysis and experiments demonstrate FSLs comparable performance to FedAvg with reduced communication costs and enhanced robustness to attacks.  Strong Points 1. Novel Approach: FSLs paradigm of using supermask (binary masks) on a supernetwork for FL is innovative and addresses the central FL challenges (robustness and communication efficiency) directly. 2. Robustness by Design: The mechanism of sharing edge rankings rather than actual weights inherently limits the opportunities for effective poisoning offering a builtin defense mechanism. 3. Communication Efficiency: The method significantly reduces the required communication bandwidth by sending compact integerbased edge rankings instead of highprecision weight values. 4. Empirical Validation: The authors have presented a comprehensive set of experiments to validate FSLs performance demonstrating its effectiveness in realworld FL settings including its robustness to poisoning attacks and communication efficiency. 5. Theoretical Analysis: The theoretical underpinnings provided for FSLs robustness give the technique a strong foundation and credibility.  Weak Points and Questions 1. Scalability Concerns: Its unclear how FSLs requirement for serverside aggregation scales with an extremely large number of clients which is common in FL deployments. How does the voting mechanism and rankings aggregation perform under such conditions 2. Comparison with StateoftheArt Methods: While FSL is compared with FedAvg and robust aggregation methods it is not clear how it stands against other stateoftheart FL approaches focusing on similar issues. A broader comparison could provide more context regarding its relative advantages. 3. Dependency on Initial Random Initialization: Given that FSLs performance hinges on the supernetworks initial random initialization how sensitive is the system to this factor Would different initialization strategies significantly impact the effectiveness of the discovered supermask 4. Adaptability and Generalization: How adaptable is FSL to different types of neural network architectures and tasks beyond the experiments conducted (mostly on image classification)  Feedback to the Author The paper presents a compelling technique addressing key federated learning challenges. However to strengthen the paper further it would be beneficial to explore the scalability of FSL in more depth especially in scenarios with a vast number of clients. Additionally broadening the comparison to include a wider array of stateoftheart FL methods could offer readers a clearer understanding of FSL\u2019s positioning within the research landscape. Investigating the impact of different supernetwork initialization methods and the systems adaptability to various architectures and tasks could also enrich the discussion. Lastly elucidating the theoretical aspects with more detailed proofs or providing them in an accessible supplementary form might enhance comprehension for readers not deeply familiar with the theoretical foundations.  Review Summary FSL represents a significant step forward in federated learning particularly in improving robustness against poisoning attacks and in enhancing communication efficiency. Its innovative use of supermask learning within a federated framework is wellvalidated both theoretically and empirically. However there are areas where the paper could be improved specifically regarding scalability broader comparisons with existing FL approaches and exploration into the effects of supernetwork initialization. Addressing these points could reinforce the paper\u2019s contributions and provide clearer insights into the practical deployment and potential limitations of FSL. Overall the paper is a valuable addition to FL research pushing the boundaries of what is achievable in terms of efficiency and security in decentralized machine learning models.", "p98WJxUC3Ca": "1. Summary of the Paper The paper proposes an innovative active learning strategy aimed at addressing domain adaptation problems particularly focusing on scenarios where domain shifts occur. By leveraging the concept of localized discrepancy distance which narrows down the hypothesis space to functions closely matching the source domains labeling the study introduces a framework that enhances the active learning process under domain shift conditions. The authors present a novel algorithm based on an accelerated Kmedoids approach tailored to manage large datasets efficiently. The study thoroughly explores the theoretical underpinnings establishing generalization error bounds that are grounded in Rademacher averages and localized discrepancy metrics for diverse loss functions adhering to a regularity condition. Through extensive numerical experiments including regression and classification tasks in domain adaptation contexts the proposed algorithm demonstrates competitive performance against existing stateoftheart active learning methods showcasing its efficacy particularly in handling large datasets comprising around one hundred thousand images. 2. Strong and Weak Points Questions and Feedback Strong Points: a. Theoretical Foundation: The paper builds a solid theoretical foundation by delving into localized discrepancy measures and establishing generalization error bounds. This rigorous approach adds credibility and depth to the proposed method. b. Practical Algorithm Implementation: With the development of an accelerated Kmedoids algorithm the study addresses scalability issues headon making the proposed method applicable to realworld largescale data sets \u2014 a significant advance over some existing methods. c. Comprehensive Evaluation: By conducting a wide range of experiments on both regression and classification tasks in a domain shift context the paper provides substantial evidence of the proposed methods effectiveness and versatility. Weak Points: a. Complexity for Practitioners: While the theoretical depth is a strength it may pose a barrier to practitioners looking to apply the proposed method. Simplifying the approach or providing more userfriendly tools could enhance its accessibility. b. Limited Discussion on Realworld Applicabilities: Despite extensive experiments the paper could benefit from more detailed discussions on realworld applications illustrating how the method can be deployed in specific domain adaptation scenarios beyond image datasets. c. Comparative Analysis: The comparison with stateoftheart methods is comprehensive yet it could be deepened by providing a more nuanced understanding of why certain methods underperform or outperform others in specific contexts. Questions and Feedback: 1. Could the authors elaborate on how the proposed method could be adapted or extended to handle nonimage data especially in cases where data dimensionality and type significantly differ from the tested datasets 2. The accelerated Kmedoids algorithm is a notable contribution. Could the authors provide insights into potential limitations of this approach when dealing with data structures that are highly irregular or when the dimensionality increases significantly 3. In the realworld application of the proposed method how sensitive is the algorithms performance to the choice of \\xcf\\xb5 and the assumptions made about Lipschitz functions Some practical guidelines or explorations around these parameters would be valuable for practitioners. 3. Review Summary This paper presents a significant contribution to the field of active learning and domain adaptation by proposing a novel active learning strategy backed by a strong theoretical framework. The introduction of an accelerated Kmedoids algorithm that efficiently scales to large datasets addresses a critical challenge in the field setting the study apart from existing approaches. While the theoretical rigor is commendable the complexity of the approach might limit its accessibility to a broader audience of practitioners. The experiments conducted are thorough and demonstrate the methods effectiveness across various tasks however further explorations into the methods applicability beyond image datasets and detailed discussions on realworld deployments could enhance the papers impact. The constructive feedback revolves around illuminating the practical aspects of implementing the proposed method potentially increasing its utility and adoption in solving diverse domain adaptation challenges. Overall the paper makes a substantial contribution to the literature with recommendations for minor improvements that could broaden its applicability and ease of use.", "m8bypnj7Yl5": " Peer Review Report  1. Summarization of the Paper This paper introduces a novel approach for synthesizing optimal controllers for dynamical systems under stringent computational constraints. The core innovation lies in the use of hypersolvers a hybrid of differential equation solvers and neural networks aimed at improving the quality of optimized control policies within a fixed computational budget. The authors propose a methodology that permits the application of hypersolvers to control dynamical systems focusing on enhancing solution accuracy without compromising computational efficiency. They introduce pretraining strategies that target either average or worstcase solution improvements and validate their approach through experiments in different control contexts including direct and model predictive control tasks across various dimensionalities. The results suggest that the proposed method can achieve significantly higher solution accuracy and control performance evidencing consistent Pareto improvements relative to conventional solver approaches.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovativeness: The amalgamation of differential equation solvers with neural networks via hypersolvers to tackle the optimization of control policies under realtime constraints is novel and promising.  Comprehensive Evaluation: The authors conducted extensive experimental validations across multiple control tasks and dimensionalities. These experiments demonstrate the versatility and effectiveness of the proposed hypersolver methodology.  Theoretical Justification: The inclusion of specific loss functions pretraining strategies and detailed discussions on architectural choices provides a solid theoretical foundation for the hypersolver approach. Weak Points:  Generalizability: While the paper presents extensive experimental evidence the generalizability of the approach to systems beyond those tested remains unclear. Particularly the adaptability of hypersolvers to highly stochastic or highly nonlinear systems outside the tested scenarios is not discussed.  Complexity Analysis Lacking: The paper does not provide a thorough computational complexity analysis comparing hypersolvers with traditional solvers. Such an analysis would bolster the argument about efficiency gains from using hypersolvers.  Implementation Details: Readers seeking to replicate the study or verify the presented results might find the lack of explicit implementation details (e.g. neural network architectures hyperparameters) a limitation. Questions and Feedback: 1. Can the authors detail the adaptability of hypersolvers to other types of dynamical systems especially those with high nonlinearity or stochasticity 2. A more detailed computational complexity analysis would be beneficial. How do hypersolvers compare to traditional solvers in terms of computational resources and time especially in highdimensional systems 3. The paper would benefit from a clearer exposition of implementation details. Can the authors provide additional information on neural network specifications used in the experiments  3. Review Summary and Recommendation The paper introduces an innovative and promising methodology for enhancing the quality of optimized control policies within strict computational limits by employing hypersolvers. This approach combines the accuracy of higherorder differential equation solvers with the efficiency of neural networks demonstrated across various control tasks to achieve remarkable Pareto improvements in solution accuracy and control performance. While the empirical validations are comprehensive and the theoretical underpinnings are wellarticulated the paper could be strengthened by addressing its generalizability to a broader spectrum of dynamical systems providing a detailed computational complexity analysis and elaborating on implementation specifics. Addressing these points would significantly enhance the papers contribution and its utility to readers seeking to replicate or build upon the presented work. Given the relevance and potential impact of the work in the field of control systems and robotics I recommend acceptance contingent upon considerations and potential revisions addressing the identified weaknesses and questions. The innovative nature of the approach combined with the solid experimental validations represents a significant contribution to the literature on optimal control under computational constraints.", "yuv0mwPOlz3": " Peer Review  1. Summarize the paper The paper addresses the challenge of active learning (AL) in scenarios where labeled data are scarce and the source data comes from multiple potentially outofdistribution domains. It seeks to identify which active learning methods are most effective in these multidomain settings particularly for natural language processing tasks like question answering and sentiment analysis. The authors examine four families of methods\u2014uncertaintybased HDivergence reverse classification accuracy (RCA) and semantic similarity detection\u2014across several datasets. They introduce a variant called DALE within the HDivergence family and demonstrate its effectiveness noting the importance of diversity in domain allocation and the potential for improving both domain and example selection strategies. The paper provides an empirical evaluation of 18 acquisition functions underscoring the challenges and opportunities in multidomain active learning and suggests directions for future work.  2. Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Analysis: The paper offers a thorough examination of multiple active learning methods across various domains which is beneficial for practitioners in the field of natural language processing.  Novel Method Introduction: The introduction of DALE as an effective variant within the HDivergence family is a significant contribution providing a new tool for improving active learning outcomes in multidomain settings.  Practical Insights: The findings regarding the importance of domain diversity and the potential strategies for effective example and domain selection are practical insights that can guide future research and application in active learning. Weak Points:  Limited Method Families: Despite the broad analysis the paper focuses on four families of methods potentially overlooking other relevant approaches in active learning or domain adaptation that might offer additional benefits.  Lack of Theoretical Analysis: The empirical evaluation is strong but the paper could benefit from a deeper theoretical analysis to understand why certain methods (e.g. DALE) outperform others in these specific settings.  Performance Variability: The paper notes the inconsistent performance of active learning strategies particularly in comparison to random selection. A more detailed exploration of factors contributing to this variability would strengthen the findings. Questions and Feedback:  Could the authors elaborate on how DALE differs in its approach and execution from existing HDivergence methods and what specifically contributes to its enhanced performance  It would be helpful to discuss the potential of integrating the examined active learning methods with domain adaptation techniques. How might such integration impact the effectiveness of active learning in multidomain settings  The implications of the findings for practical applications are clear but could the authors speculate on the theoretical underpinnings that might explain the observed performances of different methods This could provide valuable insights for developing future methods.  3. Review Summary The paper presents a valuable and extensive analysis of active learning methods in multidomain settings for NLP tasks contributing important insights for practitioners. The introduction of a new method DALE within the HDivergence family and the empirical demonstration of its effectiveness represent notable strengths of this work. Additionally the focus on domain diversity and the exploration of example and domain selection strategies address crucial challenges in active learning. However the paper could benefit from a broader exploration of method families a deeper theoretical understanding of why certain methods outperform others and an investigation into the variability of active learning performance compared to random selection. Overall the paper makes a significant contribution to the field by highlighting the complexities of multidomain active learning and suggesting avenues for future research that could further enhance active learning outcomes in NLP and potentially other domains.", "rxF4IN3R2ml": " Peer Review for \"MQTransformer: Enhancing Neural Probabilistic Forecasting with Transformer Architectures\"  Summary of the Paper This paper proposes MQTransformer a novel architecture for probabilistic forecasting that addresses several limitations of existing neural forecasting models particularly the MQForecaster. The authors integrate several enhancements inspired by the recent advances in Transformer architectures used in Natural Language Processing (NLP). These enhancements include a novel decoderencoder attention for context alignment to improve forecasting accuracy a positional encoding mechanism that enables the model to learn contextdependent seasonality functions and a scheme named decoderself attention to reduce excess forecast volatility by leveraging previous errors in forecasts. Through extensive experiments on a largescale ecommerce retailers demand forecasting problem and four public datasets the MQTransformer demonstrates significant improvements in forecast accuracy and efficiency outperforming the current stateoftheart models including a dramatic reduction in forecast volatility.  Strong and Weak Points Questions and Feedback Strong Points: 1. The paper addresses a significant problem in timeseries forecasting particularly the excess volatility in forecasts and the limitation of stateoftheart models to adapt to contextdependent seasonality. 2. Incorporating elements from Transformer architectures into probabilistic forecasting is both novel and timely given the Transformers success in other domains like NLP. 3. The experimental results are compelling and demonstrate clear improvements over existing models in both accuracy and efficiency. The reduction in excess forecast volatility is particularly noteworthy. 4. The methodology section is detailed enabling replication and understanding of the intricate model architecture and its components. Weak Points: 1. The paper could benefit from a deeper discussion on the theoretical underpinnings of why the proposed Transformerinspired modifications result in the observed improvements. Specifically an analysis of how each component (contextual embeddings horizonspecific attention decoder selfattention) individually contributes to the overall performance would be valuable. 2. While the proposed methodology demonstrates improved performance across selected datasets the paper lacks a broader evaluation across a wide range of forecasting problems. This limitation raises questions about the generalizability of the proposed approach. 3. The computational requirements for training the MQTransformer model while improved over prior models could still be prohibitive for some realworld applications. Insights into the computational tradeoffs and potential simplifications of the model could enhance its practical appeal. Questions and Feedback: 1. Can the authors provide more details on the hyperparameter tuning process for the MQTransformer Understanding this aspect could help in assessing the models sensitivity and robustness. 2. How does the model perform in forecasting scenarios with sparse data or when encountering new events not present in the training data Additional insights here would be beneficial. 3. It would be interesting to see a breakdown of the models performance on different components of the time series (e.g. trend seasonality remaining components) to better understand where the model excels or struggles.  Review Summary and Recommendation The paper presents an innovative approach to improving neural probabilistic forecasting by leveraging advancements from Transformer architectures. The proposed MQTransformer not only outperforms existing stateoftheart models in terms of forecast accuracy and computational efficiency but also offers the novel capability of reducing forecast volatility significantly. This work has the potential to set a new benchmark in the field and guide future research directions. However to strengthen the papers contribution and address its generalizability and theoretical clarity I recommend the authors to: 1. Elaborate on the theoretical rationale behind the effectiveness of the proposed Transformerinspired components. 2. Expand the experimental validation beyond the selected datasets to include a wider variety of forecasting tasks addressing the models generalizability. 3. Provide deeper insights into the computational requirements and potential model simplifications to broaden the practical applicability of the MQTransformer. Given the significant contributions and potential for future improvements I recommend this paper for acceptance albeit with minor revisions to address the points raised about generalizability theoretical clarity and computational considerations.", "kroqZZb-6s": " Review of \"Clusterbased Importance Learning for Timeseries (CAMELOT): A Deep Learning Approach to Clustering Electronic Health Records (EHR) for Predicting Patient Trajectory Outcomes\"  1. Summary: The paper introduces CAMELOT a novel supervised deep learning model designed for clustering Electronic Health Records (EHR) to predict patient outcomes and trajectories particularly focusing on TypeII Respiratory Failure (T2RF) in hospitalized patients. The authors highlight the challenges of working with EHR data due to its sparsity heterogeneity and multidimensionality. CAMELOT addresses these challenges by proposing a novel loss function to mitigate class imbalance and cluster collapse coupled with a featuretime attention mechanism that enhances model interpretability by identifying important phenotypes over time. The models effectiveness is validated on a dataset from the Oxford University Hospitals NHS Foundation Trust comprising over 100000 patient trajectories. CAMELOT demonstrates a superior ability to predict patient outcomes with an increase in mean AUROC by at least 5 over benchmark models alongside providing clinically interpretable clusters.  2. Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses a critical gap in EHR modeling by offering a solution that accounts for data heterogeneity and imbalance which are significant challenges in the field.  The introduction of a featuretime attention mechanism is novel and valuable enhancing interpretability which is a crucial aspect often overlooked in deep learning applications in healthcare.  The comprehensive dataset and rigorous evaluation methods employed significantly contribute to the credibility and relevance of the findings. Weak Points:  While the model demonstrates improved performance metrics the realworld applicability in terms of computational cost and integration within existing healthcare systems is not discussed.  The paper could benefit from a deeper exploration into the clinical implications of the identified phenotypes and clusters. More detailed case studies or expert validations could provide insights into the practical utility of the models outputs.  There seems to be a lack of comparison or discussion around the potential biases that could arise from the dataset used considering it excludes ICU patients and observations from the emergency department. Questions and Feedback:  Could the authors provide more detail on how CAMELOT\u2019s featuretime attention mechanism compares to existing attention mechanisms in terms of computational efficiency and interpretability  It would be beneficial to discuss potential limitations or biases introduced by the datasets preprocessing steps and how they might impact the models generalizability across different hospital settings or patient populations.  Feedback: An exploration into the integration challenges of CAMELOT within clinical workflows and how it might interact with existing EHR systems could significantly enhance the paper\u2019s contribution to realworld applicability.  3. Review Summary: The paper \"Clusterbased Importance Learning for Timeseries (CAMELOT): A Deep Learning Approach to Clustering Electronic Health Records (EHR) for Predicting Patient Trajectory Outcomes\" presents a significant advancement in predictive modeling of patient trajectories using EHR data. The authors effectively address critical challenges in the field such as data heterogeneity and class imbalance and introduce an innovative interpretable model capable of clustering EHR data to predict patient outcomes. Despite the notable contributions the paper can be strengthened by delving deeper into the practical implementation challenges addressing potential dataset biases and providing a clearer insight into the clinical significance of the models findings. Overall CAMELOT represents a promising step forward in the application of deep learning to improve patient care and clinical decisionmaking warranting further exploration and validation in diverse clinical settings.", "pzgENfIRBil": " Peer Review of \"Solving Approximated Schr\u00f6dinger Equations Using Selfconsistent Gradientlike Eigen Decomposition\"  Summary of the Paper This paper introduces a novel computational framework Selfconsistent Gradientlike Eigen Decomposition (SCGLED) to tackle the challenge of solving approximated Schr\u00f6dinger equations without necessitating expert domainspecific initial guesses. The approximated Schr\u00f6dinger equations crucial for understanding quantum mechanical systems are reformulated into a nonlinear generalized eigenvalue problem characterized by a selfconsistency dilemma. Traditional methods to solve these rely on initial guesses derived from quantum mechanics heuristics which can be a major barrier for nonexperts and often lead to convergence issues. The key innovation presented is SCGLED which treats the equations matrix as an online data generator enabling the use of gradientlike eigendecomposition algorithms akin to those in streaming kPCA. This approach effectively bypasses the need for domainspecific heuristics for initial guesses and demonstrates significant improvements in both convergence reliability and computational efficiency. The paper thoroughly evaluates SCGLED against traditional initial guess methods and as a standalone solver showcasing its superior performance on the W417 dataset of singlet molecules.  Strengths and Weaknesses Strengths: 1. Innovation and Relevance: The approach of treating the matrix F(V) as an online data generator is innovative and relevant. It enables the application of machine learning techniques specifically stochastic kPCA methods to a traditionally physicsdominated problem. 2. Performance Improvement: The reported results indicate a significant performance advantage of SCGLED over traditional heuristicsbased methods in terms of both precision and computational efficiency. 3. Robustness to Initial Guesses: By eliminating the need for domainspecific initial guesses SCGLED presents a more accessible and potentially more reliable approach to solving approximated Schr\u00f6dinger equations. 4. Clear Implementation Details: The paper provides thorough implementation details including the algorithmic steps of SCGLED making replication and further research more feasible. Weaknesses: 1. Limited Theoretical Analysis: The convergence analysis is primarily empirical. While the authors mention this is due to the intrinsic nonlinearity of the problem a more rigorous theoretical framework or analysis would strengthen the paper. 2. Dependency on Hyperparameters: The performance of SCGLED seems to depend on the proper tuning of several hyperparameters (e.g. learning rate damping technique parameters). More discussion on the sensitivity to these parameters and guidelines for their selection would be beneficial. 3. Comparison with StateoftheArt: While the paper compares SCGLED with traditional methods it lacks a direct comparison with other recent advances that might utilize machine learning techniques for similar problems. 4. Generalization Concerns: The experiments are conducted on the W417 dataset. Additional testing on a broader set of molecules or systems would help validate the generalizability of the proposed method. Questions and Feedback to the Author: 1. Could the authors provide more insight into how the methods performance might be affected by the choice of hyperparameters Are there any strategies recommended for their optimization 2. How does SCGLED compare to other recent machine learningbased approaches if any for solving approximated Schr\u00f6dinger equations 3. The paper mentions numerical improvements without delving deeply into their specifics. Could the authors elaborate on these improvements and their impact on the algorithms performance 4. Are there plans to test the methods applicability and efficiency on larger and more complex systems beyond the W417 dataset  Review Summary and Recommendation This paper presents a compelling and innovative approach to solving approximated Schr\u00f6dinger equations demonstrating significant performance advantages over traditional heuristicbased methods. The introduction of SCGLED is a notable contribution potentially opening new paths for applying machine learning techniques in computational physics and chemistry. However there are areas in which the paper could be strengthened particularly around the theoretical justification of the methods convergence the impact of hyperparameters and broader comparisons with other modern approaches. Based on the strengths of the innovation presented the clear potential for impact in computational physics and the thoroughness of the experimental evaluation I recommend the publication of this paper. I encourage the authors to address the raised weaknesses which would further solidify the works significance and applicability in the field.", "lEoFUoMH2Uu": " Peer Review for the Paper: \"Reconstructing Visual Stimulus Images from fMRI Using ForegroundAttention and LoopEncDec Training Strategy\"  1) Summary of the Paper This paper introduces a novel approach for reconstructing visual stimulus images from functional Magnetic Resonance Imaging (fMRI) data. The authors propose a twofold technique leveraging human visual attention specifically foreground attention (Fattention) and a unique encodingdecoding framework named LoopEncDec. They start with the premise that extracting valuable information from highdimensional noisy fMRI data for image reconstruction is a considerable challenge. To address this they first decode the visual attention distribution from fMRI data using a dedicated Fattention decoder. Then utilizing this decoded visual attention as a guide the LoopEncDec framework endeavors to reconstruct the visual stimulus images. This methodology aims to closely mimic the human brains natural prioritization of prominent objects within its field of view. The paper reports that their approach guided by Fattention and reinforced by the LoopEncDec strategy achieves superior image reconstruction quality compared to existing methodologies.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper successfully integrates human visual attention (foreground attention) into the reconstruction process which is a novel and promising direction in the field of fMRIbased image reconstruction.  Comprehensive Methodology: The authors have detailed the components of their approach meticulously from decoding Fattention from fMRI data to leveraging it within the LoopEncDec framework. This thoroughness provides clarity and reproducibility.  Experimental Results: The presented results convincingly demonstrate that the proposed approach outperforms existing methods in terms of the quality of the reconstructed images particularly in capturing the form and orientation of prominent objects. Weak Points:  Limited Exploration of the SelfAttention Module: While the inclusion of a selfattention module to capture global information from fMRI data is mentioned there is sparse detail on its specific deployment and impact relative to other model components.  Generalizability Concerns: The paper primarily focuses on the reconstruction quality without deeply exploring the models generalizability across different fMRI datasets or variability among individual subjects.  Computational Efficiency: There is an absence of discussion regarding the computational demands of the proposed method especially relevant given the highdimensional nature of fMRI data and the complexity of deep learning models. Questions and Feedback:  Could the authors elaborate on the specific architecture and workings of the selfattention module within their model Understanding its role in capturing global information could provide deeper insights into the models efficacy.  It would be beneficial to discuss the approachs generalizability. Do the authors anticipate that their method would perform equally well across diverse fMRI datasets or across different subjects with varying brain architectures  A comparison of computational efficiency with other existing methods might be enlightening. Given the intricate processing involved how does the proposed method fare in terms of computational resources and time  3) Review Summary The paper presents a compelling method for reconstructing visual stimulus images from fMRI data by incorporating human visual attention mechanisms and a novel LoopEncDec training strategy. The approach is innovative aligning with neuroscientific understandings of visual attention and the reported results underscore its potential in producing highquality reconstructions. However the paper would benefit from a more detailed discussion on the integration of the selfattention module an exploration of the methods generalizability across different datasets and subjects and an assessment of computational demands. Addressing these points could significantly enhance the papers contribution to the field offering clearer insights into the methods applicability and practical considerations. Overall the proposed method represents a promising direction in the domain of visual reconstruction from fMRI data meriting further exploration and development.", "i3RI65sR7N": " 1) Summary of the Paper The paper introduces a novel approach to fewshot learning emphasizing its effectiveness in crossdomain and withindomain fewshot classification tasks. The authors propose a hierarchical variational memory model that stores features at multiple semantic levels contrary to existing models which only utilize features from the last layer. This hierarchical memory is combined with a hierarchical variational prototype model capable of fetching and leveraging features at various semantic levels to improve adaptability and generalization across domain shifts. The models are optimized through a new hierarchical variational inference framework and a datadriven method is introduced to learn weights for prototypes at each level enabling the dynamic selection of the most generalizable features based on the domain. Extensive ablation studies validate the significance of each component and the proposed method achieves stateoftheart performance on crossdomain fewshot learning benchmarks and competitive results in traditional withindomain scenarios.  2) Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Hierarchical Memory Concept: The introduction of hierarchical memory to store features at different levels and its integration into the variational prototype model is a significant advancement in addressing domain shifts in fewshot learning. 2. DataDriven Prototype Weights: Learning prototype weights in a datadriven manner to adaptively choose the most generalizable features for new domains is an impressive approach that enhances model flexibility and generalization ability. 3. Comprehensive Evaluation: The systematic ablation studies coupled with thorough experiments on multiple benchmarks provide strong evidence of the models effectiveness and the importance of each proposed component. Weak Points: 1. Model Complexity and Computational Overhead: The hierarchical nature of the proposed models might introduce significant computational complexity making the training and inference processes resourceintensive. 2. Potential for Overfitting: The paper does not thoroughly address potential overfitting issues that might arise when learning to weight prototypes especially with limited training samples typical of fewshot scenarios. 3. Domain Shift Assumptions: The effectiveness of hierarchical memory relies on assumed domain shift characteristics. It might not generalize well to all types of domain shifts especially those where low and highlevel feature distinctions are not as clear. Questions and Feedback: 1. Model Complexity Management: How do the authors propose to manage the increased computational complexity and resource demands of the hierarchical models in practical deployment scenarios 2. Overfitting Mitigation Strategies: Are there any specific strategies or techniques that the authors recommend to mitigate potential overfitting risks when learning prototype weights with limited data 3. Generalization to Unconventional Domain Shifts: How does the model perform in scenarios with unconventional domain shifts where the assumption of hierarchical feature relevance may not hold  3) Review Summary and Recommendation This paper presents a significant contribution to the field of fewshot learning by introducing a hierarchical variational memory and prototype model that enhances adaptability and generalization across domain shifts. The idea of storing features at multiple semantic levels and learning prototype weights in a datadriven manner are commendable advancements. The comprehensive experimental validation across multiple benchmarks alongside systematic ablation studies offers strong support for the proposed methods effectiveness. However the paper would benefit from a deeper exploration of the computational complexities associated with the hierarchical models and strategies to mitigate potential overfitting risks. Additionally examining the models performance across a broader range of domain shifts would further solidify its adaptability and generalization capabilities. Given the clear strong points outweighing the weak points and considering the groundbreaking nature of the hierarchical approach to memory and prototypes in fewshot learning I recommend this paper for publication. Nonetheless addressing the posed questions and potentially providing further insights into the discussed concerns could enhance the papers impact and applicability.", "p3DKPQ7uaAi": " 1. Summarize the paper This paper introduces Temporal Alignment Prediction (TAP) a novel learnable sequence distance methodology that uses a lightweight convolutional neural network (CNN) to predict optimal sequence alignment without the need for traditional optimization approaches during inference. TAP addresses the challenges posed by the temporal variance in sequence data such as different sampling rates or elastic warpage by providing an efficient and straightforward calculation method for sequence alignment that significantly accelerates inference. The authors demonstrate TAPs applicability in various tasks such as supervised representation learning and fewshot action classification. By employing TAP in metric learning scenarios the paper showcases superior or comparable performance to stateoftheart methods on several datasets while benefiting from reduced computational costs and faster inference times.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation in Sequence Alignment: The paper proposes an innovative approach to sequence alignment that bypasses traditional optimization and dynamic programming methods offering a significant reduction in computation time and facilitating endtoend learning.  Versatility and Broad Application: The manuscript effectively demonstrates the versatility of TAP across different tasks including supervised learning and fewshot action recognition. This wide applicability signifies TAPs potential impact on various sequencebased machine learning challenges.  Experimental Validation: Comprehensive experiments across seven datasets substantiate the efficacy of TAP indicating that it is not only competitive with existing stateoftheart methods but does so with enhanced efficiency. Weak Points:  Assumption on Temporal Order Preservation: Although the flexibility of TAP in handling local temporal reorders is highlighted the paper admits limitations in cases where strict temporal order is crucial. This limitation might affect TAPs applicability to specific sequence analysis tasks requiring precise temporal alignment.  Asymmetric Property of TAP: The paper acknowledges that TAP is asymmetric and not a true metric. This aspect could complicate its use in certain applications or necessitate additional steps to symmetrize the distance for certain tasks.  Exploration of Network Architecture: While a lightweight CNN architecture is proposed for the alignment prediction network the paper does not provide extensive analysis or justification for the choice of architecture. Further exploration or comparison with alternative architectures could strengthen the confidence in TAPs design. Questions and Feedback:  Could you elaborate on how TAP would perform on sequences where strict temporal order and alignment are critical Are there potential modifications that could address this limitation  Has the impact of TAPs asymmetric nature been experimentally evaluated in scenarios where symmetry is typically expected If not could such evaluations offer insights into the practical implications of this characteristic  Further explanation on the selection of the CNN architecture used in TAP would be useful. Were alternative architectures considered and if so what were the findings from such comparisons  3. Review Summary This paper introduces a significant advancement in the field of sequence analysis through the proposal of Temporal Alignment Prediction (TAP) a learnable and efficient method for sequence alignment. By leveraging a convolutional neural network to predict optimal alignments directly TAP circumvents the computational complexities associated with traditional alignment inference methods notably enhancing inference speed. The methods broad applicability demonstrated through its deployment in tasks such as supervised sequence representation learning and fewshot action classification underscores its potential impact on a wide range of sequencebased machine learning challenges. Despite its strengths certain limitations such as its handling of sequences requiring strict temporal order and its innate asymmetry might restrict its utility in specific contexts. These concerns however do not significantly detract from the methods overall contribution but rather highlight areas for future research and potential refinement. Given TAPs demonstrated efficacy across multiple datasets and its ability to substantially reduce computational burdens while maintaining competitive performance it represents a valuable contribution to sequence analysis. Future work addressing the outlined limitations and exploring further applications could expand its utility and impact in the field.", "izj68lUcBpt": " Review of TemporallyAdaptive Convolutions (TAdaConv) for Video Understanding  1. Summary The paper introduces TemporallyAdaptive Convolution (TAdaConv) a novel approach that adapts convolution weights dynamically across frames to enhance the temporal modelling capabilities of convolutional neural networks (CNNs) for video understanding tasks. Building on the premise that temporal dynamics in videos require different treatment than spatial features TAdaConv modifies the weights of the convolutional kernels based on both local and global temporal contexts thereby allowing for a more nuanced handling of temporal information. The authors demonstrate the effectiveness of TAdaConv by integrating it into existing network architectures like ResNet and ConvNeXt resulting in the TAda2D and TAdaConvNeXt networks. Experiments conducted on various video action recognition and localization benchmarks show that networks equipped with TAdaConv match or exceed stateoftheart performances with minimal computational overhead. The paper is wellstructured providing a detailed background a clear explanation of the TAdaConv mechanism and extensive experimental validation.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper addresses the temporal modelling challenge in video understanding by introducing an innovative temporally adaptive weight calibration method that outperforms traditional spatial and temporal convolutions in efficiency and performance.  Comprehensive Evaluation: The authors provide a robust evaluation of TAdaConv including its integration into different models (TAda2D and TAdaConvNeXt) and on various video understanding tasks. The results convincingly demonstrate TAdaConvs versatility and effectiveness.  Plugin Compatibility: TAdaConvs design as a plugandplay component that can easily be integrated into existing CNN architectures without needing to retrain from scratch is particularly compelling for practical applications. Weak Points:  Limitation Discussion: The paper could benefit from a more detailed discussion regarding the limitations of TAdaConv including potential scenarios where its performance might not be optimal.  Comparison with Recent Techniques: While the paper benchmarks TAdaConv against several stateoftheart models it would be valuable to see more direct comparisons with the most recent temporal modeling techniques especially those leveraging Transformer architectures.  Computational Efficiency Analysis: More indepth analysis of the computational efficiency of TAdaConv particularly in terms of memory footprint and inference time would be useful for realworld applications. Questions and Feedback: 1. Could the authors elaborate on the choice of datasets used for evaluation Specifically how do these datasets challenge the temporal modeling capabilities of TAdaConv 2. How does TAdaConv perform under conditions of severe motion blur or occlusions Would its performance degrade significantly or does the model demonstrate resilience against such challenging conditions 3. The paper mentions that TAdaConv can be seen as a dropin replacement for spatial convolutions. Can the authors provide insights into any challenges or requisite conditions for achieving optimal performance when integrating TAdaConv into other existing CNN architectures not discussed in the paper  3. Review Summary The paper presents a significant advancement in temporal modeling for video understanding through the introduction of TemporallyAdaptive Convolutions (TAdaConv). The innovation lies in dynamically adjusting convolution weights based on the temporal context thereby enhancing the models ability to understand complex temporal dynamics in videos. The comprehensive evaluation demonstrates TAdaConvs effectiveness and efficacy as both a standalone and a plugin module an aspect that is highly commendable. However the paper could be strengthened by addressing its identified weaknesses such as providing a more detailed discussion of limitations including comparisons with the latest temporal modeling techniques and offering a deeper analysis of computational efficiency. Moreover addressing the posed questions would enrich the readers understanding of TAdaConvs adaptability robustness and integration challenges into various architectures. Overall TAdaConv represents a promising direction for video understanding research. Its ability to be integrated easily into existing models without retraining and its demonstrated improvement in modeling complex temporal dynamics make it a valuable contribution to the field.", "mF122BuAnnW": " Review Summary  1. Summarize the paper The manuscript presents a novel collective robustness certification framework for multioutput classifiers referred to as softly local models. These models attribute varying levels of importance across different regions of the input in contrast to strictly local models. The authors propose localized randomized smoothing as a technique to individually smooth outputs according to their relative importance to certain input regions. They then integrate these peroutput certificates through a mixedinteger linear programming (MILP) approach to generate a collective robustness certificate. This certificate concretely quantifies the models robustness against adversarial attacks considering the models soft locality. The paper conducts experiments on image segmentation and node classification tasks to demonstrate the superior robustnessaccuracy tradeoff achieved by this method compared to existing approaches.  2. Strong and weak points Questions and Feedback Strong Points:  Innovativeness: The introduction of localized randomized smoothing represents a significant contribution to collective robustness certifications addressing the limitations of existing certificates which do not adequately account for soft locality in models.  Rigor: The paper provides a thorough theoretical framework for the proposed certification technique alongside with a detailed MILP formulation for combining peroutput base certificates into a collective certificate.  Empirical Evaluation: The experiments encompass two distinct applications (image segmentation and node classification) demonstrating the versatility and effectiveness of the proposed method. Weak Points:  Generalizability: The paper mentions the limitation of the approach mainly being applicable to softly local models which potentially restricts its applicability across a broader range of multioutput classifiers.  Complexity of Implementation: The reliance on MILP for certificate calculation may pose practical challenges in terms of computational complexity especially for largescale applications. The paper could benefit from a discussion on computational demands and scalability.  Experimental Baseline: The comparison to i.i.d. smoothing baselines is welljustified however including comparisons to other stateoftheart adversarial defense mechanisms could further validate the superiority of the proposed method. Questions:  Can the authors elaborate on potential strategies to reduce the computational overhead associated with MILP in largescale applications  How sensitive is localized randomized smoothing to the choice of smoothing distributions and is there a systematic way to select these distributions for different applications Feedback:  Including a discussion on potential mitigation strategies for the identified limitations (e.g. model generalizability and computational complexity) would be valuable.  Expanding the experimental comparison to include a broader range of adversarial defense methods could strengthen the result section.  3. Review Summary The paper introduces a novel approach to improve the collective robustness of softly local multioutput classifiers through localized randomized smoothing and a MILPbased certification. The method significantly advances the stateoftheart in collective robustness certification by accounting for soft locality in model predictions. The theoretical foundation is solid and empirical evaluations demonstrate the effectiveness of the approach. However the paper acknowledges limitations regarding model generalizability and computational overhead which could be further addressed to enhance the practical applicability of the proposed method. Overall this work presents a significant contribution to the field offering a more nuanced and effective approach to certifying collective robustness in softly local models. With improvements to address the computational complexity and an extended range of experimental comparisons the method has the potential to be widely adopted for enhancing the robustness of various multioutput classification tasks.", "rSI-tyrv-ni": "Peer Review for \"Incorporating Entity Type Abstractions into PreTrained Transformers for Enhanced Logical Reasoning and Generalization\" 1) Summary of the Paper: This paper explores the effect of incorporating entity type abstractions into pretrained Transformer models to improve their logical reasoning abilities and compositional generalization across various NLP tasks. The authors propose three methods for integrating abstract knowledge into Transformers: additional input embeddings a separate sequence to encode and as an auxiliary prediction task. The effectiveness of these methods is evaluated on four NLP tasks requiring different levels of logical reasoning: compositional language understanding (CLUTRR) abductive reasoning (ProofWriter) multihop question answering (HotpotQA) and conversational question answering (CoQA). The study reveals that abstract entity knowledge enhances performance in tasks with formally defined logical structures (CLUTRR and ProofWriter) but shows marginal benefits for tasks involving more natural language (HotpotQA and CoQA). 2) Strong and Weak Points Questions and Feedback:  Strong Points:  The paper tackles an important and challenging problem in NLP regarding the generalization and reasoning capabilities of language models.  Comprehensive experimentation across four different NLP tasks with varying reasoning requirements provides a robust analysis of the proposed methods.  The exploration of three distinct methods for incorporating abstraction adds significant value to the research offering insights into effective strategies for enhancing Transformer models.  Weak Points:  The benefits of entity type abstractions are less pronounced in more natural languagebased tasks (HotpotQA and CoQA) which raises questions about the applicability of the proposed approach in realworld settings where natural language complexity is common.  While the paper addresses the improvement in logical reasoning it does not extensively discuss the potential tradeoffs or negative impacts such as increased computational requirements or potential biases introduced by the abstraction process.  The paper could benefit from a more detailed comparison against existing approaches that aim to enhance reasoning capabilities in language models offering a clearer picture of the unique contributions and improvements of the proposed methods.  Questions and Feedback:  Can the authors expand on the reasoning behind the marginal improvements observed in natural language tasks Is this limitation inherent to the abstraction process or the tasks themselves  It would be beneficial if the paper included a more thorough discussion on the scalability of the proposed methods especially concerning the computational overhead introduced by entity abstraction.  How do the proposed abstraction methods integrate with languages other than English especially considering the reliance on entity type information that may vary across languages 3) Review Summary (Supporting Arguments for Recommendation): This paper presents an insightful exploration into improving the reasoning abilities of pretrained Transformers through entity type abstractions. The empirical analysis is thorough covering various methods and NLP tasks which substantiates the authors conclusions about the effectiveness of entity abstraction in tasks with a formal logical structure. However the limited benefits in natural language tasks and lack of detailed comparison with existing methods are notable shortcomings. Given the significance of the addressed problem the novel exploration of abstract entity knowledge and the comprehensive empirical evaluation this paper makes a valuable contribution to the field of NLP. Nevertheless to strengthen the paper further addressing the outlined weak points and questions will be essential. Specifically expanding the discussion on realworld applicability computational efficiency and comparisons with existing approaches would provide a more comprehensive understanding of the proposed methods merits and limitations. The strengths of the research outweigh the identified weaknesses especially considering the novelty and potential impact of the work. Therefore I recommend accepting this paper contingent on the authors addressing the aforementioned feedback and questions.", "uB12zutkXJR": " Peer Review for the paper titled \"GRAPHIX: An Automated Program Repair Model Leveraging Graph Edit Techniques for Java Programs\"  1. Summary The paper introduces GRAPHIX a novel approach for automatically detecting and fixing bugs and code quality issues in Java programs. GRAPHIX differs from existing sequencetosequence models by utilizing a graphbased representation of the codes abstract syntax structure employing a multihead graph encoder combined with an autoregressive tree decoder. This configuration allows GRAPHIX to perform specific graph edit actions yielding improved program repair capabilities. Through a unique pretraining strategy called deleted subtree reconstruction the model gains an understanding of program structures using unlabeled source code. The authors comprehensively evaluated GRAPHIX on the Patches in The Wild Java benchmark demonstrating its superior performance against established baselines including Transformerbased models with fewer parameters.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: Utilizing a graphbased representation and focusing on the abstract syntax structure of code is a novel approach that captures the hierarchical nature of programming languages more naturally than tokenbased methods.  Pretraining Strategy: The deleted subtree reconstruction strategy for pretraining is creative and wellsuited for the task at hand offering a promising direction for future research in program analysis and automated repair.  Performance: The models ability to outperform other stateoftheart methods including those with significantly more parameters is impressive and speaks to the efficacy of the approach. Weak Points:  Generalizability: The focus on Java programs might limit the immediate applicability of GRAPHIX to other programming languages. Future work should address how this model can be adapted or generalized.  Evaluation Metrics: While top1 exact match accuracy is a standard metric additional metrics such as partial correctness or functional equivalence might provide a more nuanced understanding of the models performance.  Realworld Applicability: The paper lacks discussion on the integration of GRAPHIX into existing development environments or its effectiveness in a realworld software development workflow. Questions and Feedback:  How does GRAPHIX handle the complexity and diversity of programming styles and patterns found across different Java repositories  Can the authors elaborate on any limitations encountered when pretraining the model with the deleted subtree reconstruction strategy  Feedback would be to investigate the models performance across a broader range of languages and to explore metrics beyond top1 exact match accuracy.  3. Review Summary The paper presents a significant contribution to the field of automated program repair with the introduction of GRAPHIX. Its innovative use of graphbased representations and the novel pretraining strategy enable superior performance on bug detection and fixing tasks. However the assessment of its broader applicability to various programming languages and realworld scenarios remains an open question. Further research on these fronts alongside an exploration of additional evaluation metrics would strengthen the findings and potential impact of this work. In light of the strong theoretical foundation promising results and the avenues for future exploration it opens this paper makes a compelling case for publication.", "fR-EnKWL_Zb": " Summarize the paper This paper introduces QuadTree Attention an innovative method that addresses the computational complexity issue plaguing conventional transformers when applied to dense vision tasks like object detection feature matching and stereo matching. The method hinges on the concept of building token pyramids and using a coarsetofine approach for computing attention. Specifically it selects the top K patches based on attention scores at each pyramid level focusing subsequent computations only within these relevant regions. This strategic selection process enables the reduction of computational complexity from quadratic to linear. The authors demonstrate that QuadTree Attention outperforms existing transformer models across a variety of tasks including feature matching on ScanNet stereo matching image classification on ImageNet object detection on COCO and semantic segmentation achieving stateoftheart results with significantly reduced computational costs.  Strong and Weak Points Questions and Feedback  Strong Points: 1. Innovative Approach: The introduction of a quadtreebased attention mechanism is novel and addresses a significant bottleneck in applying transformers to vision tasks requiring dense predictions. By effectively reducing computational complexity from quadratic to linear it allows for greater scalability and application. 2. Comprehensive Evaluation: The paper thoroughly evaluates the proposed method across a wide range of vision tasks demonstrating its versatility and effectiveness. The comparisons with stateoftheart models provide clear evidence of its superior performance and efficiency. 3. Accessible Implementation: Providing the codebase is a strong point as it encourages replicability and further exploration by other researchers.  Weak Points: 1. Lack of Theoretical Insight: While the paper provides empirical evidence of the effectiveness of QuadTree Attention it lacks a deeper theoretical analysis of why this approach works so effectively across different tasks. Such insights could be valuable for the advancement of the field. 2. Limited Discussion on Limitations: The paper does not thoroughly discuss the potential limitations or scenarios where QuadTree Attention might not perform as well. Understanding these conditions could be crucial for future applications and improvements. 3. Comparison with Linear Transformers: Although the paper asserts superior performance over linear transformers a more detailed comparison highlighting specific strengths and weaknesses would provide a clearer picture.  Questions and Feedback: 1. Theoretical Explanation: Could the authors provide more theoretical background or intuitive explanation on why the quadtree approach significantly reduces computational complexity and why its particularly effective for vision tasks 2. Potential Limitations: Are there any specific types of vision tasks or data conditions where QuadTree Attention may not perform optimally How might these limitations be addressed in future work 3. Comparison Criteria: Regarding the comparison with linear transformers could the authors elaborate on specific scenarios or tasks where QuadTree Attention shows the most significant improvements  Review Summary The introduction of QuadTree Attention by the authors represents a significant contribution to the field especially in addressing the computational inefficiencies of applying transformers to vision tasks. The methods ability to reduce computational complexity from quadratic to linear without sacrificing performance is its most commendable feature. Through extensive experimentation the paper convincingly demonstrates that QuadTree Attention outperforms existing methods across several tasks establishing new stateoftheart benchmarks. However the paper could be strengthened by providing deeper theoretical insights into why the method is so effective and exploring potential limitations that could inform future research. Additionally a more detailed comparison with linear transformers including specific scenarios where QuadTree Attention excels would enrich the papers contributions. Overall the proposed QuadTree Attention method is a significant step forward in making transformers more applicable and efficient for a wider range of vision tasks. With further exploration and refinement this approach has the potential to pave the way for new advancements in the field.", "hk3Cxc2laT-": " Review of \"Clustered TaskAware MetaLearning (CTML) Framework\"  Summary This paper introduces a novel framework named Clustered TaskAware MetaLearning (CTML) aimed at addressing the challenge of task heterogeneity in metalearning. Traditional metalearning methods leverage a globally shared metalearner to efficiently learn new tasks from few samples based on common knowledge acquired from existing tasks. However they often struggle with task heterogeneity wherein tasks come from different underlying distributions due to their reliance on featurebased task representations. CTML tackles this by learning task representations from both the features and the optimization (learning path) perspective of base learners. Specifically it incorporates a rehearsed task learning process to collect geometric quantities describing the learning path which are then abstracted into path representations through a meta path learner. This is combined with featurebased task representations for downstream clustering and modulation of the global initialization with an aim to balance customization and generalization across tasks. Additionally a shortcut tunnel is proposed to improve computational efficiency during metatesting. The frameworks superiority is demonstrated through extensive experiments on fewshot image classification and coldstart recommendation scenarios compared to stateoftheart baselines.  Strengths and Weaknesses Strengths: 1. Novel Integration of Feature and Path Informations: The frameworks dual approach in learning task representations from both features and optimization paths is innovative. It addresses the oversight of previous methods that neglected the rich information available in the optimization trajectories. 2. Comprehensive Addressal of Task Heterogeneity: By incorporating task clustering and modulation based on combined path and feature representations CTML effectively navigates the challenge of task heterogeneity which is a significant leap towards more adaptable and flexible metalearning algorithms. 3. Efficiency Improvements with Shortcut Tunnel: The introduction of a shortcut tunnel to predict path cluster assignments directly from feature cluster assignments during metatesting is a clever addition that significantly cuts computational costs without substantially compromising performance. Weaknesses: 1. Complexity and Overhead of Implementation: The additional steps of rehearsed learning and dualpathway task representation contribute to the methods complexity potentially making it cumbersome to implement and tune in practical settings. 2. Assumption of Correlation between Feature and Path Representations: The shortcut tunnels effectiveness relies on the assumption that there is a meaningful mapping between feature and path representations. This may not always hold true across different types of tasks or domains limiting the frameworks applicability. 3. Potential for Overfitting: With the increased model complexity and multiple components contributing to task modulation theres a risk of overfitting especially in scenarios where data is extremely scarce or noisy which is not thoroughly investigated in the paper.  Questions and Feedback 1. Generalizability Across Domains: How well does the CTML framework generalize across significantly different domains or more diverse tasks beyond the experimented fewshot image classification and coldstart recommendation 2. Impact of Task Clustering Choices: The choice of soft Kmeans clustering is central to CTML\u2019s functionality. Can you discuss the sensitivity of the framework\u2019s performance to different types of clustering algorithms or the method used to determine the number of clusters 3. Evaluation on Model Robustness: Given that metalearning aims for models to quickly adapt to new tasks it would be beneficial to evaluate CTML\u2019s robustness against various types of data perturbations or adversarial scenarios. Has such an analysis been conducted  Review Summary CTML represents a significant advancement in metalearning spearheading an innovative approach to tackle task heterogeneity by integrating both feature and optimization path representations. Its performance demonstrated across two application domains validates its potential to enhance learning efficiency and effectiveness in fewsample settings. Despite its strengths questions remain regarding the frameworks generalizability sensitivity to clustering choices and robustness which should be addressed to bolster its applicability further. The introduced method contributes meaningfully to the literature encouraging further exploration in taskaware metalearning algorithms.", "fkjO_FKVzw": " Review of \"Coarformer: A TwoView Architecture for Efficient Graph Transformer\"  1) Summary: The paper introduces Coarformer a novel method designed to overcome the challenges of applying Transformers to large graphs which suffer from distractions caused by numerous distant nodes and quadratic computational complexity. Coarformer leverages the strengths of both Graph Neural Networks (GNNs) and Transformers by incorporating a twoview architecture that combines finegrained local information captured by a GNNbased module and coarse yet essential global information captured by a Transformerbased module applied to a coarsened version of the graph. This architecture allows for efficient handling of large graphs while preserving critical structural information. The paper presents a crossview propagation scheme to enable mutual enhancement of the local and global views thereby synthesizing comprehensive node representations. Extensive experiments demonstrate Coarformers superior performance in terms of accuracy running time and GPU memory consumption compared to existing methods.  2) Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Combination of Techniques: The novel twoview architecture that combines GNNs for local structure encoding with a Transformer applied to a coarsened graph is both innovative and logically sound. It addresses significant limitations of existing models.  Efficiency Improvements: The approach to reduce the computational complexity of Transformers from quadratic to linear by applying them on a coarsened graph without significantly comprising the quality of information capture is a notable improvement.  Extensive Evaluation: The comprehensive experiments conducted across various datasets comparing the proposed method with multiple baselines and analyzing it under different configurations provide strong evidence of the effectiveness and efficiency of Coarformer. Weak Points:  Lack of Theoretical Analysis: While the paper provides ample empirical evidence of its claims a more detailed theoretical analysis or discussion on why the twoview architecture leads to superior performance would add depth to the understanding of the proposed method.  Generalization to Heterogeneous Graphs: The experiments mainly focus on homogeneous graphs. An exploration of how Coarformer performs on heterogeneous graphs with multiple types of nodes and edges could further extend its applicability and usefulness. Questions and Feedback: 1. Regarding Oversmoothing: Did the authors consider the issue of oversmoothing common in deep GNN architectures within the finegrained local view If so how does Coarformer mitigate this issue 2. Impact of Coarsening Algorithm: The paper states that different coarsening algorithms do not significantly impact performance. Could the authors elaborate on whether specific graph characteristics might favor one algorithm over others 3. Comparison with StateoftheArt Models: How does Coarformer compare with the very latest stateoftheart models particularly those that may not use Transformers but achieve high efficiency and accuracy on large graphs 4. Scalability and Threshold of Efficiency: Is there an upper limit to the graph size or density for which Coarformer maintains its efficiency advantage Understanding this would help in defining the scope of its applicability more clearly.  3) Review Summary: Coarformer presents a significant advancement in the domain of graph representation learning by ingeniously combining the strengths of GNNs and Transformers. Its twoview architecture effectively addresses the limitations of applying Transformers to large graphs by reducing computational complexity and focusing attention on meaningful contextually relevant information. The methods superior performance validated through extensive experiments showcases its potential to push the boundaries of current graph analysis capabilities. However enriching the paper with theoretical insights on the performance drivers and extending the evaluation to heterogeneous graphs could broaden the understanding and application scope of Coarformer. Based on the comprehensive evaluation and the novelty of the approach the recommendation is strongly in favor of publication with consideration for the suggested enhancements to further solidify the contribution of this work.", "vkZtFD0zga8": " Peer Review of \"UncertaintyAware Deep Video Compression\"  1) Summary of the Paper This paper introduces an innovative approach to deep learningbased video compression that specifically addresses the problem of predictive uncertainty in intermediate predictions of motion vectors (MVs) and residuals. Traditional approaches struggle with accurately compressing videos due to the deterministic nature of their intermediate predictions which do not account for the inherent aleatoric (observation noise) and epistemic (model uncertainty) uncertainties. To combat this the authors propose an ensemblebased model that utilizes multiple predictions (ensemble members) to better represent the underlying uncertainty of these intermediate predictions. By doing so the model can selectively compress less reliable predictions enhancing overall compression efficiency. Additionally the paper demonstrates the benefits of incorporating adversarial training to further improve the robustness and performance of the video compression task. The experimental results conducted on 1080p video sequences showcase significant bitrate savings over the stateoftheart models particularly highlighting a more than 20 improvement compared to DVC Pro.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality and Innovation: The paper presents a novel ensemblebased approach to handle predictive uncertainty in video compression an area that has not been extensively explored in prior studies. This represents a significant advancement in the field.  Comprehensive Experimental Evaluation: The authors undertake a thorough experimental evaluation demonstrating the effectiveness of their model across various datasets and against stateoftheart competitors.  Practical Significance: Addressing predictive uncertainty can significantly enhance the efficiency of video compression which has profound implications for video streaming and storage especially given the increasing consumption of highresolution video content. Weak Points:  Complexity Analysis: While the paper mentions negligible complexity increase with the ensemble approach a more detailed discussion on computational overhead in terms of both inference time and resource usage would be beneficial especially for practical applications.  Generalization to Other Codecs: The focus is primarily on optical flowbased video codecs. It would be interesting to know how the proposed ensemble method and the adversarial training strategy generalize to other types of codecs including traditional codecs like H.264 or HEVC.  Dataset Diversity: The experiments are conducted on 1080p sequences. Including results from a broader range of resolutions particularly higher resolutions like 4K or even 8K would significantly strengthen the paper. Questions and Feedback:  Model Robustness: How does the model perform under extreme conditions such as very high motion videos or videos with significant background noise Is there a noticeable degradation in compression efficiency  Extension to Realtime Streaming: Can the proposed model be adapted for realtime video streaming applications where latency is a critical factor If so what are the expected challenges and tradeoffs  Future Work Directions: The paper outlines potential future directions regarding encoderside uncertainty modeling. Could the authors elaborate on how they envision this could be implemented and its possible impact on performance  3) Review Summary with Recommendations The paper presents a compelling and innovative approach to video compression tackling the often overlooked issue of predictive uncertainty in deep learningbased codecs. The introduction of an ensemblebased model combined with adversarial training not just contributes to the theoretical advancements in the field but also showcases significant practical improvements over existing stateoftheart methods. The experimental results support the proposed models efficacy illustrating its potential to redefine the video compression landscape. However for a stronger contribution it would be beneficial to address the mentioned weaknesses particularly by providing a detailed computational complexity analysis and exploring the models generalizability beyond optical flowbased codecs and 1080p video sequences. Addressing these areas would enhance the papers impact and widen its applicability in realworld scenarios. Given the notable strengths and the potential to mitigate the outlined weaknesses I recommend this paper for publication after considering the provided feedback and questions. The novel insights and demonstrated improvements it brings forth are valuable to the field of video compression and deep learning.", "sS0dHmaH1I": " Peer Review for \"A Novel Framework for Fast Adaptive Anomaly Detection and Localization in Images\"  1. Summary The paper presents a novel framework aimed at addressing the challenges in anomaly detection and localization within images focusing on rapid adaptability to new tasks without the need for extensive retraining or large datasets of normal samples. The core of the proposed solution is an Energy Based Model (EBM) paired with an adaptive sparse coding layer. This combination allows the system to assign low energy to normal samples while identifying abnormal samples through higher energy values generated via Langevin Dynamics (LD). The adaptive sparse coding layer facilitates quick task adaptation by updating the dictionary of normal sample features for every new task with only a few normal shots. Additionally the paper leverages metalearning for simulating fewshot settings during training which aids in quick adaptation to novel tasks. The frameworks effectiveness is demonstrated through experiments on industrial inspection and video surveillance tasks showing competitive performance against other methods including those requiring large normal sample datasets.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Use of EBMs and Sparse Coding: The papers approach to leveraging EBMs alongside an adaptive sparse coding layer for anomaly detection is innovative. This combination addresses the challenge of rapidly adapting to new tasks without large datasets.  Metalearning for Fewshot Adaptation: Employing metalearning to simulate fewshot scenarios during training is a clever strategy. It effectively reduces the need for extensive datasets while ensuring the models adaptability.  Comprehensive Evaluation: The evaluation across different applications such as industrial inspection and video surveillance underlines the models versatility and effectiveness. The comparisons to other methods and detailed ablation studies add credibility to the findings. Weak Points:  Limited Exploration of Alternative Adaptive Techniques: While the proposed method shows promise the paper does not explore or discuss the potential of other adaptive learning techniques that could complement or compete with the proposed approach.  Lack of Discussion on Computational Efficiency: There is little to no discussion on the computational costs associated with the proposed method especially concerning the LD procedure and online updates of the sparse coding layer. This information would be crucial for realworld applications.  Sparse Detailing on Metalearning Strategy: The paper could benefit from a more detailed explanation of the metalearning strategy particularly on how exactly the fewshot settings are simulated during training. Questions and Feedback:  Can the authors clarify if there were any computational bottlenecks observed during the application of LD for generating abnormal samples especially in realtime scenarios  How robust is the proposed method against variations in anomaly scale and complexity within different tasks Some qualitative results or discussions around this would be beneficial.  The paper would benefit from a deeper discussion on the potential limitations of the proposed method and scenarios where it might not perform optimally.  3. Review Summary The paper introduces a compelling framework for anomaly detection and localization demonstrating significant innovation in employing EBMs and sparse coding for this purpose. The usage of metalearning for facilitating quick adaptation with minimal data samples represents a considerable advancement in the field. Despite these strengths the paper could be improved by addressing the outlined weaknesses such as exploring computational implications in more detail considering other adaptive techniques and elaborating on the metalearning process. Given the papers contributions and the relatively minor nature of its shortcomings my recommendation is for acceptance contingent upon the authors addressing the provided feedback particularly around computational efficiency and expanding upon the methodologys limitations and the metalearning strategy. The frameworks capability to adapt rapidly to new tasks with limited data sets it apart in the field of anomaly detection and localization marking a noteworthy contribution to the domain.", "yjsA8Uin-Y": " Review of \"A TrainingFree Solution to Detect Noisy Labels Using Similar Representations\"  1) Summary of the Paper The paper introduces a novel trainingfree method to detect noisy labels in datasets by leveraging the representations of data instances. Recognizing the pervasive issue of label noise in realworld datasets the authors propose an alternative to the common practice of designing robust training techniques aimed at preventing Deep Neural Networks (DNNs) from overfitting to corrupted patterns. This common approach is criticized for requiring customized training processes for each dataset and the inevitability of overfitting as long as models are trained on noisy supervision. The paper offers a solution that utilizes \"good representations\" to establish neighbors for each instance and infer clean labels through \"local voting\" or a \"rankingbased\" approach thereby bypassing the need for training on noisy labels. The methods are validated theoretically showing how representations influence local voting and providing guidelines for tuning the neighborhood size. Furthermore the authors demonstrate through experiments that their trainingfree solutions yield significant improvements over many trainingbased baselines in detecting label noise.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality and Innovation: The trainingfree approach to detecting noisy labels is novel and offers a fresh perspective compared to the prevalent trainingbased methods. This is particularly impactful given the universal applicability of the proposed solution.  Theoretical Foundation: The paper presents a solid theoretical analysis that not only supports the proposed methods but also provides a framework for understanding the role of good representations in label noise detection.  Experimental Validation: The authors conduct comprehensive experiments with both synthetic and realworld label noise demonstrating the effectiveness and robustness of their methods across various scenarios. Weak Points:  Assumption of Good Representations: The effectiveness of the proposed methods heavily relies on the availability of \"good representations.\" However the paper lacks a clear definition or characterization of what constitutes good representations and how they can be consistently obtained or recognized in practical scenarios.  Lack of Comparison with Latest Approaches: While the paper offers comparisons with several baseline methods it might benefit from including recent stateoftheart techniques in label noise detection to further validate its superiority.  Generalizability Concerns: The focus is mainly on classification tasks it would be beneficial to explore how the approach performs on other types of tasks or in less conventional data domains. Questions and Feedback:  Could the authors elaborate on how \"good representations\" are identified or ensured in practice especially in scenarios where pretrained models or domainspecific representations are not readily available  How scalable are the proposed methods when applied to very large datasets given the reliance on neighborhood information that might require extensive computational resources  It would be insightful if the paper could discuss the implications of applying this approach to semisupervised or unsupervised learning schemes enhancing its applicability to a wider range of learning tasks.  3) Review Summary and Recommendation The paper presents a significant contribution to the field of machine learning specifically in the context of label noise detection. By introducing a trainingfree solution that leverages the power of good representations the authors address the limitations of existing trainingbased methods offering a potentially universal tool for improving data quality. The theoretical substantiation and the promising experimental results underscore the potential of the proposed methods. However there are areas for enhancement notably in defining and ensuring the quality of representations used in the method and extending the validation to encompass a broader range of tasks and newer baseline comparisons. Addressing these could further solidify the papers contributions and applicability. Given the novelty of the approach its theoretical grounding and the positive experimental findings I recommend this paper for publication. It opens up new avenues for research in label noise detection and potentially other related areas. Nevertheless I encourage the authors to consider the raised points and questions to refine and deepen the impact of their work.", "gWGexz8hFH": " Summarize the paper This paper presents a novel approach the Distributed Skellam Mechanism (DSM) to enforce differential privacy in federated learning systems utilizing multiparty computation (MPC) without a trusted third party. Traditional methods struggle with compatibility issues between MPCs integer requirements and differential privacys realvalued noise often leading to either compromised privacy or utility. DSM introduces a Skellamdistributionbased noise addition strategy which is both MPCcompatible and allows for tighter privacy cost analysis independent of the gradients dimensionality. Extensive theoretical analysis and experimental validation using benchmark datasets (MNIST and Fashion MNIST) showcase DSMs superior performance in maintaining high model utility while ensuring robust privacy guarantees as compared to existing solutions like cpSGD and DDG.  Strong and weak points Question and Feedback to the author Strong Points: 1. Innovation: The proposed DSM represents a significant methodological advancement in reconciling the needs for differential privacy (utilizing Skellam distribution) with the constraints of MPCbased federated learning. Its design to be compatible with secure aggregation protocols without sacrificing privacy or model utility is particularly noteworthy. 2. Rigorous Theoretical Foundation: The paper provides a comprehensive theoretical analysis of DSMs privacy guarantees including proofs of its compliance with Re\u0301nyi Differential Privacy and the classic (\u03b5 \u03b4)Differential Privacy definitions. The treatment of intricate mathematical concepts like the Skellam distribution and its properties in this context is both thorough and insightful. 3. Empirical Validation: The extensive experiments conducted offer convincing evidence of DSMs practical viability demonstrating its superior model utility over competitors across various settings including different privacy levels communication constraints and dataset complexities. Weak Points: 1. Complexity and Accessibility: The mathematical rigor while a strength also makes the paper somewhat inaccessible to practitioners or researchers outside a highly specialized domain. A more intuitive explanation or visualization of key concepts could enhance its accessibility. 2. Limited Scope of Experimental Validation: While the results on MNIST and Fashion MNIST are impressive these datasets are relatively simple and wellstudied. The performance of DSM in more complex realworld federated learning scenarios remains untested. Applications to larger and more heterogeneous datasets could better demonstrate its utility and robustness. 3. Lack of Discussion on Implementation Challenges: The paper could benefit from a deeper discussion on the practical challenges of implementing DSM in real federated learning systems such as computational overhead the efficiency of the secure aggregation protocol and interoperability with existing federated learning frameworks. Questions and Feedback: 1. Scalability and Performance in Complex Settings: How does DSM scale with increased model complexity and data heterogeneity Specifically is there a degradation in utility or an increase in computational costs when applied to larger neural networks or more diverse federated learning environments 2. Robustness to Adversarial Attacks: While DSM shows promising privacy guarantees its robustness against advanced adversarial attacks targeting the federated learning setup (e.g. model inversion attacks) is not discussed. Can the authors comment on this aspect 3. Comparison with Novel Privacypreserving Techniques: The paper compares DSM with cpSGD and DDG. However the landscape of privacypreserving algorithms is rapidly evolving. How might DSM perform in comparison to more recent approaches particularly those leveraging advanced cryptographic techniques or alternative differential privacy mechanisms  Review Summary The paper introduces a promising novel approach DSM to enhance privacy in federated learning systems using an MPCcompatible differential privacy mechanism based on the Skellam distribution. Its strong theoretical foundation coupled with impressive experimental results underscores its potential as a significant advancement in reconciling privacy guarantees with practical utility in federated learning. However the complexity of the mathematical exposition and the somewhat narrow scope of empirical testing limit its immediate accessibility and the broader applicability of its findings. Future work could benefit from addressing these limitations by expanding the experimental scope simplifying the presentation for a wider audience and exploring the mechanism\u2019s performance and robustness in more diverse and complex federated learning scenarios. In conclusion the paper represents a substantial contribution to the field laying a solid foundation for further exploration and development of privacypreserving techniques in federated learning.", "ivQruZvXxtz": "Review of \"Effectiveness of Sequential Reptile in Preventing Catastrophic Forgetting During Multilingual Model Finetuning\" 1. Summary The paper addresses the issue of catastrophic forgetting in multilingual models pretrained on multiple languages and finetuned for monolingual downstream tasks. The authors propose a strategy termed Sequential Reptile that aims to prevent negative transfer and catastrophic forgetting by aligning gradients between tasks during the finetuning phase. Through extensive experiments with multilingual tasks and zeroshot crosslingual transfer tasks they demonstrate the superiority of Sequential Reptile over existing methods. The proposed approach includes performing inner optimization by sequentially sampling batches from all tasks followed by a Reptile outer update thus efficiently aligning gradients between tasks and retaining linguistic knowledge from pretraining. 2. Strengths and Weaknesses Strengths: a) Innovation and Relevance: The paper addresses a critical and relevant problem in the field of multilingual natural language processing. The innovative Sequential Reptile method effectively aligns task gradients which is a significant contribution to preventing catastrophic forgetting and negative transfer thus advancing the stateoftheart in multilingual model finetuning. b) Comprehensive Evaluation: The extensive validation of Sequential Reptile across various multitask learning and zeroshot crosslingual transfer tasks adds credibility to the proposed solution. Comparisons against a wide range of relevant baselines underline the effectiveness of the proposed method. c) Theoretical and Practical Implications: The conceptual framework provided for understanding the need for gradient alignment in multitask learning along with detailed implementation details furnishes both theoretical insights and practical guidelines for applying the technique in realworld scenarios. Weaknesses: a) Generalization beyond NLP: The paper focuses exclusively on multilingual natural language processing tasks. It remains unclear how generalizable the proposed Sequential Reptile method is to other domains or types of multitask learning problems outside NLP. b) Analysis Depth: While the paper provides a quantitative analysis of performance improvements and comparisons with baselines it lacks an indepth qualitative analysis or case studies to understand why and how Sequential Reptile outperforms other methods in specific instances. c) Impact of Hyperparameters: The paper discusses the choice of hyperparameters like learning rates but does not explore their impact on performance comprehensively. Specifically an analysis of how different settings of the innerlearning rate \u03b1 and outerlearning rate \\xce\\xb7 affect the models ability to prevent catastrophic forgetting would strengthen the findings. Questions and Feedback: a) Can the authors provide more insight into the generalizability of Sequential Reptile to other domains beyond NLP b) A deeper qualitative analysis or case studies highlighting instances where Sequential Reptile significantly outperforms other methods could give readers a better understanding of its advantages. Can the authors incorporate such an analysis c) It would be beneficial if the authors could explore the sensitivity of Sequential Reptiles performance to various hyperparameters providing guidelines for their effective setting in different scenarios. 3. Review Summary The paper \"Effectiveness of Sequential Reptile in Preventing Catastrophic Forgetting During Multilingual Model Finetuning\" presents a compelling solution to the critical problem of catastrophic forgetting in the finetuning phase of multilingual models. The proposed Sequential Reptile methodology distinguished by its innovative approach to gradient alignment showcases significant performance improvements across a range of NLP tasks. While the study is wellconducted with a solid experimental setup extending the methods application beyond NLP deeper qualitative analysis and an exploration of hyperparameter impacts could strengthen the paper further. Nonetheless the contributions of this paper are valuable and could significantly influence future research in multilingual model training. I recommend acceptance considering the important advancements the paper introduces to the field along with minor revisions addressing the highlighted weaknesses.", "z2B0JJeNdvT": "Summarize the paper This paper delves into the domain of zerothorder optimization techniques crucial for solving complex optimization tasks in machine learning especially when cost functions are not analytically describable. Through a comprehensive study the paper transitions from centralized to distributed zerothorder optimization within largescale multiagent systems characterized by decentralized data and costs alongside timevarying communication networks. The core contributions involve establishing convergence rates for distributed zerothorder subgradient algorithms leveraging both onepoint and twopoint zerothorder oracles. Remarkably despite the extra communication overhead in a distributed setting convergence rates akin to centralized algorithms are achieved. Additionally an innovative multistage distributed zerothorder algorithm is proposed enhancing learning rates reducing computational complexity and thus achieving superior convergence rates for compact decision sets. Numerical examples on distributed ridge regression validate the theoretical conclusions indicating the competitive if not superior performance of the introduced zerothorder algorithms compared to traditional gradient methods. Strong and Weak Points Questions and Feedback Strong Points: 1. Theoretical Contribution: The paper successfully extends the understanding of zerothorder optimization from centralized scenarios to more complex distributed multiagent systems providing a valuable and theoretically rigorous framework for future research in distributed optimization. 2. Practical Relevance: By addressing common challenges in machine learning optimization such as nonanalytically describable cost functions and the need for decentralized data processing the proposed methods are highly relevant for realworld applications. 3. Numerical Validation: The inclusion of numerical examples specifically the distributed ridge regression problem offers empirical evidence supporting the theoretical advancements and demonstrates the practical utility of the methodology. Weak Points: 1. Accessibility and Readability: The paper is densely packed with technical content which might make it challenging for readers not already familiar with zerothorder optimization or distributed computing to fully grasp the contributions without considerable effort. 2. Limited Comparison: While the numerical examples compare the proposed methods with traditional gradient approaches a broader comparison including other stateoftheart distributed optimization algorithms would provide a clearer picture of the proposed methodologys competitiveness. 3. Realworld Scalability: The paper could further benefit from discussions or examples on the scalability of the proposed algorithms in realworld largescale problems that involve more complex network topologies or higher dimensions of data. Questions and Feedback: 1. Extension to Nonconvex Functions: Is there potential to extend the presented algorithms and theoretical results to nonconvex objective functions This would greatly enhance the applicability of the research. 2. Impact of Network Dynamics: The paper assumes a positive integer B such that the union graph remains strongly connected. How sensitive are the proposed algorithms and their convergence rates to more drastic changes in network topology 3. Algorithm Adaptability: How adaptable are the proposed algorithms to different oracles or function classes not covered in the paper Insights into the methods flexibility can promote wider adoption in varying scenarios. Review Summary and Recommendation The paper represents a significant stride toward understanding and enhancing zerothorder optimization in distributed multiagent systems. Its theoretical contributions coupled with practical numerical examples underscore the methods efficacy and relevance to contemporary challenges in machine learning optimization. While the work stands on solid ground with its rigorous approach and validation it can be improved by addressing accessibility for a broader audience extending comparisons with a wider range of algorithms and discussing scalability and adaptability aspects in more detail. Given the strengths outweigh the identified weaknesses which mainly pertain to enhancing clarity and breadth of comparisons I recommend accepting this paper. The feedback provided aims to guide the authors in refining their manuscript for an even broader impact ensuring that readers can fully appreciate the significance and applicability of the research.", "oaKw-GmBZZ": " Review of the Paper on Developing Efficient PDE Solvers Using Graph Neural Networks  Summary The paper presents a novel approach to solving timedependent partial differential equations (PDEs) using Message Passing Graph Neural Networks (MPGNNs). The authors propose a method that leverages the inherent structure of PDEs to create domaininvariant solvers capable of operating on arbitrary discretizations and physical geometries. Key innovations include the introduction of domaininvariant features inspired by classical PDE solver techniques graph representation of PDE data on unstructured meshes and a recurrent neural network methodology for temporal sequence prediction. The paper demonstrates the effectiveness of this approach across several test scenarios including the heat equation NavierStokes equation and advectiondiffusion equation showing capable generalization to new geometries and improvement over existing neural networkbased methods.  Strong and Weak Points Strong Points: 1. Novel Approach: The paper introduces a groundbreaking method of using MPGNNs for solving timedependent PDEs integrating domaininvariant features and demonstrating successful generalization across different mesh discretizations and geometries. 2. Comprehensive Evaluation: The authors provide a thorough evaluation across various PDEs and compare their results against existing neural networkbased approaches showcasing the superior performance and generalization capability of their model. 3. Clear Rationale: The rationale behind each design choice such as the selection of domaininvariant features and recurrent neural network structure is clearly articulated and grounded in computational PDE solving principles. Weak Points: 1. Implementation Detail Scarcity: The paper occasionally lacks indepth discussion on specific implementation details such as exact architectural nuances of the MPGNNs or how boundary conditions are incorporated into the learning process. 2. Performance Benchmarks Limitations: Although the paper includes comparisons with other methods it does not provide benchmarks against traditional numerical PDE solvers which could provide more context regarding the computational efficiency and accuracy tradeoffs. 3. Exploration of Limitations: The authors could further explore the limitations of their approach such as the impact of graph size and connectivity on solver performance or potential challenges in applying the method to highly irregular or dynamic meshes. Questions and Feedback to the Author: 1. Could the authors provide more details on how the boundary conditions are represented and processed within the MPGNN framework 2. How does the performance of the proposed method compare to traditional numerical solvers in terms of computational efficiency and accuracy especially for largescale problems 3. Are there specific types or classes of PDEs for which this approach might not be suitable or might require significant modification  Review Summary This paper presents a promising and innovative approach to solving timedependent PDEs using MPGNNs addressing the challenge of developing computationally efficient solvers that are accurate stable and capable of generalizing across different discretizations and geometries. The introduction of domaininvariant features and a recurrent neural network model for temporal prediction represents a significant advancement in the field of computational mathematics and machine learning applications to PDE solving. The evaluation of the method across different PDEs and comparison with existing neuralnetworkbased approaches highlight its potential for broad applicability and superior performance. However further details on implementation specifics benchmarks against traditional numerical solvers and a deeper exploration of the methods limitations would enhance the papers contribution to the field. Considering the innovative approach comprehensive evaluation and significant potential impact of the work I recommend this paper for publication contingent upon addressing the discussed weak points and questions. The advancements presented have the potential to influence future research directions in computational PDE solving and machine learning applications in this space significantly.", "hpBTIv2uy_E": " Review of \"AllSet: A General Framework for Learning in Hypergraph Neural Networks\"  Summary The paper introduces AllSet a novel hypergraph neural network framework. Hypergraphs which model higherorder interactions through hyperedges connecting multiple nodes offer a means to represent complex relationships beyond binary associations common to graphs. Existing methods to work with hypergraphs often rely on heuristic propagation rules (e.g. clique expansion (CE)) which can distort the data and impact performance. Addressing this AllSet provides a general framework that overcomes the limitations of CE and tensorbased propagation by implementing hypergraph neural network layers as compositions of two multiset functions enabling the efficient learning of these functions tailored to specific tasks and datasets. Incorporating Set Transformers and Deep Sets into this framework for the first time the authors demonstrate through exhaustive experiments that AllSet can match or exceed competing methods across a broad spectrum of datasets thereby establishing a new stateoftheart in hypergraph neural network performance.  Strong Points 1. Novelty and Generality: The AllSet framework is a significant advancement in hypergraph neural network research offering a highly generic and flexible approach that subsumes existing propagation schemes as special cases. Its generality is robustly backed by the theoretical foundation laid out in the paper. 2. Integration of Set Transformers and Deep Sets: This integration is innovative and leverages the power of these models for learning multiset functions within the context of hypergraph neural networks contributing to the high expressive power and flexibility of AllSet. 3. Extensive Experimental Validation: The comprehensive nature of the experimental evaluation covering thirteen datasets including ten benchmarks and three new ones is commendable. It not only establishes the superiority of AllSet over existing methods but also validates the framework across a variety of contexts and challenges bolstering the authors claims significantly. 4. Open Source Contribution: By making the AllSet implementation publicly available the authors encourage reproducibility and further experimentation aligning well with the principles of open science.  Weak Points and Feedback 1. Comparison with Baseline Methods: While the paper presents extensive comparisons with existing methods it would be beneficial to include a more detailed discussion on why certain existing methods underperform relative to AllSet. This could offer deeper insights into the limitations of current models and further highlight AllSets strengths. 2. Complexity Analysis: The paper could benefit from a more detailed complexity analysis comparing AllSet with other methods particularly in terms of computational and memory requirements. This would provide a more complete perspective on the practical applicability of AllSet especially for largescale hypergraphs. 3. Hyperparameter Sensitivity: There is limited discussion on the sensitivity of AllSets performance to hyperparameter settings which is crucial for practical applications. Expanding the experimental evaluation to include hyperparameter sensitivity analysis would strengthen the paper. 4. Broader Impact Discussion: While the ethics statement briefly addresses potential issues a more thorough exploration of the broader societal and ethical implications including misuse and privacy concerns would enhance the paper.  Review Summary AllSet represents a significant leap forward in the field of hypergraph neural networks introducing a versatile and powerful framework that marks notable improvements over existing methods. The papers strengths lie in its innovative approach thorough theoretical grounding and extensive experimental validation which collectively make a compelling case for the adoption of AllSet in various applications involving hypergraphs. Nonetheless elaborating on the comparison with baseline methods providing a detailed complexity analysis exploring hyperparameter sensitivity and expanding the discussion on broader impact would further solidify the papers contributions. Considering the comprehensive experimental evidence supporting AllSets superior performance and the novel integration of Deep Sets and Set Transformers I recommend acceptance provided the authors address the feedback to enrich the analysis and discussion concerning the weak points mentioned.", "jaLDP8Hp_gc": " Review of \"Hallucinating Feature Correspondences for Enhanced Pose Estimation: A Deep Learning Approach\"  Summary This paper introduces an innovative approach to the problem of establishing correspondences between partially overlapping images where traditional local feature matching methods fail due to occlusions or portions of the image being outside the field of view. The central contribution of the work is the development of a neural network named NeurHal designed to predict the probable location of a keypoints correspondence in the target image irrespective of visibility constraints. Through comprehensive experiments on various datasets the paper showcases the networks capability to hallucinate correspondences effectively in unseen scenes substantially improving the robustness of absolute camera pose estimation compared to stateoftheart local feature matching techniques. The work stands on the novel idea of transforming the correspondence finding task from a purely recognition problem to one that involves prediction in nonvisible regions leveraging geometric reasoning analogous to human perception.  Strengths and Weaknesses Strong Points: 1. Originality: The concept of learning to hallucinate correspondences in partially overlapping images introduces a fresh perspective to the field of computer vision particularly in the challenges of image matching and pose estimation. 2. Methodological Rigor: The paper provides a detailed analysis of the problem and thoughtfully designs the network architecture and loss function (Neural Reprojection Error) to address the specific challenges posed by the task of correspondence hallucination. 3. Experimental Validation: Extensive experiments demonstrate the networks generalization ability across different settings (indooroutdoor) and superior performance in absolute camera pose estimation when compared to existing methods. Weak Points: 1. Resolution of Predicted Correspondences: The low resolution of the correspondence maps generated by NeurHal a consequence of the architecture\u2019s design might limit the precision of the hallucinated correspondences. 2. Complexity and Computation: The quadratic complexity associated with the attention layers and the overall architecture might impose practical limitations on scalability and realtime applications. 3. Finite Correspondence Map Size: The potential for incorrect correspondence maps due to keypoints falling outside the predefined size emphasizes a limitation in the networks current architecture. Questions and Feedback: 1. Resolution Improvement: Have the authors considered approaches to enhance the resolution of the correspondence maps without significantly increasing the computational burden 2. Scalability: Could the authors expand on potential methods or future work that might address the scalability and computational efficiency for realtime applications 3. Beyond Pose Estimation: The paper briefly mentions the potential for integrating NeurHal into StructurefromMotion pipelines. Could the authors elaborate on how they envision this integration and its challenges  Review Summary The paper represents a significant contribution to image matching and pose estimation by innovatively tackling the problem of finding correspondences in images where traditional methods fail. Its approach to learning to hallucinate correspondences opens up new avenues in computer vision research particularly in dealing with occlusions and field of view limitations. While it demonstrates compelling results there are acknowledged limitations such as the resolution of the correspondence maps and the computational intensity of the architecture. Despite these limitations the paper provides a solid foundation for future work not only in improving the proposed method but also in exploring its integration into broader computer vision pipelines. The method\u2019s ability to generalize across different datasets and its demonstrated practical benefits in pose estimation underscore the value of this work. Given the strengths of the originality methodological rigor and the novelty of the approach alongside the thorough experimental validation this paper is a valuable addition to the landscape of computer vision research. It is recommended that the authors address the questions and feedback mentioned as these could further strengthen the paper and guide future research directions.", "tJhIY38d2TS": "Summarize the paper: The paper introduces Locally Reweighted Adversarial Training (LRAT) a novel adversarial training method aimed at improving the robustness of neural network models against a variety of attacks. Unlike previous strategies that either do not reweight training instances (Standard Adversarial Training SAT) or apply global reweighting based on the vulnerability of instances to a specific attack (InstanceReweighted Adversarial Training IRAT) LRAT focuses on local reweighting within each pair of an instance and its adversarial variants. This approach ensures that all instances regardless of their initial vulnerability contribute to training the model enabling it to defend against both the attacks simulated during training and new unseen attacks. The central idea is to adjust the weight of instances based on their susceptibility to various attacks thereby tailoring the training process more closely to the inherent variability of adversarial threats. Experimental results demonstrate the effectiveness of LRAT over both IRAT and SAT across multiple attacks confirming its utility as a more flexible and robust adversarial training framework. Strong and Weak Points Questions and Feedback: Strong Points: 1. Innovative Approach: The concept of locally reweighting instances based on their vulnerability to specific attacks is novel and addresses a significant limitation of previous adversarial training methods. 2. Comprehensive Evaluation: The paper presents a thorough experimental analysis comparing LRAT with existing methods (IRAT and SAT) across different attacks convincingly demonstrating its superior performance. 3. Clear Rationale: The paper provides a clear and logical reasoning for the need for local reweighting in adversarial training supported by visual evidence (e.g. tSNE plots) and theoretical arguments. Weak Points: 1. Lack of Theoretical Analysis: While the empirical results are strong the paper lacks a theoretical framework or analysis that explains why local reweighting leads to improved robustness. 2. Generalization Concerns: The evaluation mainly focuses on L\\xe2\\x88\\x9enorm bounded attacks simulated during training raising questions about how LRAT performs against a broader range of adversarial threats including those with different constraints or objectives. 3. Implementation Details: While the paper promises to provide source codes for reproducibility more details about the implementation (e.g. network architectures hyperparameters) within the main text would be beneficial for readers wanting to replicate the results or build upon this work. Questions and Feedback:  Could the authors provide more insight into the decision process for choosing hyperparameters (\u03b1 \u03b2 C) in the reweighting function How sensitive is LRATs performance to these hyperparameters  It would be interesting to see a theoretical analysis or discussion on why local reweighting provides an advantage over global reweighting and no reweighting strategies possibly through the lens of optimization landscape or model generalizability.  Exploring the performance of LRAT on a wider array of adversarial attacks particularly those not bounded by L\\xe2\\x88\\x9e norms or those employing different strategies (e.g. transfer attacks querybased attacks) could provide a more comprehensive picture of its robustness. Review Summary: This paper presents a compelling advancement in adversarial training methodologies by introducing the concept of locally reweighted adversarial training (LRAT). By focusing on the local reweighting of instances based on their vulnerability to specific attacks LRAT addresses a critical shortfall in existing adversarial training approaches that rely on global or no reweighting strategies. The experimental results convincingly demonstrate LRATs superior performance in defending against various adversarial attacks making a significant contribution to the field of adversarial robustness. However the paper could be strengthened by providing a deeper theoretical understanding of why local reweighting is effective and examining the performance of LRAT against a broader spectrum of adversarial threats. Furthermore additional details on the implementation would enhance the reproducibility and applicability of LRAT. Overall this paper offers valuable insights and a promising direction for future research in adversarial training warranting further investigation and development.", "luO6l9cP6b6": " Peer Review  1. Summary of the Paper The paper undertakes a systematic exploration into the puzzle of why pretrained language models such as BERT exhibit substantial crossdomain transfer capabilities even when finetuned on data that is significantly different from the training set including data where word identity is obscured through random scrambling. To investigate this phenomenon the authors conduct experiments on four classification tasks and two sequence labeling tasks using models like LSTMs with GloVe embeddings and BERT as well as baseline models comparing performance on scrambled vs. unscrambled datasets. Their key discovery is that BERT but not other models maintains strong performance on classification tasks even when trained on scrambled data suggesting that BERTs pretraining enables it to capture transferable linguistic patterns beyond mere word identity. The paper also delves into exploring aspects such as the role of word frequency finetuning versus pretraining contributions and the ability of models to relearn or reassociate word meanings from scrambled data.  2. Points of Strength and Weakness Strong Points:  Systematic Approach: The paper impressively applies a systematic and innovative exploration of crossdomain transfer capabilities in language models by denying the model access to word identities offering a novel angle in the study of language model transferability.  Comprehensive Analysis: It goes beyond merely presenting experimental results to analyze the underlying reasons for observed phenomena investigating the role of word frequency and disentangling the contributions of pretraining versus finetuning.  Significant Finding: The discovery that BERT unlike LSTMs or BoW models can successfully leverage crossdomain transfer even with scrambled inputs for classification tasks is highly relevant both for theoretical understanding and practical applications of language models. Weak Points:  Limited Scope on Sequence Labeling Tasks: The paper finds that BERT does not transfer as successfully for sequence labeling tasks with scrambled inputs. This limitation is noted but not deeply explored leaving a gap in the understanding of taskspecific transfer capabilities.  Potential Overemphasis on BERT: While the focus on BERT is justified by its performance this might limit insights into how newer or different types of language models may handle crossdomain transfer under similar conditions.  Methodological Clarifications Needed: The process of scrambling (how it might inherently preserve some linguistic properties despite changing word identity) and its potential impact on model learning could be more thoroughly explained to address any confounding factors. Questions and Feedback to the Author:  Could the authors elaborate on how the findings might translate to newer language models especially those that have arisen postBERT (e.g. GPT3)  The paper could benefit from a more nuanced discussion of why sequence labeling tasks do not show similar transferability. Is there a fundamental difference in how these tasks leverage linguistic knowledge compared to classification tasks  More detail on the scrambling methodology would be beneficial specifically regarding how semantic consistency checks ensure that linguistic properties are not inadvertently preserved potentially confounding the results.  3. Review Summary The paper offers an innovative and insightful look into the capabilities of pretrained language models specifically BERT to achieve crossdomain transfer even when deprived of word identity information. This finding is significant shedding light on the robustness and flexibility of such models particularly in classification tasks. However the exploration into sequence labeling tasks and the potential limitations imposed by focusing primarily on BERT calls for further investigation. Moreover clarifications around the scrambling methodology could strengthen the papers claims by ensuring that the experimental design accurately isolates the factor of interest. Despite these areas for improvement the paper makes a valuable contribution to our understanding of language model transferability posing intriguing questions for future research on the mechanisms underpinning this phenomenon. The combination of significant novel findings and thoughtful analysis supports a positive evaluation of this work.", "qSTEPv2uLR8": " Review of \"A Novel Deep Learning Approach for Solving the Continuous Optimal Mass Transport Problem\"  1. Summary This paper presents a new deep learning framework for estimating solutions to the L2optimal mass transport (OMT) problem using Breniers theorem. The authors propose the use of Physics Informed Neural Networks (PINNs) coupled with Input Convex Neural Networks (ICNNs) to solve a MongeAmpere type equation as mandated by Breniers theorem. This approach which they term Physics Informed Convex Artificial Neural Networks (PICANNs) not only promises to tackle the computational complexity associated with the OMT problem especially in high dimensions but also provides an efficient method for density estimation and generation of probabilistic models. The proposed method is benchmarked against existing deep learning OMT solvers and through application in density estimation and generative modeling the utility and improved accuracy of PICANNs over traditional approaches is demonstrated.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Theoretical Foundation: The paper provides a solid theoretical groundwork leveraging Breniers theorem to reduce the continuous OMT problem to a solvable PDE thus establishing a clear connection to the mathematical underpinnings of OMT. b. Innovative Framework: The integration of PINNs with ICNNs to form PICANNs is a novel approach that effectively utilizes the strengths of both networks in solving complex PDEs and ensuring the convexity of the solution respectively. c. Benchmarking and Application: Comprehensive comparisons to stateoftheart methods and detailed applications to density estimation and generative modeling demonstrate the practical relevance and superiority of the proposed method. Weak Points: a. Hyperparameter Optimization: The paper does not provide enough detail on the process of hyperparameter selection which is crucial for the reproducibility and efficiency of deep learning models. b. Computational Complexity: Despite its effectiveness the computational demands and scalability of PICANNs especially concerning network training times and resource usage are not adequately discussed. c. Generality of the Approach: The focus is primarily on the L2OMT problem its unclear how this method performs or adapts to other cost functions or variations of the OMT problem. Questions and Feedback: 1. Hyperparameter Selection: Could the authors elaborate on the hyperparameter tuning process Were any automatic tuning methods employed 2. Computational Resources: What were the computational resources required for the experiments and how does the training time of PICANNs compare with the traditional methods 3. Extension to Other Cost Functions: Can the PICANN framework be adapted to solve OMT problems with cost functions other than the quadratic one If so what modifications would be necessary  3. Review Summary The paper introduces a significant advancement in solving the continuous OMT problem through deep learning. The theoretical justification is strong and the proposed PICANN framework intelligently combines PINNs and ICNNs to address the challenges of solving PDEs related to the OMT problem. The demonstrated improvements over existing methods in terms of accuracy alongside applications in density estimation and generative modeling underscore the potential of this approach. However for broader adoption and replication of this work the authors need to provide more insights into the computational complexities and the process of hyperparameter optimization. Additionally discussing the frameworks adaptability to other cost functions and variations of the OMT problem will enhance the general applicability and significance of the proposed method. In conclusion this paper contributes a promising direction for leveraging deep learning in the domain of optimal mass transport. With further clarifications and possibly addressing the identified weak points this work stands to significantly impact the field offering improved computational methods for both theoretical and applied problems within OMT and related applications.", "q23I9kJE3gA": " Peer Review of \"Improving Conditional Set Generation in SEQ2SEQ Models with Informative Orders and Cardinality Modeling\"  1. Summary of the Paper The paper presents a novel methodology aimed at enhancing the task of conditional set generation within natural language processing (NLP) through sequencetoset prediction tasks. The authors identify that conventional sequencetosequence (SEQ2SEQ) models inadequately leverage the intrinsic set properties\u2014orderinvariance and cardinality\u2014when treating output as a sequence. To address these issues the paper introduces a data augmentation strategy TSAMPLE that imposes informative orders over labels by transforming label sets to sequences using their dependency information and a partial order graph. Alongside it proposes jointly modeling the set output and its cardinality by appending the set size as the sequences first element utilizing autoregressive factorization of SEQ2SEQ models. These modifications are empirically validated through experiments on simulated settings and three NLP datasets showing a significant improvement in performance compared to existing SEQ2SEQ models.  2. Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a critical gap in existing SEQ2SEQ models for set generation tasks specifically the models inability to capture orderinvariance and cardinality which are fundamental set properties.  The proposed method TSAMPLE shows creativity in leveraging data augmentation to generate informative orders based on label dependency potentially eliminating the need for exhaustive search over the label space.  Jointly modeling the cardinality of sets by appending the set size at the beginning of the sequence is innovative and aligns naturally with autoregressive properties of SEQ2SEQ models. Weak Points:  While the paper provides a theoretical grounding for TSAMPLE and its benefits it slightly glosses over potential challenges in scaling the approach to very large label spaces or sets with complex interdependencies.  The analysis is heavily quantitative focusing on performance gains in F1 scores. A more qualitative analysis or case studies could offer deeper insights into how the models predictions change with the proposed improvements.  The paper mentions less about the computational overhead introduced by the augmentation process and the cardinality modeling which could be significant considering the complexity analysis provided. Questions and Feedback:  How does TSAMPLE perform in scenarios where label dependencies are highly dynamic or contextually sensitive Further clarification on this could validate the robustness of the proposed method.  The role of cardinality is emphasized but the paper could benefit from a detailed exploration of cases where incorrect cardinality prediction significantly impacts set generation quality.  It would be beneficial to discuss the limitations of the proposed method more comprehensively particularly concerning scalability and computational efficiency.  3. Review Summary and Recommendation The paper makes substantial contributions to the field of NLP by addressing a nuanced yet critical aspect of SEQ2SEQ modeling for set generation tasks. The novel approach of using data augmentation to incorporate informative orders and jointly modeling set cardinality showcases both creativity and technical depth. The empirical results solidify the argument that these enhancements significantly improve upon the baseline SEQ2SEQ models. However the paper could improve by addressing the scalability of TSAMPLE especially in handling large or complex label spaces. Additionally a deeper qualitative analysis could provide insights into the models behavior beyond performance metrics. Finally discussing the computational implications of the proposed methods would offer a more rounded understanding of their applicability in realworld scenarios. Given the novelty empirical rigor and potential impact of the work I recommend the acceptance of the paper. I advise the authors to consider the feedback and questions raised to strengthen the paper further particularly around the scalability qualitative analysis and computational overhead of their proposed methods.", "sTNHCrIKDQc": " Review of \"Clustering and TwoSample Testing of NetworkValued Data using Graph Distance Metrics\"  Summary The paper addresses the analytical challenge of learning from networkvalued data particularly emphasizing the need for clustering methods applicable where there is no vertex correspondence between networks and traditional analysis methods struggle. Recognizing the limitations of existing approaches in terms of their theoretical depth and applicability beyond classification tasks this work introduces an innovative method based on graphon theory\u2014a field that provides a statistical framework for analyzing infinitely large networks. Specifically the authors propose a new graph distance metric derived from sorting and smoothing graphon estimators which they apply to two novel clustering algorithms demonstrating superior performance compared to current stateoftheart methodologies. Further the paper presents a compelling application of this graph distance metric to the problem of graph twosample testing establishing statistical consistency under specific Lipschitz conditions.  Strong Points 1. Innovative Approach: The methodological novelty leveraging graphon theory to estimate distances between networks without assumption of vertex correspondence is both innovative and a significant contribution to the field of network analysis. 2. Comprehensive Theoretical Foundation: The paper stands out for its rigorous theoretical analysis particularly the proof of statistical consistency for the proposed methods under welldefined assumptions. 3. Extensive Empirical Evaluation: The authors present a thorough empirical evaluation of their methods on both simulated and real datasets showing clear improvements over existing clustering and twosample testing methods. 4. Clarity and Organization: The paper is wellorganized methodically walking the reader through the motivation methodology theoretical findings and empirical validations which aids comprehensibility.  Weak Points 1. Assumption Specificity: While the theoretically grounded assumptions (such as Lipschitz continuity) are a strength they also limit the general applicability of the proposed methods. The realworld relevance of these assumptions could be questioned as many practical networks may not adhere to them. 2. Algorithm Complexity: The sortingandsmoothing strategy for distance calculation could be computationally expensive particularly for large networks. Though the authors mention scalability a more detailed discussion on computational complexity and efficiency would be beneficial. 3. Parameters Selection: The process for selecting key parameters (such as n0 and \u03c3i) requires further clarification. While theoretical bounds are provided practical guidelines or adaptive methods for setting these in realworld scenarios would enhance the papers applicability. 4. Comparative Analysis: Although the paper includes a comprehensive performance comparison a deeper analysis on why certain algorithms underperform or the conditions under which the proposed methods might not yield the best results would provide a more balanced view.  Questions and Feedback to the Author 1. Could you elaborate on how the assumptions made (Lipschitz continuity etc.) align with characteristics of realworld networks Examples or justification would strengthen the grounding of these assumptions. 2. It would be valuable to discuss the potential computational bottlenecks in greater detail. How does the sortingandsmoothing approach scale with network size and are there strategies to mitigate potential inefficiencies 3. On parameter selection can the authors suggest practical approaches or automatic methods for selecting n0 and \u03c3i that would make implementation more userfriendly for practitioners 4. A deeper comparative analysis with underperforming methods might reveal insights about the limitations and scope of the proposed methodology. Can the authors provide additional discussion in this direction  Review Summary This paper makes a significant contribution to the field of network analysis by proposing a novel graph distance metric based on graphon theory for clustering networks without vertex correspondence and applying this distance for graph twosample testing. The theoretical rigor comprehensive empirical evaluations and methodological innovations constitute strong points. Nonetheless the paper would benefit from addressing certain limitations such as the specificity of the assumptions made potential computational bottlenecks parameters selection and a more nuanced comparative analysis. Overall the strengths of the paper far outweigh its limitations and with some revisions particularly in addressing the questions raised this work is wellpositioned to make a substantial impact in its field.", "jKzjSZYsrGP": "Summarize the paper: The paper introduces SCformer a novel Transformerbased architecture for longterm time series forecasting. Recognizing the limitations of existing methods in handling the complexity of long sequence dependencies due to their pointwise attention mechanisms the authors propose a segment correlation attention mechanism (SCAttention) that focuses on segmentwise relationships thereby reducing computational complexity and enhancing efficiency. Additionally a dual task setup is introduced to improve the models stability by ensuring the forwardpredicted series can be used to restore the past series accurately. Extensive experiments across different datasets demonstrate SCformers superiority over existing Transformerbased methods validating the effectiveness of segment correlation attention and the dualtask regularization strategy. Strong and Weak Points Questions and Feedback: Strong Points: 1. Innovation: The segment correlation attention mechanism is a novel approach that efficiently captures longterm dependencies by focusing on segmentwise relationships addressing the computational challenges present in traditional Transformer models. 2. Dual Task Regularization: The introduction of a dual task that leverages the reversibility of time series predictions enhances the models robustness and stability a creative strategy not commonly seen in time series forecasting models. 3. Extensive Evaluation: The paper presents comprehensive experiments across various datasets demonstrating the models effectiveness in both univariate and multivariate time series forecasting. This thorough evaluation strengthens the papers claims about SCformers capabilities. Weak Points: 1. Generalizability Concerns: While SCformer shows impressive performance on the tested datasets the paper does not discuss its applicability to time series data with less pronounced periodicity or datasets where the underlying patterns are not clearly segmented. 2. Hyperparameter Sensitivity: The segment length (Lseg) is a critical hyperparameter in the proposed SCattention mechanism. The paper could benefit from a more detailed discussion on how to choose this parameter optimally and its sensitivity across different datasets. 3. Comparison with NonTransformer Models: Although the paper positions SCformer against other Transformerbased models it would be insightful to see a comparison with stateoftheart models from different architectures such as advanced RNN or hybrid models to contextualize its performance further. Questions: 1. How does SCformer perform on datasets with irregular or nonexistent periodicity Is there a fallback mechanism in SCAttention for such scenarios 2. Could you elaborate on the process and considerations for selecting the segment length (Lseg) in SCAttention Are there any automated or datadriven methods to determine this parameter 3. Is the dual task regularization compatible with and beneficial to other sequence modeling tasks beyond time series forecasting such as text or speech processing Feedback: 1. Expanding the discussion on the models generalizability and providing guidelines or automated methods for hyperparameter selection (especially Lseg) could significantly enhance the papers impact. 2. A broader comparison including nonTransformerbased stateoftheart models would provide a more complete picture of SCformers standing in the field of time series forecasting. 3. Clarity on the limitations and potential future directions of the research would be appreciated offering insights into how the proposed methods could evolve or be applied to other domains. Review Summary: The paper presents a compelling and innovative approach to longterm time series forecasting addressing key limitations in existing Transformerbased models through the introduction of segment correlation attention and a novel dual task regularization strategy. The experimental results are strong demonstrating the models effectiveness across various datasets and forecasting scenarios. However further discussion on the models generalizability hyperparameter sensitivity and broader comparisons with nonTransformerbased models could strengthen the paper. The introduction of SCAttention is a significant contribution to the field potentially influencing future research on efficient and effective time series analysis. I recommend this paper for publication contingent upon the authors addressing the mentioned concerns particularly around the models applicability to a wider range of datasets and its comparison with the broader landscape of time series forecasting methods.", "vgqS1vkkCbE": "Review Draft for \"Value Function Spaces as State Abstractions in Hierarchical Reinforcement Learning\" 1. Summary: This paper introduces Value Function Spaces (VFS) a novel approach toward state abstraction in hierarchical reinforcement learning (HRL) by leveraging the value functions corresponding to available lowerlevel skills of an agent. The core idea is to encode states into a representation that captures actionrelevant information while ignoring extraneous details thus facilitating efficient higherlevel planning and control in longhorizon tasks. The authors argue that since value functions inherently capture the affordances of skills stacking them forms a rich yet compact representation the VFS which simplifies the planning problem for highlevel policies. VFS is evaluated across mazesolving and robotic manipulation tasks demonstrating superior longhorizon performance and zeroshot generalization when compared to existing modelfree and modelbased methods. The paper is organized into sections that introduce the motivation related work the proposed approach empirical evaluations and discussions on the implications and future work possibilities. 2. Strong and Weak Points Questions and Feedback: Strong Points:  The concept of using value functions to abstract state representations in HRL is innovative and wellmotivated. It leverages intrinsic properties of value functions to implicitly capture taskrelevant features thus promising better scalability and generalization across varying environments.  Empirical evaluations are thorough with the method being tested on both mazesolving tasks and robotic manipulation tasks against a wide array of baselines. This demonstrates the versatility and robustness of VFS across different domains and task complexities.  The paper is wellstructured and clearly written making it accessible to readers with a basic understanding of reinforcement learning and hierarchical structures within it. Weak Points:  While the paper provides substantial empirical evidence for the efficacy of VFS theres limited discussion on the potential limitations or challenges in scaling the approach to even more complex tasks or environments that might not conform well to the assumptions made about skill value functions.  The discussion on related work while comprehensive could benefit from a deeper comparative analysis on how previous approaches fall short on the specific problems that VFS addresses providing a stronger motivation for the need of a new state abstraction method. Questions and Feedback to the Author: 1. How does the performance of VFS compare in environments where skill discovery is also part of the learning process Will the addition of new skills require retraining of the highlevel policy from scratch 2. The paper assumes a prespecified set of skills. How sensitive is the performance of VFS to the quality and granularity of these skills Might poor skill design significantly undermine the benefits of VFS 3. There is mention of the potential for VFS to work in unsupervised skill discovery methods. Could the authors elaborate on preliminary ideas or future directions on integrating VFS with unsupervised approaches 4. Could the authors discuss any computational considerations or tradeoffs when implementing VFS For instance does the dimensionality of the VFS representation significantly increase the computational complexity of the highlevel policy learning 3. Review Summary: The paper \"Value Function Spaces as State Abstractions in Hierarchical Reinforcement Learning\" presents a compelling new method for state abstraction by utilizing the value functions of available lowerlevel skills to construct a representation known as Value Function Spaces (VFS). This approach cleverly leverages the intrinsic capabilities and affordances captured by value functions to simplify the highlevel planning problem in HRL promising scalability and effective generalization. Empirical evaluations demonstrate the methods superior performance in mazesolving and robotic manipulation tasks highlighting its robustness and adaptability across different domains. Nonetheless further exploration into the limitations computational considerations and potential integration with unsupervised skill discovery methods are areas suggested for future work. In conclusion this paper contributes significantly to the field of hierarchical reinforcement learning by introducing an innovative and practical approach for state abstraction. Its potential for aiding in complex longhorizon task solving and generalization to novel environments makes it a valuable asset for further research and application.", "zBOI9LFpESK": " Review of \"Adaptive Metalearner of Behavioral Similarities (AMBS) for Reinforcement Learning from Highlevel Visual Observations\"  Summary The paper introduces a novel framework called Adaptive Metalearner of Behavioral Similarities (AMBS) aimed at addressing the challenge of learning effective reinforcement learning (RL) models for control tasks based on highdimensional visual observations. The core innovation lies in learning lowdimensional state representations that encapsulate reward and dynamics information using two distinct metalearners. These metalearners are designed to quantify and incorporate behavioral similarities (in terms of reward and dynamics) between state representations into the RL process. The approach also includes a mechanism for adaptively balancing the influence of reward and dynamics based on the specific task or environment. The efficacy of AMBS is demonstrated through experiments on several benchmarks achieving new stateoftheart results.  Strong Points 1. Novelty: The introduction of metalearners to quantify behavioral similarities in state representations is a significant and novel contribution to the field of representation learning in RL. This approach addresses a critical gap in existing methods which often overlook the nuances of reward and dynamic similarities. 2. Adaptive Balancing Mechanism: The adaptive strategy for balancing the impact of rewards and dynamics similarities is exceptionally innovative. This methods capability to adjust based on task requirements could greatly enhance model adaptability and performance across diverse environments. 3. Comprehensive Evaluation: The paper presents an extensive set of experiments across varied benchmarks including DM Control Suite with distractions demonstrating the robustness and effectiveness of the proposed framework. The comparison with stateoftheart baselines and diverse testing environments strengthens the claim of the paper.  Weak Points 1. Computational Complexity: While the proposed framework shows impressive results there is a lack of discussion regarding the computational complexity introduced by the dual metalearners and the adaptive mechanism. Insights into the computational overhead would provide a more balanced view of the approachs feasibility for realworld applications. 2. Generalization to Nonvisual Tasks: The focus is predominantly on highlevel visual observations. Though highly relevant this focus leaves a gap in understanding how the AMBS framework might adapt or perform in nonvisual highdimensional observation spaces or abstract tasks without direct visual inputs. 3. Ablation Studies: More comprehensive ablation studies exploring the individual contributions of the reward and dynamics metalearners as well as the adaptive balancing mechanism could further solidify the understanding of how each component influences overall performance.  Questions and Feedback 1. Complexity and Scalability: Can the authors provide insights or quantitative analysis on the computational complexity of AMBS especially in comparison to the baseline methods Understanding the tradeoffs between performance gains and computational requirements is crucial for evaluating the frameworks applicability. 2. Extension to Nonvisual Tasks: Is there potential or planned research to extend the AMBS framework to tasks with nonvisual or more abstract state representations Insights into this possibility could broaden the impact of the research. 3. Impact of Data Augmentation: The use of data augmentation techniques has been briefly mentioned. Could the authors elaborate on how critical this component is for the robustness of the learned representations particularly under varying environmental conditions  Review Summary The paper presents a novel and compelling approach to learning state representations for reinforcement learning based on visual observations. The introduction of metalearners for quantifying behavioral similarities in state representations is both innovative and potentially impactful for the field. The adaptive balancing of reward and dynamics similarities is another notable advancement potentially offering a more nuanced approach to handling varied tasks and environments. Despite these strengths several areas could benefit from further exploration or clarity such as the computational implications of the proposed method its applicability beyond visual tasks and a deeper investigation into the role of data augmentation. Addressing these points could elevate the work even further. In conclusion the paper significantly advances the stateoftheart in reinforcement learning by introducing a welldesigned framework for robust state representation learning. The comprehensive experimental evaluation convincingly supports the frameworks effectiveness. Nevertheless addressing the identified gaps and questions would provide a more complete understanding of the frameworks scalability adaptability and broader applicability.", "fwzUgo0FM9v": " Review of \"Breaking Privacy in Federated Learning with Minimal Model Modifications\"  1. Summary The paper presents a significant advancement in understanding privacy vulnerabilities in Federated Learning (FL) systems. By introducing a novel attack method the authors demonstrate that it is feasible for a malicious server to retrieve exact user data from gradient updates  even when aggregated over large batches which were previously considered secure against such invasions of privacy. The attack leverages minimal but malicious modifications to the shared model architecture specifically via an introduced mechanism known as the \"imprint module.\" This module intelligently manipulates gradients such that exact user data can be retrieved without the necessity of solving complex inverse problems. The paper extensively discusses the formulation of this attack its implementation details and validates its effectiveness through experiments with image data showing that the proposed method surpasses existing strategies in terms of efficiency and the scale of data it can exploit.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Impact: The identified vulnerability presents a significant threat to the privacy assurances of federated learning systems. By addressing a gap in the current understanding of FL security this work potentially triggers further research into more robust privacypreserving mechanisms.  Technical Rigor: The authors provide a rigorous mathematical foundation for their attack along with a detailed explanation of the threat model attack implementation and a proof for the proposed methods effectiveness.  Comprehensive Evaluation: The empirical evaluation using image data from ImageNet along with PSNR and IIP scores showcases the practical implications of the attack in realworld scenarios. Weak Points:  Generalization across Data Types: Although the authors claim the attack is dataagnostic the experiments primarily focus on image data. The effectiveness of the attack on other data types such as text or tabular data is less clear and warrants further investigation.  Defenses and Mitigations: The paper touches on potential defense mechanisms against the presented attack but does not delve deeply into their effectiveness or implementation. This leaves an open question about how FL systems can protect against such sophisticated attacks.  Complexity and Feasibility: The attacks reliance on specific model modifications and the need for a malicious server could limit its applicability in practice. More discussion around the practical challenges of implementing such an attack in real federated learning setups would be beneficial. Questions and Feedback: 1. Can the authors provide insights into the attacks effectiveness on nonimage data types such as text or tabular data This would significantly enhance the understanding of the attacks scope and limitations. 2. The mitigation strategies mentioned (e.g. local differential privacy) are briefly discussed. Could the authors elaborate on potential defense mechanisms that could specifically counteract the proposed attack method 3. Implementing the imprint module requires a malicious server with the ability to modify model architectures. How likely is this scenario considering the security measures in place in typical federated learning systems  3. Review Summary and Recommendation This paper represents an important contribution to the field of federated learning and its associated privacy concerns. The novel attack method challenges the assumed privacy guarantees of federated learning especially in scenarios where user data is aggregated over large batches. By demonstrating the feasibility and effectiveness of this attack the authors highlight a critical vulnerability that needs addressing. However to solidify the papers contributions further exploration into the generalizability of the attack across different data types and comprehensive discussions on feasible mitigation strategies are necessary. Additionally examining the practical implementation challenges and requirements for such an attack would provide more clarity on its realworld implications. Given the significance of the identified vulnerability and the papers contribution to ongoing discussions about privacy in federated learning I recommend this paper for acceptance provided the authors address the mentioned concerns particularly around the attacks applicability and defense mechanisms. This work undeniably adds valuable insights and prompts a timely reevaluation of privacypreserving techniques in federated learning systems.", "lkQ7meEa-qv": " Summarize the Paper The paper introduces Neural Acoustic Fields (NAFs) a novel method for implicitly capturing and representing the acoustic properties of a scene in a highfidelity continuous manner. Leveraging learned implicit functions NAFs model the impulse responses between any emitterlistener pair within a scene allowing for accurate recreation of the scenes reverberations and sound propagation characteristics. This approach extends existing work by utilizing a neural network to predict impulse responses in the Fourier frequency domain conditioned on the local geometry around emitter and listener locations. The models efficacy is demonstrated through improved performance over baseline methods in tasks like crossmodal generation of novel views and sound source localization. The NAF framework is validated on the Soundspaces dataset across several experiments showcasing its ability to outperform traditional interpolation techniques and its utility in audiovisual learning contexts.  Strengths and Weak Points  Strengths 1. Innovativeness: The paper proposes a novel method of representing the acoustics of environments in a continuous differentiable form which is a significant advancement over previous discrete or manually designed models. 2. CrossModal Learning: Demonstrating the utility of NAFs in improving visual representations with sparse visual data by leveraging the learned acoustic structure is a compelling application showcasing the methods versatility. 3. Sound Source Localization: The paper not only focuses on accurately modeling acoustics but also applies NAFs to practical tasks like sound source localization making it relevant for realworld applications.  Weak Points 1. Complexity and Computation: The method seems to require substantial computational resources and training time which could limit its practicality for larger or more complex scenes. 2. Lack of Phase Information: The paper acknowledges the use of phasefree models for spatial acoustic modeling which may disregard certain nuances of sound perception and localization in humans potentially affecting the realism of the synthesized sounds. 3. Evaluation Metrics: While the paper presents quantitative results there is a lack of subjective evaluation or user studies that might provide insight into the perceived quality of the generated acoustic fields.  Questions and Feedback 1. Scalability: How well does the NAF model scale to larger environments or more complex scenes Are there strategies to mitigate the computational demands 2. RealWorld Application: How do the authors envision the application of NAFs in realworld scenarios Are there certain domains or industries where NAFs could be particularly impactful 3. Phase Information Handling: Could the incorporation of phase information improve the realism and utility of NAFs especially for sound source localization or other applications where spatial cues are crucial  Review Summary This paper presents a significant advancement in the modeling of acoustic environments through Neural Acoustic Fields (NAFs). It successfully addresses a gap in the current literature by providing a continuous differentiable representation of scene acoustics that outperforms existing methods. The application of NAFs in crossmodal learning and sound source localization further demonstrates the methods practicality and versatility. However the computational complexity omission of phase information and lack of subjective evaluation present areas for potential improvement. Despite these points the innovative approach and robust validation make a compelling case for the utility and impact of NAFs in both the research community and potential realworld applications. The paper is wellwritten thoroughly describes the methodological framework and provides a strong theoretical and empirical foundation for NAFs. I recommend publication accompanied by further research into addressing the identified weak points.", "iPHLcmtietq": " Review of \"Eliminating Spatial Variability with Phase Collapses for Improved Linear Separability in Image Classification\"  Summary of the Paper This paper explores the significance of phase collapses in deep convolutional networks (DCNs) for image classification highlighting how these collapses improve linear separability of image classes. The authors contrast the conventional interpretation of ReLU nonlinearities and biases as tools for enforcing sparsity with their insight that phase collapses\u2014particularly the elimination of complex wavelet coefficient phases\u2014are central to enhancing classification accuracy. Through the introduction of a Learned Scattering network that leverages phase collapses without the need for biases or thresholding nonlinearities they provide evidence that these collapses alone can achieve ResNetlevel accuracy on standard image datasets like ImageNet and CIFAR10. Importantly the paper not only demonstrates the practical benefits of phase collapses but also delves into the mathematical framework underpinning their efficacy in linear class separation offering a theoretical analysis to support empirical findings.  Strong Points 1. Novelty and Insight: The paper introduces an intriguing perspective on the role of phase collapses in DCNs challenging existing paradigms that prioritize sparsity and thresholding nonlinearities. This fresh viewpoint could stimulate further research in the area. 2. Theoretical and Empirical Support: It commendably provides both theoretical explanations and empirical evidence to validate the claims regarding phase collapses. This dual approach strengthens the argument and increases the papers credibility. 3. Practical Contributions: By achieving ResNetlevel accuracies with the proposed Learned Scattering network the paper shows the practical viability of phase collapses in realworld scenarios underscoring the potential applicability of the research in image classification tasks. 4. Reproducibility: The commitment to reproducibility through the provision of code and detailed experimental setups in supplementary materials is exemplary and facilitates further exploration by the research community.  Weak Points and Questions 1. Comparison with Conventional Systems: While the paper illustrates the superiority of phase collapses over sparsityinduced mechanisms it could benefit from deeper comparative analyses against a broader range of existing deep learning models to contextualize its performance gains more effectively. 2. Generalization Across Datasets: The papers experiments focus on two wellknown datasets (ImageNet and CIFAR10). Expanding the empirical evaluation to include more diverse or challenging datasets could help in assessing the robustness and adaptability of the proposed approach. 3. Underlying Assumptions: The theoretical framework primarily addresses linear class separation through phase collapses. However it might be beneficial to discuss any assumptions or conditions under which these findings may not hold providing a more nuanced understanding. 4. Complexity and Implementation Concerns: While achieving high accuracy the paper does not extensively address the computational costs or potential implementation challenges associated with the Learned Scattering network especially in comparison to simpler or more conventional architectures.  Questions and Feedback to the Author 1. How does the performance of the Learned Scattering network scale with increasing dataset complexity or size 2. Could the authors elaborate on any potential limitations or drawbacks of relying on phase collapses as the primary mechanism for class separation 3. Are there specific types of image classification tasks or domains where phase collapses might be less effective  Review Summary The paper makes significant contributions to our understanding of deep convolutional networks functioning by spotlighting the role of phase collapses in improving linear class separability. This innovative lens not only challenges prevailing beliefs centered around sparsity and thresholding but also paves the way for novel approaches in image classification. Supported by both theoretical insight and empirical evidence the research presents a compelling case for the potential of phase collapses as exemplified by the Learned Scattering network. Nevertheless the paper could be further enhanced by addressing questions related to broader applicability limitations and comparative analysis with a wider range of models. Overall this work stands out for its originality rigorous analysis and practical contributions making a notable impact on the field. In light of the strong points and considering the areas for improvement my recommendation is to accept the paper encourage the authors to provide further clarifications on the discussed questions and broaden the comparative analyses and evaluations to bolster their claims. The contributions of this work are substantial opening new avenues for research and application in deep learning and image classification.", "tT9t_ZctZRL": " Summarize the paper This paper addresses a key challenge in deep Graph Convolutional Networks (GCNs)  the oversmoothing problem which leads to indistinguishable node representations as the network depth increases. The focus is on the concept of trainability from an optimization perspective leveraging the Graph Neural Tangent Kernel (GNTK) to characterize how the trainability of ultrawide GCNs deteriorates exponentially with depth during the optimization process. The authors also evaluate the effectiveness of residual connectionbased techniques in mitigating this trainability decay finding them only partially successful. To more fundamentally address this issue they introduce a new method Critical DropEdge a connectivityaware and graphadaptive sampling technique. Their theoretical insights and experimental results demonstrate that Critical DropEdge can significantly improve the trainability and performance of deep GCNs on node classification tasks compared to other stateoftheart methods.  Strong Points 1. Theoretical Depth and Originality: This work provides a novel theoretical examination of the trainability of deep GCNs by analysing the asymptotic behaviors of GNTK. The introduction of Critical DropEdge based on these theoretical insights is original and contributes significantly to the field. 2. Practical Relevance: The oversmoothing problem is a significant barrier to utilizing GCNs to their full potential. Addressing this with a theoretically grounded solution that can be applied to both infinitely wide and finite GCNs is highly relevant for practitioners. 3. Comprehensive Evaluation: The experimental evaluation is thorough comparing the proposed method against relevant counterparts across several realworld datasets which highlights the practical effectiveness of the proposed techniques.  Weak Points 1. Assumption Specificity: The theoretical analysis relies heavily on the assumption of using infinitelywide networks. While the authors extend their methodology to finitelywide networks empirically the theoretical underpinning for how well these findings translate to more commonly used finitewidth networks could be more robust. 2. Complexity and Accessibility: The heavy reliance on advanced mathematical frameworks while necessary for the depth of analysis might limit the accessibility of the work to a broader audience or practitioners without a strong mathematical background. 3. Potential Overfitting Concerns: While the proposed Critical DropEdge method shows promising results there is limited discussion on the potential for overfitting especially in scenarios with smaller graphs or less complex graph structures.  Questions and Feedback to the Author 1. Extending Theoretical Insights to Finitewidth Networks: Could the authors comment on how they anticipate their theoretical insights on trainability and the effectiveness of Critical DropEdge to scale with smaller more practical network widths 2. Assessing Generalizability: Have there been any studies to evaluate how well the proposed Critical DropEdge method generalizes across varied graph structures beyond the datasets tested (e.g. highly sparse vs. dense graphs) 3. Addressing Potential Overfitting: Could the authors elaborate on any mechanisms or additional modifications to Critical DropEdge that might help prevent overfitting especially in smaller or less complex graph environments  Review Summary The paper presents a compelling theoretical and empirical investigation into the trainability issues of deep Graph Convolutional Networks focusing on the oversmoothing problem that plagues these models. By leveraging the Graph Neural Tangent Kernel the authors not only deepen our understanding of why trainability deteriorates in deep GCNs but also propose a novel mitigation strategy Critical DropEdge that demonstrates considerable performance improvements. Despite some limitations such as a heavy focus on infinitelywide networks and potential overfitting concerns the contribution is of high significance to the field. Theoretical robustness combined with practical effectiveness shown through extensive experiments makes a strong case for the value of this work. Future research building on these findings could explore more varied graph structures and address the scalability of insights to finitelywide networks further enhancing the utility and impact of deep GCN models in practical applications.", "lKcq2fe-HB": " Review of \"Improving SelfPaced Reinforcement Learning with Optimal Transport\"  Summary The paper explores the limitations of selfpaced reinforcement learning (SPRL) concerning its reliance on Gaussian task distributions. It identifies that while the Gaussian approximation has facilitated SPRL\u2019s empirical success it simultaneously restricts the model\u2019s flexibility making it unsuitable for nonGaussian task distributions. To address this the authors propose the incorporation of optimal transport theory specifically through Wasserstein metrics into SPRL. This modification allows for a nonparametric version of SPRL that not only accommodates a broader range of task distributions but also ensures stable learning performance across these varieties. The paper evaluates this enhanced model using multiple curriculum reinforcement learning (CRL) frameworks across different learning environments thereby demonstrating its superiority over conventional Gaussianbased SPRL and highlighting its practical implications in curriculum RL.  Strengths and Weaknesses Strengths: 1. Theoretical Innovation: The paper presents a novel integration of optimal transport theory into SPRL addressing a significant limitation of previous models. This approach is both innovative and theoretically sound offering a new direction for future research in curriculum RL. 2. Comprehensive Evaluation: The experimental section provides a detailed comparison between the proposed model (WBSPRL) and both parametric (GSPRL) and nonparametric (NPSPRL) versions of SPRL. The inclusion of various learning environments and a comparison with other CRL methods showcase the practical benefits and the general applicability of the proposed approach. 3. Significant Empirical Results: The empirical evidence demonstrates that WBSPRL outperforms existing SPRL models especially in complex and nonGaussian task distributions. These results strongly support the papers claims and highlight the potential of Wasserstein metrics in enhancing learning performance. Weaknesses: 1. Scalability Concerns: While the paper provides an insightful theoretical framework and promising empirical results it does not sufficiently address scalability issues. Given the computational complexity associated with optimal transport particularly with Wasserstein barycenters the proposed models applicability to highdimensional contexts remains questionable. 2. Limited Discussion on Parameter Sensitivity: The experiments lack a detailed investigation into the sensitivity of the proposed model to its hyperparameters (e.g. the weights \u03b2 and performance threshold \u03b4). Such an analysis could further validate the robustness of the model across various settings. 3. Comparison with Very Recent CRL Methods: Although the paper compares the proposed WBSPRL with several existing CRL methods it misses out on including the latest advances in the field. A comparison with more recent methods could further establish the proposed model\u2019s position within the current stateoftheart.  Questions and Feedback to the Author 1. Scalability: How does the computational complexity of WBSPRL compare with traditional SPRL models especially in highdimensional task spaces Could you propose any potential solutions to mitigate scalability issues 2. Hyperparameter Sensitivity: Can the authors provide more insights into how different settings of the model\u2019s hyperparameters affect its performance A sensitivity analysis would be valuable for understanding the models robustness. 3. Comparison with Recent Methods: Could the authors comment on how WBSPRL might compare with the latest CRL methods not included in the paper Are there plans to evaluate WBSPRL against these methods in future work  Review Summary The paper introduces an important advance in curriculum reinforcement learning by embedding optimal transport theory into SPRL thus overcoming the limitation of relying on Gaussian task distributions. This approach not only broadens the applicability of SPRL to a wider range of tasks but also demonstrates superior empirical performance across various learning environments. Despite the noted weaknesses such as potential scalability issues and the need for a more comprehensive hyperparameter sensitivity analysis the contributions of this paper are significant. The proposed model (WBSPRL) represents a substantial improvement over existing SPRL frameworks and opens up new avenues for research in curriculum RL. Therefore I recommend the acceptance of this paper provided that the authors consider addressing the scalability concerns and enhancing their discussion on model robustness related to hyperparameters.", "xZ6H7wydGl": " Review of \"Efficient Estimation of Probabilities in SDE Models for HighDimensional Learning\"  Summary The paper addresses the challenge of learning stochastic differential equations (SDEs) from data a process that has traditionally been computationally expensive due to the sequential nature of SDE integrators. The authors propose an importancesampling estimator for probabilities of observations of SDEs that does not rely on sequential SDE integrators. This novel approach leads to lower variance in gradient estimates and enables embarrassingly parallel computation thus significantly reducing computation time. The methodology involves generating sample paths directly passing through observations or appropriately marginalizing out the observation noise making it particularly suitable for sparsely observed data in high dimensions. The paper also discusses strategies for dealing with statedependent diffusion and provides comprehensive experiments to validate the efficacy of the proposed method.  Strong Points 1. Innovation: The proposed method innovatively bypasses the need for SDE integrators overcoming a major bottleneck in the learning process. This marks a significant advancement in the field. 2. Scalability: By enabling parallel computation the authors offer a scalable solution to learning SDEs in high dimensions a task previously deemed challenging. 3. Robust Experimental Validation: The experiments are welldesigned and effectively demonstrate the advantages of the proposed method over traditional integratorbased approaches in terms of computation time and accuracy. 4. Detailed Theoretical Background: The presentation of related work alongside the detailed enlightenment of the challenges faced by traditional methods provides a solid foundation for understanding the significance of their contribution. 5. Reproducibility and Open Science: Providing code the Dockerfile and detailed algorithmic descriptions embodies best practices in scientific communication facilitating reproducibility and further innovation.  Weak Points 1. Complexity and Accessibility: While the paper is rich in detail its high technical complexity might pose accessibility issues for readers not deeply versed in SDEs or probability theory. 2. Generalization of Findings: While the proposed method shows promise in the experiments conducted the generalizability of these findings across a wider range of SDEs and application domains remains to be explored. 3. Comparisons With State of the Art: The paper could benefit from a more detailed comparison with stateoftheart methods particularly in terms of quantitative performance metrics across a broader range of datasets and SDEs. 4. Impact on Interpretability: The paper does not delve into how the proposed method affects the interpretability of learned SDE models which is critical for many scientific applications.  Questions and Feedback to the Author 1. Extensibility: Can the proposed method be extended or adapted to other types of differential equations such as partial differential equations (PDEs) 2. Comparison With Other Domains: How does the proposed method perform on SDEs from various domains (e.g. biological processes climate modelling) A broader analysis could strengthen the papers impact. 3. Implementation Details: Could the authors elaborate on the choice of neural network architectures used in the experiments and whether different architectures might affect the performance of their method  Review Summary This paper introduces a groundbreaking approach to efficiently estimate probabilities in stochastic differential equation models especially pertinent to highdimensional data. The novel use of importance sampling independent of traditional SDE integrators offers a paradigm shift that could significantly accelerate research in fields reliant on SDE modeling. Experimentation validates the proposed methods efficiency and accuracy advantages promising substantial computational savings and enhanced scalability. However the papers technical density may limit its accessibility and broader comparisons with existing methods could further elucidate its advantages and potential limitations. Questions about extensibility applicability across different domains and the impact of various neural network architectures on performance are areas that if addressed could enhance the papers contribution.  Recommendation I recommend this paper for publication provided the authors address the concerns regarding broader comparisons with existing methods clarify questions on extensibility and domain applicability and explore the methods impact on interpretability within SDE models. The innovative nature of the work and its potential impact on a wide range of applications make it a valuable contribution to the field.", "gdWQMQVJST": " Peer Review of \"A Neural Tangent Kernel Empowered Federated Learning Paradigm\" 1. Summary This paper introduces a novel federated learning (FL) paradigm leveraging the neural tangent kernel (NTK) framework to address the challenge of statistical heterogeneity inherent in FL environments. Unlike traditional FL methods such as Federated Averaging (FedAvg) which relies on model weight or gradient sharing the proposed approach NTKFL utilizes samplewise Jacobian matrices for updates. This method allows for more expressive data sharing among participants without the explicit use of gradient descent promising faster convergence and enhanced privacy. The authors also propose a variant CPNTKFL which further improves communication efficiency and privacy by incorporating random projection and data subsampling strategies. The effectiveness of NTKFL and CPNTKFL is demonstrated through experiments showing superior performance in terms of accuracy and communication rounds reduction when compared to FedAvg. 2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The paper presents a novel use of the NTK framework in the context of FL replacing traditional gradient descent methods with a kernelbased approach that inherently tackles the nonIID data problem.  Improvement in Efficiency and Privacy: The proposed method significantly reduces the number of communication rounds needed and enhances privacy without a severe compromise on accuracy.  Theoretical and Empirical Validation: The authors provide a thorough theoretical analysis of the proposed method complemented by empirical results that demonstrate its effectiveness over FedAvg and its resilience to various degrees of data heterogeneity. Weak Points:  Complexity and Scalability Concerns: The shift of computation from clients to the server especially for constructing and evolving the global kernel might introduce scalability issues for very large networks. More discussions on computational overheads and scalability tests would be valuable.  Limited Architecture Exploration: The experiments primarily focus on multilayer perceptrons (MLPs). Assessing how the NTKFL paradigm performs with more complex architectures like CNNs and RNNs would provide a more comprehensive understanding of its applicability.  Privacy Guarantees Ambiguity: While improved privacy is claimed as a benefit the paper lacks a rigorous privacy analysis especially concerning differential privacy guarantees or potential vulnerabilities introduced by sharing Jacobian matrices. Questions and Feedback:  Can the authors elaborate on potential scalability limitations of their method specifically regarding the servers computational and storage requirements  How does the performance of NTKFL compare across different neural network architectures beyond MLPs  Could the authors provide more detailed privacy analyses or discuss plans to address potential vulnerabilities associated with the Jacobian matrices sharing  It would be beneficial to see a comparison or integration of this method with existing privacyenhancing techniques like secure multiparty computation or homomorphic encryption to evaluate compatibility and performance impacts. 3. Review Summary The paper introduces an innovative FL paradigm that effectively utilizes the NTK framework to address challenges associated with statistical heterogeneity and efficiency in federated settings. By replacing gradient descent with a kernelbased method the authors demonstrate promising results in terms of convergence speed communication efficiency and privacy preservation. However concerns about computational scalability applicability to various neural network architectures and detailed privacy analysis present avenues for further exploration. The methods theoretical foundation supplemented by strong empirical results underlines its potential to significantly impact the FL field. Nevertheless the concerns and suggestions highlighted above should be addressed to ensure the methods broader applicability and deeper understanding of its implications on privacy and scalability. Supporting Arguments for Recommendation: The recommendation to consider this work for publication is based on its novelty the significant improvement it introduces to FL and its thorough theoretical and empirical validation. Addressing the raised concerns and questions will only strengthen the papers contribution to the FL literature offering a novel approach that could inspire future research directions in efficient privacypreserving federated learning methods.", "y8zhHLm7FsP": " Review of \"Learning Optimal Control Law for LQG Problems Using Ensemble Kalman Filter\"  Summary The paper introduces a simulationbased algorithm utilizing the Ensemble Kalman Filter (EnKF) for learning the optimal control law in Linear Quadratic Gaussian (LQG) problems avoiding the explicit solution of the Riccati Differential Equation (DRE). The approach takes inspiration from data assimilation processes in highdimensional simulationbased models such as those found in weather prediction to address challenges in solving highdimensional DREs and when model parameters are not explicitly known. By employing a novel representation of the optimal control law through the variance of EnKF particles and designing interactions between simulations the paper claims to achieve a learning rate that approximates the exponential convergence rate of DRE solutions. This is done for both fully and partially observed settings of the LQG problem and numerical simulations are presented to demonstrate the efficacy of the proposed algorithm compared to stateoftheart approaches.  Strong Points 1. Novelty of Approach: The papers method leverages data assimilation techniques particularly the EnKF in a novel way to circumvent solving DREs directly for LQG problems. This creative bridging of concepts from different areas (data assimilation and control theory) is commendable. 2. Comprehensive Coverage: The paper doesnt restrict itself to the fully observed scenario typically considered in reinforcement learning (RL) but extends its algorithm to address the partially observed LQG problem as well enhancing the applicability of their method. 3. Quantitative Analysis: The introduction of a simulationbased algorithm is backed by rigorous theoretical proofs (Propositions 1 and 2) and numerical simulations that not only demonstrate the algorithms accuracy but also compare its computational efficiency against other stateoftheart methods.  Weak Points 1. Assumptions on System Models: The paper assumes access to a simulator that can implement additional terms outside the control channel. This assumption may not always hold especially in purely experimental setups potentially limiting the applicability. 2. Complexity and Scalability: While the authors demonstrate the algorithms efficacy through numerical simulations theres limited discussion on the computational complexity and scalability particularly in extremely highdimensional settings typical of realworld applications. 3. Comparison with StateoftheArt: The comparisons provided are valuable yet it would be beneficial to include a broader spectrum of existing algorithms especially those with proven efficacy in highdimensional LQG problems for a more comprehensive evaluation.  Questions and Feedback 1. Assumption Justification: How critical is the assumption regarding the simulators ability to implement extra control terms and can the proposed method be adapted for cases where such an assumption doesnt hold 2. Handling Nonlinear Systems: While the paper extends the traditional LQG into a partially observed realm quite elegantly is there potential to further adapt the proposed algorithm to nonlinear systems considering the universality of nonlinearity in practical applications  Review Summary The paper presents an innovative approach to solving LQG optimal control problems by applying the Ensemble Kalman Filter in a novel simulationbased algorithm framework. The authors skillfully address the challenges associated with highdimensional DREs and inaccessible model parameters demonstrating both theoretical and practical efficacy through propositions and numerical simulations. Despite its novel approach and promising results the paper could benefit from a deeper investigation into the algorithms scalability and its applicability to a broader range of problems especially in cases where the assumptions made may not hold or in the presence of system nonlinearity. Overall the paper contributes valuably to the field by introducing a method that potentially bridges the gap between data assimilation techniques and reinforcement learning in control theory indicating avenues for future research in making RL algorithms more efficient and widely applicable.", "zuqcmNVK4c2": " Peer Review of \"SelfJoint Learning: A Novel Supervised Learning Paradigm for Robust Deep Models\"  1. Summary of the Paper The paper introduces a novel supervised learning paradigm named \"selfjoint learning\" aimed at addressing the known limitations of conventional supervised learning such as overconfidence in predictions vulnerability to overfitting adversarial attacks and poor performance on outofdistribution (OOD) data. Unlike traditional models that operate under the assumption of independent and identically distributed (i.i.d.) samples selfjoint learning models learn the joint conditional distribution of pair samples thereby capturing sampletosample relationships. The method not only incorporates labeled data but also utilizes auxiliary unlabeled data to improve model performance further. Empirical results on benchmark image datasets demonstrate the effectiveness of selfjoint learning in enhancing classification accuracy robustness against adversarial attacks and OOD detection capability while mitigating overconfidence.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novel Approach: The novel paradigm of using selfjoint learning to capture pairwise conditional independence is a significant departure from traditional i.i.d. assumptions in supervised learning. This approach intuitively aligns more closely with how human cognition operates by using reference points or samples when making decisions.  Comprehensive Evaluation: The paper presents a thorough evaluation of the proposed method across several fronts  classification accuracy robustness to adversarial attacks OOD data detection and mitigation of overconfident predictions. Such a wideranging evaluation strengthens the case for the utility and versatility of the proposed paradigm.  Utilization of Auxiliary Unlabeled Data: The methods ability to incorporate auxiliary unlabeled data effectively for improving model performance is an excellent advantage making it relevant for realworld applications where labeled data might be scarce but unlabeled data abundant. Weak Points:  Lack of Theoretical Underpinning: While the empirical results are promising the paper lacks a deep theoretical analysis of why selfjoint learning leads to better performance. Understanding the theoretical basis could be crucial for further development and trust in this framework.  Computational Efficiency Concerns: Although briefly mentioned the additional computational cost associated with selfjoint learning especially in terms of inference could be a significant drawback for certain applications. A more detailed discussion on this aspect including possible solutions would be beneficial.  Generalization Beyond Image Classification: The paper focuses predominantly on image classification tasks. It remains unclear how well the selfjoint learning paradigm generalizes to other types of data and learning tasks such as sequence prediction reinforcement learning etc. Questions and Feedback to the Author:  Could the authors provide more insight or theoretical justification on why learning the joint distribution of sample pairs leads to improvements in model robustness and performance  In practical applications computational resources are often a limiting factor. Could the authors elucidate on potential strategies to mitigate the increased computational demand of selfjoint models particularly during inference  How generalizable is the selfjoint learning framework beyond image classification tasks Have there been any experiments or considerations regarding its applicability to other domains or types of data  3. Review Summary The paper presents a promising new supervised learning paradigm that significantly deviates from the traditional i.i.d. assumption by learning the joint conditional distribution of sample pairs. This novel approach dubbed selfjoint learning addresses several critical shortcomings of current deep learning models such as overfitting susceptibility to adversarial attacks and poor OOD performance while also reducing overconfidence in predictions. The empirical evaluation of the method across various benchmarks is thorough showcasing substantial improvements over standard learning paradigms. However the paper could be strengthened by providing a deeper theoretical foundation for why the selfjoint learning approach yields the observed benefits. Additionally concerns regarding computational efficiency particularly in inference need to be addressed in greater detail to assess the practical feasibility of the proposed method in resourceconstrained settings. Moreover expanding the discussion around the generalizability of this framework to domains beyond image classification would greatly enhance its relevance and applicability. In summary the introduced selfjoint learning framework represents an innovative step towards developing more robust and reliable machine learning models. With further theoretical insights efficiency optimizations and generalizability studies this work has the potential to significantly influence future research directions in supervised learning.", "oC12z8lkbrU": "Review for \"Generate Annotate and Learn (GAL): Advancing SemiSupervised Learning and Knowledge Distillation\" 1) Summarize the paper: This paper introduces \"Generate Annotate and Learn (GAL)\" a fresh approach designed to enhance semisupervised learning (SSL) and knowledge distillation (KD) by leveraging synthetic unlabeled data generated through large language models (LLMs). Existing methods in SSL and KD largely depend on the availability of taskspecific unlabeled data which can be scarce especially in certain domains like NLP. GAL circumvents this issue by generating indomain unlabeled data using unconditional LLMs. After generating synthetic data stateoftheart classifiers are used to annotate this data with pseudo labels forming an expanded training set for further learning or distillation. The paper substantiates GALs effectiveness via theoretical backing and empirical validations showcasing noticeable improvements in fewshot learning SSL and KD performance across various NLP tasks and some tabular data tasks from the UCI repository. 2) Strong and weak points Questions and Feedback: Strong Points:  Novel Approach: GAL\u2019s novel use of unconditional LLMs to generate synthetic unlabeled data for SSL and KD is refreshing and addresses a significant challenge in the field about the dependency on available unlabeled data.  Theoretical Foundation: The paper provides a solid theoretical linkage between GAL and concepts like empirical and vicinal risk minimization offering a robust explanation for why and how the approach works.  Broad Applicability and Performance Gains: The method not only establishes new stateoftheart results on the 6layer transformer across the GLUE benchmark but also demonstrates versatility by showing improvements on tabular tasks and in fewshot learning scenarios. Weak Points:  Compute Resources: The reliance on large language models and the generation of extensive amounts of synthetic data may entail significant compute resources potentially limiting accessibility or applicability in resourceconstrained environments.  DomainSpecific Language Models: While the paper elaborates on the effectiveness of finetuning LLMs for domain specificity it doesn\u2019t thoroughly address the challenges in constructing highly effective domainspecific generative models particularly in nuanced domains.  Evaluation Depth: While substantial the evaluation mainly focuses on improvements in performance metrics. A deeper analysis into why certain tasks or datasets benefit more from GAL compared to others would provide more nuanced insights. Questions and Feedback to the Author: 1. Can the authors elaborate on how GAL\u2019s framework might adapt to extremely resourceconstrained environments or propose a scaleddown version that preserves some benefits with lower computational demand 2. The paper briefly mentions the possibility of combining GAL with XGBoost for tabular tasks but does not delve into it. Could the authors speculate on potential methodologies or challenges in integrating GAL with nonneural algorithms 3. A comparative analysis with methodologies that use adversarial approaches or more traditional data augmentation techniques in SSL and KD contexts might add valuable perspective. Could the authors comment on how GAL compares or could be combined with these techniques 3) Review Summary: \"Generate Annotate and Learn (GAL)\" presents an innovative approach that creatively utilizes unconditional large language models to synthesize unlabeled indomain data caters to advancing semisupervised learning and facilitates knowledge distillation in NLP and certain tabular tasks. This work stands out for its strong theoretical foundation broad applicability and the significant performance gains it achieves across a range of tasks establishing new stateoftheart results that underscore its effectiveness. Nonetheless considerations around computational demand and the challenges of constructing highly effective domainspecific models indicate areas for further exploration or refinement. Though the reliance on substantial compute resources might limit the frameworks accessibility the potential scalability or adaptability of GAL to more constrained settings poses an interesting avenue for future research. Additionally exploring the interplay between GAL and nonneural algorithms like XGBoost for tabular data tasks could further broaden its applicability and impact. By proposing a method that improves upon existing limitations in SSL and KD through the generation of highquality synthetic data this paper not only advances the state of the art but also opens up new pathways for leveraging the growing capabilities of large language models in various domains of machine learning.", "niZImJIrqVt": " Peer Review:  Summary of the Paper: This paper addresses a critical concern in reinforcement learning (RL) for sequential decisionmaking under uncertainty particularly focusing on the meanvariance (MV) tradeoff in policy optimization. Existing methods for considering the MV tradeoff are computationally challenging due to difficulties in estimating the gradient of the variance term. The authors propose an innovative approach by training a reinforcement learning agent to maximize the expected quadratic utility function thereby achieving Pareto efficiency in terms of the MV tradeoff without encountering the computational difficulties of gradient estimation of the variance term. The key idea is that the maximizer of the objective in their proposed method Expected Quadratic Utility Maximization Reinforcement Learning (EQUMRL) corresponds to a Pareto efficient policy. Experimental validations demonstrate the superiority of EQUMRL over traditional methods in obtaining more MVefficient policies especially highlighted in applications like financial portfolio management where meanvariance considerations are pivotal.  Strong Points: 1. Originality: The paper presents a novel method EQUMRL addressing the gradient estimation issue of variance in MV tradeoffs which is a significant advancement over existing RL methods. 2. Relevance: Given the increasing importance of riskaware RL in areas such as finance and robotics this work is timely and contributes meaningful insights to both the RL and applied finance communities. 3. Clarity and Depth of Methodological Framework: The problem is wellformulated the proposed solution is articulated with clarity and the empirical validation is comprehensive. The paper elaborately discusses the theoretical underpinnings including connections to economic theory and finance making a compelling case for the practical relevance of EQUMRL. 4. Experimental Validation: The experiments are meticulously designed showing EQUMRLs practical benefits over existing methods. The use of both synthetic and realworld financial datasets for empirical testing enhances the papers credibility and applicability.  Weak Points: 1. Comparison with StateoftheArt (SOTA): While the paper compares EQUMRL with traditional\\xe2\\x80\\xafMVRL methods there\u2019s a missed opportunity in not discussing or comparing it with the latest SOTA methods outside of the MVRL paradigm that also address riskawareness in RL. 2. Robustness Analysis: The paper could benefit from a more thorough robustness analysis particularly exploring the sensitivity of EQUMRL to its hyperparameters (\u03b1 \u03b2) which are crucial for the quadratic utility function. 3. Generalization Capabilities: The generalization of EQUMRL across different domains beyond finance is not explored. Insights into how the method performs in other areas where riskaware decisionmaking is critical would add value.  Questions and Feedback to the Author: 1. Methodological Clarification: Could the authors elaborate on the theoretical premise that guarantees EQUMRL\u2019s convergence to Pareto efficiency in diverse settings beyond the tested scenarios 2. Comparison with SOTA: Are there other riskaware RL frameworks or models outside the traditional MVRL methods that EQUMRL was compared against Insights into how EQUMRL stacks up against these would provide a clearer picture of its novelty and efficiency. 3. Hyperparameter Sensitivity: How sensitive is EQUMRL to its hyperparameters and how does this compare to traditional methods More discussion on this would be beneficial. 4. Application Scope: Could the authors provide thoughts or preliminary results on the application of EQUMRL in domains other than finance This would help in understanding the methods versatility and potential wider impact.  Review Summary: The paper introduces an innovative and computationally efficient method EQUMRL addressing a substantial gap in reinforcement learning concerning the meanvariance (MV) tradeoff. The proposed method is original theoretically wellfounded and demonstrates significant empirical improvements over traditional approaches within the realm of financial portfolio management. However the paper would benefit from a deeper analysis concerning its sensitivity to hyperparameters comparative discussion with the latest SOTA outside the MVRL paradigm and exploration of its generalization capabilities across various problem domains. Despite these areas for enhancement the contribution of this work to the field of reinforcement learning and its application in riskaware sequential decisionmaking contexts is both valuable and timely. The strengths of the paper\u2014the novel approach to tackling MV tradeoffs indepth methodological exposition and compelling empirical validation\u2014strongly support its significance and potential impact on the field.", "i7O3VGpb7qZ": " Peer Review of \"A Novel Approach to Code Editing with Few Exemplars Using Deep Learning\" 1) Summary of the Paper: The paper presents a deep learning method aimed at automating the process of editing computer source code by leveraging a few exemplars which demonstrate the desired modifications. It introduces an innovative approach that identifies and adapts editorial patterns from these exemplars applying them to query code snippets that require editing. This process involves parsing the code into abstract syntax trees (ASTs) and measuring similarities across different extents or levels ranging from individual nodes to the collective tree representation for precise matching between the support exemplars and the query snippets. The paper evaluates this method on two datasets one in C and another in Python showcasing its superiority over existing noncompositional methods by achieving up to 8.6 absolute accuracy improvement. 2) Strengths and Weaknesses: Strong Points:  Novelty and Relevance: The paper addresses a crucial challenge in software engineering\u2014automating code editing using machine learning\u2014with a novel deep learningbased solution. The innovative use of multiple extents for similarity measurement between exemplars and queries is particularly commendable.  Technical Soundness: The methodology which combines edit representations extracted from support exemplars and generalizes them compositionally via a multiextent similarities ensemble is both technically sophisticated and wellarticulated.  Evaluation on Realworld Datasets: The comprehensive evaluation using realworld datasets in two different programming languages and the comparison with baselines and existing approaches provide a strong validation of the proposed methods effectiveness. Weak Points:  Limited Generalizability Discussion: While the paper presents results on C and Python datasets the discussion on the methods generalizability to other programming languages or code editing tasks is somewhat limited.  Complexity and Usability Concerns: The complexity of the proposed approach including the need for parsing code into ASTs and measuring multiextent similarities might pose challenges for practical adoption and usability which the paper does not sufficiently address.  Lack of Broader Impact Discussion: The paper could have benefited from a discussion on the broader implications of automated code editing including potential effects on software development workflows and the role of human developers in a scenario where such tools are widely used. Questions and Feedback to the Author:  How does the proposed method perform on code editing tasks involving other programming languages beyond C and Python Are there specific challenges or considerations in adapting the method to different syntax rules and structures  Could the authors comment on the computational efficiency of their approach particularly when applied to large codebases or complex editing tasks Are there strategies to mitigate potential performance bottlenecks  The paper briefly touches on the potential of the proposed method to support various programming applications such as code refactoring and bug repair. Could the authors expand on how their approach could integrate with existing development tools or workflows to enhance software engineering practices 3) Review Summary: The paper introduces an innovative deep learning approach to address the problem of automated code editing using a few exemplars. It is a significant contribution to the field of software engineering advancing the capabilities of machine learning techniques in understanding and modifying source code meaningfully. The methods ability to parse code snippets into abstract syntax trees and measure similarities at multiple extents to align the support exemplar with the query code snippet represents a notable technical achievement yielding performance improvements over established methods. However the paper could be strengthened by a more detailed discussion on the methods applicability across different programming languages its computational efficiency and its integration into software development practices. With further development and clarification on these points the proposed approach has the potential to be a valuable tool for software developers aiding in a variety of code editing tasks with implications for productivity and code quality.", "h4EOymDV3vV": " Review of \"Diffusionbased Representation Learning (DRL): A NonAdversarial Approach with Adversarial Insights\"  1. Summary The paper presents a novel framework for representation learning within diffusionbased generative models named Diffusionbased Representation Learning (DRL). Leveraging stochastic differential equations (SDEs) DRL augments denoising score matching by introducing a framework that allows for hierarchical representation learning without supervised signals. Unlike conventional methods such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) that transform latent codes to data samples directly DRL adopts a differentiated approach by reformulating the denoising score matching objective to encode information necessary for denoising. The paper highlights the potential for manual control over the granularity of information encoded in the representations. Furthermore the paper proposes learning an infinitedimensional latent code showcasing improvements in semisupervised image classification over stateoftheart models. Additional contributions include demonstrating the benefits of adversarial training in diffusion models for enhanced sample quality and faster sampling through an innovative approximation of the prior at smaller noise scales.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper introduces a novel formulation for representation learning in the realm of diffusionbased generative models. The method distinguishes itself by not restricting representations to specific tasks in contrast to prior work that utilized contrastive learning.  Control Over Representation Granularity: DRL provides a way to control the amount of detail encoded in the representation which is a significant advancement over existing methods that lack such flexibility.  Promising Empirical Results: The proposed framework demonstrated stateoftheart performance in semisupervised image classification on benchmark datasets establishing the effectiveness of the approach.  Broadening the Potential of Diffusion Models: By incorporating adversarial insights into a traditionally nonadversarial context the paper opens up new avenues for enhancing sample quality and speeding up generation processes in diffusion models. Weak Points:  Limited Domain Evaluation: The empirical evaluations focus primarily on image data. It remains unclear how DRL would perform across different domains or more complex tasks.  Complexity and Computation: The paper acknowledges limited computational resources and a constrained hyperparameter search raising concerns about the practicality and scalability of the proposed method especially for researchers with limited access to computational power. Questions and Feedback:  How does DRL handle overfitting especially considering its ability to control the granularity of encoded features  Could the authors elaborate on potential strategies for choosing optimal \u03c3(t) values and the weighting function \u03bb(t) to further refine the frameworks performance  Future directions: How do the authors envision extending DRL to other types of data or tasks beyond image classification  3. Review Summary The paper introduces Diffusionbased Representation Learning (DRL) a compelling new method for learning hierarchical representations in diffusionbased generative models. Its contribution lies in redefining denoising score matching objectives to encode detailed information useful for denoising without depending on supervised signals. This innovation allows for unmatched control over the granularity of representations and has demonstrated substantial empirical success especially in semisupervised image classification tasks. However the research could benefit from broader domain evaluations and more extensive exploration into the strategic selection of key parameters. The computational intensity and complexity of the framework also suggest a potentially high barrier to entry for replicating or extending the proposed methods. Despite these limitations the paper represents a significant step forward in the application of diffusionbased models for representation learning providing a foundation for further exploration and development in the field. In sum this work not only showcases the untapped potential of diffusion models but also provides practical insights into improving generative model performance and efficiency. The findings encourage further research into adaptable representation learning methods and their applications across a wider range of data types and tasks.", "qynB_fAt5TQ": " Peer Review  1) Summary of the Paper This paper introduces a novel method to enhance the efficiency of bootstrap methods for uncertainty quantification in largescale machine learning applications particularly in deep learning. Traditional bootstrap techniques while theoretically sound suffer from computational and memory intensiveness due to the necessity to draw a large number of i.i.d. samples. The authors propose an alternative approach that optimizes a small set of \"centroid\" points minimizing a simple objective function asymptotically equivalent to the Wasserstein distance to the ideal bootstrap distribution. This approach theoretically grounded and empirically validated significantly reduces the computational load and outperforms the standard bootstrap method by providing an accurate estimation of uncertainty with fewer samples. The paper demonstrates the efficacy of this method across various applications like confidence interval estimation bootstrap methods for contextual bandit bootstrap deep Qnetwork and bagging method for neural networks.  2) Strengths and Weaknesses Strengths:  Innovative Approach: The proposed method innovatively addresses the computational and memory issues of traditional bootstrap methods making it particularly suitable for deep learning applications.  Solid Theoretical Foundation: The theoretical explanation including connections to Kmeans and formal theoretical justification provides a strong foundation for the method.  Empirical Validation: The method is comprehensively evaluated across different applications significantly outperforming standard methods. Weaknesses:  Limited Comparison with Existing Methods: While the paper mentions other particle improvement approaches a more detailed quantitative comparison with stateoftheart methods would strengthen the paper.  Potential for Overfitting: The centroid approximation method by focussing on optimizing a set of points might risk overfitting to the training data especially with a very small number of centroids. This aspect was not thoroughly investigated.  Implementation Details and Reproducibility: Although the paper mentions algorithms for the proposed method more detailed implementation details or availability of code would aid in reproducibility and adoption by other researchers. Questions and Feedback to the Authors:  Comparison with Bayesian Approaches: How does the proposed method compare with Bayesian inference methods especially regarding computational efficiency and uncertainty estimation accuracy  Impact on Overfitting: Could you elaborate on potential overfitting risks with the centroid approximation method and any strategies to mitigate it  Extension to HighDimensional Settings: Is there any insight into how the proposed method performs in very highdimensional settings which are common in deep learning applications  3) Review Summary The paper presents a novel and theoretically sound approach to optimize bootstrap methods for uncertainty quantification in largescale machine learning tasks addressing key limitations associated with computational and memory requirements. The methods ability to accurately estimate uncertainty with a significantly reduced sample size is impressive and of great practical relevance. However the paper could be strengthened by addressing comparisons with existing approaches more thoroughly exploring potential overfitting issues and providing more detailed guidance on implementation for reproducibility. Despite these points the innovation and potential impact of the proposed method make it a valuable contribution to the field. The paper is wellwritten and the empirical results convincingly support the proposed approach warranting its publication. However addressing the outlined concerns could substantially enhance the papers impact and applicability.", "ufGMqIM0a4b": " Review of the InfinityGAN Paper for ArbitrarySized Image Generation  1. Summary of the Paper This paper introduces InfinityGAN a novel generative adversarial network (GAN) framework designed to tackle the challenge of producing highfidelity arbitrarysized images. The primary hurdles in generating largescale images include computational resource limitations and the necessity for both local and global consistency within generated images. InfinityGAN addresses these by training on image patches and inferring images patchbypatch significantly reducing computational load. The framework disentangles global appearances local structures and textures allowing for the generation of large images with cohesive structure and realistic details. The authors demonstrate InfinityGANs superiority over existing methods through qualitative and quantitative evaluations showcasing applications like spatial style fusion multimodal outpainting and image inbetweening.  2. Review Strong Points:  Innovative Approach: InfinityGANs strategy to disentangle global appearance from local structures and textures is novel and addresses a significant challenge in arbitrarysize image generation.  Resource Efficiency: The methods low computational demands by inferring images in a patchbypatch manner make it highly practical and accessible.  Versatility in Applications: The demonstrated applications including style fusion and multimodal outpainting highlight InfinityGANs flexibility and potential for a wide range of image synthesis tasks.  Extensive Evaluation: The thorough experimental evaluation including a user study and the introduction of the ScaleInv FID metric provides a solid validation of InfinityGANs performance. Weak Points:  Lack of Diversity Analysis: While the paper addresses structural consistency and realistic texturing there is less emphasis on the diversity of generated images. It would be beneficial to analyze and discuss the variability in the images produced by InfinityGAN especially when generating larger scenes.  Dataset Bias: The primary focus on landscape images might limit the understanding of InfinityGANs performance across different image domains. Examining its efficacy on diverse datasets such as urban environments or interiors would strengthen the claims.  Comparative Analysis: Although the paper compares InfinityGAN with several baseline methods a deeper exploration of its performance relative to the latest methods in largescale image generation could offer more insights into its innovation and efficacy. Questions and Feedback to the Author: 1. Can the authors provide insights or preliminary results on InfinityGANs performance across datasets beyond landscapes to underscore its generalizability 2. A discussion on the diversity of the generated images by InfinityGAN would be appreciated. How does the model ensure varied and nonrepetitive outputs when generating extensive scenes 3. The paper mentions the removal of zero paddings in the texture synthesizer for seamless patch composition. Can the authors elaborate on how this design choice impacts the learning dynamics or the quality of generated images  3. Review Summary InfinityGAN represents a significant advancement in the field of largescale image generation. Its innovative approach to disentangle global appearance from local textures and structures addresses a crucial challenge in generating cohesive and realistic images of arbitrary sizes. This frameworks efficiency and versatility in applications are notably commendable with solid experimental validations supporting its claims. However the paper could benefit from a more comprehensive exploration of the diversity in generated images and an assessment of its performance across varied image domains. These additional analyses would offer a fuller understanding of InfinityGANs capabilities and potential areas for future improvement. The promising results and practical implications of InfinityGANs approach strongly justify its further development and exploration. With adjustments to address the discussed weaknesses and expansion in the scope of evaluation InfinityGAN can substantially contribute to advancements in image synthesis and related applications in computer vision and digital content creation.", "nEfdkfAyRT8": " Peer Review for \"Cubic Regularized Gradient DescentAscent for NonconvexStronglyConcave Minimax Optimization\"  Summary of the Paper This paper introduces the CubicGDA algorithm a novel approach leveraging Newtontype gradient descentascent to tackle nonconvexstronglyconcave minimax optimization challenges. Unlike existing Gradient DescentAscent (GDA) algorithms which are prone to getting stuck at suboptimal saddle points CubicGDA utilizes cubic regularization and approximates secondorder information to efficiently escape these strict saddle points. The authors provide a theoretical framework demonstrating global convergence to secondorder stationary points with convergence rates surpassing that of standard GDA under varying nonconvex geometries. The study is motivated by the broad applicability of nonconvex minimax optimization in fields such as game theory and machine learning where overcoming saddle points is crucial for training highdimensional models effectively.  Strong Points 1. Innovation: The development of a Newtontype GDA algorithm for nonconvexstronglyconcave minimax optimization is novel and addresses a significant gap in the optimization field especially for machine learning applications. 2. Theoretical Contribution: The paper provides a rigorous mathematical foundation for the algorithm\u2019s efficiency in escaping strict saddle points and achieving faster convergence rates under a wide array of nonconvex geometries. 3. Comprehensive Analysis: The convergence analysis covers both global and local convergence properties demonstrating the algorithm\u2019s robustness across different scenarios and parameterizations.  Weak Points 1. Practical Implementation Details: While the paper mentions the computational efficiency of implementing CubicGDA it lacks detailed discussion on practical challenges that might arise when applying the algorithm in realworld scenarios such as computational resource requirements and handling of very largescale problems. 2. Empirical Evaluation: The study predominantly focuses on theoretical analysis and does not provide empirical evidence to support the claimed benefits of CubicGDA over existing algorithms in practical applications. 3. Comparison with Contemporary Works: The related work section covers a broad range of existing approaches but falls short in directly comparing the proposed CubicGDA\u2019s performance with these methods especially in terms of empirical results.  Questions and Feedback to the Author 1. Can the authors discuss potential strategies to mitigate the computational complexity of CubicGDA especially when dealing with largescale optimization problems 2. It would be beneficial if the authors could provide empirical results demonstrating the algorithms performance on benchmark datasets or realworld applications comparing it with standard GDA and other stateoftheart optimization algorithms. 3. The paper would benefit from a deeper discussion on the limitations of CubicGDA specifically in contexts where obtaining Hessian or approximate Hessian information is not feasible or too expensive computationally.  Review Summary The paper introduces an innovative and theoretically sound approach to tackle the longstanding problem of escaping saddle points in nonconvexstronglyconcave minimax optimization relevant to a wide array of fields including machine learning. The proposed CubicGDA algorithm represents a significant advancement in this domain with strong theoretical backing for its global convergence properties and efficiency in overcoming strict saddle points. However the paper could be further strengthened by addressing practical implementation concerns providing empirical validation and offering a more nuanced comparison with existing methods. Despite these areas for enhancement the paper makes a substantial theoretical contribution and lays a solid foundation for future research into efficient optimization algorithms capable of handling the complexities of nonconvex landscapes.", "jJis-v9Pzhj": "Peer Review for \"PUUPL: PositiveUnlabeled UncertaintyAware PseudoLabeling for Enhanced Binary Classification Performance\" 1) Summary of the Paper: This paper introduces PUUPL an innovative framework for positiveunlabeled (PU) learning that leverages uncertainty quantification to improve pseudolabeling accuracy. PU learning a variant of binary classification uses only positive and unlabeled data for training binary classifiers. Traditional methods often suffer from incorrectly estimated pseudolabels leading to error propagation throughout the training process. PUUPL addresses this by utilizing an ensemble of neural networks to quantify epistemic uncertainty and assign pseudolabels based on highconfidence predictions. Through extensive experiments the authors demonstrate that PUUPL outperforms existing approaches in various datasets learning tasks and is particularly robust against hyperparameter misspecification and biased positive data. The paper includes a detailed methodology algorithmic implementation and thorough exploration of related work in PU learning and uncertaintyaware pseudolabeling. 2) Strengths and Weaknesses Strengths:  Originality: The incorporation of epistemic uncertainty into the pseudolabeling process for PU learning is novel and addresses a significant challenge in the field.  Comprehensiveness: The paper provides a thorough comparison with existing methods reviewing the latest literature in both PU learning and uncertainty quantification areas signifying a wellresearched contribution.  Experimental Validation: The method is validated on various datasets and compared to stateoftheart methods showcasing its robustness and versatility. The inclusion of source code enhances reproducibility and realworld applicability.  Methodological Clarity: The presentation of the PUUPL method is clear and detailed with concise pseudocode and a welldefined experimental protocol. Weaknesses:  Limited Discussion on Drawbacks: The paper could benefit from a deeper discussion on potential limitations or scenarios where PUUPL might not perform optimally.  Lack of Broad Comparison: While stateoftheart methods are compared a broader set of baseline methods especially recent uncertainty quantification techniques could provide a more detailed performance landscape.  Potential Overfitting Concerns: The paper briefly touches on robustness to hyperparameter specifications but lacks a detailed analysis of overfitting risks especially given the iterative pseudolabeling process.  Ethical Considerations: The ethics statement is somewhat generic. A more detailed analysis of the potential for bias in pseudolabeling and strategies for mitigation would be valuable. Questions and Feedback:  How does PUUPL handle scenarios with extremely sparse positive examples within the unlabeled set  Could the authors elaborate on the choice of datasets and whether they include cases with extreme class imbalances  What strategies can be employed to mitigate potential overfitting risks inherent in iterative pseudolabeling processes  It would be beneficial for the authors to discuss the computational costs associated with PUUPL given the use of ensemble models. 3) Review Summary PUUPL represents a significant advancement in the field of PU learning by integrating an ensemble of neural networks to quantify and utilize epistemic uncertainty enhancing the reliability of pseudolabeling. The methodological clarity comprehensive literature review and extensive experiments underline the papers strengths and its contribution to pushing the stateoftheart in PU learning. However the paper could be improved by addressing the mentioned weaknesses particularly by offering a deeper discussion on the methods limitations broadening the comparison to include a wider array of baseline methods analyzing risks of overfitting in greater depth and elaborating on ethical considerations specific to PUUPL application scenarios. Overall the paper provides compelling evidence of PUUPLs superiority over existing methods in terms of predictability and robustness making a strong case for its adoption in realworld PU learning tasks. Given these strengths along with suggestions for improvement this work is wellpositioned to make a significant impact in the field.", "zyrhwrd9EYs": " Review of \"Selective Imputation for Treatment Effect Estimation with Missing Data\"  Summary The paper addresses the pervasive issue of missing data in estimating treatment effects an area crucial for decisionmaking in various fields. It identifies a novel missingness mechanism termed Mixed Confounded Missingness (MCM) where missingness both influences and is influenced by treatment selection. The core argument is that both complete imputation and no imputation of missing values can bias treatment effect estimations due to the unique nature of MCM. The paper introduces selective imputation as a solution where decisions on imputation are made based on the relationship of missingness with treatment. Empirical evidence demonstrates that selective imputation outperforms other imputation methods in reducing bias and improving the accuracy of treatment effect models across various scenarios.  Strong Points 1. Novelty of the Concept: The introduction of MCM and selective imputation addresses a significant gap in the literature on treatment effect estimation with missing data providing a nuanced understanding of the relationship between missingness and treatment. 2. Thorough Analytical Framework: The paper provides a comprehensive theoretical foundation for MCM supported by rigorous definitions assumptions and a theorem proving the effectiveness of selective imputation in ensuring unbiased treatment effect estimations. 3. Empirical Validation: The empirical studies encompass a wide range of scenarios demonstrating the general applicability and robustness of selective imputation. The use of synthetic data allows for controlled experiments to validate the proposed methods effectiveness.  Weak Points 1. Assumptions and Realworld Applicability: While the paper provides strong theoretical support for selective imputation it assumes certain conditions (e.g. the partition of Z into two distinct factors) that may not always hold in realworld datasets. The practical challenges of applying selective imputation in complex reallife scenarios may warrant further discussion. 2. Comparison with Existing Methods: The paper could benefit from a more detailed comparison with existing methods particularly those that might have partially addressed the issue of MCM or proposed alternative solutions for imputation in the context of treatment effect estimation. 3. Addressing Imputation Methods: Though selective imputation is proposed as a superior approach the paper does not thoroughly discuss how different imputation techniques might interact with selective imputation. Given the variety of imputation methods available exploring this interaction could provide valuable insights.  Questions and Feedback to the Author 1. Could the authors clarify the conditions under which the assumptions made for MCM and selective imputation may not hold in realworld data How might these conditions affect the applicability of their method 2. How does selective imputation compare with other recent advances in handling missing data within treatment effect estimation Are there specific scenarios where one method might be preferred over the other 3. The paper mentions synthetic data for empirical validation. Could the authors discuss the potential challenges or adjustments needed when applying selective imputation to realworld datasets which might be messier and less predictable than synthetic ones  Review Summary This paper makes a significant contribution to the field of causal inference particularly in addressing the challenges posed by missing data in treatment effect estimation. The conceptualization of MCM and the introduction of selective imputation are both novel and valuable offering a more refined approach to handling missing data in this context. The theoretical foundation is solid and the empirical validation is comprehensive. However the paper could be strengthened by addressing the practical applicability of its assumptions in realworld scenarios expanding on the comparison with existing methods and discussing the interaction between selective imputation and different imputation techniques. By exploring these areas further the paper could provide a more rounded understanding of how selective imputation can be applied effectively in practice enhancing its contribution to the literature.  Recommendation Recommendation for acceptance with minor revisions. The study presents a significant advancement in the field but could be improved by addressing the mentioned weak points and providing more detailed discussions on certain aspects for clarity and completeness.", "tV3N0DWMxCg": " Summarize the paper The paper introduces the Natural Posterior Network (NatPN) a novel approach for estimating uncertainty in machine learning models without requiring outofdistribution (OOD) data for training. NatPN operates by parametrizing conjugate prior distributions allowing for versatility across tasks described by the exponential family of distributions including classification regression and count prediction tasks. It leverages normalizing flows to fit densities on a learned lowdimensional taskdependent latent space enabling fast uncertainty estimation in a single forward pass through inputdependent Bayesian updates. Theoretically it promises high uncertainty for samples far from training data and empirically it demonstrates competitive performance in terms of accuracy calibration and OOD detection across various tasks. An ensemble version NatPE further enhances prediction accuracy and calibration.  Strong and Weak Points Questions and Feedback Strong Points: 1. Versatility Across Tasks: NatPNs ability to estimate uncertainty for tasks across the exponential family of distributions is a significant advantage making it widely applicable in many realworld situations. 2. No OOD Data Required: Unlike many existing approaches NatPN does not require OOD data for training which is a practical advantage because obtaining such data can be challenging. 3. Theoretical Underpinnings: The paper provides a solid theoretical basis for why NatPN should assign high uncertainty to inputs far from the training data which is crucial for reliable uncertainty estimation. 4. Practical Performance: The extensive empirical evaluation demonstrates that NatPN performs well across various metrics (accuracy calibration OOD detection) and tasks showing its effectiveness in practice. Weak Points: 1. Model Complexity and Training Difficulty: While not extensively discussed the introduction of normalizing flows and the requirement for a lowdimensional taskspecific latent space might introduce complexity in training and model tuning. 2. Limited Discussion on Model Limitations: The paper could benefit from a deeper discussion on potential limitations such as scenarios where NatPN might not perform as expected or the impact of the choice of prior parameters on model performance. 3. Comparison to StateoftheArt Models: While the paper compares NatPN to several baseline models a more direct comparison with the latest stateoftheart models in uncertainty estimation would provide a clearer picture of NatPNs positioning. Questions and Feedback: 1. Scalability to Very Large Datasets: How does NatPNs performance and computational efficiency scale with the size of datasets especially in highdimensional spaces 2. Robustness to Adversarial Attacks: Has NatPN been tested against adversarial attacks aimed at uncertainty estimation models and if so how does it perform 3. Influence of Encoder Architecture: The paper mentions that the encoder architecture can be arbitrarily complex. Could the authors provide more guidance on how to choose or design encoders for different tasks 4. Extensibility to Other Distribution Families: While NatPN is designed for the exponential family of distributions is there potential to extend this approach to other families and what would such an extension entail  Review Summary The Natural Posterior Network (NatPN) presented in the paper represents a significant advancement in uncertainty estimation for machine learning models. Its ability to handle tasks across the exponential family of distributions without requiring OOD data during training sets it apart from many existing approaches. The theoretical basis for its uncertainty estimation methods and the empirical evidence of its performance across different tasks and metrics are compelling. However the paper could be strengthened by addressing potential limitations more thoroughly providing more comparative analysis with stateoftheart models and exploring the models scalability robustness to adversarial attacks and the impact of encoder architecture choices. Overall NatPN shows great promise for practical applications and further research in reliable machine learning models.", "syzTg1vyBtL": " Review of the Paper on Congested Bandits  Summarize the paper This paper introduces and explores a novel problem called Congested Bandits where the reward from each action (or \"arm\") in a multiarmed bandit (MAB) setup depends on the number of times that action was chosen in a recent history denoted by a time window \\xe2\\x88\\x86. This leads to a more complex dynamic where past choices influence future rewards effectively making the rewards for each action nonstationary. The authors propose algorithms for both traditional MAB setups and for contextual bandits with linear rewards. For the MAB setting they develop a UCBstyle algorithm that achieves a policy regret scaling of \\xc3\\x95(\\xe2\\x88\\x9aK\\xe2\\x88\\x86T) and for the linear contextual bandit setting they leverage an iterative least squares planner for a policy regret of \\xc3\\x95(\\xe2\\x88\\x9adT  \\xe2\\x88\\x86). The theoretical contributions are supported by simulation studies which show the effectiveness of the proposed algorithms in managing the dynamic congestion effects on rewards.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty: The paper addresses a unique and realistic problem of congestion in bandit frameworks which is highly relevant in scenarios like traffic routing and digital platform recommendations. This adds a significant layer of complexity to the classic MAB problem. 2. Robust Analysis: The theoretical analysis including the development of algorithms with policy regret bounds is rigorous. The authors present a deep understanding of the problem by subdividing it into tractable parts and tackling each with appropriate methodologies. 3. Practical Relevance: The motivation behind the study i.e. the application of congested bandit models to realworld problems like traffic management and digital content recommendation is strong and compelling. Weak Points: 1. Assumptions Clarity: It\u2019s not entirely clear how realistic or restrictive the assumptions made for theoretical analysis are especially in realworld applications. A more detailed discussion on the applicability and limitations of these assumptions would enhance the papers practical relevance. 2. Complexity and Scalability: The paper could benefit from a clearer explanation of the computational complexity of the proposed algorithms and their scalability particularly in scenarios with a very large number of arms or highly dynamic contexts. 3. Comparative Analysis: While the paper does mention related work a more direct comparison with existing approaches (e.g. those addressing nonstationary bandits without the specific congestion model) in terms of theoretical and empirical performance would be valuable. Questions and Feedback: 1. Can the authors provide more insight into the assumptions made for the congestion function and discuss their practical implications 2. How does the performance of the proposed algorithms change with different congestion window sizes \\xe2\\x88\\x86 in more diversified simulation settings beyond what is presented 3. Is there potential for adapting the proposed algorithms to scenarios where the congestion function or the reward dynamics are unknown or partially known to the learner  Review Summary This paper successfully tackles a novel and complex problem of Congested Bandits extending the classic multiarmed bandit framework to scenarios where past choices influence future rewards because of congestion effects. The introduction of a dynamic component where rewards for actions are dependent on their recent selection history adds both relevance and complexity to the problem addressing scenarios encountered in traffic systems and digital platforms. The theoretical contributions including the development of algorithms with bounded policy regret are substantial and wellsupported by simulation results. However the paper would benefit from a more detailed discussion on the assumptions practicality the algorithms\u2019 computational complexity and scalability and a more direct empirical comparison with existing solutions to similar nonstationary problems. Overall the paper makes a significant contribution to the bandit literature opening up several pathways for future research in dynamic congested environments.", "jNsynsmDkl": "Summarize the paper: This paper addresses the challenge of applying contrastive learning (CL) to multilabel image classification tasks. It introduces a novel framework called MulCon which utilizes a fully supervised setting to generate multiple labellevel representations for each image thereby enabling the effective application of CL in a multilabel context. This method diverges from traditional CL applications in singlelabel classification by focusing on labellevel rather than imagelevel representations. The framework includes an encoder block for generating imagelevel embeddings a labelwise attention block for deriving labellevel embeddings and a contrastive learning projection network. MulCon also incorporates a classification network that predicts the presence of labels based on labellevel embeddings. The paper demonstrates the superiority of this approach through extensive experiments on benchmark datasets showing that it outperforms stateoftheart multilabel classification methods. Strong Points: 1. Novel Framework: The introduction of MulCon which leverages labellevel embeddings for applying contrastive learning in multilabel classification is a significant innovation. This approach addresses the inherent complexity of multilabel classification and the challenge of defining positive and negative samples for contrastive learning in this context. 2. Comprehensive Methodology: The paper provides a detailed explanation of the proposed framework including the encoder block labelwise attention block contrastive learning projection network and classification network. This thorough exposition helps readers understand the workings of MulCon. 3. Extensive Experiments: The experiments conducted on benchmark datasets and the subsequent comparison with advanced methods in multilabel classification add credibility to the claims of superior performance. The ablation studies further elucidate the contribution of each component of the framework to its overall effectiveness. Weak Points: 1. Complexity of Implementation: The paper introduces a sophisticated framework that might be challenging to implement for practitioners without a deep understanding of contrastive learning or attention mechanisms. A simpler explanation or provision of code would enhance accessibility. 2. Lack of Discussion on Limitations: While the paper highlights the strengths of MulCon it does not adequately discuss its limitations or scenarios where it might not perform as well. This discussion would have given a more balanced view of the frameworks applicability. 3. Comparison with Recent Works: The paper compares with stateoftheart methods but does not provide a comparison with the very latest works in the domain which might have emerged after the methodologies discussed were proposed. This comparison could have offered a current perspective on the frameworks position in the research landscape. Questions and Feedback to the author: 1. Can the authors provide insights into the computational complexity of implementing MulCon compared to traditional multilabel classification methods 2. How does MulCon handle images with a large number of labels given the increased complexity in generating labellevel embeddings 3. Is there a possibility of extending MulCons application beyond image classification for instance to other domains like text or video 4. Could the authors discuss any limitations or scenarios where MulCon might not perform optimally 5. Are there any considerations or modifications needed to apply MulCon in realworld scenarios with highly imbalanced datasets Review Summary: The paper presents a novel and comprehensive framework MulCon for applying contrastive learning in multilabel image classification by leveraging labellevel embeddings. This approach addresses the inherent challenge of defining positive and negative samples for contrastive learning in a multilabel context and demonstrates superior performance on benchmark datasets. The methodology is wellexplained and backed by extensive experiments offering a significant contribution to the field. However the complexity of the framework might limit its accessibility to practitioners and the lack of discussion on limitations and comparison with the very latest works in the domain leaves some questions about its applicability in all scenarios. Providing additional insights into these areas could strengthen the paper further. Overall the innovative approach and demonstrated performance of MulCon make it a valuable contribution to the field of multilabel image classification with potential applications and extensions into other domains worth exploring.", "qLqeb9AjD2o": "Review of \"ConfidenceAware Training for Randomized Smoothing: A Method for Certified Robustness with Improved AccuracyRobustness TradeOff\" 1) Summary: This paper introduces a novel approach ConfidenceAware Training for Randomized Smoothing (CATRS) aimed at enhancing the certified robustness of smoothed classifiers. This is achieved by controlling robustness samplewise during training using the confidence of predictions as a proxy for adversarial robustness. The proposed method addresses the tradeoff between accuracy and robustness which is a notable challenge in the field of adversarial defense. Specifically CATRS employs two new losses: bottomK Gaussian training for lowconfidence samples and worstcase Gaussian training for highconfidence samples. It dynamically adjusts the focus during training based on sample confidence. Experimental validation on MNIST and CIFAR10 datasets demonstrates that CATRS outperforms existing stateoftheart methods for certified robustness while efficiently managing the accuracyrobustness tradeoff. 2) Strong and Weak Points Questions and Feedback: Strong Points: a) Novel Approach: The idea of incorporating samplewise control of robustness based on prediction confidence is innovative and addresses a wellknown tradeoff in adversarial defense literature. b) Comprehensive Validation: The paper performs extensive experiments and comparisons with stateoftheart methods across standard benchmarks showing significant improvements in certified robustness without compromising accuracy. c) Clear Theoretical Foundation: The positioning of the work within the broader context of adversarial defense especially randomized smoothing is wellarticulated. The motivation and theoretical basis for using prediction confidence as a proxy for adversarial robustness are clearly explained. Weak Points: a) Generalization to Other Domains: While the paper presents results on MNIST and CIFAR10 datasets it is not clear how the proposed method would perform on more complex datasets or different domains (e.g. natural language processing tasks). b) Hyperparameter Sensitivity: The paper introduces additional hyperparameters (e.g. p0 \u03bb). An indepth analysis of the sensitivity of these parameters on the final model performance and guidance on their selection would be beneficial. c) Comparison to Empirical Defenses: While the focus is on certified robustness including comparisons with empirical defenses could provide a more comprehensive picture of the method\u2019s effectiveness. Questions and Feedback: 1) Generalization Capabilities: How would CATRS perform on datasets with higher dimensional inputs or in domains other than image classification Including such evaluations could strengthen the paper\u2019s contributions. 2) Hyperparameter Selection: Can the authors provide insights or recommendations on the selection of new hyperparameters introduced by CATRS This information would be valuable for practitioners looking to apply this method to their tasks. 3) Empirical Defense Comparison: The authors might consider including or discussing comparisons with some of the leading empirical defenses to highlight the advantages of CATRS over those approaches even if the primary focus is certified robustness. 3) Review Summary: This paper presents CATRS an innovative approach to enhancing certified robustness by controlling samplewise robustness during training with a special focus on the tradeoff between accuracy and adversarial robustness. The methodological contributions are significant with strong theoretical underpinnings and extensive experimental validation that demonstrates improvements over existing methods. However expanding the evaluation to more complex datasets and tasks providing detailed guidance on hyperparameter selection and contrasting with empirical defenses could further strengthen the work. Based on the comprehensive analysis and the improvements shown I recommend acceptance contingent upon addressing the aforementioned points to ensure a wider applicability and understanding of the proposed methods impacts and limitations.", "sBT5nxwt18Q": "Peer Review: 1. Summary of the paper The paper introduces Critical Classification Regions (CCRs) a novel Explainable Artificial Intelligence (XAI) technique that aims to enhance posthoc explanationbyexample methods with nearest neighbors by highlighting important parts in classifications. The authors propose methods to compute CCRs either based on latent image representations (latentbased CCRs) or using superpixels for agnostic ANN models (pixelbased CCRs specifically SPCCRs). Experimental work evaluates the performance of these CCR methods in highlighting significant parts of test images and their influence on classification. Furthermore a user study tests the impact of CCRs on users\u2019 perceptions regarding the correctness of CNN predictions particularly under conditions of misclassification due to ambiguity. The main contributions are presenting CCRs as a method to link critical classification features to learned features from the training data and empirically testing and validating the effectiveness of CCRs in enhancing users\u2019 trust in CNN classifications. 2. Strong and Weak points Questions and Feedback Strong points:  Innovative Contribution: The introduction of CCRs addresses a gap in the literature by linking critical features used for classification to where they were learned in the training data potentially aiding in the detection of biases and improving explanation quality.  Comprehensive Experiments: The paper features an extensive experimental setup including comparisons between different methods of computing CCRs and a user study to evaluate their practical effectiveness.  Detailed Methodological Approach: The paper\u2019s methodology from algorithm development to user study design is meticulously detailed offering a solid basis for reproducibility and further research in the field. Weak points:  Complexity for NonExperts: The technical depth while comprehensive might be overwhelming for nonexpert audiences or practitioners looking to apply these findings potentially limiting its accessibility.  Potential Overfitting in User Study Design: The user study while rigorously controlled might have benefitted from additional diversity in test images or exploration of varying levels of misclassification ambiguity to ensure broader applicability of the findings. Questions:  How might the CCR approach be adapted or extended to other types of neural network models beyond CNNs  Could the authors clarify if there were any measures taken to mitigate potential biases in the user study particularly concerning participants\u2019 familiarity with the subject matter of the test images Feedback:  Considering the complexity and technical depth of the work it would be beneficial to include a section or supplementary material aimed at practitioners outlining the potential applications and implications of CCRs in realworld scenarios.  Further investigation into the generalizability of the CCR method across diverse datasets and different network architectures could enhance the robustness and applicability of the findings. 3. Review Summary The paper presents a promising advancement in the field of Explainable AI through the introduction of Critical Classification Regions (CCRs) offering a novel means to enhance posthoc explanations by linking critical classification features to learned data. The comprehensive experiments and user study provide strong evidence of the technique\u2019s effectiveness particularly in improving users perceptions of CNN predictions in cases of misclassification due to ambiguity. While the work is technically sound and thoroughly conducted further efforts could be made to enhance the accessibility of the findings to a broader audience and to explore the generalizability of the CCR approach across different models and datasets. Given the novelty potential impact and thorough validation presented I recommend the acceptance of this paper suggesting that addressing the outlined concerns could further strengthen its contribution to the field of XAI.", "t3E10H8UNz": " Review on \"Dual Meta Imitation Learning for Hierarchical Structures\"  Summary The paper introduces a novel approach named Dual Meta Imitation Learning (DMIL) aimed at resolving the limitation of hierarchical imitation learning (HIL) systems where the learned hierarchical structures are inflexible for task transferability. DMIL iteratively metalearns the highlevel network and subskills using modelagnostic metalearning facilitating rapid adaptation to new tasks by leveraging stateaction pair likelihoods for highlevel network adaptation and using the adapted network for subskill adaptation. The authors provide theoretical proof of convergence for this training process and establish a connection with the ExpectationMaximization algorithm. They evaluate DMILs performance on the metaworld benchmark and Kitchen environment showing stateoftheart results in fewshot imitation learning tasks and solid performances in longhorizon tasks.  Strong Points 1. Novelty and Theoretical Foundation: The paper introduces a novel framework that effectively combines hierarchical imitation learning with modelagnostic metalearning addressing a significant gap in the field. The theoretical grounding including the convergence proof and connection to the EM algorithm adds credibility to the proposed method. 2. Empirical Validation: Demonstrating stateoftheart performance on the metaworld benchmark and competitive results in the Kitchen environment showcases the practical effectiveness of DMIL. The comprehensive evaluation across different settings emphasizes the adaptability and generalizability of the method. 3. Depth of Analysis: The ablation studies along with analysis on the effect of subskill numbers and detailed examples of task adaptation provide insights into the behavior of DMIL and factors influencing its performance. This depth adds a strong understanding of the method\u2019s operational nuances.  Weak Points 1. Complexity and Implementation Challenges: While DMIL demonstrates impressive results the hierarchical metalearning framework introduces significant complexity which could pose implementation and scalability challenges. The paper could benefit from a discussion on computational costs and potential solutions to mitigate complexity in practical applications. 2. Lack of Comparative Baseline Diversity: The experimental setup while thorough could be enhanced by including a broader set of comparative baselines particularly those involving recent advancements in hierarchical and meta imitation learning. This might provide a clearer picture of DMILs positioning within the current research landscape. 3. Details on Failure Modes and Limitations: The paper could further improve by discussing potential failure modes limitations and the conditions under which DMIL might not perform as expected. Including such insights would help in setting realistic expectations and guiding future research directions.  Questions and Feedback to the Authors 1. Could you elaborate on the computational demands of DMIL especially concerning the iterative metalearning process and suggest strategies to improve efficiency 2. How does DMIL perform in environments with higher complexity or more nuanced subskills compared to those tested Are there scalability limits to the approach 3. The paper mentions the theoretical connection between DMIL and the EM algorithm. Could you discuss potential extensions or modifications that exploit this relationship further 4. Could you provide insights on how DMIL could be adapted or extended to multiagent settings or environments with dynamic task distributions  Review Summary This paper presents a compelling contribution to the field of hierarchical imitation learning by addressing the critical issue of task transferability through Dual Meta Imitation Learning (DMIL). The methods theoretical foundation coupled with significant empirical results underscores its potential to advance the capabilities of robotic systems in adapting to new tasks rapidly. However the complexity of the approach and the specifically chosen comparative baselines highlight areas that could be improved for a more comprehensive understanding and application of DMIL. Given the strengths and potential for broader impact this work constitutes a valuable addition to the literature with recommendations for further exploration on efficiency enhancements application scope and direct leveraging of the theoretical properties identified.", "yhjfOvBvvmz": " Review of \"WEaklysupervised learning approach for learning Disentangled and Interpretable Skills (WEDIS)\"  Summary: The paper introduces a novel method named WEDIS for learning continuous skills from disentangled and interpretable representations of trajectories in the context of hierarchical reinforcement learning (RL). The authors propose using a weaklysupervised Trajectory Variational Autoencoder (WETVAE) leveraging weak labels to enforce inductive bias thus learning disentangled representations of trajectories as interpretable skills. These skills are utilized by a policy network trained to generate similar trajectories by minimizing the KL divergence between two trajectory distributions focusing on singlestep transitions for training simplicity and largescale planning efficiency. The efficacy of WEDIS is demonstrated through experiments in challenging navigation tasks within the Mujoco Ant environment showing superior performance in tackling longhorizon problems with sparse rewards compared to existing unsupervised skill learning methods.  Strong Points: 1. Novelty and Relevance: The proposed WEDIS method addresses a significant and challenging problem in RL  learning interpretable and reusable skills in hierarchical settings. By integrating weaklysupervised learning with a trajectorybased VAE the paper contributes a novel approach to skill disentanglement which is both significant and timely in the RL research domain. 2. Clear Methodological Framework: The paper presents a coherent and wellstructured methodology for implementing the WEDIS approach. The use of weaklysupervised learning to enforce disentanglement in the model is both innovative and logically sound. The detailed explanation enhances the replicability of the research. 3. Empirical Evaluation: The comprehensive experimentation in the Mujoco Ant environment demonstrates the methods capability to solve longhorizon and sparse reward problems effectively. The comparison with other unsupervised skilllearning methods including detailed qualitative and quantitative results underscores the approaches improvements and realworld applicability.  Weak Points: 1. Weak Supervision Labels\u2019 Generality: While the use of weak labels is innovative the paper could further elaborate on how generalizable this method is across different environments or tasks. The selection criteria for weak labels and their availability or creation process for varied scenarios need clarification. 2. Comparison with Supervised Methods: The paper focuses on comparing WEDIS with unsupervised skill learning methods. Including comparisons with supervised or semisupervised baselines could provide a more comprehensive evaluation of the methods performance and advantages. 3. Potential for Overfitting: The methods potential for overfitting to specific task environments due to the weaklysupervised learning approach is not thoroughly addressed. More discussion on this along with strategies or experimental evidence to mitigate overfitting would strengthen the paper.  Questions and Feedback: 1. How generalizable are the weak labels used in WEDIS across different tasks or environments Can the authors provide guidelines or a framework for selecting or generating weak labels in new unseen environments 2. Could the authors elaborate on any attempts to compare WEDIS with supervised or semisupervised learning methods Understanding its performance relative to these methods would offer deeper insights into its efficacy. 3. How does WEDIS mitigate the potential risk of overfitting especially given the methods reliance on weak labels for training Are there specific strategies employed or does the disentanglement inherently reduce overfitting risks  Review Summary: WEDIS presents a compelling approach to learning disentangled and interpretable skills in hierarchical RL through weaklysupervised learning. Its novel application of weak labels within a trajectory VAE framework significantly contributes to the field addressing vital challenges in skill interpretability and reusability. The empirical evaluations effectively demonstrate its advantages in solving complex navigation tasks bolstering its potential for broader applications. However enhancing the paper with a discussion on the generality of weak labels comparative analysis with supervised methods and strategies to mitigate potential overfitting would provide a more comprehensive perspective on WEDISs effectiveness. By addressing these points the authors can further solidify the methods significance and applicability ensuring its impact on advancing skill learning in hierarchical RL. Recommendation: Accept with minor revisions. The paper is a valuable contribution to hierarchical RL research with its novel approach and significant results. Addressing the discussed weak points and questions can further strengthen the paper.", "y1faDxZ_-0a": " Peer Review of \"SelfSupervised Federated Learning (SSFL) for Addressing Data Heterogeneity and Label Deficiency\"  1) Summarize the paper The paper introduces SelfSupervised Federated Learning (SSFL) a novel framework designed to tackle the challenges of data heterogeneity (where edge devices generate datasets with different distributions) and label deficiency (where users are reluctant to label their private and sensitive data) inherent in federated learning (FL). The authors present a series of algorithms under the SSFL framework that leverage stateoftheart selfsupervised learning methods specifically highlighting the performance of SimSiam networks with the standard FedAvg algorithm. By incorporating personalized federated selfsupervised learning algorithms such as PerSSFL the framework seeks to balance personalization with consensus across distributed training environments. Experiments on synthetic and intrinsically nonI.I.D datasets demonstrate the effectiveness of SSFL showing minimal gaps in evaluation accuracy between supervised and unsupervised learning scenarios. The paper further covers comparative analysis ablation studies and introduces a distributed training system for comprehensive evaluation.  2) List strong and weak points of the paper Questions and Feedback to the author Strong Points:  Innovative Approach: The SSFL framework elegantly combines federated learning with selfsupervised learning techniques to address the critical issues of label deficiency and data heterogeneity providing a novel direction for FL research.  Comprehensive Evaluation: The empirical evaluation spans several stateoftheart selfsupervised learning methods extensive ablation studies and realworld datasets offering solid evidence of SSFLs effectiveness.  Addressing Practical Challenges: By focusing on the rarely explored aspect of label deficiency in FL the paper addresses a significant practical hurdle improving FLs applicability in realworld settings. Weak Points:  Limited Discussion on Theoretical Foundations: While the paper provides some theoretical insights especially in the formulation of the SSFL and PerSSFL algorithms a more detailed theoretical analysis particularly regarding convergence properties and privacy implications would strengthen the contribution.  Generalizability and Scalability Concerns: The papers experiments although comprehensive are limited to image classification tasks and specific datasets. Further exploration into how SSFL performs in different domains or tasks and its scalability to larger more complex networks is necessary.  Comparison with Baseline Methods: While the paper introduces innovative algorithms and compares them within the SSFL framework more extensive comparisons with existing FL and SSL methods could better contextualize SSFLs advantages and limitations. Questions and Feedback:  How does SSFL tackle the computational and resource constraints of edge devices especially given the typically larger model sizes in SSL  Can the authors elaborate on the privacy implications of SSFL particularly in relation to the data sharing necessary for selfsupervised methods  The paper could benefit from a deeper investigation into the failure of certain SSL methods (like BYOL and SwAV) in FL settings. What specific aspects of the FL environment contribute to these methods underperformance  3) Review Summary (Supporting arguments for your recommendation) The paper presents a significant advancement in federated learning by introducing the SelfSupervised Federated Learning (SSFL) framework. SSFLs innovative incorporation of selfsupervised learning to address the critical challenges of label deficiency and data heterogeneity is both novel and timely potentially expanding federated learnings applicability in privacysensitive applications. Empirically the paper stands out through its comprehensive evaluation across different datasets and learning scenarios effectively demonstrating SSFLs potential. However the papers impact is somewhat limited by its lack of extensive theoretical analysis particularly around the proposed algorithms convergence properties and the frameworks privacy considerations. Furthermore the generalizability of the findings to other domains and tasks beyond image classification as well as scalability to larger models or datasets remains an open question. To strengthen the paper further the authors are encouraged to address the raised concerns regarding theoretical depth privacy implications and extend the frameworks evaluation to a wider array of tasks and domains. Despite these limitations the papers contributions to the federated learning landscape are noteworthy and its exploration of selfsupervised learning within this context opens new avenues for research and application. Therefore my recommendation would lean towards acceptance contingent on addressing the highlighted feedback points for improvement.", "kavTY__jxp": " Peer Review for \"Distilled Graph Attention Policy Network for Generating Novel Chemical Structures\"  1) Summary of the Paper This paper presents a novel framework named Distilled Graph Attention Policy Network (DGAPN) aiming to automate the generation of novel molecular structures with optimized properties for specific objectives particularly focusing on generating molecules that can bind to SARSCoV2 proteins. The authors introduce a reinforcement learning model that efficiently navigates the massive and discrete chemical space through an action space comprised of chemically valid molecular fragments. They employ a spatial Graph Attention (sGAT) mechanism for encoding graph objects (molecules in this case) attributes structure and spatial information leveraging selfattention over node and edge attributes. The combination of these techniques allows the model to generate novel chemical structures while navigating physical constraints efficiently. The frameworks performance was evaluated through molecular docking simulations targeting the SARSCoV2 nonstructural protein NSP15 and it showed superior results in comparison to existing stateoftheart algorithms both in terms of the quality of generated molecules and the efficiency of chemical path exploration.  2) Strong and Weak Points Questions and Feedback Strong Points: a) Innovative Approach: The integration of a reinforcement learning model with a spatial graph attention mechanism to generate chemically valid molecule structures is novel and shows a promising direction for drug discovery. b) Efficiency in Exploration: The use of a fragmentbased action space significantly reduces the chemical spaces complexity enhancing the efficiency and effectiveness of the exploration process. c) Experimental Validation: The comprehensive evaluation against stateoftheart models and the impressive results in terms of docking scores and synthetic accessibility demonstrate the frameworks practical significance and potential impact. Weak Points: a) Generalizability: While the framework shows excellent performance on the targeted task the paper lacks explicit discussion on its adaptability and performance across various objectives beyond inhibiting SARSCoV2 proteins. b) Complexity and Accessibility: The complexity of the proposed approach might pose a barrier for broader adoption especially for those without a deep background in reinforcement learning or graph neural networks. c) Validation beyond Docking Scores: Solely relying on docking scores for evaluating the generated molecules might not fully capture their pharmacological potential including their safety or bioavailability profiles. Questions and Feedback: a) Generalizability Question: Can the authors comment on the frameworks adaptability for tasks with different objectives especially those not related to protein binding b) Complexity Concern: It would be beneficial if the authors could provide further insights or potential guidelines for nonexperts in the field to understand or utilize this framework. c) Further Validation Suggestion: Consider including further assessments such as ADMET predictions to evaluate the pharmacological potential of the generated molecules comprehensively.  3) Review Summary and Recommendation The paper presents a significant leap forward in leveraging reinforcement learning and graph attention mechanisms for novel molecular generation particularly for antiviral drug discovery. The DGAPN frameworks ability to efficiently explore the vast chemical space and generate molecules with desirable properties while ensuring synthetic accessibility is impressive and of considerable importance to the field. However addressing the mentioned weak points and questions such as demonstrating the models generalizability across different objectives and elucidating its complexity for broader accessibility would further strengthen the paper. Additionally extending validation to include pharmacological potential assessments beyond docking scores could provide a more holistic view of the generated molecules utility. Given the innovative approach strong experimental validation and potential impact on drug discovery I recommend this paper for publication. The suggested areas for improvement and clarification if addressed would make this already commendable work even more valuable to the field.", "g5odb-gVVZY": " Review of \"Multilevel Physics Informed Neural Networks (MPINNs) for Solving PDEs\"  1) Summary of the Paper The paper introduces Multilevel Physics Informed Neural Networks (MPINNs) an innovative approach for solving partial differential equations (PDEs) by leveraging the principles of multigrid (MG) methods combined with the capabilities of Physics Informed Neural Networks (PINNs). The approach decomposes the solution into coarse and fine components separately optimized and then integrated mirroring the efficiency of classical MG methods in accelerating the convergence of the solution to PDEs. The authors demonstrate through various tests that MPINNs outperform traditional PINN models in terms of accuracy sensitivity to learning rate computational effort and speed in reducing approximation error across different types of PDEs including elliptic and nonlinear equations.  2) Strengths and Weaknesses Strong Points:  Innovative Integration: The paper effectively combines the established multigrid methods with modern neural network architectures for PDE solving providing a fresh perspective to the field.  Demonstrated Efficiency: The empirical results showing MPINNs\u2019 superiority over classical PINNs in terms of learning rate sensitivity computational resources and error reduction are compelling.  Broad Implications: Success in lessening the computational load and improving accuracy has wideranging implications for the numerical solving of PDEs across various scientific and engineering domains.  Reproducibility: Providing the code as supplementary material greatly aids in verifying the claimed results and fosters further research. Weak Points:  Lack of Theoretical Underpinning: Despite practical validations the paper lacks a rigorous theoretical framework or analysis explaining why MPINNs outperform traditional PINNs beyond empirical observations.  Specificity in Experimentation: The numerical experiments while comprehensive in demonstrating advantages over classical PINNs could benefit from more diverse testing scenarios beyond the specific types of PDEs considered.  Scalability Unaddressed: There is little discussion on the scalability of the proposed MPINNs architecture particularly regarding very highdimensional problems or extremely large datasets. Questions and Feedback to the Author: 1. Could the authors provide a deeper theoretical insight or analysis into why the multilevel approach presents such significant improvements over classical methods 2. Is there evidence or potential insight into how MPINNs might perform with highdimensional PDEs or realworld datasets that pose scalability challenges 3. The paper could benefit from a more detailed discussion on the limitations of the MPINNs approach. Are there scenarios where MPINNs might not provide a significant advantage or might even underperform compared to classical PINNs 4. Further clarification on how MPINNs handle the transfer of learning between the coarse and fine neural networks would be beneficial. How critical is the choice and design of transfer operators (R and P) to the overall effectiveness of the MPINNs  3) Review Summary The paper represents a significant advancement in the field of neural networkbased solutions for differential equations by introducing MPINNs. The approach cleverly combines classical multigrid methods with modern PINN architectures demonstrating improved performance in solving PDEs with reduced sensitivity to learning rates and decreased computational effort. This work opens up promising directions for more efficient numerical solutions of complex PDEs and has the potential for broad applications in computational science and engineering. However the paper could be strengthened by providing a more robust theoretical foundation to the empirical observations. Additionally further discussions on scalability broader applicability and detailed mechanics of the coarsefine integration process could make the paper more comprehensive. Overall the contribution is notable and the presented MPINNs represent a meaningful step forward in the use of neural networks for numerical problemsolving. Despite some areas needing further clarification and exploration the paper lays the groundwork for potentially transformative applications in computational physics and beyond.", "fPhKeld3Okz": " Summarize the Paper This paper introduces a new PlugandPlay (PnP) method aimed at improving upon existing techniques in image restoration tasks like deblurring superresolution and inpainting. Traditional PnP approaches replace a regularization term with a generic denoiser often leading to issues with convergence due to the denoiser not fitting into the proximal mapping framework. The proposed method integrates a denoiser based on a gradient descent step that is parameterized by a deep neural network. This novel approach leverages halfquadratic splitting and ensures that the denoiser inherently satisfies critical convergence conditions without sacrificing denoising performance. The paper provides theoretical guarantees for convergence to stationary points of a global functional. Testing on various illposed inverse problems showcases the algorithms convergence empirically and highlights its stateoftheart performance in terms of visual and quantitative results.  Strong and Weak Points Questions and Feedback to the Author Strong Points: 1. Theoretical Foundation: A significant strength of the paper is its robust theoretical analysis providing novel convergence guarantees for PnP methods. This sets a solid framework for future research and applications. 2. Innovation in Denoising: By conceptualizing the denoiser as a gradient descent step parameterized through deep learning the authors address a major limitation of traditional PnP methods regarding the explicit objective function and convergence. 3. Empirical Validation: The thorough experimental section including comparisons with stateoftheart methods and diverse applications (deblurring superresolution inpainting) convincingly supports the proposed methods effectiveness. Weak Points: 1. Generalization of Results: While impressive the results are primarily demonstrated on image restoration tasks. The broader applicability and effectiveness of the proposed methodology in other contexts or for more varied types of imaging data remain to be further explored. 2. Complexity of Training and Evaluation: Details on the computational resources required for training and evaluating the proposed method are scant. Given the complex architecture and iterative nature of the algorithm these aspects are crucial for assessing the practical feasibility. Questions and Feedback: 1. Handling of Different Noise Levels: The paper mentions training the denoiser for Gaussian noise by minimizing the MSE loss function. How does the algorithm adapt to realworld scenarios where noise characteristics might not be accurately known or might be nonGaussian 2. Comparison with NonDeep Learning Methods: Given the emphasis on deep learningbased denoisers it would be insightful to see a discussion on how the proposed method compares or potentially integrates with traditional denoising techniques especially in contexts where deep learning approaches might not be feasible. 3. Scalability and Efficiency: The practical applicability of the proposed method would benefit from a more detailed discussion on its scalability and computational efficiency. Are there strategies in place or in development for reducing computational costs without significantly impacting performance  Review Summary The paper presents a compelling addition to the field of image restoration addressing significant challenges associated with PlugandPlay methods. The innovative approach of defining a denoiser through a gradient descent step parameterized by deep learning not only ensures theoretical convergence guarantees but also demonstrates superior empirical performance across a range of tasks. This work advances the understanding and capabilities of PnP methods offering a promising direction for future research. However the paper could benefit from a broader exploration of the methods applicability beyond the presented tasks a detailed analysis of computational requirements and strategies for handling varied and unknown noise distributions. The authors are encouraged to address these aspects to strengthen the papers contribution and applicability further. Overall the work represents a significant step forward in the field meriting attention and further study.", "zrdUVVAvcP2": " Peer Review  1) Summarize the paper The paper introduces a novel method named GrASP (Gradientbased Affordance Selection for Planning) to address the challenge of planning with continuous actionsoptions in largescale reinforcement learning (RL) environments. The authors present a technique to select a small set of affordances which are actions or options beneficial for planning. These affordances are learned through a gradientbased approach enabling the efficient exploration of action spaces without evaluating every possible action. The paper demonstrates that planning with the identified affordances and a learned valueequivalent model can achieve superior sample efficiency compared to modelfree RL approaches. The empirical evaluation includes tests in both hierarchical tasks requiring policies over continuous options and domains from the DeepMind Control Suite requiring policies over continuous primitive actions. The results show that GrASP can learn meaningful affordances that significantly improve the performance of planning agents.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: GrASP presents a unique solution to the problem of planning in continuous action spaces by discovering useful affordances which is a significant contribution to the field of modelbased reinforcement learning.  Empirical Evaluation: The comprehensive experimental setup including both hierarchical tasks and a variety of control tasks from the DeepMind Control Suite provides strong evidence for the effectiveness and versatility of GrASP.  Theoretical and Practical Relevance: The concept of gradientbased affordance selection not only has theoretical novelty but also demonstrates practical utility in improving sample efficiency and overall performance over modelfree RL baselines. Weak Points:  Comparison with StateoftheArt: Although the paper states that GrASP is not shown to be stateoftheart a direct comparison with current leading methods in the domain of continuous action spaces would have provided a clearer understanding of its relative performance and limitations.  Scalability and Complexity: The paper could benefit from a discussion on the scalability of GrASP to even larger more complex environments and its computational complexity especially in comparison with existing approaches.  Algorithm Integration: The integration of GrASP with more sophisticated modelbased RL algorithms such as MuZero is mentioned as future work. Insights into potential challenges and considerations for such integration would have been valuable. Questions and Feedback:  Could the authors elaborate on the potential challenges and strategies in scaling GrASP to more complex environments with higherdimensional action spaces  How does the computational complexity of GrASP compare with that of other modelbased and modelfree approaches in practice  The paper mentions the possibility of integrating GrASP with sophisticated algorithms like MuZero as future work. Could the authors hypothesize about the potential outcomes and difficulties of such integration  3) Review Summary This paper introduces GrASP an innovative method for selecting affordances in continuous action spaces for reinforcement learning addressing a significant challenge in modelbased RL. The proposed gradientbased approach for discovering useful actionsoptions for planning represents a novel contribution to the field enabling more efficient exploration and planning. Empirically GrASP demonstrates a clear advantage in sample efficiency and performance over modelfree RL baselines across a range of tasks. However the paper could benefit from a more direct comparison with stateoftheart methods to better understand GrASPs competitiveness. Additionally discussions on scalability computational complexity and potential integration with advanced RL algorithms would enhance the papers impact. Despite these areas for further elaboration the work undertaken presents valuable insights and advancements in addressing the complexities of continuous action spaces in reinforcement learning. The empirical evidence combined with the theoretical foundations of GrASP supports the potential of affordancebased approaches in advancing the capabilities of modelbased RL. The promise of integrating such methods with established algorithms like MuZero opens exciting avenues for future research and applications.", "xspalMXAB0M": "Summary of the Paper This paper introduces an innovative approach to tackling the computational challenges of reinforcement learning (RL) in Markov decision processes (MDPs) with large state spaces. Unlike traditional methods that rely on structural assumptions about the MDP or approximate solutions this work draws from supervised learning techniques\u2014specifically the boosting method\u2014to iteratively enhance weak learning methods. The proposed algorithm creates a potent policy by combining weak learners through a novel use of a nonconvex variant of the FrankWolfe optimization algorithm and gradient boosting techniques. This strategy enables achieving global optimality guarantees without the need for the policy to explicitly account for each state thereby sidestepping the scalability issues posed by MDPs with vast state spaces. The paper validates the efficacy of this approach by providing theoretical bounds on sample complexity and running time which are polynomial in terms of the problems natural parameters. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The paper presents a novel use of boosting techniques for RL extending a wellunderstood method in supervised learning to a new domain. This interdisciplinary approach opens up fresh avenues for research in both fields. 2. Scalability: By formulating an algorithm whose complexity does not depend on the number of states the paper addresses a critical challenge in applying RL to realworld problems with extensive state spaces. 3. Theoretical Foundations: The paper provides comprehensive theoretical backing for the proposed method giving bounds on sample complexity and running time. These guarantees are crucial for understanding the algorithms efficiency and practical applicability. 4. Handling NonConvexity: The devised solutions ability to deal with the nonconvexity of the value function over policy space offers a significant contribution as this issue is a common stumbling block in deploying RL in complex environments. Weak Points: 1. Assumption of Weak Learnability: The paper assumes access to a weak learner capable of generating approximate solutions to linear optimization objectives over policies. It may be challenging to ensure this assumption holds in practice especially in highly complex or poorly understood environments. 2. Lack of Empirical Validation: Although the paper provides theoretical proofs of the algorithms effectiveness it lacks empirical evidence or case studies to illustrate its performance on realworld problems. 3. Complexity and Practical Implementation Concerns: Despite addressing computational scalability the proposed methods implementation details could be daunting due to the intricate layering of optimization techniques (FrankWolfe and boosting). Additional guidance on practical aspects would enhance the papers usefulness. Questions and Feedback: 1. Empirical Evidence: Can the authors provide evidence of the algorithms performance on benchmark RL problems or realworld scenarios This would greatly strengthen the case for the practical applicability of the proposed method. 2. Handling Exploration: While the paper mentions statespace exploration in the conclusion it is unclear how exploration is effectively integrated into the boosting framework. Could the authors elaborate on potential strategies for incorporating exploration effectively 3. Simplification and Optimization: Are there opportunities to simplify the boosting algorithm without significantly compromising its effectiveness Simplifications could make the approach more accessible to practitioners. 4. Broader Impact and Applications: How does the proposed method compare to existing RL techniques in terms of environmental or societal impacts particularly in sensitive applications like autonomous driving or healthcare Review Summary This paper makes a significant theoretical contribution to the field of RL by ingeniously applying boosting a technique from supervised learning to develop an efficient algorithm for MDPs with large state spaces. The approach cleverly circumvents the computational scalability challenge a notorious obstacle in applying RL to realworld problems. Theoretical underpinnings including bounds on sample complexity and running time are robust and constitute a strong foundation for future work. However the paper could be improved by addressing several issues. Most notably it lacks empirical results that demonstrate the proposed algorithms effectiveness on concrete RL problems. Moreover the assumption of weak learnability might restrict the methods applicability in complex realworld scenarios where such weak learners are not readily available or identifiable. The discussion on integrating exploration into the algorithm remains abstract and warrants further clarification and elaboration. In conclusion this paper posits a promising direction for RL research especially in dealing with large state spaces. Expanding the work to include empirical validations addressing practical implementation concerns and further exploring the integration of statespace exploration techniques could make this innovative approach a cornerstone in the advancement of RL methodologies.", "hfU7Ka5cfrC": " Review of \"Scalable OnePass Optimisation of HighDimensional WeightUpdate Hyperparameters by Implicit Differentiation\"  Summary of the Paper This paper presents an innovative algorithm designed to optimize hyperparameters relevant to the learning rate weight decay momentum and individual parameter learning rates with a single training pass and without necessitating multiple restarts. It expands on the work by Lorraine et al. (2020) by enabling the application of approximate hypergradient optimization to any continuous hyperparameter involved in differentiable weight updates. The method leverages theoretical insights from implicit function theorem (IFT) and employs a practical approximation to enable efficient and tractable hyperparameter optimization. The authors provide comprehensive empirical validation across several datasets demonstrating the method\u2019s competitive performance and efficiency which while exceeding that of vanilla training offers significant computational savings compared to methods requiring multiple training reruns.  Strong Points 1. Novelty and Theoretical Foundation: The paper introduces a novel approach to hyperparameter optimization that significantly reduces the computational burden by avoiding repetitive training episodes. The theoretical foundation particularly the use of IFT and the development of an approximate hypergradient method is compelling and welljustified. 2. Comprehensive Empirical Validation: The authors validate their approach across a variety of datasets and model architectures demonstrating its versatility and effectiveness. The inclusion of a comparative analysis with both traditional and contemporary benchmarks enriches the paper\u2019s empirical contribution. 3. Practicality and Efficiency: The proposed method\u2019s ability to operate within only 23 times the computational cost of vanilla training without sacrificing performance is particularly noteworthy. This practical efficiency makes the technique attractive for a wide range of applications.  Weak Points 1. Clarity on Implementation Details: While the paper provides a general overview of the algorithm\u2019s implementation it could benefit from more detailed descriptions of the optimization process particularly regarding the adjustments made for hypergradient computation and the management of computational overhead. 2. Analysis of MetaHyperparameters: The paper mentions the use of metahyperparameters (e.g. metalearning rate \u03ba) but does not delve into how these were chosen or their impact on the optimization process. A deeper analysis or discussion on this aspect could enhance the paper\u2019s thoroughness. 3. Discussion on Convergence Properties: The paper provides a motivating argument for convergence based on IFT but lacks a detailed theoretical analysis or proof of convergence for the proposed algorithm. An exploration of the conditions under which convergence is guaranteed or the rate of convergence would strengthen the paper.  Questions and Feedback to the Author 1. Implementation Details: Could the authors provide more insight into how the computational overhead is managed especially regarding the calculation of approximate hypergradients and their integration into the optimization loop 2. MetaHyperparameters: How were the metahyperparameters such as the metalearning rate determined Is there a sensitivity analysis to understand their influence on the optimization performance 3. Convergence Analysis: Could the authors elaborate on the theoretical properties of the proposed method specifically focusing on convergence guarantees or conditions under which the method is expected to perform optimally  Review Summary The paper introduces a significant advancement in the domain of hyperparameter optimization offering a scalable and efficient method that addresses the computational challenges of existing approaches. The theoretical basis of the method grounded in implicit function theorem and its practical application through approximating hypergradients for a broad range of hyperparameters is innovative and compelling. The empirical validation across multiple datasets and model architectures underscores the method\u2019s effectiveness and versatility. However the paper could be improved by providing more detailed information on implementation specifics including the management of computational overhead and the role of metahyperparameters in the optimization process. Furthermore a deeper theoretical analysis regarding the convergence properties of the algorithm would substantially strengthen the paper. Overall the paper presents a valuable contribution to the field of machine learning particularly in the automated optimization of hyperparameters. Its practical efficiency combined with its theoretical underpinning makes it a noteworthy development with potential for widespread applicability in machine learning workflows.", "gzeruP-0J29": " Review of \"FASTBAT: Fast BiLevel Adversarial Training for Improved Stability and Robustness\"  Summary of the Paper This paper introduces a novel approach to adversarial training (AT) known as FASTBAT (Fast BiLevel Adversarial Training) which addresses the computational inefficiency stability issues and catastrophic overfitting problems inherent in traditional AT methods. The authors propose leveraging bilevel optimization (BLO) to reformulate the adversarial training problem moving away from a minmax optimization to a more structured and theoretically grounded framework. By interpreting the conventional FASTAT through the lens of BLO the authors show that it can be seen as a linearized BLO problem. The FASTBAT method is articulated on this foundation offering a systematic and effective fast BLOoriented AT framework which unlike its predecessors achieves notable improvements in model robustness stability and SARA (Standard Accuracy  Robust Accuracy) tradeoff without the need for explicit robust regularization methods during training. The paper presents extensive experimental results demonstrating the superiority of FASTBAT over stateoftheart Fast AT baselines in various settings including different datasets (CIFAR10 and ImageNet) model architectures and adversarial attack scenarios.  Strong Points 1. Theoretical Contribution: The paper makes a significant theoretical contribution by establishing a connection between adversarial defense and BLO. This novel perspective allows for a more structured approach to AT moving beyond the limitations of minmax optimization. 2. Improved Performance: FASTBAT demonstrates superior performance in terms of stability mitigating catastrophic overfitting and enhancing the accuracyrobustness tradeoff. The extensive experimental results provide strong evidence of its effectiveness across different datasets models and attack budgets. 3. No Need for Explicit Robust Regularization: One of the notable achievements of FASTBAT is its ability to achieve improvements in stability RA and SA without resorting to explicit robust regularization. This stands in contrast to methods like FASTATGA making FASTBAT potentially more practical and easier to implement.  Weak Points and Questions 1. Generalizability and Scalability: Although the paper presents convincing experimental results on CIFAR10 and ImageNet datasets with specific model architectures it remains unclear how generalizable FASTBAT is across a wider range of datasets and architectures particularly in domains other than image classification. 2. Computational Complexity Analysis: The paper positions FASTBAT as computationally efficient compared to traditional AT methods. However a detailed analysis of its computational complexity especially in comparison to FASTAT and FASTATGA is not provided. Furthermore insights into the computational tradeoffs introduced by the implementation of BLO would be valuable. 3. Hyperparameter Sensitivity: While the paper mentions the importance of specific hyperparameter setups for successful implementation it does not provide a thorough sensitivity analysis. Understanding the influence of these parameters on the performance of FASTBAT would enhance the papers contribution.  Feedback and Questions to the Author 1. Generalizability: Can the authors provide insights or results on the application of FASTBAT in domains other than image classification or on datasetsarchitectures not covered in the paper 2. Computational Complexity: Could the authors elaborate on the computational complexity of FASTBAT in comparison to existing methods providing empirical or theoretical analysis 3. Hyperparameter Sensitivity: Would it be possible to include a more detailed sensitivity analysis of crucial hyperparameters particularly the regularization parameter (\u03bb) and the learning rate for the IGinvolved descent term  Review Summary This paper introduces an innovative approach to adversarial training FASTBAT leveraging BLO for improved stability efficiency and robustness in modeling. The theoretical grounding and experimental validation are strong showcasing promising results across various metrics and settings. However there are areas where the paper could be enhanced including a more detailed discussion on the computational complexity generalizability across domains and hyperparameter sensitivity. Addressing these questions would strengthen the papers contribution and provide a more comprehensive understanding of FASTBATs practical implications and limitations. Overall given the importance of robust machine learning models and the challenges associated with adversarial training FASTBAT represents a meaningful advancement in the field.", "mHu2vIds_-b": " Peer Review of \"Theoretical and Empirical Evaluation of Randomized Smoothing for Ensembles\"  1. Summary The paper delves into enhancing the effectiveness of Randomized Smoothing (RS) for obtaining robustness certificates in deep neural networks by leveraging ensembles as base models. The authors provide both theoretical motivation and empirical evidence to support the claim that ensembles can significantly reduce variance induced by perturbations in RS leading to more consistent classifications and consequently larger certifiable radii. Key contributions include a novel softensemble scheme that amplifies the probabilistic robustness guarantees an adaptive sampling technique that substantially lowers sample complexity and extensive experimental validation demonstrating stateoftheart performance on CIFAR10 and ImageNet datasets. The paper is wellstructured presenting a comprehensive review of related work detailed theoretical underpinnings and a variety of empirical evaluations that underscore the benefits of the proposed methodologies.  2. Evaluation Strong Points:  Theoretical Foundation: The paper offers a solid theoretical framework that not only explains the utility of ensembles in reducing variance under RS but also articulates how this leads to improvements in certifiable radii. The mathematical rigor adds substantial credibility to the claimed contributions.  Empirical Validation: A wide range of experiments across different settings model architectures and datasets emphatically illustrate the effectiveness of the proposed methods. The achievements of new stateoftheart results in terms of average certified radius (ACR) are particularly notable.  Practical Relevance: By addressing the computational inefficiency of ensembles through an adaptive sampling scheme and a KConsensus aggregation mechanism the paper contributes practical solutions that could facilitate wider adoption in safetycritical applications. Weak Points:  Limited Discussion on Limitations: While the paper is comprehensive it lacks a dedicated discussion on potential limitations or scenarios where the proposed methods might underperform. Understanding these situations is vital for practitioners.  Generalizability Concerns: The paper focuses primarily on CIFAR10 and ImageNet. Extending the experimental analysis to include more diverse datasets especially those with distinct characteristics or higherdimensional data could strengthen the claims.  Comparative Analysis: While the paper presents an indepth comparison with the stateoftheart it could benefit from a more detailed discussion on how the proposed methods interact with or improve upon existing ensemble techniques specifically designed for RS. Questions and Feedback:  Can the authors elaborate on any observed or theoretical limitations of their approach especially in cases of extremely highdimensional data or under different adversarial attack methodologies  How might the proposed adaptive sampling and KConsensus aggregation mechanisms perform or adapt in a realtime inference scenario where computational resources are dynamically changing  Given the focus on CIFAR10 and ImageNet does the efficacy of the proposed methods hold for datasets with intrinsically different characteristics such as text or timeseries data  3. Review Summary The paper makes a significant contribution to the field of adversarial robustness by effectively leveraging ensemble techniques within the randomized smoothing framework to enhance the certifiable robustness of deep learning models. The theoretical motivation coupled with extensive empirical validation attests to the efficacy of the proposed methods in reducing variance increasing certified radii and achieving stateoftheart performance all while addressing computational efficiency concerns. Nevertheless a discussion on the limitations broader applicability and a more exhaustive comparative analysis could further enrich the paper. Despite these points the novel contributions particularly in the adaptive sampling and softensemble schemes represent valuable advancements with practical implications for enhancing the reliability and safety of deep learning in adversarial settings.", "hgKtwSb4S2": " Peer Review for \"Generalized Randomized SVD for Matrices and Hilbert\u2013Schmidt Operators\"  Summary of the Paper This paper extends the computational methodology of randomized Singular Value Decomposition (SVD) beyond its traditional constraints enabling a more nuanced analysis of matrices and Hilbert\u2013Schmidt (HS) operators. Traditionally randomized SVD leverages standard Gaussian random vectors for constructing lowrank approximations of matrices. This research innovates on two fronts: firstly by introducing multivariate Gaussian vectors with tailored covariance matrices to refine approximations using prior knowledge and secondly by applying the methodology to the continuous domain of HS operators via Gaussian Process (GP) drawn functions and a novel covariance kernel based on weighted Jacobi polynomials. This approach facilitates efficient sampling from the GP and offers control over the smoothness of generated functions. The paper provides theoretical bounds demonstrating the efficacy of these generalizations and showcases numerical examples that validate the theoretical improvements.  Strong Points 1. Theoretical Innovation: The introduction of a generalized randomized SVD that incorporates prior knowledge through tailored covariance matrices is a significant theoretical advancement. By expanding the algorithm to HS operators and utilizing GPs the paper breaks new ground in the field of numerical linear algebra and machine learning. 2. Practical Relevance: The devised algorithm has considerable practical implications particularly in domains requiring lowrank approximations of large matrices or the learning of integral kernels in the context of PDEs. The covariance kernel based on weighted Jacobi polynomials is especially promising for applications in PDE learning. 3. Numerical Validation: The paper features comprehensive numerical experiments that not only demonstrate the practical applicability of the theoretical concepts but also offer insight into the performance of the algorithms. The experiments corroborate the theoretical findings and provide a strong empirical foundation for the proposed methodologies.  Weak Points and Questions 1. Computational Complexity: While the numerical examples validate the algorithms effectiveness there is a lack of detailed discussion on computational complexity particularly in comparison to existing methods. How does the computational time of the proposed generalizations compare to the standard randomized SVD especially for largescale problems 2. Robustness to Noise: The paper assumes ideal conditions. There is a limited discussion on how noise in the data affects the approximation accuracy and the stability of the proposed algorithms. How robust are these methods to observational noise especially when dealing with realworld data 3. Extension to Unbounded Operators: The limitations section mentions that the methodology does not extend to unbounded operators. Could the authors speculate on potential approaches or methodologies that might overcome this limitation  Review Summary This paper makes significant theoretical and practical contributions to the field of numerical linear algebra by generalizing the randomized SVD algorithm for enhanced lowrank approximations of matrices and by introducing its application to HS operators through a novel GP approach. The innovations presented particularly in incorporating prior knowledge and designing a new GP covariance kernel are commendable and lay the groundwork for future research. However the paper would benefit from a deeper analysis of computational complexity a broader discussion on the algorithms robustness to noise and some speculative remarks on addressing current limitations such as the extension to unbounded operators. Overall the strengths of this paper\u2014its theoretical innovations practical relevance and robust numerical validation\u2014outweigh its weaknesses. With additional details on computational aspects and noise robustness this research could significantly impact both the academic community and industry applications reliant on matrix approximations and integral kernel learning. The introduction of the generalized randomized SVD to HS operators and the efficient GP sampling method are particularly promising offering new tools and frameworks for tackling complex problems in computational mathematics and engineering.", "metRpM4Zrcb": " 1) Summarize the paper This paper presents a novel approach to continual learning (CL) aimed at addressing the challenge of catastrophic forgetting in deep neural networks (DNNs). By introducing a lowrank filter subspace decomposition technique the method proposes learning a new set of filter atoms for each task within convolutional neural networks (CNNs) while maintaining shared subspace coefficients across tasks. This approach dubbed as filter atom swapping enables the efficient storage and retrieval of taskspecific knowledge with a significantly reduced memory footprint. The effectiveness of this method is empirically demonstrated across various benchmark datasets showing superior performance in both accuracy and scalability compared to stateoftheart methods. Additionally the paper explores the use of both intertask and intratask model ensemble techniques which further enhance the models performance by leveraging past and withintask knowledge respectively.  2) Strong and Weak Points Question and Feedback to the author Strong Points:  Innovative Approach: The idea of decomposing convolutional filters into a set of filter atoms and enforcing a subspace with shared coefficients across tasks is original and addresses the challenge of catastrophic forgetting effectively.  Scalability: The method achieves stateoftheart results on various datasets while significantly reducing the memory footprint required for storing taskspecific knowledge making it highly scalable for realworld applications.  Empirical Validation: The paper provides comprehensive empirical evidence demonstrating the effectiveness of the proposed method. The inclusion of both intertask and intratask ensemble strategies and their impact on performance adds depth to the validation.  Theoretical Analysis: The paper includes a theoretical exploration with an excess risk bound analysis which helps in understanding the approachs theoretical foundation and its expected behavior in continual learning settings. Weak Points:  Complexity in Implementation: The complexity involved in implementing filter atom swapping and managing atom memory may be high although this is not thoroughly discussed. More insights into the practical challenges and computational costs would be beneficial.  Lack of Comparison on More Diverse Tasks: While the paper validates the approach on several benchmark datasets an evaluation across more diverse realworld tasks could further establish the methods efficacy and applicability.  Limited Discussion on Limitations: The paper could benefit from a more detailed discussion on the potential limitations or scenarios where the proposed method might not perform optimally. Questions and Feedback to the Author:  Could you elaborate on the computational overhead introduced by the filter atom swapping mechanism especially in scenarios with a large number of tasks  How does the method handle tasks with significantly different complexities or data distributions Is there a risk of subspace dilution where the shared subspace might not fully capture the specificity of each task  It would be valuable to see comparisons or potential integration with other continual learning strategies such as fewshot learning or metalearning. Have you explored such directions or do you plan to  3) Review Summary The presented paper introduces a promising approach to continual learning that creatively mitigates catastrophic forgetting through a filter atom swapping mechanism within CNNs. By learning taskspecific filter atoms and maintaining shared subspace coefficients across tasks the method not only ensures efficient knowledge retention but also significantly reduces the memory footprint required for model archiving. Empirically the approach demonstrates superior performance compared to existing stateoftheart methods further enhanced by the application of model ensemble strategies. Despite the innovation and strong empirical results the paper could provide a deeper dive into the methods computational complexity its adaptability to diverse tasks and a broader discussion on potential limitations. Addressing these areas could enhance the papers impact and its contribution to the field of continual learning. Overall the novel methodology and the promising results presented make a significant contribution to the continual learning landscape paving the way for future research and realworld applications in efficiently managing knowledge over sequential tasks.", "wMXYbJB-gX": " Peer Review of \"Studying the Impact of Label Smoothing Regularization on the Convergence of Stochastic Gradient Descent in Deep Learning\"  1) Summary The paper presents a comprehensive examination of Label Smoothing Regularization (LSR) within the context of training deep neural networks utilizing stochastic gradient descent (SGD). Importantly it introduces a novel algorithm TwoStage LAbel smoothing (TSLA) designed to enhance the training process by employing LSR in the initial stage and transitioning to onehot labels in the succeeding stage. The authors provide a theoretical analysis of how LSR influences the convergence behavior of SGD demonstrating that appropriate application of LSR can significantly reduce variance in gradient estimation thereby accelerating the convergence process in the early stages of training. However it may retard the process as optimization approaches completion. To mitigate this TSLA is proposed proven theoretically and validated through extensive experimentation to improve both the convergence rate and generalization performance on benchmark datasets.  2) Strong and Weak Points Questions and Feedback Strong Points: a) Novelty and Theoretical Depth: The paper is the first to provide a theoretical analysis of LSR\u2019s impact on the convergence of SGD in deep learning addressing a significant gap in the literature. The introduction of TSLA supported by theoretical proof and experimental validation is both novel and practical. b) Comprehensiveness and Clarity: The paper does an excellent job of detailing related work the limitations of current knowledge and the steps taken to expand this understanding. The methodology theoretical analysis and experimental setup are clearly explained and wellstructured. c) Practical Impact: The ability of TSLA to improve model generalization and convergence speed has significant implications for the practical training of deep learning models making it a valuable contribution to the field. Weak Points: a) Lack of Real World Applications: While the paper provides empirical evidence from benchmark datasets it does not elaborate on the application of the proposed method to realworld scenarios or problems. This limits the scope of understanding its potential impact. b) Limited Discussion on Parameter Sensitivity: The paper could benefit from a more extensive discussion on the sensitivity of TSLA\u2019s performance to the choice of its hyperparameters particularly the label smoothing strength (\u03b8) and the decision point for switching from LSR to onehot labels. c) Comparison with Other Regularization Techniques: The paper focuses on LSR but does not provide a comprehensive comparison with other contemporary techniques that might compete with or complement TSLA. Questions and Feedback: 1. Can the authors elaborate on how TSLA performs in conjunction with other regularization and optimization methods commonly employed in training deep networks 2. How sensitive is TSLA to the selection of hyperparameters and is there a recommended strategy for their selection 3. Would TSLAs benefits extend to other optimization challenges in deep learning such as training instability or very deep networks susceptible to vanishing gradients Feedback: It would enhance the papers impact by including discussions or experiments comparing TSLA with other stateoftheart regularization methods shedding light on its practical implementation in a broader range of applications.  3) Review Summary and Recommendation The paper makes a substantial theoretical and practical contribution by analyzing the impact of label smoothing regularization on SGD convergence in deep learning and proposing the TSLA algorithm. The novel contributions backed by rigorous theoretical analyses and empirical validations are compelling. However the paper could be significantly strengthened by addressing its limitations regarding realworld applications parameter sensitivity analysis and comparison with other regularization methods. Recommendation: Accept with minor revisions. Addressing the aforementioned weak points particularly by broadening the discussion on parameter sensitivity and comparative analysis with other techniques would make this paper a valuable addition to the literature on deep learning training strategies.", "in1ynkrXyMH": " Review of \"Introduction of Introspective Learning in Neural Networks for Improved Generalization and Calibration\"  Summary: The paper presents a novel twostage neural network framework termed \"introspective learning\" aiming to enhance the generalization capabilities and calibration of neural networks. It differentiates itself by implementing an initial feedforward sensing stage followed by a reflective stage where the network reevaluates its initial decision by contemplating all possible outcomes. The reflection stage leverages gradients of trained neural networks to extract introspective features which are then visualized posthoc for justification. Introspective learning demonstrated a 4 improvement in robustness and a 42 reduction in calibration errors on noisy datasets compared to traditional singlestage learning networks. The methodology and its implications were evaluated through various applications such as active learning outofdistribution detection and image quality assessment illustrating the approachs versatility and effectiveness in enhancing network predictions under generalization and calibration criteria.  Strengths and Weaknesses Strong Points: 1. Innovative Concept: The introduction of a reflection stage in the neural networks decisionmaking process is innovative and addresses the issue of overfitting and generalization directly. This mirrors human cognitive processes providing an intuitive basis for the approach. 2. Robustness and Calibration: The empirical results showcasing improvements in robustness and calibration errors are significant. The methods ability to improve network performance on noisy data is particularly noteworthy. 3. Versatility: The paper successfully demonstrates the methods application across several very different tasks indicating its versatility and potential for a wide range of applications in machine learning. 4. Visualization of Introspective Features: The idea of visually presenting the introspective features for human interpretation helps bridge the gap between human understanding and model decisionmaking aiding in model explanation and transparency. Weak Points: 1. Complexity and Scalability: The implementation of a twostage process specifically the reflection stage that requires computing gradients for introspection could significantly increase the computational overhead affecting the models scalability and efficiency. 2. Generalization of Results: While the paper provides specific instances where introspective learning outperforms traditional methods it lacks a broader validation across more datasets and different network architectures to fully establish the generalizability of the approach. 3. Lack of Theoretical Foundation: The paper could benefit from a more indepth theoretical analysis of why and how the reflection stage improves model performance providing insights into the conditions and settings where this approach would be most effective. Questions and Feedback: 1. Computational Complexity: How does the additional computational overhead of the reflection stage impact the overall efficiency and scalability of the approach particularly when applied to larger and more complex datasets 2. Broad Applicability: Can the authors provide further validation of the introspective learning approach across a wider range of datasets and network architectures to solidify its applicability and robustness 3. Theoretical Insights: Could the paper include a deeper theoretical analysis or discussion on the principles governing the observed improvements in generalization and calibration due to the reflection stage  Review Summary: The paper introduces a compelling concept of introspective learning which significantly enhances neural networks generalization capabilities and calibration by integrating a novel reflection stage into the decisionmaking process. This twostage process inspired by human cognitive reflection not only improves the robustness and accuracy of the models on noisy data but also makes strides towards more interpretable and transparent machine learning models through the visualization of introspective features. Despite these strengths the approach\u2019s scalability computational efficiency and broader applicability need further exploration and validation. Additionally a deeper theoretical understanding of why the reflection stage impacts the networks positively would enrich the paper. Supporting arguments for the recommendation come from the novel methodological contribution significant improvements in model performance and the approach\u2019s potential applicability across various machine learning tasks. However recommendations for future work include addressing the computational overhead concerns and broadening the empirical validation to establish the method\u2019s generalization and efficiency robustly.", "l4IHywGq6a": " PeerReview  1) Summarize the paper This paper addresses the challenge of generating physically synthesizable molecules with limited training data commonly encountered in classspecific chemical datasets. To overcome the limitations of deep learning (DL) methods that require large datasets the authors propose a novel dataefficient generative model leveraging a learnable graph grammar. This grammar automatically constructs molecules from a sequence of production rules optimizing various chemical metrics (e.g. distribution statistics and synthesizability) while ensuring the molecules meet specific chemical constraints. The model is capable of learning from significantly smaller datasets compared to traditional DL methods and demonstrates stateoftheart results in generating highquality molecules for monomer classes with as few as \\xe2\\x88\\xbc20 samples. In addition the model shows competitive performance in polymer generation tasks with only 117 samples challenging existing models trained on datasets with tens of thousands of points. The strength of this model lies in its ability to incorporate domainspecific knowledge directly into the grammar construction process leading to highquality molecular predictions.  2) List strong and weak points of the paper Questions and Feedback to the author Strong Points: 1. Innovative Approach: The proposed method of integrating domainspecific optimizations within the grammatical construction of molecules is novel and addresses the significant challenge of data scarcity effectively. 2. Data Efficiency: The models ability to learn from datasets orders of magnitude smaller than those required by traditional DL methods is highly beneficial for areas where data collection is costly and timeconsuming. 3. DomainSpecific Knowledge Integration: Allowing the incorporation of additional chemical knowledge directly into the graph grammar optimization process enhances the models relevance and applicability to specialized molecular synthesis tasks. 4. Explainability and Reproducibility: The use of a graph grammar improves explainability over traditional blackbox DL models and the availability of code enhances reproducibility. Weak Points: 1. Complexity and Accessibility: The methods conceptual and technical complexity may limit its accessibility to researchers without extensive backgrounds in graph grammars or optimization techniques. 2. Scalability Concerns: While the paper demonstrates effectiveness on relatively small datasets the scalability of the method to larger more complex molecular systems or diverse chemical spaces is not thoroughly explored. 3. Comparative Analysis: Although the model outperforms existing methods on small datasets a more comprehensive comparison across a broader range of molecular generation tasks would better position the model within the current landscape of generative models. 4. Practical Deployment: The paper lacks discussion on the practical implications of deploying such a model in realworld scenarios including computational costs and integration with existing drug discovery pipelines. Questions and Feedback: 1. Can the authors elaborate on the potential scalability of the proposed method to more complex or larger chemical spaces 2. It would be beneficial if the paper included case studies or examples of practical applications in drug discovery or material science to highlight the models realworld utility. 3. A deeper analysis comparing the proposed method with existing models on a broader range of tasks would provide a clearer understanding of its strengths and limitations.  3) Review Summary The paper presents a highly innovative approach to the generation of physically synthesizable molecules using a dataefficient generative model based on learnable graph grammars. This model addresses a critical challenge in the field of molecular generation by effectively learning from significantly smaller datasets than required by current deep learning methods integrating domainspecific knowledge directly into the optimization process and offering improved explainability. The results demonstrate remarkable success in generating highquality molecules for specific monomer classes with small sample sizes as well as competitive performance in more demanding polymer generation tasks. While the papers strengths such as innovation in methodology and effectiveness in datalimited scenarios are significant there are areas that could be improved for a more comprehensive understanding of the models capabilities and limitations. Specifically exploring scalability providing practical application examples and performing broader comparisons with existing methods would enhance the papers impact. Overall the proposed method represents a significant advancement in molecular generation particularly in scenarios where data scarcity is a limiting factor. The integration of domainspecific knowledge into the models learning process and its ability to perform competitively with far less training data than current stateoftheart methods mark a substantial contribution to the field.", "fOsN52jn25l": " Summarize the Paper The paper introduces the Dual Lottery Ticket Hypothesis (DLTH) which complements the original Lottery Ticket Hypothesis (LTH) by proposing that randomly selected subnetworks from a randomly initialized dense network can be transformed into wellperforming sparse networks without the need for pretraining followed by iterative pruning. The transformation leverages a technique called Random Sparse Network Transformation (RST) which employs a gradually increasing regularization term facilitating the information extrusion from weights set to be masked effectively enhancing the target sparse structures learning capacity. Their approach is validated through extensive experiments across various datasets and compared against other strong baselines including LTH itself. The paper posits that DLTH could inspire new methods for effective sparse network training.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty: The DLTH presents an innovative perspective on sparse network training that sidesteps the computationally expensive pretraining and pruning stages of LTH. This could significantly reduce the training time and computational resources required. 2. Comprehensive Experimental Validation: The paper extensively validates the DLTH approach across multiple datasets and compares it against not only LTH but also strong baselines. This thorough validation underscores the effectiveness of the proposed method. 3. Resource Efficiency: The RST techniques design to utilize a regularization term that facilitates information extrusion from the weights to be masked is both novel and resourceefficient potentially making DLTH applicable in resourceconstrained environments. Weak Points: 1. Lack of Theoretical Insight: The paper lacks a deep theoretical analysis explaining why the DLTH works effectively. Providing a theoretical foundation could strengthen the hypothesis and help in understanding the limitations and scope of DLTH. 2. Limited Exploration of Hyperparameter Impact: While the paper details the experiment setups there seems to be limited discussion on how different hyperparameters (such as the increase rate of the regularization term) impact the effectiveness of RST and consequently the validation of DLTH. 3. Scalability Concerns: Although DLTH shows promising results on the datasets tested theres insufficient discussion on its scalability to larger more complex datasets or different network architectures beyond those tested. Questions and Feedback: 1. Can the authors provide more theoretical insights or intuition on why DLTH particularly RST is effective in transforming randomly selected subnetworks into highperforming sparse networks 2. How sensitive is the DLTH especially the RST method to the choice of hyperparameters such as the regularization terms increase rate Some ablation studies focusing on these hyperparameters could provide deeper insights. 3. The paper would benefit from a discussion on the scalability of DLTH and RST methods to more complex datasets or different architectures. Are there any limitations encountered or expected when scaling up 4. How does the performance of DLTH vary with different definitions of \"randomly selected subnetworks\" For instance does the methods effectiveness change significantly if certain rules are applied in the selection process  Review Summary This paper introduces and validates the Dual Lottery Ticket Hypothesis (DLTH) a significant step forward in sparse neural network training proposing that randomly selected subnetworks can be transformed into wellperforming networks without the computationally expensive process of pretraining and pruning required by the original LTH. The novel Random Sparse Network Transformation (RST) method is a key innovation showing promising results across several datasets and benchmarks. However the paper could be strengthened with a deeper theoretical understanding of why DLTH works and a thorough exploration of hyperparameter impact on the effectiveness of RST. Additionally discussions on the scalability of this approach and its applicability to different network architectures or more complex datasets would highly benefit the paper. Despite these few shortcomings the paper presents a compelling concept backed by solid experimental evidence paving the way for more efficient and flexible methods in sparse network training.", "s2UpjzX82FS": " Review of \"A Novel MultiTask Distributed Learning Framework Using TaskAgnostic Vision Transformer\"  Summary This paper introduces a new framework for distributed collaborative learning (DCL) particularly focusing on image processing applications across multiple clients performing different tasks without sharing data. The proposed method named Taskagnostic Vision Transformer (TAViT) leverages the Vision Transformer (ViT) architecture to create a split network consisting of taskspecific heads and tails at the client side and a shared taskagnostic Transformer body on the server side. This setup enables clients to learn taskspecific features locally while benefiting from a global feature representation learned centrally. An alternating training strategy is proposed for the concurrent training of taskspecific and taskagnostic components enhancing performance across various tasks while preserving data privacy. The effectiveness of TAViT is demonstrated through experiments with multiple image processing tasks like deblocking denoising deraining and deblurring achieving superior results compared to existing deep learning and distributed learning methods.  Strong Points 1. Innovative Approach: The paper presents a novel integration of ViT with distributed learning particularly for handling multiple diverse tasks across clients. This approach is commendable for its potential to advance multitask learning and privacypreserving methods in distributed settings. 2. Comprehensive Experimental Evaluation: The authors conducted extensive experiments across various image processing tasks demonstrating the methods effectiveness through improvements in performance metrics (PSNR and SSIM) over existing methods. The inclusion of results on medical image data further underlines the method\u2019s applicability to sensitive domains. 3. Communication Efficiency: The paper highlights the communication efficiency of TAViT a critical aspect of distributed learning scenarios especially in FL. By detailing the communication cost and comparing it with classical FL methods the authors make a strong case for TAViT\u2019s practical application.  Weak Points 1. Reproducibility Concerns: While the paper provides a link to the source code further details on the hyperparameters the specific data augmentation techniques used and additional instructions for replicating the experimental settings would enhance reproducibility. 2. Limited Discussion on Privacy Implications: The authors mention the potential privacy risks and suggest directions for incorporating privacypreserving techniques. However a more detailed analysis or empirical evaluation of privacy risks (e.g. membership inference attacks) would provide a clearer understanding of the methods robustness against such vulnerabilities. 3. Scalability Analysis Lacking: The paper demonstrates results with a relatively small number of clients. An analysis of TAViT\u2019s scalability particularly its performance and efficiency in a largescale distributed learning scenario with hundreds or thousands of clients is missing.  Questions and Feedback for the Author 1. Could you provide more details on the data augmentation techniques used in your experiments Were all tasks subjected to the same augmentation methods or were they taskspecific 2. In the ethics section you briefly mention potential privacy attacks and countermeasures like differential privacy. Have you conducted any preliminary tests on integrating such techniques into TAViT and if so what were the impacts on learning performance and privacy guarantees 3. The paper would benefit from a more detailed discussion on the scalability of TAViT. What are the expected challenges and solutions when scaling up the framework to support a high number of clients and tasks  Review Summary The paper introduces TAViT a pioneering framework that integrates Vision Transformer with distributed learning to enable multitask learning across clients without data sharing. This work stands out for its novel approach comprehensive experimental evaluation and attention to communication efficiency pushing the boundaries of current DCL methods. Despite its strengths the work could be improved with more details on reproducibility a deeper analysis of privacy implications and an exploration of scalability aspects. Given the significant contributions and the potential impact of this research I recommend acceptance provided that the authors address the points of concern particularly around reproducibility and privacy risk analysis.", "h0OYV0We3oh": " 1. Summarize the Paper The paper introduces a novel slotbased autoencoder architecture named SLATE which merges the strengths of DALL\u00b7Es compositionbased systematic generalization in image generation with objectcentric representation learning aiming for systematic generalization in zeroshot image generation without relying on text. Unlike the pixelmixture decoders used in prior objectcentric models SLATE employs an Image GPT decoder conditioned on slots enabling it to better capture complex interactions among slots and pixels. Through extensive experiments the authors demonstrate SLATEs superior performance in indistribution and zeroshot (outofdistribution) image generation settings as well as its ability to achieve finer slotattention structure compared to models based on mixturedecoders. The paper posits SLATE as the first textfree model akin to DALL\u00b7E and highlights its simplicity and ease of implementation urging further exploration and utilization in future objectcentric models.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Novel Architecture: SLATE ingeniously combines the strengths of DALL\u00b7E and objectcentric representation without requiring textual input for zeroshot image generation addressing a significant gap in the field.  Robust Experimental Validation: The comprehensive experiments spanning various datasets effectively showcase SLATEs superior capability in handling both indistribution and outofdistribution image generation tasks setting a strong empirical foundation.  Simplicity and Implementability: The models simple architecture coupled with detailed documentation and publicly available code promotes reproducibility and further innovation in the community.  Weak Points:  Lack of Comparative Analysis: While SLATEs superiority is demonstrated a deeper comparative analysis with more contemporary models using objectcentric learning or other textfree image generation approaches would enrich the papers context.  Potential Ethical Concerns: The paper briefly touches on ethical considerations related to future realistic image generation capabilities but lacks a thorough discussion on mitigating potential misuses such as creating deepfakes.  Explanatory Gaps: The explanation of how SLATE resolves specific challenges associated with slotdecoding and pixel independence in zeroshot settings would benefit from more indepth theoretical discussion making the models contributions clearer.  Questions and Feedback:  Model Generalizability: Given the impressive results on the datasets used how well does the SLATE model generalize to more complex and diverse natural image datasets  Computational Efficiency: How does the computational efficiency of SLATE compare to DALL\u00b7E and other objectcentric models particularly regarding training time and resource requirements  Further Applications: Could SLATEs architecture be extended to tasks beyond image generation such as video generation or 3D rendering Insights into potential applications would highlight the models versatility.  Ethical Safeguards: What measures can be integrated into SLATEs development and deployment to prevent potential misuses and how can the AI community ensure responsible use of such advanced image generation technologies  3. Review Summary SLATE emerges as a groundbreaking model that adeptly fuses the systematic generalization capabilities of DALL\u00b7E with the objectcentric representational approach all while eliminating the need for text inputs in zeroshot image generation tasks. Its demonstrated superiority in handling diverse image generation scenarios coupled with its straightforward and reproducible architecture sets a new benchmark in the field. However the paper would benefit significantly from a more thorough comparative analysis a detailed ethical discussion and an indepth theoretical exploration of how SLATE navigates the mentioned challenges. Addressing these points could enhance the papers impact and guide future research directions. Overall the promising results of SLATE underline the models potential as a foundational framework for future exploration and development in image generation and beyond calling for further attention and research into its capabilities and applications.", "xOHuV8s7Yl": " Peer Review of \"Interpretable Neural Network Constructions for Universal Approximation\"  1. Summary of the Paper This paper introduces two novel neural network (NN) architectures aimed at addressing the interpretability issues of traditional deep neural networks (DNNs). The proposed models Triangularlyconstructed NN (TNN) and SemiQuantized Activation NN (SQANN) offer a bottomup approach to neural network design allowing for universal function approximation while maintaining interpretability at a granular level. Both models demonstrate resistance to catastrophic forgetting provide mathematical proofs for achieving arbitrarily high accuracy on training datasets and enable the identification of outofdistribution samples through activation patterns. TNN utilizes a simple organized structure of neuron activations for scalar functions while SQANN extends the concept to multidimensional inputs and involves a layerwise construction that preserves the interpretive benefits of TNN in a more complex framework.  2. Strengths and Weaknesses Strengths:  Novelty: The proposed TNN and SQANN architectures address the critical issue of interpretability in neural networks presenting a fresh approach with their ordered activation and semiquantized activation structures.  Mathematical Rigor: The inclusion of mathematical proofs for the models\u2019 capabilities (e.g. resistance to catastrophic forgetting high accuracy on training datasets) provides a solid theoretical foundation.  Identifying OutofDistribution Samples: The ability of both models to detect outofdistribution samples is particularly notable enhancing their utility in practical applications. Weaknesses:  Complexity in Construction: While the paper provides detailed descriptions of the construction process for TNN and SQANN the complexity and computational cost of these processes in practical largescale applications arent thoroughly discussed.  Limited Experimental Results: The paper could benefit from more extensive experimental validation across a broader range of datasets and comparison with stateoftheart models for both interpretability and performance.  Addressing Overfitting: There is a mention of preventing overfitting in TNN through a noise model but a more detailed exploration of how both models handle overfitting especially in SQANN would be beneficial. Questions and Feedback to the Author:  Could the authors provide more details on the computational efficiency of TNN and SQANN especially when scaling to complex highdimensional datasets  The paper would benefit from a broader comparison with existing models known for their interpretability. Could the authors include such comparisons to highlight the advantages of TNN and SQANN more effectively  Are there any considerations or modifications required in the TNN and SQANN models to prevent overfitting particularly in scenarios with noisy data  3. Review Summary The paper is a significant contribution to the field of interpretable AI introducing two novel architectures TNN and SQANN aimed at providing a more interpretable alternative to traditional DNNs. The models stand out for their mathematical rigor novel approach to function approximation and ability to detect outofdistribution samples addressing crucial needs in machine learning applications. However the paper falls short in thoroughly exploring the computational efficiency and scalability of the proposed models providing limited experimental validation and discussing strategies to prevent overfitting. I recommend accepting this paper with minor revisions that address the mentioned weaknesses especially enhancing the experimental validation section to more clearly delineate the models performance and utility compared to existing approaches. Further exploration of the models behavior in preventing overfitting along with a more detailed discussion on computational efficiency and scalability would greatly strengthen the papers contribution to the field.", "fwsdscicqUm": " Peer Review of \"FEDFB: A Federated Learning Framework for Decentralized Fair Classification\"  Summary The paper addresses a significant gap in the field of fair machine learning by focusing on how to train fair classifiers on decentralized data while ensuring privacy. The authors introduce a novel theoretical framework to assess the performance of various fair learning approaches in a decentralized setting including Unfederated Fair Learning (UFL) Federated Fair Learning via FEDAVG (FFL via FEDAVG) and Centralized Fair Learning (CFL). They rigorously analyze these approaches and demonstrate that federated learning can enhance model fairness beyond nonfederated algorithms. Notably they propose FEDFB a new federated learning algorithm that modifies FEDAVG to improve fairness in decentralized data scenarios. FEDFB significantly surpasses existing methods in achieving fairness showing performance on par with centralized fair learning algorithms through extensive experimentation.  Strong Points 1. Novel Theoretical Framework: The paper introduces an innovative framework for analyzing fairness in decentralized learning environments filling a crucial research gap. 2. Comprehensive Analysis: It undertakes a thorough comparison between federated and nonfederated approaches to fair learning providing clear evidence that federated learning can enhance fairness. 3. FEDFB Algorithm: The introduction of FEDFB as a novel algorithm for achieving fairness in federated learning environments represents a significant advancement in the field. The algorithms ability to achieve performance similar to centralized approaches while ensuring privacy is noteworthy. 4. Extensive Experiments: The paper includes a broad set of experiments to validate the theoretical findings which adds robustness to the claims made.  Weak Points 1. Limited Scope on Fairness Notions: While the paper focuses on demographic parity it does not thoroughly explore other fairness notions. Expanding the analysis to include other fairness criteria could enhance its impact. 2. Assumptions on Data Distribution: The theoretical framework makes certain assumptions on data distribution for the analysis. Clarifying the implications of these assumptions on realworld applicability would strengthen the paper. 3. Privacy Concerns: While FEDFB is praised for its privacypreserving capabilities a more detailed discussion on potential privacy implications or limitations would be beneficial.  Questions and Feedback 1. Other Fairness Notions: Have the authors considered extending their framework to other fairness notions beyond demographic parity If so how adaptable is FEDFB to other fairness criteria 2. Realworld Applicability: Can the authors elaborate on the realworld scenarios where FEDFB could be most beneficial considering the data distribution assumptions made 3. Privacy Analysis: Could the authors provide a more detailed analysis of the privacy guarantees of FEDFB especially in comparison to other federated learning approaches  Review Summary The paper makes a significant contribution to the field of fair machine learning by addressing the challenge of training fair classifiers on decentralized data. The introduction of a novel theoretical framework and the FEDFB algorithm are standout contributions. FEDFB in particular represents a significant advancement in achieving fairness in decentralized learning settings. While the scope of fairness notions explored could be expanded and further discussion on privacy implications is warranted the papers strengths far outweigh its weaknesses. The extensive experimental validation further supports the theoretical contributions making a compelling case for the adoption of FEDFB in practical scenarios. It is recommended for publication subject to consideration of the feedback provided. Recommendation: Accept", "figzpGMrdD": " Review of the Paper \"A Comprehensive Comparison of Continual Learning Performance Across Pretrained Language Models\"  1. Paper Summary This paper presents an exhaustive comparative study on continual learning (CL) performance across various pretrained language models (PLMs) and CL approaches. Continual learning aims to enable models to learn from a stream of data without forgetting previously acquired knowledge. The study evaluates the performance of five PLMs (BERT ALBERT RoBERTa GPT2 and XLNET) across four CL approaches on three benchmark datasets in task and classincremental settings. An indepth experimental analysis identifies performance disparities across PLMs and CL methods while representativeness probing analyses offer insights into how different layers within PLMs are affected by forgetting. The study exposes key research questions that could influence the design of more effective CL techniques in natural language processing (NLP).  2. Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Study: The paper stands out for its broad and systematic comparison across multiple PLMs and CL approaches providing a valuable resource for the NLP community.  Insightful Analyses: The layerwise and taskwise probing of PLMs performance characteristics offers novel insights into how different layers are affected by catastrophic forgetting and the impact of various CL strategies.  Resource Contribution: The authors contribute an opensource toolkit and dataset facilitating further research and ensuring replicability of results. Weak Points:  Generalization of Results: The paper focuses on NLP tasks and it is unclear how generalizable the findings are to other domains such as computer vision or robotics.  Consideration of Recent Models: The rapid evolution of PLMs means that even newer models like T5 or GPT3 might offer different insights and their exclusion could limit the findings applicability.  Evaluation Metrics: While average accuracy forward transfer (FWT) and backward transfer (BWT) are standard metrics incorporating additional metrics like computational efficiency or memory usage could provide a more rounded evaluation of CL methods. Questions and Feedback: 1. Generalizability: How do the authors envision these findings being adapted or tested in nonNLP domains 2. Model Selection: Could the inclusion of newer PLMs like GPT3 or T5 provide different insights into CL performance and effectiveness 3. Additional Metrics: Would considering metrics related to computational efficiency and memory usage change the comparative analysis outcomes or highlight different aspects of the CL approaches 4. RealWorld Application: Can the authors provide examples or scenarios where specific combinations of PLMs and CL methods would be particularly beneficial in realworld applications  3. Review Summary The paper provides a valuable and comprehensive comparative study of various pretrained language models under continual learning settings highlighting significant insights and opening up new research avenues within NLP. The methodical approach coupled with indepth experimental analysis offers a robust foundation for understanding the dynamics of continual learning in PLMs. However the studys current scope primarily focused on NLP tasks and excluding the latest PLMs might limit the broader applicability and relevance of the findings. Addressing the generalizability of the results considering newer PLMs and expanding the evaluation metrics could enhance the studys impact. Nonetheless this work represents a significant step forward in the continual learning research area providing clear directions for future investigations and contributing valuable resources to the NLP community. My recommendation is to accept this paper given that the authors contemplate the suggestions to broaden the evaluative scope and address the papers current limitations.", "vr39r4Rjt3z": " Peer Review of \"Mitigating Catastrophic Forgetting in Neural Networks through Masked Highway Connections and LayerWise Normalisation\"  1. Summary of the Paper This manuscript introduces two novel architectural modifications namely Masked Highway Connections (MHCs) and LayerWise Normalisation (LWNs) aimed at reducing catastrophic forgetting in neural networks that learn tasks sequentially. Catastrophic forgetting refers to a networks degradation in performance on previously learned tasks upon learning new tasks. Unlike traditional approaches that rely on external constraints to prevent forgetting the authors propose internal modifications to the architecture. Through extensive experiments across different datasets and continual learning setups the paper demonstrates that MHCs and LWNs not only outperform baselines but also enhance the effectiveness of existing continual learning techniques thereby setting new performance benchmarks.  2. Strengths and Weaknesses of the Paper Strengths:  Innovativeness: The paper introduces creative modifications to neural network architectures that address catastrophic forgetting internally contrasting with the majority of existing work which primarily focuses on external constraints.  Compatibility: The proposed techniques are shown to be compatible with all major archetypes of continual learning methods which is a significant advantage for practical application.  Comprehensiveness: The evaluation covers a wide range of experiments datasets and setups providing a thorough demonstration of the efficacy of the proposed modifications.  Clarity: The paper is wellwritten with clear explanations of the proposed methods and a logical flow that is easy to follow. Weaknesses:  Lack of Theoretical Insights: While the empirical results are compelling the paper lacks a deep theoretical analysis of why and how the proposed modifications mitigate catastrophic forgetting.  Potential Overfitting to Tested Scenarios: The experiments are limited to specific datasets and settings. Its unclear how generalizable these results are to other types of tasks datasets or more complex sequential learning scenarios.  Hyperparameter Sensitivity: The paper provides limited discussion on the sensitivity of the proposed methods to their hyperparameters and the complexity involved in choosing these parameters optimally for different tasks and datasets. Questions and Feedback:  Are there theoretical frameworks or analyses that can further explain the mechanism behind MHCs and LWNs resistance to catastrophic forgetting  How sensitive are the proposed methods to their hyperparameters Some insights into the robustness of these methods to variations in hyperparameter settings would be useful.  Considering the potential of these methods to generalize across various tasks and datasets how well do the authors think they would perform on nonimage based tasks  3. Review Summary This paper presents a significant contribution to the field of continual learning by proposing two internal modifications to neural network architectures that successfully mitigate catastrophic forgetting. The empirical evaluation is thorough and demonstrates clear benefits over existing methods. However the manuscript would be strengthened by addressing potential limitations such as the lack of theoretical support for the mechanisms behind the proposed modifications and questions around their generalizability and hyperparameter sensitivity. Given the novelty potential impact and comprehensive experimental validation I recommend accepting this paper for publication contingent upon minor revisions to discuss the aforementioned weaknesses. The introduction of MHCs and LWNs as effective compatible and potentially generalizable solutions to catastrophic forgetting represents a valuable advancement in continual learning research.", "nWprF5r2spe": "Summarize the paper This paper introduces WAFL (WAsserstein distributionally robust Federated Learning) a novel approach aiming to enhance the robustness and generalization capabilities of global models in federated learning (FL) settings particularly when dealing with nonindependent and identically distributed (noni.i.d.) client data. WAFL leverages Wasserstein distributionally robust optimization to model an ambiguity set (a Wasserstein ball) that encompasses all potential adversarial distributions near the nominal data distribution. Through this formulation WAFL addresses the challenge of statistical heterogeneity in FL by remolding the original optimization problem into an empirical surrogate risk minimization problem solvable via a proposed local SGDbased algorithm. The authors prove the convergence of this algorithm and demonstrate through experiments that WAFL outperforms traditional FL approaches such as FedAvg especially in noni.i.d. settings and against adversarial distributions shifts. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The paper presents a creative solution to handle the issue of noni.i.d. data in FL a problem that significantly impacts model generalization and robustness in distributed environments. 2. Theoretical Foundation: The authors provide a rigorous theoretical analysis including convergence guarantees for the proposed optimization algorithm. This adds credibility and depth to the work. 3. Empirical Evaluation: The empirical results convincingly demonstrate the effectiveness of WAFL compared to existing methods particularly in noni.i.d. settings and against adversarial distribution shifts which are critical issues in FL. Weak Points: 1. Complexity and Scalability: The methods complexity particularly the implementation of the Wasserstein distance and the subsequent optimization might pose scalability issues especially with a very large number of clients. 2. Parameter Sensitivity: The choice of the Wasserstein balls radius and the robust parameter \u03b3 seems to be crucial for the performance. However the paper lacks a detailed discussion on the sensitivity and selection process of these parameters. 3. Practical Deployment Considerations: While the theoretical foundation and empirical results are strong there is limited discussion on the practical deployment of WAFL including computational overhead communication costs and applicability to realworld noni.i.d. scenarios beyond the datasets tested. Questions and Feedback to the Author: 1. Could you elaborate on the scalability of the proposed WAFL framework in federated learning environments with a very large number of clients especially considering the computational overhead of calculating Wasserstein distances 2. How sensitive is WAFL\u2019s performance to the choice of the Wasserstein ball radius and the robust parameter \u03b3 Could you provide more insights or guidance on how these parameters should be selected in practice 3. Have there been considerations or evaluations pertaining to the communication costs associated with the proposed method especially in comparison to FedAvg and other robust FL methods 4. Are there potential extensions or modifications to WAFL that could further improve its applicability and efficiency in realworld federated learning scenarios with diverse and dynamic noni.i.d. data distributions Review Summary The paper introduces WAFL a novel federated learning framework that significantly advances the current state of robust and generalizable FL models in the presence of noni.i.d. data distributions. WAFLs innovative use of Wasserstein distributionally robust optimization to form an ambiguity set for adversarial distributions is both theoretically sound and empirically effective as demonstrated in the experimental comparisons with existing methods like FedAvg. However for the recommendation of acceptance it is crucial for the authors to address questions related to the scalability of WAFL in realistic largescale FL settings the sensitivity and selection process for crucial parameters (such as the Wasserstein ball radius and \u03b3) and the potential implications on communication costs and practical deployment. Addressing these considerations can vastly improve the comprehension of WAFLs applicability in realworld scenarios and its superiority over existing FL approaches.", "v3aeIsY_vVX": " Peer Review of the Paper: \"Chunked Autoregressive GAN (CARGAN) for Conditional Waveform Synthesis\"  1) Summary The paper introduces Chunked Autoregressive GAN (CARGAN) a novel model for conditional waveform synthesis which leverages the strengths of autoregressive and generative adversarial network (GAN) approaches to generate highquality audio. The authors identify key shortcomings in existing GANbased models particularly in accurately preserving pitch and periodicity which leads to artifacts in synthesized audio. CARGAN aims to address these flaws by incorporating an autoregressive component that enhances pitch accuracy and phase learning. The model achieves significant improvements in training efficiency pitch accuracy and subjective audio quality without sacrificing generation speed making it suitable for realtime or interactive applications.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a critical challenge in waveform synthesis specifically the artifacts caused by pitch and periodicity inaccuracies in GANbased models. This focus is both relevant and timely given the increasing interest in highfidelity audio synthesis.  CARGAN demonstrates impressive improvements over stateoftheart models including a substantial reduction in pitch error and training time and maintains a fast generation speed suitable for practical applications.  The approach of leveraging autoregression within a GAN framework to improve the accuracy of pitch and phase representations is innovative and welljustified through both theoretical and empirical evaluations.  The paper is commendable for its thorough empirical evaluation including both objective metrics and subjective listening tests providing a comprehensive assessment of CARGANs performance. Weak Points:  While the improvements in pitch accuracy and training efficiency are significant the impact on the overall perceived quality of the audio (beyond pitch accuracy) could be discussed in more detail. Specifically the subjective tests results on the VCTK dataset only show equality with HiFiGAN suggesting that improvements in pitch accuracy may not always translate to noticeable improvements in subjective audio quality.  The discussion on the limitations and potential future directions could be expanded. For instance the risk of overfitting to specific types of audio or languages and the adaptability of CARGAN to different audio synthesis tasks beyond the datasets evaluated could be further explored.  The ethical considerations section while appreciated could benefit from a more indepth exploration of the social implications of advancing voice cloning technologies including potential misuse and privacy concerns. Questions and Feedback: 1. How does CARGAN perform on nonEnglish datasets or more diverse acoustic environments Are there any limitations in its current formulation that might affect its generalizability 2. The model reduces training time significantly but could the authors comment on the computational resources required for training compared to other stateoftheart models 3. Could the authors elaborate on the decision process for choosing the chunk size and the impact of different chunk sizes on both training efficiency and audio quality 4. It would be beneficial if the authors could discuss potential approaches to mitigate occasional boundary artifacts further and whether any preliminary experiments in this direction have been conducted.  3) Review Summary The paper presents a significant contribution to the field of waveform synthesis through the development of CARGAN which adeptly addresses key issues observed in contemporary GANbased synthesis models mainly related to pitch and periodicity accuracy. The innovative use of autoregression within the GAN framework to improve pitch and phase accuracy substantiates the models superiority over existing methods as demonstrated through comprehensive objective and subjective evaluations. Notwithstanding the exploration of CARGANs subjective quality improvements and its generalizability to diverse audio datasets could be further enriched. Moreover considering the rapid advancements in voice cloning technology a deeper discussion on ethical implications and potential safeguards against misuse would be beneficial. Overall the paper puts forward a compelling case for the efficacy of CARGAN in enhancing the stateoftheart in conditional waveform synthesis paving the way for future research in this exciting domain.", "gfwON7rAm4": " Review of the Paper on Markov Potential Games (MPGs) and Policy Gradient Convergence  Summary This paper explores the integration of the intuitive framework of potential games into Markov games aiming to understand multiagent coordination with state dependence. The authors introduce a class of Markov Potential Games (MPGs) as a generalization to capture complex stateoriented multiagent coordination dynamics. Despite initial insights from normalform potential games not directly translating to MPGs due to the presence of zerosum game instances MPGs maintain desirable properties such as the existence of deterministic Nash policies. The paper\u2019s primary contribution is the proof of convergence for independent policy gradients (both direct and stochastic) towards Nash policies within MPGs providing a polynomial rate concerning the error approximation. This addresses the gap in understanding the convergence behaviors of policy gradients in multiagent RL settings beyond purely competitive or cooperative extremes.  Strengths and Weaknesses Strengths: 1. Novelty and Relevance: The paper tackles an underexplored area within AI and ML extending the theory of potential games to Markov settings which is highly relevant for advancing multiagent reinforcement learning. 2. Clear Theoretical Contributions: The authors provide rigorous mathematical formulations and proofs particularly the convergence theorems for policy gradients in MPGs filling a significant theoretical gap. 3. Broad Applicability: The introduction of MPGs and the associated convergence results have potential applications across a wide range of domains including robotics and autonomous systems. Weaknesses: 1. Assumptions and Practical Applicability: While the theoretical framework is robust the paper could benefit from a discussion on the assumptions made (e.g. the structure of rewards actions and the environment) and their practical applicability or limitations in realworld scenarios. 2. Empirical Validation: The paper largely focuses on theoretical contributions with limited empirical analysis to validate the theoretical findings. Including more comprehensive experiments possibly in diverse settings could strengthen the paper. 3. Accessibility and Complexity: The high technical depth while necessary for proving the main results might make the paper less accessible to readers not wellversed in game theory or policy gradient methods. Adding intuitive explanations or simplifying certain arguments could broaden its appeal. Questions and Feedback: 1. Robustness to Model Misprediction: How robust are MPGs and the associated policy gradient convergence to model mispredictions or errors in estimating the state transition probabilities 2. Extension to Nondeterministic Policies: The paper focuses on deterministic Nash policies could you discuss potential extensions or limitations when considering stochastic policies 3. Impact of Agent Heterogeneity: How does agent heterogeneity (in terms of capabilities objectives or information access) affect the structure of MPGs and the convergence to Nash policies  Review Summary The paper makes important theoretical contributions to our understanding of Markov Potential Games and their implications for multiagent reinforcement learning. The rigorous analysis and proofs provided offer valuable insights into the convergence behavior of policy gradients in such settings marking a significant step forward in the field. However to strengthen the paper further it would be beneficial to address the practical applicability of MPGs clarify assumptions incorporate more extensive empirical validation and consider the implications of agent heterogeneity. Overall the papers novel framework and detailed theoretical treatment present a substantial advance in multiagent RL with promising directions for future research in achieving more effective coordination and policy convergence in complex statedependent environments. Supporting Arguments:  The introduction of MPGs expands our theoretical toolkit for understanding multiagent coordination an area of growing importance in AI and ML.  The convergence proofs for policy gradients in MPGs bridge a vital knowledge gap potentially facilitating the design of more efficient learning algorithms in multiagent settings.  The lack of extensive empirical evidence and discussion on practical considerations may limit the immediate applicability of the findings underscoring the need for further research in these areas.", "uUN0Huq-n_V": " Review of \"Reinforcement and Inverse Reinforcement Learning for Music Harmony Composition\"  1. Summary This paper presents an innovative approach to automatic music harmony composition by integrating Deep Supervised Learning (DSL) Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL). This methodology diverges from traditional deep supervised learning techniques that heavily rely on humancomposed songs for training which often result in creations that may not be harmonically pleasing due to the limitations inherent in the chosen deep learning models. The proposed model treats music composition as a reinforcement learning problem selecting chords based on previous notes and seeking to maximize an overall accumulated reward. A notable challenge in this approach is defining an effective reward function which the authors address by employing Adversarial Inverse Reinforcement Learning (AIRL) to learn a reward function from a set of human compositions supplemented by music theory rules. This paper goes through several stages: initially training a Biaxial LSTM model via DSL then refining the reward function through IRL and finally tuning the pretrained model using DRL. The efficacy of this approach is evidently shown through both objective metrics and user preference evaluations demonstrating improvements over pretrained models not subjected to reinforcement learning tuning.  2. Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Approach: The fusion of DSL DRL and IRL to tackle the problem of music harmony composition is highly innovative offering a fresh perspective compared to traditional methods. 2. Comprehensive Methodology: The paper thoroughly explains the training phases the rationale behind choosing the Biaxial LSTM model and the process of learning a reward function using AIRL. 3. Objective and Subjective Evaluations: The inclusion of both objective metrics and subjective user evaluations to assess the models performance provides a wellrounded analysis of its effectiveness. 4. Reproducibility and Transparency: The commitment to reproducibility by pledging to release the code publicly and detailing the experimental setup including datasets and preprocessing steps is commendable. Weak Points: 1. Complexity and Accessibility: The advanced nature of the combined methodologies might make it challenging for readers not familiar with machine learning particularly AIRL to fully grasp the technical details. 2. Limited Discussion on Feature Representation: While the paper efficiently outlines the process of transforming midi fragments into pianorolls a deeper discussion on the choice and impact of feature representation on the models performance is missing. 3. Data Set Limitations: The reliance on the Lahk Pianoroll Dataset although valid raises questions about the models adaptability to compositions outside the datasets genre diversity and time signature constraints. Questions and Feedback: 1. Generalization Capability: How well does the model generalize to music styles or compositions not well represented in the training set 2. Impact of Music Theory Rules: Could the authors elaborate on how the music theory rules were selected and their specific impact on the models performance 3. Potential for Live Applications: Have the authors considered or tested the model\u2019s application in realtime music composition scenarios such as live performances  3. Review Summary The paper introduces a compelling and innovative approach to automatic music harmony composition by leveraging a combination of Deep Supervised Learning Deep Reinforcement Learning and Inverse Reinforcement Learning. The methodology is wellstructured with a clear progression from training through DSL to refinement via DRL and IRL. The models success as demonstrated through both objective metrics and subjective user evaluations suggests a significant step forward in the field of automatic music composition. Despite the paper\u2019s strengths it would benefit from a deeper exploration of its models generalization capabilities and a more detailed discussion on the choice and impact of music theory rules incorporated into the reward function. These enhancements could provide readers with a fuller understanding of the models operational dynamics and its applicability to a broader array of musical styles. In conclusion the paper presents a robust innovative model that marks a notable advancement in automatic music composition. With improvements in the areas of model generalization feature representation and a more comprehensive discussion on music theory integration the paper could make a significant impact in the field. The commitment to reproducibility and the thorough evaluation of the models performance are particularly praiseworthy aspects of this work.", "fSeD40P0XTI": " Peer Review  Summary: The manuscript introduces a novel concept Continuous Classification of Time Series (CCTS) tailored for applications that require realtime classification of evolving time series data. The primary motivation is derived from critical use cases such as continuous patient monitoring in ICUs where timely and accurate classification of data like vital signs is crucial for lifesaving decisions. The paper identifies the primary challenges in CCTS as catastrophic forgetting and overfitting issues that arise due to the evolving nature of time series data leading to multiple distributions that a model must learn simultaneously. To address these challenges the authors propose an Adaptive model training policy for CCTS (ACCTS) which includes an adaptive multidistribution extraction policy and an adaptive importancebased replay policy. The former dynamically extracts data distributions based on the evolution of the time series and model changes while the latter selects significant samples for replay to balance accuracy and time efficiency. The effectiveness of ACCTS is demonstrated through experiments on four realworld datasets showing superior classification accuracy at every time point compared to baseline models.  Strong Points: 1. Innovative Concept: The paper introduces a new problem domain (CCTS) that is highly relevant for timesensitive applications especially in critical care settings. This focus on continuous classification as opposed to oneshot classification addresses an unmet need in the literature. 2. Comprehensive Approach: The proposed solution ACCTS intelligently combines two adaptive policies (multidistribution extraction and importancebased replay) to address both catastrophic forgetting and overfitting showcasing a thorough understanding of the challenges. 3. Empirical Evaluation: The authors conduct extensive experiments on diverse realworld datasets providing a robust validation of their methodology. The comparison with baselines from both ECTS and CL literature underlines the superiority of ACCTS in the CCTS context. 4. Clarity and Organization: The manuscript is wellwritten with clear logically structured sections. The distinction between CCTS and existing classification paradigms is particularly well articulated.  Weak Points: 1. Lack of Theoretical Underpinnings: While the practical efficacy of ACCTS is demonstrated the paper lacks a theoretical analysis or justification for the choice of adaptive policies. A more indepth discussion on why these specific strategies work could strengthen the paper. 2. Generalization Across Domains: The experiments are conducted on datasets from relatively similar domains (primarily healthcare and environmental monitoring). Its unclear how well ACCTS would generalize to vastly different time series data domains. 3. Comparison to StateoftheArt Models: The baseline models chosen for comparison seem to lean heavily on earlier works. Including more recent advancements in time series classification particularly from the deep learning domain could provide a better benchmark. 4. Parameter Sensitivity Analysis: The paper briefly mentions two definable coefficients (\u03b1 and ) but does not provide a detailed sensitivity analysis. Such an analysis could offer insights into the robustness of ACCTS across varying parameters and application domains.  Questions and Feedback to the Author: 1. Could the authors elaborate on the theoretical foundations for the adaptive policies used in ACCTS Specifically how were these policies validated as the optimal choice for addressing the challenges of CCTS 2. How does ACCTS perform in time series classification tasks outside the medical and environmental monitoring domains Are there limitations to its applicability 3. More information on the choice and impact of parameters \u03b1 and in the models performance would be beneficial. How sensitive is the models performance to these parameters and what guidelines can be provided for their selection 4. Considering the dynamic nature of CCTS how does the computational complexity of ACCTS compare with traditional methods especially in realtime applications  Review Summary: The manuscript presents a compelling and wellvalidated approach ACCTS to the newly defined problem of Continuous Classification of Time Series (CCTS) with a primary focus on applications requiring realtime accurate data classification. The proposed adaptive model training policy adeptly addresses the core challenges of catastrophic forgetting and overfitting through innovative adaptive mechanisms. While the paper stands out for its novel problem definition and solution approach supplemented by comprehensive empirical validation some areas could be enhanced for a more rounded contribution. Specifically a deeper theoretical insight into the chosen strategies expansion of experiments to a broader set of domains and a rigorous analysis of model sensitivity to parameters would enrich the paper. Additionally a clearer delineation of the models computational demands in live scenarios would be valuable for practical implementation considerations. Overall this work undoubtedly advances the field of time series classification particularly for critical realtime applications. The authors are encouraged to address the mentioned weak points and questions to further solidify their valuable contribution to the literature.", "vDwBW49HmO": " Review of \"InterDomain Gradient Matching for Domain Generalization\"  1. Summary This paper introduces a novel approach to domain generalization a crucial machine learning challenge where models are expected to perform well on unseen outofdistribution data. The authors propose the InterDomain Gradient Matching (IDGM) objective that aligns gradient directions across different domains by maximizing the inner product between these gradients. This method aims at focusing the learning process on invariant features across domains rather than domainspecific spurious correlations. Due to the computational complexity of direct optimization of IDGM (which involves secondorder derivatives) the paper presents a simpler firstorder algorithm called Fish which approximates the IDGM optimization process. The effectiveness of Fish is empirically validated on two benchmarks: the WILDS benchmark capturing distribution shift in the real world and the DOMAINBED benchmark focusing on synthetictoreal transfer. Fish demonstrates competitive results underlining its potential for wide application in domain generalization tasks. The paper also provides an indepth theoretical analysis experimental validation and comparison with baseline methods and current stateoftheart approaches.  2. Strengths and Weaknesses Strengths:  Innovative Approach: Proposing the IDGM objective is a novel idea that intuitively aligns with the goal of learning invariant features across domains addressing a significant limitation in many domain generalization methods.  Comprehensive Evaluation: The paper thoroughly evaluates Fish against competitive benchmarks providing extensive empirical evidence of its effectiveness and efficiency.  Theoretical Underpinning: The authors provide a solid theoretical foundation for Fish including approximations to avoid secondorder derivatives and the empirical validation of these theoretical insights.  Publicly Available Implementation: Making the code publicly available enhances reproducibility and allows for further experimentation and development by the community. Weaknesses:  Limited Discussion on Limitations: While the paper mentions that Fish struggles with a large number of domains it does not fully explore this limitation or propose potential pathways to address it.  Comparison Depth: While the paper provides significant empirical results a deeper analysis comparing Fish to the most recent advances specifically those not included in the current benchmarks could offer more insight into its position within the field.  Hyperparameter Sensitivity: The paper briefly mentions hyperparameter settings but does not deeply investigate the sensitivity of Fish to these parameters which could be crucial for practitioners trying to apply this method to new datasets or tasks. Questions and Feedback to the Author:  Can the authors comment on the scalability of Fish and potential strategies to mitigate its performance drop as the number of domains increases  It would be beneficial to have a deeper discussion on the theoretical implications of using a firstorder approximation (Fish) of the IDGM objective. Specifically what effects do these approximations have on the learning process and the types of invariances that can be captured  More information on the sensitivity of the model to hyperparameters including the learning rate and the scaling term \u03b3 would be valuable. How robust is Fish to variations in these parameters across different datasets  Considering the computational efficiency of Fish how does it compare to existing methods in terms of training time and are there specific scenarios where Fish is less efficient  3. Review Summary The paper presents an innovative and theoretically grounded approach to domain generalization a pressing challenge in machine learning. Through the introduction of the IDGM objective and its practical approximation via the Fish algorithm the authors contribute significantly to advancing domain generalization research. Fish not only demonstrates strong empirical performance across multiple challenging benchmarks but also represents a computationally efficient solution to a problem typically associated with high computational costs. While the paper could benefit from a more critical examination of its limitations and a deeper exploration of hyperparameter sensitivity these points do not detract significantly from the overall quality and impact of the work. The availability of code enhances the paper\u2019s contribution by facilitating reproducibility and further exploration. Recommendation: Accept with minor revisions focusing on a deeper discussion of limitations hyperparameter sensitivity and further explanation of the theoretical implications of adopting a firstorder approximation.", "qI4542Y2s1D": " 1) Summary of the Paper The paper introduces FILM (Following Instructions in Language with Modular methods) a novel approach to Embodied Instruction Following (EIF). Unlike traditional EIF methods that rely on endtoend training with expert trajectories and detailed lowlevel instructions FILM adopts a modular architecture that significantly enhances the tasks efficiency and accuracy. This architecture comprises four main components: Language Processing Semantic Mapping Semantic Search Policy and a Deterministic Policy for action execution all working in tandem to understand and execute complex compositional instructions in a given environment. By generating structured representations from highlevel instructions and dynamically updating a semantic map of the scene FILM can forgo the need for detailed stepbystep guidance. It achieves StateoftheArt (SOTA) performance on the ALFRED benchmark notably outperforming previous methods by a substantial margin while requiring less data.  2) Strong and Weak Points Questions and Feedback Strong Points:  Modular Design: FILMs modular framework is a significant advancement over existing endtoend models for EIF. This design not only makes the system more interpretable but also allows for targeted improvements and debugging of individual components.  No Need for Expert Trajectories or Detailed Instructions: By moving away from reliance on detailed lowlevel instructions and expert trajectories FILM addresses a significant barrier in scaling EIF systems \u2013 the extensive requirement of annotated data.  SOTA Performance with Less Data: Achieving a notable improvement in performance on the ALFRED benchmark with reduced data dependence marks a significant achievement demonstrating the efficiency and effectiveness of FILMs strategies. Weak Points:  Scalability and Generalization: While FILM shows strong generalizability across seen and unseen environments in testing the true test of scalability to much more diverse and dynamic realworld scenarios remains. How well can FILM adapt to drastically different environments or instructions is not entirely clear.  Dependence on Semantic Mapping Accuracy: The performance of FILM heavily relies on the accurate semantic mapping of the environment. It is uncertain how the system would perform in environments where semantic segmentation models might struggle (e.g. low lighting occluded objects). Questions and Feedback:  Could the authors elaborate on how FILM would perform or could be adapted to more dynamic environments where objects are not static and lighting conditions may dramatically change  It would be beneficial if future work could explore the integration of an active learning framework where FILM can identify gaps in its understanding or mapping and seek information actively to fill these gaps.  A comparison with other modular and hybrid EIF systems not benchmarked against the ALFRED dataset might provide further insights into FILM\u2019s versatility and areas for improvement.  3) Review Summary FILM represents a notable advancement in the field of Embodied Instruction Following by leveraging a modular approach that integrates language understanding semantic mapping and search policy to navigate and interact within an environment based on highlevel instructions. Its ability to outperform existing SOTA methods without relying on detailed expert data marks a significant step forward showing promise for more efficient learning and adaptation in complex tasks. The modular design of FILM not only contributes to its high performance but also improves the systems interpretability and allows for focused enhancements in its individual components. The innovative use of a semantic search policy in particular provides a novel approach to navigating and identifying objectives within an environment that could have wideranging implications for improving robotic autonomy and interaction. However challenges remain regarding the scalability of FILM to more diverse and unpredictable realworld scenarios. The dependence on the accuracy of semantic maps and the potential issues around generalization to different settings or task types are critical areas for future investigation. Addressing these challenges perhaps through active learning mechanisms or more adaptive modeling approaches could significantly enhance FILMs applicability and performance in realworld EIF tasks. Overall FILMs contributions to improving the efficiency and efficacy of EIF systems are commendable and its strategies for leveraging modular components to surpass the limitations of existing approaches offer valuable insights and directions for future research in the field.", "vHVcB-ak3Si": " Peer Review of \"A Deeper Understanding of Integral Pose Regression for Human Body and Hand Pose Estimation\"  1) Summary: This paper delves into the integral pose regression method for human body and hand pose estimation aiming to understand its performance and training peculiarities compared to detectionbased methods. Integral pose regression leveraging a differentiable expectation operation instead of the nondifferentiable argmax used in detection methods theoretically offers endtoend training benefits. However its practical application has shown mixed results with superior performance mainly in challenging scenarios. The research thoroughly analyzes inference and backpropagation mechanisms highlighting heatmap activation shrinkage and gradient inefficiencies as primary concerns. A spatial prior is proposed to enhance training efficiency and accuracy supported by theoretical arguments and empirical experiments.  2) Strong and Weak Points Questions and Feedback: Strong Points:  Theoretical and Empirical Analysis: The papers strength lies in its thorough theoretical analysis complemented by empirical evidence providing a comprehensive understanding of the observed phenomena in integral pose regression versus detection methods.  Practical Improvements: The introduction of a spatial prior to counteract activation shrinkage and its positive impact on training speed and performance addresses a significant limitation in integral regression methods.  Relevance: The topic is highly relevant due to the increasing interest in human pose estimation technologies in various applications from gaming to healthcare. Weak Points:  Complexity for Practical Implementation: While the introduction of spatial priors is innovative the paper might benefit from a clearer presentation of its implementation details making it easier for practitioners to adopt.  Generalization of Findings: The experiments focus on specific architectures and data splits. Additional experiments across diverse architectures and datasets could strengthen the argument about the generalizability of the findings.  Comparison with StateoftheArt (SOTA): While the paper provides a detailed comparison between integral regression and detection methods an explicit comparison with the current SOTA methods in pose estimation could contextualize the findings better. Questions and Feedback:  How does the proposed spatial prior affect the computational efficiency during inference  Is there a risk of overfitting with the introduction of spatial priors especially in datasets with limited pose variations  The paper could benefit from a discussion section drawing parallels with other fields or methods where similar challenges with activation shrinkage or gradient inefficiencies have been observed. Are there learnings from those that could further inform improvements in pose estimation methods  3) Review Summary: The paper presents an insightful analysis into the differential performance between integral regression and detection methods in human pose estimation. Its strength lies in the meticulous theoretical underpinning and empirical evidence supporting the hypothesized reasons for performance discrepancies particularly highlighting the role of heatmap activation shrinkage and gradient inefficiencies. The proposed solution of incorporating spatial priors is a meaningful step towards mitigating these challenges showing potential for substantial improvements in training efficiency and accuracy. While the paper stands out for its depth of analysis and practical improvements there are areas where it could be enhanced. Specifically detailing the implementation of spatial priors for ease of adoption broadening the experimental validation across diverse settings to affirm generalizability and situating the work within the broader context of SOTA pose estimation methods could elevate its contribution further. In conclusion this paper makes a significant contribution to the understanding of integral pose regression in human pose estimation proposing actionable insights for overcoming its limitations. With some enhancements for broader applicability and contextualization this work has the potential to guide future research and practices in the field offering a basis for more efficient and accurate pose estimation methods.", "hl9ePdHO4_s": " Peer Review of \"Efficient Graph Convolution (EGC): Challenging Common Wisdom in GNN Architecture\"  Summary The paper introduces Efficient Graph Convolution (EGC) an isotropic Graph Neural Network (GNN) that aims to challenge the prevailing belief in the GNN community regarding the necessity of anisotropic models for achieving stateoftheart performance. Contrary to previous studies that favored anisotropic models for their higher expressivity the authors of this paper demonstrate that EGC by leveraging spatiallyvarying adaptive filters outperforms existing anisotropic models in terms of accuracy while offering significant advantages in memory consumption latency and suitability for accelerator implementation. The paper thoroughly evaluates EGC across six large graph datasets showing superior or competitive performance against common baselines including the popular GAT (Graph Attention Network) and PNA (Principal Neighbourhood Aggregation) models. Additionally the paper discusses the implications of their findings for both the research community and practical applications shedding light on potential areas for further inquiry.  Strong Points 1. Novelty and Impact: The proposition of an isotropic GNN model that outperforms anisotropic counterparts is highly novel and challenges the existing paradigm in the GNN sphere. This could inspire a reevaluation of current approaches and stimulate research into more efficient GNN models. 2. Comprehensive Evaluation: The extensive evaluation across six distinct datasets covering both transductive and inductive scenarios provides robust evidence for the effectiveness of EGC. The inclusion of ablation studies further strengthens the credibility of their claims. 3. ResourceEfficiency: The demonstrated efficiency of EGC in terms of memory consumption and latency is of great practical importance especially for deployment in resourceconstrained environments. This can broaden the applicability of GNNs in realworld scenarios.  Weak Points and Questions 1. Lack of Theoretical Insight: While the paper presents empirical evidence of EGCs performance it lacks a deep theoretical analysis explaining why EGC outperforms anisotropic models. Such insights could help in understanding the conditions under which EGC would be expected to perform well. 2. Generalization to Heterophilous Graphs: The paper primarily discusses homophily in graphs as a context where EGC excels. It would be helpful to know how EGC performs on heterophilous graphs considering the papers mention of isotropic models limitations in such contexts. 3. Implementation Complexity: Although the paper mentions EGC being a \"dropin replacement\" for existing GNN architectures it doesnt discuss the complexities or potential challenges in integrating EGC within existing frameworks in detail.  Questions and Feedback to the Author 1. Can the authors provide more theoretical insights or intuitions on why EGC manages to outperform anisotropic models despite the latters higher expressive power 2. It would be beneficial if the paper included comparisons or evaluations of EGC on heterophilous graph datasets to assess its versatility and performance in less homophilic settings. 3. Could the authors comment on any challenges or limitations encountered when integrating EGC into existing GNN frameworks or applications  Review Summary The paper presents a significant contribution to the field of graph neural networks by introducing the Efficient Graph Convolution (EGC) model which successfully challenges the prevailing notion that anisotropic GNNs are inherently superior. EGCs superior performance across various benchmarks combined with its efficiency advantages positions it as a noteworthy advancement in the GNN landscape. However the paper could be strengthened by addressing the theoretical underpinnings of EGCs success and expanding its evaluation to include heterophilous graphs. Additionally discussing the integration challenges of EGC could provide valuable insights for researchers and practitioners aiming to adopt this model. Given the compelling empirical evidence and the potential impact on future GNN research and applications the recommendations for this paper would be highly positive contingent on addressing the aforementioned feedback to further elucidate the mechanisms and applicability of EGC.", "moHCzz6D5H3": " Review:  Summary of the Paper The paper introduces a novel concept and framework named PeekaBoo (PaB) targeting the training of sparse neural networks (NNs). The focus is on disguised subnetworks within randomly initialized NNs\u2014subnetworks that are not just hidden due to initial sparsity but require specific transformations (like sign flipping) on the remaining weights to reveal or \"unmask\" their optimal performance capabilities. This approach deviates from traditional methods that prune after training by finding and optimizing these disguised subnetworks at the initialization phase therefore avoiding the extensive computational costs of training dense networks before pruning. PaB involves a twostage process: initial sparsification via pruning methods at random initialization and subsequent optimization of weight signs to enhance performance leveraging a technique similar to binary neural network optimization but applied in the context of sparse subnetworks. Comprehensive experiments demonstrate PaBs effectiveness over existing methodologies like edgepopup and others in terms of achieving superior performance with greater computational efficiency on several large network architectures and datasets.  Strong Points 1. Novelty: The concept of disguised subnetworks and their identification through an innovative twostage process is a significant contribution. This idea extends previous works by not only finding sparse subnetworks but also optimizing them efficiently at initialization. 2. Comprehensive Experiments: The paper presents extensive empirical evidence across multiple datasets and neural network architectures showcasing the superior performance and efficiency of PaB against benchmark methods. 3. Theoretical and Practical Implications: The introduction of PaB potentially shifts the paradigm in training sparse neural networks by highlighting the feasibility and advantages of pruning and optimizing at initialization which could inspire further research in this direction.  Weak Points 1. Lack of Theoretical Foundation: While the empirical results are promising the paper does not provide a deep theoretical analysis or justification of why the sign flipping and particular transformation strategies are effective for unmasking disguised subnetworks. 2. Generalization Concerns: Although the paper includes results from a variety of networks and datasets it lacks an exploration of the techniques effectiveness in different or more challenging contexts such as tasks with inherent data sparsity or tasks beyond image classification. 3. Potential Overhead from Unmasking Process: The efficiency gains are highlighted however details on the computational and memory overhead introduced by the unmasking process (sign flipping and transformation) are not thoroughly discussed.  Questions and Feedback to the Author 1. Theoretical Grounding: Can the authors expand on the theoretical rationale behind the effectiveness of the PaB method particularly why specific weight transformations are chosen 2. Expanded Experimentation: Would the authors consider testing PaB on other tasks beyond traditional image classification such as object detection or natural language processing to evaluate the methods versatility and robustness 3. Efficiency Overhead Analysis: Could the authors provide a more detailed analysis or discussion on any potential computational or memory overhead introduced by the PaB process especially during the unmasking phase  Review Summary This paper presents an innovative technique PeekaBoo (PaB) for training sparse neural networks by identifying and optimizing disguised subnetworks through a twostage process at network initialization. The approach is theoretically intriguing and empirically strong proven through extensive experiments. However the paper lacks a solid theoretical foundation explaining the methods effectiveness and does not fully explore potential limitations or overheads introduced by the unmasking process. It also misses the opportunity to test the methods generalizability across different tasks. Nonetheless PaBs introduction is a significant contribution to the sparse NN literature potentially opening new avenues for efficient NN training methods. Given these observations I recommend acceptance provided that the authors address the highlighted concerns particularly by expanding the theoretical rationale and exploring the methods application breadth and efficiency overheads in greater detail.", "jJWK09skiNl": "Peer Review for the Manuscript: \"A Novel Approach to Generalized ZeroShot Object Detection in Manufacturing Environments Using YOLOv5\" 1) Summary of the Paper: The paper presents an innovative approach to tackle the problem of detecting new unseen objects in dynamic manufacturing environments without the need for retraining the vision system. The core contribution is the modification of the YOLOv5 neural network architecture to achieve generalized zeroshot detection (gZSD) of objects by leveraging an attribute labeling strategy and a novel dataset splitting method. This approach enables industrial robots to recognize a wide range of objects both seen and unseen by bounding box localization without explicit class labeling (classification). The authors use the YCB Video dataset for their experiments splitting it into seen and unseen objects to train and validate their model. The methodology network architecture alterations and loss function design are discussed in depth with experiments showing the models efficacy through recall rates for object detection. 2) Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses a highly relevant and challenging problem in the realm of smart manufacturing and robotics where highmixlowvolume production demands flexibility in object detection capabilities.  Utilizing the YOLOv5 architecture and modifying it for generalized zeroshot detection is innovative and demonstrates a promising direction for research in object detection technologies.  The novel dataset splitting method and attribute labeling approach for objects in the YCB Video dataset represent significant methodological contributions to the field of zeroshot object detection.  The paper is wellstructured providing a comprehensive literature review clear problem statement detailed methodology and a thorough discussion of the experimental results. Weak Points:  There is limited discussion on the comparison with existing methods particularly within the context of manufacturing environments. Including a performance comparison with traditional and other zeroshot learning approaches could strengthen the paper.  The paper focuses on detection (localization) and not on the classification aspect of zeroshot learning. This choice limits the models applicability in scenarios where knowing the specific type of object (beyond its physical attributes) is crucial.  The experiments primarily focus on recall rates without much discussion on precision or the overall F1 score which would provide a more holistic view of the models performance. Questions:  How does the proposed method compare to existing zeroshot detection models in terms of speed and accuracy particularly in dynamic industrial environments  Is there a specific reason for choosing only the YOLOv5 architecture Could the proposed modifications be applicable to other object detection frameworks like Faster RCNN or SSD  Could the authors elaborate on the challenges and potential solutions for integrating the classification task into their zeroshot detection model Feedback:  To enhance the papers contribution to the field it would be beneficial to include a comparative analysis with existing gZSD and object detection methods highlighting the unique advantages and potential limitations of the proposed approach.  Expanding the discussion on the impact of lighting conditions background variation and occlusions on model performance could offer deeper insights into the robustness of the proposed method.  Considering future work in extending the model to handle object classification in addition to detection would make the research more comprehensive and applicable to a broader range of manufacturing scenarios. 3) Review Summary: The manuscript presents a compelling approach to generalized zeroshot object detection using a modified YOLOv5 architecture tailored for dynamic manufacturing environments. The authors novel dataset splitting method and attribute labeling strategy are particularly noteworthy representing significant advances in the flexibility of robot vision systems. However to elevate the papers impact a comparative analysis with existing methods extending discussions on classification capabilities and providing a fuller picture of model performance metrics are recommended. Despite these opportunities for enhancement the paper makes a valuable contribution to the field of automated manufacturing and robotics showing promising avenues for future research in zeroshot learning and object detection.", "wogsFPHwftY": " Peer Review on the Paper presenting \"Feature Integrationbased Retrieval (FIRe) for Landmark Image Retrieval\"  1. Summary of the Paper This paper presents a novel approach called Feature Integrationbased Retrieval (FIRe) for landmark image retrieval which focuses on addressing the discrepancy between training and testing conditions observed in current stateoftheart methods that utilize local and global features. The core novelty of FIRe lies in its use of Superfeatures an ordered set of midlevel features constructed through an iterative attention module called the Local feature Integration Transformer (LIT). Superfeatures capture localized and discriminative patterns within images and are trained directly using a contrastive loss alongside a complementary decorrelation loss to ensure feature diversity. The proposed method shows significant improvements over existing stateoftheart techniques across common landmark retrieval benchmarks while also reducing the memory footprint supported by extensive experiments and evaluations.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Methodology: The introduction of Superfeatures and the LIT module for direct training on these features is a significant innovation. This approach addresses key limitations in existing methods by bridging the gap between training and testing conditions reducing redundancy and ensuring discriminativity of the features.  Extensive Experimental Validation: The methodology is validated across multiple landmark retrieval benchmarks demonstrating superior performance to stateoftheart methods. This is further accompanied by exhaustive ablation studies that meticulously analyze the impact of various components and parameters on the overall performance.  Efficiency and Scalability: The proposed method not only achieves higher accuracy but also does so with a reduced memory footprint addressing two critical aspects of landmark image retrieval  computational efficiency and scalability. Weak Points:  Lack of Realworld Testing: While the method shows impressive performance on standard benchmarks the paper does not discuss its applicability or performance in realworld scenarios beyond these datasets. Landmark image retrieval in practical applications may face additional challenges such as image occlusions dynamic lighting conditions and more diverse viewpoints.  Complexity and Interpretability: The complexity introduced by the iterative attention mechanism and the creation of Superfeatures may make it challenging to interpret the retrieval and matching process. Further insights into how Superfeatures correspond to specific image elements or patterns could enhance the understanding and trustworthiness of the method. Questions and Feedback:  How does the method perform under conditions of significant image occlusions or drastic viewpoint changes Are there any strategies that could be considered to further enhance robustness in such scenarios  Could the authors provide more insights into the interpretability of the Superfeatures Specifically it would be interesting to know if theres a way to visualize or understand what specific patterns or image segments these features are capturing.  Considering the iterative nature of the LIT module and the structured output of Superfeatures could there be potential extensions of this work towards tasks that require more detailed spatial understanding such as object detection or segmentation  3. Review Summary The paper introduces a compelling approach for landmark image retrieval that innovatively utilizes midlevel features termed Superfeatures constructed via an iterative attention module. This approach not only addresses the limitations of current stateoftheart methods in bridging the gap between training and testing phases but also showcases superior retrieval performance across standard benchmarks. The extensive experimental validation and the emphasis on efficiency and scalability are significant strengths of this work. However further exploration into the realworld applicability of the proposed method and enhancing the interpretability of Superfeatures could make this approach even more impactful. Overall the paper presents a solid contribution to the field of image retrieval suggesting promising directions for future research.", "u6ybkty-bL": " Review of \"Recurrent Deep Learning Methods for Time Series Outlier Detection: A Comparative Study\"  1. Summarize the paper The paper explores the efficiency of recurrent deep learning methods over nonrecurrent methods for detecting outliers in time series data. Specifically the focus is on when to apply recurrent neural networks (RNNs) to identify unusual data points or sequences that deviate significantly from the norm considering various types of outliers such as point outliers subsequencecollective outliers and outlier time series. The study involves testing multiple algorithms on synthetic and realworld datasets including a specialized dataset that tracks the movement of people living with dementia crossreferenced with hospital admissions and infection episodes. Two new outlier detection methods based on transformer models are introduced and evaluated against classical outlier detection techniques. The paper aims to determine the scenarios where recurrent methods are advantageous due to their capacity to learn complex temporal relationships supporting the detection of contextual outliers in time series data.  2. Strong and Weak Points Question and Feedback to the Author Strong Points:  Comprehensive Evaluation: The paper thoroughly evaluates both traditional and recurrent methods across various datasets with different contamination levels and types of outliers. This extensive testing provides a solid basis for comparison.  Innovation: Introducing two new methods based on transformer models enriches the field of outlier detection in time series data offering potential improvements over existing LSTMRNN techniques.  Relevance to Realworld Problems: The inclusion of a healthcare monitoring dataset related to dementia care presents a compelling application of the research highlighting the practical significance of improving outlier detection methods. Weak Points:  Model Complexity and Hyperparameter Tuning: While recurrent methods particularly those based on transformers are shown to have advantages the paper acknowledges the potential difficulty in hyperparameter tuning and the increased complexity of these models. This can limit the accessibility of the proposed methods for practitioners without deep technical knowledge or resources for extensive model training.  Limited Discussion on Explainability: The paper does not extensively address the explainability of the models particularly the transformerbased methods. Given the critical application areas (e.g. healthcare monitoring) understanding why a model flags an outlier is as important as the detection itself. Question and Feedback:  Question: Can the authors provide more insights into the hyperparameter settings for the transformer models and their impact on model performance across different datasets  Feedback: It would be beneficial for the authors to explore and discuss the explainability aspect of the outlier detection methods further. Specifically techniques or approaches that could help interpret the decisions made by recurrent models especially in sensitive domains like healthcare could enhance the papers contribution to realworld applications.  3. Review Summary The paper provides a valuable comparison between recurrent deep learning methods and nonrecurrent methods for time series outlier detection showcasing the potential of recurrent models particularly transformerbased techniques in capturing complex temporal relationships. The introduction of new methods and their evaluation across diverse datasets is commendable and contributes to the advancement of outlier detection methodologies. However the elevated complexity and hyperparameter sensitivity of these models raise important considerations about their practical application and accessibility for a broader audience. Additionally the paper slightly falls short in discussing the explainability of these complex models which is crucial for applications in sensitive domains such as healthcare. In conclusion while the paper effectively demonstrates the scenarios where recurrent methods surpass traditional outlier detection techniques enhancing the discussion around model complexity hyperparameter tuning and explainability would strengthen its contributions. The research presents a significant step forward in the field especially with the potential of transformerbased models in detecting sophisticated temporal dependencies within time series data. Recommendations for future work would ideally address these additional dimensions to fully harness the advantages of recurrent deep learning methods in practice.", "qpcG27kYK6z": " Peer Review for \"Learning 3D Representations of Point Clouds for Rotationally Robust Applications\"  Summarize the Paper This paper introduces a novel method for learning 3D representations of point clouds that are invariant to arbitrary rotations crucial for applications in computer vision and molecular modeling. The core innovation lies in a concentric spherical representation formed by nesting spheres discretized using an icosahedral grid. The authors propose unique intrasphere and intersphere convolutions within this structure facilitating learning of volumetric and rotationally equivariant representations over point clouds. They incorporate these convolutions into a new convolutional framework named CSGNN demonstrating its superiority in tasks like 3D object classification and determining the electronic structure in atomistic systems. The methods effectiveness is shown through rigorous experiments outperforming stateoftheart methods in rotational robustness and accuracy.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovation in Representation: The concentric spherical representation addresses the limitation of spherical CNNs in handling 3D data by allowing volumetric feature learning without information loss. 2. Rotationally Equivariant Convolutional Design: The approach elegantly combines intrasphere and intersphere convolutions ensuring rotational equivariance while scaling linearly with grid resolution which is significant for computational efficiency. 3. Experimental Validation: Comprehensive experiments demonstrate the methods stateoftheart performance in classifying rotated 3D objects and resolving electronic structures underlining the practical applicability and effectiveness of the proposed method. Weak Points: 1. Complexity and Comprehensibility: The complex mathematical formalism and convolution operations might be difficult for readers unfamiliar with graphbased convolutions and spherical harmonics. A simpler or more intuitive explanation could widen the papers accessibility. 2. Limited Ablation Studies: While the paper provides ablation studies on the number of concentric spheres and radial convolution kernel size more detailed experiments on each components individual contribution could offer deeper insights. 3. RealWorld Applicability and Scalability: Although experiments are conducted on synthetic and controlled datasets the scalability and effectiveness in realworld noisy or largescale datasets remain to be fully established. Questions and Feedback:  Could the authors clarify how the model performs under noisy conditions or with incomplete data common in realworld point clouds  It would be beneficial if future work could explore the integration of this approach with other data types or modalities such as combining point clouds with texture or other sensor data enhancing its utility in more complex scenarios.  Feedback includes considering extensions of this work that incorporate adaptive resolution techniques to balance the computational efficiency and accuracy further especially for outer spheres where finer resolution might be less critical.  Review Summary The paper presents a highly original and theoretically sound method for learning rotationally robust 3D representations of point clouds addressing a crucial challenge in fields like computer vision and molecular modeling. The proposed concentric spherical representation and novel convolutional approach offer significant improvements in handling rotations outperforming existing models in benchmark tasks. However addressing the noted weak points like improving comprehensibility and expanding ablation studies could further strengthen the paper. The demonstrated effectiveness in classification tasks and molecular modeling showcases the practical applicability of the method. Yet to solidify its realworld utility future studies should explore performance in more varied and noisy conditions along with scalability considerations for larger datasets. In conclusion this work marks a significant step forward in the representation and analysis of 3D point clouds with promising avenues for future research and applications. My recommendation is for acceptance provided the authors consider the feedback for improvement mainly clarifying realworld applicability and further expanding on the individual components contributions through additional ablation studies.", "yrD7B9N_54F": " Peer Review  1) Summary of the Paper This paper presents an adversarial trainingbased framework for improving link prediction in highly skewed and imbalanced graphs targeting fewshot learning scenarios often encountered in link prediction across multiple domains. The authors propose using a domain discriminator on graphlevel embeddings to make the embeddings domain agnostic thereby enhancing the models performance on underrepresented domains. Through experiments conducted on three benchmark datasets the authors demonstrate the efficacy of their method particularly when significant domain variance exists outperforming stateoftheart methods like SEAL and traditional heuristic methods.  2) Strengths and Weaknesses Strengths:  Novelty and Relevance: The approach of using adversarial learning to generate domainagnostic embeddings for graph link prediction is both novel and relevant addressing the significant challenge of dealing with imbalanced datasets in realworld applications.  Thorough Experimental Validation: The authors tested their framework across three different datasets providing a robust validation of their approach. The use of tSNE plots to demonstrate the effectiveness of their method is also commendable.  Clear Definition of Contributions: The paper clearly outlines its contributions making it easy for readers to understand the significance and the novel aspects of the work. Weaknesses:  Limited Discussion on Failure Cases: The paper lacks an indepth discussion on scenarios where the proposed method does not outperform the baselines significantly especially on the ogbgppa dataset. A more detailed analysis could provide insights into potential limitations or areas for improvement.  Model Complexity and Training Stability: While the paper discusses the impact of the adversarial components weighting (\u03b1) on performance there is limited discussion on the overall complexity and training stability of the proposed model especially in terms of convergence times and handling model collapse during training. Questions and Feedback to the Author:  Could you elaborate on the methods performance in scenarios with minimal domain variance as observed in the ogbgppa dataset Understanding these cases could help in identifying the boundaries of the proposed approachs applicability.  In terms of model training and stability how does the proposed adversarial framework fare against the baseline methods Specifically could you provide more details on strategies employed to prevent model collapse beyond the initial warmup stages  Have the authors considered exploring the impact of different graph neural network architectures within their framework It would be interesting to see if specific architectures are more conducive to learning domainagnostic embeddings.  3) Review Summary The paper introduces a compelling approach to improve link prediction in highly skewed and imbalanced graphs through an adversarial trainingbased framework. The methods novelty lies in its focus on generating domainagnostic embeddings to enhance performance across multiple domains. The experimental results validated on three benchmark datasets clearly demonstrate the potential of this approach especially in the presence of significant domain variance. However the paper could benefit from a deeper analysis of failure cases or scenarios where improvements are marginal. Additionally addressing concerns around model complexity and training stability particularly regarding the initialization and convergence of the adversarial framework could strengthen the paper further. Overall the paper contributes meaningfully to the domain of graph link prediction addressing a critical challenge through a wellvalidated and innovative approach. The clearly defined contributions and the practical implications of this work make it a valuable addition to the field.", "wqD6TfbYkrn": "Summary of the Paper This study introduces the Conditional Point DiffusionRefinement (PDR) paradigm a novel approach for 3D point cloud completion. The technique aims to tackle issues associated with current models that generate nonuniform points due to reliance on Chamfer Distance (CD) loss during training. The PDR method employs a twostep process starting with a Conditional Generation Network (CGNet) that utilizes a denoising diffusion probabilistic model (DDPM) to generate a coarse point cloud from Gaussian noise conditioned on partial observations. This is followed by the Refinement Network (RFNet) which further refines the quality of the point cloud. The system is enhanced by a novel dualpath network architecture for both CGNet and RFNet which effectively manipulates spatial locations for improved point cloud detail and smoothness. Extensive tests show that the PDR paradigm outperforms existing methods on various benchmark datasets potentially speeding up DDPM generation by up to 50 times with minimal quality loss. Strengths and Weaknesses: Strengths: 1. Novel application of DDPM for uniform point cloud generation addresses the nonuniformity issue prevalent in existing models. 2. The coarsetofine approach with CGNet and RFNet supported by the novel dualpath architecture enables the effective capture and refinement of point cloud details. 3. Demonstrated improvements over current stateoftheart methods across several benchmarks. 4. The ability to significantly accelerate the generation process without substantial loss in quality is a considerable achievement. 5. The paper is wellstructured with a clear explanation of methodologies and significant results verified through extensive experimentation. Weaknesses: 1. While the acceleration of DDPM is noted the potential implications on computational resources needed for training were not thoroughly discussed. 2. The discussion on limitations and potential failure cases of the proposed PDR paradigm is missing which could provide a more balanced view. 3. The extension to controllable generation is intriguing but this aspect feels underdeveloped compared to the main focus of the paper. Questions and Feedback: 1. Can the authors provide insights on the computational and memory requirements for training and inference using the PDR paradigm especially in comparison to existing methods 2. It would be beneficial to discuss potential limitations or scenarios where PDR might not perform as expected. Are there specific types of point clouds or conditions under which the methods efficacy is reduced 3. Regarding the extension to controllable generation can the authors share more details on how this process was implemented and its potential applications Review Summary: The paper presents a significant advancement in 3D point cloud completion with the introduction of the PDR paradigm. The novel approach successfully tackles the issue of nonuniform point generation by employing DDPM within a dualpath network architecture demonstrating both efficiency and effectiveness in generating highquality point clouds. The methodological clarity comprehensive testing and substantial improvements over current methods are commendable. However the paper would benefit from a more indepth discussion on resource requirements limitations and an expanded exploration of the controllable generation process. Considering these points the proposal holds great promise for advancing the field of 3D point cloud completion and with slight improvements in addressing the mentioned weaknesses this work could serve as a substantial contribution to the community.", "gggnCQBT_iE": " Summary of the Paper The paper introduces a novel framework for causal inference specifically targeting the challenges arising from modeling cyclic causal relationships\u2014a problem not efficiently addressed by existing causal models. The proposed metastructural causal model (metaSCM) framework advances the theoretical landscape of causal inference by conceptualizing causality as a process of information transfer. A distinctive aspect of metaSCM is the introduction of active mechanisms as a bridge between observed data and the array of underlying causal mechanisms offering a method to bypass the complexities associated with cyclic causality in structural causal models (SCMs). The paper also introduces the sufficient activated mechanisms (SAM) assumption detailing its relationship with prior assumptions in the field and demonstrating its utility in overcoming the limitations of conventional approaches to cyclic causality. Through theoretical proofs and examples the authors underscore the conceptual and theoretical innovations of the metaSCM framework positioning it as a significant contribution to the field of causal inference.  Strong Points 1. Novelty and Theoretical Contribution: The introduction of the metaSCM framework represents a significant theoretical advancement in the field of causal inference particularly in addressing cyclic causal relationships that have proven challenging for traditional models. The conceptual foundation of treating causality as information transfer and the operationalization of this concept through active mechanisms are both innovative and promising for advancing causal analysis. 2. Depth of Analysis: The paper provides a thorough examination of the proposed framework from its philosophical underpinnings to its mathematical formulation. The use of examples to illustrate unsolvable and cyclically causal scenarios and how the metaSCM approach can address these demonstrates a deep analytical approach to problemsolving in causal inference. 3. Relevance to Current Challenges: By focusing on cyclic causal relationships the paper addresses a crucial and unresolved issue in causal inference. This focus is timely and relevant given the increasing complexity of the systems being modeled in various disciplines.  Points Needing Improvement and Questions 1. Empirical Validation: While the theoretical contributions are notable the paper lacks empirical validation of the proposed framework. How does the metaSCM perform in realworld data scenarios especially compared to existing methodologies 2. Clarity on SAM Assumption: The SAM assumptions role is crucial but its operationalization is somewhat abstract. Could the authors provide more practical guidelines or examples on identifying and evaluating whether the SAM assumption holds in specific scenarios 3. Scalability and Computational Implications: Implementing metaSCMs especially with a potentially large number of active mechanisms might pose computational challenges. Can the authors discuss the feasibility of applying the metaSCM to largescale systems and datasets 4. Comparison with Existing Models: While the paper touches on how the metaSCM framework stands in relation to current models a more detailed comparison with specific models especially those dealing with cyclic causality could elucidate the unique benefits and limitations of the metaSCM approach.  Review Summary This paper makes a profound theoretical contribution to the field of causal inference by introducing the metaSCM framework aimed at addressing the longstanding challenge of modeling cyclic causal relationships. The novel concept of using active mechanisms as a bridge between data and causal mechanisms is both innovative and theoretically solid offering a fresh perspective on handling complex causal relationships. However the paper could be strengthened by addressing the lack of empirical validation clarifying the practical application of the SAM assumption discussing the scalability of the proposed approach and providing a more detailed comparison with existing models. Despite these areas for improvement the metaSCM framework presents a promising direction for advancing the theory and practice of causal inference especially in systems characterized by cyclic causality. The theoretical foundation laid by the authors has the potential to spur further research and application in this challenging domain making it a valuable contribution to the literature.", "rdBuE6EigGl": " Review of \"Improving Sequence Modeling using a Dual Connection Architecture\"  1. Summary This paper introduces a novel approach to enhance sequence modeling in natural language processing (NLP) tasks by modifying recurrent neural network (RNN) architectures. Specifically it proposes adding a direct connection from the input to the output bypassing the recurrent module termed a \"dual connection.\" The authors hypothesize that this architecture enables the model to maintain a direct dependency on input data alongside the processed information from the recurrent layers potentially improving prediction accuracy. They test this hypothesis using moderatescale language modeling datasets\u2014Penn Treebank and Wikitext2\u2014across various RNN architectures including standard LSTM doublelayer LSTM (dLSTM) and a mogrifier LSTM (mLSTM). The experiments demonstrate that models incorporating the dual connection consistently outperform their traditional counterparts in terms of perplexity scores.  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Innovative Approach: The papers proposal of a dual connection architecture is novel and addresses a welldefined problem in RNNs\u2014 the loss of direct input information through successive recurrent layers.  Comprehensive Evaluation: The authors thoroughly evaluate their approach across multiple RNN architectures and datasets providing a robust evidence base for their claims.  Significant Performance Improvement: The results convincingly show that the dual connection improves model performance in language modeling tasks sometimes substantially. Weak Points:  Lack of Theoretical Justification: While the empirical results are strong the paper lacks a theoretical discussion on why the dual connection improves performance. Such an analysis could provide deeper insights into the models behavior.  Limited Evaluation Metrics: The paper primarily focuses on perplexity scores. Including additional evaluation metrics such as BLEU scores for translation tasks or F1 scores for summarization could provide a more rounded view of the models capabilities.  Scalability and Generalization Concerns: The experiments are conducted on moderatesized datasets. The scalability of the proposed architecture to larger datasets and its generalization capabilities across different NLP tasks remain unaddressed. Questions and Feedback to the Author:  Could the authors elaborate on the theoretical underpinnings of why adding a direct inputoutput connection benefits the sequence modeling task Any intuition or related work that could ground this hypothesis would be valuable.  The paper could benefit from a broader evaluation across different NLP tasks and larger datasets to test the models generalizability and scalability. Are there plans to conduct such experiments  How does the introduction of the dual connection affect the models training time and computational complexity Given the increased performance an analysis of the tradeoff between efficiency and accuracy would be insightful.  The dual connection resembles skip connections used in convolutional neural networks (e.g. ResNets). Do the authors see any theoretical or practical parallels between these techniques in how they improve model performance  3. Review Summary The paper introduces a promising modification to RNN architectures for sequence modeling tasks demonstrating that a dual connection between the input and output can significantly improve performance on language modeling benchmarks. The experimental results are compelling and the approach is innovative. However the paper could be strengthened by addressing theoretical motivations for the proposed changes expanding the evaluation to include a variety of metrics and tasks and exploring the models scalability. Given the substantial performance improvements reported the approach merits further investigation and could have broad implications for sequence modeling tasks in NLP and beyond. The recommendations for improvements particularly regarding the theoretical grounding and broader applicability of the model are crucial for maximizing the impact of this promising research direction.", "voEpzgY8gsT": " Review of the Additive Poisson Process Paper  Summary The paper introduces the Additive Poisson Process (APP) a new methodology for modeling higherorder interaction effects in Poisson processes. This approach incorporates techniques from information geometry and generalized additive models to project multidimensional data into lowerdimensional spaces thereby addressing the curse of dimensionality. It specifically aims to estimate the intensity function of events (such as taxi pickups) in sparse data environments by leveraging lowerdimensional projections and solving a convex optimization problem. The paper details the technical underpinnings of the APP establishes its connection with existing models outlines the algorithm and presents empirical evidence of its superior performance in varying sparsity scenarios compared to traditional approaches.  Strengths 1. Novelty: The combination of information geometry with generalized additive models for approximating highdimensional intensity functions in Poisson processes is innovative. This provides a fresh perspective on tackling the sparse data problem. 2. Theoretical Foundation: The paper offers a strong theoretical background effectively bridging the gap between generalized additive models information geometry and Poisson processes. The use of the KolmogorovArnold representation theorem is particularly noteworthy. 3. Empirical Results: Empirical evaluations show the models efficacy in handling sparse observations with robust performance across different sample sizes and improved accuracy over stateoftheart methods which substantiates the proposed approachs practical value. 4. Realworld Application: The application of the APP model to the New York Taxi dataset to uncover common pickup patterns demonstrates its realworld relevance and potential impact.  Weaknesses 1. Complexity and Scalability: Despite demonstrating the models efficiency compared to existing methods the paper does not thoroughly address the computational complexity and potential scalability issues. Given the combinatorial increase in complexities with dimensions and order of interactions this aspect warrants a deeper discussion. 2. Comparison Baseline: While the paper includes a comparison with other models it lacks a comprehensive evaluation against a wider range of existing methodologies. Additionally the discussion on how the APP outperforms these models could be more detailed to highlight the unique advantages and potential limitations. 3. Implementation Details: The paper could benefit from more detailed exposition on implementation specifics particularly concerning the selection of hyperparameters M and h which are crucial for reproducibility and practical application. 4. Theoretical Assumptions: The assumptions underlying the proposed model such as the discretization of time and the homogeneous observation times across dimensions may limit applicability in scenarios where these conditions do not hold. Discussing how the model could adapt to more general settings would be beneficial.  Questions and Feedback for the Author 1. How does the APP models computational complexity scale with the dimensionality of the data and the order of interactions Are there strategies to mitigate potential scalability issues for very highdimensional data 2. Could the authors compare the APP model with a broader range of existing approaches and elaborate on the scenarios in which APP particularly outshines these alternatives 3. Are there specific criteria or recommendations for choosing the hyperparameters M and h particularly in cases where crossvalidation may not be feasible due to computational constraints 4. How might the assumptions of homogeneous observation times and discretization impact the models applicability to realworld datasets with heterogeneous observation intervals  Review Summary The paper presents a significant contribution to modeling higherorder interactions in Poisson processes through the novel APP framework. Its theoretical grounding combined with promising empirical results highlights the potential of this approach to effectively handle sparse datasets. However potential scalability concerns limited comparative evaluation and assumptions requiring further exploration could be addressed to strengthen the paper further. Overall the APP model represents a valuable advancement in the field offering fresh insights and practical utility in understanding complex interaction dynamics in varied domains such as transportation finance and ecology.", "t5s-hd1bqLk": " Review of \"Neural Network Conditioning by Learning Intermediate Layer Activations\"  1) Summary of the Paper This paper introduces a novel approach to conditioning neural networks through learned layerspecific activation functions influenced by conditioning vectors aiming to produce more compact and efficient models. Conditional neural networks crucial in various sequencetosequence modeling tasks often rely on conditioning vectors alongside inputs to tailor outputs. Traditional conditioning methods like concatenation and modulation significantly increase model size by inflating the dimensionality of intermediate features or requiring numerous additional parameters. The proposed method in contrast modifies intermediate activation functions dynamically based on conditioning vectors reducing the increment of model parameters substantially. The research particularly focuses on applications in personalized sound enhancement (PSE) and personalized automatic speech recognition (PASR) to showcase the effectiveness of the method. The study systematically explores learned activation (LA) functions across different model architectures and datasets demonstrating comparable or superior performance to traditional conditioning methods while achieving considerable savings in model size proposing an advantageous solution for resourceefficient deployments.  2) Strong Points and Weak Points along with Questions and Feedback to the Author Strong Points: a. Innovation: The paper pioneers a novel approach to neural network conditioning focusing on learning activation functions based on conditioning vectors. This strategy is distinctly different from traditional methods and introduces a new path for efficient network design. b. Comprehensive Evaluation: The authors conducted extensive experiments across multiple datasets (LibriSpeech and VoxForge) and models to validate the efficiency of the proposed method. These experiments not only compare the new method with standard conditioning techniques but also investigate its application in both PSE and PASR tasks providing a solid empirical foundation for its claims. c. Impactful Contribution: By significantly reducing model sizes without sacrificing performance the work presents a substantial contribution towards deploying sophisticated neural network models on resourceconstrained devices addressing a critical challenge in the widespread application of deep learning technologies. Weak Points: a. Limited Experimentation Domains: The paper\u2019s focus is predominantly on audio sequencetosequence modeling tasks specifically PSE and PASR which may limit its applicability insights. Exploring additional domains like generative modeling or computer vision tasks would provide a more comprehensive understanding of the methods versatility and effectiveness across different applications. b. Initialization Strategy Underexploration: While mentioning the importance of initialization choices for dynamically learned coefficients the paper lacks an indepth exploration of how different initialization strategies may impact the learning dynamics and final performance. c. Evaluation on Robustness Lacking: The paper evaluates the methods effectiveness primarily in terms of performance metrics and model size reduction. An evaluation of robustness particularly regarding the conditioning inputs quality (e.g. noisy or distorted conditioning vectors) would be beneficial to understand its practical deployment limits. Questions and Feedback: 1. Generalization Across Domains: Have the authors considered exploring the application of their method in domains beyond audio such as generative tasks or visionbased models Insights into such explorations would be valuable for understanding the methods general applicability. 2. Robustness to Quality of Conditioning Data: How does the performance of models conditioned using learned activation functions respond to variations in the quality of the conditioning vector especially in realworld scenarios where such vectors might not be clean 3. Hyperparameter Sensitivity Analysis: The paper introduces multiple hyperparameters including the selection of basic activations and initialization choices. Can the authors shed light on the sensitivity of model performance to these hyperparameters  3) Review Summary The submission offers a significant contribution to the field of neural network conditioning introducing a method that dynamically learns activation functions based on conditioning vectors. This approach addresses the critical issue of inflated model sizes associated with traditional conditioning techniques paving the way for the deployment of sophisticated network models on resourceconstrained devices. The comprehensive evaluation focusing on personalized sound enhancement and automatic speech recognition tasks substantiates the methods efficacy in maintaining or improving performance metrics while significantly reducing model sizes. However the paper would benefit from additional explorations beyond the audio domain to showcase the approachs versatility across different applications. Furthermore a deeper investigation into the methods robustness concerning the quality of conditioning vectors and a sensitivity analysis regarding introduced hyperparameters would enrich the research findings. Nonetheless the work opens promising avenues for efficient neural network design and deployment highlighting its potential impact across various machine learning applications.", "v3LXWP63qOZ": " Review of \u201cLeveraging Model Invariance for Learning Minimal Representations in Neural Networks\u201d  Summary The paper introduces a novel method MODINV (Model Invariance) aimed at learning minimal representations by incorporating multiple predictors over a single shared representation for various tasks. The approach stands on the premise that sparsity and minimality in learned representations can significantly enhance generalization in machine learning models. By employing a bottleneck architecture where multiple predictor heads are trained independently and with varying learning rates MODINV effectively creates a common representation that serves as an implicit invariance mechanism against spurious correlations thus promoting better generalization. The methodology is validated through experiments in both reinforcement learning (RL) settings using the DeepMind Control Suite and vision tasks utilizing CIFAR10 and STL10 datasets demonstrating substantial improvements over existing stateoftheart methods and baselines.  Strengths and Weaknesses Strong Points: 1. Innovative Approach: The concept of using multiple predictors over a single representation for learning minimal and sparse representations is novel and thoughtprovoking. It offers a fresh perspective on combating overfitting and improving generalization without increasing the complexity or computational cost of the model. 2. Broad Applicability: The application of MODINV across different domains (RL and vision) and its adaptability to various problem settings underscore its versatility and potential for wideranging implications in the field of deep learning. 3. Empirical Validation: Substantial experimental work demonstrates the method\u2019s efficacy with MODINV outperforming established methods in both RL and vision scenarios at different stages of training. Weak Points: 1. Lack of Theoretical Underpinning: While the paper presents a compelling empirical case for MODINV it lacks a strong theoretical analysis to back the observed improvements. Specifically deeper insight into why the diversification of predictors leads to better generalization would be beneficial. 2. Potential Limitations in Scalability: The paper does not discuss the scalability of MODINV. When applied to very largescale problems or models managing diverse learning rates and independent training of multiple predictor heads could become challenging. 3. Comparison with Baselines: The comparison with stateoftheart methods is thorough. However the paper could further reinforce its arguments by including comparisons with other approaches that utilize multiple predictors or representational sparsity. Questions and Feedback: 1. Can the authors provide additional theoretical analysis or intuition as to why the diversification of learning rates among predictors contributes to the methods effectiveness 2. How does the performance of MODINV scale with the complexity of the task or model size Are there practical limits to the number of predictors that can be effectively managed 3. The paper touches upon the implementation simplicity but could the authors elaborate on any challenges faced during implementation or integration into existing frameworks  Review Summary This paper introduces a promising approach MODINV for learning minimal representations through a model invariance mechanism involving multiple predictors over a shared representation. The method not only challenges conventional singlepredictor models but also opens new pathways in pursuit of sparse and generalizable neural network architectures. By demonstrating its effectiveness across both RL and vision tasks the authors have made a solid case for the practicality and adaptability of MODINV. However the paper could be strengthened by addressing its theoretical foundations more deeply offering insights into its scalability and potential limitations and perhaps a broader comparison with related approaches. Despite these areas for improvement the contributions of MODINV are noteworthy presenting a significant step forward in the quest for more robust and generalizable neural network models. Given the strong empirical results innovative methodology and the significance of the problem being addressed I recommend the acceptance of this paper. Future work as suggested by the authors along with a deeper theoretical exploration could further solidify the place of MODINV in the toolkit of methods for learning efficient neural network representations.", "y_tIL5vki1l": " Review  Summary The paper introduces LatentKeypointGAN a novel generative adversarial network (GAN) architecture designed for controllable image generation by integrating spatial and appearance factors of images without domain knowledge and supervision signals. Unlike traditional GANs reliant on paired images and transformations LatentKeypointGAN leverages unsupervised learning to identify keypoints within images providing a more flexible and domainagnostic approach to image manipulation. The authors demonstrate this models capability to perform detailed local edits such as repositioning or swapping facial features by adjusting the keypoint embeddings and also propose a new methodology for unsupervised keypoint detection.  Strong Points 1. Innovative Approach to Editability: The introduction of a twostage GAN architecture that includes keypoints as a latent embedding is innovative. This enables precise editing operations that were previously challenging such as rearranging image content spaciously and stylistically. 2. Unsupervised Learning of Keypoints: The methodology for unsupervised learning of keypoints and their application in image manipulation is particularly intriguing. The paper convincingly argues and demonstrates that the learned keypoints align meaningfully with the image landmarks which is a significant achievement in the unsupervised learning space. 3. Comprehensive Empirical Evaluation: The paper provides a detailed empirical study comparing LatentKeypointGAN with existing methods across various datasets showcasing its superior capability in generating realistic images and detecting keypoints unsupervisedly.  Weak Points 1. Lack of Clarity on Network Complexity: While the paper elaborates on the effectiveness of the proposed model it does not sufficiently address the computational complexity and scalability of the model. Given the detailed architecture understanding its computational demands and limitations would be essential for practical applications. 2. Insufficient Discussion on Limitations: The paper briefly discusses limitations at the end but does not integrate this discussion within the experiments section. Addressing limitations more thoroughly and suggesting possible solutions or future research directions could enhance the papers depth. 3. Limited Analysis on Disentanglement Mechanism: Although the disentanglement of spatial and appearance factors is a core contribution the paper could benefit from a more rigorous analysis or demonstration of how these factors are disentangled effectively beyond the qualitative examples provided.  Questions and Feedback 1. Computational Efficiency: Could the authors comment on the computational efficiency of LatentKeypointGAN especially in comparison to existing methods Understanding resource demands is crucial for assessing the models applicability in different scenarios. 2. Robustness and Generalization: How robust is the proposed model to variations in datasets or image domains not covered in the experiments Further elaboration on the models performance on outofdistribution data would be insightful. 3. Disentanglement Analysis: Could the authors dive deeper into the disentanglement process Specifically are there quantitative metrics or additional experiments that could highlight how effectively the model separates spatial and appearance factors  Review Summary The paper by introducing LatentKeypointGAN significantly advances the field of image generation and manipulation by enabling precise control over the generated images through an innovative unsupervised learning approach. The models ability to learn meaningful keypoints and utilize them for detailed editing tasks is both fascinating and demonstrably effective as shown in the extensive experimental evaluations. However to strengthen the paper further clarifying the computational aspects addressing the limitations more comprehensively and providing a deeper analysis of the disentanglement mechanism would be beneficial. Nevertheless LatentKeypointGAN presents a promising direction for research in image generation promising novel applications in digital content creation editing and beyond.", "v-v1cpNNK_v": " Peer Review for \"Neural Architecture Search at Initialization: Exploiting Neural Tangent Kernel\"  Summary The paper introduces a groundbreaking Neural Architecture Search (NAS) algorithm named NAS at Initialization (NASI) leveraging the Neural Tangent Kernel (NTK) to predict the performance of neural architectures without necessitating model training. This novel approach aims to significantly improve the search efficiency\u2014by eliminating the need for training during the search phase\u2014while maintaining or even surpassing the competitive generalization performance of existing NAS algorithms across various benchmarks like CIFAR10100 and ImageNet. The key innovation lies in the capability of NASI to estimate the performance of architectures at the initial stage using NTK making it possible to conduct the NAS as an optimization problem without training. Additionally NASI is demonstrated to be label and dataagnostic under certain conditions suggesting promising transferability of selected architectures across different datasets. The validity and efficacy of NASI are empirically evaluated on NASBench1Shot1 and the DARTS search space showcasing its superiority in search efficiency and effectiveness.  Strong Points 1. Innovative Approach: The paper addresses a critical bottleneck in NAS  the computational cost associated with model training. By proposing a method that bypasses this through the use of NTK for performance estimation at initialization it opens new avenues for efficient architecture search. 2. Theoretical Foundation: The utilization of NTK to estimate potential performance without training is wellgrounded in theory providing a robust basis for the proposed NASI algorithm. 3. Empirical Validation: Comprehensive experiments conducted on established benchmarks like NASBench1Shot1 CIFAR10100 and ImageNet underpin the search efficiency and effectiveness of NASI. These include comparisons not only with other NAS algorithms but also demonstrations of label and dataagnostic search capabilities. 4. Transferability of Architecture: The assertion and subsequent empirical evidence that architectures selected by NASI are transferrable across different datasets without performance degradation are notable. This is particularly relevant for applications in unsupervised learning or domains with limited data.  Weak Points and Questions 1. Computation Cost of NTK: While NASI circumvents the training process the paper mentions that NTK is \"prohibitively costly to evaluate.\" The exact computational cost and the efficiency of the proposed approximation method (gradient flow approximation) need more detailed discussion. How does the computational effort compare to the training avoided 2. Generalizability and Robustness: The experiments focus on image classification tasks and datasets. How well does NASI perform on tasks from other domains e.g. natural language processing or voice recognition 3. Impact of Hyperparameters: The penalty coefficient (\u00b5) and constraint constant (\u03bd) are critical to the NASI algorithm. More insight into their sensitivity and impact on the performance and selection process would be beneficial. 4. Data and Label Agnosticism: While NASI is demonstrated to be label and dataagnostic under \"mild conditions\" these conditions and their practical implications warrant further exploration. How robust is NASI across drastically different or more challenging datasets  Review Summary The paper presents a significant contribution to the field of neural architecture search with the NASI algorithm enabling efficient and effective architecture selection without the need for model training. By leveraging the Neural Tangent Kernel for early performance estimation NASI demonstrates remarkable search efficiency and generalization performance across several benchmarks alongside promising architecture transferability. However challenges such as the computational aspects of NTK evaluation broader applicability of NASI sensitivity to hyperparameters and the practical bounds of its label and dataagnostic capabilities raise questions that could be addressed in future work. Overall the advantages of NASI particularly in improving NAS efficiency and enabling architecture transferability make it a valuable contribution to the NAS field and a promising direction for further research and application in various domains.", "lP11WtZwquE": " Review of the Paper: \"Enhanced Pretraining Methods to Counteract False Negative Predictions in Discriminative Language Models\"  1) Summary The paper introduces innovative pretraining techniques for discriminative Pretrained Language Models (PrLMs) focusing on the faced challenge of false negative predictions. Discriminative PrLMs such as BERT are trained to distinguish between original and artificially corrupted texts. However conventional methods treat all corrupted samples as negative without verifying their validity leading to the inadvertent incorporation of false negatives. This not only reduces training efficiency but also compromises the robustness of the resulting models. To address this the authors define and tackle the false negative issue in PrLMs by proposing two novel pretraining objectives: soft regularization (SR) and hard correction (HC). These methods aim to correctly identify true negatives thereby enhancing training outcomes. Leveraging the ELECTRA architecture the paper demonstrates the improved performance and robustness of PrLMs trained with these methodologies on wellrecognized benchmarks such as GLUE and SQuAD.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality: The paper addresses a neglected yet significant problem in discriminative PrLM training \u2013 the false negative issue. The introduction of SR and HC pretraining methods is both innovative and practical.  Methodology: The comprehensive approach combining both soft regularization and hard correction provides a multifaceted solution to the addressed problem.  Empirical Evidence: The experimental results showing notable performance improvements on GLUE and SQuAD benchmarks strongly support the efficacy of the proposed methods. Weak Points:  Generalization to Generative PrLMs: While the paper focuses on discriminative models it does not explore the applicability or potential modifications required for generative PrLMs like GPT.  Variability in Synonym Sources: The paper benefits from WordNet for HC but does not thoroughly explore or compare the efficacy of using more diverse or dynamic synonym sources beyond WordNet and Word2Vec embeddings. Questions and Feedback: 1. Generalization of Methods: Can the proposed SR and HC methods be adapted for generative PrLMs If so what modifications would be necessary 2. Alternative Synonym Sources: Have the authors considered leveraging realtime contextaware synonym retrieval systems to enhance HC effectiveness further  3) Review Summary This paper makes a significant contribution to the field of NLP by addressing the overlooked issue of false negatives in discriminative Pretrained Language Models (PrLMs). The innovative pretraining strategies introduced namely soft regularization and hard correction not only mitigate the identified problem but also enhance the overall performance and robustness of PrLMs as evidenced by the experimental results. Nonetheless the exploration of these methodologies applicability to generative PrLMs and the potential for integrating more dynamic synonym sources could further elevate the studys impact. Overall the proposed methods demonstrate clear benefits in effectively training discriminative PrLMs making a substantial step forward in advancing the robustness and efficiency of language model pretraining. The paper is methodologically sound and the empirical evidence is convincing supporting a strong case for the adoption of these techniques in future PrLM training endeavors.", "zLb9oSWy933": " Summary of the paper The paper focuses on addressing the practical challenges around computing the Neural Tangent Kernel (NTK) for finitewidth neural networks an area critical yet computationally expensive within the domain of deep learning. The NTK critical for better understanding the training and generalization of neural network architectures faces computational and memory scalability issues when applied to finitewidth networks. The authors of the paper perform the first extensive analysis of these computational and memory requirements and introduce two innovative algorithms designed to significantly enhance the efficiency of computing finite width NTKs. These algorithms leverage inherent structural properties of neural networks to reduce the overall computational and memory burden. The implementations of these algorithms have been made openly available as JAX function transformations offering broad applicability across various neural network components without the introduction of new hyperparameters.  Strengths and Weaknesses of the paper Strong Points: 1. Innovative Methodology: The authors present two novel algorithms that significantly improve the computational efficiency and memory usage for calculating the NTK of finitewidth networks. This advancement could make NTKbased analyses more tractable for a wider range of network architectures and sizes. 2. Practical Relevance: Given the computational challenges previously associated with NTK computations for finitewidth networks the practical implications of this work are substantial. The methods enable more efficient model initialization architecture search and metalearning among others. 3. Broad Utility and Open Sourcing: The authors have not only developed methods that apply to a wide range of differentiable computations but also opensourced their implementations. This enhances the papers contribution to the deep learning community by making the proposed solutions readily testable and adaptable. Weak Points: 1. Potential Overhead Concerns: While the algorithms introduced aim to reduce computational and memory requirements the paper briefly touches upon the fact that practical gains might sometimes diverge from theoretical predictions due to various overheads. The specifics of these situations and how frequently they might occur were not deeply explored. 2. Limited Evaluation of Generalizability: The paper offers a comprehensive evaluation of the proposed methods on a range of neural network architectures. However it primarily focuses on convolutional and fully connected networks. More insights into how these algorithms perform with emerging or less conventional architectures would be valuable. 3. Complexity and Accessibility: The complexity of the paper both in terms of the mathematical formulations and the descriptions of the algorithms may make it less accessible to practitioners who are not deeply versed in the theoretical underpinnings of deep learning. Questions and Feedback to the Author:  Could the authors provide more details or examples of situations where practical gains from the proposed algorithms might not align with theoretical predictions Understanding these cases could help practitioners anticipate and mitigate such issues.  The paper demonstrates clear benefits for convolutional and fully connected networks. Could the authors speculate on or provide preliminary insights into the applicability and effectiveness of their algorithms on other network types such as graph neural networks or transformer models  Given the substantial reduction in computational and memory costs achieved could the authors discuss any observed or potential impact on the accuracy or stability of NTKbased analyses and tasks when using these algorithms  Review Summary This paper tackles the significant challenge of computational and memory efficiency in computing the Neural Tangent Kernel for finitewidth networks offering a critical contribution to deep learning research. By devising and opensourcing two innovative algorithms that leverage the structure of neural networks the authors not only push forward theoretical understanding but also provide practical tools that could transform how practitioners implement NTKbased methodologies in model development and analysis. Despite some limitations such as potential overheads and the need for broader validation across various network types the paper represents a substantial advance in making NTK computations more accessible and scalable. The strengths of this paper greatly outweigh its weaknesses marking it as an important contribution to the field.", "hyuacPZQFb0": " Review of \"ADATIME: A Benchmarking Approach for Time Series Domain Adaptation\"  1. Summary of the Paper This paper addresses the challenge of unsupervised domain adaptation (UDA) in time series data by proposing a comprehensive benchmarking suite named ADATIME. The primary aim is to systematically and fairly evaluate different domain adaptation methods on time series data mitigating inconsistencies found in existing works related to evaluation schemes datasets and neural network architectures. The authors firstly standardize the evaluation framework by selecting four benchmark datasets from healthcare and human activity recognition domains and by fixing the backbone neural network architecture for all experiments. Secondly the paper explores realistic model selection strategies that do not rely on labeled target domain data. Through extensive experiments the study evaluates ten stateoftheart domain adaptation methods on twenty crossdomain scenarios. The results show that with appropriate hyperparameter selection visual domain adaptation methods are competitive with or even superior to time series specific UDA methods. The authors also demonstrate the feasibility of selecting hyperparameters without access to labeled target data challenging the prevailing reliance on such data in existing approaches.  2. Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Evaluation: The systematic approach taken to evaluate a wide range of UDA methods under a controlled and standardized environment is commendable. This significantly contributes to the transparency and reproducibility of the results.  Innovative Model Selection Strategies: The exploration of realistic model selection approaches especially those that dispense with the need for labeled target domain data is particularly noteworthy. This directly addresses a major limitation in UDA research and practice.  Practical Insights: The paper offers valuable insights for applying domain adaptation methods to time series data contributing to the advancement of the field and providing a solid foundation for future research. Weak Points:  Dataset Diversity: While the selected benchmark datasets cover healthcare and human activity recognition the generalizability of the findings could benefit from the inclusion of datasets from additional domains with different characteristics.  Comparison with NonUDA Baselines: The paper focuses exclusively on comparing different UDA methods. Including nonUDA baselines could provide further insights into the absolute necessity and efficacy of UDA techniques in time series applications.  Discussion on Architecture Complexity: While the paper briefly touches on the impact of different backbone networks a deeper analysis of how architecture complexity influences UDA performance especially in light of overfitting concerns would be beneficial. Questions and Feedback: 1. Further Exploration of Datasets: Was the selection of datasets influenced by specific characteristics of time series data and how do the authors envisage the inclusion of more diverse datasets influencing the findings 2. Impact of Backbone Network Complexity: Can the authors provide more insights into how different complexities of the backbone network architecture would affect the performance of UDA methods on time series data 3. HyperParameter Sensitivity: Given the emphasis on hyperparameter selection how sensitive are the findings to hyperparameter variations and what implications does this have for practical applications  3. Review Summary The paper \"ADATIME: A Benchmarking Approach for Time Series Domain Adaptation\" makes a significant contribution to the field of domain adaptation by systematically addressing the challenge of evaluating UDA methods in time series contexts. The thorough experimental design alongside the exploration of innovative model selection strategies stands out as a substantial advance providing both theoretical and practical insights that pave the way for more effective application of UDA techniques in time series data. While the study demonstrates the competitive edge of visual UDA methods over time seriesspecific approaches and the feasibility of realistic model selection without labeled target data it would benefit from a broader exploration of datasets and a more detailed discussion on the role of backbone network complexity in UDA performance. Additionally comparing UDA methods with nonUDA baselines could further contextualize the benefits of UDA approaches. Overall the paper is wellstructured and its contributions are of high relevance and impact for researchers and practitioners in the domain adaptation field. It lays down a solid foundation for future research and opens up avenues for exploring domain adaptation across more diverse time series applications.", "tge0BZv1Ay": " Peer Review of \"Deep Reinforcement Learning for Scheduling in Semiconductor Manufacturing\"  1. Summary The paper introduces a novel approach leveraging Deep Reinforcement Learning (DRL) to tackle the complex problem of scheduling in semiconductor manufacturing systems. Traditionally operations have been managed using simpler often less effective methods due to the complexity of scheduling tasks. The proposed Predictron Deep QNetwork (PDQN) model integrates the Deep QNetwork (DQN) with the predictron planning model aiming to optimize the scheduling process by better handling the systems stochastic nature delayed rewards and dynamic uncertainties such as machine downtimes. Through simulated experiments modeled after realworld semiconductor manufacturing settings the PDQN is shown to outperform traditional scheduling policies (Critical Ratio and FIFO) and the standalone DQN model in minimizing the lateness of parts.  2. Strengths and Weaknesses Strengths:  Novelty: The paper presents an innovative approach by combining DQN with predictron models for scheduling filling a gap in research for complex manufacturing systems.  Relevance: Addressing the scheduling challenge in semiconductor manufacturing is highly pertinent due to the sectors complexity and economic importance.  Experimental Validation: The use of simulated environments based on realworld data for the evaluation instills confidence in the practical applicability of the proposed method. Weaknesses:  Lack of Comparison to Recent DRL Approaches: While the paper compares PDQN to traditional methods and DQN it does not benchmark against other recent DRL strategies that might be applicable to complex scheduling problems.  Limited Objective Function: The focus on minimizing lateness alone may not capture the full scope of the scheduling problem where other factors like throughput and cost are also critical.  Complexity and Reproducibility: The complexity of the proposed approach while a testament to its sophistication might pose challenges in reproduction and implementation in real settings without substantial expertise in both semiconductor manufacturing and DRL. Questions and Feedback: 1. Have alternative recent DRL approaches been considered for comparison to better situate PDQN among current methodologies 2. Could the authors elaborate on how PDQN might adapt to or incorporate other key metrics beyond lateness such as throughput or operational cost 3. A deeper discussion on the steps toward implementation in actual manufacturing settings could enhance the papers practical relevance.  3. Review Summary The paper makes a meaningful contribution to the field of operations research particularly in the intricate domain of semiconductor manufacturing scheduling. The introduction of PDQN reflects a promising avenue for leveraging deep reinforcement learning to address longstanding challenges in this area. However the focus on a singular objective metric and the absence of comparisons to a broader range of contemporary DRL methods slightly limit the scope of its contributions. The experimental results convincingly demonstrate PDQNs superiority over traditional methods and standalone DQN in minimizing part lateness. This outcome aligns well with the authors hypothesis that a blend of DQN and predictron models capable of managing delayed rewards and dynamic uncertainties would offer a robust solution to scheduling challenges. However for the research to have a stronger impact it would benefit from addressing its noted weaknesses. Expanding the objective function to account for more facets of the scheduling problem and drawing comparisons with more recent DRL approaches would offer a more comprehensive evaluation of PDQNs effectiveness. Moreover guidance on practical implementation could significantly enhance the papers appeal to practitioners in the field. In conclusion while the paper stands as a valuable contribution to operations research and the specific challenge of semiconductor manufacturing scheduling it has room for refinement and expansion to solidify its findings and extend its applicability.", "mhv2gWm3sf": " Review of the Paper \"A Generalized fDivergence Thermodynamic Variational Objective for Improved Variational Inference\"  Summary The paper introduces a novel approach for variational inference (VI) termed as the fdivergence Thermodynamic Variational Objective (fTVO). Building upon the Thermodynamic Variational Objective (TVO) which uses Kullback\u2013Leibler (KL) divergence fTVO generalizes it by incorporating arbitrary differentiable fdivergence enabling the approximation of the dual function of model evidence \\(f(p(x))\\) instead of the log model evidence log \\(p(x)\\) as in the conventional TVO. The methodology is grounded on a deformed \\(\\chi\\)geometry perspective which allows for the integration of fTVO along the \\(\\chi\\)path \u2014 a defined geodesic between the variational posterior distribution and the true posterior distribution. The optimization of fTVO employs the reparameterization trick and Monte Carlo approximation. Experiments conducted on Variational Autoencoders (VAE) and Bayesian neural networks demonstrate fTVO\u2019s superiority over baseline fdivergence variational inference models.  Strengths and Weaknesses Strengths: 1. Novel approach: The paper presents an innovative expansion of the TVO framework by incorporating a broad range of fdivergences enhancing flexibility in variational inference tasks. 2. Theoretical grounding: The introduction of the \\(\\chi\\)geometry perspective and the extensive mathematical formulation provide a solid theoretical foundation for fTVO. 3. Empirical validation: Experiments with VAEs and Bayesian neural networks establish the practical superiority of fTVO over traditional fdivergencebased methods offering empirical evidence of its effectiveness. Weaknesses: 1. Complexity of the methodology: The deep theoretical grounding while thorough presents a complex landscape that may be difficult for practitioners to navigate without a substantial mathematical background. 2. Limited scope of experimentation: While the experiments demonstrate improvement over baseline methods the scope is somewhat limited to VAEs and Bayesian neural networks. Additional domains could further validate the versatility of fTVO. 3. Lack of comparison with recent advances: There is no mention of comparison with the latest methods in variational inference that may not necessarily fit into the fdivergence framework which could provide a broader context for evaluating fTVO\u2019s performance. Questions and Feedback: 1. Could the authors elaborate on how fTVO might be adapted or extended to models beyond VAEs and Bayesian neural networks 2. It would be beneficial if insights into the computational complexity of implementing fTVO in practical applications were provided. Can the authors comment on this aspect 3. The paper would gain from a more detailed discussion on the potential limitations of adopting the \\(\\chi\\)geometry perspective in variational inference and any scenarios where fTVO might not yield optimal results.  Review Summary The proposed fTVO represents a significant contribution to the field of variational inference offering a new avenue for enhancing model evidence approximation through the generalization of the divergence term. The theoretical underpinnings grounded on \\(\\chi\\)geometry and the impressive empirical results underscore the potential of fTVO to improve the performance of variational inference models profoundly. However the complexity of the methodological framework and the relatively narrow scope of experimental validation point to areas where further work could enhance the paper\u2019s impact. Expanding the range of applications and offering more insights into the practical implementation challenges could make the approach more accessible and applicable to a broader audience. Nonetheless the paper sets a promising direction for future research in VI by convincingly demonstrating that generalizing the divergence term to arbitrary fdivergences under a thermodynamic integration framework can yield superior inference outcomes.", "l8It-0lE5e7": "Summarize the paper The paper investigates the implicit bias introduced by adversarial training in homogeneous deep neural networks (DNNs) including both linear and nonlinear networks without explicit regularization. The authors set out to answer whether adversarial training for DNNs imposes any implicit bias and if so the nature and implications of this bias. They focus on adversarial training using \\(\\ell2\\) \\(\\ell\\infty\\) FGSM FGM and PGD perturbations. For deep linear networks they prove that the direction of the weight matrix products converges to the maxmargin solution of the dataset. For nonlinear networks its shown that the normalized parameters converge to a KKT point of a marginmaximizing constrained optimization problem for adversarial examples. These findings theoretically support the notion that adversarial training refines the decision boundary to enhance robustness. The paper also includes numerical experiments on the MNIST dataset to corroborate the theoretical insights. Strong Points: 1. Theoretical Significance: Providing a theoretical understanding of the implicit bias in adversarial training for both linear and nonlinear networks represents a significant contribution to the literature on DNN robustness. 2. Comprehensiveness: The paper covers a range of adversarial perturbations offering a broad view of adversarial trainings effects across different attack strategies. 3. Empirical Validation: Numerical experiments with MNIST support the theoretical claims adding empirical credibility to the study. Weak Points: 1. Generalizability to RealWorld Datasets: The numerical experiments are limited to MNIST which is relatively simple. Including more complex datasets could provide a deeper insight into the practical implications of the findings. 2. Assumptions and Constraints: The assumptions made (e.g. separability for adversarial examples) might not always hold in practical scenarios. A discussion on the limitations these assumptions impose on the applicability of the results would be beneficial. 3. Comparison with Other Techniques: While adversarial training is compared to standard training a more extensive comparison with other defense mechanisms could offer a richer perspective on its relative effectiveness and limitations. Questions and Feedback to the Author: 1. Could the authors elaborate on the realworld applicability of their findings particularly concerning complex and nonseparable datasets 2. Are there practical strategies the authors could recommend for leveraging the insights on implicit bias to develop more effective adversarial training techniques 3. How do the findings apply to nonhomogeneous networks and what would the implications be for such models Review Summary: This study makes a valuable theoretical contribution to understanding the implicit biases introduced by adversarial training in DNNs and sheds light on how these biases affect the models robustness against adversarial attacks. The thorough theoretical analysis covering a wide range of adversarial perturbations and both linear and nonlinear networks is commendable. The empirical evidence from MNIST experiments further bolsters the validity of the theoretical claims. However the paper would benefit from addressing the generalizability of the results to more complex realworld datasets and scenarios where the assumptions made might not hold. Additionally expanding the discussion to include comparisons with a broader range of defense mechanisms could provide a more nuanced understanding of where adversarial training stands within the ecosystem of DNN robustness strategies. Overall this paper offers critical insights that could pave the way for developing more robust DNN architectures and training methodologies.", "p7LSrQ3AADp": " Review of the Paper: \"A Framework for Characterizing and Comparing Saliency Methods\"  1. Paper Summary This paper introduces a novel framework for evaluating and comparing saliency methods used in machine learning (ML). The authors argue against the traditional focus on \"faithfulness\" as the sole criterion for assessing saliency methods suggesting instead that these methods serve as abstractions of model behavior with inherent tradeoffs. The proposed ninedimensional framework categorizes saliency methods based on how they compute saliency (methodology) their relationship with the model or input data (sensitivity) and how their outputs are perceived by users (perceptibility). These dimensions are designed to break down the singular notion of faithfulness allowing for a nuanced understanding and comparison of different saliency methods. To illustrate the utility of their framework the authors apply it to document various saliency methods (\"saliency cards\") and to guide the selection of saliency methods in a radiology use case. They identify potential research directions by analyzing current gaps in the saliency method landscape.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Framework: The ninedimensional framework is a significant contribution offering a nuanced approach to understanding and comparing saliency methods beyond the binary measure of faithfulness. This perspective is refreshing and adds depth to the analysis of explainability in ML systems.  Practical Applications: The paper demonstrates the frameworks practicality through use cases such as the radiology example which provides clear guidance on how the framework can be applied in realworld settings. The concept of \"saliency cards\" is also a novel tool for documenting and communicating the nuances of different saliency methods.  Identification of Research Gaps: The framework does an excellent job highlighting underexplored areas within the field of saliency methods guiding future research efforts to fill these gaps. Weak Points:  Limited Evaluation: While the paper illustrates the frameworks application with examples there is a noticeable lack of quantitative analysis to support the efficacy and accuracy of the framework in distinguishing and comparing saliency methods.  Potential Overemphasis on Abstraction: The drive to frame saliency methods as abstractions might overlook the importance of developing more faithful explanation methods that do not sacrifice as much detail especially in critical applications. Questions and Feedback:  Could the authors provide empirical evidence or a case study demonstrating the frameworks effectiveness in aiding decisionmaking among ML practitioners For example how does the framework influence the choice of saliency methods in a new unseen ML task  How does the framework accommodate the rapid evolution of ML models and saliency methods Is there a process for updating or expanding the dimensions as new challenges emerge  The concept of saliency cards is intriguing but how do the authors envision their adoption in practice Are there any plans to develop a repository or platform for sharing and comparing these cards  3. Review Summary The paper presents a wellconceived framework for analyzing and contrasting saliency methods used in ML models. The shift away from a singular focus on faithfulness towards a more holistic analysis of saliency methods is both timely and necessary given the increasing complexity and deployment of ML systems in critical domains. The introduction of \"saliency cards\" and the detailed framework provide valuable tools for both researchers and practitioners to understand and communicate the tradeoffs involved in selecting saliency methods. However the paper could be strengthened by including more empirical evidence or quantitative analysis to validate the frameworks practical utility. Additionally considering the rapid advancement of ML and saliency methods the authors should elaborate on how the framework can be kept uptodate. Overall the paper makes a significant contribution to the field of interpretability in ML with the potential to influence future research directions and the development of more nuanced and effective saliency methods. Its recommendations for future work are well justified and if addressed could lead to a deeper understanding of how saliency methods can be optimally selected and applied across various ML tasks and domains.", "gLqnSGXVJ6l": " Review of the Paper on Solving Vehicle Routing Problem with Time Windows Using Reinforcement Learning and Neural Networks  1. Summary: The paper presents a novel framework for solving the Vehicle Routing Problem with Time Windows (VRPTW) using a combination of neural networks specifically an Attention Model (AM) and Reinforcement Learning (RL). The vehicle routing problem is a complex combinatorial optimization problem and the addition of time windows adds another layer of complexity. The proposed model aims to generate nearoptimal solutions for VRPTW instances without the need for manual adjustments or prior heuristic knowledge. The model architecture consists of an encoderdecoder configuration where the encoder processes inputs into embeddings which are then used by the decoder to construct the solution sequence. The model is trained in a reinforcement learning environment leveraging a stochastic policy gradient to optimize the predicted solutions. The authors evaluate the performance of the model against existing baselines using synthetic data demonstrating superiority in terms of solution quality (average tour length) and computational efficiency (inference time) for small to mediumsized problem instances.  2. Strong and Weak Points Questions and Feedback: Strong Points:  The integration of reinforcement learning with neural networks to solve the VRPTW represents a significant advancement in the field of combinatorial optimization particularly in terms of automating solution generation.  The paper is wellstructured systematically presenting the problem the proposed solution and the experimental results which substantiates the effectiveness of the approach.  Utilizing synthetic data enables the controlled evaluation of the models performance and provides a clear comparison against established baselines. Weak Points:  The paper primarily focuses on synthetic data for evaluation which might not capture the complex dynamics and variability inherent in realworld VRPTW instances.  While the model outperforms existing baselines on small to mediumsized instances its unclear how it scales to larger problem sizes. This scalability is crucial for practical applications.  The training and inference computational requirements are not discussed in depth which would be relevant for understanding the practicality of deploying such a model in realtime applications. Questions:  How does the models performance scale with increasing problem size especially for realworld instances that may have hundreds or thousands of nodes  Can the authors provide insights into the models computational efficiency regarding training and inference times and how they might impact realworld deployment  How does the model handle instances where data significantly deviate from the training distribution considering the variability in realworld VRPTW instances Feedback:  It would be beneficial to include a detailed analysis of the model\u2019s performance on realworld VRPTW data sets to further validate its applicability and robustness in practical scenarios.  Expanding the discussion on the models computational requirements (both hardware and time for training and inference) would provide a clearer picture of its feasibility for realworld applications.  Including a sensitivity analysis regarding the models parameters might offer additional insights into its behavior and efficiency across different problem instances and sizes.  3. Review Summary: The paper contributes a novel methodology employing reinforcement learning and neural networks to solve the VRPTW demonstrating significant advancements in generating nearoptimal solutions without manual heuristic adjustments. The proposed framework showcases notable improvements in solution quality and computational efficiency over existing methods for small and mediumsized problem instances. However the paper would benefit from addressing concerns related to realworld data applicability scalability to larger problem sizes and a detailed discussion on computational requirements. Overall the work opens promising avenues for future research in combining machine learning with combinatorial optimization problems. Nonetheless for a strong recommendation of acceptance the authors should consider augmenting the manuscript with further analyses on realworld data applicability scalability and computational feasibility to strengthen the papers relevance and applicability to practical VRPTW scenarios.", "xMJWUKJnFSw": " Peer Review for \"NodePiece: ParameterEfficient Learning of Node Representations in Knowledge Graphs\"  Summary of the Paper This paper introduces NodePiece an innovative approach to generating node representations in Knowledge Graphs (KGs) that contrasts with conventional methods requiring unique embeddings for each entity which are often costly both in terms of memory and computational resources. Inspired by subword tokenization techniques in Natural Language Processing NodePiece proposes a more parameterefficient strategy by learning a fixedsize vocabulary from anchor nodes and relations within the graph. This methodology allows for any entity including those not seen during training to be represented as a combination of these anchor nodes and relation types dramatically reducing the total number of parameters required. The authors demonstrate through various experiments that NodePiece is competitive in node classification link prediction and relation prediction tasks while requiring significantly fewer parameters than existing shallow embedding models.  Strong Points 1. Innovative Approach: The paper introduces a novel idea of using anchorbased subentity tokenization as a means to compress node representations in KGs drawing inspiration from subword tokenization in NLP. This approach is creative and addresses a key problem in KG representation learning  the scalability issue due to the linear relationship between the number of nodes and the parameter size. 2. Comprehensive Experiments: The paper presents a thorough experimental analysis comparing NodePiece against stateoftheart methods across different datasets and tasks. The experiments convincingly show that NodePiece can achieve competitive performance with significantly fewer parameters. 3. Inductive Learning Capability: The ability of NodePiece to handle unseen entities by leveraging a fixedsize vocabulary is a considerable advantage improving the models generalization and application to realworld scenarios where KGs are continuously evolving.  Weak Points and Questions 1. Model Complexity: While the paper emphasizes parameter efficiency there is less discussion on the computational complexity of NodePiece especially regarding the tokenization process and the encoding function. How does the computational complexity of NodePiece compare with conventional methods during inference 2. Impact of Anchor Selection: The paper mentions that random anchor selection was found to be effective but also suggests that other deterministic strategies might be better suited for specific tasks. Can the authors clarify the impact of different anchor selection strategies on the performance and whether theres a systematic way to choose these anchors based on the graph topology or the task at hand 3. Scalability and Limitations: Although NodePiece shows excellent performance on various tasks its scalability to extremely large KGs (e.g. billions of nodes) is not discussed. Are there inherent limitations in the anchorbased tokenization approach when scaling to massive KGs 4. Ablation Studies and Encoder Choices: More details on the ablation studies could solidify the papers contributions. Specifically how do different components (anchor distances relational context types of encoders) contribute to the overall performance Moreover the choice of MLP and Transformer as encoders is justified but did the authors explore other encoding mechanisms and how might these impact the results  Review Summary The paper presents a significant advancement in KG representation learning by proposing a novel parameterefficient approach for node embedding. The concept of using a fixedsize vocabulary of anchors and relations for node tokenization is both innovative and practically relevant addressing the memory consumption and computational cost issues observed in conventional embedding methods. The experimental results are persuasive demonstrating NodePieces effectiveness across different KG tasks while offering substantial reductions in parameter count. However the paper would benefit from a deeper exploration of the computational complexity involved the impact of various anchor selection mechanisms and its scalability to even larger KGs. Additional insights on ablation studies and the exploration of other encoding methods could also provide further evidence of NodePieces advantages and versatility. Overall the papers strengths significantly outweigh its weaknesses presenting a compelling case for NodePiece as a valuable contribution to the field of KG representation learning. With improvements in addressing the outlined concerns this work has the potential to set a new direction for research in scalable and efficient KG embeddings.", "mmUA7_O9mjY": " 1. Summary The paper presents CPDeform a novel framework designed to address the challenge of finding optimal contact points for softbody manipulation using differentiable physics. Differentiable physics despite its effectiveness in gradientbased optimization for softbody tasks suffers from the issue of local minima particularly when initial contact points are suboptimal or when tasks require switching contact points across multiple stages. CPDeform seeks to overcome these limitations by incorporating an optimal transportbased method for discovering contact points thus enabling the differentiable physics solver to navigate past local minima and achieve task completion more efficiently. CPDeform operates by comparing the current softbody state to the target state and employing transport priorities from optimal transport calculations to identify advantageous initial or subsequent contact points for the manipulators. This approach is shown to enhance the solvers ability to perform both single and multistage soft body manipulation tasks as evidenced by the experimental results obtained from the newly introduced PlasticineLabM benchmark which extends the original PlasticineLab with more complex tasks.  2. Review  Strong Points  Novelty: The integration of optimal transportbased contact point discovery with differentiable physics is innovative and addresses a significant challenge in softbody manipulation tasks.  Practical Relevance: CPDeform has a wide range of potential applications in fields such as healthcare cooking and manufacturing making it highly relevant and beneficial for advancing robotics and automation.  Comprehensive Evaluation: The introduction of the PlasticineLabM benchmark for evaluation and the extensive experimental results demonstrate the effectiveness and robustness of CPDeform across a variety of tasks.  Weak Points  Generalization beyond Plasticine: While impressive within the confines of the PlasticineLabM environment the paper does not discuss the generalization of CPDeform to realworld scenarios or different types of soft materials with potentially more complex behaviors.  Computational Efficiency: The paper lacks a thorough analysis of the computational demands associated with integrating optimal transport into the differentiable solver which could be significant given the iterative nature of the approach and the complexity of multistage tasks.  Limitations on Shape Topology: The authors acknowledge that CPDeform might struggle with drastic topological changes but provide limited insights into potential remedies or workarounds for this issue.  Questions and Feedback  How well does CPDeform generalize to realworld manipulation tasks with actual robotic end effectors and materials with properties that might differ from the simulated plasticine  Can the authors elaborate on the computational efficiency of CPDeform particularly in comparison to other methods that aim to solve similar tasks  Given the limitations mentioned concerning dramatic topology changes what future directions do the authors envision for addressing these challenges  3. Review Summary CPDeform represents a significant advancement in the field of softbody manipulation offering a novel approach to overcome the limitations associated with local minima in differentiable physics. By leveraging optimal transport for contact point discovery the framework demonstrates clear improvements in task performance especially in complex multistage environments as evidenced in the PlasticineLabM benchmark. The practical implications of this work are substantial with potential applications across several domains. However the paper leaves open questions regarding the generalizability of the approach to realworld scenarios and the computational demands of incorporating optimal transport into differentiable physics solvers. Additionally the acknowledged limitations concerning shape topology changes point to the need for further research. Overall CPDeform is a promising framework that addresses a critical challenge in softbody manipulation paving the way for more effective and efficient methods. Future work focusing on generalization computational efficiency and addressing topological changes will be crucial for extending the applicability and effectiveness of such innovative approaches in realworld tasks.", "tD7eCtaSkR": " Review of \"Certifying Robustness of 1Lipschitz Convolutional Neural Networks with Householder Activation Functions\"  Summary This paper addresses the pivotal role of maintaining a strict Lipschitz constraint under the l2 norm in training convolutional neural networks (CNNs) for enhancing adversarial robustness interpretabilty of gradients and ensuring stable training. The novelty of this work lies in its approach to designing 1Lipschitz CNNs by relaxing the orthogonalization requirement in the last linear layer introducing a novel Gradient Norm Preserving (GNP) activation function known as the Householder activation and a certificate regularization process for robustness certification. These proposed methodologies collectively lead to significant improvements in both standard and provable robust accuracies on CIFAR10 and CIFAR100 datasets thereby addressing limitations of existing 1Lipschitz CNNs with respect to scalability and robustness guarantees at larger l2 radii.  Strong Points 1. Novel Methodologies: The introduction of Last Layer Normalization (LLN) Certificate Regularization (CR) and Householder (HH) activation functions are significant contributions that address previous limitations in training 1Lipschitz CNNs for robustness. These methods collectively enable the maintenance of a balance between standard and provable robust accuracy. 2. Theoretical Foundation: The paper provides a solid theoretical backbone for each of the proposed methods including a rigorous proof demonstrating that the Householder transformation is necessary for any GNP piecewise linear function to maintain continuity. 3. Extensive Experimental Validation: The authors conducted comprehensive experiments that clearly demonstrate the advantages of their approach over existing methods. The gains in both standard and provable robust accuracies across different architectures and datasets provide strong empirical support for their claims.  Weak Points 1. Overlooked Comparison with Similar Work: While the paper positions its contributions as advancements over existing methods it lacks a detailed comparison with similar works that have also aimed to improve the training and certification of 1Lipschitz CNNs particularly in the context of adversarial robustness. A more thorough discussion on how the proposed methods differentiate or improve upon these would strengthen the paper. 2. Potential for Broader Impact Discussion: The paper focuses on CIFAR10 and CIFAR100 for experiments. Though these datasets are standard discussing or hinting at the potential impact of the proposed methods on larger and more complex datasets or different domains would enhance the scope of the findings. 3. Computational Efficiency: The paper does not discuss the computational implications of implementing the proposed methods specifically the Last Layer Normalization and Certificate Regularization in detail. Given that these methods are additional steps in training understanding their computational overhead or efficiency is crucial.  Questions and Feedback to Author 1. Comparison with Parallel Works: Could the authors provide more insights or a dedicated discussion on how their approach compares with other recent methodologies aimed at enhancing the robustness of 1Lipschitz CNNs 2. Extension to Other Datasets: Have the authors considered evaluating their proposed methods on datasets beyond CIFAR10 and CIFAR100 particularly larger datasets or those from different domains to assess scalability and generalizability 3. Computational Overhead: Can the authors comment on the computational efficiency of the proposed Last Layer Normalization and Certificate Regularization processes Specifically how do they impact the training time and computational resources required compared to traditional methods  Review Summary In summary this paper presents significant advancements in training 1Lipschitz CNNs for achieving higher standards of provable robustness. The introduction of novel methodologies namely Last Layer Normalization Certificate Regularization and Householder activation functions addresses critical challenges in the field. The theoretical and empirical evidence provided robustly supports the claims of enhanced standard and provable robust accuracies. However the paper would benefit from a more detailed comparison with similar works a discussion on scalability and generalizability and clarity on the computational efficiency of the proposed methods. Overall this work makes a notable contribution to the area of robust machine learning particularly in the design and training of adversarially robust CNNs.", "zxEfpcmTDnF": " Review Summary The paper presents an innovative study on leveraging the structural properties of the Variational Autoencoder (VAE) model to mimic the classical sourcefilter model of speech production. Focusing on the fundamental frequency and formant frequencies\u2014which are crucial for speech signal characterization\u2014the work demonstrates for the first time that these elements are encoded in orthogonal latent subspaces of the VAE. Through a weaklysupervised approach utilizing a few seconds of artificially generated labeled speech signals the paper outlines a method for accurately controlling these speech factors within the identified subspaces thereby proposing a new model for speech signal transformation and generation without the need for text or humanlabeled data.  Strong Points 1. Novelty and Relevance: The paper addresses a novel and nontrivial problem by intersecting deep generative modeling with the classical sourcefilter model of speech. Its findings have significant implications for speech signal processing and representation learning offering a new avenue for explorations in unsupervised learning and speech synthesis. 2. Methodological Rigor: The methodology is thorough with clear steps on training the VAE identifying and controlling latent subspaces and validating the weaklysupervised approach. The use of orthogonal subspaces for fundamental frequency and formants within the VAE\u2019s latent space is mathematically grounded and welljustified. 3. Experiments and Evaluation: The experimental setup is comprehensive covering aspects from orthogonality of latent subspaces to modifications in speech spectrograms and generation. The qualitative and quantitative analyses including comparisons with traditional signal processing methods and other baselines effectively demonstrate the advantages of the proposed approach.  Weak Points 1. Dataset and Generalizability: The experiments primarily focus on the English language and utilize datasets that might not fully represent the diversity of speech across different languages and dialects. Generalizability of the method across various languages and the effectiveness in capturing idiosyncratic traits of speech warrants further investigation. 2. Complexity of Implementation: Although the paper outlines the methodology in detail the intricacies in implementation and the need for a few seconds of labeled data might pose challenges in practical scenarios. The ease of applying this method outside academic research especially in realworld applications is not fully addressed. 3. Consideration of Noise and Variability: The consideration of how external factors such as background noise recording quality and speaker variability affect the performance of the proposed model is somewhat overlooked. Moreover the impact of these factors on the orthogonality of latent subspaces and the accuracy of speech factor control could be further explored.  Questions and Feedback to the Author 1. Dataset Diversity: Could the authors comment on the potential applicability and adaptability of the proposed method across languages and dialects that were not covered in the experiments 2. Practical Implementation Challenges: Are there identified challenges in implementing the proposed method in realworld speech processing applications And if so what are potential strategies to address them 3. Impact of Noise and Variability: How do factors such as background noise and speaker variability affect the models performance and what measures can be taken to mitigate these effects  Review Summary The paper presents a compelling intersection of deep learning and traditional speech signal processing by leveraging a VAE to analyze and control aspects of speech based on the classical sourcefilter model. This unique approach not only advances the stateoftheart in speech representation learning but also opens new directions for research in speech synthesis and transformation. The novelty of the method combined with thorough experiments and clear potential for broader impact across languages and speech technologies makes this work a valuable contribution to the field. However addressing the outlined weak points particularly regarding the diversity of datasets and practical implementation considerations would further strengthen the paper. The exploration into the effects of noise and speaker variability on the models accuracy and the proposal of mitigation strategies will also be crucial for transitioning this academic work into practical applications. Overall the paper is wellstructured and presents significant advancements in speech representation learning laying a foundation for future explorations in this area.", "uF_Wl0xSA7O": " Peer Review  1) Summarize the paper This paper introduces a new gradientbased multitask learning (MTL) method designed to improve the training balance in multitask systems by aligning independent components of the training objective gradient. Unlike existing methods this novel approach maintains a stable ratio between the gradients of highly correlated tasks handling large differences in task gradient magnitudes more seamlessly. It is scalable mitigates overfitting and offers a computational efficiency that is applicable to largescale problems. The authors evaluate their method across a range of MTL problems such as digit classification multilabel image classification camera relocalization and scene understanding showing favorable comparisons to other gradientbased adaptive balancing methods. The key innovations include the dynamic construction of consistent losses a novel metric called the dominance rate for measuring gradient dominance and empirical demonstrations of superior performance on various MTL benchmarks.  2) Strengths and Weaknesses Strengths:  Novel Methodology: The paper proposes a unique solution to gradient dominance in MTL systems by introducing an orthogonal component alignment approach a significant step forward in addressing gradient interference issues.  Comprehensive Evaluation: It evaluates the proposed method on a variety of benchmarks covering a broad spectrum of MTL applications demonstrating its versatility and effectiveness.  Theoretical Support: The mathematical formulations and theorems provided offer a strong theoretical foundation for the novel method enhancing its credibility and reproducibility.  Efficiency: The introduction of an efficient alignment for encoderdecoder networks addresses the scalability challenge making the approach feasible for largescale applications. Weaknesses:  Implementation Details: The paper could benefit from clearer implementation details for some of the more complex experiments to foster reproducibility.  Comparison with the StateoftheArt: While the paper compares its approach to existing methods a more detailed discussion on how it fares against the very latest MTL approaches would be beneficial.  Hyperparameter Sensitivity: There is little discussion on the sensitivity of the proposed method to hyperparameter settings which could be crucial for practitioners. Questions and Feedback to Authors:  Could the authors provide more insight into the choice of task preferences vector (`w`) and its impact on model performance  How robust is the proposed method to hyperparameter choices especially in terms of step size (`\u03b1`) and the initial rate of dominance (`D(G\u03b8)`)  Are there specific scenarios or types of MTL problems where the proposed method might not perform as expected and how can such cases be identified  3) Review Summary The paper presents a compelling new approach to addressing one of the central challenges in multitask learning: maintaining a balance in training multiple tasks simultaneously without letting one tasks gradient dominate the learning process. Through the novel concept of aligning orthogonal components of the training objective gradient and introducing the dominance rate metric the authors offer both a theoretical and practical advance in MTL. The extensive benchmarks across different domains validate the methods superiority over current stateoftheart solutions in terms of both performance and computational efficiency. However the paper could improve by delving deeper into the practical aspects of implementation including hyperparameter sensitivity and the methodologys applicability limits. Additionally a comparative analysis with very recent MTL approaches could further solidify the proposed methods standing within the research community. Overall the papers contributions are significant and mark a notable step forward in making MTL systems more efficient and effective. The proposed approach not only addresses the critical issue of task gradient dominance but does so in a way that is scalable and applicable across a wide range of applications. The inclusion of theoretical proofs adds rigor to the work while the comprehensive empirical evaluation showcases its practical value.", "pLNLdHrZmcX": " Summarize the Paper This paper introduces SANE (SpecializationAware Neural Network Ensemble) an innovative endtoend ensemble learning method that focuses on actively enforcing model specialization. The core idea revolves around training base models to specialize in distinct subregions of a latent space that represents a composition of simpler distributions and then aggregating their outputs based on these specialties. The authors argue that traditional efforts to diversify base models in ensemble learning methods do not guarantee model specialization which is crucial for enhancing ensemble performance. Through extensive experiments on both tabular and image datasets SANE demonstrates superior performance over stateoftheart ensemble methods underscoring the efficacy of specializationaware ensemble learning.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovation in Ensemble Learning: The paper tackles the challenge of model specialization in ensemble learning with a novel specializationaware approach bridging a significant gap in existing literature. 2. Comprehensive Empirical Evaluation: The authors conduct extensive experiments across different types of datasets (image and tabular) showcasing SANEs superiority over existing methods and reinforcing the importance of specialization. 3. Contribution to Theoretical Understanding: By challenging the sufficiency of diversification in traditional ensemble methods and introducing the concept of model specialization this paper contributes significantly to the theoretical foundation of ensemble learning. Weak Points: 1. Generalization to Other Domains: While the paper demonstrates impressive results on the chosen datasets it does not explicitly discuss the methods applicability to other complex domains such as timeseries analysis or unsupervised learning tasks. 2. Computational Efficiency: There is a lack of detailed analysis on the computational efficiency of SANE especially in comparison with traditional ensemble methods. This information is crucial for evaluating the practicality of implementing SANE in realworld scenarios. 3. Explanation of Model Specialization Mechanisms: The mechanism through which base models achieve specialization in their respective subregions could be better elaborated. Specifically more insights into how specialty anchors guide the specialization process would be beneficial. Questions and Feedback: 1. Could the authors provide more insights into the computational costs associated with SANE compared to traditional ensemble methods 2. The paper could benefit from a deeper discussion on the limitations of SANE especially concerning scenarios where model specialization might be less effective or harder to achieve. Can the authors comment on this 3. Feedback: Incorporating case studies or examples of SANE applied to realworld problems could significantly enhance the papers impact by demonstrating its practical utility beyond theoretical superiority.  Review Summary The paper presents a significant advancement in the field of ensemble learning by proposing SANE a method that emphasizes model specialization over mere diversity. The extensive experimental results convincingly demonstrate SANEs superiority in improving ensemble learning performance across various types of data. However the manuscript could be further strengthened by addressing its computational efficiency expanding on the explanation of its specialization mechanisms and discussing its applicability and potential limitations in broader contexts. Overall this work is a valuable contribution to the literature pushing the boundaries of traditional ensemble methods and opening new avenues for research in specializationaware learning strategies.", "fwJWhOxuzV9": " Review of \"Pretrained Decision Transformer for Semisupervised Offline Reinforcement Learning\"  Summary This paper introduces a novel approach to reinforcement learning (RL) that straddles the domains of semisupervised learning and offline RL. The method dubbed Pretrained Decision Transformers (PDT) is designed to exploit large taskagnostic rewardfree datasets for pretraining followed by finetuning on smaller taskspecific datasets that include reward annotations. This approach draws inspiration from successful paradigms in natural language processing and computer vision where models are first trained on extensive unlabeled datasets before being adapted to specific tasks with much smaller labeled datasets. The authors extend the recently proposed Decision Transformers (DTs) by incorporating a pretraining stage where reward tokens are masked enabling the model to learn to autoregressively predict actions from states and action context without relying on reward signals. During the finetuning phase the reward tokens are reintroduced allowing the model to adapt its behavior towards achieving higher rewards in the specified tasks. Evaluations on the D4RL benchmark show that PDT can achieve competitive performance in settings with limited reward annotations highlighting its potential for semisupervised offline RL applications.  Strong Points 1. Innovative Approach: The paper proposes an interesting adaptation of DTs for semisupervised offline RL leveraging the strengths of pretraining on rewardfree datasets\u2014a largely untapped resource in RL. 2. Theoretical and Practical Contributions: The authors not only introduce a new architecture but also provide insights into how different components of PDT contribute to its final performance blending theoretical novelty with practical utility. 3. Experimental Validation: Comprehensive experiments demonstrate the effectiveness of the proposed method across various environments and settings supporting the claim that PDT can efficiently finetune downstream tasks with small rewardlabeled datasets.  Weak Points 1. Lack of Comparison with StateoftheArt (SOTA): While PDT is compared with several baseline models including variants of itself the paper lacks a direct comparison with other SOTA methods in semisupervised or fully unsupervised offline RL. 2. Limited Insight into Reward Masking: The mechanism of masking reward tokens during pretraining is central to PDTs design. However the paper could benefit from a deeper investigation into how this process affects learning perhaps by examining the models behavior with varying degrees of reward information availability. 3. Generalization Beyond D4RL: The experiments are confined to the D4RL benchmark. An exploration of PDTs performance on a broader range of tasks including those with more complex reward structures or realworld applications would be valuable.  Questions and Feedback 1. SOTA Comparison: Can the authors provide comparisons with other SOTA methods in semisupervised or unsupervised offline RL to contextualize PDTs performance more fully 2. Impact of Reward Masking: It would be beneficial to include additional experiments or analyses that elucidate how reward masking during pretraining influences the models learning dynamics and its ability to generalize during finetuning. 3. Applicability to Realworld Scenarios: How does PDT perform in more complex or realworld scenarios outside the D4RL benchmark Are there limitations to the types of problems PDT can address effectively  Review Summary The paper presents a significant contribution to the field of RL by proposing PDT a novel method that effectively utilizes large rewardfree datasets for pretraining followed by finetuning on smaller rewardannotated datasets. This approach addresses a critical gap in the RL literature by adopting a semisupervised learning paradigm rarely explored in this context. The experimental results convincingly demonstrate PDTs capability to efficiently leverage limited rewardlabeled data for effective finetuning positioning PDT as a promising direction for future research in semisupervised offline RL. However the paper could be strengthened by including comparisons with other SOTA methods in similar settings offering deeper insights into the impact of reward masking on learning dynamics and exploring the models applicability to a wider range of tasks especially those with complex reward structures or realworld implications. Overall the paper represents a notable advance in the development of more efficient and versatile RL methods with the potential to significantly expand the applicability of RL to domains where reward annotations are scarce or expensive to obtain.", "psh0oeMSBiF": " Review of \"Certifying Robust Policies for Offline RL Against Poisoning Attacks\"  1. Paper Summary This paper introduces the COPA framework the first attempt at certifying the robustness of offline reinforcement learning (RL) against poisoning attacks. The authors focus on the offline RL setting where training datasets may be contaminated by adversaries aiming to manipulate the learning outcomes. Two critical certification criteria are proposed: perstate action stability and cumulative reward bound. Additionally the paper introduces three policy aggregation protocols  PARL TPARL and DPARL  designed to bolster policy robustness against poisoning. Theoretical tightness and computational hardness of the certification methods for these protocols are rigorously proven. Extensive experimental validation is conducted across three RL environments (Freeway Breakout Highway) and algorithms (DQN QRDQN C51) demonstrating the efficacy of temporal aggregation protocols (TPARL DPARL) and highlighting the differences in robustness across various training algorithms and environments.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Originality and Timeliness: This work addresses a largely unexplored issue of robustness against poisoning attacks in offline RL. Given the rising deployment of RL systems in critical applications this research is highly timely. b. Comprehensive Theoretical Analysis: The paper not only proposes novel certification criteria and robust training protocols but also rigorously proves their effectiveness and computational properties contributing valuable theoretical insights to the field. c. Thorough Experimental Evaluation: The extensive experiments encompass different RL algorithms and environments providing a robust empirical foundation for the claims. The inclusion of a leaderboard further promotes transparency and encourages comparative research. Weak Points: a. Complexity of Implementation: The paper\u2019s complexity in terms of theoretical analysis and the proposed algorithm may limit its accessibility and applicability by a broader audience including practitioners. b. Limited Discussion on Practical Impacts: While the paper demonstrates the effectiveness of its methods theres limited discussion on how significant the improvement in robustness is in practical realworld applications of offline RL. c. EnvironmentSpecific Performance: The different performances of the aggregation protocols across environments suggest that the choice of protocol is highly dependent on the specific characteristics of the task which might limit the general applicability of the findings. Questions and Feedback: 1. Scalability Concerns: Given the computational complexity mentioned how do the proposed certification and training protocols scale with larger more complex environments 2. RealWorld Applicability: Could the authors elaborate on potential realworld applications where this robust offline RL framework would be critically beneficial 3. Adversarial Model Assumptions: The paper assumes certain capabilities and goals of the adversary. How might varying these assumptions affect the efficacy of the proposed methods  3. Review Summary This paper is a valuable contribution to the field of reinforcement learning specifically targeting the understudied area of offline RLs robustness against poisoning attacks. The novel certification criteria policy aggregation protocols and rigorous theoretical analysis are noteworthy strengths distinguishing this work. The empirical validation across various environments and algorithms further underscores the frameworks potential to enhance the reliability of offline RL applications. However the practical implications scalability issues and environmentspecific performances of the proposed methods warrant further discussion. Addressing these aspects could significantly enhance the comprehensiveness and applicability of this research. Given the strengths of the theoretical contributions and experimental results presented I recommend this paper for publication suggesting that the final version incorporates discussions addressing the aforementioned weak points and questions.", "rzvOQrnclO0": " Summarize the paper The paper presents a novel approach to enhancing modelbased reinforcement learning (RL) by introducing a twomodelbased learning method specifically designed to optimize policy with improved accuracy and sample efficiency. The core innovation is the explicit consideration of both prediction error and gradient error during the model learning phase addressing a critical gap in traditional modellearning approaches which only focus on reducing prediction error. The authors carry out a theoretical analysis to demonstrate the impact of model gradient error on the convergence rate of policy optimization methods. Based on these insights they develop the Directional Derivative Projection Policy Optimization (DDPPO) algorithm which involves estimating the environment models gradient using directional derivatives and refining policy optimization with two separate models: one for predicting the next state and reward (prediction model) and another for calculating the policy gradient (gradient model). Extensive experiments on continuous control tasks in the MuJoCo environment validate the theoretical findings and show the superior performance of DDPPO in terms of sample efficiency and overall effectiveness compared to stateoftheart modelbased and modelfree methods.  Strong and weak points of the paper Strong Points: 1. Theoretical Contribution: The paper offers a significant theoretical analysis on the impact of gradient error in modelbased RL effectively filling a gap in existing literature which predominantly focuses on prediction error. This offers a new perspective on improving policy optimization algorithms. 2. Innovative Approach: Introducing a twomodelbased system that distinguishes between prediction error and gradient error is innovative. It utilizes models in a more specialized manner which is shown to enhance policy optimization efficiency. 3. Practical Implementation: The DDPPO algorithm provides a practical implementation of the theoretical concepts backed by an opensource code repository. This fosters reproducibility and further exploration of the proposed method. 4. Empirical Validation: The comprehensive experimental evaluation on MuJoCo continuous control tasks demonstrates the effectiveness of the proposed method both in terms of sample efficiency and performance metrics providing strong empirical support for the theoretical claims. Weak Points: 1. Complexity and Computation Overhead: The twomodel approach inherently increases the systems complexity and may lead to higher computational overhead compared to traditional singlemodel methods. This aspect is not thoroughly discussed or analyzed in the paper. 2. Limited Context: While the experimental results are promising they are limited to the MuJoCo continuous control tasks. The applicability and effectiveness of DDPPO in other contexts or types of RL problems remain unclear. 3. Hyperparameter Sensitivity: The paper touches upon the sensitivity of the algorithm to hyperparameters n and w but does not provide an extensive analysis of hyperparameter tuning or its impact across different environments or tasks. Questions and Feedback to the author: 1. Could the authors elaborate on the computational complexity and potential overhead introduced by managing two models instead of one and how it impacts the overall efficiency of the algorithm 2. Are there specific types or categories of RL problems where the proposed twomodel approach may not be suitable or effective Any insights or theoretical boundaries on its applicability would be valuable. 3. An extended discussion on hyperparameter sensitivity and tuning strategies would enhance the robustness and practical applicability of the DDPPO algorithm. Can the authors provide guidelines or recommendations for hyperparameter selection across diverse environments  Review Summary The paper introduces a significant advancement in modelbased reinforcement learning by proposing a dualmodel approach that separately optimizes for prediction error and gradient error accompanied by a practical implementation through the DDPPO algorithm. The theoretical analysis provided is both novel and crucial shedding light on the importance of considering gradient error in policy optimization. Empirical results demonstrate the effectiveness of the approach in achieving better sample efficiency and performance compared to existing methods. However the increased complexity and computational requirements of the dualmodel approach the limited exploration of its applicability beyond MuJoCo tasks and the somewhat cursory treatment of hyperparameter sensitivity are areas that could benefit from further exploration and clarification. Addressing these concerns and extending the scope of experimental validation could solidify the contribution and applicability of the presented work. Overall the paper makes a notable contribution to the field of modelbased RL with its strengths in theoretical insight and innovative algorithm design outweighing its current limitations. Future work as suggested by the authors particularly in optimizing gradient model efficiency and exploring applicability to broader problems is eagerly anticipated.", "qEGBB9YB31": " Peer Review: On Parameter Saliency in Neural Networks for Error Correction  1. Summary The paper introduces a novel approach to neural network explainability focusing on identifying salient network parameters responsible for misclassification rather than conventional input featurebased saliency maps. This approach involves defining a parameterwise saliency profile based on the gradient information of the loss function which highlights the network parameters most responsible for classification errors. The authors verify the identified salient parameters impact by showcasing improved predictions upon disabling these parameters compared to random or least salient parameters. Additionally finetuning a small subset of these salient parameters on misclassified samples is demonstrated to correct errors in other similarly misclassified samples underscoring the methods efficacy. The paper also presents how semantically similar samples affect similar network parameters unraveling the semantic connection in parameter saliency space. An inputspace saliency counterpart is introduced to visualize how image features trigger specific network components malfunctions providing a comprehensive understanding of model mistakes.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Perspective: The shift from input feature saliency to parameter saliency is novel and provides a fresh angle to model explainability.  Empirical Validation: The method is supported by thorough empirical evidence including error correction through parameter pruning and finetuning and the semantic analysis of salient parameters.  Semantically Meaningful Analysis: Demonstrating that samples causing similar parameters to malfunction are semantically similar offers insightful implications for understanding model errors and relations within the model parameter space. Weak Points:  Generalization of Findings: The experiments are limited to image datasets and Convolutional Neural Networks (CNNs). The generalizability of the method to different data types and model architectures remains unclear.  Dependency on Misclassifications: The methodology primarily focuses on misclassified samples which might limit its applicability in scenarios with high accuracy or where misclassifications are not readily identifiable.  Computational Efficiency: While the paper claims the computation of saliency profiles is fast a detailed analysis of the computational cost especially on large models and datasets is missing. Questions:  How does the method compare to existing explainability techniques like LIME or SHAP in terms of interpretability and computational efficiency  Is there a threshold for determining which parameters are considered sufficiently salient If so how is this threshold defined  Could this approach be adapted for use in models beyond CNNs particularly in nonvisual tasks such as natural language processing or tabular data Feedback:  Consider extending the methodology to cover more diverse model architectures and data types to demonstrate broader applicability.  A deeper analysis of the computational efficiency across different model sizes and a comparison to other explainability methods could strengthen the paper.  Exploring applications of this method in correcting biases and understanding model fairness could provide valuable insights into broader implications.  3. Review Summary The paper presents a compelling and novel methodology for model explainability focused on network parameter saliency diverging from traditional inputfocused methods. The comprehensive experimental validation demonstrates the approachs effectiveness in identifying and correcting misclassifications. However the generalization of findings and the computational cost analysis are areas where the paper could be strengthened. Additionally exploring broader applications and extending the methodology to different models and data types would enhance its impact. Moreover addressing the methodologys potential limitations including its reliance on misclassifications and how it compares with other explainability methods in terms of efficiency and interpretability would provide a more rounded view. Overall the paper delivers a valuable contribution to the field of explainability in neural networks offering an innovative tool for diagnosing and correcting model errors.", "krI-ahhgN2": " Summarize the Paper This paper introduces a novel supervised contrastive learning framework termed SelfContrastive (SelfCon) Learning which contrasts multiple outputs from different levels within a multiexit architecture network. Unlike typical contrastive learning methods that rely on augmented data samples to form positive and negative pairs SelfCon avoids this by utilizing multiple outputs for the same input thus alleviating issues such as high computational cost and generalization errors. The paper theoretically establishes that SelfCon learning ensures a lower bound of labelconditional mutual information between intermediate and final features. Empirically across several benchmarks like ImageNet100 results exhibit superior classification performance over traditional crossentropy and Supervised Contrastive (SupCon) learning techniques without the need for augmented multiview batches. The success of SelfCon is attributed to its regularization effect stemming from its unique use of singleview and subnetwork outputs.  Strong Points 1. Novelty: The SelfCon learning framework introduces an innovative approach to contrastive learning by minimizing the need for data augmentation which is a significant step forward in reducing computational requirements and addressing generalization issues associated with augmented multiview batches. 2. Theoretical Foundation: The paper provides a strong theoretical foundation for SelfCon learning demonstrating how it guarantees a lower bound on labelconditional mutual information between layers within the network. This theoretical backing is crucial for understanding why the approach works. 3. Empirical Validation: The paper presents comprehensive experiments across various datasets showing that SelfCon outperforms existing methods in terms of classification accuracy. These results are compelling evidence of the effectiveness of the proposed method.  Weak Points 1. Lack of Comparison with Other Regularization Techniques: While the paper effectively compares SelfCon with crossentropy and SupCon learning it does not compare its effectiveness against other common regularization techniques or contrastive learning variations not reliant on data augmentation. 2. Complexity and Implementation Details: The paper could benefit from a more detailed discussion on the complexity involved in implementing SelfCon including the specifics of choosing the architecture for subnetworks and how this might affect generalization across different tasks or domains. 3. Analysis of Failure Modes: The paper does not discuss potential limitations or failure modes of SelfCon learning. Understanding under what conditions SelfCon might not perform as expected would be valuable for future research and application.  Questions and Feedback 1. Comparison with Other Regularization Methods: How does SelfCon learning compare to other regularization methods such as dropout or batch normalization especially in scenarios where data augmentation is already limited or not feasible 2. Implementation Complexity: Can the authors provide more details on the complexity associated with implementing the SelfCon learning framework specifically regarding the selection and optimization of subnetwork architectures 3. Failures and Limitations: Could the authors discuss any observed limitations or failure cases in applying SelfCon learning Such insights could be valuable for readers to understand the boundaries of the proposed methods applicability.  Review Summary The submission introduces an innovative supervised contrastive learning method SelfCon which leverages the outputs of multiple levels within a multiexit network architecture to enhance classification performance without relying on data augmentation. The theoretical underpinnings provided by the authors along with robust empirical validation across various datasets present a strong case for the effectiveness and novelty of the proposed approach. However the paper could be strengthened by addressing its weak points particularly by expanding the comparison to include other regularization methods providing more details on implementation complexity and discussing potential limitations or failure scenarios of SelfCon learning. Overall the contributions of this paper are significant paving the way for further exploration in efficient and effective contrastive learning methodologies.", "oj2yn1Q4Ett": "Summarize the Paper This paper presents a novel framework for communicationefficient distributed learning in overparameterized models focusing on a class of kernel learning problems including Neural Tangent Kernel (NTK). The proposed framework departs from classical consensusbased approaches by introducing a multiagent kernel approximation technique that allows agents to estimate the full kernel function without sharing local data or parameters. This approach significantly reduces communication complexity while achieving minimax optimal generalization performance. The framework is demonstrated to be more communicationefficient compared to stateoftheart algorithms across UCI benchmark datasets. Strong Points: 1. Innovative Approach: The paper breaks new ground by addressing the communication overhead in distributed learning of overparameterized models through a novel kernel approximation technique. This stands as a significant contribution towards scalable decentralized learning. 2. Comprehensive Theoretical Analysis: A detailed analysis of the optimization and generalization performance of the proposed framework provides a solid theoretical foundation for the approach. The performance bounds and minimax optimality claims are wellsupported through rigorous proofs. 3. Empirical Validation: The effectiveness of the proposed algorithms is convincingly demonstrated on UCI benchmark datasets. The comparisons with stateoftheart algorithms highlight the practical benefits of the proposed framework in terms of communication efficiency. Weak Points: 1. Applicability to NonUCI RealWorld Scenarios: While the results on UCI benchmark datasets are promising the paper lacks an exploration of the frameworks effectiveness in more complex realworld scenarios that might present challenges not captured by the benchmark datasets. 2. Analysis of Computational Complexity: The paper thoroughly analyzes communication complexity but does not provide a detailed examination of computational costs. Given the overparameterized nature of the models understanding computational overheads especially in resourceconstrained environments would add valuable insight. 3. Consideration of Model Heterogeneity: The paper assumes homogeneous models across agents. An extension to heterogeneous models where agents might have different model architectures or capabilities could enhance the practical applicability of the proposed framework. Questions and Feedback to the Author: 1. How does the proposed framework perform under model heterogeneity among agents This is crucial for wider applicability in realworld distributed systems where such heterogeneity is common. 2. Could the authors comment on the computational complexity of the proposed algorithm especially in comparison to traditional consensusbased and parametersharing approaches 3. The paper demonstrates the frameworks performance primarily on UCI benchmark datasets. Are there plans to validate the approach in more complex realworld distributed learning settings 4. The frameworks privacy considerations particularly in terms of indirect data sharing through kernel approximations warrant further discussion. How does the framework ensure data privacy and are there potential vulnerabilities Review Summary: The paper presents a significant advancement in the field of distributed learning for overparameterized models introducing a novel framework that leverages a multiagent kernel approximation technique for communicationefficient learning. The departure from consensusbased approaches to avoid direct exchange of local data or parameters is both innovative and practically relevant. The theoretical analysis underpinning the frameworks performance is sound and wellpresented offering clear insights into its optimization and generalization capabilities. Empirical validations further underscore the utility and efficiency of the proposed algorithms in comparison to existing methods. However the paper could be strengthened by addressing its applicability to more complex realworld distributed learning scenarios beyond UCI benchmark datasets. Additionally a detailed analysis of computational complexity and considerations for model heterogeneity could enhance the understanding and applicability of the proposed framework. Also more discussion on privacy concerns related to the indirect sharing of data through kernel approximations would be beneficial. Overall the paper makes a valuable contribution to the field of decentralized learning providing a foundation for further research and potential realworld applications. The proposed frameworks ability to reduce communication complexity while maintaining high learning performance holds promise for advancing distributed learning technologies.", "oVE1z8NlNe": " Review of \"Exploring Federated SelfSupervised Learning\"  Summary This paper contributes to the rapidly evolving areas of selfsupervised learning (SSL) and federated learning (FL) by investigating their intersection specifically how SSL can be adapted to the federated setting to handle decentralized unlabeled data which is often nonindependently and identically distributed (nonIID). The authors introduce a generalized federated selfsupervised learning framework (FedSSL) compatible with existing SSL methods that use Siamese networks with a focus on crosssilo FL scenarios. Their empirical studies within this framework reveal intriguing insights about the applicability and optimization of SSL methods in federated contexts especially regarding stopgradient operations the importance of retaining local knowledge and the utility of target networks. Based on these insights the authors propose a novel approach Federated Divergenceaware Exponential Moving Average update (FedEMA) designed to dynamically adapt model updates based on the divergence between local and global models thus addressing the challenges brought by nonIID data. Their experimental evaluation demonstrates that FedEMA provides significant performance improvements over existing methods.  Strong Points 1. Novel Framework and Approach: The introduction of a generalized FedSSL framework and the novel FedEMA approach are both significant contributions. FedEMAs dynamic adaptation based on model divergence is particularly clever and addresses a critical challenge in federated learning with nonIID data. 2. Empirical Insights: The detailed empirical study is a strength providing valuable insights into the functioning of SSL in federated environments such as the surprising nonnecessity of stopgradient operations in FedSSL and the importance of retaining local data knowledge. 3. Extensive Experiments: The paper benefits from an extensive set of experiments that clearly demonstrate the superiority of their proposed method over standard approaches thus validating the practical significance of their contributions.  Weak Points 1. Generalization of Results: While the paper presents impressive results on CIFAR datasets it lacks exploration of larger more complex datasets or realworld scenarios that could further validate the scalability and generalization capabilities of FedEMA. 2. Detail on Implementation and Reproducibility: The paper could benefit from more detailed descriptions regarding the implementation specifics and hyperparameter settings for the sake of reproducibility. Providing code or more detailed pseudocode would significantly enhance the papers utility to the research community. 3. Comparison with Related Work: The paper could strengthen its argument by including a more nuanced comparison with closely related work particularly those approaches tackling nonIID data challenges in federated settings to highlight FedEMAs unique contributions more clearly.  Questions and Feedback to the Author 1. Scalability and Generalization: Could the authors comment on the expected performance of FedEMA on largerscale datasets or in realworld federated learning scenarios where data distribution complexity and privacy concerns are significantly higher 2. Hyperparameter Sensitivity: How sensitive is FedEMAs performance to the choice of its decay rate and the autoscalers \u03c4 parameter Some insight into the process for selecting these hyperparameters would be valuable. 3. Comparison with State of the Art: While the manuscript does include comparisons to existing methods providing deeper insights into why FedEMA outperforms these methods possibly by visualizing feature representations or analyzing model divergence in more detail could enrich the discussion.  Review Summary The manuscript makes significant contributions to the field of federated learning and selfsupervised learning by introducing a new framework and a novel approach (FedEMA) that outperforms existing methods in handling nonIID data challenges. The empirical insights provided are valuable for understanding the mechanics of selfsupervised learning in decentralized environments. However the paper could be further strengthened by expanding its experiments to more complex datasets offering more detailed implementation specifics for reproducibility and enhancing the discussion on its comparative analysis with related work. Overall the paper is wellwritten the proposed method is sound and the findings are significant warranting publication after addressing the provided feedback and questions.", "ygGMP1zkiD1": " Paper Summary This paper presents a novel debiasing method aimed at reducing social stereotypes in sentencelevel text encoders without significantly harming their semantic capabilities. The authors identify that most social biases in text encoders especially those based on the transformer architecture like BERT GPT3 and T5 are encoded within the attention mechanism. Their proposed method AttD redistributes the attention scores across different social groups evenly thus mitigating the preference towards historically advantaged groups. This process is complemented by knowledge distillation from an unaltered \"teacher\" text encoder to minimize semantic loss. The authors address biases concerning gender race religion age and sexual orientation and demonstrate the effectiveness of their method through likelihood and inferencebased evaluations. The experimental results suggest that AttD outperforms existing debiasing methods reducing biases while preserving and in some instances slightly enhancing the semantic performance on downstream tasks.  Strong Points 1. Innovative Approach: The paper takes a unique stance by focusing on the attention mechanisms of text encoders which has been largely overlooked in previous debiasing studies. This deeper dive into the models operational framework allows for a more nuanced understanding and addressing of biases. 2. Comprehensive Bias Coverage: The inclusion of multiple dimensions of social bias (gender race religion age sexual orientation) represents a comprehensive approach to fairness in NLP surpassing many efforts that concentrate solely on gender bias. 3. Thorough Evaluation: The evaluation methodology is robust utilizing both likelihood and inferencebased measures across multiple dimensions of bias. The comparative analysis against existing methods provides clear evidence of the proposed methods efficacy. 4. Semantic Integrity: The methods emphasis on minimizing semantic loss through knowledge distillation is commendable. This dual focus ensures that the debiasing process does not detract from the models performance on NLP tasks as evidenced by the GLUE benchmark results.  Weak Points 1. Definition Simplifications and Potential Omissions: The authors acknowledge the simplification in the definitions of biases and the limitation in addressing all potential social divisions. This simplification might overlook nuanced and intersectional aspects of biases. 2. Potential for Generalization: While the study convincingly demonstrates the methods effectiveness on specific models (e.g. BERT) its less clear how it generalizes across the diverse ecosystem of text encoders particularly newer or less conventional architectures. 3. Negative Sampling Methodology: The paper introduces negative sampling to overcome possible spurious associations but does not delve into detail about the selection process for negative samples. This detail is crucial for reproducing and understanding the potential impact of this choice on the outcomes.  Questions and Feedback to the Author 1. Could the authors elaborate on the criteria used for negative sampling especially how negative examples are chosen and their proportion compared to biasrelated inputs 2. Considering the simplification of bias definitions how do the authors foresee their method adapting to more complex and intersectional forms of bias that may not be as easily classified into discrete groups 3. The paper briefly mentions the potential application of AttD across various text encoder architectures but does not explore this in depth. Could future work include a broader analysis of the methods applicability and effectiveness across a wider range of NLP models  Review Summary The paper introduces a promising debiasing method that innovatively targets the attention mechanism in text encoders setting it apart from existing approaches that primarily focus on embeddings. The methods ability to significantly reduce biases across various social dimensions while preserving semantic performance is welldocumented through rigorous evaluation. However the approach does bring up questions regarding the generalizability across different models the potential oversimplification of bias categories and the specifics of the negative sampling strategy. Despite these considerations the paper contributes valuable insights and a novel tool for the ongoing effort to mitigate social biases in NLP pushing the field towards more equitable and fair representations in language models. The comprehensive approach to addressing multiple forms of bias coupled with meticulous experimentation and an eye towards semantic preservation underscores the papers importance and potential impact on fair representation learning.", "nO5caZwFwYu": " Review of \"Efficient Active Search Strategies for Machine Learningbased Combinatorial Optimization\"  Summary This paper presents an advancement in machine learning (ML) based methods for solving combinatorial optimization problems by focusing on efficient active search (EAS) strategies. These strategies aim to update only a subset of model parameters during the search process unlike the conventional active search that updates all model parameters for each test instance which is both timeconsuming and memoryintensive. The authors propose three implementations of EAS: updating instancespecific embeddings (EASEmb) adding instancespecific residual layers to the model (EASLay) and using a lookup table to modify the policy of the model (EASTab). These strategies are compared against existing ML approaches and traditional heuristic solvers using the traveling salesperson problem (TSP) the capacitated vehicle routing problem (CVRP) and the job shop scheduling problem (JSSP). The results show significant improvements in search performance and efficiency with some EAS implementations even surpassing the wellknown heuristic solver LKH3 on certain problems. The paper posits EAS as a means to enhance the adaptability and performance of MLbased construction methods for combinatorial optimization problems ultimately leading to solutions for larger problem instances that were not possible before.  Strong Points 1. Novelty of Approach: The introduction of EAS as a method to update only a subset of parameters is innovative and effectively addresses the main limitations of the original active search approach. This offers a more efficient and scalable solution to combinatorial optimization problems. 2. Comprehensive Evaluation: The paper presents a thorough comparison of EAS strategies against both traditional operations research solvers and stateoftheart MLbased methods across different problem types (TSP CVRP JSSP). This is backed by extensive experiments that demonstrate the efficacy and efficiency of EAS. 3. Generalization Capabilities: The authors investigation into the generalization performance of EAS where models trained on smaller instances are tested on significantly larger ones is particularly impressive. This aspect is crucial for practical applications where problem sizes can vary greatly.  Weak Points 1. Limited Discussion on the Selection Process for Parameters to Update: Although the paper presents three strategies for selecting parameters to update there is little discussion on the theoretical or empirical justification behind the choice of these specific parameters over others. A deeper analysis of why certain parameters are chosen for updates and others are not could strengthen the research. 2. Scalability Concerns: While the results are promising the scalability of EAS strategies when applied to very large and complex realworld problems remains somewhat ambiguous. Details on memory usage computational complexity and limitations in extremely large instances would provide more clarity on the practical applicability of EAS. 3. Lack of Detail on Implementation Challenges: The paper could benefit from a more detailed discussion on the practical challenges of implementing EAS such as tuning the hyperparameters (e.g. \u03bb \u03c3 \u03b1) and the implications of different values on search performance and efficiency.  Questions and Feedback to the Author 1. Parameter Selection Justification: Can the authors provide a more detailed theoretical or empirical rationale for the specific subset of parameters chosen for updates in each EAS strategy 2. Scalability and Applicability: How do the authors envision the scalability of EAS strategies to more complex and larger realworld problems specifically regarding computational resources and time 3. Hyperparameter Tuning: Could the authors elaborate on the process and challenges of tuning the hyperparameters in EAS strategies Additionally how sensitive are the models to these hyperparameters and what impact do they have on the generalization ability of the models  Review Summary The paper introduces an innovative approach to tackling combinatorial optimization problems using machine learning by proposing efficient active search strategies. These strategies significantly improve upon existing methods by updating only a subset of parameters leading to considerable gains in efficiency and performance. The comprehensive experiments across multiple problem types and comparisons with both traditional and MLbased solvers demonstrate the potential of EAS in enhancing the capabilities of MLbased construction methods. However the research could be further strengthened by addressing several areas such as the justification for selecting specific parameters for updates discussing scalability in the context of extremely large and complex problems and providing more insight into the practical challenges of implementing EAS including hyperparameter tuning. Overall this paper makes a significant contribution to the field of MLbased combinatorial optimization offering a promising direction for future research and practical applications. The innovative nature of EAS strategies coupled with their demonstrated efficacy lays a strong foundation for advancing the stateoftheart in this area.", "nL2lDlsrZU": " Summarize the paper This paper introduces SAINT (SelfAttention and INtersample attention Transformer) a novel deep learning model for handling tabular data. The model capitalizes on the transformer architecture utilizing both selfattention to capture relationships within individual data points and a newly proposed intersample attention to relate different data points to each other. It also applies enhanced embedding methods for categorical and continuous variables and employs contrastive pretraining to improve performance in scenarios with limited labeled data. Comprehensive evaluations on 30 benchmark datasets show that SAINT competitively outperforms both existing deep learning models and traditional gradientboosting methods (XGBoost CatBoost LightGBM) across a range of tasks including regression binary classification and multiclass classification.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Architecture: The dual attention mechanism (selfattention and intersample attention) is a novel approach in the tabular domain. It effectively captures both intrarow (featurelevel) and interrow (samplelevel) dependencies potentially offering a richer representation of the data. 2. Enhanced Embedding Method: The separate handling and embedding of continuous variables before passing them through the transformer architecture is a significant improvement over predecessors that either neglected continuous variables or failed to effectively integrate them. 3. Comprehensive Evaluation: The paper evaluates SAINT against a wide array of baseline models (including industry standards) across 30 diverse datasets ensuring the results are robust and the models competitiveness is reliably established. Weak Points: 1. Complexity and Training Costs: The introduction of intersample attention might significantly increase model complexity and computational resource requirements although this is not thoroughly discussed. Questions remain regarding the scalability to extremely large datasets and the efficiency of training. 2. Lack of Theoretical Justification: While the empirical results are impressive the paper lacks a theoretical discussion on why the proposed methods (contrastive pretraining and intersample attention) work better for tabular data limiting the understanding of the underlying principles. 3. Interpretability Concerns: Despite providing attention maps for interpretation the models decisions are not transparent or intuitive especially compared to treebased methods. This \"black box\" nature could be a barrier to adoption in fields requiring high explainability. Questions and Feedback:  Could the authors comment on the computational efficiency of SAINT particularly in comparison to gradient boosting methods and other deep learning approaches for tabular data  The paper could benefit from a deeper exploration into the failure modes of SAINT. Are there specific types of tabular datasets or feature distributions where it underperforms  It would be interesting to see further investigation into the models robustness to noisy or missing data particularly in comparison to methods like XGBoost which are known for handling such issues well.  Feedback for improvement could include adding a more detailed theoretical framework explaining why the proposed innovations work and exploring more transparent decisionmaking processes within the SAINT model to address interpretability concerns.  Review Summary The SAINT model presents a significant advance in the application of deep learning techniques to tabular data. Its introduction of intersample attention and contrastive pretraining alongside an enhanced embedding strategy for continuous variables sets it apart from existing methods showing superior performance across a wide range of tasks and datasets. The robust empirical evaluation process convincingly demonstrates its effectiveness. However there are areas for improvement including addressing the models complexity and training efficiency expanding on the theoretical underpinnings of the proposed techniques and enhancing interpretability to make the model more accessible for users requiring explainable AI solutions. Addressing these issues could help consolidate SAINTs position as a competitive and practical tool for tackling tabular data problems in various domains.", "qiMXBIf4NfB": " 1) Summary of the Paper The paper presents a theoretical analysis of iterative selftraining in onehiddenlayer neural networks marking a first in the study of nonlinear neural networks through selftraining. Selftraining a semisupervised learning algorithm leverages unlabeled data to improve learning outcomes when labeled data are scarce. Despite its empirical success across different applications a theoretical understanding has been lacking. This paper addresses the gap by proving that iterative selftraining benefits both convergence rate and generalization accuracy as a function of unlabeled data volume (\\xe2\\x88\\x9aM). Key contributions include a quantitative validation of generalization improvement utilizing unlabeled data analytical proof of iterative selftrainings superiority over singleshot alternatives and a sample complexity analysis showing reduced requirement for labeled data. Experiments conducted on synthetic and realworld datasets (CIFAR10) supplement the theoretical claims showcasing improved generalization performance and convergence rate with increased unlabeled data.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Contribution: The paper fills a significant gap in the theoretical understanding of iterative selftraining by providing quantitative analyses and proofs which was previously unexplored.  Comprehensive Analysis: The exploration of generalization performance convergence rate and sample complexity in the context of both labeled and unlabeled data is thorough and wellrounded.  Empirical Validation: The inclusion of empirical results not only on synthetic but also on realworld data strongly supports the theoretical findings and broadens the papers appeal. Weak Points:  Scope Limitation: The study restricts its theoretical analysis to onehiddenlayer neural networks. While the authors argue this focus is due to the complexity of the analysis it does limit the applicability of the findings to more complex realworld neural network architectures.  Assumptions and Generalization: The Gaussian distribution assumption for input data might not always hold in realworld scenarios potentially impacting the generalizability of the results.  Technical Density: The paper is dense with theoretical analysis and proofs which may limit its accessibility to readers not wellversed in advanced mathematics and theoretical computer science. Questions and Feedback:  Extension to Deeper Networks: Can the authors comment on how they anticipate their findings might extend to deeper neural networks Are there specific challenges they foresee  Impact of Data Distribution Assumptions: How critical is the Gaussian distribution assumption for the inputs Would other distributions significantly alter the outcomes  Robustness to Label Noise: Semisupervised learning often grapples with the issue of label noise in pseudo labels. Does the iterative selftraining approach account for or mitigate the impacts of such noise  3) Review Summary The paper represents a significant advancement in understanding iterative selftraining from a theoretical perspective marking the first formal analysis in the context of nonlinear neural networks. Its strengths lie in its novel contributions comprehensive analysis spanning several critical aspects of selftraining and empirical validations that strengthen its theoretical propositions. However the papers limited scope to onehiddenlayer networks assumptions on data distribution and its technical nature might restrict its accessibility and applicability. Nevertheless these shortcomings do not overshadow the papers contributions but rather highlight avenues for future work such as extending the analysis to more complex networks and exploring the effects of different data distributions. Given the strong theoretical foundation laid out coupled with empirical evidence supporting the theoretical claims the paper is a valuable contribution to the field of semisupervised learning. It not only advances our theoretical understanding of iterative selftraining but also opens up new directions for research in extending these findings to more complex models and realworld applications.", "xs-tJn58XKv": "Review of \"Leveraging Related Tasks to Identify and Transfer Unstable Features for Robust Machine Learning Models\" 1. Summary: The paper presents a novel approach Transfer OF Unstable features (TOFU) which aims at improving model robustness by identifying and transferring knowledge about unstable (spurious) features from a related source task to a target task. The uniqueness of this method lies in its ability to derive a representation that encodes unstable features using source task environments and then applying group distributionally robust optimization (DRO) across clusters of data in the target task that are informed by this representation. The approach is evaluated across synthetically generated and realworld environments on text and image classification tasks demonstrating a substantial improvement in model robustness by outperforming existing baselines and achieving nearoracle performance in many scenarios. 2. Strong and Weak Points Questions and Feedback: Strong Points: a) Innovativeness: The TOFU approach addresses the challenging problem of reducing bias in situations where only inputlabel pairs are available for the target task leveraging related tasks to identify and transfer knowledge about unstable features. This novel methodology extends the capabilities of automatic debiasing without requiring additional annotations. b) Comprehensive Evaluation: The paper rigorously evaluates the approach on both synthetic and realworld datasets involving various tasks and compares it against a substantial number of baseline methodologies. c) Quality of Theoretical Insights: The formulation of the problem and the development of a representation to encode unstable features are well rooted in theoretical insights providing a clear rationale for why the approach works. The authors effectively use a theorem (Theorem 1) to logically connect the classifiers predictions with the nature of unstable features facilitating a methodological innovation in tackling bias. Weak Points: a) Complexity and Implementation: The approach while innovative seems to be complex involving multiple stages and dependencies on accurate clustering and identification of unstable features. The practicality of implementing and tuning such a model for different realworld scenarios could be challenging. b) Generalizability across All Types of Biases: While the approach is effective in the settings tested further exploration is needed to confirm its effectiveness across a wider array of tasks and biases especially in scenarios where related tasks might not share similar biases or when biases are complex and multifaceted. c) Dependency on Related Source Tasks: The methodologys success hinges significantly on the availability of related source tasks with identifiable and transferable biases. The criteria for selecting or determining such tasks remain somewhat vague and could benefit from a more detailed exploration. Questions and Feedback: 1. Clustering Sensitivity: How sensitive is the performance of TOFU to the effectiveness of the clustering process Could variations in clustering algorithms significantly impact the robustness achieved 2. Adaptability to Changing Biases: In realworld scenarios the nature of biases can evolve. How adaptable is TOFU to such changes Would it require reidentification of biases from source tasks periodically 3. Tips for Source Task Selection: Could the authors provide more guidelines or criteria for selecting related source tasks Understanding the characteristics that make certain source tasks more informative for a target task could be invaluable for practitioners. 3. Review Summary: The paper introduces an innovative approach to improve machine learning model robustness by transferring knowledge about unstable features from related source tasks. The approachs novelty and potential impact on various applications especially where bias minimization is critical are commendable. The thorough evaluation and theoretical grounding of the proposed method underscore its effectiveness and potential utility. However the complexity of implementation dependence on the relatedness of source tasks and the generalizability of the findings across diverse scenarios and biases represent areas for further exploration and clarification. Given the strength of the results the ingenuity of the approach and its contribution to the field I recommend acceptance but suggest that the authors address the raised questions and consider elaborating on the methodologys practical implementation aspects potential limitations and guidelines for selecting related source tasks.", "pQ02Y-onvZA": "Review of \"\u03b42exploration: Sample Average Uncertaintybased Exploration for Sequential Reinforcement Learning\" 1. Summary: This paper introduces \u03b42exploration a novel exploration strategy designed to address the explorationexploitation dilemma in sequential reinforcement learning (RL) scenarios extending the concept of Sample Average Uncertainty (SAU) from bandit problems to more complex RL tasks. The authors effectively leverage SAUs simplicity and computational efficiency demonstrating \u03b42explorations adaptability to both tabular Qlearning and Deep Q Networks (DQN) showcasing substantial empirical performance improvements over the traditional \u03b5greedy action selection approach. The paper is structured around introducing the explorationexploitation dilemma discussing limitations of existing strategies and proposing \u03b42exploration as a scalable alternative. Empirical evaluations confirm its practical advantage and adaptability across various settings particularly highlighting its performance in the Atari 2600 domain. 2. Strong and Weak Points Questions and Feedback: Strong Points: a. Novelty and Practicality: The paper successfully extends the SAU measure to the sequential RL setting offering a pragmatic exploration strategy (\u03b42exploration) that is both simple to implement and computationally light making it accessible for a wide range of RL applications. b. Empirical Evaluation: The comprehensive experimental analysis showcases \u03b42explorations superior performance over conventional exploration strategies like \u03b5greedy action selection both in tabular settings and in complex environments represented by DQN in the Atari 2600 domain. c. Clear Exposition: The paper is wellorganized and articulate making a complex topic accessible. The stepbystep walkthrough of algorithmic implementations and the direct comparison between \u03b42exploration and traditional methods provide clear insights into the proposed methods benefits. Weak Points: a. Lack of Theoretical Analysis: While the empirical results are strong the paper lacks a theoretical exploration of \u03b42explorations properties such as convergence guarantees or an analysis of how \u03b42exploration balances exploration and exploitation theoretically. b. Comparison with StateoftheArt: The paper primarily compares \u03b42exploration against \u03b5greedy action selection. Including a broader comparison with other stateoftheart exploration strategies could further contextualize \u03b42explorations performance and elucidate its position within the current research landscape. c. Generalization beyond Presented Domains: While impressive results are presented within the Qlearning and DQN frameworks discussions on \u03b42exploration\u2019s adaptability to other RL algorithms or more complex scenarios are scarce. Feedback and Questions: 1. Could the authors elaborate on any theoretical underpinnings that might support the empirical success of \u03b42exploration Any formal analysis or proofs regarding its convergence or efficiency would be valuable. 2. It would be beneficial to see a comparison of \u03b42exploration not just with \u03b5greedy strategies but also with other modern exploration strategies that have shown promise in recent studies. 3. Are there limitations to the scalability or applicability of \u03b42exploration to other RL methods beyond Qlearning and DQN such as policy gradient methods or actorcritic models 3. Review Summary: The paper presents a significant contribution to the field of reinforcement learning by introducing \u03b42exploration an exploration strategy extending SAU to sequential RL problems. Its simplicity computational efficiency and impressive empirical results strongly argue for its potential as a viable alternative to conventional strategies like \u03b5greedy action selection. Despite these strengths the paper would benefit from a deeper theoretical exploration of \u03b42explorations mechanisms broader comparison with other exploration strategies and discussion on its generalizability to different RL paradigms. Overall the paper makes a compelling case for \u03b42exploration suggesting it as both a competitive baseline and a potential cornerstone for future advances in efficient exploration strategies in reinforcement learning.", "sNuFKTMktcY": " Review Summary The paper presents a novel method in the domain of goalconditioned hierarchical reinforcement learning (GCHRL) aiming to address the challenges associated with longhorizon tasks characterized by sparse external rewards. The proposed approach named Hierarchical Exploration with Stable Subgoal (HESS) introduces a regularization technique for stabilizing subgoal representation learning and develops a dual measure (novelty and potential) based active exploration strategy to efficiently navigate sparsereward environments without resorting to intrinsic rewards.  Summarize the Paper The paper identifies the critical issue of nonstationarity in the highlevel exploration caused by the dynamic subgoal space and proposes a regularization method to enhance the stability and efficiency of subgoal representation learning. Leveraging the stabilized subgoal representation the paper then introduces measures for assessing the novelty and potential of subgoals facilitating an active exploration strategy that directly influences the behavior policy for better task exploration and learning outcome. Experimental results compared against stateoftheart baselines in challenging continuous control tasks illustrate the superior performance of HESS demonstrating its ability to enhance sample efficiency and overall task performance in sparsereward settings.  Strong Points 1. Innovation in Stabilizing Subgoal Representations: The introduction of a statespecific regularization to balance the stability and efficiency of subgoal representation learning is both novel and impactful addressing the problem of subgoal space nonstationarity effectively. 2. Novel Active Exploration Strategy: The papers approach to devise an exploration strategy based on novelty and potential measures without relying on intrinsic rewards is innovative. It naturally avoids additional nonstationarity and potentially accelerates the learning process. 3. Comprehensive Evaluation: The experimental evaluation is thorough with a comprehensive comparative analysis against several baselines across multiple challenging environments demonstrating the effectiveness of the proposed method. The ablation studies further elucidate the contribution of individual components of HESS.  Weak Points 1. Generalization to Highdimensional Subgoal Spaces: The paper does not address how the approach could be scaled or adapted to environments with highdimensional subgoal spaces. The employed count estimation in the representation space might not be effective in such cases. 2. Lack of Theoretical Analysis: While the proposed method is empirically validated the paper lacks a formal theoretical analysis which could strengthen the confidence in the proposed approachs efficacy and its underlying principles. 3. Detail on Hyperparameter Sensitivity: Although the paper mentions an ablation study on hyperparameter selection it provides limited insights into the sensitivity of the model performance to these hyperparameters potentially affecting the reproducibility and practical application of the method.  Questions and Feedback to the Author 1. Scalability to Highdimensional Spaces: Can the authors elaborate on how HESS could be adapted or extended to function effectively in environments with highdimensional subgoal spaces 2. Theoretical Underpinnings: Could the authors provide more theoretical insights or analysis on why the proposed stabilization regularization and the active exploration strategy outperform the baselines especially from a theoretical reinforcement learning perspective 3. Hyperparameter Sensitivity: More detail on the models sensitivity to hyperparameter adjustments would be beneficial. Can the authors provide guidance on how to select or tune these parameters in various environments  Review Summary and Recommendation The proposed HESS method introduces significant innovations in stabilizing subgoal representation and in employing an active exploration strategy for GCHRL tasks with sparse rewards demonstrating impressive empirical performance. Despite the strong points including novelty comprehensive evaluation and addressing a critical issue in GCHRL the paper would greatly benefit from a more detailed discussion on scalability issues theoretical analysis and hyperparameter sensitivity. Considering the contributions and the potential impact of the proposed method in the GCHRL field I recommend this paper for acceptance provided the authors address the questions and feedback especially regarding generalization theoretical grounding and hyperparameter insights which I believe would further strengthen the papers contributions.", "fgcIb5gd99r": " Peer Review  1) Summary of the Paper The paper presents an innovative approach to enhancing the selfattention mechanism widely used in natural language processing (NLP). By introducing a multiscale fusion selfattention model the research aims to address the limitation of the traditional selfattention mechanism that focuses solely on the onetoone relationships between words neglecting the significance of phraselevel information. The proposed model applies convolution kernels of varying sizes to capture phrase level information from sentences adjusting the attention mechanism to consider both wordtoword and wordtophrase relationships. This approach enables a more nuanced understanding of sentence structure and meaning. The novel addition of an attention matrix sparsity strategy further refines the models ability to prioritize relevant information enhancing its effectiveness. Experimental results on relation extraction tasks and the General Language Understanding Evaluation (GLUE) benchmark demonstrate that the proposed model outperforms existing baseline models showcasing its potential to significantly improve NLP applications.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a significant limitation in the existing selfattention mechanism by incorporating phraselevel information which is essential for understanding complex language structures.  The introduction of convolution kernels to extract phrase information at different scales is a novel and practical approach potentially increasing the models versatility and accuracy.  The attention matrix sparsity strategy is an innovative solution to improve information selection during the attention process potentially reducing computational overhead and increasing model efficiency. Weak Points:  The paper could benefit from a more detailed discussion of the computational implications of the proposed model including its scalability to large datasets and longer texts.  While the paper presents positive results on specific tasks it lacks a broader evaluation across a wider range of NLP tasks which would help understand the models generalizability.  The methodology section could provide a clearer explanation of the models architecture and the rationale behind the selection of convolution kernel sizes. Questions: 1. Can the authors elaborate on the computational efficiency of the proposed model especially in comparison to standard selfattention mechanisms 2. How does the model perform on tasks involving very long texts or documents given the added complexity of phraselevel information processing 3. Could the authors provide insights into how the model might be adapted or extended to other NLP tasks not covered in the experimental evaluation Feedback:  It would be valuable to include a more comprehensive evaluation of the models performance across a diverse set of NLP tasks to demonstrate its versatility and general applicability.  Expanding the methodology section with more details on the models architecture and design choices (e.g. selection of convolution kernel sizes) would enhance the papers clarity and replicability.  Consideration of the computational efficiency and potential scalability challenges in the discussion would provide a more balanced view of the model\u2019s applicability in realworld scenarios.  3) Review Summary The paper introduces a compelling advancement to the selfattention mechanism by incorporating multiscale phraselevel information addressing a crucial limitation in understanding complex language structures. The novel methodology including the use of convolution kernels and an attention matrix sparsity strategy demonstrates significant potential in improving the accuracy and efficiency of NLP models. Experimental results provide promising evidence of the models superiority over baseline models in relation extraction tasks and the GLUE benchmark. However the paper would benefit from a deeper exploration of the computational implications of the model a more extensive evaluation across diverse NLP tasks and a clearer articulation of the models architecture and design rationales. With these enhancements the paper could make a substantial contribution to the field of natural language processing offering a more nuanced and efficient approach to capturing the complexity of human language.", "s51gCxF70pq": " Review of \"kStep Latent (KSL): Improving Sample Efficiency in Reinforcement Learning through Temporally Consistent Representations\"  1. Summary This paper introduces kStep Latent (KSL) a novel representation learning methodology aimed at enhancing the efficiency of reinforcement learning (RL) agents operating in continuous control tasks within highdimensional imagebased state spaces. The essence of KSL lies in its unique approach to encouraging temporal consistency in the learned representations of the state space through a selfsupervised auxiliary task. This task requires the RL agents to predict actionconditioned future latent representations across multiple steps leveraging the underlying Markov decision process (MDP) in a way that complements the primary RL task. KSL enhances the Soft ActorCritic (SAC) algorithm demonstrating superior performance on the PlaNet benchmark suite across both data efficiency and asymptotic performance metrics when compared to several stateoftheart algorithms like DrQ RAD CURL and SACAE. The paper provides a thorough evaluation indicating that KSL not only accelerates learning efficiency but also generates representations that are more invariant to state space perturbations closely aligned with reward and capable of smoother transitions over time. Moreover KSL shows promising generalizability to new unseen tasks and outperforms other methods significantly in tasks within the DMControl suite.  2. Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty: The approach of enforcing temporal consistency in representations for enhancing learning efficiency is innovative and addresses a critical bottleneck in RL. 2. Comprehensive Evaluation: Rigorous comparisons with several baselines across different metrics provide credible evidence of KSLs effectiveness. 3. Generalization: KSLs ability to generalize better to unseen tasks highlights its potential for realworld applications. 4. Open Source Code: Providing access to the implementation is excellent for reproducibility and further research. Weak Points: 1. Complexity: The implementation of KSL appears to introduce significant complexity into the learning process and its integration with other RL algorithms beyond SAC may be nontrivial. 2. Wall Clock Time: Increased computational demands compared to other methods could be a drawback for practical applications especially where training time is a constraint. Questions and Feedback: 1. Scalability: How does KSL scale with increasingly complex environments beyond those in the PlaNet suite particularly in terms of computational resources 2. Integration with Other Algorithms: The paper focuses on SAC. How adaptable is KSL to other RL algorithms especially those with different learning dynamics 3. Impact on Exploration: Does the introduction of KSL and its focus on temporal consistency affect the exploration strategies of the RL agents If so how 4. Realworld Applicability: The paper hints at applications in industrial robotics and autonomous vehicles. Can the authors provide specific insights or potential modifications required for transitioning KSL from simulated to realworld tasks  3. Review Summary The paper presents a significant leap forward in the domain of representation learning within reinforcement learning addressing the critical challenge of learning efficiency in continuous control tasks. KSL through its innovative auxiliary task of predicting future latent representations conditioned on a series of actions not only speeds up the learning process but also ensures that the learned representations are robust closely tied to the reward and temporally coherent. Such capabilities suggest that KSL has the potential to vastly improve the realworld applicability of RL agents in sophisticated environments. However the complexity and the computational demands of implementing KSL alongside the potential challenges in integrating it with diverse RL algorithms raise important considerations for future research. Questions remain regarding the broader applicability of KSL in more complex environments and its adaptability to varied RL frameworks. Furthermore the impact of KSL on the exploration behavior of agents and its implications for realworld deployment merit deeper investigation. Overall the paper contributes notably to the field by not only proposing a novel solution to a longstanding problem but also by thoroughly validating its effectiveness and advantages over current stateoftheart methods. The insights gained from the analyses of learned representations and the reported improvement in generalization to new tasks underscore the potential of KSL to significantly enhance the capabilities of RL agents across various domains.", "lKrchawH4sB": " Review  1. Summary of the Paper This paper introduces a novel normalization technique named Heterologous Normalization (HN) for deep neural networks aiming to address the limitations of Batch Normalization (BN) in scenarios with small batch sizes. The core idea of HN is to calculate the mean and standard deviation (std) used for normalization from different pixel sets leveraging the strengths of various existing normalization methods. Specifically HN computes the mean across the (N H W) dimensions similar to BN while it calculates the std across the (NCHW) dimensions. This approach is designed to maintain BNs benefits for large batch scenarios while mitigating the inaccuracies in batch statistics estimation that arise with small batch sizes. The paper supports its claims with experiments showing that HN can achieve comparable or superior performance to existing normalization methods across different datasets and with both large and small batch sizes. Additionally HNs compatibility with Switchable Normalization (SN) is explored indicating that combining these methods can further enhance performance.  2. Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a significant limitation of Batch Normalization especially relevant in federated learning and edge computing scenarios where managing small batch sizes is common.  Heterologous Normalization is a novel approach that creatively separates the pixel sets for mean and std computation which is a clear departure from traditional normalization methods.  The comprehensive experimental validation across multiple datasets and comparison with a range of normalization techniques including a discussion on combination with SN strengthens the credibility of the proposed method. Weak Points:  While the paper presents a variety of experiments there is limited discussion on the theoretical underpinning behind why separating the pixel sets for computing mean and std is effective.  The computational overhead introduced by HN especially in comparison to BN and other normalization techniques is not thoroughly explored.  There is minimal consideration of the limitations or potential drawbacks of the proposed method in specific scenarios or with particular types of networks. Questions and Feedback:  Could the authors provide more theoretical insights or intuitive explanations on why calculating mean and std from distinct pixel sets leads to improved performance especially in small batch sizes  An analysis comparing the computational efficiency of HN against other normalization methods would be beneficial. Particularly how does the complexity of deriving statistics from larger pixel sets impact training and inference time  The paper could benefit from a deeper exploration into any scenarios where HN may not perform as expected or might introduce challenges offering a balanced view of the methods applicability.  3. Review Summary The paper makes a significant contribution to the field of deep learning by proposing Heterologous Normalization a method intelligently designed to overcome the small batch size limitation of Batch Normalization. The idea of separating the pixel sets for computing mean and std is innovative and shows promising empirical results across a variety of datasets and network architectures. However the paper would be strengthened by including a theoretical rationale for why this heterologous approach enhances performance and by assessing the computational implications of the proposed method more comprehensively. Additionally discussing potential limitations or less ideal scenarios for applying HN would provide a more balanced and nuanced view of its effectiveness. Overall the merits of the proposed Heterologous Normalization technique particularly its ability to improve performance in challenging small batch size conditions and its compatibility with existing normalization strategies like SN represent a valuable advancement in deep learning practices. Given the substantial experimental evidence presented the approach warrants further investigation and could have meaningful implications for a wide range of applications from edge computing to federated learning environments.", "qQuzhbU3Gto": " Review of \"An EdgeIndependent Graph Generative Model for Heterophily and Overlapping Communities\" Summary: The presented paper introduces a novel graph generative model that addresses significant limitations in current edgeindependent dot product models particularly in their inability to capture heterophily \u2014 the phenomenon where dissimilar nodes are connected. The authors argue that existing models which generally thrive on homophily (similaritybased connections) fall short in modeling complex realworld networks where both heterophily and homophily coexist. The proposed model innovates by utilizing nonnegative matrix factorization to output link probabilities that are interpretable in terms of community memberships effectively handling both homophily and heterophily and allowing for overlapping community detection. The paper validates the models effectiveness through theoretical proofs and experiments on realworld datasets demonstrating its superior performance in tasks like multilabel clustering and link prediction compared to current stateoftheart methods. Strong Points: 1. Novelty and Relevance: The paper addresses a significant gap in graph modeling literature by proposing a model that can capture both homophily and heterophily. Given the prevalence of heterophilous structures in realworld graphs this is highly relevant. 2. Theoretical Depth: The inclusion of theoretical results demonstrating the models expressiveness and its ability to exactly reconstruct graphs is compelling. The proofs add rigor to the claims of superiority over existing models. 3. Experimental Validation: The experiments conducted on realworld graphs substantiate the models practical effectiveness. Comparisons with wellknown graph generative models (like BIGCLAM and SYMNMF) and achieving lower reconstruction errors and better alignment with groundtruth communities illustrate the models advantages. 4. Interpretability: By utilizing nonnegative embeddings the model ensures that the link probabilities are interpretable in community detected terms. This is a significant strength as interpretability is crucial for the adoption of machine learning models in various applications. Weak Points: 1. Model Complexity and Scalability: The paper could better address concerns regarding the models complexity and scalability. Specifically how does the computational cost compare to alternative methods and is the model practical for very large networks 2. Generalization Beyond Undirected Unweighted Graphs: The focus is on undirected unweighted graphs. Its unclear how the model would perform or could be adapted for directed graphs or graphs with weighted edges which are also common in realworld applications. 3. Detailed Comparison with Related Work: While the paper does compare its method with others a more detailed analysis of how the proposed model fundamentally differs from specific related approaches (e.g. in model formulation or optimization strategy) could provide deeper insights. 4. Evaluation of Community Detection Quality: Though the paper evaluates the similarity of detected communities to groundtruth communities through F1Score further analysis regarding the interpretability and meaningfulness of these communities in the context of realworld applications would be valuable. Questions and Feedback to the Author:  Could the authors clarify how the models performance and computational cost scale with the size of the graph  How adaptable is the proposed model to directed or weighted graphs  It would be helpful if the authors could discuss any limitations encountered during the models development and potential avenues for addressing these in future work.  Can the authors provide more insights into how the communities detected by their model can be interpreted in realworld scenarios For instance are there qualitative differences in the communities detected in heterophilous vs. homophilous settings Review Summary: The paper makes a significant contribution to the graph modeling literature by introducing a model that captures both homophily and heterophily addressing a notable gap in current methodologies. The theoretical underpinnings and empirical validations underscore the models effectiveness and superiority over existing models for a range of tasks. However considerations regarding the models scalability adaptability to different types of graphs and a deeper exploration of detected communities interpretability could further strengthen the work. Given its strong theoretical foundation demonstrated practical utility and the relevance of the addressed problem the paper stands out as a valuable contribution to the field. Nonetheless addressing the mentioned weak points could enhance its impact and applicability to a broader range of graph modeling tasks.", "oJGDYQFKL3i": " Review of \"Object Dynamics Distillation Network (ODDN) for Scene Representation in Videos\"  1. Paper Summary This paper introduces the Object Dynamics Distillation Network (ODDN) a novel architecture designed to learn objectcentric representations and dynamics from video input in an unsupervised manner. It extends upon existing work on scene decomposition from static images by focusing explicitly on the distillation of object dynamics (e.g. velocity) and modeling object interactions through a relation module. The key innovation lies in the ability of ODDN to distill and disentangle object dynamics from sequential frames enabling enhanced performance in video understanding tasks such as video event reasoning and video prediction. This is achieved by a framework that combines amortized variational inference for scene decomposition with a singlelayer transformer encoder for dynamic distillation and a relation module for modeling interactions between object pairs. The proposed method achieves stateoftheart performance on the CLEVRER dataset for tasks involving physical event reasoning and demonstrates superior scene decomposition quality significantly advancing the field of video understanding and objectcentric scene representation.  2. Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a significant gap in the literature by focusing on the distillation of object dynamics from video inputs which is underexplored compared to static scene decomposition.  The unsupervised approach to learning both objectcentric representations and their dynamics is particularly compelling potentially reducing the need for costly labeled data.  The incorporation of a relation module to model object interactions demonstrates a nuanced understanding of scene dynamics offering a more comprehensive approach than previous methods.  Extensive experimentation and comparison with baseline methods alongside detailed ablation studies provide strong evidence of ODDNs performance benefits. Weak Points:  The paper relies heavily on synthetic datasets (e.g. CLEVRER) which may not fully capture the complexities of realworld videos. The generalizability of ODDN to more diverse or challenging datasets remains an open question.  While the unsupervised nature of ODDN is a strength there is limited discussion of its limitations such as potential scenarios where the method may fail or produce inaccurate dynamics.  The computational complexity of ODDN especially when processing long video sequences or highresolution videos is not sufficiently addressed or compared to baseline methods. Questions and Feedback:  How does ODDN perform on realworld datasets with more varied and complex object interactions compared to the synthetic datasets used in the experiments  Can the authors elaborate on any potential limitations or failure modes of ODDN particularly in scenarios with rapid object dynamics or highly cluttered scenes  Feedback: It would be beneficial for future work to explore the application of ODDN to more diverse datasets including realworld videos to assess its robustness and generalizability. Additionally a deeper analysis of model scalability and efficiency would augment the papers contributions.  3. Review Summary This paper presents a significant advancement in the field of video understanding by proposing the ODDN framework which effectively distills and disentangles object dynamics from video input in an unsupervised manner. The novel integration of a relation module to model object interactions further distinguishes ODDN from existing methods leading to stateoftheart performance in tasks of video event reasoning and prediction. Despite its strengths questions remain about the models generalizability to realworld datasets and its computational efficiency. Nonetheless the comprehensive experimentation alongside thorough discussions of the methods underlying principles and comparison to baseline methods provide convincing evidence of ODDNs value. Future work addressing the noted limitations and exploring applications to diverse video datasets would be beneficial.", "gjNcH0hj0LM": " Review of \"A Novel Label Propagation Framework for TimeSeries Active Learning Using Temporal Coherence\"  1. Summary The paper introduces a novel label propagation framework Temporal Coherencebased Label Propagation (TCLP) to address the challenge of efficiently labeling timeseries data for classification purposes. Traditional active learning strategies fail to consider the inherent temporal coherence in timeseries data leading to inefficient labeling. TCLP leverages this temporal coherence by propagating the queried label across a segment of data points estimated to have the same label thus significantly reducing the number of required labels for effective learning. This approach is shown to improve classification accuracy by up to 7.1 times with only 0.8 queried labels across various datasets outperforming existing methods.  2. Strengths and Weaknesses Strong Points:  Innovative Approach: The paper addresses the label sparsity problem in timeseries data with an innovative solution that exploits temporal coherence an often overlooked characteristic of timeseries data.  Significant Improvement in Efficiency: TCLP demonstrates a profound increase in classification accuracy with minimal labeling showcasing its potential to reduce time and costs associated with data labeling.  Extensive Experimental Validation: The authors conduct comprehensive experiments across multiple datasets and query selection strategies providing robust evidence for TCLPs effectiveness.  Generalizability: The framework is compatible with any query selection strategy displaying its adaptability to various active learning settings. Weak Points:  Complexity Analysis Missing: While the paper provides a detailed description of the TCLP algorithm and its components it lacks a formal analysis of the computational complexity which is critical for understanding its scalability.  Limited Discussion on Limitations: The paper underexplores potential limitations or challenges in deploying TCLP such as its sensitivity to the accuracy of the initial classifier or the impact of highly irregular time series on its performance.  Lack of Realworld Application Scenarios: Despite extensive validation the paper does not discuss realworld applications or scenarios where TCLP could be particularly beneficial or face challenges. Questions and Feedback:  How does TCLPs performance change with varying degrees of temporal coherence within the datasets It would be helpful to understand its limitations or failure points.  Can the authors provide more insights into the computational complexity of TCLP and how it scales with large datasets  The paper could benefit from a discussion on how TCLP copes with highly irregular or noisy timeseries data. Are there specific strategies employed to mitigate such challenges  3. Review Summary The paper presents a compelling advancement in active learning for timeseries data through the proposed TCLP framework. By leveraging temporal coherence TCLP remarkably improves labeling efficiency without compromising classification accuracy a significant step forward compared to existing methods. The extensive experimental validation across various datasets underscores TCLPs effectiveness and general applicability with different query selection strategies. However the lack of computational complexity analysis leaves questions about TCLPs scalability and practical deployment in realworld scenarios. Additionally the paper could be strengthened by addressing potential limitations and providing insights into handling irregular and noisy data. Therefore while TCLP demonstrates substantial promise a detailed examination of its computational demands and broader discussion on its practical implications would make a more compelling case for its widespread adoption. Despite these areas for improvement the innovative approach and substantiated results make a significant contribution to the field of timeseries active learning.", "vh-0sUt8HlG": " Summarize the paper The paper introduces MobileViT a lightweight and generalpurpose vision transformer tailored for mobile devices. It overcomes the challenges of adapting selfattentionbased vision transformers (ViTs) which usually require large model sizes and computational resources making them unsuitable for mobile deployment. Unlike traditional ViTs that lack spatial inductive biases and rely heavily on data augmentation and regularization to prevent overfitting MobileViT thoughtfully integrates the spatial inductive biases of Convolutional Neural Networks (CNNs) with the global information processing capabilities of ViTs. It does so through a novel MobileViT block that encodes both local and global information effectively thereby achieving high performance with fewer parameters and simple training recipes. The study showcases that MobileViT significantly outperforms both CNN and ViTbased networks on vision tasks like ImageNet classification and MSCOCO object detection with a smaller parameter count. It also demonstrates MobileViTs generalpurpose nature robustness to hyperparameter choices and superior generalization capability over existing CNNs and ViTs.  Strong and weak points of the paper Strong points: 1. Innovative Approach: The combination of CNNs spatial inductive biases with ViTs global information processing in a lightweight design for mobile devices is novel and addresses a significant gap in the current landscape of vision models. 2. Comprehensive Evaluation: The paper presents extensive experiments across different tasks datasets and network sizes to evaluate MobileViTs performance demonstrating its robustness and versality. 3. Open Source: Providing the source code is beneficial for reproducibility and further research fostering an open science environment. Weak points: 1. Comparative Analysis Depth: While comparisons are made with existing CNN and ViT models deeper insights into why MobileViT outperforms each category specifically especially in the context of mobile devices could be more thoroughly discussed. 2. Model Interpretability: The paper lacks an exploration of the internal workings or the interpretability of the MobileViT which could offer insights into its efficacy and areas for improvement. 3. Energy Consumption Analysis: For mobile devices energy consumption is as critical as model size and latency. The paper primarily discusses FLOPs latency and parameter count without providing details on energy efficiency. Questions and Feedback: 1. Hardware Dependency: How does MobileViTs performance and efficiency vary across different mobile device hardware 2. Scalability: Can MobileViT be scaled up for more complex tasks beyond the scope of the evaluated datasets while maintaining its lightweight nature 3. Impact of Different Data Augmentation Techniques: How sensitive is MobileViT to various data augmentation techniques given its lesser dependence on extensive data augmentation  Review summary This paper presents a compelling advancement in the field of mobile vision through the introduction of MobileViT. The methodology cleverly combines the strengths of CNNs and ViTs in a manner wellsuited for the constraints of mobile devices pushing the boundaries of what is achievable in terms of accuracy model size and latency on such platforms. MobileViTs performance gains over existing models across several benchmarks are a testament to its effectiveness and the potential it holds for future mobile vision applications. However the paper could benefit from a deeper exploration into the interpretability of the model variance in performance across different hardware and a more comprehensive analysis of its energy consumption. Addressing these concerns would not only bolster the technical contribution but also provide crucial insights for practical deployment scenarios. Overall the strength of the proposed approach combined with rigorous evaluation makes a compelling case for the adoption of MobileViT in mobile vision tasks signaling a significant step forward in the development of lightweight highperforming vision models for edge devices.", "jXKKDEi5vJt": " Peer Review of \"Byzantine Robust Optimization in Heterogeneous Federated Learning\"  1. Summary of the Paper This paper addresses the significant challenge of Byzantine robust optimization in distributed or federated learning environments where data across workers is heterogeneous (noniid). Recognizing the failure of existing Byzantine defenses due to unrealistic assumptions of data homogeneity the authors propose a novel method to adapt these defenses to heterogeneous data scenarios. The paper introduces a \"bucketing\" step that when combined with existing robust aggregation rules can effectively mitigate the impact of Byzantine attacks under noniid data distribution. This approach not only remedies the vulnerability of current aggregation schemes to simple attacks but also establishes theoretical convergence guarantees under realistic assumptions marking a first in the field. The proposed solution is comprehensive incorporating theoretical analyses experimental validation and a discussion on optimal rates and lower bounds for Byzantine robust optimization despite heterogeneity.  2. Strong and Weak Points Questions and Feedback  Strong Points: 1. Novel Approach: This work innovatively addresses the overlooked issue of data heterogeneity in Byzantine robust federated learning providing a practical solution with \"bucketing.\" The approach is not only novel but also critical for realworld applications where data across workers are naturally noniid. 2. Theoretical and Experimental Validation: The paper thoroughly validates the proposed methods through both rigorous theoretical analysis and extensive experiments. The combination of theoretical rigor and empirical evidence effectively demonstrates the effectiveness and efficiency of the proposed solution. 3. Practical Implications: The proposed method has significant practical implications for the deployment of Byzantineresistant federated learning systems making it a valuable contribution to both academia and industry.  Weak Points: 1. Complexity and Usability: The paper could benefit from a clearer discussion on the computational complexity and practical usability of the proposed \"bucketing\" scheme. How does the bucketing step impact the overall computational cost and what tradeoffs might practitioners face when implementing this approach in largescale environments 2. Limited Exploration of Alternative Approaches: While the bucketing approach is effective the paper could explore and possibly benchmark against other potential methods for handling data heterogeneity in Byzantine scenarios providing readers with insight into why bucketing is superior. 3. Discussion on Parameter Selection: The paper introduces several parameters such as the bucket size \"s\" and the optimal rate parameters. It would be beneficial to include more detailed guidelines or strategies for selecting and tuning these parameters in practical scenarios.  Questions and Feedback: 1. Can the authors elaborate on the potential computational overhead introduced by the bucketing step and how it scales with the number of workers and the size of the dataset 2. It would be insightful to see a comparison with alternative approaches that tackle noniid data in Byzantine environments. Could the authors comment on this 3. Further clarification on the parameter selection process for practitioners looking to implement this method would be valuable. Can the authors provide more detailed recommendations or examples of parameter tuning in different scenarios  3. Review Summary and Recommendation The paper presents a significant contribution to the field of federated learning by addressing the critical challenge of Byzantine robust optimization in the presence of heterogeneous (noniid) data. The proposed bucketing scheme is both innovative and practically relevant offering a scalable solution to a previously unaddressed problem. The combination of theoretical proofs and experimental validation underlines the strength and applicability of the method. Despite its strengths the paper would benefit from additional clarifications regarding the practical implementation of the proposed method especially concerning computational costs and parameter selection. Addressing these areas would enhance the papers utility for practitioners and researchers alike. Based on the above analysis I recommend the acceptance of this paper with minor revisions aimed at addressing the discussed weak points. The papers contributions are valuable advancing the current understanding and addressing a gap in Byzantine robust federated learning research.", "pETy-HVvGtt": " Peer Review  1. Summary The paper introduces a novel framework for analyzing how multivariate representations manage to disentangle the underlying generative factors of data. The primary criticism of existing metrics for evaluating disentanglement is that they mostly fail to recognize entanglements involving interactions among more than two variables. To address this limitation the authors leverage the Partial Information Decomposition (PID) framework to decompose the information in representations into entangled and disentangled components. They propose a new metric for disentanglement called UNIBOUND that quantifies the extent to which a representation is disentangled by capturing interactions among multiple variables. Through a series of experiments involving variational autoencoders (VAEs) they demonstrate the effectiveness of their proposed metric in capturing complex entanglements and provide insights into various strategies for achieving disentangled representations. They argue that their findings challenge existing approaches and suggest that novel methods could be developed based on their analysis.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novel Framework: The paper presents a significant advancement in disentanglement analysis by introducing a framework that accounts for multivariable interactions using PID an approach not explored thoroughly in existing literature.  Robust Quantitative Metric: UNIBOUND is a novel metric that provides a more nuanced understanding of disentanglement by detecting entanglements involving multiple variables a capability that current metrics lack.  Comprehensive Evaluation: Extensive experiments with VAEs and the introduction of entanglement attacks successfully illustrate the practical relevance and robustness of the proposed metric. Weak Points:  Complexity and Accessibility: The conceptual and mathematical complexity of PID and the proposed framework may pose a barrier to understanding for readers unfamiliar with information theory.  Generalizability: The experiments are focused on VAEs and it remains uncertain how well the proposed framework and UNIBOUND metric generalize to other types of generative models or representation learning paradigms.  Potential for Bias in Evaluation Methods: The selection criterion for \"the best half\" models based on random seeds as mentioned in the empirical analysis might introduce bias in evaluating the effectiveness of the proposed metric. Questions and Feedback:  Clarity on Definitions: Can the authors provide a more accessible explanation or intuitive examples to aid understanding of PID terms (i.e. unique redundant and synergistic information) for readers less familiar with Partial Information Decomposition  Comparison with Other Domains: How does the proposed framework perform in domains other than images such as text or audio data Exploring these areas could further validate the generalizability of the approach.  Implications for Model Design: Could the authors discuss the implications of their findings for the design and training of generative models Specifically how could model architectures or objectives be adjusted based on insights gained from analyzing representations using the proposed framework  3. Review Summary The paper addresses a crucial and underexplored problem in the field of representation learning by introducing a novel framework for disentanglement analysis using Partial Information Decomposition. The proposed framework and the UNIBOUND metric represent a significant leap forward in our ability to quantitatively analyze and understand disentanglement in multivariate representations. The extensive experimentation and the introduction of entanglement attacks provide strong evidence of the proposed metrics capabilities to uncover complex entanglements that existing metrics fail to detect. Despite the novelty and strengths of the work concerns regarding the complexity and accessibility of the theoretical foundations generalizability beyond VAEs and potential biases in model evaluation methods need to be addressed. Further clarifications additional experiments across diverse domains and discussions on practical implications for model design would greatly enhance the contribution of this work. In summary the paper presents important and innovative research with the potential to significantly impact the field of representation learning. However addressing the issues related to complexity generalizability and evaluation methods will be crucial for ensuring its broader acceptance and applicability.", "i1ogYhs0ByT": "Given the complexity and depth of the paper on Transformer with a Mixture of Gaussian Keys (TransformerMGK) my review is categorized as follows:  1. Summary of the Paper The paper introduces TransformerMGK a novel architecture designed to address the issue of redundancy among the heads in traditional multihead attention mechanisms found in transformers. By employing a Gaussian mixture model to create a mixture of keys for each attention head TransformerMGK enables a more efficient and expressive representation of attentions. This not only reduces the models parameters and computational requirements but also maintains or enhances accuracy across various tasks compared to baseline models. The performance and efficiency of TransformerMGK are empirically demonstrated through experiments on language modeling and the Long Range Arena benchmark.  2. Strong and Weak Points Questions and Feedback to the Author  Strong Points: 1. Innovation in Architecture: The concept of replacing redundant heads in transformers with a mixture of Gaussian keys is both novel and innovative. It addresses the inefficiencies in the traditional attention mechanisms effectively. 2. Efficiency and Performance: The paper successfully demonstrates that TransformerMGK requires fewer parameters and computational resources while maintaining or exceeding the performance of conventional transformers in several benchmarks. 3. Extensibility: The ability to extend TransformerMGK towards linear attentions is a strong point showing the model\u2019s flexibility and potential applicability in a wide range of tasks. 4. Theoretical Grounding: The correspondence established between selfattention in transformers and a Gaussian mixture model provides a solid theoretical foundation for the proposed architecture.  Weak Points: 1. Complexity in Implementation: Despite the theoretical appeal the practical implementation of TransformerMGK might be complex potentially hindering its adoption. The paper could benefit from a more detailed discussion on implementation challenges. 2. Generalization across Tasks: While results on language modeling and the LRA benchmark are promising the paper might have explored its applicability and performance across a broader set of tasks including outside NLP to validate its generalization capability. 3. Resource Requirements for Training: The paper lacks details on the training resources required. Given the complexity of GMM it would be valuable to understand if training the proposed model demands significantly more computational resources than traditional models.  Questions and Feedback: 1. Can the authors elaborate on the practical challenges they anticipate in implementing TransformerMGK in realworld applications 2. How does TransformerMGK perform in tasks with highly unstructured or sparse data given its reliance on Gaussian mixtures 3. Could the paper provide insight into the models training time and resource requirements compared to baseline transformers 4. It would be beneficial if future work could explore the application of TransformerMGK in other fields such as computer vision or speech recognition to further validate its versatility.  3. Review Summary The paper presents a compelling modification to the traditional transformer model aiming to enhance efficiency by addressing redundancy in attention heads. The proposed TransformerMGK is theoretically sound and empirically validated to be both more efficient and equally or more accurate than its counterparts in selected benchmarks. Despite its strengths the models practical implementation complexity its generalization ability across a wider range of tasks and the lack of detail on training resource requirements are areas that could be improved. My recommendation would be to accept this paper provided the authors can address the raised questions especially concerning model generalization and practical implementation details. The contribution of this work to reducing model redundancy and enhancing computational efficiency in transformers is substantial and promises significant impacts on future research and applications.", "mz7Bkl2Pz6": " Review of \"Probing the Limits of Stochastic Gradient Descent in Nonconvex Stochastic Optimization\"  Summary This paper presents a comprehensive analysis of Stochastic Gradient Descent (SGD) in the context of nonconvex stochastic optimization problems focusing particularly on scenarios that closely align with practical applications in machine learning. The authors highlight the inadequacies in the existing SGD theory which predominantly centers around restrictive assumptions of global Ho\\xcc\\x88lder continuity for the gradient and unrealistic noise models. To bridge this gap the paper introduces relaxed assumptions: local Ho\\xcc\\x88lder continuity for the gradient and a generalized potentially unbounded noise model. Under these conditions the authors prove that SGDs iterates either converge to a stationary point or diverge with certainty. Further they demonstrate that under a slightly more restrictive set of conditions the objective function remains bounded thus ensuring the algorithms stability. The study also introduces novel analytical strategies  the pseudoglobal strategy and the local strategy  to tackle the challenges posed by relaxed assumptions. The contributions are poised to extend the theoretical foundations of SGD making the theory more applicable to reallife optimization problems encountered in machine learning.  Strong Points 1. Significant Theoretical Contribution: The paper addresses a critical gap in SGD theory making significant strides towards aligning the theory with practices commonly found in machine learning optimization problems. The formulation and proof of the new theorems under less restrictive assumptions are commendable. 2. Novel Analytical Strategies: The introduction of the pseudoglobal and local strategies for analysis is insightful. These strategies could be widely applicable and benefit future research in optimization and machine learning areas. 3. Practical Implications: The work has strong practical implications suggesting that SGD can be confidently applied to a wider range of problems without the fear of divergent iterates under the given conditions. The clear delineation of conditions under which global convergence and stability are guaranteed is particularly useful.  Weak Points 1. Complexity of Assumptions: While the relaxed assumptions bridge theoretical and practical gaps their complexity and the practicality of verifying them in realworld scenarios could be challenging. Further clarification or guidance on how practitioners can ascertain these conditions would enhance the papers utility. 2. Empirical Validation: The paper is predominantly theoretical. Incorporating empirical studies to validate the theoretical advancements could strengthen the authors claims and increase the relevance to practitioners in the field. 3. Discussion on Computational Implications: The authors provide an indepth theoretical analysis but do not discuss the potential computational implications of their findings. Insights into how the proposed theory affects the computational efficiency of SGD in practical settings would be valuable.  Questions and Feedback to the Author 1. Could the authors provide more insight or examples on how the new assumptions can be verified in practice for specific machine learning models 2. Are there identified scenarios or case studies where the previous restrictive assumptions of SGD theory failed but the current proposed theory holds Elaborating with examples could be illustrative. 3. The potential computational impact of implementing SGD under the new theoretical framework is not discussed. Do the authors anticipate any changes in computational complexity or efficiency  Review Summary The paper proposes an enriched theoretical framework for analyzing SGDs convergence in nonconvex stochastic optimization addressing significant limitations in existing theory. The introduction of less restrictive assumptions and novel analytical strategies represents a notable advancement in the understanding of SGDs behavior. These contributions are theoretically solid and potentially impactful for practical applications. However the complexity of the new assumptions and the lack of empirical validation slightly hinder the papers immediate applicability. Moreover discussions on how this advanced theory impacts computational practices and efficiency are notably absent. The paper would benefit from further elucidation on implementing the theoretical insights in practical settings possibly supported by empirical studies and computational analysis. Overall this work is a valuable step towards bridging the gap between SGD theory and its practical application in machine learning. With some enhancements particularly in empirically validating the proposed theory and examining its computational implications the paper could significantly influence both theoretical and practical domains of stochastic optimization.  Supporting Arguments for Recommendation Given the theoretical depth and novelty along with the implications for broader application fields I would recommend this paper for publication. However I also encourage the authors to address the identified weaknesses particularly around practical applicability empirical validation and the computational implications of their theory. Doing so will enhance the papers relevance and utility to both the research community and practitioners in machine learning and optimization fields.", "fyLvrx9M9YP": " Review of the Paper on Unsupervised Disentanglement of Syntactic Roles Using ADVAE  Summary of the Paper This paper presents a novel approach to disentangle and understand the impact of syntactic roles in sentences by using AttentionDriven Variational Autoencoders (ADVAEs). The main contribution lies in showing that syntactic roles (like subjects verbs direct objects) can be independently encoded into separate latent variables within a sentence without any supervision. This is achieved through a deep generative model that exploits transformerbased attention mechanisms. The model ADVAE leverages crossattention rather than traditional selfattention allowing it to map different syntactic components to unique latent variables. The authors develop an inventive evaluation protocol to measure the level of disentanglement concerning the syntactic roles realizations using a set of perturbation and attentionbased techniques. They empirically demonstrate that ADVAE can achieve higher syntactic role disentanglement than existing sequence and transformer variational autoencoders. Through experiments on the SNLI dataset the paper verifies that ADVAE not only disentangles syntactic roles effectively but also enables controlled manipulation of sentence structures by altering the corresponding latent variables.  Strengths and Weaknesses Strengths: 1. Innovative Approach: The paper introduces an original method for understanding and manipulating the syntactic structure of sentences in an unsupervised manner. This is a significant step forward in interpretability and controllability within NLP models. 2. Solid Experimental Design: The evaluation protocol designed to assess disentanglement is comprehensive and wellthoughtout including both encoder and decoder analyses as well as qualitative and quantitative assessments. 3. Broad Applicability: While focused on syntactic roles the papers proposed model and methodology have potential applications across various NLP tasks including documentlevel analysis and other linguistic structures. Weaknesses: 1. Complexity and Accessibility: The model while powerful is complex. The paper could benefit from a more detailed explanation or simplification of the underlying architecture for better accessibility to a wider audience. 2. Evaluation on Other Datasets: The primary evaluation is conducted on the SNLI dataset. Testing on more diverse datasets especially those with complex sentence structures would strengthen the claims of generalizability. 3. Analysis of Misalignments: The paper could delve deeper into cases where the model fails to accurately disentangle or control syntactic roles providing insights into limitations and areas for improvement. Questions and Feedback to the Author: 1. Could the authors provide more detail on the limitations of ADVAE especially in cases of complex sentence structures or lowerresource languages 2. How does ADVAE handle sentences with ambiguous or multiple plausible syntactic interpretations 3. Is there any insight into how the model might be adapted or extended to work on languages with free word order  Review Summary The paper introduces a significant advancement in the field of NLP by presenting a novel method to disentangle and manipulate syntactic roles within sentences using ADVAE a deep generative model leveraging attention mechanisms. This work successfully bridges the gap between syntactic analysis and neural representation learning offering a pathway towards more interpretable and controllable NLP models. Despite its strengths like innovative approach solid experimental design and broad applicability the paper could be improved by addressing its complexity expanding evaluations to more varied datasets and providing deeper analyses of limitations and failure cases. Overall the contributions of this paper are substantial opening new avenues for research in unsupervised learning and syntactic manipulation in NLP. The methodology and results are wellvalidated through robust experiments making a compelling case for the effectiveness of ADVAE. This work stands as a promising step towards achieving finegrained control over linguistic structures in neural models warranting recognition and further exploration in the research community.", "yRYtnKAZqxU": " Peerreview for the paper titled \"An Empirical and Theoretical Evaluation of Graph Contrastive Learning and Reconstruction Approaches in Unsupervised Graph Representation Learning\"  Summary of the Paper The paper presents a comprehensive study on the efficacy of Graph Contrastive Learning (GCL) and reconstructionbased approaches within unsupervised graph representation learning. Initially it theorizes that the success of GCL when used with generic graph augmentations is highly contingent upon the graph edit distance between classes. This is followed by an introduction to a synthetic dataset designed to control the \"style vs. content\" ratio within samples facilitating an empirical analysis of graph representation methodologies under varied dataset conditions. The findings suggest that reconstruction strategies excel in scenarios with minimal \"style\" (irrelevant information) while GCL shows improved performance with moderate amounts of \"style\". These insights are bolstered by evaluations using untrained Graph Neural Networks (GNNs) as a baseline showcasing their surprising competitiveness. The research culminates in proposing a new augmented graph autoencoder approach (AAGAE) and offers a systematic framework to assess and compare different paradigms of unsupervised graph representation learning.  Strong Points 1. Novel Framework Introduction: The paper\u2019s introduction of a synthetic data generation process for controlling the style vs. content ratio in samples is innovative. This approach not only advances the understanding of how different learning methods perform under various conditions but also fills a significant gap in the literature regarding systematic evaluation frameworks in graph representation learning. 2. Theoretical and Empirical Validation: The dual approach of theoretical analysis followed by empirical validation offers robust evidence for the papers claims. The theoretical grounding provides a clear rationale for expected outcomes which are then empirically tested using both benchmark datasets and the newly introduced synthetic benchmark. 3. Comprehensive Methodology: The studys methodology is comprehensive incorporating analyses of invariance sample complexity and accuracy across different learning paradigms GNN architectures and augmentation strategies. This thoroughness ensures a broad and deep understanding of the fields current state.  Weak Points 1. Baseline Comparisons: While untrained GNNs are used as a baseline to showcase the effectiveness of pretrained models the selection and justification of these specific baselines are not sufficiently discussed. More insight into the choice of untrained models and their architectural specifics could strengthen the studys conclusions. 2. Augmentation Strategies: The paper discusses generic graph augmentations (GGA) and introduces contentaware augmentations (CAA) in the context of GCL. However the comparative analysis of these augmentations lacks depth regarding their design and implementation specifics potentially leaving out considerations that could impact the performance of contrastive learning frameworks significantly. 3. DomainSpecific Implications: The implications of the findings for domainspecific applications such as molecular property prediction are not thoroughly explored. A more detailed discussion on how the style vs. content ratio and the choice of augmentations could impact practical applications would enhance the papers relevance to practitioners in the field.  Questions and Feedback to the Author 1. Baseline Justification: Can the authors provide more details on the choice of untrained GNNs as a baseline Understanding the rationale behind this decision will help readers appreciate the benchmarks significance and contextualize the competitiveness of untrained models. 2. Augmentation Strategy Details: Could the authors elaborate on the design principles behind the generic graph augmentations (GGA) and contentaware augmentations (CAA) Specific insights into their implementation could offer valuable guidance to researchers attempting to replicate or build upon this work. 3. Implications for DomainSpecific Applications: The paper briefly mentions molecular property prediction as a potential application area. Can the authors discuss in greater detail how the findings particularly around the style vs. content decomposition could be leveraged in such domainspecific contexts  Review Summary This paper offers significant contributions to the understanding of unsupervised graph representation learning particularly in analyzing the conditions under which GCL and reconstructionbased methods are expected to perform well. The introduction of a synthetic data generation process for systematically varying the style vs. content ratio is particularly commendable providing a novel tool for future research. Both theoretical and empirical analyses are robust giving comprehensive insights into the efficacy of different learning paradigms under varied conditions. However the paper could benefit from a deeper discussion on the choice of baselines detailed elaboration on augmentation strategies and a more explicit exploration of domainspecific implications. Addressing these areas could make the findings more accessible and immediately actionable for practitioners in the field. Overall the paper contributes valuable frameworks and insights to the burgeoning field of graph representation learning. With further detail and clarification in the areas mentioned it could serve as a foundational piece for future research exploring unsupervised learning paradigms in graphstructured data.", "k-sNDIPY-1T": " 1) Summary of the Paper This paper explores the development of lowcomplexity datadriven blackbox models for simulating the nervous system of the nematode C. elegans using recurrent neural networks (RNNs) specifically Long ShortTerm Memory (LSTM) and Gated Recurrent Units (GRU) architectures. The study grounds itself in the challenge of understanding the human nervous system by initially focusing on simpler organisms. Given the detailed biological and structural data available on C. elegans the organism serves as an ideal candidate for developing and benchmarking simulation methods. The authors propose a methodology that bypasses the need for detailed internal system knowledge by utilizing peripheral inputoutput data to generate reducedorder models. These models aim to reproduce the system dynamics of C. elegans with low Root Mean Square Error (RMSE) thereby facilitating the study of neuronal behaviour in complex systems. The paper meticulously compares the performance of different RNN architectures in modeling the C. elegans nervous system concluding that GRU models with a hidden layer size of 4 units offer the most optimal balance of simplicity and accuracy in replicating the organisms response to various stimuli.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Approach: The paper introduces an innovative approach to modeling complex biological systems through datadriven blackbox models significantly reducing the computational complexity associated with traditional simulation methods.  Comprehensive Analysis: The detailed comparison of RNN LSTM and GRU architectures provides valuable insights into their respective capabilities in capturing the dynamic behaviour of neuronal systems.  Significant Results: Demonstrating that a GRU network with just 4 hidden units can accurately simulate C. elegans neural activity is a remarkable achievement offering a promising avenue for further research in neural simulation. Weak Points:  Lack of Biological Validation: While the paper presents a technical validation of the models it does not delve into biological validation with actual C. elegans specimens to verify the practical applicability of the simulation results.  Limited Scope of Stimuli: The paper focuses on reproducing responses to \"very different stimuli\" but does not explicitly detail the variety and complexity of these stimuli which could impact the evaluation of model versatility. Questions and Feedback:  Could the authors provide more insight into the biological validation of the models possibly involving experimental data from actual C. elegans specimens to strengthen the findings  It would be beneficial to include a broader spectrum of stimuli in the study to thoroughly test the models robustness and applicability to various realworld scenarios. Can the authors expand on this aspect  The paper could benefit from a section discussing the potential limitations of the blackbox approach particularly in contexts where understanding the internal mechanisms of the nervous system is crucial. Could such a discussion be included  3) Review Summary This paper takes a significant step forward in the simulation of biological neural systems offering a method that substantially lowers the computational demands by focusing on peripheral inputoutput behaviours rather than internal systems details. The use of C. elegans as a model organism is welljustified given its simplicity and the abundance of available data making it an excellent subject for benchmarking the proposed modelling approach. The finding that GRU networks with minimal complexity can achieve high fidelity in simulating neuronal responses provides a compelling argument for the potential of blackbox models in neuroscience research. However the study would greatly benefit from more explicit details regarding the stimuli used for model testing and incorporating biological validation to ensure that the simulation results are grounded in the biological realities of C. elegans nervous system. In conclusion this paper presents a promising avenue for simplifying the simulation of complex biological systems. It lays a foundation for future research that could extend these methodologies to more complex organisms including humans. Further exploration and validation of these models incorporating a wider variety of stimuli and direct biological evidence will be crucial to fully realize their potential and applicability in neuroscience and beyond.", "u2GZOiUTbt": " Review of the Paper Utilizing Asymmetric Affinity Score in FewShot Learning  Summary The paper introduces an innovative fewshot learning (FSL) approach leveraging an asymmetric affinity score termed Task Affinity Score (TAS) aimed at quantifying the complexity of utilizing knowledge from one task to aid in learning another. This approach grounded on the maximum bipartite matching algorithm and the Fisher Information matrix seeks to identify and utilize relevant training data for episodically finetuning a fewshot model. The authors offer a theoretical examination of the TAS declaring it mathematically sound and demonstrate the practical benefits of their method through extensive simulations. They report improvements in classification accuracy across several benchmark datasets using models with fewer parameters thereby underscoring the efficacy and efficiency of their proposed methodology.  Strengths 1. Innovation: The papers concept of an asymmetric affinity score is novel and addresses a significant challenge in fewshot learning \u2014 the identification and utilization of relevant tasks for improving learning efficiency and performance. 2. Theoretical Foundation: The authors provide a solid theoretical background for the TAS ensuring it is mathematically welldefined and justifying its application in FSL. 3. Empirical Validation: Extensive experiments across multiple benchmark datasets not only validate the approach but also highlight its superiority in terms of classification accuracy when compared to existing stateoftheart methods. 4. Efficiency: The method\u2019s ability to improve classification performance while employing smaller models is particularly notable suggesting significant implications for computational efficiency and resource utilization.  Weaknesses 1. Complexity of Implementation: The paper introduces a complex approach combining maximum bipartite matching with the Fisher Information matrix. This complexity could limit its accessibility or implementation in various practical scenarios. 2. Breadth of Application: While the paper demonstrates effectiveness across several benchmark datasets theres a limited exploration of its applicability outside the specific domain of fewshot learning or in realworld tasks beyond classification. 3. Comparison to State of the Art: Although comparisons are made with stateoftheart methods the paper could benefit from a more detailed analysis or direct juxtaposition of their approach against others including discussions on scenarios where it may not exhibit superior performance. 4. Robustness and Generalization: There is limited discussion on the robustness of the approach against diverse or adversarial datasets. More exploration into these aspects could strengthen the paper\u2019s claims.  Questions and Feedback to the Author 1. Implementation Complexity: Can the authors provide insights or suggestions on simplifying the implementation of their approach to make it more accessible for practical applications 2. Applicability Beyond Classification: Are there potential applications of the proposed TAS outside the realm of FSL classification tasks If so how might the approach be adapted for such scenarios 3. Comparison Clarity: Could the authors expand on the comparative analysis particularly highlighting scenarios where the proposed method might not perform as well as existing techniques 4. Robustness and Generalization: Could future works explore the robustness of the methodology against datasets with inherent noise or adversarial data points Understanding how TAS handles such challenges could be invaluable.  Review Summary This paper introduces a highly innovative approach to tackling the limited data problem in fewshot learning through the development of an asymmetric task affinity score (TAS). The proposed method which is theoretically sound and empirically validated shows clear advantages over existing stateoftheart FSL methods particularly in terms of classification accuracy and model efficiency. While the approach is commendable for its novelty and proven efficacy certain aspects such as its implementation complexity and limited insights into robustness and generalization could be addressed in more detail. Moreover expanding the comparative analysis and exploring the method\u2019s applicability beyond classification tasks could further validate and possibly broaden the scope of this research. Overall the contributions of this paper are substantial offering promising directions for future research in fewshot learning and similar domains where data limitations are prevalent. With further development and elucidation on some of the outlined concerns this work has the potential to significantly impact the field.", "xiXOrugVHs": " Peer Review  Summary of the Paper The paper introduces a novel approach for text summarization by proposing a hierarchical latent structure framework that integrates both bottomup and topdown inference for inferring token representations in documents. This model architecture is designed to address two major challenges in summarization tasks  capturing longrange dependency while maintaining efficiency in handling large documents. The bottomup inference employs local selfattention to infer token representations efficiently minimizing quadratic complexity issues common with traditional full attention transformers. The topdown correction further allows for the incorporation of longrange contextual information into token representations. The framework is evaluated across various summarization datasets showing competitive or superior performance against existing stateoftheart models particularly in long document summarization tasks. Additionally it significantly outperforms a GPT3based model in summarizing an entire book with far fewer parameters and less training data highlighting its efficiency and scalability.  Strengths and Weaknesses Strengths: 1. Innovative Approach: The integration of both bottomup and topdown inference mechanisms for summarization tasks is novel addressing the efficiency and longrange dependency capture issues in a unique way. 2. Wide Applicability: The frameworks effectiveness across diverse document types and lengths including challenging datasets demonstrates its versatility. 3. Efficiency: The model achieves significant reductions in parameter count and training data requirements particularly highlighted by the comparison with a GPT3based model showing potential for scalable and resourceefficient applications. Weaknesses and Questions: 1. Generalization to Other Languages: The paper focuses on English datasets. How well does the proposed framework generalize to other languages especially those with different syntactic or semantic structures 2. TopLevel Unit Representation: The choice of fixedlength segments for the toplevel units aims at simplicity but might not always align with the logical structure of a document (such as paragraphs or chapters). Could adaptive segmentation that reflects document structure lead to further improvements 3. Interpretability: While the framework shows strong performance the interpretability of bottomup and topdown inferred representations could provide additional insights. How interpretable are the models intermediate representations in comparison to those from traditional transformers Feedback to the Author: The proposed framework marks a significant advancement in text summarization tasks primarily through its efficient handling of long documents and its novel integration of hierarchical inference mechanisms. However exploring its application to nonEnglish languages and improving its adaptability to the inherent logical structures of documents could enhance its versatility and efficacy further. Additionally providing an analysis of the models interpretability in terms of its inferred representations could bolster understanding and trust in its summarization capabilities.  Review Summary The paper presents an innovative and effective approach to text summarization particularly excelling in the summarization of long documents through a novel hierarchical inference framework that strategically combines bottomup and topdown processes. This model not only addresses significant challenges in existing summarization models such as the efficient capture of longrange dependencies and scalability but also demonstrates its prowess across a wide array of summarization tasks achieving stateoftheart performance in many cases. Despite its strengths there are opportunities for further exploration particularly in generalizability across languages adaptive segmentation aligning with document structure and the interpretability of its latent representations. Overall the proposed framework signifies a noteworthy contribution to the field of text summarization suggesting a promising avenue for future research and application. The innovative approach combined with demonstrated efficiency and effectiveness across diverse datasets supports a strong recommendation for publication contingent upon consideration of the identified potential enhancements.", "j97zf-nLhC": "Review for \"Exploring the Influence of Network Architectures on ZeroShot Coordination\" 1. Summary of the paper This paper explores the significant yet challenging realm of zeroshot coordination (ZSC) in the context of multiagent reinforcement learning (MARL). Specifically it delves into how the semantic relationships between the features of actions and observations can be leveraged for coordination without prior experience or established conventions. Through a novel DecPOMDP environment named the \"hintguess\" game the paper evaluates different neural network architectures including feedforward recurrent and attention mechanisms for their effectiveness in achieving ZSC. It finds that attentionbased architectures which process both the action\u2019s and the observations featurized representations jointly exhibit a better inductive bias towards utilizing semantic relationships for coordination resulting in high crossplay performance and the generation of humancompatible policies. The paper also investigates human coordination mutual exclusivity and implicature through the designed game scenarios with findings indicating a strong parallel between the agents\u2019 behaviors in certain conditions and human intuitive responses. The engagement with humanbased experiments further highlights the potential of the attentionbased model in understanding and replicating humanlike coordination strategies. 2. Strong and Weak Points Questions and Feedback to the author Strong Points:  Innovative Approach: The paper\u2019s exploration of attention mechanisms within DecPOMDPs for understanding semantic relationships in ZSC settings represents a novel and meaningful contribution to the field of MARL.  Comprehensive Evaluation: The introduction of the \"hintguess\" game as a diagnostic tool and the subsequent thorough analysis across various neural network architectures provide insightful evaluations of their capacity for ZSC.  HumanCompatible Coordination: The findings that attentionbased models can generate policies compatible with human strategies are particularly compelling offering promise for more intuitive humanagent interactions. Weak Points:  Limited Scope of Tested Environments: While the \"hintguess\" game offers a novel environment extending the evaluation to other more complex DecPOMDPs could enhance the papers conclusions.  Assumption of Feature Representations: The work assumes that actions and observations can be readily decomposed into semantic features which might not always be feasible or straightforward especially in highdimensional or less structured environments.  Scalability Concerns: The paper does not address the scalability of the proposed attention mechanisms especially regarding their computational demands which could limit their applicability in more complex or largerscale tasks. Questions and Feedback:  How do the proposed attentionbased architectures handle scenarios with highdimensional or continuous action and observation spaces where feature decomposition might not be clearly defined  Could the authors elaborate on any scalability challenges when implementing attention mechanisms in more complex environments and propose potential solutions or optimizations  It would be beneficial if future research directions could include strategies for autonomously identifying and leveraging semantic features within actionobservation pairs without prior specification. 3. Review Summary The paper presents a wellexecuted study on the role of network architectures particularly attention mechanisms in facilitating zeroshot coordination in MARL environments. Its innovative approach robust evaluation methodology and promising results regarding humancompatible policies considerably advance the understanding of how semantic relationships can be harnessed for coordination without preestablished conventions. However the work has room for improvement in terms of environment diversity assumptions about feature representations and addressing scalability. The recommendation for the paper would lean towards acceptance given its significant contributions but it is advised that the authors consider expanding their evaluation to diverse and complex settings clarify the scalability and computational demands of their approach and contemplate methods for autonomous feature identification and utilization in future work. This would not only enhance the robustness of their findings but also broaden the applicability and impact of their research within the MARL community and beyond.", "tyrJsbKAe6": " Review of \"Constrained Pessimistic Policy Optimization (CPPO) for Modelbased Offline RL with Partial Data Coverage\"  1) Summarization of the Paper This paper introduces an innovative approach named Constrained Pessimistic Policy Optimization (CPPO) in the realm of modelbased Offline Reinforcement Learning (RL). The crux of CPPO lies in its ability to learn a highquality policy from an offline dataset without requiring a full coverage assumption on the offline data distribution. Unlike classical methods that fall under the umbrella of FittedQ Iteration (FQI) and demand full coverage realizability in a function class and Bellman completeness CPPO requires only realizability and partial coverage thus significantly relaxing previous assumptions. The algorithm is designed to compete against any policy covered by the offline data including historydependent policies leveraging a general function class and enforcing pessimism through a constraint on the model class. The paper also showcases the versatility of CPPO across various specialized Markov Decision Processes (MDPs) including lowrank MDPs and factored MDPs by adapting the density ratiobased concentrability coefficient to more refined problemspecific quantities.  2) Strong and Weak Points Questions and Feedback Strong Points: a) Relaxation of Assumptions: The major strength of CPPO is its relaxation of the full coverage and Bellman completeness assumptions which are quite stringent and unrealistic for largescale MDPs. b) Generality and Flexibility: CPPO demonstrates high versatility by providing a framework applicable to various types of MDPs reflecting its practicality in realworld scenarios where the underlying MDP structures can significantly differ. c) Competence Against a Wide Range of Policies: The ability to learn policies that can compete not just with the optimal policy but with any highquality comparator policy covered by the data including nonMarkovian policies is particularly noteworthy. Weak Points: a) Computational Efficiency Concerns: The paper does not clearly address the computational challenges involved in the minimax optimization especially when dealing with large function classes or in nontabular settings. b) Empirical Evidence: The paper heavily focuses on theoretical aspects and the lack of empirical evidence or case studies limits the understanding of CPPOs realworld efficacy and scalability. c) Complexity and Accessibility: The high technicality of the content makes it less accessible to practitioners or researchers outside the theoretical RL community. Questions and Feedback: 1. Can the authors elucidate on the computational complexity of CPPO especially in comparison to existing modelbased and modelfree offline RL algorithms 2. How does CPPOs performance vary across different scales of data coverage It would be beneficial to understand its efficacy in extremely sparse coverage scenarios. 3. It would be insightful if the authors could provide empirical validations of CPPO on benchmark datasets or realworld applications to complement the theoretical findings.  3) Review Summary and Recommendation This paper advances the modelbased offline RL domain by introducing CPPO a novel algorithm that significantly relaxes previously held stringent assumptions enhancing flexibility and applicability across a range of MDPs. Its theoretical foundation is robust offering a fresh perspective on policy optimization with partial data coverage. However the paper falls short in discussing computational aspects and lacks empirical evidence to support its claims fully. Given the strong theoretical contributions and the novelty of the approach I recommend the acceptance of this paper. Nonetheless I strongly advise the authors to address the computational efficiency of CPPO and to consider including empirical results or simulations in future work to establish the practical viability of their approach further. These additions would immensely strengthen the paper making the findings more relatable and convincing to both theoretical and applied audiences in the field of RL.", "siCt4xZn5Ve": "Review of \"Understanding the Implicit Bias of Stochastic Gradient Descent (SGD) in the Overparametrized Regime\" 1. Summary: This paper proposes a novel analytical framework for characterizing the implicit bias introduced by Stochastic Gradient Descent (SGD) in overparametrized models a fundamental aspect in understanding why deep learning models generalize well in practice. Specifically the authors extend previous local analyses to a more global understanding utilizing a stochastic differential equation (SDE) framework inspired by Katzenberger (1991). This approach results in a comprehensive characterization of the SGDs implicit bias over \\xce\\x98(\u03b7\u22122) steps a significant advancement over the previous local perspective that was valid for only \\xce\\xb7\u22121.6 steps. The paper demonstrates the utility of this framework through its application to label noise SGD showcasing its ability to escape the kernel regime and achieve a minimax optimal sample complexity for learning sparse overparametrized linear models. Notably this work answers open questions posed by Blanc et al. (2020) and presents a novel technical approach that could be extended to analyze SGDs implicit bias in other settings. 2. Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses a significant problem in the deep learning community providing a more profound understanding of the role of noise in SGD and its impact on generalization.  The utilization of the SDE framework and the adaptation of Katzenberger\u2019s approach for a global analysis of SGDs implicit bias is both novel and mathematically rigorous.  The theoretical results are not only significant but also practical as demonstrated by the application to label noise SGD and the derivation of a minimax optimal sample complexity for learning sparse models.  The presented framework has the potential to be a valuable tool for future research in understanding the algorithmic regularization effects of stochastic optimization methods beyond SGD. Weak Points:  While the paper is strong in its theoretical contributions it lacks empirical validation of the theoretical findings. Although the theoretical analysis is thorough empirical evidence demonstrating the practical implications of these findings would strengthen the paper.  The paper is highly technical and dense making it potentially inaccessible to a broader audience. Simplifying the presentation or providing more intuitive explanations for the key concepts and results could make the work more accessible to nonexperts.  The application of the framework is primarily focused on label noise SGD for overparametrized linear models. Exploring its implications on more complex models or other types of noise could provide a more comprehensive understanding of its practical benefits. Questions and Feedback:  Could the authors discuss the potential limitations of applying their framework to analyzing SGDs implicit bias in nonlinear or less overparametrized models  How sensitive are the key results to the assumptions made in the paper notably the assumptions regarding the loss landscape and the noise characteristics  Providing a brief discussion on how the proposed framework could be adapted to analyze other stochastic optimization techniques such as Adam or SGD with momentum might be beneficial. 3. Review Summary: The paper presents a valuable contribution to the field of deep learning by offering a novel framework for understanding the implicit bias of SGD in overparametrized models. The theoretical advancements made by adapting and extending Katzenbergers approach are both significant and innovative potentially paving the way for new avenues of research in the area. While the paper could benefit from empirical validation of its theoretical results and a broader application scope its insights into the regularization effect of label noise SGD and the derivation of minimax optimal sample complexity are noteworthy achievements. Given the strong theoretical foundation and the potential for future exploration I recommend acceptance provided that the authors can address the concern regarding empirical validation and explore the applicability of their framework to a broader set of models and optimization techniques.", "zHZ1mvMUMW8": " Peer Review for \"Succinct Compression for Efficient Neural Network Inference\"  1) Summary of the Paper This paper introduces a novel framework termed \u2018Succinct Compression\u2019 for compressing Deep Neural Networks (DNNs) in a way that boosts inference performance while maintaining nearoptimal compression levels. The proposed method is a threestage process that leverages Succinct Data Structures to perform fast queries directly on compressed data without the need for decompression. This is achieved through the development of semistructured formulations (Elementwise and Blockwise Runtime Accessible Sequence  RAS) of DNN models that are conducive to compression via Succinct Data Structures. The paper demonstrates that the application of this methodology results in significant space savings and performance speedup during inference runtime over Huffman Coding a stateoftheart compression technique. Moreover the compatibility of Succinct Compression with other compression strategies such as Pruning and Quantization is also shown indicating its flexibility and broad applicability. Experimental results on networks like AlexNet and VGG16 showcasing these benefits are provided to substantiate the claims.  2) Strengths and Weaknesses Strong Points:  Innovation: The use of Succinct Data Structures for compressing and querying DNN models is a novel application that addresses the critical issue of runtime performance and space consumption.  Comprehensive Approach: The paper provides a holistic view of DNN compression from the initial structuring of models for optimal compression to the execution of inference directly on the compressed models.  Clear Synergy with Existing Techniques: The paper articulates well how Succinct Compression works in tandem with Pruning and Quantization providing a path for integrating this approach into existing DNN optimization workflows.  Solid Experimental Validation: The experimental setup and comparison against Huffman Coding provide convincing evidence of the superiority of Succinct Compression in terms of speed and space efficiency. Weak Points:  Lack of Theoretical Analysis: The paper does not provide a theoretical foundation or analysis for why Succinct Data Structures are optimal for this particular application or how they compare to other potential structures in this space.  Limited Scope of Experimentation: The experiments are conducted on a relatively small set of models and do not explore the full extent of DNN architectures (e.g. recurrent neural networks transformers).  No Discussion on Computational Overheads: There is no explicit discussion on the computational overhead or potential latency introduced by the Succinct Compression process itself especially in the context of realtime applications. Questions and Feedback to the Author: 1. Could the authors provide a theoretical justification or a more detailed rationale behind the choice of Succinct Data Structures for this application 2. It would be beneficial to expand the experimental validation to include a wider variety of DNN architectures and more realworld datasets to solidify the papers claims. 3. Can the authors comment on the computational overhead introduced by the compression and decompression processes in Succinct Compression especially in comparison to Huffman Coding  3) Review Summary The paper presents a compelling approach to compressing DNNs in a manner that enhances inference efficiency while maintaining compression levels comparable to the bestinclass methods. The introduction of Succinct Compression as a method to enable fast direct queries on compressed data without requiring decompression represents a significant leap forward in the domain of DNN optimization. This is particularly relevant as the deployment of DNNs expands into resourceconstrained environments. However to strengthen the paper further it would be beneficial for the authors to provide a deeper theoretical background to their approach expand the range of experiments to cover more DNN architectures and clarify the computational costs associated with their method. Addressing these points would make a strong paper even more robust and compelling. Overall the contributions of this paper are significant introducing a promising direction for research and application in DNN compression. The authors work opens the door to more efficient deployment of complex neural network models across a variety of devices from highend servers to lowcost embedded systems.", "kDF4Owotj5j": "Review of \"Overcoming the Overthinking Problem in Deep Thinking Systems for Enhanced Logical Extrapolation\" 1) Summary This paper addresses the limitations of classical machine learning systems which typically do not perform well when tasked with solving problems that were not included in their training distribution. Specifically it tackles the challenge observed in existing recurrent neural networks (RNNs) of deteriorating performance with increased iterations\u2014a phenomenon referred to as \"overthinking.\" The authors propose a novel architecture Deep Thinking (DT) networks designed to overcome this issue by maintaining an explicit memory of the problem instance (termed \"recall\") and employing an incremental training routine. This approach aims to prevent the network from developing behaviors tied to a specific number of iterations and instead encourages the learning of scalable repeatable processes. The paper presents experimental results demonstrating the efficacy of these innovations across several benchmark tasks highlighting the ability of the proposed model to generalize to more challenging problem instances far beyond the scope of the training data. 2) Strong and Weak Points Questions and Feedback Strong Points:  The concept of leveraging an explicit recall mechanism and an incremental training methodology to combat the overthinking problem in recurrent networks is innovative and welljustified.  Comprehensive experimental setup with benchmark tasks for evaluating logical extrapolation offering clear evidence of the proposed models superiority over existing approaches.  The detailed exploration of the overthinking phenomenon including manipulations of feature maps and inputs adds depth to the analysis and helps underline the robustness and unique characteristics of the DTRecall model. Weak Points:  While the results are promising there might be a lack of discussion on the scalability of the proposed approach especially regarding the computational and memory requirements for both training and inference.  The paper could benefit from a broader discussion on the limitations of the proposed model particularly in scenarios where recalling the entire problem instance might not be feasible or when the problem structure significantly differs from the benchmark tasks.  It would be interesting to see a comparison with other stateoftheart techniques in machine learning that tackle the issue of generalization to outofdistribution tasks to better contextualize the performance and innovation level of the proposed approach. Questions and Feedback:  Have the authors considered the impact of problem size and complexity on the recall mechanism\u2019s efficiency and the overall model performance For instance would the performance gains hold for significantly larger problem instances  It would be valuable to elaborate on potential realworld applications of the proposed model. Which specific domains or types of problems could benefit most from this approach  A deeper exploration of how the model reacts to corrupted or ambiguous input data could strengthen the papers argument about the robustness and practical applicability of the DTRecall system. 3) Review Summary The paper presents a significant contribution to the field of machine learning by addressing the wellknown issue of logical extrapolation in neural networks. The innovative use of a recall mechanism combined with an incremental training strategy represents a novel approach to mitigating the overthinking problem in recurrent networks. The experimental results convincingly demonstrate the models ability to generalize to more complex problems beyond the training distribution showcasing its potential for a wide range of applications. However further discussion on the computational and practical scalability of the model as well as a comparative analysis with other generalization techniques would enhance the papers impact. Moreover exploring the models limitations and its performance under different conditions or problem structures could provide a more rounded view of its applicability. In conclusion this work significantly advances our understanding of modeling logical extrapolation in neural networks and opens up new avenues for research into overcoming the challenges associated with deep thinking systems.", "zeGpMIt6Pfq": "Summary of the Paper This paper introduces BioLCNet a novel spiking neural network (SNN) architecture designed to offer a more biologically plausible approach to image classification. Unlike traditional convolutional neural networks (CNNs) that rely on weight sharing and backpropagation BioLCNet utilizes locally connected layers spiking neurons and is trained using spiketimingdependent plasticity (STDP) and its rewardmodulated variant (RSTDP) along with reinforcement learning mechanisms. These features aim to mimic more closely the operational principles and efficiency of the biological visual system. The paper evaluates BioLCNet\u2019s performance on the MNIST dataset comparing it with existing SNN approaches and discussing its biological plausibility. Strong Points 1. Innovative Approach: The introduction of BioLCNet represents a significant innovation by incorporating spiking neurons and local connections which are inherently more akin to biological neural processing into the networks architecture. This approach also promises energy efficiency and a reduction in the need for labelled data advantages attributed to SNNs. 2. Biological Plausibility: The emphasis on biological plausibility including the use of RSTDP learning rules and a locally connected layer structure makes this study particularly valuable. It contributes meaningfully to the ongoing research on braininspired computing and neuromorphic engineering. 3. Detailed Experimental Evaluation: The comprehensive experimental section including image classification tasks on the MNIST dataset and a classical conditioning experiment illustrates the networks capabilities and the effectiveness of the proposed learning mechanisms. The inclusion of various hyperparameter configurations and comparisons with other models enriches the analysis. Weak Points 1. Comparative Analysis: While the paper mentions the novel architectures performance relative to other SNN approaches a more direct comparison with stateoftheart CNNs and other recent advances in nonspiking architectures could strengthen the case for BioLCNet\u2019s practical applications and advantages. 2. Scalability and Generalization: The study primarily focuses on the MNIST dataset which while a standard benchmark does not fully test the models scalability to more complex image classification tasks or larger datasets. Additional experiments on datasets like CIFAR10 or ImageNet could provide invaluable insights into the network\u2019s generalization capabilities. 3. Biological Evidence for Learning Rules: While the paper argues for the biological plausibility of its learning mechanisms specifically RSTDP the discussion would benefit from a more detailed justification possibly including direct comparisons with known biological learning processes. Questions and Feedback to the Author 1. Could the authors elaborate on the networks performance and efficiency when scaled to larger and more complex datasets beyond MNIST 2. The paper could benefit from a discussion on the limitations or challenges in implementing BioLCNet for realworld applications. What are the current technological or theoretical hurdles 3. How does the networks computational efficiency compare to traditional CNNs especially when implemented on neuromorphic hardware 4. Could the authors provide more insight into the biological basis of RSTDP and how closely it mirrors real neural learning processes Review Summary The paper presents BioLCNet an innovative spiking neural network architecture that emphasizes biological plausibility through the use of local connections spiking neurons and rewardmodulated learning rules. This approach distinguishes itself from traditional CNNs by potentially offering more energyefficient and biologically realistic models for image classification tasks. BioLCNet\u2019s design grounded in principles derived from the biological visual system is a commendable step towards bridging the gap between artificial and biological neural networks. The comprehensive experimental evaluation including a comparative analysis with other SNN architectures and the application to the MNIST dataset underscores the models potential. However to solidify the argument for the practical application and advantages of BioLCNet further comparisons with stateoftheart CNNs and experiments on more complex datasets would be beneficial. Additionally the discussion on the biological basis of the learning rules employed could be expanded to further validate the model\u2019s biological plausibility. In conclusion BioLCNet represents an intriguing advancement in the field of neuromorphic computing and SNN research. With further development and exploration it has the potential to contribute significantly to the development of more efficient biologically inspired computing models.", "nxcABL7jbQh": " Review of \"A Novel Approach for Boundary Detection Using Vector Transform Representation\"  Summary This paper presents an innovative approach to boundary detection that addresses the challenges of class imbalance and the production of thick boundaries prevalent in traditional methods. By reinterpreting boundaries as 1D surfaces and introducing a onetoone vector transform function the authors provide a method that enables accurate boundary prediction without suffering from class imbalance. This vector field representation points at the closest boundary surface from any given point allowing for the estimation of boundary direction and richer contextual information. This method which is hyperparameter free at training and involves a fixed hyperparameter at inference is theoretically supported and evaluated across several datasets showing superior performance compared to existing methods.  Strong Points 1. Innovative Representation: The method of representing boundaries as a vector field directed towards the closest boundary is novel and effectively overcomes the class imbalance issue which is a significant improvement over traditional boundary detection approaches. 2. Theoretical Justification: The paper provides a solid theoretical foundation for the proposed vector transform representation including thorough discussions and proofs of its properties. This depth adds credibility and depth to the research. 3. Performance: The experimental results convincingly demonstrate that the proposed method outperforms existing losses and representations on several large datasets. The inclusion of detailed comparisons and metrics is beneficial for understanding the methods strengths. 4. Application Versatility: Highlighting the methods utility in differentiable straight line detection and superpixel grouping presents the approachs versatility beyond simple boundary detection which is promising for future applications and research.  Weak Points 1. Dataset Limitations: The authors mention the limitations of the BSDS500 dataset however it would be beneficial to discuss ways to overcome or mitigate these limitations or provide insight into how the method might perform on other datasets with similar issues. 2. Hyperparameter Selection: While the method is described as virtually hyperparameter free at training the paper could benefit from a more detailed discussion on the fixed stable hyperparameter at inference including its selection process and impact on different types of datasets. 3. Comparison Breadth: The paper conducts comprehensive evaluations against certain losses and methods but it may also be worthwhile to compare this approach against more recent or less conventional boundary detection strategies to further contextualize its performance. 4. Implementation Details: Providing more insights into the implementation such as computational costs training times and potential scalability issues would offer a more rounded understanding of the method\u2019s practical applications.  Questions and Feedback to the Author 1. Could the authors elaborate on strategies for selecting the fixed hyperparameter at inference and how it might vary across different dataset characteristics 2. How does the proposed method perform in terms of computational efficiency and scalability especially when handling large highresolution datasets 3. Are there particular types of visual features or boundary characteristics where the proposed method shows notably higher or lower performance  Review Summary The paper introduces a unique and theoretically sound approach to boundary detection that addresses key issues in the field notably class imbalance and boundary thickness. The novel vector field representation not only achieves superior performance across several benchmarks but also opens up new possibilities for research and applications in boundaryrelated tasks. Despite its strengths the paper could benefit from a deeper discussion on potential limitations broader comparisons and implementation specifics to enhance its contributions further. Overall this work represents a significant step forward in boundary detection technology with promising implications for both academic research and practical applications in computer vision.  Recommendation Recommend Acceptance with minor revisions. The paper presents a significant contribution to the field of boundary detection supported by strong experimental results and theoretical grounding. Addressing the provided feedback could further strengthen the paper.", "oxxUMeFwEHd": " Draft Peer Review for \"TOGL: Incorporating Global Topological Information into Graph Neural Networks\"  1. Summary of the Paper The paper introduces TOGL (Topological Graph Layer) a novel graph neural network layer that incorporates global topological information into existing GNN architectures through persistent homology a key concept in topological data analysis (TDA). TOGL is designed to address the limitation of traditional GNNs in capturing significant substructures like cycles within graphs. The layer is differentiable making it suitable for endtoend learning and is demonstrated to increase the expressivity of GNNs beyond the capabilities of the Weisfeiler\u2013Lehman (WL) isomorphism test. Experimental results on both synthetic and realworld datasets show that GNN models augmented with TOGL achieve superior performance on graph and node classification tasks compared to standard GNNs and other baseline models. This improved performance is attributed to TOGLs ability to leverage topological features at multiple scales.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The introduction of a layer leveraging topological data analysis within GNN architectures is novel and addresses a clear gap in the existing methods for graph representation learning.  Rigorous Theoretical Basis: The paper provides a solid theoretical foundation for TOGL proving that it enhances GNN expressivity by incorporating the ability to process multiscale topological information.  Empirical Validation: The extensive experimental evaluation across multiple datasets and tasks alongside comparisons with several baselines convincingly demonstrates the utility and performance gains of the proposed method.  Differentiability and Integration: Ensuring that TOGL is differentiable and can be easily integrated into existing GNNs without altering the original model architecture significantly is a considerable advantage. Weak Points:  Complexity and Scalability: The paper briefly mentions the computational complexity of TOGL but lacks a detailed discussion on scalability especially concerning very large graphs and higherdimensional topological features.  Limited to LowDimensional Homology: The current implementation of TOGL focuses on 0dimensional and 1dimensional topological features. The exploration of higherdimensional homological features could potentially further enhance model capabilities.  Overfitting Concerns: Although touched upon there is insufficient detail on how TOGL might affect overfitting particularly on smaller datasets or those with less pronounced topological features. Questions and Feedback:  Could the authors provide more insights into the scalability of TOGL especially for very large graphs or when integrated into deep GNN architectures  Have the authors considered extending TOGL to capture higherdimensional topological features If yes what are the potential challenges and complexities involved  The paper mentions the potential issue of overfitting with smaller datasets. Could the authors elaborate on strategies to mitigate this possibly including regularization techniques specifically designed for TOGL  Feedback: Including case studies or indepth analyses on how TOGL manages to capture and leverage topological information that traditional GNN layers overlook could further strengthen the papers contributions.  3. Review Summary The paper presents a compelling and innovative approach to enhancing GNNs through the incorporation of global topological information using persistent homology. The theoretical underpinnings are strong and the empirical evidence robustly supports the claimed benefits of the proposed TOGL layer. Despite its notable contributions the paper could be improved by addressing concerns around scalability the exploration of higherdimensional homological features and the potential for overfitting especially on smaller or less topologically rich datasets. Given the significance of the problem addressed the novelty of the solution and the quality of the experimental validation I recommend the publication of this paper. Further work as suggested could make the approach even more valuable for a wider range of applications and datasets in graph machine learning.", "vSix3HPYKSU": " Review of the Paper: \"A Fully Neural Message Passing Solver for Partial Differential Equations\"  1. Summary This paper introduces a fully neural messagepassingbased solver for partial differential equations (PDEs) aiming to address the complex challenge of numerically solving PDEs  a cornerstone problem across various scientific disciplines. The proposed solver uniquely positions itself by embracing the flexibility of neural architectures to satisfy the broad spectrum of structural requirements typical of PDE problems such as varying resolutions domain geometries and boundary conditions. The core innovation lies in its neural message passing framework that encapsulates classical solving schemes like finite differences and volumes enabling endtoend learning while potentially outperforming existing numerical solvers in terms of speed and accuracy especially in lowresolution scenarios. To address the stability issues commonly associated with training autoregressive models for solving PDEs the authors propose two novel techniques: temporal bundling and the pushforward trick both contributing to significantly improved stability and accuracy. The efficacy of the approach is demonstrated through various experiments including fluidlike flow problems in both one and two dimensions showcasing superior performance over traditional and some neural solvers across different domain settings.  2. Strong and Weak Points Questions and Feedback  Strong Points: 1. Innovative Approach: Combining the classical concept of message passing with neural networks to solve PDEs is novel and theoretically enticing especially the claim that such an approach can encapsulate classical methods like finite differences and volumes. 2. Addressing Stability: The introduction of the pushforward trick and temporal bundling as methods to improve stability in training autoregressive models is commendable. These techniques are not only innovative but address a critical challenge in the neural solving of PDEs. 3. Comprehensive Evaluation: The paper presents a thorough evaluation of the proposed method across various scenarios including different domain geometries boundary conditions and equation types. The comparisons made with stateoftheart numerical and neural solvers are comprehensive and wellstructured.  Weak Points: 1. Generalization Capacity: While the paper claims superior performance in generalization across multiple PDEs the evidence primarily focuses on fluidlike flow problems. It remains unclear how well this approach would generalize to a broader class of PDEs including those with more complex or nonfluid dynamics. 2. Computational Efficiency: Despite the claimed speed and accuracy advantages over traditional solvers in lowresolution regimes the paper lacks a detailed analysis of the computational costs including training times and resource requirements making it hard to evaluate the practical usability of the proposed solver. 3. Lack of Theoretical Underpinning: The paper could benefit from a deeper theoretical discussion on why neural message passing can effectively approximate classical solving schemes and under what conditions the proposed stability techniques are guaranteed to work.  Questions and Feedback: 1. Generalization Across Different PDE Classes: Can the authors provide insights or additional results on the applicability of the proposed neural message passing solver to a wider array of PDEs especially those significantly different from fluid dynamics 2. Computational Cost Analysis: It would be beneficial if the authors included a detailed analysis of the computational costs associated with their method especially in comparison to both traditional and existing neural solvers. 3. Theoretical Insights: Any additional theoretical insights or proofs on why and how the neural message passing approach encapsulates classical solving schemes would greatly enhance the papers contribution to the field.  3. Review Summary The paper presents a novel and potentially groundbreaking approach to solving partial differential equations using a fully neural messagepassing framework. The introduction of stabilityenhancing techniques like the pushforward trick and temporal bundling is particularly praiseworthy and addresses a key challenge in the field. While the experiments demonstrate impressive results especially in lowresolution regimes and fluidlike flow problems several areas require further elaboration. These include the generalization capability of the solver across a broader class of PDEs a detailed analysis of computational costs and a deeper theoretical understanding of the methodologys underpinnings. Addressing these points would significantly strengthen the papers contributions and its appeal to both the numerical methods and machine learning communities. The paper is wellwritten and presented making a substantial contribution to the field pending improvements on the mentioned fronts.", "scSheedMzl": " Review of \"Locally INvariant EXplanations (LINEX) for Interpretable Machine Learning\"  Summary The paper introduces a novel explanation method named Locally INvariant EXplanations (LINEX) designed to generate interpretable stable and unidirectional explanations for blackbox models across different data modalities including tabular image and text data. By leveraging the notion of Invariant Risk Minimization (IRM) and employing a game theoretic approach LINEX aims to provide robust featurebased explanations against neighborhood sampling discrepancies a known limitation of existing methods like LIME. The paper theoretically argues for LINEXs capability to prioritize stable feature attributions and eliminate features with rapidly changing impacts enhancing recourse. Empirical evaluations demonstrate LINEXs superiority in providing highquality stable and unidirectional explanations compared to LIME and other recent methods emphasizing its efficiency and practicability without requiring additional costly data manipulation or access to causality graphs.  Strong Points 1. Innovative Approach:  The paper successfully introduces a new angle to local explanations by integrating invariant risk minimization principles with a game theoretic formulation. This originality in approach is commendable and contributes to advancing the field of interpretable machine learning. 2. Comprehensive Evaluation:  Extensive empirical testing across various data modalities (tabular image text) showcasing the methods generalizability and effectiveness in producing better explanations in terms of fidelity stability and unidirectionality. 3. Theoretical Foundation:  It thoroughly discusses the theoretical underpinnings that motivate the design of LINEX providing a solid foundation for the proposed methodology and its expected advantages over existing methods. 4. Addressing Practical Concerns:  LINEX addresses practical aspects of explainability emphasizing its efficiency and modelagnostic nature which makes it suitable for realworld application scenarios where computational resources or specific model knowledge might be limited.  Weak Points 1. Assumption Clarity:  While the paper makes certain assumptions for theoretical analysis (e.g. independence of feature values across samples in local environments) it could further discuss the practical implications and limitations these assumptions might introduce. 2. Comparison with WhiteBox Models:  The paper primarily focuses on comparing LINEX with other modelagnostic post hoc explainability approaches. Including comparisons with whitebox models could provide additional insight into the absolute fidelity and stability of explanations LINEX can offer. 3. Impact on Decision Making:  While the paper highlights LINEXs potential for better recourse through stable and unidirectional explanations empirical evidence or case studies illustrating how these explanations impact enduser decisionmaking or model debugging would enrich the analysis. 4. Sensitivity to Hyperparameters:  Though the algorithms performance is evaluated under varied settings a deeper analysis of the sensitivity to its hyperparameters (e.g. the number of environments l\\xe2\\x88\\x9e threshold) and guidance on their selection would enhance practical utility.  Questions and Feedback  Q1: Could the authors elaborate on how LINEX\u2019s performance and explanation quality might vary with increasingly complex blackbox models (deep neural networks with complex architectures)  Feedback: While the paper effectively communicates the advantages of LINEX incorporating visualizations of explanations for cases where LINEX provides significant improvement or faces challenges could facilitate better understanding and trust in LINEX\u2019s output.  Review Summary The paper makes significant contributions to the interpretable machine learning domain by proposing LINEX a novel explanation method focusing on stability and unidirectionality. The clear theoretical foundation combined with extensive empirical evidence across diverse data modalities successfully establishes LINEX as a robust efficient and practically applicable approach for explaining blackbox models. However addressing the outlined weak points and questions could further strengthen the paper particularly regarding practical implications of underlying assumptions comparative analysis and detailed impact assessment on decisionmaking. Overall the innovation and results presented justify a strong recommendation for this paper with minor revisions suggested to improve its clarity and applicability further.", "kO-wQWwqnO": " Peer Review for the Manuscript Entitled \"An Unpaired GANBased Approach for Enhanced Brightness and Texture in Low Light Images\"  1. Summary of the Paper This paper introduces L2BGAN a novel Generative Adversarial Network (GAN)based model for enhancing lowlight images into brighter versions without relying on paired supervision. Specifically it leverages geometric and illumination consistency along with contextual loss to maintain the fidelity of the enhanced images. Through a triad of discriminators that focus on color texture and edge information separately L2BGAN aims to achieve highquality image enhancement. The authors benchmark their model against various datasets including realtime driving scenarios and image understanding tasks using DarkFace and ExDark datasets demonstrating competitive performance compared to stateoftheart methods.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novel Approach: The paper presents an innovative approach by combining geometric and illumination consistencies with contextual loss which is a significant contribution to the field of unsupervised image enhancement.  Extensive Experiments: A comprehensive set of experiments on benchmark datasets and relevant applications (e.g. face detection in lowlight conditions) underscores the models practical utility and competitive performance.  Detailed Comparison: The comparison with stateoftheart methods is thorough encompassing both visual and quantitative analyses which greatly aids in understanding the relative positioning of L2BGAN. Weak Points:  Lack of Novelty in Components: While the ensemble approach is novel the individual components (GANS cycle consistency context loss) are wellexplored concepts in the literature. A deeper exploration of how these existing concepts are uniquely adapted could strengthen the paper.  Performance on Backlit Images: The paper notes that L2BGAN does not perform well on backlit images. This limitation could significantly impact its applicability in realworld scenarios and further investigation or a proposed solution would be beneficial.  Limited Discussion on Failure Cases: Beyond backlit images there is scant discussion on scenarios where L2BGAN might not perform as expected or on the challenges and limitations inherent to the approach. Questions and Feedback: 1. Adaptability to Other Domains: How adaptable is L2BGAN to other domains beyond driving and image understanding tasks especially where geometric consistency might be differently characterized 2. Impact of Diverse Lighting Conditions: Could the authors elaborate on how L2BGAN handles images with diverse or mixed lighting conditions particularly where parts of an image might be welllit while others are in shadow 3. Potential Improvements: Given the identified limitations such as performance on backlit images are there ongoing efforts or future directions aimed at addressing these challenges Insights into potential model adjustments or additional training strategies would be enlightening.  3. Review Summary The paper introduces L2BGAN a compelling and innovative model for unsupervised translation of lowlight images to brighter counterparts leveraging the synergy of geometric and illumination consistency with contextual loss. The extensive experimental analysis underscores its competitive edge and practical utility particularly in enhancing image understanding tasks in challenging lighting conditions. However the innovation could be overshadowed by the reliance on familiar components without a detailed discussion on their unique adaptations for this specific task. Furthermore the models limitations such as its underperformance on backlit images necessitate further investigation and could impact its broader applicability. To strengthen the paper the authors should consider a more thorough exploration of the models adaptability across diverse domains and lighting conditions. Additionally addressing the noted limitations through future work could bolster the papers contribution to the field. In summary L2BGAN represents a significant step forward in unsupervised image enhancement. With further refinement and exploration it has the potential to set new standards for image quality improvement in lowlight conditions.", "xRK8xgFuiu": " Peer Review for the Paper: \"Causal Discovery via Cholesky Factorization (CDCF)\"  1. Summary The paper presents an innovative algorithm named CDCF for discovering causal relationships between variables represented in a Directed Acyclic Graph (DAG) structure from observed data. This task which is essential across various domains such as biology economics and machine learning is recognized as computationally challenging due to its combinatorial nature. The proposed CDCF algorithm leverages the Cholesky factorization of the covariance or precision matrix to recover the DAG structure boasting improved time and sample complexities (O(pn  p)) over previous methods. Theoretical underpinings are provided backed by assumptions that allow the CDCF algorithm to recover the DAG structure with high probability. Empirical evaluations on synthetic proteins and knowledgebased datasets demonstrate CDCFs superior performance in terms of speed and accuracy compared to existing approaches.  2. Strengths and Weaknesses Strengths:  Innovative approach: The usage of Cholesky factorization for DAG recovery is novel and represents a significant departure from existing methods that often rely on scorebased searches or continuous optimization programs.  Improved Complexity: The proposed CDCF algorithm achieves better time and sample complexities compared to previous algorithms which is a substantial advancement in the field of causal discovery.  Empirical Validation: The authors have conducted extensive experiments on various datasets showcasing the superior performance of CDCF in terms of efficiency and accuracy. This rigorous empirical validation adds credibility to the proposed method. Weaknesses:  Assumption specificity: The theoretical guarantees provided for the CDCF algorithm rest on specific assumptions about the observed datas structure and the noise characteristics. The practical implications of these assumptions on the algorithms applicability to realworld datasets are not thoroughly discussed.  Comparison with stateoftheart methods: While the paper presents comparisons with several baseline methods a more detailed comparison with the very latest stateoftheart methodologies including their strengths and weaknesses relative to CDCF would give a clearer position of CDCF in the current research landscape.  Implementation details: Although the paper mentions that the code will be opensourced upon acceptance preliminary details about the implementation such as the choice of programming language dependencies and computational requirements would enhance the reproducibility and accessibility of the proposed method. Questions and Feedback:  Can the authors elaborate on the impact of the assumptions made for theoretical guarantees on the applicability of the CDCF algorithm to diverse realworld datasets  How does CDCF perform in scenarios where the linearity assumption of the SEM may not hold Is there scope for extending the CDCF to nonlinear SEMs  Are there specific types of datasets or domains where CDCFs performance might be particularly strong or weak Insights into the algorithms limitations or bestuse cases would be valuable.  3. Review Summary The proposed CDCF algorithm represents a significant advancement in the field of causal discovery offering a novel factorizationbased approach that outperforms existing methods in both speed and accuracy. The innovative use of Cholesky factorization rigorous theoretical analysis and comprehensive empirical validation across diverse datasets underscore the papers contributions. However the applicability of CDCF under different assumptions detailed comparisons with the latest methodologies and concrete implementation details could be further elaborated to enhance the papers impact and accessibility. Overall considering the novel approach theoretical contributions and empirical results presented I recommend the acceptance of this paper. Simultaneously addressing the highlighted weaknesses and incorporating the suggested feedback could significantly strengthen the papers contribution to the field of causal discovery.", "gfUPGPMxB7E": " Review of \"Conservative Unsupervised Data Sharing for MultiTask Offline Reinforcement Learning\"  1. Summary This paper tackles the challenge of multitask offline reinforcement learning (RL) where the goal is to learn effective control policies across multiple tasks without active interaction with the environment. The proposed method termed Conservative Unsupervised Data Sharing (CUDS) allows for data to be shared across different tasks without requiring manual reward labeling for each transition especially under a binaryreward setting. The method is grounded on the observation that simply utilizing data from other tasks with na\u0131\\xcc\\x88ve constant reward labels (e.g. zero for all shared data) can substantially improve learning outcomes over relying solely on singletask data. CUDS refines this approach by selectively deciding which transitions to include from other tasks based on a valueaware strategy thereby enhancing performance without introducing any additional models or classifiers. The paper evaluates CUDS across several multitask offline RL scenarios including robotic manipulation from visual inputs and antmaze navigation showing that it outperforms existing methods that either learn the reward function explicitly or use other means of propagating reward labels.  2. Strong and Weak Points Questions and Feedback Strong Points: a) Innovative Approach: The paper presents a novel and practically significant approach to handling the challenge of reward relabeling in multitask offline RL. By leveraging existing data with constant reward labels and refining this strategy through conservative unsupervised data sharing the method is both innovative and applicable in varied contexts. b) Empirical Validation: The comprehensive evaluation across multiple domains and comparisons with baseline and oracle methods demonstrate the efficacy of CUDS in improving performance in multitask offline RL scenarios. c) Theoretical Insights: The paper provides theoretical insights into why the proposed method works including discussion of how UDS and CUDS manage conservativeness in policy learning which underpins the practical effectiveness observed in empirical results. Weak Points: a) Binary Reward Assumption: The studys reliance on binary rewards is a significant limitation. While applicable in some scenarios this assumption restricts the method\u2019s generalizability to more complex environments where rewards are continuous or multifaceted. b) Lack of Comparative Analysis on Data Efficiency: While the methods performance is compared to several baselines there is minimal analysis on how dataefficient CUDS is relative to these baselines which is crucial for offline RL scenarios where data collection is expensive or risky. Questions and Feedback: 1. Regarding Binary Rewards: How do the authors envision the extension of CUDS to handle environments with continuous or more complex reward structures Would this necessitate significant modifications to the conservative datasharing strategy 2. Data Efficiency: Can the authors provide insights or additional results on the data efficiency of CUDS compared to the baseline methods Understanding how much data is necessary for CUDS to outperform other techniques would be valuable. 3. Impact of Incorrect Data Sharing: The paper assumes the benefits of sharing data across tasks but does not deeply explore the potential negative impacts of incorrectly shared data. How sensitive is CUDS to such scenarios and are there mechanisms in place to mitigate these effects 4. Applicability to Realworld Scenarios: Given the focus on binaryreward settings can the authors discuss any practical scenarios or domains where they envisage CUDS being particularly beneficial Additionally insights into challenges faced when deploying CUDS in realworld settings would be enriching.  3. Review Summary The paper offers a significant contribution to the domain of multitask offline RL by introducing an efficient and practical method for data sharing among tasks without the need for manual reward relabeling. The theoretical and empirical strengths of the paper underscore the novelty and applicability of CUDS. However the binaryreward assumption and the lack of detailed analysis on data efficiency and sensitivity to incorrect data sharing are notable limitations. Addressing these issues could further solidify the papers contributions and broaden the applicability of CUDS to a wider range of realworld scenarios. Overall the study presents a compelling argument for the utility of conservative unsupervised data sharing in multitask offline RL paving the way for future work to explore its extension to more complex reward structures and its realworld deployment challenges.", "vyn49BUAkoD": "Summarize the Paper This paper addresses the biasvariance tradeoff in machine learning particularly in contexts where data scarcity is an issue such as in metamodeling active learning and Bayesian optimization. It highlights the inefficiencies in querying and data labeling that arise from neglecting this tradeoff. Focusing on Gaussian Processes (GPs) used in metamodeling with active learning the authors propose optimizing the GPs hyperparameters (length scale and noiseterm) to regulate the biasvariance tradeoff. They introduce two novel acquisition functions designed to approximate the joint posterior of the GP hyperparameters: a Bayesian Variant of QuerybyCommittee (BQBC) and a Query by Mixture of Gaussian Processes (QBMGP). These functions aim to improve querying efficiency by incorporating biasvariance considerations. Empirical evaluations across seven simulators demonstrate the superiority of QBMGP in offering robustness and accuracy with fewer iterations compared to standard acquisition functions. Strong and Weak Points of the Paper Strengths: 1. Innovativeness: The proposal of BQBC and QBMGP acquisition functions based on the GPs joint posterior approximation is innovative and addresses a critical gap in the literature regarding the biasvariance tradeoff in datascarce environments. 2. Empirical Evaluation: The comprehensive empirical evaluation across multiple simulators provides strong evidence of the effectiveness and robustness of the proposed methods compared to benchmark functions. 3. Practical Application: The methodology directly translates to practical improvements in active learning and Bayesian optimization potentially reducing computational and financial costs associated with data labeling. Weaknesses 1. Generalization of Results: The papers conclusions are based on the specific set of simulators tested. More diverse testing including realworld datasets would strengthen the argument for the proposed acquisition functions generalizability. 2. Theoretical Justification: While the empirical results are compelling the paper could benefit from a deeper theoretical analysis on why the proposed methods outperform existing ones potentially providing insights into situations where they might not be as effective. 3. Complexity and Implementability: The proposed methods especially QBMGP appear computationally demanding. More discussion on their computational complexities along with guidelines for efficient implementation would be beneficial for practitioners. Questions and Feedback to the Author 1. Can the authors provide additional insights or theoretical backing on why QBMGP outperforms BQBC in certain simulators but not others 2. How do the proposed acquisition functions perform in the presence of highly noisy or highdimensional data 3. Could the authors discuss the computational cost and scalability of the proposed methods especially when contrasting with simpler acquisition functions Review Summary The paper introduces two novel acquisition functions BQBC and QBMGP for improving the efficiency of querying and reducing unnecessary data labeling in metamodeling with active learning particularly under data scarcity. By approximating the joint posterior of GP hyperparameters and incorporating this approximation into the design of acquisition functions the authors present a methodologically sound approach to mitigating the biasvariance tradeoff in such environments. The empirical evaluation is thorough and provides convincing evidence of the proposed methods superiority over existing benchmark functions. However the paper would benefit from a more comprehensive theoretical exploration to deepen the understanding of why these functions perform as reported. Questions around the generalizability of results and the computational complexities involved with implementing these methods remain and should be addressed to strengthen the papers contribution further. Overall the proposed acquisition functions represent a significant advancement in leveraging Gaussian Processes for metamodeling and active learning. If the identified limitations are addressed this work could have substantial implications for both the research community and practical application in fields dealing with sparse datasets and the necessity of efficient data querying.", "gaYko_Y2_l": " Review of \"Weakly Supervised Graph Clustering and its Applications\"  1. Summary This paper introduces a novel graph learning problem called Weakly Supervised Graph Clustering (WSGC) aiming to improve the clustering of nodes in graphs with the help of graphlevel labels and additional hierarchical structures among the graphs. The authors propose a model Gaussian Mixture Graph Convolutional Network (GMGCN) which employs Gaussian Mixture Layer (GML) for node representation learning and hierarchical Graph Attention Networks (GAT) to incorporate graphlevel structures and labels into the clustering process. The framework includes a unique consensus loss function to leverage the graphlevel labels effectively. Extensive experiments on both synthetic and realworld datasets demonstrate the superior performance of the proposed method over traditional graph clustering methods and multiple instance learning approaches showcasing the utility of weak supervision in graph clustering tasks.  2. Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Problem Formulation: The introduction of Weakly Supervised Graph Clustering (WSGC) addresses a gap in the literature of graph learning opening new research avenues. 2. Innovative Approach: The proposed GMGCN model combining Gaussian mixture models with graph convolutional networks and leveraging a new consensus loss function presents a robust method for WSGC. 3. Comprehensive Experiments: Extensive empirical evaluation across synthetic and realworld datasets provides a strong validation of the approach. The inclusion of various settings and scenarios of WSGC adds to the depth of evaluation. Weak Points: 1. A lack of theoretical grounding: The paper would benefit from a deeper theoretical analysis of why and how the integration of GMM with GCN and the specific design choices within GMGCN contribute to its success. 2. Limited Discussion on Scalability: There is inadequate discussion around the scalability of the proposed method especially concerning larger and more complex graph structures. 3. Comparison with Recent Baselines: The paper mentions comparisons with existing methods but lacks a discussion on how GMGCN performs against the latest stateoftheart weakly supervised learning or graph neural network models. Questions: 1. How does the choice of hyperparameters (e.g. number of GAT layers dimensions of representations) affect the performance of GMGCN and were any hyperparameter sensitivity analyses conducted 2. Can the authors provide insight into how the model behaves with incorrectly labeled graphs considering the weakly supervised setting may introduce label noise Feedback:  It would be enlightening to see a more detailed discussion on the limitations of the proposed method particularly regarding its application to dynamic graphs where node and edge attributes may change over time.  An exploration of how the model can be adapted or extended to handle multilabel classification where a node can belong to multiple categories simultaneously could broaden the applicability of this research.  Adding case studies or more detailed analyses of realworld applications (beyond the synthetic and PHEME datasets) could further illustrate the practical relevance and impact of WSGC.  3. Review Summary (Recommendation) The paper presents a significant advancement in graph learning by defining and addressing Weakly Supervised Graph Clustering. The proposed GMGCN model is innovative and demonstrates notable improvement over traditional methods through comprehensive experiments. However the paper would benefit from a more thorough theoretical analysis discussions on scalability model robustness against label noise and detailed comparisons with recent baselines. Considering the novelty of the problem effectiveness of the proposed solution and potential for future work I recommend this paper for publication with minor revisions addressing the discussed weaknesses and considering the feedback provided. This work significantly contributes to the community by opening new research directions in graph learning particularly in weakly supervised scenarios.", "yztpblfGkZ-": " Summarize the Paper The authors introduce a novel graph convolution operator named BankGCN designed to enhance the capabilities of message passing graph convolutional networks (MPGCNs) and to address the limitations of spectral graph convolutional networks (GCNs) in handling diverse spectral properties of graph data. The primary innovation of BankGCN lies in its use of an adaptive filter bank that enables the decomposition of multichannel graph signals into subspaces where each subspace is processed by a distinct filter. This approach allows for the adaptive representation of graph data across different frequency ranges thus overcoming the focus on lowfrequency information that characterizes many existing GCN models. Through extensive experiments on various graph datasets the authors demonstrate that BankGCN achieves superior performance in both node and graph classification tasks compared to stateoftheart methods by effectively utilizing diverse spectral characteristics of graph data.  Strong and Weak Points of the Paper Strong Points: 1. Innovative Approach: The introduction of an adaptive filter bank that processes signals in different subspaces is a novel and significant contribution. This allows BankGCN to capture a broad spectrum of spectral characteristics more effectively than existing GCN architectures. 2. Comprehensive Evaluation: The extensive experimental evaluation across multiple graph datasets provides strong evidence of BankGCNs performance advantages in handling diverse spectral properties of graph data. 3. Theoretical Justification: The paper includes a theoretical analysis that supports the effectiveness of BankGCN in reducing the redundancy often present in spectral GCN approaches while maintaining the ability to capture diverse spectral information. Weak Points: 1. Complexity and Implementation Details: The paper does not provide sufficient details on the computational complexity of BankGCN compared to other models nor does it offer clear guidance on the implementation which might hinder reproducibility and practical application. 2. Lack of Discussion on Limitations: While the paper presents extensive experimental results there is a lack of discussion on the potential limitations or scenarios where BankGCN may not perform as expected. A more balanced view including potential drawbacks and challenges would provide a clearer picture of BankGCNs applicability. 3. Generalization to Other Graph Types: The paper focuses on undirected graphs thus its applicability to directed graphs or graphs with edge attributes remains unclear. An exploration of BankGCNs performance on these other graph types would be valuable. Questions and Feedback to the Author: 1. Computational Complexity: Could the authors elaborate on the computational complexity of BankGCN especially in comparison with other stateoftheart models 2. Implementation Details: Can the authors provide more details on the practical implementation of BankGCN including hyperparameter tuning and optimization strategies used during the experiments 3. Applicability to Directed Graphs: How well does BankGCN adapt to other graph types such as directed graphs or graphs with edge attributes Are there modifications or considerations needed to apply BankGCN in these contexts  Review Summary The paper introduces a promising novel graph convolution operator BankGCN that addresses a significant limitation in current graph convolutional network models \u2013 their focus on lowfrequency information and inability to adaptively capture the diverse spectral characteristics of graph data. BankGCNs use of an adaptive filter bank represents a significant step forward in representation learning on graphs enabling more nuanced and powerful analyses of networked data. The comprehensive experimental evaluations demonstrate the efficacy of BankGCN across a range of datasets supporting the authors claims regarding its performance superiority. The theoretical analysis further adds to the papers strengths providing a solid foundation for the proposed architecture. However the paper could be improved by addressing its weak points particularly by providing more details on the computational aspects of BankGCN and discussing its limitations and applicability to different types of graphs in more depth. Addressing these aspects would greatly enhance the papers contribution and provide clearer guidance for researchers and practitioners looking to apply BankGCN in their work.", "gc8zLQWf2k": "Summarize the paper: The paper presents a comprehensive study on the role of memorization in adversarially trained deep neural networks (DNNs) particularly focusing on the performance differences encountered when memorizing atypical samples. Initial observations confirmed that while memorizing atypical samples boosts clean accuracy on these samples it detrimentally affects adversarial robustness and even hampers performance on typical samples. To address these challenges the authors propose a novel approach named Benign Adversarial Training (BAT) designed to mitigate the adverse effects of memorizing harmful atypical samples while preserving or enhancing the benefits of memorizing benign atypical samples. Experimental validation of BAT across benchmark datasets CIFAR100 and Tiny ImageNet demonstrates its superiority over various baseline adversarial training methods achieving a better tradeoff between clean accuracy and adversarial robustness. Strong points: 1. Novel Contribution: The paper introduces an innovative algorithm BAT targeting a critical issue in adversarial training regarding the memorization of atypical samples. This approach is both novel and valuable for improving DNNs\u2019 performance and robustness. 2. Comprehensive Evaluation: The experimental design is rigorous encompassing a wide range of benchmarks adversarial attacks and comparison with stateoftheart methods. Such thorough evaluation strongly supports the claims regarding BAT\u2019s effectiveness. 3. Theoretical and Empirical Insights: The work excellently combines theoretical analysis with empirical results providing a deeper understanding of the memorization effect in adversarial training and its implications on model performance. Weak points: 1. Generalization to Other Domains: While the paper demonstrates the effectiveness of BAT on image classification tasks its unclear how well these findings generalize to other domains or tasks such as natural language processing or regression problems. 2. Analysis of Computational Overhead: The paper lacks a discussion on the computational overhead introduced by BAT especially considering the reweighting mechanism and the additional discrimination loss. This information is crucial for practical adoption. 3. Exploration of Alternative Approaches: The study focuses primarily on BAT without much consideration for alternative methods that might also address the identified issues with memorizing atypical samples in adversarial training potentially limiting the scope of the findings. Questions and Feedback to the author: 1. Generalization and Applicability: Can the authors comment on the applicability of BAT to other domains beyond image classification Are there specific characteristics of image data that make BAT particularly effective in this context 2. Computational Overhead: How does the inclusion of BAT affect the training time and computational resources required compared to standard adversarial training methods Quantitative benchmarks would be helpful. 3. Alternatives and Comparisons: Were other strategies considered or tested to address the challenges posed by the memorization of atypical samples during adversarial training How does BAT compare to these alternative approaches if any Review Summary: The paper addresses an important aspect of adversarial training focusing on the memorization of atypical samples and its impact on DNNs performance. The proposed Benign Adversarial Training (BAT) method is wellmotivated and demonstrates superior performance over existing methods through extensive experimental validation. Notably the combination of empirical and theoretical insights enhances the papers contributions. However discussions on the methods generalizability to other domains its computational efficiency and comparisons with alternative approaches could further strengthen the paper. Overall amid the identified weaknesses the novel contributions and positive experimental results provide compelling reasons to consider the paper favorably. The findings significantly contribute to the existing body of knowledge offering a practical solution to a pertinent challenge in adversarial training while opening avenues for future research in making DNNs more robust and performant.", "qTHBE7E9iej": " Peer Review  1. Summary of the Paper This paper aims to address the inefficiencies in the conventional reinforcement learning (RL) paradigm where robots or agents must learn from scratch for every new environment or task. The proposed solution Hierarchical Latent Mixtures of Skills (HeLMS) introduces a novel approach for learning a hierarchy of motor skills from offline data accentuating both discrete and continuous latent variables. This structure allows for capturing highlevel abstract behaviors and variance in execution providing a more dataefficient means of transferring and finetuning learned skills to new tasks objects and modalities (e.g. from statebased to visionbased policies). The paper showcases the effectiveness of HeLMS through a series of experiments in object manipulation tasks demonstrating significant improvements in sample efficiency and performance in various transfer scenarios compared to other skill and imitationbased methods. The paper also examines the conditions under which the hierarchical skillset is most advantageous particularly highlighting its utility in sparsereward settings by facilitating directed exploration.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper introduces an original method of learning reusable motor skills using a hierarchical mixture latent variable model that combines both discrete and continuous behaviors. This innovative approach addresses a significant gap in the current literature by incorporating the advantages of both behavior types.  Comprehensive Evaluation: The experiments conducted are thorough and welldesigned covering multiple scenarios including transfer to new tasks objects and modalities. This extensive evaluation effectively demonstrates the versatility and efficiency of the proposed method.  Utility in SparseReward Settings: The papers analysis of HeLMS in sparsereward environments is particularly compelling showcasing how the method aids in directed exploration and thereby increases the likelihood of encountering rewards. This is a critical feature for the practical application of RL in realworld scenarios. Weak Points:  Complexity and Interpretability: The hierarchical structure while powerful introduces complexity that might make the model difficult to interpret and debug. The paper could benefit from a deeper discussion on the interpretability of the learned skills and any strategies employed to mitigate these concerns.  Dependence on the Quality of Offline Data: The efficacy of HeLMS relies heavily on the quality and diversity of the offline dataset. While this is partly addressed through the experiments a more detailed exploration of how the method performs under varying quality of data and strategies to compensate for less ideal datasets would be beneficial.  Limited Exploration of FineTuning Flexibility: The conclusion mentions the future work possibility of finetuning lowerlevel behaviors for greater flexibility. More insight into the current limitations of HeLMS in adapting mid and lowlevel skills to new tasks would provide better clarity on the scope of this issue. Questions and Feedback: 1. Interpretability of Learned Skills: Can the authors elaborate on any steps taken or future plans to enhance the interpretability of the hierarchical skills learned by HeLMS This could be crucial for debugging and improving the system in practice. 2. Performance in DataConstrained Scenarios: How does HeLMS perform when trained on significantly smaller or less diverse datasets Are there any mechanisms within HeLMS to alleviate performance drops in such scenarios 3. Extension to MultiAgent Systems: Is the proposed method applicable to multiagent settings and if so what modifications if any would be necessary to adapt HeLMS for such environments  3. Review Summary The paper presents a uniquely compelling approach to learning reusable motor skills through a hierarchical latent variable model HeLMS which stands out for its innovative combination of discrete and continuous behaviors. The comprehensive experiments and the methods proficiency in sparsereward settings underscore HeLMSs potential for significantly enhancing data efficiency and performance in robot manipulation tasks. Nonetheless the methods complexity and heavy reliance on the quality of offline data suggest areas for future improvement particularly regarding model interpretability and adaptability in less ideal conditions. The potential for extending HeLMS to broader applications such as multiagent systems also presents an exciting avenue for exploration. Overall despite some limitations the proposed method marks a notable advancement in the quest for more versatile and efficient reinforcement learning strategies warranting strong consideration for publication with minor revisions to address the pointed feedback.", "xm6YD62D1Ub": "Summarize the paper: The paper introduces VICReg (VarianceInvarianceCovariance Regularization) a novel selfsupervised method for learning image representations that aims to avoid the collapse problem in joint embedding architectures. This collapse occurs when encoders produce identical and noninformative vectors significantly hindering the learning process. The approach is grounded on maintaining the variance of each embedding dimension above a threshold and decorrelating each pair of variables within the embeddings. Unlike other methods that rely on weight sharing batch normalization featurewise normalization or stop gradient among others VICReg requires none making it more flexible and broadly applicable. It achieves competitive results across several downstream tasks without needing a large batch size memory banks or contrastive samples. The method is tested on selfsupervised pretraining using a ResNet50 backbone on the ImageNet dataset and evaluated on various image recognition tasks demonstrating stability and performance on par with stateoftheart methods.  List strong and weak points of the paper Question and Feedback to the author: Strong Points: 1. Novelty: VICReg introduces a new perspective to avoid collapse in selfsupervised learning without relying on previously established techniques such as contrastive learning large batch sizes or memory banks. 2. Simplicity and Generality: The method doesn\u2019t require complex architectures or weight sharing between branches making it broadly applicable including to multimodal signals. 3. Performance: The paper presents comprehensive experiments showing that VICReg achieves competitive results on a wide range of downstream tasks including image classification detection instance segmentation and retrieval tasks. 4. Stability: The variance regularization term introduced is shown to stabilize training and improve performance which is a significant contribution considering the instability issues often associated with selfsupervised learning methods. Weak Points: 1. Lack of Theoretical Analysis: The paper could benefit from a deeper theoretical analysis explaining why the variance and covariance regularization techniques are effective in preventing collapse. 2. Comparison and Analysis: While the paper compares VICReg with other stateoftheart methods it could provide a more detailed analysis of how and why VICReg outperforms (or does not outperform) specific methods in certain tasks. 3. Implementation Details: Some aspects of the implementation are only briefly mentioned or referred to appendices. A more detailed description within the main text could enhance the papers replicability. Questions and Feedback: 1. Could the authors provide more insight into the choice of hyperparameters (\u03bb \u00b5 and \u03bd) and their sensitivity analysis How do these parameters affect the balance between the invariance variance and covariance terms in practice 2. Would the authors elaborate on the potential applications of VICReg in multimodal learning settings beyond the mentioned audio and video signals 3. Regarding the performance improvement reported when incorporating variance preservation into other selfsupervised learning frameworks can the authors discuss any limitations or specific conditions under which these improvements are observed 4. Feedback: It would be valuable to see future work exploring the integration of VICReg with other selfsupervised paradigms or its adaptation to other nonimage data types for broader application scope.  Review Summary: VICReg represents a significant advancement in the field of selfsupervised learning by providing a simple yet effective method to prevent collapse without the complicated requirements of other methods. Its flexibility stability and impressive performance across a range of tasks make it a valuable contribution. However the exploration could be deepened with more theoretical insights into why the method works and expanded comparisons and analyses against other stateoftheart methods. Addressing these points could strengthen the paper further. Overall I recommend this paper for publication believing it will inspire future research in selfsupervised learning and its applications to multimodal and complex data.", "fuYtttFI-By": " Review  Summary The paper introduces an innovative approach to 3D snapshot microscopy leveraging deep learning for enhanced volumetric imaging particularly suitable for capturing rapid neural activities in biological samples such as larval zebrafish. The authors embark on optimizing both the optical encoder characterized by a phase mask on a spatial light modulator (SLM) with a significant number of tunable parameters and a deep learningbased decoder. They propose using Fourier convolutional neural networks that enable global decoding capabilities a necessity for effectively interpreting nonlocally encoded 3D information in 2D snapshots. This methodology surpasses the limitations of existing architectures like UNet by facilitating endtoend optimization of highly nonlocal optical encoders. Through simulations and application to lensless photography the paper demonstrates the superiority of the proposed FourierNets in deciphering complex optical encodings and reconstructing highfidelity images from 3D volumes underscoring their potential applicability to diverse imaging tasks beyond the discussed neuroscience context.  Strong Points 1. Novel Architecture: The introduction of FourierNets tailored for decoding nonlocally encoded images presents a significant advancement in computational microscopy. The ability to handle global information efficiently addresses a crucial bottleneck in 3D snapshot microscopy. 2. EndtoEnd Optimization: The methodology enabling direct optimization of both the imaging (optical encoder) and decoding (reconstruction network) components marks a robust approach to tailoring microscopy techniques for specific tasks or sample types potentially facilitating broader applicability and adaptability. 3. Comprehensive Validation: Extensive simulation studies alongside empirical evaluations on a lensless photography dataset demonstrate the effectiveness and generalizability of the proposed methods. The inclusion of comparative analyses with current stateoftheart approaches like UNet strengthens the claim of superiority. 4. Contribution to Understanding Optical Encoding: The work provides valuable insights into the relationship between sample structure and the optimality of engineered optical encoders contributing to a deeper understanding of how different samples interact with encoding schemes.  Weak Points 1. Experimental Validation on Real Microscopy Data: While simulation results and applications to lensless photography are compelling experimental validation using real microscopy data especially in the context of 3D snapshot microscopy is missing. This would provide a more comprehensive assessment of the techniques practical utility. 2. Complexity and Computational Resources: The approach requiring parallel multiGPU setups and extensive parameter optimization might limit its accessibility and implementation in labs with limited computational resources. 3. Fixed Input Size Requirement: The necessity for a fixed input size due to the global nature of Fourier convolutions might restrict the flexibility of the method in handling diverse datasets without preprocessing steps to standardize input dimensions.  Questions and Feedback to the Authors 1. Scalability and Practical Implementability: Could the authors comment on the scalability of the proposed method concerning larger datasets or higher resolution imaging demands considering the computational intensiveness and the requirement for multiGPU setups 2. Robustness to Noise: How does the model perform under different levels of noise which is common in real microscopy settings Is there a need for additional noise robustness features in the architecture 3. Transferability Across Different Sample Types: The paper discusses optimizations specific to sample types. How transferable are learned encodings and decodings across diverse biological samples  Review Summary The paper represents a significant stride forward in computational microscopy introducing an efficient architecture for endtoend optimization of both optical encoders and decoders in 3D snapshot microscopy. The proposed FourierNet demonstrates remarkable capabilities in globally decoding nonlocally encoded images overcoming the limitations of existing deep learning architectures like UNet. This work not only enhances volumetric imaging for biological applications but also provides valuable insights into designing samplespecific optical encoders. Despite the advantages the methods computational demands and the lack of experimental validation using genuine microscopy data mark areas for future improvement. Nonetheless the potential of this approach to revolutionize highspeed highresolution imaging across various scientific disciplines is undeniable. I recommend this paper for publication suggesting that addressing the highlighted practical implementation concerns could further elevate its impact.", "wNsNT56zDkG": " Review of \"Analyzing the Generalization of Adversarial Training Through Adversarial Rademacher Complexity\"  Summary The paper presents a comprehensive study on the generalization gap of adversarial training in deep neural networks (DNNs) emphasizing the role of adversarial Rademacher complexity. The authors extend the analysis of adversarial Rademacher complexity beyond the existing scope which was previously limited to twolayer neural networks to deep neural networks. They propose a method to overcome analytical challenges related to the depth of the network and the interaction with adversarial perturbations. The study introduces upper bounds for the adversarial Rademacher complexity which includes the product of weight norms demonstrating that these norms are larger in adversarially trained models compared to standard training. The findings are supported by experiments showing the relationship between the generalization gap and adversarial Rademacher complexity suggesting that larger weight norms in adversarial training contribute to its poor generalization performance.  Strong Points 1. Novelty of Approach: The method to extend adversarial Rademacher complexity analysis to deep neural networks is innovative. The paper addresses a significant gap in the literature by developing upper bounds for adversarial Rademacher complexity in multilayered networks which was a previously uncharted area. 2. Theoretical Contribution: Providing a theoretical framework to understand the generalization gap in adversarial training is valuable. The incorporation of weight norms into the analysis and the empirical validation of these theoretical findings contribute significantly to our understanding of adversarial training\u2019s limitations. 3. Experimental Validation: The experiments conducted elucidate the relationship between adversarial Rademacher complexity and the generalization gap. These experiments not only validate the theoretical predictions but also offer insights into the practical implications of the findings.  Weak Points 1. Assumptions and Generalizability: While the paper introduces a novel method to deal with deep neural networks the assumptions made for the sake of analysis (such as the bound on Frobenius norms) and their implications on the generalizability of the results to more complex or different types of networks need further discussion. 2. Empirical Evaluation Scope: The experimental validation focuses on VGG networks trained on CIFAR10. Additional experiments including other architectures and datasets could enhance the robustness of the findings and their applicability to a broader range of scenarios. 3. Discussion on Practical Applications: The implications of the findings on the practical application of adversarial training including potential strategies for improving generalization are somewhat limited. The paper would benefit from a more detailed discussion on how these insights can be translated into actionable strategies for practitioners.  Questions and Feedback to the Author 1. Clarification on Assumptions: Can the authors elaborate on the assumptions made for the analysis particularly the assumption on weight norms and its implications for the applicability of the results to various network architectures 2. Broader Impact and Applications: How do the authors envision the application of their findings in improving the robustness and generalization of adversarially trained models in realworld settings 3. Extension to Other Architectures: Are there plans to extend the analysis to other architectures or to incorporate other types of adversarial perturbations Insights into potential challenges and opportunities in these directions would be valuable.  Review Summary The paper makes a significant theoretical contribution to understanding the generalization gap in adversarial training offering a novel analysis of adversarial Rademacher complexity in deep neural networks. The findings highlight the role of weight norms in the poor generalization of adversarially trained models supported by welldesigned experiments. However the paper would benefit from a broader discussion on the generalizability of the results additional empirical validation across a wider range of architectures and datasets and deeper insights into the practical implications of the findings. Overall the paper represents a valuable step forward in the understanding of adversarial training and sets a strong foundation for future work in this area.", "viWF5cyz6i": "Summarize the Paper The paper presents an algorithm for computing an approximate truncated singular value decomposition (TSVD) based on a userprescribed accuracy tolerance aiming to address the high computational cost associated with conventional TSVD methods especially when applied to large matrices and only a few principal components are needed. It builds on the existing body of research noting that many fast PCA algorithms require prespecified component numbers which may not always be practical or desirable. By introducing a novel approach that relies on a tolerance parameter for determining the number of principal components the proposed algorithm achieves computational efficiency without sacrificing accuracy. This is particularly relevant for matrices where a few principal components explain a significant amount of variance. The paper details the algorithms methodology which includes a blocked version of FlipFlop Spectrum Revealing QR and a criterion for determining when to stop factorization. Experimental results with various types of matrices demonstrate the algorithms ability to accurately and efficiently approximate TSVDs making it a promising tool for approximate PCA in diverse applications. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The paper introduces a novel approach to computing TSVD that is both efficient and accurate leveraging a tolerance parameter instead of a fixed rank. This is highly relevant for practical applications where the number of principal components is not known a priori. 2. Comprehensive Experiments: The experiments cover a range of matrix types demonstrating the algorithms versatility and effectiveness across different application areas. The detailed comparison with existing methods such as TSVD and randQB EI highlights the proposed algorithms advantages. 3. Theoretical Justification: The authors provide theoretical bounds for approximation error ensuring that the algorithm not only works well in practice but is also supported by solid mathematical foundations. Weak Points: 1. Complexity for Practitioners: The paper is heavily weighted towards mathematical detail which might make it less accessible to practitioners who are more interested in application than theory. A section discussing practical implementation considerations could improve its utility. 2. Comparison Depth: While the comparison with existing methods is valuable the paper could benefit from a deeper analysis of the tradeoffs involved such as computational resources required (time and memory) and impact on performance for varying matrix sizes and conditions. 3. Software Implementation: The absence of a publicly available software implementation or reference to an implementation limits the papers immediate impact. Providing such resources could greatly enhance its applicability in realworld scenarios. Questions and Feedback:  Could the authors elaborate on the impact of different tolerance values on the algorithms performance Some guidance on choosing \u03b5 and \u03b4 for different applications would be useful.  It would be interesting to understand how the algorithm performs in the presence of highly noisy data. Could the authors comment on or provide experiments in such scenarios  How does the algorithm scale with matrix size especially for very large matrices encountered in big data applications Some insights or additional experiments on scalability would enhance the papers relevance. Review Summary This paper makes a significant contribution to the field of dimensionality reduction in data science by proposing an efficient algorithm for computing an approximate truncated singular value decomposition (TSVD) based on a userprescribed tolerance. The methodology is wellfounded with theoretical justifications and the experimental results showcase its effectiveness across different types of matrices. However the paper could be improved by addressing its accessibility to practitioners expanding on the comparison with existing methods and providing or referring to an actual software implementation. The proposed algorithm stands out for its novel tolerancebased approach offering a practical solution to the often impractical requirement of prespecifying the number of components in fast PCA algorithms. Overall the paper presents a valuable advancement in efficient PCA computing with the potential for broad applicability in data compression machine learning training data preparation and beyond pending some enhancements to maximize its impact and accessibility.", "giBFoa-uS12": " Review of \"Leveraging Iterated Rationalizability for Collaborative MultiAgent Reinforcement Learning with InfoPG\"  Summary: The paper introduces InfoPG an innovative informationtheoretic policy gradient framework for cooperative multiagent reinforcement learning (MARL). It aims to enhance team cognition through iterated rationalizability and maximizing mutual information (MI) among agents without explicit regularization. By reformulating agent policies to depend on neighboring agents policies InfoPG encourages strategic decisionmaking and communication ultimately facilitating improved cooperative behaviors. The authors ground their approach in cognitive hierarchy theory extending the policy gradient (PG) objective to include actionconditional policies that allow for recursive reasoning about teammates actions. They showcase InfoPGs effectiveness across several cooperative domains demonstrating its superiority in terms of sample efficiency and cumulative rewards compared to existing methods.  Strengths: 1. Novel Approach: The paper introduces a unique blend of iterated rationalizability and MI maximization in a fully decentralized setting distinct from existing work that requires explicit regularization or assumes perfect rationality. 2. Theoretical Foundation: InfoPG is wellgrounded in cognitive hierarchy theory and the authors provide a detailed mathematical basis for how it maximizes MI implicitly and improves cooperative behavior among agents. 3. Empirical Validation: The authors conduct comprehensive experiments exhibiting that InfoPG outperforms stateoftheart baselines in various complex cooperative MARL tasks providing tangible evidence of its effectiveness.  Weak Points and Questions: 1. Computational Efficiency: While the paper discusses the superior performance of InfoPG it does not thoroughly address the computational costs. Given the iterated communication and actionconditional policy updates how does the computational complexity of InfoPG compare to the baselines 2. Scalability to Large Agent Networks: The effectiveness of InfoPG in small to mediumsized agent networks is clear. However the scalability to very large networks where direct communication between all neighboring agents might be impractical remains uncertain. Can the authors comment on InfoPGs performance and feasibility in these scenarios 3. Robustness to NonCooperative Agents: The experiments demonstrate InfoPGs robustness in scenarios like the Byzantine Generals Problem. However the behavior under environments with a mix of cooperative and competitive agents or agents following nonstandard reward paradigms is less clear. How does InfoPG adapt to such mixedmotive environments  Feedback:  The paper could benefit from a deeper exploration of how InfoPGs computational requirements scale with the number of agents and the complexity of the environment.  It would be insightful to see additional ablation studies that isolate the impact of actionconditional policy formulation and the iterated communication mechanism on the overall performance.  Considering the practical implications of deploying MARL systems in realworld situations elaborating on InfoPGs resilience to dynamic changes in the agent network (e.g. agents joining or leaving) might enrich the discussion.  Review Summary: This paper makes a significant contribution to the field of cooperative MARL by proposing InfoPG a novel framework that seamlessly integrates iterated rationalizability and MI maximization to foster improved cooperation and strategic interaction among agents. The approach is theoretically sound drawing from cognitive hierarchy theory and is empirically validated across various complex environments clearly demonstrating its advantages over current methods. Despite its strengths the paper would benefit from further discussion on computational efficiency scalability and robustness to diverse and dynamic agent behaviors. Nonetheless the innovative approach and promising results presented make InfoPG a noteworthy advancement in collaborative MARL research with potential implications for a wide range of applications.", "iGffRQ9jQpQ": " Review of \"SemiInterested Coalitional Learning for Enhanced SemiSupervised Learning\"  1. Summary The paper presents a novel framework for semisupervised learning (SSL) named SelfInterested Coalitional Learning (SCL). This framework introduces a cooperative yet competitively inclined game between a primary SSL task and an auxiliary task that discerns label observability. The objective is to alleviate the heavy dependency on labeled data and mitigate error accumulation commonly associated with pseudolabeling in SSL. The approach leverages a discriminator that determines label reliability thus facilitating the primary SSL task with additional discriminative information. By integrating this mechanism SCL aims to enhance the utilization of both labeled and unlabeled data potentially leading to improved learning outcomes. The effectiveness of SCL is validated across various tasks including image classification label propagation and data imputation with reported enhancements in performance and robustness over conventional and selftraining SSL methods.  2. Evaluation Strong Points:  Innovative Framework: The introduction of SCL as a novel learning paradigm offers a fresh perspective on handling semisupervised learning challenges particularly the overreliance on labeled data and the pitfalls of pseudolabeling.  Theoretical Foundation: The paper provides a sound theoretical basis for SCL substantiating its methodology with calculus of variations and demonstrating theoretical connections to loss reweighting on noisy labels.  Broad Applicability and Enhanced Performance: SCLs adaptability to various SSL tasks (classification regression) and its demonstrated ability to consistently outperform baseline SSL algorithms underscore its practical value and versatility.  Error Mitigation: By effectively addressing error accumulation through its unique learning strategy SCL contributes to more reliable learning from both labeled and unlabeled data. Weak Points:  Implementation Complexity: The paper while detailed in theoretical explanation might benefit from a more indepth discussion on the implementation challenges or considerations for applying SCL in realworld settings.  Limited Discussion on Limitations: While SCL shows promising results the paper could provide a more comprehensive analysis of its limitations including potential scenarios where its performance might be hindered or the computational overhead involved.  Comparative Analysis: Additional comparative analyses with more recent or diverse SSL algorithms might enrich the papers evaluation section offering readers a broader context of SCLs standing within the field. Questions and Feedback to the Author:  Could the authors elaborate on the practical challenges of implementing SCL particularly regarding computational complexity and scalability to large datasets  How does SCL perform in highly imbalanced datasets where labeled data are scarce for certain classes Is there a risk of the discriminator becoming biased toward more represented classes  The paper primarily focuses on image classification label propagation and data imputation tasks. Could the authors speculate on the applicability and potential adjustments needed for SCL to be effective in other domains such as natural language processing or timeseries analysis  3. Review Summary The paper introduces a promising framework SelfInterested Coalitional Learning (SCL) for improving semisupervised learning by addressing critical challenges like the heavy reliance on limited labeled data and error propagation from pseudolabeling. The theoretical foundations coupled with comprehensive evaluation results convincingly demonstrate SCLs potential to enhance SSL tasks performance and robustness. However a more detailed discussion on the practical challenges of implementing SCL and an expanded comparative analysis with a wider range of SSL algorithms could further strengthen the paper. Given its innovative approach and significant contributions to semisupervised learning I recommend this paper for publication with minor revisions addressing the discussed weaknesses and incorporating feedback questions.", "l5aSHXi8jG5": " Peer Review of the Paper on Transferability of Adversarial Samples Against Automatic Speech Recognition Systems  1) Summary The paper explores the concept of targeted transferability of adversarial samples especially focusing on why optimization attacks that have been successful in other domains fail against Automatic Speech Recognition (ASR) systems. The authors highlight the significance of this issue given the widespread use of ASRs in securitycritical systems. Through a comprehensive ablation study on the ASR pipeline six factors are identified that impact the transferability of these attacks on ASRs: the input type Mel Frequency Cepstral Coefficient (MFCC) the Recurrent Neural Network (RNN) output type and the sizes of vocabulary and sequence. The study reveals a nearzero transferability rate for the optimization attacks on ASRs which were previously thought to be effective due to their success in other domains. The authors propose that understanding these factors can help in designing more robust ASRs and reducing vulnerabilities in systems from other domains.  2) Strong and Weak Points Questions and Feedback Strong points:  The paper addresses an underexplored yet critical area in cybersecurity concerning ASRs. Given the increasing reliance on ASRs this research is timely and relevant.  The methodology is robust and comprehensive. The exhaustive ablation study covering the entire ASR pipeline adds depth to the research providing clear insights into the factors affecting transferability.  The link between increased ASR robustness and accuracy as a byproduct of addressing the identified factors is an interesting finding that could have broader implications for model design across domains. Weak points:  While the paper does a commendable job of identifying factors affecting transferability it is less clear on specific mitigation strategies for these vulnerabilities beyond the general suggestion of increasing robustness.  The focus is solely on the failure of optimization attacks without considering the performance of other adversarial attack forms against ASRs which could give a more rounded view of ASR vulnerabilities. Questions and Feedback:  Can the authors provide more detailed examples or case studies on how the identified factors can be adjusted or mitigated in realworld ASR implementations to enhance robustness  The paper could benefit from a discussion on the potential tradeoffs involved in modifying ASR architectures to tackle the identified vulnerabilities. For example would increasing robustness against transferable attacks significantly impact the systems efficiency or cost  It would be interesting to see a comparison with adversarial attacks in other domains. For instance why do these factors limit transferability in ASRs but not in image recognition systems  Feedback: The authors might consider expanding the scope of their future work to include the development of specific defense mechanisms based on the identified factors. Additionally exploring the impact of emerging technologies and techniques in ASR such as deep learning advancements on the transferability issue could provide valuable insights.  3) Review Summary This paper makes a significant contribution to the field of cybersecurity especially concerning the robustness of ASRs against adversarial attacks. The identification of previously unknown factors affecting the transferability of optimization attacks offers a novel insight that could guide the development of more secure ASRs. However the paper could be strengthened by providing more actionable insights into how these vulnerabilities can be mitigated and by broadening the scope to include a comparison with other forms of adversarial attacks. Despite these areas for improvement the paper\u2019s strengths lie in its timely subject matter comprehensive methodology and the practical relevance of its findings. The research opens up new avenues for exploring ASR vulnerabilities and highlights the need for a deeper understanding of how adversarial samples interact with different components of the ASR pipeline. Given its contribution to the field the paper is a valuable addition to the conversation on ensuring the security of ASRs against adversarial attacks.", "wfZGut6e09": " Peer Review of \"A Policy Gradient Method for MultiObjective Reinforcement Learning with Unknown Linear Preferences\"  1. Summary This paper introduces a novel policy gradient method aimed at multiobjective reinforcement learning (MORL) scenarios where the preferences among objectives are unknown and assumed to be linear. The authors tackle the challenge of MORL by proposing a simple yet effective algorithm that infers unknown preferences and approximates the Pareto front\u2014a set of noninferior solutions. They base their approach on enforcing Pareto stationarity as a necessary condition for Pareto optimality. Their primary contribution the Pareto Policy Adaptation (PPA) is a loss function that allows the policy to be optimal across any distribution of preferences. Notably the paper leverages implicit differentiation for efficient gradient backpropagation avoiding the computational complexity of directly dealing with the operations of the projected gradient descent solver used in the process. The authors evaluate their method on various reinforcement learning tasks demonstrating its effectiveness.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The approach of using a projected gradient descent solver alongside a PPA loss function for optimizing policies in MORL settings under unknown linear preferences is innovative. It advances the existing work by addressing adaptability to any preference distribution without prior explicit knowledge of the preferences.  Theoretical Foundation: The paper provides a solid theoretical backing for its algorithm connecting Pareto stationarity with Pareto optimality and demonstrating how the algorithm finds common ascent directions for all objectives.  Practicality and Compatibility: A significant advantage of the proposed method is its compatibility with existing policy gradient and actorcritic methods making it a potentially versatile tool for realworld applications in multiobjective decisionmaking problems. Weak Points:  Experimental Validation Limitation: While the method is evaluated on several environments the selection seems somewhat constrained potentially limiting the understanding of its performance across a wider range of complex multiobjective scenarios.  Comparison Baseline: The paper could benefit from a more comprehensive comparison with stateoftheart methods especially those that might indirectly infer preferences or employ alternative strategies for navigating the tradeoffs between multiple objectives.  Complexity Analysis Absence: There is little discussion on the computational complexity and scalability of the proposed method particularly when applied to environments with a high number of objectives or complex action spaces. Questions and Feedback:  How does the proposed method perform in a highly dynamic environment where preferences might change more frequently Is there a mechanism for quickly adapting to new preferences without retraining from scratch  Considering the implicit differentiation used for gradient backpropagation could you elaborate on the computational savings compared to direct differentiation methods Are there any specific scenarios where this advantage becomes more pronounced  A detailed analysis of how the proposed PPA loss functions performance varies with different weight allocations (\u03bb) between the scalarized reward and the preference deviation penalty could offer deeper insights.  3. Review Summary The paper presents a significant contribution to the field of multiobjective reinforcement learning addressing the challenge of optimizing policies under unknown linear preferences. Its innovative use of a projected gradient descent solver combined with the PPA loss function sets a new direction for research in MORL. The methods theoretical underpinning is robust and initial evaluations show promising results substantiating the authors claims. However the study could be strengthened by addressing the highlighted weak points notably expanding the experimental environments and providing a detailed complexity analysis. Despite these minor shortcomings the papers contributions are valuable and it should be considered a noteworthy advancement in multiobjective reinforcement learning research.", "s03AQxehtd_": " Review for \"Development of a Learnable Neural Representation of Human Pose for AIAssisted Animation Tooling\"  1) Summary of the Paper This paper presents a novel neural network architecture ProtoRes designed to reconstruct full human poses from sparse and variable user inputs such as positions or orientations of body joints aiming at significantly advancing AIassisted animation tooling. It introduces a hybrid approach that leverages both prototype encoding and residual connections to generate complete poses from learned latent spaces showing superior performance over existing methods including traditional inverse kinematics (IK) models and a Transformerbased architecture in terms of both accuracy and computational efficiency. Additionally the authors integrate their model into Unity providing userfriendly tools for animating poses with ease. They also offer two new datasets derived from highquality motion capture data to foster further research in static human pose modeling.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The novel ProtoRes architecture that combines prototype encoding with a multiblock residual approach addresses the challenge of reconstructing fullbody poses from sparse inputs innovatively.  Integration into Unity: Developing user interfaces and integrating the neural model directly into Unity makes this work highly applicable to realworld game development and animation projects.  Datasets and Code Availability: The release of two new datasets and model code is highly commendable and will likely accelerate research and application development in the field of human pose modeling. Weak Points:  Lack of Comparison to More Baselines: The evaluation focuses on comparing ProtoRes with Transformer models and traditional IK solutions without considering other recent deep learning models specialized in pose estimation which might have provided a broader context regarding the models performance.  Limited Evaluation on RealWorld Scenarios: While the introduction of userinterface tools in Unity is a strength the paper does not extensively discuss or evaluate how the model performs in realworld game development scenarios or compare its user experience with existing tools.  Limited Discussion on Generalizability: There is sparse discussion about the models generalizability to poses or user inputs significantly different from those seen during training which is crucial for practical applications. Questions and Feedback:  Can the authors provide insights into how the model handles highly irregular poses or inputs it has not been explicitly trained on  It would be beneficial to see a more detailed comparison including qualitative evaluations between ProtoRes and other stateoftheart pose estimation methods to understand the unique advantages of the proposed approach.  Clarification on the computational requirements and efficiency of ProtoRes during realtime applications within Unity would be valuable especially when compared to traditional methods.  Feedback includes suggesting the exploration of the models performance when integrating dynamic poses or considering temporal consistency in future work.  3) Review Summary The paper introduces an innovative approach to human pose reconstruction addressing notable gaps in AIassisted animation with the proposed ProtoRes architecture. Its integration into Unity and the contribution of valuable datasets are among the key strengths of this work poised to make significant impacts on both academic research and practical applications in animation and game development. However there are some areas where the evaluation and discussion could be expanded particularly around the models performance compared to a broader range of baselines its generalizability and usability in diverse realworld scenarios and computational efficiency in dynamic or complex animations. Despite these areas for improvement the work represents a substantial contribution to the field offering novel tools and perspectives for more natural and efficient human pose modeling. The integration with widely used platforms like Unity combined with the public release of resources sets a commendable precedent for future research and development efforts in this space.", "j8J97VgdmsT": " Review Summary  1) Summarize the paper The paper introduces FLAMEinNeRF a novel neural rendering method designed for controllable portrait video synthesis specifically in terms of facial expressions and viewing directions. This method incorporates volumetric neural rendering advancements particularly neural radiance fields (NeRF) along with a lowdimensional expression representation from a 3D Morphable Model (3DMM) to enable novel view synthesis and dynamic control over facial expressions within static scenes. The authors utilize 3DMM fitting to guide the network learning process ensuring a disentangled control over static scene appearance and dynamic facial actions. The innovation of FLAMEinNeRF is demonstrated through its ability to accurately reproduce high fidelity facial expressions and scene details from portrait videos captured with a mobile device showing significant improvement over existing techniques that either lack detailed control over expressions or fail to incorporate the dynamic scene aspects entirely.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Integration: The integration of 3DMM with NeRF to allow explicit control over human facial expressions in portrait videos represents a significant advancement in neural rendering and novel view synthesis.  High Fidelity and Detail Reproduction: The paper presents compelling evidence through experiments that FLAMEinNeRF can handle detailed scene aspects like hair glasses and other face details with a high degree of fidelity.  Comprehensive Experimental Validation: The authors provide extensive qualitative and quantitative comparisons with baseline methods showcasing the superiority of their proposed approach in modeling both facial expression dynamics and the overall scene. Weak Points:  Limited by Head Movement: The methods limitation in handling large head movements restricts its applicability for more dynamic scenes.  Dependency on Quality of Training Data: The systems performance heavily relies on the quality and range of expressions captured in the short video clips used for training potentially limiting its adaptability.  Complexity and Computation Requirements: While not discussed extensively the methods reliance on detailed 3D modeling and neural rendering techniques may imply significant computational requirements for training and inference. Questions and Feedback:  How does the model perform under varying lighting conditions in the training videos It would be beneficial if the authors could elaborate on the robustness of FLAMEinNeRF to such variations.  The paper mentions future work addressing the limitation regarding head movements. Could the authors speculate on potential approaches for incorporating larger head movement dynamics  Feedback: It would be highly instructive to include a discussion on the computational demands of FLAMEinNeRF especially in comparison to the baseline methods. This information would give readers a clearer understanding of the practicality of deploying this system in various applications.  3) Review Summary and Recommendation This paper presents a significant contribution to the field of neural rendering for portrait video synthesis achieving impressive results in terms of fidelity and control over facial expressions and viewing directions. The innovative use of an expressionconditioned neural radiance field combined with a spatial prior for disentanglement sets a new standard for dynamic object modeling within static scenes. Despite facing limitations in handling large head movements and potential concerns over computational efficiency FLAMEinNeRF represents a notable advancement in controllable human head modeling. Considering the clear advancements over existing methods the careful experimental validation and the clarity of presentation I recommend the acceptance of this paper. Future work on extending the methods capabilities to include larger head movements and optimizing computational demands will greatly enhance its applicability and audience.", "shpkpVXzo3h": " 1. Summary of the Paper The paper introduces a novel approach to optimizer memory reduction by developing 8bit optimizers that maintain the performance of traditional 32bit optimizer states but significantly reduce the memory footprint required during training. This approach is aimed at enabling the training of larger models within the same memory constraints. The core innovation is the blockwise dynamic quantization technique supplemented by dynamic quantization and a stable embedding layer which addresses the challenges of quantization accuracy computational efficiency and stability for largescale models. The authors demonstrate the efficacy of their method across several tasks including language modeling image classification machine translation and more showcasing that their 8bit optimizers can serve as a direct replacement for 32bit counterparts without adjustments to hyperparameters and with improved memory efficiency.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation in Optimizer Memory Efficiency: The development of 8bit optimizers using blockwise dynamic quantization is a significant advancement in optimizer efficiency addressing a crucial bottleneck in training larger models.  Broad Applicability and Ease of Integration: The authors\u2019 efforts to validate their approach across a wide range of tasks and their provision of the optimizer as an easytointegrate tool (requiring minimal code changes) significantly enhance the utility and accessibility of their contribution.  Comprehensive Evaluation: The exhaustive experimental section coupled with a sensitivity analysis and a detailed ablation study provides a thorough validation of the method\u2019s effectiveness and stability. Weak Points:  Lack of Discussion on Tradeoffs: The paper could benefit from a deeper discussion on potential tradeoffs or limitations particularly concerning the impact of 8bit quantization on longterm training dynamics or specific scenarios where the approach may be less effective.  Comparative Analysis: While the paper compares 8bit optimizers with their 32bit counterparts and briefly mentions Adafactor a more extensive comparison with other memory reduction techniques or optimizers might further contextualize the advantages or unique contributions of the proposed method. Questions and Feedback:  Impact on FineGrained Model Dynamics: Can the authors discuss if and how the 8bit quantization might affect the precision of gradient updates especially in the context of very deep or complex models where minor differences in optimizer behavior could accumulate  Generalizability to Other Domains: The paper focuses on specific tasks like language modeling and image classification. How generalizable is the proposed method to other domains particularly those with different data distribution characteristics or optimization challenges  Future Directions: The research opens exciting avenues for memory efficiency in model training. It would be beneficial to outline potential future work such as exploring the integration of these optimizers with other efficiencyenhancing techniques (e.g. model pruning sparse training) or adapting the approach for emerging model architectures.  3. Review Summary and Recommendation The paper presents a significant contribution to the field of deep learning optimization by introducing a method to substantially reduce the memory footprint of optimizer states through the novel application of 8bit quantization. This development holds the potential to democratize the training of large models by alleviating one of the major computational bottlenecks. The methodology is rigorously validated across several benchmarks demonstrating the approachs robustness and wide applicability. However the paper would benefit from more discussion on the potential limitations and tradeoffs providing the readers with a clearer understanding of scenarios where 8bit optimizers might be less advantageous. Furthermore expanding the comparative analysis with alternative memory reduction strategies could strengthen the argument for the method\u2019s unique value proposition. Recommendation: Given the novelty potential impact and thorough validation of the presented work I recommend acceptance. However I encourage the authors to address the discussed weak points particularly by expanding on the discussion around the methods limitations and comparative analysis with other techniques to further strengthen the paper.", "o6dG7nVYDS": " Review of the Paper \"A LearningTheoretic Perspective on Domain Generalization\"  Summary This paper addresses the challenge of domain generalization (DG) where a model trained on multiple known data distributions is expected to perform well on unseen data distributions. It critically evaluates the existing heuristicbased DG methods by highlighting their inconsistent performance and lack of theoretical grounding. The papers primary contribution is the proposition of a learningtheoretic generalization bound for DG based on the model\u2019s Rademacher complexity. This new bound provides a theoretical basis for understanding the erratic performance of current DG methods and suggests a simple yet effective approach for domain generalization advocating for regularized empirical risk minimization (ERM) with leaveonedomainout crossvalidation. The paper validates its theoretical findings through empirical results demonstrating the effectiveness of its proposed approach against existing methods on the DomainBed benchmark.  Strong Points 1. Theoretical Novelty and Insight: The introduction of a learningtheoretic generalization bound for DG based on Rademacher complexity is the papers most significant strength. This theoretical framework clarifies why existing DG methods vary in effectiveness and offers a principled approach to tackling DG by understanding it as a balance between empirical risk and model complexity. 2. Practical Implications: The paper successfully translates its theoretical insights into a practical methodology recommending regularized ERM with domainwise crossvalidation for DG. This approach is accessible and potentially more effective than the more complex existing methods. 3. Comprehensive Evaluation: By engaging with the DomainBed benchmark for empirical validation the paper ensures its findings are grounded in a thorough and relevant experimental setting. The clear demonstration that simpler models with controlled complexity achieve competitive or superior performance is compelling.  Weak Points 1. Assumption of Linear Models and Preselected Features: The primary empirical evaluation relies heavily on linear models and precomputed deep features. While useful for demonstrating the papers core arguments it limits the generalizability of the findings to more complex scenarios and models. 2. Depth of Empirical Analysis: While the empirical analysis supports the paper\u2019s claims it could be enhanced by more direct comparisons to stateoftheart DG methods in endtoend training scenarios beyond the preselected feature framework. 3. Discussion on RealWorld Applicability: While the paper\u2019s contributions are significant it could benefit from a deeper discussion on the applicability of its findings in realworld DG scenarios especially considering the diversity and complexity of such tasks beyond the datasets in DomainBed.  Questions and Feedback 1. Generalization Beyond Linear Models: Could the authors elaborate on how the proposed Rademacher complexitybased generalization bound and the resultant methodology might extend to or be adapted for nonlinear and deep learning models in realworld applications 2. Impact of Data Diversity: How might the diversity or lack thereof in the training distributions influence the efficacy of the proposed method especially given its reliance on domainwise crossvalidation 3. Comparative Analysis: Could the authors provide insights into how their proposed method performs in comparison to stateoftheart DG methods in scenarios where models and features are both learned endtoend  Review Summary The paper presents a notable advancement in the domain generalization field by introducing a theoretical generalization bound based on Rademacher complexity offering a fresh perspective on the underlying challenges of DG. This theoretical contribution is laudable for not only highlighting limitations in existing approaches but also for guiding the development of a pragmatic theoretically informed solution. The empirical validation underscores the potential of simpler regularized ERM methods challenging the prevailing complexity of current DG methods. However the paper could be strengthened by addressing the applicability of its insights to a broader range of models and realworld scenarios and by providing a more direct competition with current stateoftheart methods in more varied and demanding settings. Overall this work makes a significant contribution to the understanding of domain generalization setting a strong foundation for future theoretical and empirical advancements in the field.", "kK3DlGuusi": " Peer Review of \"Quantized Sparse PCA for Weight Compression in Neural Networks\"  Summary of the Paper The paper introduces a novel approach for neural network weight compression by combining concepts from sparse Principal Component Analysis (PCA) quantization and tensor factorization. The method involves representing the weight tensors of neural networks in a factorized quantized and sparse form to achieve high compression ratios without significantly compromising the models performance. This approach is designed to work efficiently both in moderate and extreme compression scenarios making it suitable for deployment in resourceconstrained environments like edge devices as well as in the cloud to reduce latency and power consumption. The authors propose an iterative projected gradient descent algorithm for solving the quantized sparse PCA problem and demonstrate that their method offers competitive compressionaccuracy tradeoffs when compared to existing techniques like scalar and vector quantization SVDbased methods and weight pruning. The experimental results on ImageNet classification tasks using ResNet18 and MobileNetV2 architectures support the methods effectiveness across a broad range of compression regimes.  Strengths and Weaknesses of the Paper Strong Points: 1. Novelty: The paper presents a unique approach to weight compression by integrating quantized sparse PCA which distinguishes it from existing methods in literature. 2. Comprehensive Evaluation: It provides a thorough experimental analysis comparing the new method against various baselines and demonstrating its efficacy on standard ImageNet classification benchmarks. 3. Broad Applicability: The techniques compatibility with both moderate and extreme compression regimes expands its applicability to a wide range of deployment scenarios. 4. Theoretical and Practical Insight: The paper seamlessly blends theoretical innovation with practical utility providing a detailed algorithm for the proposed method and discussing its relation to other approaches. Weak Points: 1. Complexity and Implementation Details: The paper could benefit from a deeper discussion on the computational complexity of the approach and more detailed insights into the implementation aspects for practical deployments. 2. Hyperparameter Sensitivity: While the paper explores various compression hyperparameters it does not fully address the sensitivity of model performance to these parameters or provide guidelines for their selection. 3. Limited Scope of Experiments: Although the experiments demonstrate the methods effectiveness additional results on different datasets and neural network architectures could bolster its generality. 4. Comparison with Recent Advances: The literature review and baseline comparisons might not fully encapsulate the most recent advances in neural network compression techniques. Questions and Feedback to the Author: 1. Could the authors provide more details on the computational complexity of the proposed method especially in comparison to the baseline methods 2. Are there any specific challenges in selecting the optimal set of hyperparameters (rank bitwidth sparsity) for a given compression task Can the authors suggest any heuristics 3. The paper focuses primarily on ImageNet classification tasks. How does the method perform on other tasks such as object detection or natural language processing models 4. Would it be possible to extend the methodology to incorporate automatic rank bitwidth and sparsity ratio selection to further simplify the compression process  Review Summary The paper introduces an innovative and effective approach to neural network weight compression showcasing its potential to achieve stateoftheart compressionaccuracy tradeoffs across a broad spectrum of compression ratios. By unifying sparse PCA quantization and tensor factorization into a quantized sparse PCA framework the authors address a critical need for efficient model deployment in resourceconstrained environments. The proposed method is wellconceived and the experimental results substantiate its benefits compared to conventional compression techniques. However the paper would benefit from a more detailed discussion on the computational aspects of the method insights into practical deployment challenges and expanded experimental validation across a wider range of tasks and network architectures. Additionally addressing the issue of hyperparameter sensitivity and exploring avenues for automatic parameter selection could significantly enhance the methods appeal and utility. In conclusion the paper contributes valuable insights to the field of neural network compression with the potential for broad impact on model deployment for edge computing cloud inference and beyond. Further exploration and elaboration on the identified weak points could make the paper an even more compelling and referenceworthy contribution to the literature.", "iMqTLyfwnOO": " Review of \"Augmented Sliced Wasserstein Distances for HighDimensional Data Distribution Comparisons\"  Summary This paper introduces the Augmented Sliced Wasserstein Distance (ASWD) a novel metric for comparing data distributions in machine learning applications. ASWD is designed to address the computational inefficiencies and accuracy limitations of existing Wasserstein distance variants such as the Sliced Wasserstein Distance (SWD) and its nonlinear extensions by embedding data samples onto highdimensional hypersurfaces. These hypersurfaces are parameterized by neural networks allowing for more flexible nonlinear projections of data distributions that can capture complex data structures more effectively. The authors provide a theoretical foundation for ASWD proving it as a valid metric under specific conditions related to the injectivity of the neural network architecture used. Numerical results showcase the superior performance of ASWD over other Wasserstein metrics in both synthetic and realworld datasets.  Strengths and Weaknesses Strengths: 1. Novelty: The conceptual framework of ASWD presents a significant methodological advancement in the field of optimal transport metrics addressing critical shortcomings of existing approaches with an innovative solution. 2. Theoretical Foundation: The paper provides a rigorous theoretical examination of the conditions under which ASWD functions as a valid metric reinforcing the reliability of the method. 3. Empirical Validation: Extensive experiments including those involving generative modeling tasks and distribution morphing validate the practical effectiveness of ASWD in a variety of scenarios. 4. Computational Efficiency: By leveraging neural networks for projecting distributions onto hypersurfaces ASWD offers a computationally efficient solution to the otherwise demanding calculations involved in Wasserstein distance estimations. Weaknesses: 1. Model Complexity: The implementation of ASWD although efficient introduces complexities due to the involvement of neural networks which may pose challenges in terms of parameter tuning and computing resource requirements. 2. Generalization of Results: While the paper demonstrates the effectiveness of ASWD across a variety of datasets further investigation might be needed to determine its performance in other potentially more challenging data distributions and application areas. 3. Theoretical Assumptions: The validity of ASWD relies on specific theoretical assumptions such as the injectivity of the neural network architecture. These conditions limit the flexibility in choosing network configurations and might not always be straightforward to verify in practical applications. Questions and Feedback for the Author: 1. Model Interpretability: Given the increased complexity with neural networks how does ASWDs interpretability compare with other Wasserstein distances Are there measures in place to understand how hypersurfaces are learned 2. Scalability: How does ASWD scale with the dimensionality of the data and the size of the datasets Are there considerations for optimizing the neural network architectures to handle very large datasets 3. Robustness: Can the authors comment on the robustness of ASWD to outliers or noisy data distributions Are there mechanisms within the proposed method to mitigate potential biases introduced by these factors 4. Extension to Other Applications: Could the authors elaborate on potential extensions of ASWD to other machine learning tasks or domains not covered in the paper  Review Summary The paper presents a significant contribution to the application of Wasserstein distances in machine learning introducing the augmented sliced Wasserstein distance (ASWD) to overcome the computational and accuracy limitations of existing methods. Through a wellfounded theoretical framework and extensive empirical validation the authors convincingly demonstrate the superiority of ASWD in capturing complex data distributions efficiently. While the models complexity and the assumptions underlying its theoretical validity pose challenges the potential benefits of ASWD in various application domains are clear. Further investigations into its scalability robustness and interpretability could augment the appeal of ASWD for broader adoption in machine learning and statistics. Given the strengths outlined I recommend the publication of this paper subject to considerations for addressing the mentioned weaknesses and questions.", "rbPg0zkHGi": " Review of \"Utilizing Noise Stability for Uncertainty Estimation in Active Learning with Deep Neural Networks\"  Summary The paper introduces a novel method for uncertainty estimation in active learning called the SingleTraining MultiInference algorithm leveraging noise stability of model parameters. This is particularly aimed at addressing the challenge of selecting informative unlabeled data for annotation in the context of deep neural networks (DNNs) which often suffer from overconfidence in their predictions. The authors propose the technique of perturbing model parameters with noise and measuring the output deviation to estimate data uncertainty. This approach is validated with theoretical analysis drawing parallels with classical variance reduction theory and is empirically tested across several datasets and tasks demonstrating better performance than current stateoftheart methods.  Strong Points 1. Novelty: The approach of using noiseinduced parameter perturbation as a metric for uncertainty estimation in active learning is innovative. It effectively addresses the overconfidence issue of DNNs without requiring multiple training passes or complex model modifications. 2. Theoretical Soundness: The paper provides a comprehensive theoretical analysis connecting the proposed method with classical variance reduction theory. This not only establishes a strong foundation for the method but also enhances its credibility. 3. Empirical Validation: Extensive experiments across a range of datasets and tasks (image classification semantic segmentation and 3D cryoET subtomogram classification) convincingly demonstrate the superiority of the proposed method over existing active learning baselines. 4. Task Agnostic: The methods applicability across different tasks without needing taskspecific adaptations is a significant advantage highlighting its versatility and ease of integration into various learning scenarios.  Weak Points 1. Sensitivity Analysis: While the paper discusses hyperparameter choices a thorough sensitivity analysis detailing the impacts of different noise magnitudes and distributions on performance could provide deeper insights into the robustness of the method. 2. RealWorld Applicability: Although the paper presents results from several benchmark datasets the realworld applicability especially in complex scenarios or with highly imbalanced datasets is not explicitly covered. A discussion on these aspects would enhance the papers practical relevance. 3. Comparison with Recent Methods: The paper compares the proposed method against a range of existing approaches but lacks a direct comparison with the most recent active learning strategies that utilize similar noisebased or uncertainty estimation techniques.  Questions and Feedback to the Author 1. Could the authors elaborate on the choice of hyperparameters especially the noise distribution parameters and their impact on the algorithms performance across different tasks 2. How does the method perform with highly imbalanced datasets or in realworld scenarios where data may not be as cleanly categorized as in benchmark datasets 3. Is there a consideration for the computational overhead introduced by generating multiple perturbed models especially in scenarios with large models or datasets 4. The paper mentions task agnosticism as a strength. Could the authors provide insights on any modifications or considerations required when applying this method to tasks radically different from the ones tested  Review Summary This work introduces a significant advancement in active learning for deep neural networks through its novel noise stabilitybased uncertainty estimation method. The theoretical grounding and extensive empirical validation across diverse tasks underscore this methods potential to effectively address the challenge of data selection in active learning especially under the common issue of model overconfidence. However deeper exploration into the parameter sensitivity realworld applicability and computational considerations could further strengthen the paper. Given its solid theoretical basis demonstrated effectiveness and straightforward applicability I recommend this paper for publication contingent upon a more comprehensive discussion of the mentioned aspects to ensure the methods robustness and practical utility.", "q9zIvzRaU94": " Review of \"StateDependent Causal Inference for NonStationary TimeSeries Data\"  Summary This paper presents a novel approach called StateDependent Causal Inference (SDCI) designed to infer causal relationships within nonstationary timeseries datasets where underlying states influence the dynamics of observed variables. The approach leverages a deep probabilistic model to accommodate variations in data dynamics prompted by hidden and observable states. Unlike existing works which mainly focus on stationary data or scenarios with fully observed variables SDCI aims to uncover causal structures under more complex realworld conditions where the state may change over time and not be directly observable. Using synthetic datasets the authors demonstrate how SDCI outperforms existing methods in discovering causal relationships even when states are hidden. This work extends upon prior literature by introducing a methodology capable of handling the conditional stationarity of timeseries data and managing the challenges posed by statedependent dynamics.  Strong Points 1. Novel Approach: The introduction of SDCI to tackle nonstationary timeseries data by conditioning on states represents a significant advancement in the field of causal discovery. It addresses a critical gap in existing literature where most methods struggle with or entirely overlook scenarios where underlying states influence observational dynamics. 2. Comprehensive Methodology: The paper lays out a very detailed methodology including problem formulation assumptions model implementation and algorithmic details. This thoroughness provides clarity on how SDCI operates and its theoretical underpinnings ensuring the approach is reproducible and well understood. 3. Empirical Evaluation: The authors provide a robust empirical evaluation of SDCI against a baseline demonstrating its effectiveness in complex scenarios across different synthetic datasets. These experiments highlight SDCIs superior performance in identifying causal relationships under varying states showcasing its practical utility.  Weak Points 1. Lack of RealWorld Data Validation: The paper exclusively uses synthetic datasets for evaluation. While these datasets are instrumental in demonstrating SDCIs effectiveness the absence of realworld data applications leaves a gap in understanding how the method performs under less controlled more complex conditions. 2. Limited Discussion on Scalability: Theres minimal discussion around the scalability of the proposed method especially concerning computational resources and processing time as dataset size and complexity increase. This detail is crucial for assessing the feasibility of applying SDCI to largescale realworld datasets. 3. Comparison with StateoftheArt: While the paper does offer a comparison with existing approaches it primarily contrasts SDCI with a single baseline method. A broader comparison with other stateoftheart causal discovery techniques especially those designed for nonstationary data would strengthen the papers claims about SDCIs efficacy.  Questions and Feedback to the Author 1. RealWorld Data Application: Could the authors provide insights or preliminary results on applying SDCI to realworld datasets Understanding its performance in such scenarios would greatly increase the methods credibility and applicability. 2. Scalability Considerations: Can the authors comment on the scalability of SDCI Specifically how does the methods performance and computational requirements change as the size of the dataset grows 3. Comparison with Other Techniques: How does SDCI compare to other nonstationary causal discovery techniques that were not mentioned or evaluated in the paper Expanding on this could further highlight SDCIs unique contributions and advantages.  Review Summary The paper introduces an innovative approach SDCI for causal discovery in nonstationary timeseries data influenced by underlying states marking a significant advancement in causal inference research. The methodology is comprehensively detailed and empirical evaluations demonstrate its superiority in uncovering causal structures under challenging scenarios compared to a baseline method. However the paper could be strengthened by including evaluations on realworld datasets discussing scalability aspects in more detail and broadening the comparison with other stateoftheart methods. Addressing these points would make a strong case for SDCIs applicability and efficiency in practical largescale scenarios beyond the controlled conditions of synthetic data.", "mF5tmqUfdsw": "Review of \"ZerothOrder ActorCritic algorithm (ZOAC) for Reinforcement Learning\" 1) Summary of the Paper The paper introduces the ZerothOrder ActorCritic (ZOAC) algorithm which amalgamates evolutionbased zerothorder and policy gradientbased firstorder methods into a singular onpolicy actorcritic architecture. This integration aims to leverage the strengths of both approaches: the flexibility and robust exploration of zerothorder methods and the sample efficiency of firstorder methods. The key innovation lies in the use of timestepwise parameter perturbation for rollout collection coupled with alternating phases of firstorder policy evaluation (PEV) and zerothorder policy improvement (PIM) within each iteration. The paper contends that these modifications along with the introduction of a critic network significantly enhance sample efficiency and stabilize the learning process. Evaluations conducted on continuous control tasks from the OpenAI gym suite show that ZOAC outstrips both zerothorder and firstorder baselines in terms of sample efficiency final performance and policy robustness. The paper is rounded off with robustness analyses visualizations of learned policies and comprehensive ablation studies to underline the contribution of each component of ZOAC. 2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Integration: The idea to fuse zerothorder and firstorder methods within an actorcritic framework is innovative and addresses the limitations of both approaches effectively.  Comprehensive Evaluation: The paper provides a thorough empirical analysis of the proposed method comparing it with both zerothorder and firstorder baselines across several benchmarks which substantiates the claimed improvements.  Indepth Analysis: The theoretical analysis especially regarding the variance of gradient estimators and the ablation studies offer deep insights into the workings and efficacy of ZOAC.  Robustness Testing: The robustness analysis against observation and parameter noise adds value by showing the practical resilience of policies learned through ZOAC. Weak Points:  Complexity of Implementation: While the paper presents impressive results the complexity of integrating timestepwise perturbation a critic network and alternating PEV and PIM may deter practical adoption.  Limited Policy Forms Evaluation: The paper primarily evaluates linear policies and neural networks. The performance and adaptability of ZOAC on other forms of policy parameterizations remain unexplored.  Scalability Concerns: There is little discussion on the scalability of ZOAC to very highdimensional problems or extremely complex environments. Questions and Feedback:  Can the authors comment on the computational overhead introduced by ZOAC compared to traditional actorcritic methods  The paper could benefit from a deeper theoretical comparison between ZOACs approach to exploration and that of purely zerothorder or firstorder methods. Can the authors provide insights into how ZOACs exploration strategy theoretically balances exploration and exploitation  It would be interesting to see how ZOAC performs in environments with discrete action spaces or in scenarios with partial observability. Are there plans to extend this work in those directions 3) Review Summary The paper presents a significant contribution to the field of Reinforcement Learning by proposing a novel algorithm ZOAC which intelligently combines the strengths of zerothorder and firstorder methods within an actorcritic framework. The method addresses key challenges in RL such as sample efficiency and policy robustness validated by comprehensive empirical evaluations and theoretical discussions. Despite its strengths the practical complexity of implementing ZOAC its performance on a broader spectrum of policies and scalability concerns are areas that could be better addressed or acknowledged. Additionally diving deeper into the theoretical underpinnings of ZOACs exploration strategy and extending its evaluation to more diverse environments would further bolster the papers contributions. In conclusion ZOAC represents an innovative step forward merging the robust exploration capabilities of zerothorder methods with the sample efficiency of firstorder methods. The paper is wellwritten methodologically sound and contributes valuable insights and empirical evidence to the domain of Reinforcement Learning.", "hcQHRHKfN_": "Summary This paper introduces RewardSwitching Policy Optimization (RSPO) an innovative method designed to discover diverse strategies in complex Reinforcement Learning (RL) environments. RSPO iteratively finds novel policies that are both locally optimal and distinctly different from existing ones by alternating between extrinsic and intrinsic rewards using a trajectorybased novelty measurement to guide this switching process. The main contributions include a novel algorithm that significantly enhances the efficiency of discovering strategically diverse policies the adoption of a crossentropybased diversity metric for policy optimization and two additional diversitydriven intrinsic rewards to encourage exploration. The effectiveness of RSPO is demonstrated across various domains including singleagent navigation tasks MuJoCo control multiagent staghunt games and the StarCraft II MultiAgent Challenge (SMAC) showcasing its ability to discover a wide spectrum of strategies efficiently. Strong Points: 1. Novelty and Innovation: The introduction of the RSPO method is novel addressing the significant challenge of discovering diverse strategies in RL with a unique rewardswitching technique. The approach of alternating between extrinsic and intrinsic rewards based on the novelty of trajectories is innovative and thoughtfully designed to encourage the exploration of new policies. 2. Empirical Validation: The paper provides extensive empirical evidence of the effectiveness of RSPO across a variety of domains. This rigorous validation demonstrates the methods generalizability and effectiveness in discovering diverse strategies significantly outperforming existing baselines. 3. Comprehensive Exploration Techniques: The adoption of crossentropybased diversity metrics and the proposal of two types of intrinsic rewards for promoting exploration are welljustified and effectively contribute to the method\u2019s success. These techniques address the critical challenge of balancing diversity with performance in the environment. Weak Points: 1. Theoretical Justification: While the paper mentions leaving further theoretical justifications for future work a preliminary discussion or theoretical insight into why and how RSPO achieves its success could strengthen the paper. Providing some theoretical backing even if partial would help validate the empirical results and the intuition behind the approachs design. 2. Comparison with Baselines: Although the paper compares RSPO with several baselines the selection of these baselines could be broadened. Including more recent or alternative approaches to diversitydriven exploration in reinforcement learning could provide a clearer picture of RSPOs positioning within the field. 3. Impact on Sample Efficiency: The paper does not discuss the impact of RSPO on sample efficiency which is a critical aspect of RL algorithms. Understanding whether the iterative process and reward switching lead to more efficient learning regarding the number of environment interactions would be valuable. Question and Feedback to the Author:  Can the authors provide some insight or preliminary theoretical justification for why the rewardswitching technique is effective in discovering diverse strategies Understanding the theoretical underpinnings even at a high level could add depth to the paper.  Including comparative analysis against a broader set of baselines particularly recent methods in diversitydriven exploration in RL could help highlight RSPO\u2019s advantages more distinctly. Are there plans to extend this comparison in future work  The paper would benefit from a discussion on the implications of RSPO on sample efficiency. Is there an observed impact on the number of samples needed to achieve convergence compared to baseline methods Review Summary: The paper introduces an innovative method RewardSwitching Policy Optimization (RSPO) targeting the discovery of diverse strategies in reinforcement learning environments. The approach grounded in switching between extrinsic and intrinsic rewards based on trajectory novelty shows significant promise as demonstrated through extensive empirical validation across various domains. The novel techniques introduced including a crossentropybased diversity metric and two types of intrinsic rewards significantly contribute to the methods effectiveness. However the paper could be strengthened with some theoretical insights into the mechanisms of RSPO a broader comparison with recent baselines and a discussion on its impact on sample efficiency. Overall the paper presents a significant contribution to the field of reinforcement learning particularly in addressing the challenge of discovering diverse strategies efficiently.", "vfsRB5MImo9": " Review of the Paper: \"Continual Knowledge Learning for Large Language Models\"  1. Summarize the paper: This paper addresses the challenge of keeping Large Language Models (LMs) like T5 and GPT3 updated with the worlds everchanging knowledge without forgetting previously acquired knowledge. To tackle this the authors introduce a novel Continual Learning (CL) problem named Continual Knowledge Learning (CKL) along with a new benchmark and metric for evaluating models ability to retain old knowledge update outdated knowledge and acquire new knowledge. They then evaluate several baseline methods including regularization rehearsal and parameterexpansion techniques to establish strong initial performances on this benchmark. Their findings reveal that parameterexpansion methods are most effective in this context though they come with significant memory inefficiencies. The paper not only establishes CKL as an important problem but also opens up avenues for future research in developing more efficient and robust methods for updating LMs with new knowledge.  2. Strong and weak points Questions and Feedback: Strong Points:  Novelty: The introduction of CKL as a distinct problem within the realm of CL and the creation of a benchmark and metric specifically designed for evaluating LMs on this problem are highly innovative.  Thorough Evaluation: The extensive experiments with various baseline methods provide a comprehensive understanding of how different approaches perform in the context of CKL.  Resource Availability: By making their benchmark datasets model checkpoints and code publicly available the authors have significantly contributed to fostering further research in this field. Weak Points:  Memory Inefficiency of Best Methods: The paper shows that parameterexpansion methods which perform the best across experiments are not memoryefficient which could severely limit their practical applications.  Limited Exploration of Alternative Approaches: While the paper provides a good overview of applicable methods from the literature there seems to be limited exploration of novel approaches that could address the unique challenges of CKL more efficiently.  Lack of Realworld Application Scenarios: While the theoretical framework and benchmarks are clearly defined the paper could benefit from concrete examples or case studies showcasing realworld applications or implications of CKL. Questions and Feedback:  Could the authors discuss potential strategies or future research directions that might overcome the memory inefficiency of parameterexpansion methods  Have the authors considered exploring methods that dynamically adjust the architecture of LMs in response to the evolving knowledge landscape as a potential solution for CKL  It would be beneficial if the authors can provide insights or preliminary results on how CKL impacts the overall performance of LMs on downstream tasks especially those that critically depend on uptodate knowledge.  3. Review Summary: The paper introduces an important and novel challenge in the everevolving field of Large Language Models  Continual Knowledge Learning (CKL). By establishing a benchmark and metric to assess models performance on CKL the authors lay a solid foundation for future research in this area. The evaluation of various baseline methods provides valuable insights into the current capabilities and limitations of existing approaches to address CKL. However the memory inefficiency of the most effective methods identified raises critical concerns for their practical applicability. Additionally broadening the exploration of novel methods and addressing the realworld relevance of CKL could further strengthen the impact of this research. Despite these considerations the significance and potential of CKL as outlined in this paper are clear and the resources provided will undoubtedly be of great value to the research community. The paper successfully motivates the need for innovative approaches to designing LMs that can adapt to the dynamic nature of world knowledge paving the way for future advancements in the field.", "qynwf18DgXM": " Peer Review for \"Evolution of Linearly Nearly Euclidean Metrics Under the RicciDeTurck Flow and Its Application to Neural Networks\"  1. Summary of the Paper This paper delves into the realm of manifold microsurgery using Ricci flow particularly focusing on the evolution of linearly nearly Euclidean metrics under the RicciDeTurck flow. The authors extend their investigation into the applications of such metrics in the training of neural networks. The paper establishes the dynamical stability and convergence of linearly nearly Euclidean metrics providing a novel geometric optimization method for solutions on a manifold through regular shrinking or expanding of Ricci solitons. Additionally it introduces an approximation method for the steepest descent gradient flow on these manifolds by employing information geometry and mirror descent algorithms. This methodology is then applied to train neural networks on classification tasks demonstrating the evolution of the metrics consistent with the RicciDeTurck flow process.  2. Strong Points  Innovation: The paper presents a novel approach by integrating Ricci flow into the realm of neural networks opening new avenues for employing geometric optimization methods in machine learning.  Theoretical Foundation: The authors provide a robust theoretical framework supporting the stability and convergence of linearly nearly Euclidean metrics under the RicciDeTurck flow. The proofs are rigorous detailed and grounded in existing literature.  Practical Application: They successfully apply the theoretical constructs to train neural networks presenting empirical evidence showcasing the feasibility and relevance of their method in practical scenarios.  Interdisciplinary Approach: The paper bridges diverse domains such as differential geometry information geometry and machine learning demonstrating an effective interdisciplinary approach.  3. Weak Points  Accessibility and Complexity: The paper is highly complex and dense potentially limiting its accessibility to readers not wellversed in differential geometry or Ricci flow concepts.  Experimental Section Limitations: While the experiments showcase the feasibility of the proposed method more varied datasets and neural network architectures could provide a more comprehensive understanding of its applicability.  Lack of Comparison: The paper could further strengthen its arguments by comparing the proposed method with other neural network training approaches highlighting its advantages and potential limitations.  Implementation Details: It lacks explicit details on the implementation of the approximated gradient flow which could hinder reproducibility and experimentation by the research community.  Questions and Feedback to the Authors 1. Can the authors elaborate on the choice of datasets and whether the methods effectiveness varies across different data complexities 2. Could the paper benefit from a section comparing traditional neural network training methods with the proposed geometric optimization approach especially in terms of computational efficiency and training performance 3. Are there specific neural network architectures or types of learning tasks where the proposed method is expected to underperform or excel  Review Summary The paper introduces an innovative approach by integrating the concept of Ricci flow with neural network training offering a new geometric optimization method. It provides a solid theoretical foundation with rigorous proofs and applies these concepts to practical scenarios demonstrating the method\u2019s viability. However the paper could improve its impact by addressing its complexity expanding its experimental validation offering comparisons with existing methods and providing more implementation details for better reproducibility. Overall this paper presents a significant contribution to the intersection of differential geometry and machine learning. Its innovative approach grounded in rigorous theory and demonstrated through practical application makes a compelling case for further exploration and development of geometric optimization methods in neural network training. With improvements to accessibility and a more extensive validation this work has the potential to open new research directions and applications in machine learning and beyond.", "izvwgBic9q": " Review of \"Unsupervised Learning of FullWaveform Inversion with Convolutional Neural Networks\"  1. Paper Summary This paper introduces an unsupervised learning approach to FullWaveform Inversion (FWI) termed UPFWI which predicts subsurface velocity maps from seismic data without the need for labeled data pairs. The proposed method leverages a convolutional neural network (CNN) to estimate velocity maps from seismic inputs followed by differentiable forward modeling to regenerate seismic data forming a closedloop training mechanism. The approach bypasses the traditional requirement for costly velocity maps by using seismic data alone facilitated by a novel integration of physicsdriven and datadriven methodologies. A large new dataset OpenFWI is also introduced to offer a comprehensive benchmark for the geophysics community. The results indicate that UPFWI achieves comparable if not superior accuracy to supervised methods especially with an increased volume of unlabeled seismic data. The paper highlights the incorporation of perceptual loss in improving the quality of predicted velocity maps and suggests the potential applicability of their unsupervised learning framework to other domains involving partial differential equations (PDEs).  2. Strong and Weak Points Questions and Feedback Strong Points:  The integration of PDE forward modeling with CNNs represents a novel and effective strategy for addressing the FWI problem in an unsupervised manner providing a viable alternative to costly supervised methods.  Introduction of the OpenFWI dataset substantially contributes to the field creating new opportunities for research in geophysics and specifically FWI methods by providing a largescale challenging benchmark.  Demonstrating that UPFWI can outperform supervised models with a sufficient amount of unlabeled seismic data is particularly noteworthy highlighting the scalability and efficiency of the unsupervised approach. Weak Points:  The paper provides limited detail on the implementation nuances of the CNN architecture which is critical for reproducing and understanding the performance of the proposed method.  There is a lack of indepth discussion on the theoretical implications of combining CNNs with differentiable forward modeling specifically regarding how this methodology impacts the learning dynamics and generalizability of the model.  While the introduction of OpenFWI is commendable the paper could benefit from further validation of the UPFWI model on external realworld datasets to demonstrate its applicability beyond synthetic benchmarks. Questions and Feedback:  Could the authors elaborate on the choice of perceptual loss over other potential loss functions and whether combinations with other loss functions were explored  How sensitive is the UPFWI model to the hyperparameters (e.g. learning rate \u03bb values in loss functions) and what guidance can the authors provide on tuning these for different datasets  The paper could enhance its discussion on the limitations and future work section particularly in exploring adversarial loss and other computational efficiency methods. Could the authors provide preliminary insights on these aspects  3. Review Summary The paper presents a significant advancement in the domain of FullWaveform Inversion (FWI) by proposing an unsupervised learning approach UPFWI which effectively sidesteps the need for expensive velocity maps. Integrating CNNs with differentiable forward modeling grounded in PDEs the authors offer a scalable solution to FWI a key challenge in geophysics. Their contribution is further solidified by the introduction of the OpenFWI dataset setting a new benchmark for the community. Although there are areas such as the detailed CNN implementation theoretical justification of method selection and external validation that could be further strengthened the overall impact of the work is clear. The successful application of UPFWI demonstrated through comprehensive experiments and enabling comparable or better results than supervised methods with increased data marks a pivotal step forward. The potential applicability of this unsupervised framework in other domains involving PDEs opens new research avenues. The recommendation is for acceptance noting that addressing the posed questions and feedback could enhance the papers clarity and depth.", "wRODLDHaAiW": " Review of \"iLQRVAE: Variational Inference in Nonlinear Dynamical Systems with Applications to Neuroscience\" 1. Summary: This paper introduces iLQRVAE a novel approach to learning the latent dynamics of nonlinear dynamical systems and inferring ongoing external inputs by employing a controlbased strategy within the framework of variational autoencoders (VAEs). Leveraging the iterative Linear Quadratic Regulator (iLQR) within the recognition model of a sequential VAE the authors tackle the challenges of learning in complex highdimensional systems like the mammalian brain where neural dynamics underlying behavior are only partially observable. Distinctly iLQRVAE optimizes the standard evidence lower bound by incorporating iLQR solutions directly a method now feasible due to advances in differentiable control. The authors demonstrate the models effectiveness through synthetic examples and real neural and behavioral datasets involving primates performing reaching tasks. Notably iLQRVAE yields highquality inferences and kinematic reconstructions illustrating its potential as a robust tool for understanding neural mechanisms in neuroscience and beyond. 2. Evaluation: Strong Points:  Innovative Approach: Integrating the iLQR algorithm with a sequential VAE represents a significant methodological innovation that offers a new way to infer latent dynamics and external inputs in nonlinear dynamical systems.  Broad Implications: The method holds promise beyond neuroscience applicable to any field involving complex dynamical systems thereby opening new avenues for research across disciplines.  HighQuality Inferencing: The ability of iLQRVAE to perform highquality kinematic reconstructions from neural data as demonstrated in the primate reaching tasks underscores its practical utility and effectiveness.  Flexibility and Robustness: The models flexibility in dealing with heterogeneous trial lengths and its robust training regime which avoids common pitfalls like posterior collapse are particularly commendable. Weak Points:  Computational Demand: The reliance on iLQR for optimization could be computationally intensive especially for larger datasets potentially limiting its accessibility to researchers with substantial computational resources.  Generalizability and Identifiability Issues: The paper lightly touches upon the degeneracy problems and identifiability concerns inherent in learning such complex models. More detailed exploration and potential solutions to these issues would strengthen the work.  Comparison with Existing Methods: While the paper does benchmark against some existing methods like LFADS a more exhaustive comparison including computational efficiency and robustness across a broader range of synthetic and realworld datasets would provide deeper insights into the models practical implications. Questions and Feedback to the Author: 1. Could the authors elaborate on the scalability of iLQRVAE particularly concerning computational resources and time when applied to larger or more complex datasets 2. The paper briefly mentions the general issue of identifiability in such complex models. Could the authors discuss potential strategies or future directions to address the degeneracy problem 3. It would be beneficial if the authors could provide more detailed comparisons with other methods particularly in terms of model robustness identifiability and computational demands. 3. Review Summary: The iLQRVAE model presents a significant methodological advancement in the application of control theories to variational inference displaying impressive capabilities in inferring latent dynamics and external inputs within complex systems. Its application to neural and behavioral data showcases its potential for advancing our understanding of brain dynamics emphasizing its suitability for neuroscience and other fields. Despite its strengths challenges related to computational demands and the generalizability of findings highlight areas for further development. Additional comparisons with existing models and explicit discussions on addressing identifiability issues would further solidify the papers contributions. Given its innovative approach practical effectiveness and the broad implications of its application iLQRVAE represents a valuable addition to the computational toolkit for understanding complex dynamical systems.", "jZQOWas0Lo3": " Peer Review:  1) Summarize the paper: The paper introduces a novel algorithm called \"source fiction\" for semisupervised optimal transportbased domain adaptation utilizing the intriguing relationship between adversarial attacks and cycle monotone maps or optimal transport maps. The authors propose a unique approach where instead of mapping target samples directly to the source domain optimal transport maps target samples to a set of adversarial examples labeled target samples perturbed to resemble source samples for a classifier. This method leverages the cycle monotonicity inherent in adversarial attacks enabling a natural approximation of the transformation process. The paper demonstrates through various experiments that this approach significantly enhances the performance of optimal transport methods in semisupervised domain adaptation across different datasets.  2) Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Approach: The papers methodology of linking cycle monotonicity with adversarial attacks to enhance semisupervised domain adaptation is novel and groundbreaking.  Solid Theoretical Foundation: The authors provide a rigorous theoretical underpinning for the proposed method complete with proofs of cycle monotonicity for adversarial attacks.  Empirical Validation: Extensive experimental results on multiple datasets showcase the effectiveness of the source fiction algorithm over traditional methods. Weak Points:  Complexity and Practicality: The algorithms complexity and the practical implications of implementing such a system especially in realworld applications are not sufficiently discussed.  Limited Domain Applicability: While the paper presents results on a variety of datasets its unclear how well the proposed method generalizes across a broader range of domain adaptation scenarios beyond semisupervised tasks.  Lack of Comparison with StateoftheArt Methods: The paper compares its approach with standard OT methods but does not benchmark against the latest stateoftheart domain adaptation techniques. Questions and Feedback: 1. Generalization Across Diverse Domains: Could the authors provide more insight into the algorithms performance across vastly different domains not covered in the paper 2. Impact of Adversarial Stability: How does the stability of adversarial attacks impact the performance and reliability of the source fiction algorithm in domain adaptation 3. Integration with Other Techniques: Could this approach be synergistically combined with other domain adaptation or adversarial training methods for enhanced performance Feedback: To strengthen the paper it would be beneficial to incorporate a broader comparison with recent domain adaptation methods delve deeper into the practicality and computational demands of the approach and explore potential limitations or challenges in more diverse application scenarios.  3) Review Summary: The paper presents a novel and theoretically sound approach to semisupervised domain adaptation by innovatively leveraging the cycle monotonicity of adversarial attacks in optimal transport maps. The \"source fiction\" algorithm demonstrates significant improvements in domain adaptation tasks across various datasets. However the paper could benefit from a more comprehensive discussion on the methods broader applicability integration with existing techniques and benchmarking against the latest advancements in the field. Providing additional insights into these aspects would bolster the papers contribution and pave the way for exciting future research directions. Overall the proposed method is a valuable addition to the domain adaptation literature offering a unique angle through the incorporation of adversarial attacks cycle monotonicity.", "q1QmAqT_4Zh": "Summary of the Paper The paper presents a novel approach in the domain of offline reinforcement learning (RL) addressing the common pitfalls of current algorithms that tend to overfit to the training dataset and underperform in outofdistribution scenarios. To overcome these limitations the paper introduces a methodology leveraging Koopman spectral theory to learn latent representations that infer the symmetries of a systems underlying dynamics. This knowledge enables the extension of static offline datasets by generating augmented data that adheres to the identified symmetries\u2014effectively simulating an exploration of the environment\u2019s phase space without requiring direct interaction. This approach termed Koopman Forward (Conservative) Qlearning (KFC) is a modelfree Qlearning algorithm augmented with data derived from inferred system symmetries. Theoretical justifications for the effectiveness of KFC are provided along with empirical evaluations on benchmark offline RL tasks from D4RL Metaworld and Robosuite datasets where KFC demonstrated consistent improvement over stateoftheart Qlearning methods. Strong Points 1. Innovative Approach: The paper explores a novel idea of using Koopman spectral theory for data augmentation in offline RL distinguishing itself by addressing the challenges of overfitting and poor generalization in existing algorithms. 2. Theoretical Foundation: The thorough theoretical examination and justification for employing Koopman theory to identify system symmetries and extend offline datasets add depth and credibility to the proposed method. 3. Empirical Validation: Comprehensive empirical results across several benchmarks attest to the practical effectiveness of the KFC framework substantiating the theoretical claims with tangible performance improvements. 4. Generality and Applicability: By proving the concept across different datasets and environments the paper showcases the broad applicability of the proposed method in offline RL contexts. Weak Points 1. Complexity and Accessibility: The high level of technical and theoretical rigor while a strength could pose accessibility issues for readers not wellversed in Koopman theory or the specificities of spectral methods in control systems. 2. Assumptions and Limitations: The reliance on differentiable state transitions and the Bilinearisation Ansatz may limit the applicability in environments with discontinuities or nondifferentiable dynamics even though empirical results suggest otherwise. 3. Computational Demand: Though not explicitly discussed the implementation of such an approach particularly the learning of the Koopman latent space and the generation of symmetrybased augmentations might be computationally intensive impacting practical deployment. Questions and Feedback to the Author 1. Clarification on Practical Implications: The paper could benefit from a clearer discussion on the computational demands of implementing the KFC algorithm in practice especially compared to existing offline RL methods. 2. Robustness to NonDifferentiable Dynamics: How does the KFC framework adapt to environments where the assumption of differentiability is violated Are there specific modifications or considerations in such scenarios 3. Extensibility: Could the authors elaborate on possible extensions of the KFC framework beyond Qlearning algorithms possibly to other types of modelfree or modelbased RL methods 4. Future Directions: What are the authors views on the potential limitations of employing Koopman theory in the context of more complex dynamical systems such as those involving chaotic behaviors or underactuated systems Review Summary The paper presents a significant advancement in the field of offline RL through a wellfounded and empirically validated methodology leveraging Koopman spectral theory for data augmentation. The KFC algorithm addresses key limitations of existing approaches by enabling augmented dataset generation based on system symmetries thus avoiding overfitting and enhancing outofdistribution generalization. While the work is theoretically rigorous and supported by strong empirical results discussions around computational efficiency adaptability to nondifferentiable dynamics and potential extensions could further strengthen the paper. Additionally exploring the scalability of the proposed method in more complex or chaotic environments would be a valuable direction for future research. Overall this work offers a promising new avenue for offline RL research with the potential for widespread applicability and significant impact on realworld applications where safety and efficiency are paramount.", "k9bx1EfHI_-": " Peer Review of the Paper on Automated Seizure Detection and Classification Using GNN  1. Summary The paper proposes a novel approach for automated seizure detection and classification using electroencephalography (EEG) data through the integration of Graph Neural Networks (GNNs) and selfsupervised pretraining. Addressing three main challenges in prior works  the nonEuclidean data structure classification of rare seizure types and lack of quantitative interpretability  the study introduces two EEG graph structures to capture the spatial and dynamic connectivity of EEG data. Additionally it leverages a selfsupervised pretraining method to improve performance on rare seizure types and proposes a quantitative interpretability technique to assess the models ability to localize seizures. Tested on a large public dataset this approach outperforms existing methods in seizure detection and classification particularly enhancing accuracy for rare seizure types and providing significant improvements in the localization of focal seizures. The paper also underscores the potential clinical utility of the approach by enabling intuitive visualization of localized seizure regions.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The integration of GNNs with selfsupervised pretraining to address the nonEuclidean nature of EEG data and improve seizure classification is highly innovative.  Enhanced Performance on Rare Seizure Types: The significant improvement in classifying rare seizure types addresses a critical gap in existing seizure detection methodologies.  Quantitative Interpretability: The introduction of a quantitative method to assess model interpretability especially in localizing seizures is a notable advancement contributing to the clinical applicability of automated seizure detection models.  Comprehensive Evaluation: The use of a large public dataset for evaluation and the comparison with existing methods provide a thorough and convincing validation of the proposed model\u2019s superiority. Weak Points:  Generalizability Concerns: The studys evaluation is conducted on a single albeit large public dataset. Additional testing on diverse datasets would strengthen the case for the models generalizability across different EEG data sources.  Model Complexity and Computational Requirements: The complexity of GNNs especially when combined with selfsupervised pretraining may result in higher computational costs. The paper should discuss computational efficiency and the practicality of deploying this model in realworld clinical settings. Questions and Feedback:  Extension to Other Neurological Disorders: How adaptable is the proposed model for detecting and classifying other neurological disorders using EEG or other types of medical timeseries data  Impact of Electrode Placement Variation: How does variation in electrode placement affect the performance of the proposed EEG graph structures and how could potential inconsistencies be mitigated  LongTerm Clinical Integration: What are the anticipated challenges in integrating this model into routine clinical practice and how could these challenges be addressed  3. Review Summary The paper presents a groundbreaking approach combining graphbased modeling with selfsupervised pretraining for automated seizure detection and classification using EEG data. By effectively addressing critical challenges in existing models such as the representation of nonEuclidean EEG data structures and the accurate classification of rare seizure types the proposed method significantly outperforms current stateoftheart methods. The introduction of a quantitative interpretability analysis to evaluate the models ability to localize seizures further highlights its potential clinical utility promising advancements in seizure diagnosis and treatment strategies. However the study would benefit from additional examination of the model\u2019s generalizability across diverse EEG datasets and further discussion on its computational efficiency. Addressing these points could strengthen the argument for the widespread adoption of this model in clinical settings. This paper makes a substantial contribution to the field of automated seizure detection and classification with promising implications for enhancing patient care through more accurate and efficient diagnostic tools.", "kj8TBnJ0SXh": "1) Summarize the paper This paper introduces FaceDet3D a novel method for generating and rendering photorealistic facial geometric details consistent with various expressions from a single image. Traditional 3D Morphable Models (3DMMs) often fail to capture the fine details of facial expressions due to their reliance on Principal Component Analysis (PCA)based representations. FaceDet3D addresses this limitation by hallucinating facial details such as wrinkles through a vertex displacement map. This map is then utilized by a neural renderer to output images in any desired expression and viewpoint. The process leverages a detail hallucination network and a rendering network conditioning the rendered details on the 3D face geometry to ensure fidelity to the target expression. The model is trained on a combination of inthewild image datasets and controlled condition video datasets without the need for 3D data as supervision. The paper demonstrates the effectiveness of FaceDet3D in producing highquality renderings of facial expressions surpassing existing methods like DECA in realism and detail preservation. 2) Strong and weak points Questions and Feedback Strong Points:  The approach uniquely combines detail hallucination with neural rendering creatively overcoming the limitations of existing 3DMMs and neural rendering technologies. This enables more detailed and expressive facial renderings than previously possible.  Utilization of adversarial losses and weak supervision for training without groundtruth 3D data is innovative significantly broadening the potential for application in various fields where detailed 3D datasets are scarce.  The introduction of Augmented Wrinkle Loss and Detailed Shading Loss to ensure the fidelity and consistency of rendered details based on the underlying geometry is both a novel and impactful contribution. This ensures that the generated images maintain realism across different expressions. Weak Points:  The method might struggle with scenes where occlusions are present such as glasses or heavy makeup as mentioned. This could limit the applicability in realworld scenarios where such occlusions are common.  The reliance on FDS for the initial detail map and geometry extraction could introduce limitations based on the performance and accuracy of FDS especially in challenging conditions.  The evaluation mostly focuses on facial expressions and does not thoroughly investigate the performance on identity preservation across a wide range of ethnicities and ages which could affect its universality. Questions and Feedback:  How does the model perform in scenarios with significant occlusions and are there plans to enhance its robustness in such cases  Could incorporating more diverse datasets including those with varied ethnic backgrounds and ages improve the models universality and performance across a broader spectrum of faces  Have the authors considered the realtime application possibilities of FaceDet3D and if so what optimizations might be necessary to achieve realtime performance 3) Review Summary The paper presents a significant advancement in the generation and rendering of facial expressions by addressing the shortcomings of existing 3D Morphable Models and neural rendering methods. Through the innovative use of a detail hallucination network combined with a rendering network that takes geometric details into account FaceDet3D achieves highquality photorealistic facial detail rendering that surpasses current methods in realism and detail accuracy. The introduction of Augmented Wrinkle Loss and Detailed Shading Loss is a valuable contribution that ensures the rendered images accurately reflect the underlying facial geometry. However the methods limitations in handling occlusions and its reliance on the Facial Details Synthesis (FDS) system for initial detail and geometry extraction highlight areas for future improvement. To enhance the robustness and applicability of FaceDet3D addressing these weaknesses will be crucial. Furthermore examining the system\u2019s performance across a more diverse set of faces and exploring its potential for realtime applications are promising directions for future work. Overall the paper makes a compelling case for the effectiveness of FaceDet3D in generating and rendering expressive realistic facial details marking a notable progress in the field of computer graphics and facial expression rendering. The authors are encouraged to continue refining the model particularly in areas of occlusion handling and diversity inclusion to broaden its applicability and impact.", "jxTRL-VOoQo": " Review of \"Investigating the Limitations and Potential of Deep GNNs through Systematic Experiments\"  1. Summary This paper offers a thorough examination of the challenges that impede the development of deeper Graph Neural Networks (GNNs) and proposes a novel approach Deep Graph MultiLayer Perceptron (DGMLP) to overcome these challenges. By dissecting the dual aspects of GNN architecture\u2014propagation depth (Dp) and transformation depth (Dt)\u2014the authors identify the root cause of performance degradation in deep GNNs. Through comprehensive experiments the paper distinguishes between the oversmoothing problem caused by excessive Dp and the model degradation problem caused by excessive Dt. The authors introduce a nodeadaptive combination mechanism and incorporate skip connections to mitigate these issues culminating in the development of DGMLP. The proposed model exhibits impressive performance across various datasets demonstrating improvements in accuracy flexibility scalability and efficiency compared to existing stateoftheart GNN models.  2. Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Evaluation: The paper provides a thorough experimental analysis that not only clarifies the misconceptions regarding the oversmoothing issue but also identifies the critical challenge of model degradation due to large Dt.  Innovative Solution: The DGMLP model with its nodeadaptive weighting mechanism and inclusion of skip connections is a novel contribution that effectively addresses the limitations of current GNN designs.  Extensive Experimentation: The evaluation of DGMLP across seven realworld datasets and its comparison with numerous stateoftheart GNNs establishes its superiority in performance scalability and efficiency. Weak Points:  Limited Attention to Theoretical Insights: While the experimental analysis is thorough the paper could benefit from deeper theoretical insights into why the proposed solutions work particularly from a mathematical viewpoint.  Generalization Concerns: The paper predominantly focuses on semisupervised node classification tasks. It remains unclear how DGMLP would perform on other graphrelated tasks such as graph classification or link prediction.  Complexity and Implementation Details: The complexity analysis of DGMLP and detailed implementation guidelines for practitioners are missing. Additionally discussing the tradeoffs involved in choosing the temperature parameter T for the softmax function in the nodeadaptive weighting mechanism would be beneficial. Questions and Feedback:  Could the authors provide more theoretical justification for the effectiveness of DGMLP Specifically how do skip connections and the nodeadaptive weighting mechanism collectively work to overcome the limits of Dp and Dt  How does DGMLP perform on graph tasks other than node classification and what modifications if any would be required for such tasks  The choice of the temperature parameter T is crucial for the weighting mechanism. Could the authors discuss the sensitivity of DGMLPs performance to different values of T and provide recommendations for selecting T  3. Review Summary The paper presents a significant contribution to the field of graph neural networks by systematically delineating the problems hindering the development of deeper GNNs and offering a wellfounded solution through the DGMLP model. The experimental evidence strongly supports the superiority of DGMLP in terms of accuracy flexibility scalability and efficiency over existing methods. However the paper would benefit from augmenting its theoretical grounding exploring the models applicability to a wider range of graph tasks and providing more details on the practical implementation of the proposed techniques. Despite these minor shortcomings the paper stands out for its empirical insights innovative approach to tackling the challenges of deep GNNs and the introduction of a highly effective model making it a valuable contribution to the community.", "zhynF6JnC4q": " Review of \"Adaptive Qlearning for Offline and Online Reinforcement Learning\"  Summary: The paper presents a novel framework Adaptive Qlearning aimed at unifying offline and online reinforcement learning (RL) to overcome the limitations associated with each approach independently. Conventional online RL methods suffer from the need for extensive interactions with the environment which is impractical in scenarios where such interactions are expensive or limited. Offline RL although it circumvents this issue by learning from a logged dataset often results in suboptimal performance due to the quality of the data and the pessimistic update schemes commonly employed. Adaptive Qlearning addresses these challenges by implementing a dual strategy: employing a pessimistic update for data derived from the offline dataset and a greedy or no pessimistic update for data acquired through online interactions. This adaptive approach allows for efficient learning even with limited online interactions and suboptimal offline datasets. The paper also introduces a GreedyConservative Qensemble Learning algorithm (GCQL) that demonstrates the practical application of the framework showing superior performance in empirical tests.  Strong Points: 1. Innovative Approach: The paper\u2019s strategy to merge offline and online RL methods by adaptively updating Qvalues based on the data source is novel and creatively addresses the limitations of both approaches. 2. Comprehensive Experiments: The authors have conducted extensive experiments on continuous control tasks from the MuJoCo environment providing a strong empirical basis for the claims of improved performance. 3. Theoretical and Practical Contributions: The development of the Adaptive Qlearning framework accompanied by the practical GCQL algorithm offers both theoretical insight and a tangible toolset for researchers and practitioners in the RL field.  Weak Points: 1. Generalization Concerns: The paper focuses predominantly on continuous control tasks from the MuJoCo environment. It remains unclear how well the proposed method generalizes to other types of environments particularly discrete action spaces or environments with different characteristics. 2. Implementation Details: While the paper introduces the GCQL algorithm and discusses its components more detailed insights into the implementation specifics would be beneficial for reproducibility and further exploration by the research community. 3. Analysis of Adaptive Scheme: The paper would benefit from a deeper analytical discussion on how the adaptive updating scheme specifically impacts the learning process over time particularly in scenarios with varying qualities of offline datasets.  Questions and Feedback to the Author: 1. Generalization: Can the authors provide insights or preliminary results on how the Adaptive Qlearning framework performs in nonMuJoCo environments or tasks with discrete action spaces 2. Implementation Clarity: Could the authors elaborate on the specifics of implementing the GCQL algorithm especially regarding hyperparameter choices and tuning strategies 3. Adaptive Scheme Analysis: Could the authors provide a more detailed analysis or discussion on the mechanism through which the adaptive updating scheme optimizes learning especially when dealing with lowquality offline datasets  Review Summary: The paper \"Adaptive Qlearning for Offline and Online Reinforcement Learning\" introduces a promising framework that effectively merges the strengths of offline and online RL to mitigate their respective weaknesses. By adopting an adaptive approach to Qvalue updates the framework demonstrates significant potential for efficient learning in scenarios where online interactions are costly and offline datasets are suboptimal. Expansive experimental validation underscores the frameworks effectiveness although further exploration into its generalizability and a more indepth implementation guide would enhance its contribution. Given its innovative approach and the empirical evidence provided the paper stands as a valuable addition to the reinforcement learning field meriting consideration for publication with minor revisions focused on broadening its applicability and providing greater clarity on implementation details.", "qZNw8Ao_BIC": " Summarize the Paper The paper presents a novel approach to enhancing the robustness of Vision Transformers (ViTs) by identifying and mitigating their reliance on nonrobust features through patchbased transformations. Unlike traditional methods that utilize semanticpreserving transformations for data augmentation this work explores \"negative\" data augmentation wherein patchbased operations that disrupt the semantics of images are employed. The key hypothesis is that ViTs depend on certain features that survive these transformations but these features are not robust under distribution shifts. By incorporating these negatively augmented views into training alongside regular loss functions designed to discourage reliance on nonrobust features the authors demonstrate notable improvements in the robustness of ViTs across several ImageNetbased benchmarks. Enhanced robustness is achieved without compromising indistribution accuracy and the proposed method is shown to be complementary to existing data augmentation techniques indicating a new direction for future exploration in model robustness.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Approach: The exploration of negative data augmentation as a method to improve model robustness represents a significant departure from the more common positive augmentation techniques providing a fresh perspective on the development of robust vision models. 2. Comprehensive Evaluation: The paper provides an extensive evaluation of the proposed methodology across a wide range of robustness benchmarks clearly demonstrating its effectiveness in improving outofdistribution (OOD) performance. 3. Practical Implications: By showing that the robustness improvements are complementary to existing data augmentation strategies the paper lays the groundwork for practical applications and further optimizations in the training of robust vision models. Weak Points: 1. Understanding of NonRobust Features: While the study identifies that ViTs rely on certain nonrobust features it does not delve deeply into the nature of these features or why ViTs are predisposed to utilize them. A more thorough analysis could enhance the understanding of ViTs behavior. 2. Scope of PatchBased Transformations: The paper focuses on a specific set of patchbased transformations. Investigating a broader range of transformations or combining them with other types of negative augmentations could potentially yield even better robustness improvements. 3. Limitations on More Complex Datasets: The experiments are primarily conducted on ImageNet and its variants. The effectiveness of the proposed method on more diverse or complex datasets remains an open question. Questions and Feedback: 1. Can the authors elaborate on the nature of the nonrobust features that ViTs rely on possibly with some visual examples 2. Have the authors considered testing their methodology on datasets with higher complexity or variance to assess the generalizability of their findings 3. It would be interesting to see if combining the proposed negative augmentations with other forms of augmentation or regularization techniques could synergistically improve robustness further.  Review Summary The paper introduces a pioneering exploration into improving the robustness of Vision Transformers by leveraging negative data augmentation strategies. This method underscores the potential weaknesses in ViTs that stem from their reliance on certain nonrobust features offering a novel way to mitigate these vulnerabilities. The extensive experimental evidence supports the effectiveness of the approach showing significant improvements in model robustness across diverse benchmarks. Nevertheless the work could benefit from a deeper investigation into the characteristics of the nonrobust features that ViTs exploit as understanding this aspect could unlock further enhancements in model design and training strategies. Expanding the scope of patchbased transformations and testing on more varied datasets could also help in assessing the limits and full potential of the proposed methodology. Overall the paper represents a significant contribution to the field opening up new avenues for research in model robustness and highlighting the importance of considering alternative data augmentation approaches. This work should inspire future investigations into not only improving the robustness of ViTs but also exploring the underlying mechanisms that govern model behaviors under various transformations.", "g6UqpVislvH": " Summarize the Paper The paper presents a novel approach to positional encoding for learning highfrequency functions with multilayer perceptrons (MLPs) on nonEuclidean manifolds extending the concept of Fourier Features beyond traditional Euclidean spaces. The authors propose using orthonormal basis functions that reflect the geometry of the underlying manifold to encode input coordinates arguing that this method allows for a more accurate representation of functions on such spaces. The paper discusses the mathematical foundation for selecting appropriate basis functions for various types of manifolds including spheres rotation manifolds (SO(3)) and products of spheres. Experiments demonstrate the methods effectiveness in diverse applications such as learning spherical panoramas probability distributions on SO(3) neural radiance fields and light field rendering showing advantages over traditional Euclidean encodings. The paper also addresses limitations related to the selection of basis elements and computational overhead in some cases.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The paper introduces a significant advancement in positional encoding by generalizing Fourier Features to nonEuclidean manifolds a domain not fully explored in previous literature. 2. Theoretical Foundation: The authors provide a strong theoretical basis for their method including the use of orthonormal basis functions and how these relate to the neural tangent kernel (NTK) theory ensuring shiftinvariance and enabling the approximation of convolutions on the manifold. 3. Broad Applicability: The method is demonstrated across various tasks and manifolds highlighting its versatility and potential for wideranging applications. 4. Quality of Experiments: The experiments are comprehensive and welldesigned effectively demonstrating the benefits of the proposed method over traditional Euclidean encodings. Weak Points: 1. Computational Overhead: The authors mention the computational expense of evaluating special functions for the positional encoding which may limit the methods applicability in practice especially for tasks with large input spaces or those requiring realtime performance. 2. Tuning of Hyperparameters: Similar to issues in existing encoding methods selecting the maximum frequency (or degree) involves a tradeoff between overfitting and underfitting requiring careful tuning in practice. Questions and Feedback: 1. Scalability and Efficiency: Could the authors elaborate on potential ways to mitigate the computational overhead associated with their method Are there approximation techniques or optimizations that could make the approach more practical for largescale applications 2. Comparison with Other Manifold Learning Techniques: How does the proposed positional encoding technique compare with other existing methods specifically designed for learning on nonEuclidean manifolds A direct comparison might help in understanding the relative advantages in terms of accuracy and computational requirements. 3. Adaptation to Dynamic Manifolds: Would the proposed method adapt efficiently to scenarios where the underlying manifolds properties change over time such as in dynamic systems or environments If not suggestions on how it might be extended or modified for such contexts would be valuable.  Review Summary The paper presents a groundbreaking approach to positional encoding for learning on nonEuclidean manifolds effectively extending the concept of Fourier Features beyond Euclidean spaces. The theoretical foundation is solid and the broad range of applications demonstrated points to the methods versatility and potential for innovation in various domains. However the computational overhead and the need for careful tuning of hyperparameters present challenges to practical implementation. Addressing these limitations possibly through computational optimizations or adaptive methods for selecting basis elements could enhance the methods applicability. Overall the novel approach and significant experimental results support a strong case for the further development and application of this technique in the field.", "lL3lnMbR4WU": "Summarize the Paper: This paper presents \"ViLD\" a novel method for openvocabulary object detection that leverages knowledge distillation from pretrained openvocabulary image classification models. This approach addresses the challenge of expanding the detection vocabulary without the need to scale up data collection and annotation effort significantly. ViLD involves vision and language knowledge distillation where knowledge from a pretrained image classifier (teacher) is used to train a twostage object detector (student). This process improves the detection of novel object categories described by arbitrary text inputs. The method demonstrates remarkable performance on the challenging LVIS dataset and shows significant improvements over existing approaches when evaluated on several benchmark datasets including COCO and PASCAL VOC. Strong Points: 1. Innovative Approach: The combination of vision and language knowledge distillation for openvocabulary object detection is innovative. It capitalizes on the strengths of pretrained image classifiers and aligns them with the detection goals thus bridging the gap between image classification and object detection. 2. Outstanding Performance: The proposed method not only surpasses the performance of supervised counterparts for novel categories but also showcases remarkable transferability across different datasets without the necessity for finetuning. 3. Comprehensive Evaluation: The evaluation is thorough involving multiple datasets which substantiates the methods effectiveness and robustness. Additionally the performance comparison with the stateoftheart provides a clear understanding of the methods advancements. 4. Open Source Contribution: By releasing the code and pretrained models the authors contribute to the communitys ability to reproduce validate and extend their work fostering further research and development in openvocabulary object detection. Weak Points: 1. Inference Efficiency: Despite its impressive performance the methods inference speed especially when employing the ensembling approach (ViLDtextCLIP) is flagged as slow and impractical for realworld applications. This limitation could hinder its adoption for timesensitive applications. 2. Competition Between Objectives: The paper identifies a competition between the ViLDtext and ViLDimage components suggesting a balance that may not always yield the optimal outcome. More insight into mitigating this issue would be beneficial. 3. Limited Discussion on Failures: While the paper discusses the methods strengths there is less focus on understanding under which conditions or scenarios ViLD might fail or underperform which is crucial for practical applications. Questions and Feedback to the Author: 1. Inferring Strategy Efficiency: Could you elaborate on potential strategies or ongoing work to improve the inference speed while possibly maintaining the models high performance 2. Balancing Objectives Competition: Can you provide more insights on how to effectively balance the competition between ViLDtext and ViLDimage ensuring that both contribute positively to the detection performance 3. Failure Case Analysis: It would be insightful to include some analysis or discussion on the limitations or failure cases of ViLD as understanding these could provide directions for future improvements. Review Summary: This paper introduces ViLD a compelling method for openvocabulary object detection that creatively employs knowledge distillation from pretrained image classification models. The novel approach addresses a significant challenge in expanding detection vocabularies without extensive data collection and annotation efforts. ViLD demonstrates superior performance on various benchmarks notably outperforming supervised counterparts for novel category detection and showcasing effective transferability to different datasets. Despite these strengths the paper identifies areas needing improvement such as the inference efficiency of the method especially when ensembling is involved. Additionally the competition between the text and image components of ViLD poses a challenge that could impact optimization and performance outcomes. The paper could benefit from a deeper examination of these aspects offering strategies for enhancement and a clearer understanding of the methods limitations or conditions under which it may underperform. In conclusion the innovative approach combined with the impressive performance results and opensource contributions positions ViLD as a significant advancement in the field of openvocabulary object detection. Future work focusing on improving efficiency balancing method components and thoroughly understanding limitations will further enhance its usefulness and applicability in realworld scenarios.", "rw1mZl_ss3L": " 1. Summary The paper introduces Concurrent Adversarial Learning (ConAdv) a novel method designed to enable largebatch training for neural networks without relying on extensive data augmentation which becomes less effective as batch size increases. The motivation behind this study emerges from the challenge that largebatch training tends to lead to convergence at sharp local minima adversely affecting test performance. Traditional attempts to mitigate this issue through data augmentation face diminishing returns beyond certain batch sizes. Adversarial learning presents an alternative by smoothing the decision surface encouraging convergence to flatter minima. However its practical application has been limited due to the need for sequential gradient computations which significantly slows down the training process. ConAdv addresses this by using stale parameters to allow parallel processing of gradients effectively bypassing the bottleneck of sequential computation. The authors demonstrate the efficacy of ConAdv on the ResNet50 model trained on the ImageNet dataset achieving high accuracy with unprecedented batch sizes up to 96K.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Novelty of Approach: The introduction of ConAdv as a solution to enable adversarial learning in largebatch settings without the limitations of sequential gradient computation is innovative. It opens new avenues for efficiently leveraging computing resources for neural network training. b. Comprehensive Experimental Validation: The paper provides extensive experimental results supporting the effectiveness of ConAdv. The experiments are welldesigned comparing ConAdv with traditional methods and demonstrating its advantages in scaling batch size while maintaining accuracy. c. Theoretical Analysis: The theoretical underpinning provided for ConAdv contributes to a deeper understanding of why and how the method improves largebatch training. This analysis strengthens the papers contribution to the field. Weak Points: a. Generalization Beyond ResNet50 and ImageNet: While the results for ResNet50 on ImageNet are impressive the paper lacks exploration of ConAdvs applicability to other architectures or datasets. This raises questions about the methods generalizability. b. Comparison with Latest Techniques: The paper does not compare ConAdv with the most recent techniques in adversarial training and largebatch training optimizations making it difficult to assess its relative performance fully. c. Analysis of Computational Overhead: Although the paper emphasizes ConAdvs efficiency improvements there is insufficient discussion on the computational overhead introduced by using stale parameters and potential impacts on convergence behavior. Questions and Feedback: 1. Can the authors provide insights into how ConAdvs effectiveness varies across different network architectures and datasets 2. How does the computational overhead of ConAdv compare to traditional adversarial training methods especially concerning the use of stale parameters 3. The paper briefly mentions the setting of `\u03c41` for stale parameters. Could the authors elaborate on how different values of `\u03c4` affect performance and computational efficiency 4. A deeper analysis of ConAdvs limitations and potential areas for improvement could enhance the paper. Are there scenarios where ConAdv might not provide the expected benefits and how could these be addressed  3. Review Summary with Supporting Arguments The paper makes a significant contribution to the field of machine learning by addressing a critical challenge in largebatch training  the convergence to sharp local minima which degrades test performance. By introducing ConAdv the authors propose an inventive solution that facilitates the application of adversarial learning in largebatch scenarios thus enabling the effective use of largescale computational resources. The experimental results showcasing ConAdvs ability to scale ResNet50 training on ImageNet to a batch size of 96K while maintaining high accuracy are particularly compelling. These findings are wellsupported by both empirical evidence and theoretical analysis establishing the methods efficacy convincingly. However the scope of experiments could be expanded to include a broader range of architectures and datasets to fully understand ConAdvs applicability and potential limitations. Furthermore a more detailed comparison with contemporary methods and a thorough examination of the computational overhead introduced by ConAdv would provide a more comprehensive assessment of its benefits and drawbacks. Overall the paper brings forward a promising approach for largebatch training addressing a timely and relevant challenge in the field. The novelty of the method combined with substantial experimental validation forms a solid basis for the papers contributions. Yet further exploration of its generalizability and efficiency in broader contexts would enhance its impact.", "qiukmqxQF6": " Peer Review for \"Latent Temporal Flows (LatTeFlows) for Probabilistic Multivariate TimeSeries Analysis\"  1) Summary of the Paper: This paper introduces a novel approach Latent Temporal Flows (LatTeFlows) for probabilistic forecasting in multivariate timeseries data particularly focusing on highdimensional datasets. The method hinges on the concept of reducing the dimensionality of the data by learning a lowdimensional latent representation (latent vector) that captures the essential dynamics driving temporal changes. LatTeFlows employs a combination of autoencoder mappings and Normalizing Flows to learn the temporal distribution of these latent embeddings from input sequences. The approach focuses on joint learning of dynamics in a discriminative subspace enabling efficient uncovering of temporal transitions and subsequent direct transfer of learned latent effects back into the observed data space through a decoder network. The technique is validated through extensive experiments on several realworld datasets and a challenging sensorsignal forecasting task using measurements from wearable devices demonstrating superior performance over existing stateoftheart methods in multistep forecasting benchmarks while also offering computational efficiency.  2) Strong and Weak Points Questions and Feedback: Strong Points:  Innovativeness: The paper presents a novel methodology combining autoencoders and Normalizing Flows for efficient dimensionality reduction and probabilistic forecasting in multivariate timeseries data.  Practical Value: The application to sensor signals from wearable devices showcases the methods utility in realworld health monitoring scenarios a field with significant practical implications.  Performance: The proposed method outperforms existing stateoftheart techniques in forecasting accuracy across multiple datasets demonstrating its effectiveness.  Computational Efficiency: The reduction in dimensionality allows for scaling to large datasets with high computational efficiency. Weak Points:  Generality: The paper focuses predominantly on healthrelated applications. It would be beneficial to see the performance of LatTeFlows in other domains to assess its generality.  Model Interpretability: While latent space representations are mentioned as interpretable the paper could further elaborate on how these interpretations are derived and their implications.  Reproducibility and Accessibility: The significant results based on the proprietary AHMS dataset could limit the ability of the broader research community to reproduce and validate the findings fully. Questions and Feedback:  Question: How does the method handle irregular time series or missing data scenarios which are common in realworld datasets  Feedback: Expanding the experiments to include datasets from various fields outside of healthcare would help demonstrate the applicability and robustness of the method across different domains.  Feedback: It would be valuable to include more detailed discussions on the interpretability of latent representations providing examples of how these interpretations can influence realworld decisions or applications.  3) Review Summary and Recommendation: The paper presents a compelling and novel approach Latent Temporal Flows (LatTeFlows) for multivariate timeseries forecasting demonstrating significant advancements in terms of accuracy and computational efficiency. The methods application to realworld sensor data from wearable devices is timely and exhibits its practical value particularly in health monitoring scenarios. However a broader demonstration across various domains and a more indepth discussion on model interpretability could enhance its impact. Given its innovative contributions performance benefits and relevance to pressing realworld applications I highly recommend the publication of this paper. It offers substantial advancements to the field of timeseries analysis and has the potential to inspire further research in probabilistic forecasting methods. Future work could focus on broadening the application domains enhancing model interpretability and addressing data irregularities which would further solidify the methods utility and robustness.", "x4tkHYGpTdq": " Review of \"Dually SparsityEmbedded Efficient Tuning (DSEE) for PreTrained Language Models\"  1) Summary of the Paper This paper introduces the Dually SparsityEmbedded Efficient Tuning (DSEE) framework designed to address the resource and parameter efficiency challenges faced when finetuning gigantic pretrained language models (e.g. BERT GPT2 and DeBERTa) for downstream tasks. The DSEE framework focuses on enforcing sparsity in both the weight updates during finetuning and in the final model weights to ensure resourceefficient inference. By exploiting both unstructured and structured sparsity patterns through magnitudebased pruning and L1 sparse regularization the proposed method effectively reduces the computational resources needed for finetuning and inference while maintaining or even improving performance on various NLP benchmarks. The paper provides extensive experimental results demonstrating the efficiency and effectiveness of DSEE across different models and tasks.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper presents a novel framework that simultaneously addresses parameter efficiency during finetuning and resource efficiency during model inference which is highly relevant in the current landscape of evergrowing pretrained models.  Extensive Experiments: The authors conducted comprehensive experiments across multiple models and datasets providing strong empirical evidence of the benefits of their approach.  Significant Efficiency Gains: The paper reports impressive results in terms of reduced trainable parameters and inference FLOPs savings without sacrificing downstream task performance which is a notable achievement. Weak Points:  Lack of Comparison with Latest Techniques: While the paper compares DSEE with several baselines it could further strengthen its position by including comparisons with the very latest parameterefficient techniques in NLP that may have emerged.  Potential Hardware Dependency: The benefits of unstructured sparsity are mentioned but there is a lack of discussion on the practical speedups on existing hardware. Given that irregular sparsity patterns do not always translate to realworld speedups without specialized hardware or software its a notable omission.  Limited Discussion on the Choices of Hyperparameters: While the paper states the chosen hyperparameters for experiments there isn\u2019t much discussion on how these were selected or their sensitivity which could be valuable for replicability and further research. Questions and Feedback:  Could the authors provide insights into how DSEE would perform or be adapted for emerging giant models like GPT4 or other future models  It would be beneficial if the authors could discuss the realworld applicability of the unstructured sparsity improvements especially considering the limitations of current hardware to exploit these gains.  A deeper exploration or discussion on how the choice of hyperparameters (e.g. rank for lowrank decomposition and the sparsity level) impacts the models performance and efficiency could provide valuable guidance for practitioners looking to apply DSEE.  3) Review Summary The paper makes a significant contribution to the field of efficient finetuning for large pretrained models in NLP addressing both parameter and resource efficiency through the proposed DSEE framework. The comprehensive experiments and reported gains in both parameter reduction and inference efficiency without compromising performance are noteworthy. However this work could be further strengthened by expanding the comparison to the very latest techniques providing more hardware applicability insights and offering a more detailed discussion on the impact of hyperparameter choices. Overall the novel approach of DSEE represents a meaningful step forward in making finetuning of large models more accessible and efficient for a wider range of applications and deserves recognition for its contribution to reducing the computational barriers associated with stateoftheart NLP models.", "qPQRIj_Y_EW": " Peer Review of \"Solving the Order Fulfillment Problem in Online Retailing with Machine Learning\"  1. Summary The paper presents a novel machine learning framework aimed at addressing the realtime order fulfillment problem prevalent in online retailing. This combinatorial optimization problem pivotal for supply chain management demands a costeffective assignment of online customer orders to warehouses while considering multiple constraints such as inventory levels delivery costs and saleforbidden dates for items. The problem inherently NPhard is challenging to solve with exact mathematical programming methods due to computational infeasibility especially for largescale instances. To tackle this the authors propose using a machine learning method centered around a tripartite graph representation enabling the learning of the best assignment policy via a novel edgefeatureembedded Graph Attention Network (GAT). This approach notably considers highdimensional edge features and heterogeneous information critical to the problem at hand. The proposed model showcases size invariance effectively handling problem instances of any scale and demonstrates the ability to address completely unseen cases during training. Experimental results indicate that the model significantly outperforms existing heuristic methods in terms of optimality and computational efficiency offering solutions in milliseconds.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The application of a machine learning approach particularly the creation of a tripartite graph and utilization of edgefeatureembedded GAT for solving the realtime order fulfillment problem is highly innovative and addresses the computational inefficiency of traditional methods.  Comprehensive Testing: The methodology involved thorough experiments comparing the proposed model against traditional methods (heuristics and exact MIP) and other ML approaches providing solid evidence of its effectiveness.  Practical Relevance: This research tackles a problem of significant importance in the context of online retailing offering potential for substantial cost savings and improved efficiency in supply chain management. Weak Points:  Lack of Detail on Data Preparation: The paper could provide more insights into how the datasets were curated especially considering the models reported ability to generalize to unseen problem sizes.  Limited Discussion on Scalability: While the model is touted as sizeinvariant theres minimal discussion on its scalability concerning extremely large datasets which are common in industry applications.  Algorithm Complexity: The complexity of the proposed ML model could be a barrier to understanding and implementation by practitioners. A more detailed explanation or stepbystep breakdown could make the approach more accessible. Questions and Feedback:  How does the model perform under dynamic conditions where order and inventory information changes frequently Is there a mechanism for realtime data updating and how does it affect the models performance  It would be beneficial if the authors could elucidate more on the computational resources required for both training and inference stages of the model to better understand its practical applicability.  Offering comparisons with additional baseline ML models could enrich the paper. For instance comparing results with recent advancements in ML for combinatorial optimization could contextualize the proposed models performance better.  3. Review Summary The paper introduces a groundbreaking machine learning method to solve the realtime order fulfillment problem a critical challenge in online retailing. Through a wellconceived experimental setup the proposed model demonstrates superior performance over traditional heuristic methods and showcases promising capabilities in handling complex varyingscale problem instances efficiently. The adoption of a tripartite graph structure and an edgefeatureembedded GAT for learning assignment policies is both novel and impactful potentially setting a new direction for future research in combinatorial optimization problems in supply chain management. Nonetheless the paper could benefit from further details on data preparation scalability discussion and algorithm complexity to strengthen its contribution and applicability. Overall the paper makes a substantial contribution to the field of supply chain management and combinatorial optimization leveraging machine learning to offer practical efficient solutions where traditional methods falter. Given the impressive results and the relevance of the tackled problem this research presents a significant advancement with potential for realworld impact in online retailing logistics.", "vIC-xLFuM6": "Summarize the paper The paper presents a novel approach Fourier Feature Networks (FFN) to address the spectral bias inherent in the use of deep neural networks for value approximation in offpolicy deep reinforcement learning. The authors identify that traditional multilayer perceptron (MLP) networks underfit highfrequency components of the value function due to spectral bias leading to slower convergence and less efficient learning. By incorporating random Fourier features into the neural tangent kernel the FFN approach allows for faster convergence on higher frequency components improving sample efficiency and offpolicy learning stability. The authors demonstrate the superiority of FFN over traditional MLP networks in various continuous control domains showing faster convergence improved stability without the need for target networks and reduced computational overhead. The work is supported by theoretical analysis empirical results on challenging domains and extensive experimentation detailed in appendices. Strong and Weak Points Strong Points: 1. Innovative Solution: The application of Fourier feature networks to overcome spectral bias and improve learning efficiency in offpolicy reinforcement learning is both novel and technically sound. The approachs simplicity requiring only a single line change is particularly impressive. 2. Comprehensive Evaluation: The authors conducted extensive experiments across various continuous control domains to validate their approach. The comparative analysis against MLP baselines and the inclusion of discussion on computational efficiency bolster the papers claims. 3. Theoretical and Empirical Balance: The paper maintains a commendable balance between theoretical analysis and empirical results. The detailed background motivation through a toy MDP and subsequent kernel view provide a solid foundation for the proposed approach. Weak Points: 1. Lack of Discussion on Limitations: While the paper presents FFN as a significant improvement over existing approaches there is little discussion on the potential limitations or tradeoffs involved. Delving deeper into scenarios where FFN might underperform or its applicability across a broader range of RL tasks could provide a more nuanced understanding. 2. Sensitivity Analysis: The sensitivity of FFN performance to the choice of bandwidth parameter (b) in various domains is briefly mentioned but not thoroughly explored. A more detailed sensitivity analysis or guidelines on selecting optimal parameters could enhance the papers practical contributions. 3. Comparison with Other StateoftheArt Methods: The primary comparison in the paper is between FFN and traditional MLP networks. Including comparisons with other stateoftheart approaches in offpolicy learning could provide readers with a clearer understanding of FFNs position within the broader research landscape. Questions and Feedback to the author 1. Could you expand on the potential limitations or scenarios where FFN might not provide significant improvements over other approaches 2. How sensitive is the performance of FFN to the choice of the bandwidth parameter (b) and are there any heuristics or methods you recommend for selecting this parameter across different tasks 3. It would be beneficial to include discussions or comparisons with other stateoftheart methods in offpolicy reinforcement learning to contextualize FFNs performance. Are there plans to incorporate such analyses Review Summary The paper introduces Fourier Feature Networks (FFN) to address the challenge of spectral bias in value approximation within offpolicy reinforcement learning. By enhancing the neural tangent kernel with random Fourier features FFN achieves faster convergence improved sample efficiency and better stability across various continuous control tasks. The approachs simplicity requiring only a minor modification to existing architectures alongside comprehensive theoretical and empirical backing underscores its potential impact on deep reinforcement learning research. However the paper could benefit from a more indepth exploration of its limitations a detailed sensitivity analysis on the choice of bandwidth parameters and a broader comparison with other stateoftheart methods. Addressing these points would strengthen the papers contributions and provide clearer guidance for researchers and practitioners looking to apply FFN in different reinforcement learning contexts. Overall the proposed method represents a significant step forward in tackling spectral bias in deep learningbased value approximation with promising implications for the efficiency and effectiveness of offpolicy reinforcement learning algorithms.", "rqcLsG8Kme9": " Review of \"Regularizing Qvalue Distributions in Deep Reinforcement Learning with rQdia\"  1) Summarize the paper This paper introduces rQdia a novel approach to regularizing Qvalue distributions in deep reinforcement learning (RL) using augmented images. The core premise is that by ensuring consistency in Qvalue distributions across different state and action configurations particularly through image augmentation one can significantly enhance sample efficiency and overall performance in deep RL for pixelbased input. The method builds upon and extends concepts from earlier works such as DrQ and DataEfficient Rainbow (DER) demonstrating superior performance in both the DeepMind Continuous Control Suite and the Atari Arcade Learning Environments. The methodology revolves around comparing Qvalues for both original and augmented states applying this consistency across minibatches and integrating the resulting auxiliary loss into the existing RL framework. The paper follows with robust experimental validations showcasing significant improvements over stateoftheart methods.  2) Strong and Weak Points Questions and Feedback  Strong Points: 1. The paper addresses a critical challenge in deep RL \u2013 the data inefficiency problem \u2013 with a novel yet intuitive solution. 2. It effectively leverages concepts from both image augmentation and Qvalue distribution regularization creatively combining these elements to enhance performance. 3. The comprehensive experimental setup and adherence to statistical best practices as recommended by recent literature add credibility and robustness to the reported results.  Weak Points: 1. The paper assumes the intrinsic value of certain types of augmentation invariances such as translation which may not universally apply across different RL environments or applications. 2. It mentions potential limitations and ethical considerations but does not thoroughly explore or propose solutions to mitigate them. 3. While the paper discusses the reproducibility of their methods and intends to release the code it remains unclear how easily these methods can be implemented or adapted to different possibly more complex RL scenarios or different kinds of learning models apart from SAC and DER.  Questions and Feedback to the Author: 1. How would rQdia perform in reinforcement learning settings that significantly differ from the continuous control and game environments tested such as those with complex dynamic multiagent interactions 2. Could the authors elaborate on the potential for rQdia to adapt to environments where the assumed augmentation invariances might not hold Are there mechanisms within rQdia to identify and adapt to the most beneficial types of invariance for a given environment 3. The discussion on ethics is appreciated but brief. Can the authors propose specific guidelines or mechanisms within the RL community to ensure technologies like rQdia are used ethically  3) Review Summary This paper represents a notable advancement in addressing the data inefficiency issue in deep reinforcement learning leveraging the innovative concept of regularizing Qvalue distributions with image augmentations. The experimental evidence strongly supports the efficacy of rQdia showing significant improvements in both sample efficiency and final performance metrics over existing methods. The approach is both novel and grounded in solid experiential validation making a compelling case for its utility in enhancing deep RL applications. However the paper could benefit from a deeper exploration of the limitations and applicability of its proposed methods across varied RL environments. Moreover the discussion around ethical considerations while valuable feels somewhat cursory and warrants a more detailed examination. Overall the strength of the presented results coupled with the clear exposition and adherence to best statistical practices strongly supports the papers contributions to the field of deep reinforcement learning. With further exploration of its limitations and broader applicability rQdia holds the potential to significantly advance the efficiency and performance of RL models in realworld applications.", "qaxhBG1UUaS": " 1) Summary of the Paper The paper introduces GPTCritic an offline reinforcement learning (RL) method tailored for taskoriented dialogue systems. It capitalizes on the GPT2 model to finetune and generate conversation actions guided by a critic network that evaluates each actions potential success in achieving user goals. Unlike previous methods that either remove unsuccessful dialogues or rely on pure imitation learning GPTCritic updates its dialogue strategy by enriching the dialogue corpus with successful iterations generated by critic evaluation. This approach addresses the significant challenge of action divergence from human language by grounding the action generation in the capabilities of GPT2. The paper presents a thorough experimental evaluation showing GPTCritics superior performance over stateoftheart models on benchmark datasets like MultiWOZ 2.0 and ConvLab.  2) Evaluation Strong Points:  Innovation in Approach: The GPTCritic methodology represents a significant innovation in utilizing criticguided behavior cloning to refine dialogue strategies offering a novel solution to the divergence problem in taskoriented dialogues.  Coverage of Research Problem: The paper tackles the critical and complex issue of offline RL in NLP specifically in the vast action space of natural language for taskoriented dialogues presenting both a comprehensive problem statement and a robust solution.  Empirical Evaluation: Comprehensive experiments and comparisons with stateoftheart models demonstrate GPTCritic\u2019s effectiveness in optimizing dialogue strategies providing a solid foundation for its claims. Weak Points:  Generalization of the Model: While the model shows impressive results on specific benchmarks there is limited discussion on its generalizability across different domains or languages which is critical for realworld applications.  Computational Costs and Scalability: There is scarce information on the computational demands of training and finetuning GPTCritic an essential factor considering the known resourceintensive nature of models like GPT2.  Human Evaluation Scope: Although human evaluations are conducted the scope and depth of these evaluations could be expanded to better understand user perceptions and potential biases in generated dialogues. Questions and Feedback for the Author: 1. Generalization Capabilities: Can the authors discuss how GPTCritic might perform outside the tested benchmarks or in different languages Is there a planned approach for adapting the model to new domains 2. Computational Efficiency: Could you provide insights into the computational costs associated with GPTCritic and any strategies considered to reduce these demands for broader applicability 3. Human Evaluation: How were participants selected for human evaluations and what measures were taken to ensure diversity in their backgrounds Could expanding these evaluations provide more insights into GPTCritic\u2019s performance across different user groups  3) Review Summary The paper presents a substantial contribution to the field of taskoriented dialogue systems through the development of GPTCritic an offline RL method leveraging GPT2 for dialogue strategy optimization. The approach distinctively addresses the action space\u2019s vastness and the challenge of maintaining humanlike dialogue quality by using criticguided behavior cloning. The thorough experimental validation underscores the methods innovation and effectiveness positioning GPTCritic as a significant advancement in dialogue system research. However the paper could be strengthened by addressing a few critical areas: the models generalization capabilities across domains and languages the computational demands associated with its deployment and a more comprehensive human evaluation to understand its effectiveness across diverse user groups. Addressing these questions will provide a more rounded understanding of GPTCritic\u2019s potential impact and applicability in realworld settings offering valuable directions for future research and development in taskoriented dialogue systems.", "gULyf2IVll0": " Summarize the Paper This paper presents an empirical investigation into the internal properties of Deep Neural Networks (DNNs) and their relationship with model robustness against adversarial attacks. With a focus on the models geometric properties the authors introduce a novel concept called the Populated Region Set (PRS) to represent the internal characteristics of DNNs. PRS is defined as the set of decision regions containing at least one sample from the training dataset. The study systematically explores the link between the size of PRS and the networks robustness to adversarial attacks. Through extensive experiments across various DNN structures and datasets the authors find that a smaller PRS size correlates with greater robustness. The paper also discusses the relationship between PRS and other factors such as the similarity of parameters in the final layer the inclusion of test samples in the training PR and the sparsity of feature representation. These findings suggest that PRS can be a practical measure to analyze and improve the adversarial robustness of DNNs.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Concept: The introduction of the Populated Region Set (PRS) as a measure to investigate the internal properties of DNNs in relation to adversarial robustness is novel and insightful. It provides a new lens to assess DNNs beyond traditional metrics. 2. Comprehensive Experimental Design: The systematic experimentation with various DNN architectures and datasets adds credibility to the findings. Using PRS as a measure across different scenarios elucidates its potential as a generalizable concept in understanding DNN robustness. 3. Practical Implications: The findings have significant practical implications especially for applications where security against adversarial attacks is crucial. Identifying the PRSs role in model robustness can guide the development of more secure DNNs. Weak Points: 1. Lack of Theoretical Foundation: While the empirical findings are compelling the paper lacks a strong theoretical foundation explaining why a lower PRS correlates with higher robustness. A deeper mathematical or theoretical discussion could strengthen the claims. 2. Generalization to Other Architectures: The studied architectures (CNN6 VGG16 ResNet18) are common but the network landscape is vast. It remains unclear how PRS behaves in newer or less conventional architectures including transformers or capsule networks. 3. Potential Bias in Experimental Setup: The choice of adversarial attack methods (FGSM BIM PGD) is standard but there is a risk that the observed correlation between PRS and robustness is specifically tuned to these attack types. Evaluation against a broader range of attack vectors could address this concern. Questions and Feedback: 1. Can the authors provide a theoretical justification for the observed correlation between PRS size and adversarial robustness 2. Would incorporating other types of DNN architectures or more diverse datasets alter the conclusions drawn regarding PRS and robustness 3. How does the inclusion of regularization techniques such as dropout or weight decay impact the PRS and consequently the networks robustness  Review Summary Overall this paper offers a significant contribution by introducing the PRS concept to analyze the internal properties of DNNs concerning adversarial robustness. Its comprehensive experimental approach supports the claim that a smaller PRS correlates with higher adversarial robustness providing a novel perspective on how to evaluate and potentially enhance the security of DNNs. However to further solidify its contributions the paper would benefit from a more robust theoretical exploration of why PRS size influences robustness and an assessment of PRSs relevance across a broader spectrum of DNN architectures and adversarial attack vectors. Addressing these points would not only strengthen the findings but also broaden the papers impact on the development of robust DNN models for realworld applications.", "z8j0bPU4DIw": " Peer Review for \"Scalable Hierarchical Evolution Strategies (SHES) for Robot Locomotion and Navigation\"  Summary of the Paper This paper introduces a novel method named Scalable Hierarchical Evolution Strategies (SHES) leveraging the scalability and simplicity of Scalable Evolution Strategies (SES) in a hierarchical reinforcement learning (HRL) context. The method aims to tackle complex reinforcement learning (RL) tasks requiring both robot locomotion and navigation without necessitating methodological or hyperparameter adjustments from its flat RL applications. Through comparative analysis with stateoftheart MDPbased HRL methods in challenging environments the paper showcases SHESs competitive performance. A significant highlight is the methods ability to maintain scalability simplicity and task performance making it a robust choice for addressing complex RL tasks thus filling the gap in applying evolution strategies within an HRL framework.  Strong Points 1. Novelty and Contribution: Introducing SHES which integrates SES with HRL providing a new direction for tackling complex RL tasks is a significant contribution. This work paves the way for leveraging evolution strategies scalability and robustness in more complex hierarchical tasks. 2. Comprehensive Experimental Evaluation: The paper presents a thorough evaluation of SHES against established MDPbased HRL methods across challenging environments demonstrating its competitive performance and efficiency. 3. Clear Presentation and Rationale: The methodology is wellarticulated with a clear explanation of why SHESs components such as the primitive reward mechanism and goal encoding are chosen and how they contribute to the methods performance. 4. Scalability and Simplicity: The authors successfully demonstrate SHESs scalability (tested up to 240 cores) and simplicity making a compelling case for its applicability in realworld scenarios where these attributes are crucial.  Weak Points 1. Lack of Sample Efficiency: The paper acknowledges SHESs lower sample efficiency compared to gradientbased methods. Given the importance of sample efficiency in practical applications further work to address this limitation would be beneficial. 2. Limited Discussion on Limitations and Failures: While the paper discusses the success of SHES a more detailed exploration of its limitations potential failure modes or scenarios where SHES might not perform as well would offer a more balanced view. 3. Comparison with a Wider Range of HRL Methods: Expanding the comparison to include a broader variety of recent HRL methods could provide a more comprehensive understanding of SHESs standing within the field. 4. Detailed Analysis of Component Contributions: While the paper provides a rationale for the components of SHES a more detailed analysis or ablation study demonstrating each components specific contribution to performance would enrich the understanding.  Questions and Feedback to the Author 1. Could the authors elaborate on potential approaches to improve SHESs sample efficiency Are there any ongoing or planned investigations in this direction 2. How does SHES perform in environments with more dynamic and unpredictable elements Is there a risk of overfitting to specific types of tasks 3. Can the authors provide insights into how SHES could be integrated with other RL techniques (e.g. offpolicy corrections) to enhance its performance or address its limitations  Review Summary This paper makes a novel and significant contribution to the field of HRL by introducing SHES which utilizes SES in a hierarchical learning context. The methods scalability simplicity and competitive performance even with the acknowledged lower sample efficiency present a compelling argument for its potential in solving complex RL tasks. The comprehensive experimental analysis and the clear explanation of the methodology are notable strengths. However addressing the methods sample efficiency providing a more balanced discussion on its limitations and expanding comparative analysis could further strengthen the papers impact. Overall the introduction of SHES is a promising step forward in HRL providing an interesting direction for future research. The paper is wellwritten and presents valuable insights making a strong case for the adoption of SHES in complex RL tasks.", "jT1EwXu-4hj": " Review of \"Optimizing Recommendation with Domain Transportation for Information Retrieval Systems\"  1) Summary The paper explores the challenging aspect of the interventional nature of recommendations in information retrieval (IR) systems focusing on how these recommendations not only influence the data already observed but also the target domain in which recommendations will take effect. The authors argue that current methodologies particularly those employing importance weighting (IW) and domain adaptation (DA) fail to account for the critical assumption of overlapping and its consequences on learning and evaluating recommendations. The paper introduces an alternative approach by considering the optimization of recommendations as finding an intervention that best transports learned patterns from the observed domain to the intervention domain. This perspective leads to the development of a transportationconstraint risk minimization objective operationalized through a dualplayer minimax game. The authors provide theoretical validation for their approach demonstrating consistency generalization and excessive risk bounds. They then empirically showcase the advantages of their model through simulations realdata analysis and online experimentation in a realworld IR system.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a critical gap in the literature regarding the interventional nature of recommendations in IR systems especially the overlooked importance of overlapping between source and target domains.  It offers a novel and theoretically sound methodology transportationconstraint risk minimization which is a significant departure from traditional IW and DA approaches.  The authors back their proposition with substantial theoretical analysis proving consistency generalization and excessive risk bounds which adds to the credibility and robustness of their approach.  Comprehensive empirical analysis using simulations realdata and online testing in a realworld system demonstrates the practical effectiveness and applicability of the proposed method. Weak Points:  The complexity of the proposed method especially the requirement for solving a twoplayer minimax game might limit its accessibility and application for practitioners in the field.  While the paper provides a solid theoretical backing the direct comparison of the model\u2019s performance against existing benchmarks seems insufficiently detailed to fully articulate the practical impact.  The realworld IR system testing is highly valuable but details on the deployment complexity scalability and operational overhead compared to traditional methods are missing. Questions and Feedback:  Can the authors provide more insight into how the proposed method scales with the size of the data and the dimensionality of the problem space  It would be beneficial to have a more direct comparison with existing methods on standard benchmarks to clearly identify the scenarios where the proposed method outperforms others significantly.  The paper could benefit from a deeper discussion on the potential limitations or challenges in deploying the proposed method in realworld settings beyond the online testing conducted.  3) Review Summary This paper introduces a significant and novel approach to optimizing recommendations in IR systems underpinned by solid theoretical analysis and promising empirical results. The idea of rethinking recommendation optimization through the lens of domain transportation is intriguing and addresses a crucial yet underexplored area in the field. The authors present a robust framework that not just challenges but also advances the current understanding and methodologies employed in IR systems regarding the interventional nature of recommendations. However the paper would gain from addressing the methods scalability comparing it more directly with existing techniques and discussing deployment challenges in more detail. These enhancements would strengthen the papers contribution and its appeal to both academics and practitioners. Nonetheless the work presented significantly forwards the discourse in recommendation systems providing a strong foundation for future research in this area. The novel perspectives solutions and theoretical findings laid out by the authors are commendable and mark an essential step towards developing IRoriented solutions with a deep understanding of the system\u2019s underlying mechanism.", "nK7eZEURiJ4": "Summarize the Paper In this comprehensive study the authors delve into Distributional Reinforcement Learning (RL) contrasting it with expectationbased RL by offering a novel interpretation as entropyregularized maximum likelihood estimation within a neural Zfitted iteration framework. They showcase the benefits of Distributional RL in terms of regularization stability optimization and acceleration through its affinity with maximum entropy RL and the utilization of a novel distributional loss that promotes stability and generalization. Moreover the paper introduces a new algorithm Sinkhorn Distributional RL which leverages Sinkhorn loss to balance the advantages of Wasserstein distance and Maximum Mean Discrepancy (MMD) in distributional approximations. Through experiments on Atari games the Sinkhorn Distributional RL algorithm demonstrates competitive performance against established algorithms in the field. Strengths and Weaknesses Strong Points: 1. Theoretical Contribution: The paper presents an insightful theoretical foundation for Distributional RLs advantages over expectationbased RL contributing to a deeper understanding of its mechanisms and benefits. 2. Algorithmic Innovation: The introduction of the Sinkhorn Distributional RL algorithm utilizing Sinkhorn loss represents a novel approach that successfully combines the geometric properties of Wasserstein distance with the highdimensional sample efficiency of MMD. 3. Empirical Validation: The competitive performance of the Sinkhorn Distributional RL on a suite of Atari games provides strong empirical support for the proposed theoretical insights and algorithmic approach. Weak Points: 1. Complexity of Concepts: The dense theoretical exposition and the amalgamation of various complex notions (e.g. entropyregularization distributional loss) might be challenging for readers unfamiliar with the nuances of distributional RL. 2. Limited Comparison: While the algorithm shows competitive performance a broader comparison with more recent stateoftheart algorithms could provide a clearer picture of its standing. 3. Generalization Beyond Atari: The focus on Atari games for empirical validation raises questions about the algorithm\u2019s applicability and performance in different domains or more complex environments. Questions and Feedback for the Author: 1. Could you expand on how the Sinkhorn Distributional RL algorithm performs in environments with higher dimensional state spaces or more complex dynamics than Atari games 2. Is there any consideration for the computational efficiency of the Sinkhorn Distributional RL algorithm especially in comparison to the baseline algorithms 3. How sensitive is the performance of the Sinkhorn Distributional RL algorithm to its hyperparameters and what would be the recommended approach for tuning these in different settings Review Summary This paper is a valuable contribution to the field of reinforcement learning offering both theoretical insights and algorithmic innovations that highlight the advantages of Distributional RL. By framing Distributional RL within an entropyregularized maximum likelihood estimation context and introducing the Sinkhorn Distributional RL algorithm the authors tackle the theoretical and practical gaps in understanding and leveraging Distributional RLs potential. The empirical validation on Atari games though limited supports the argument for Distributional RLs effectiveness particularly when utilizing the proposed Sinkhorn loss to balance Wasserstein distance and MMD benefits. However the complexity of the concepts and the dense theoretical exposition could hinder accessibility for a broader audience. The paper\u2019s focus on Atari games as the primary experimental validation platform may also limit insights into the algorithms performance in varied domains. Future work exploring the algorithm\u2019s applicability in more complex environments its computational efficiency and a more thorough hyperparameter sensitivity analysis would greatly enhance the understanding and practicality of Sinkhorn Distributional RL. Overall this study is a significant step forward in the Distributional RL landscape promising to open new avenues for research and application in reinforcement learning tasks.", "nBU_u6DLvoK": "Review of \"UniFormer: Unifying 3D Convolution and Spatiotemporal SelfAttention for Efficient Video Understanding\" 1. Summary: This paper introduces a novel architecture termed UniFormer aimed at learning rich and multiscale spatiotemporal semantics from highdimensional video data. The core challenge addressed by the paper is the efficient aggregation of local context to reduce redundancy and the effective capture of global dependencies among video frames. The proposed solution uniquely combines the strengths of 3D convolutional neural networks (CNNs) and vision transformers via a concise transformer format. The main innovation resides in the MultiHead Relation Aggregator (MHRA) which tackles video redundancy in shallow layers through local relation learning and addresses global video dependencies in deeper layers by leveraging similarity comparisons for longrange token dependency building. The UniFormer architecture demonstrates superior performance on several benchmarks including Kinetics400 Kinetics600 and SomethingSomething V1V2 achieving stateoftheart accuracies with significantly reduced computational costs. 2. Strong and Weak Points Questions and Feedback:  Strong Points: 1. Innovative Integration: The paper successfully integrates the best of both worlds\u20143D convolution and spatiotemporal selfattention\u2014into a unified framework which is not only novel but also significantly impactful for the video understanding domain. 2. Computational Efficiency: The proposed UniFormer architecture requires an order of magnitude fewer GFLOPs compared to stateoftheart methods while achieving or even surpassing their accuracies. 3. Extensive Experimental Validation: The authors provide extensive experiments across multiple benchmarks demonstrating the robustness and effectiveness of UniFormer in a variety of settings.  Weak Points: 1. Lack of Theoretical Insights: While the experimental results are impressive the paper offers limited theoretical justification behind why the integration of 3D convolution and spatiotemporal attention in the proposed manner leads to superior performance. 2. Comparison with Baselines: The paper could benefit from a more detailed comparison with baselines particularly in dissecting the contributions of individual components of UniFormer to the overall performance gains.  Questions and Feedback: 1. Adaptability to Different Domains: How adaptable is the proposed UniFormer architecture to video understanding tasks beyond classification such as object detection or action localization in videos 2. Scalability: How does the UniFormer scale with increasingly larger datasets and video resolutions Is there any tradeoff in terms of accuracy or computational efficiency 3. Ablation Studies: Can the authors provide more ablation studies focusing on the individual contributions of the Dynamic Position Embedding (DPE) and the MultiHead Relation Aggregator (MHRA) components 3. Review Summary: The submission introduces a compelling approach to video understanding that cleverly unifies 3D convolution and spatiotemporal selfattention mechanisms within a transformer architecture. The proposed UniFormer not only addresses the critical challenges of local redundancy and global dependency in video data but also does so with remarkable computational efficiency as evidenced by extensive experimental validation across several benchmarks. Nevertheless to strengthen the paper further it would be beneficial to provide more theoretical insights into the design choices a more detailed comparison with existing methods and additional ablation studies to quantify the impact of key components. Overall the significance of the contributions and the demonstrated effectiveness of the UniFormer architecture make a strong case for its adoption and further exploration in the field of video understanding.", "kcadk-DShNO": " Review of \"A NonAdversarial Approach to Unsupervised Dataset Alignment Using FlowBased Models\"  Summary This paper introduces a novel framework for unsupervised dataset alignment (UDA) that deviates from traditional adversarial learning methods. Focusing on mapping multiple source domains to a shared domain without explicit pairings the work addresses challenges in generative modeling unsupervised domain adaptation and fairnessaware learning. The authors propose a unified nonadversarial approach relying on flowbased models to minimize an upper bound on the JensenShannon Divergence (JSD) converting the adversarial minmax optimization into a cooperative minmin problem. They argue this aids both optimization and evaluation supported by both theoretical insights and empirical evidence across simulated and realworld datasets.  Strong Points 1. Theoretical Contribution: The derivation of an upper bound on the generalized JSD for more than two distributions and its minimization through nonadversarial flowbased models is a significant theoretical advancement. It provides a coherent framework that generalizes previous approaches offering new insights into dataset alignment. 2. Empirical Validation: The extensive empirical analysis across both simulated and realworld datasets validates the proposed frameworks effectiveness. Comparisons are meticulously conducted against prior works demonstrating the superiority of the new method in aligning datasets with lower FID and AUB metrics. 3. Practicality and Applicability: Introducing a regularization term based on transportation cost is an innovative feature that appears to improve the naturalness of the transformation between datasets. The frameworks adaptability to incorporate pretrained density models suggests potential for significant improvements in transfer learning tasks.  Weak Points 1. Complexity and Accessibility: The theoretical concepts while foundational are dense and may be challenging for readers with limited backgrounds in flowbased models or density estimation. Simplifying these theories or providing more intuitive explanations could widen the papers audience. 2. Evaluation Metric Dependency: Although the paper introduces AUB as a novel evaluation metric the reliance on conventional metrics like FID for the realworld dataset validation raises questions about the standalone reliability of AUB especially in comparing with other models. 3. Limited Domain Exploration: While the paper effectively shows improvements over previous flowbased and adversarial methods in dataset alignment tasks the exploration outside of imagebased datasets is not substantiated. Broader application across varied data types could strengthen the argument for the frameworks generalizability.  Questions and Feedback to the Author 1. Theoretical Clarity: Can the authors provide a more intuitive explanation or graphical illustration of how minimizing the upper bound on the generalized JSD leads to effective dataset alignment This may help in better understanding the theoretical basis of the approach. 2. Evaluation Metric Validation: How does AUB compare to FID in terms of sensitivity to alignment quality and could there be scenarios where AUB might not effectively capture the alignment quality compared to established metrics 3. Generalizability to Other Data Types: Given the paper\u2019s focus on image datasets how broadly can the proposed framework be applied to other types of datasets such as text or timeseries Are there any inherent limitations  Review Summary The paper presents a significant step forward in the field of unsupervised dataset alignment introducing a novel theoretically grounded framework that moves beyond adversarial learning paradigms. The comprehensive theoretical development coupled with robust empirical validation highlights the approachs potential to outperform existing methods. However ensuring broader accessibility through simplification of complex concepts and further validation of the new evaluation metric could enhance the papers impact. Additionally expanding empirical investigations to nonperceptual data types would solidify the frameworks applicability across domains. Despite these areas for enhancement the works contributions to both theory and practice in dataset alignment and its implications for transfer learning represent a commendable advancement worthy of attention in the machine learning community.", "qRDQi3ocgR3": " Review of \"Understanding Shortcut Learning in Deep Neural Networks\"  1) Summary The manuscript addresses the phenomenon of shortcut learning in deep neural networks (DNNs) wherein models adopt easytolearn but potentially superficial or biased features for prediction possibly leading to issues with generalization and fairness. Using a diverse experimental setup called WCSTML inspired by the Wisconsin Card Sorting Test the authors investigate the preferential adoption of cues by DNNs across different architectures and datasets. The study reveals that DNNs are inclined towards cues of lower Kolmogorov complexity potentially leading to biases in model predictions. These biases are robust across different network initializations and architectures suggesting a fundamental characteristic of DNN learning processes.  2) Strong and Weak Points Questions and Feedback  Strong Points  Originality: The paper addresses the critical and timely issue of shortcut learning in deep learning with an innovative experimental setup providing new insights into the mechanisms underlying cue preference.  Comprehensive Analysis: The detailed experiments across multiple DNN architectures and datasets alongside rigorous statistical reporting offer a robust foundation for the claims made.  Societal Relevance: Highlighting the potential for biased cue selection especially concerning protected attributes like ethnicity is highly relevant in discussions about AI ethics and fairness.  Weak Points  Solutions and Mitigation Strategies: While the paper excels in diagnosing the problem of shortcut learning it stops short of proposing concrete methods or solutions to mitigate these biases.  Generalization Beyond Visual Tasks: The investigation focuses on visual recognition tasks how these findings generalize to nonvisual tasks or other domains of deep learning remains unclear.  Impact of Data Imbalance: An exploration of how class or cue imbalance within datasets might exacerbate or mitigate the observed preferences towards simpler cues would add depth to the analysis.  Questions and Feedback  Extension to NonVisual Tasks: Can the authors speculate on or discuss preliminary insights into how these findings might extend to tasks beyond visual recognition  Impact of Architectural Differences: While the paper shows robustness across architectures could a deeper investigation into why certain architectures might exacerbate or mitigate shortcut learning be valuable  Mitigation Strategies: It would be beneficial for the authors to discuss potential strategies for mitigating shortcut learning even if outside the scope of the current experiments. Are there architectural or training modifications that could reduce shortcut bias  3) Review Summary and Recommendation The paper makes significant contributions to our understanding of shortcut learning in deep neural networks through a welldesigned series of experiments that demonstrate a natural tendency of DNNs to favor cues of lower Kolmogorov complexity. This finding has profound implications especially regarding model generalization and the potential for biases towards protected attributes. The study is methodologically rigorous and the revelations are of high societal importance. However the work could be strengthened by exploring solutions to the illustrated problems delving into nonvisual tasks and investigating the effect of dataset imbalances on shortcut learning. The absence of proposed interventions or mitigation strategies is a notable gap leaving readers with a clear understanding of a problem but without pathways towards solutions. Based on the significant contributions to our understanding of shortcut biases alongside the outlined opportunities for deepening this research I recommend this paper for publication with minor revisions. Addressing the feedback regarding mitigation strategies potential generalization beyond visual tasks and the impact of architectural differences would make the paper even more robust and impactful.", "qSV5CuSaK_a": " Peer Review for \"FewShot Backdoor Attack against Siamese Network Based Visual Object Tracking\"  Summary The paper introduces a novel backdoor attack termed FewShot Backdoor Attack (FSBA) targeting visual object tracking (VOT) models particularly those based on siamese networks. These models play a critical role in various applications such as autonomous driving and intelligent surveillance. The authors articulate the threat posed by thirdparty training resources which despite their benefits introduce new vulnerabilities into VOT models potentially exploited via backdoor attacks. FSBA is characterized as a fewshot untargeted attack capable of severely degrading a trackers performance by embedding a backdoor into the model during the training phase. This enables the attacker to cause the model to lose track of specific objects when a trigger appears in a minimal number of frames. The effectiveness and resilience of FSBA against stateoftheart VOT models and potential defenses are rigorously demonstrated through digital and physicalworld experiments.  Strong Points 1. Novelty and Relevance: The paper addresses a significant yet underexplored threat to VOT models contributing novel insights into backdoor vulnerabilities within the realm of siamese networkbased trackers. The idea of a fewshot untargeted attack is both innovative and pertinent given the critical mission scenarios where VOT models are deployed. 2. Comprehensive Methodology: The proposed FSBA methodology is wellarticulated leveraging a multitask learning framework that simultaneously optimizes for feature loss (to enforce backdoor embedding) and standard tracking loss. The dual optimization approach is methodically reasoned contributing to the stealth and efficacy of the backdoor attack. 3. Empirical Validation: The empirical examinations covering both digital and physicalworld settings offer a thorough validation of FSBAs effectiveness. The inclusion of comparisons with an ineffective baseline (BOBA) and the assessment against potential defenses underscore the robustness of FSBA. 4. Potential for Impact: By revealing and addressing the susceptibility of VOT models to backdoor attacks this study paves the way for future research on secure training practices and robust defense mechanisms potentially influencing the development of safer VOT models.  Weak Points 1. Generalizability Concerns: The focus on siamese networkbased trackers while justified raises questions about the applicability of FSBA to other architectures and tracking paradigms. Future work should explore the breadth of FSBA\u2019s applicability. 2. Assumptions on Adversary Capabilities: The assumed control over the training process by the adversary is a significant assumption potentially limiting the realworld scenarios in which this attack could be practically executed. Discussion on mitigating this limitation or making the attack more feasible under constrained adversary capabilities could enrich the study. 3. Defense Strategies: While the paper demonstrates the resistance of FSBA against certain defense techniques a more comprehensive exploration of sophisticated and emerging defense mechanisms would provide a fuller picture of FSBAs resilience.  Questions and Feedback to the Author 1. Could the authors elaborate on potential strategies to extend the applicability of FSBA to other VOT architectures beyond siamese networks 2. How might the threat model be adjusted to account for scenarios where adversaries have limited control over the training process 3. Are there other perhaps less explored defense techniques that could potentially mitigate the effectiveness of FSBA  Review Summary This paper makes a significant contribution to the field of visual object tracking security by introducing a novel fewshot backdoor attack. The methodology is wellfounded and the empirical validation convincingly demonstrates the attacks effectiveness and stealth. Moreover the critical examination of resistance against common defenses highlights FSBAs robustness. However the study could be further enriched by addressing the generalizability of the attack across different VOT models reconsidering the assumptions on adversary capabilities and exploring a broader range of defense mechanisms. Overall this work represents a valuable step forward in understanding and mitigating security vulnerabilities within VOT systems.", "rOGm97YR22N": " Peerreview Report:  1) Summarize the paper: This paper introduces a novel recurrent neural network (RNN) architecture named MixedMemory RNNs (mmRNNs) that aims to efficiently model irregularlysampled time series data while overcoming the longterm dependency challenge. Traditional continuoustime RNNs (ODERNNs) struggle with vanishing and exploding gradients a problem that mmRNNs address by incorporating a memory component that operates in conjunction with the continuoustime dynamic flow of the network. The mmRNN architecture ensures constant error propagation through this memory path allowing the model to handle data with arbitrary time lags without gradient issues. The paper provides theoretical insights into why ODERNNs face these challenges and demonstrates through experiments that mmRNNs outperform existing RNN variants in tasks involving synthetic and realworld sparse time series with longterm dependencies.  2) Strong and weak points Question and Feedback to the author: Strong Points: a. Innovative Solution to a Known Problem: The introduction of a mixedmemory compartment is an innovative solution to the longstanding problem of vanishing and exploding gradients in RNNs particularly in the context of irregularlysampled time series data. b. Theoretical and Empirical Validation: Theoretical proofs offer a solid foundation for understanding the vanishing and exploding gradient issues in ODERNNs and the empirical results provide strong evidence of the mmRNNs superior performance. c. Broad Implications: If validated in further studies the mmRNN model could have significant implications for many fields that rely on time series data including finance healthcare and environmental monitoring. Weak Points: a. Complex Implementation: The paper introduces a sophisticated architecture that may be challenging to implement and optimize potentially limiting its accessibility to practitioners in the field. b. Limited Evaluation on Realworld Data: While the experiments demonstrate the effectiveness of mmRNNs the evaluation is somewhat limited in scope. More extensive testing on diverse realworld datasets could offer a clearer picture of the models applicability and performance. c. Comparison with StateoftheArt Methods: The paper lacks a direct comparison with the latest stateoftheart methods in time series prediction which could have strengthened the argument for mmRNNs superiority. Question: How does the performance of mmRNNs compare with other stateoftheart time series models especially those employing transformers or attention mechanisms These models have shown promising results in handling longterm dependencies and irregular sampling. Feedback: The authors are encouraged to address the mentioned weak points by perhaps extending their experiments to include a broader range of realworld datasets and offering a more direct comparison with the latest methods in the field. Additionally it would be beneficial to discuss the practical implications of implementing mmRNNs including the computational resources required and potential optimization strategies.  3) Review Summary: The paper makes a significant contribution to the field of time series prediction by addressing a critical issue in modeling irregularlysampled data with longterm dependencies. The introduction of mmRNNs is both novel and theoretically sound providing a muchneeded solution to the vanishing and exploding gradient problem in continuoustime RNNs. The empirical results are promising but the paper could be strengthened by broadening the evaluation to include more realworld data and comparing the mmRNNs performance directly with stateoftheart methods. Overall the work is of high quality and could have a major impact if the findings are confirmed through further research.", "j30wC0JM39Q": " Peer Review  1) Summary of the Paper This paper investigates the intriguing capabilities of embedding representations in encoding latent domainspecific structures through the lens of language formation and cultural evolution. It proposes that word and graph embeddings like those generated by popular algorithms (e.g. fastText GloVe word2vec and BERT) reflect the incremental human processes of knowledge creation supported by an empirical analysis that uncovers two consistent properties in embeddings: neighbor frequency concentration and clustering velocities that exhibit powerlaw dynamics. The authors compare various generative models designed to replicate these observed embedding traits concluding that models incorporating spatially contextdependent preferential attachment mechanisms offer the closest fit to realworld data. This conclusion is supported by nonparametric statistics applied to both natural and generated embedding spaces to validate the observed phenomena.  2) Strong and Weak Points Questions and Feedback Strong Points: a) Comprehensive Analysis: The paper thoroughly examines the structure of embedding spaces with a detailed empirical and theoretical analysis contributing valuable insights into why embeddings effectively capture semantic properties. b) Innovative Modeling Approach: The introduction of generative models that account for spatial context in the evolution of embedding spaces is both novel and illuminates a path forward for understanding the mechanisms of knowledge representation in machine learning. c) Broad Applicability: The findings of this study have widereaching implications for the fields of natural language processing and information retrieval making it a significant contribution to the research community. Weak Points: a) Comparison with Baseline Models: While the paper provides an indepth exploration of various complex models it could benefit from a clearer baseline comparison to showcase the incremental benefits of proposed models over more simplistic ones. b) Generalizability Concerns: The analysis is heavily reliant on specific datasets and algorithms. A broader exploration of datasets and embedding algorithms could further validate the generalizability of the findings. c) Lack of Detailed Methodological Justification: The paper could be strengthened by providing more justification for the choice of nonparametric statistics used for model evaluation and comparison. Questions and Feedback: a) How do these findings generalize to nonEuropean languages which might have different structural and semantic features influencing the embedding spaces b) The paper mentions future work in exploring spatial dependencies within the generating process. Could the authors elaborate on potential methodologies they consider promising for this exploration c) In evaluating the generative models what specific criteria were used to declare one model superior to another especially in realworld applicability contexts  3) Review Summary The paper presents a compelling exploration of how embedding representations encapsulate the latent structures of domainspecific information focusing on natural language and cultural processes. By identifying two key properties consistent across various embedding types and proposing generative models that mirror these properties the authors contribute significantly to our understanding of knowledge representation in computational systems. The novel methodological approach and the insights provided into the evolution of embedding spaces mark this work as a valuable contribution to the field of machine learning and natural language processing. However the paper would benefit from addressing certain limitations including expanding its dataset diversity to improve generalizability providing clearer justifications for methodological choices and offering more detailed comparisons with simpler baseline models. Addressing these concerns would bolster the papers impact and provide clearer guidance for future research endeavors aimed at understanding and improving embedding technologies.", "jNB6vfl_680": " Review of \"Global Magnitude Pruning With Minimum Threshold (GPMT): Achieving StateoftheArt Pruning Efficiency\"  1) Summary The paper introduces a novel yet straightforward approach to neural network pruning focusing on a method that has been somewhat overlooked by the research community: global magnitude pruning (GP). The primary innovation lies in combining GP with a Minimum Threshold (MT) strategy which ensures that a fixed number of weights are preserved in each layer preventing any layer from being excessively pruned. This method referred to as GPMT is showcased to achieve stateoftheart (SOTA) performance in terms of parameteraccuracy tradeoff across various datasets and architectures including challenging benchmarks like ResNet50 and MobileNetV1 on ImageNet. The authors back their claims with comprehensive experiments ablation studies and a direct comparison with SOTA pruning techniques. The simplicity and ease of implementation of GPMT stand out offering a potentially robust baseline for future pruning research.  2) Strong and Weak Points Questions and Feedback Strong Points:  Simplicity and Accessibility: The GPMT methods straightforwardness is highly appealing particularly its ease of implementation and oneshot nature which contrasts favorably with other more complex pruning methodologies.  Broad Applicability: The paper demonstrates GPMTs effectiveness across a variety of architectures and datasets marking it as a versatile pruning approach.  Innovative Solution for OverPruning: The introduction of the MT concept to mitigate the issue of overpruning in certain layers is a thoughtful and novel contribution to the field of neural network compression. Weak Points:  Lack of Discussion on Computational Efficiency: While the paper extensively covers parameter reduction and accuracy there is limited discussion on how GPMT influences the computational efficiency in terms of both inference time and energy consumption which are crucial factors for deployment in realworld applications.  FLOPs Increase: The paper acknowledges an increase in FLOPs compared to some layerwise pruning methods potentially limiting GPMTs applicability in scenarios where FLOPs reduction is a priority alongside parameter count minimization. Questions and Feedback:  Clarification on FLOPs Increase: Could the authors provide more insights into situations where the increase in FLOPs with GPMT might not be a significant concern Are there specific applications or architectural configurations where this tradeoff is particularly favorable  Impact on Different Architectural Designs: The paper mentions that the need for MT varies with the base architecture family. It would be beneficial if the authors could delve deeper into why certain architectures require MT and others do not. Are there intrinsic architectural features such as depth or types of connections that influence this requirement  Further Exploration Beyond OneShot Pruning: The authors point out that one potential future direction is exploring gradual application of GP. It would be intriguing to understand the authors preliminary views or any available data on the potential benefits or challenges of moving towards a gradual pruning approach with GPMT.  3) Review Summary The paper presents a significant contribution to the field of neural network pruning by reevaluating and enhancing a somewhat neglected pruning approach\u2014global magnitude pruning\u2014through the novel addition of the Minimum Threshold strategy. This combination not only addresses the critical issue of overpruning but also demonstrates superior performance across different datasets and architectures. The work stands out for its simplicity effectiveness and the robust experimental validation supporting its claims. However the paper could address the broader implications of increased FLOPs in certain scenarios and explore in more depth the computational efficiency aspects related to inference time and energy consumption. Such discussions would greatly enrich the work providing clearer guidance for researchers and practitioners considering GPMT for their applications. In conclusion given its strong performance simplicity and the thoughtful solution to the overpruning problem GPMT represents a valuable baseline for future work in neural network pruning. The authors are encouraged to further explore the nuanced tradeoffs involved particularly around computational efficiency to fully harness GPMTs potential in a wide range of application scenarios.", "kQ2SOflIOVC": " PeerReview  Summary of the Paper The paper addresses the challenge of fewshot learning (FSL) specifically applied to histology images an area that is significant yet underexplored compared to natural image analysis. Recognizing the expensive and laborintensive nature of collecting welllabeled datasets for histology images the authors propose a system combining contrastive learning (CL) with latent augmentation (LA) to enhance the labelefficiency and generalizability of fewshot learning systems. The work sets up three crossdomain tasks to simulate realworld clinic problems demonstrating the application and value of their approach in a clinical setting. The paper outlines the methods and experiments carried out showcasing that the proposed system offers consistent gains over baselines and exhibits improved generalization capabilities especially when compared to supervised learning methods.  Strong Points 1. Novel Application Area: The paper targets an important and less trodden path of applying fewshot learning techniques to histology images highlighting its clinical significance. 2. Innovative Methodology: By amalgamating contrastive learning with latent augmentation the authors provide a fresh approach to leverage unsupervised data for fewshot learning in a domain where data scarcity is a significant challenge. 3. Comprehensive Experiments: The experimental setup including three crossdomain tasks and the exploration of both homogeneous and heterogeneous shot selection problems provide a robust testing ground for the proposed system. 4. Extensive Ablation Studies and Analysis: The ablation studies and discussions around latent augmentation number of prototypes covariance types and patch selection strategies provide deep insights into the workings and effectiveness of the method. 5. Open Source Contribution: Making the code available on GitHub ensures transparency and encourages further research and collaboration in the field.  Weak Points 1. DomainSpecific Focus: While the paper presents significant advances in histology images the domainspecific nature might limit its immediate applicability to broader image analysis challenges without further adaptations or studies. 2. Complexity and Implementation Details: The paper would benefit from a more detailed discussion on the complexity of implementing the proposed system particularly how it integrates into existing workflows in clinical settings. 3. Comparison with StateoftheArt: The paper could strengthen its position by including a more direct comparison with the latest stateoftheart methods in fewshot learning not only within the realm of natural images but also in related medical imaging tasks. 4. Impact on Clinical Workflow: While the paper mentions clinical value a deeper exploration into how the proposed system could be embedded into or enhance current clinical workflows would be beneficial.  Questions and Feedback to the Author 1. Scalability and Adaptability: Could the authors comment on the scalability of their system to larger datasets or its adaptability to other similar medical image analysis tasks outside histology 2. RealWorld Application: Have any preliminary studies or partnerships with clinical institutions been initiated to explore the realworld applicability of this system 3. Comparison with Other Domains: Can the authors provide insights into how their findings in histology images compare with challenges in other medical imaging domains such as radiology or dermatology image analysis 4. Future Directions: What are the authors views on the potential future directions this research could take Are there plans to explore incorporating additional selfsupervised learning techniques or other forms of data augmentation  Review Summary The paper makes a significant contribution to the field of medical image analysis by presenting an innovative approach to address the challenges of fewshot learning in histology images. The combination of contrastive learning and latent augmentation represents a novel method that shows promising results in improving the generalizability and efficiency of fewshot learning systems in a clinically relevant domain. The extensive experiments and ablation studies provide valuable insights and validate the effectiveness of the proposed system. However the paper could be enhanced by addressing its domainspecific focus detailing its applicability and implementation complexity and offering a clearer comparison with stateoftheart methods. Overall this work offers valuable contributions to the field of medical image analysis and opens up new avenues for research and application in histology and potentially other imaging domains.", "xnYACQquaGV": " Review of \"NeuralLinUCB: A Neural Contextual Bandit Approach with Deep Representation and Shallow Exploration\"  Summary: The paper introduces NeuralLinUCB a novel algorithm for neural contextual bandits a subarea of multiarmed bandit (MAB) problems. These problems involve an agent making sequential decisions to maximize expected cumulative rewards in an environment where the reward associated with each action (or arm) is partially known. Specifically neural contextual bandits consider situations where the reward generating function is complex and unknown but can be related to contextaction pairs through deep neural networks. The authors propose using the last hidden layer of a deep ReLU neural network for feature transformation (deep representation) while exploring using an upper confidence bound (UCB) approach on the last linear layer of the network (shallow exploration). This approach is argued to achieve efficient computation and robust performance through theoretical guarantees and empirical validation. The authors demonstrate that their algorithm attains \\xc3\\x95(\\xe2\\x88\\x9aT) regret matching the performance of classic linear contextual bandit algorithms while benefiting from the expressiveness of neural nets.  Strong Points: 1. Novelty and Significance: The proposal of integrating deep representation learning with shallow exploration in the framework of neural contextual bandits presents a significant advancement in balancing the computational efficiency and representational power of deep learning models in sequential decision making. 2. Theoretical Contributions: The paper provides rigorous theoretical analyses proving that NeuralLinUCB achieves sublinear regret. This theoretical foundation is crucial for substantiating the efficiency and effectiveness of the proposed algorithm. 3. Empirical Validation: The authors conduct thorough experiments using realworld datasets to validate the performance and computational efficiency of NeuralLinUCB. The experimental results offer convincing evidence of the practical benefits of the proposed algorithm over existing methods like LinUCB NeuralUCB and NeuralTS.  Weak Points: 1. Complexity and Scalability Concerns: Despite the claimed computational efficiency the practical implications of implementing and scaling the proposed algorithm especially in extremely highdimensional or fastchanging environments are not fully addressed. More insights into computational complexity especially concerning the networks width and depth would enhance the papers contribution. 2. Hyperparameter Sensitivity Analysis: The paper could benefit from a more detailed discussion on the sensitivity of NeuralLinUCBs performance to hyperparameters such as the confidence level \u03b1t regularization parameter \u03bb and the network update frequency H. This would provide practitioners with clearer guidelines for applying this algorithm in diverse settings. 3. Comparison with Broader Set of Baselines: While the authors compare NeuralLinUCB with several relevant algorithms the inclusion of more recent and varied baselines possibly from related areas like reinforcement learning could offer a broader context for evaluating the proposed algorithms strengths and limitations.  Questions and Feedback: 1. Hyperparameter Selection: How were the hyperparameters specifically the exploration rate \u03b1t and the regularization parameter \u03bb chosen for the experiments Clarification on this could help in understanding the robustness of NeuralLinUCB across different settings. 2. Impact of Network Depth and Width: The authors mention that NeuralLinUCBs performance is contingent on a sufficiently large network width. Could you provide more insights into how the depth and width of the neural network impact the algorithms regret bound and practical performance 3. RealWorld Applications: Can the authors provide examples or discussions on specific realworld applications where NeuralLinUCB could be especially beneficial This would help in illustrating the practical implications of the proposed algorithm.  Review Summary: The paper presents a significant and novel contribution to the field of neural contextual bandits by introducing NeuralLinUCB an algorithm that effectively blends deep representation learning with efficient shallow exploration. The theoretical and empirical validations offered in the paper are robust making a strong case for the algorithms efficacy and efficiency. However addressing concerns regarding computational complexity hyperparameter sensitivity and broader comparisons with other algorithms could make the paper even more compelling. Additionally discussions on practical applications and the impact of neural network configurations on performance would enhance the papers relevance and applicability in realworld scenarios. Recommendation: Accept with minor revisions contingent on addressing the highlighted concerns about computational complexity hyperparameter sensitivity and broadening the context for evaluating the proposed algorithms performance.", "hxitw01k_Ql": " Peer Review  Summarize the paper The paper aims to investigate the effect of memory architecture on the performance of agents in partially observable Markov decision processes (POMDPs) focusing on a simplified twohypothesis testing problem as a case study. It compares the performance of agents with two types of memory: RAM (Random Access Memory) where the agent can transition between M memory states and a more constrained memory model the Memento memory where the agent can only remember its last m actions and rewards. The key finding is that although the Memento memorys architecture is simpler it can achieve performance comparable to RAM by exploiting a conjecture related to Grayordered binary necklaces demonstrating that constrained memory architectures can lead to expedited training and equivalently high performance without the need for large or complex memory states. The study emphasizes the tradeoff between memory flexibility and training efficiency in policy learning for POMDPs.  Strong points 1. Original Contribution: The paper addresses an underexplored aspect of reinforcement learning (RL) related to the optimization of memory architectures in agents dealing with POMDPs. The focus on comparing RAM with Memento memory through the lens of a twohypothesis testing problem is both novel and insightful. 2. Theoretical Depth: The theoretical analysis particularly the utilization of a conjecture on Grayordered binary necklaces to derive performance metrics for memory policies demonstrates a deep understanding of both the mathematical underpinnings of RL and combinatorial optimization. 3. Empirical Evidence: The authors provide empirical evidence to support their claims showing that training with the Memento memory architecture indeed leads to significantly better results than with a RAM model from random initialization. This empirical backing is crucial for validating the theoretical findings. 4. Practical Value: By highlighting the effectiveness of constrained memory architectures the paper offers valuable insights for designing more efficient RL agents particularly in terms of training time and computational resources.  Weak points 1. Assumptions and Conjecture Dependence: The papers main results partially rely on a conjecture related to Grayordered binary necklaces. While the empirical results support this conjecture the dependence on an unproven claim might limit the general applicability of the findings until the conjecture is fully validated. 2. Generalization Limitations: The study focuses on a twohypothesis testing problem. How these results generalize to more complex POMDPs or realworld scenarios remains an open question. Future work needs to address this to establish the broader applicability of the findings. 3. Implementation Details and Accessibility: While the paper provides a link to the code more detailed discussions on the implementation and potential challenges faced during the empirical studies could enhance the reproducibility and accessibility of the results.  Questions and Feedback to the author 1. Conjecture Validation: Could the authors comment on ongoing or future efforts to validate the conjecture on Grayordered binary necklaces This could potentially strengthen the theoretical foundation of the paper. 2. Generalization to Complex POMDPs: Do the authors foresee any significant challenges in applying the insights gained from the twohypothesis testing problem to more complex POMDPs or to realworld settings where observations and state spaces are significantly larger 3. Impact of Memory Size on Learning Dynamics: The paper makes a compelling case for the Memento memory architecture however the impact of varying the memory size (m) on the learning dynamics and final performance could be further explored. How does the size of Memento memory influence the training efficiency and policy optimality in more complex environments  Review Summary This paper provides a significant contribution to the field of reinforcement learning by focusing on optimizing memory architectures for agents in POMDPs. The comparative study between RAM and Memento memory reveals insightful findings emphasizing the potential of constrained memory models to achieve high performance with more efficient training processes. Despite its reliance on a conjecture and the specificity of the problem setting the theoretical analysis combined with empirical validation offers valuable insights into designing more effective RL agents. Future work addressing the conjecture validation generalization of the findings and a deeper exploration of memory size impacts would further enhance the value of this study. Overall the paper advances our understanding of memory optimization in RL making a compelling case for the strategic constraining of memory architectures to improve agent training efficiency and effectiveness.", "gijKplIZ2Y-": " Summarize the paper The paper introduces Mistill a novel machine learningbased approach for automating the design of distributed Traffic Engineering (TE) protocols in data center networks. By using Neural Networks (NNs) to distill forwarding behavior from exemplary decisions Mistill learns how to encode and exchange local state information among network devices and maps this exchanged state into effective forwarding decisions. The paper demonstrates Mistills capability by teaching it three distinct TE policies and validating its performance against both synthetic and realworld traffic patterns in simulations. The results indicate that Mistill successfully mimics the intended TE policies and can adapt to new traffic scenarios it wasnt explicitly trained on showcasing both effectiveness and generalizability.  Strong and weak points Questions and Feedback Strong Points: 1. Innovative Approach: The paper presents a novel use of neural networks to distill forwarding decisions from TE policies which is a significant departure from traditional protocol design methodologies. 2. Generalization: Mistills ability to generalize and perform well on traffic patterns not seen during training is particularly impressive. 3. Practical Considerations: The consideration for deployment on real hardware and the examination of Mistill\u2019s computational efficiency are highly relevant for practical applications. Weak Points: 1. Comparison with Existing Solutions: While the paper mentions other TE solutions a more detailed quantitative comparison with stateoftheart methods would provide clarity on Mistill\u2019s performance improvements or disadvantages. 2. Deployment Complexity: The paper does not fully address the potential complexities and challenges of deploying such a system in realworld networks particularly regarding compatibility with existing infrastructure. 3. Limited Evaluation Scenarios: The evaluation focuses on data center networks. Expanding the scope to include other types of networks might provide insights into Mistill\u2019s versatility and limitations. Questions and Feedback: 1. Scalability Considerations: How does Mistill scale with the size of the network Are there limitations to the number of switches or the complexity of the network topologies it can handle 2. Adaptability Over Time: How well does Mistill adapt to longterm changes in traffic patterns and is continuous retraining required to maintain its performance 3. Impact of Network Changes: How does Mistill handle dynamic changes in the network topology such as the addition or removal of switches Review Summary: The paper by introducing Mistill makes a significant contribution to the field of Traffic Engineering by leveraging neural networks to automate the design of TE policies. It addresses a critical challenge in traffic engineering  the need for adaptability to diverse and evolving traffic patterns in data centers. Mistills ability to learn from examples and generalize to new scenarios without being explicitly programmed is a considerable strength advancing the stateoftheart in protocol design. However the paper could benefit from a more comprehensive comparison with existing TE solutions providing a better benchmark for Mistills performance. Additionally discussing the practical challenges of implementing such a system in realworld environments and its scalability would enhance the papers relevance to practitioners in the field. Ultimately Mistill represents a promising step towards more flexible and adaptive traffic engineering solutions. Future research could expand on this work by exploring its application in different network contexts assessing its longterm adaptability and investigating strategies for efficient deployment in operational networks.", "g2LCQwG7Of": " Review of \"A Novel Probabilistic Model for Hierarchical Clustering of LargeScale Graphs Using Continuous Relaxation of TreeBased Hierarchies\"  Summary This paper introduces a novel approach to hierarchical clustering that involves a probabilistic model over hierarchies using continuous relaxation of treebased hierarchies for largescale graphs. The authors ground their model theoretically by drawing connections to Markov chain theory which enables the optimization of hierarchical quality metrics like Dasgupta cost and TreeSampling Divergence (TSD) efficiently. The model referred to as Flexible Probabilistic Hierarchy (FPH) shows superior performance in learning highquality hierarchies across various realworld graphs including a massive graph with 2.3M nodes. Notably the FPH model demonstrates strong capabilities in link prediction tasks despite not being specifically trained for them underscoring the models effectiveness in uncovering meaningful hierarchies.  Strong Points 1. Theoretical Foundation: The authors provide a strong theoretical basis for their model by linking it with absorbing Markov chains. This foundation not only supports the models conceptual underpinnings but also facilitates efficient exact computing of relevant quantities like LCA probabilities. 2. Comprehensive Evaluation: The model is rigorously tested against 11 realworld graphs including very large graphs showcasing its scalability and effectiveness. The performance is benchmarked against both traditional and recent baselines highlighting the models superior capabilities in learning hierarchies. 3. Versatility and Applicability: The models ability to perform competitively in link prediction tasks without specific training is remarkable. This aspect demonstrates the versatility of the learned hierarchies suggesting the models applicability beyond clustering possibly in recommendation systems or other predictive tasks.  Weak Points 1. Model Complexity: The complexity of the model and its theoretical underpinnings might pose challenges in understanding and implementing it. A more detailed discussion on the practicalities of deploying this model especially in nonacademic settings would be beneficial. 2. Comparison with Baselines: While the paper provides an extensive evaluation deeper insights into why certain baselines underperform in comparison to FPH beyond the highlevel reasons could offer more value. Specifically discussions on limitations of baselines that FPH overcomes would be enlightening. 3. RealWorld Applicability and Interpretability: Although the paper demonstrates the application of the model on realworld datasets discussions on the interpretability of the learned hierarchies and their direct applicability in solving realworld problems are sparse. More concrete examples or case studies could enhance understanding of the model\u2019s practical benefits.  Questions and Feedback to the Author 1. Model Complexity: Can the authors elaborate on the computational resources required e.g. in terms of memory and processing power especially when dealing with very large graphs This would offer insight into the practical deployment of the model. 2. Interpretability: How interpretable are the hierarchies learned by the model Are there mechanisms or methodologies recommended for extracting actionable insights from these hierarchies 3. Applicability: Could the authors discuss potential applications of their model in fields outside traditional clustering problems such as social network analysis or bioinformatics where understanding hierarchies could offer significant value  Review Summary This paper presents a significant advancement in hierarchical clustering on largescale graphs through the introduction of a novel probabilistic model. The proposed model stands out due to its solid theoretical foundation superior performance against a wide variety of baselines and the versatility of its application particularly demonstrated by its unexpected efficacy in link prediction tasks. Despite these strengths considerations around the models complexity interpretability and broader applicability would benefit from further clarification and discussion. Given the compelling results and contributions to the field I recommend this paper for publication contingent on addressing the aforementioned points to enhance its relevance and accessibility to a broader audience.", "nZeVKeeFYf9": " Peer Review for \"LowRank Adaptation for Efficient Model Finetuning\"  1) Summarize the paper This paper introduces LowRank Adaptation (LoRA) an efficient approach to adapt largescale pretrained language models such as GPT3 for specific downstream tasks. LoRA reduces the need for full model finetuning which is computationally expensive and memoryintensive. By freezing the pretrained model weights and injecting trainable rank decomposition matrices into each transformer layer LoRA significantly decreases the number of trainable parameters and memory requirements. The authors provide empirical evidence showing that LoRA matches or exceeds the performance of traditional finetuning methods across a variety of NLP tasks despite its reduced computational cost. Additionally LoRA avoids the introduction of additional inference latency a common drawback of other adaptation techniques. The paper also explores the role of rankdeficiency in model adaptation offering insights into why LoRA is effective.  2) Strong and Weak Points Questions and Feedback Strong Points:  Efficiency in Adaptation: The paper successfully addresses a critical concern in the deployment of large language models by reducing the number of trainable parameters and memory requirements without compromising model performance. This is a significant contribution given the growing sizes of stateoftheart models.  Broad Applicability and Integration: The authors demonstrate LoRAs efficacy across multiple models (RoBERTa DeBERTa GPT2 and GPT3) suggesting its general applicability. Additionally releasing a package that facilitates LoRA\u2019s integration with PyTorch models is valuable for the research and development community.  Empirical Rigor: The comprehensive set of experiments across various tasks and comparisons with existing adaptation methods provide a strong empirical foundation supporting the merits of LoRA. Weak Points:  Indepth Analysis of Model Quality: While the paper presents a quantitative analysis of model performance it could benefit from a deeper qualitative examination or case studies highlighting how LoRAs adaptations impact the interpretability or qualitative aspects of model outputs.  Scalability and Practicality: The paper could address scalability in more detail particularly in realworld scenarios with multiple concurrent tasks. Although LoRA allows for quick taskswitching the implications for realtime multitask environments are not thoroughly explored.  Comparison with Newest Methods: Given the rapid advancement in the field the paper could strengthen its position by including comparisons with the very latest parameterefficient training methods if any have emerged since the conducted experiments. Questions and Feedback:  Have you explored combining LoRA with other parameterefficient techniques beyond prefixtuning and if so what were the results  The paper mentions the potential for future work in selecting weight matrices for LoRA application more systematically. Could initial insights or preliminary findings on this be shared  Considering the advantages of LoRA are there specific domains or tasks where you anticipate it might underperform or not be suitable compared to full finetuning  3) Review Summary The paper makes a strong contribution to the field of natural language processing by addressing the computational and memory challenges associated with the adaptation of largescale pretrained language models for specific tasks. LowRank Adaptation (LoRA) stands out for its efficient use of trainable parameters maintaining or exceeding model performance benchmarks with significantly reduced computational costs. The experimental setup covering various models and tasks provides robust evidence of LoRA\u2019s efficacy and broad applicability. However there remains an opportunity for the paper to deepen its exploration into the qualitative impacts of LoRA and extend its analysis to more realworld scenarios particularly those involving multitask environments. Further a comparative analysis with the newest parameterefficient techniques (if available) could enhance the papers relevance and richness. Given its strengths and the potential areas for further exploration I recommend this paper for publication while suggesting the authors consider the highlighted areas for enhancement in future iterations or extensions of their work. The proposal of LoRA is a valuable step forward in making large language models more accessible and practical for a wider range of applications.", "kxARp2zoqAk": " Review of the Paper on InfoTS: An InformationAware Approach for Time Series Data Augmentation in Contrastive Learning  Summary The paper presents an innovative approach termed InfoTS addressing the challenge of generating meaningful data augmentations for contrastive learning in time series analysis. Unlike domains such as images and language where data augmentations can be intuitively designed based on human recognition time series data suffer from complex highdimensional and often inexplicable patterns making the selection of effective augmentations difficult. To tackle this the authors propose criteria based on information theory that prioritize high fidelity (preserving semantic identity) and high variety (enhancing generalization capacity) of augmentations. They employ a metalearning mechanism that adaptively selects optimal augmentations for various time series datasets enabling endtoend optimization alongside the encoder. Their approach demonstrates considerable improvements in performance metrics (reduction in MSE and improvement in accuracy) over stateoftheart baselines in both time series forecasting and classification tasks.  Strong Points 1. Novel Approach: The introduction of a metalearning mechanism for adaptively selecting data augmentations based on informationtheoretic criteria is innovative and addresses a clear gap in the existing literature on time series contrastive learning. 2. Theoretical Foundation: The paper provides a solid theoretical foundation by deriving criteria for augmentation selection from information theory which is well justified and explained. 3. Comprehensive Evaluation: The extensive experimental validation across various datasets and tasks demonstrates the effectiveness and generalizability of the proposed method substantiating the authors claims with empirical evidence.  Weak Points 1. Complexity in Implementation: The proposition of employing a metalearner for adaptive augmentation selection while effective introduces additional complexity in implementation and may increase computational resources and training time required. 2. Lack of Comparison for Augmentation Techniques: While the paper extensively compares InfoTS with stateoftheart baselines it lacks a direct comparison or ablation study on the effectiveness of different augmentation techniques within its framework. 3. Underexplored Discussion on Failure Cases: The paper does not provide significant insights into scenarios where InfoTS may not perform as expected or the types of time series datadatasets where it may be less effective.  Questions and Feedback 1. Could the authors discuss any challenges encountered in the implementation of the metalearning mechanism It would be beneficial to understand any practical limitations or computational considerations that may impact the adoption of InfoTS. 2. How does InfoTS perform in the presence of highly noisy time series data Exploring the robustness of the method in scenarios with significant noise could further validate its applicability in realworld settings. 3. Is there a detailed analysis available on the impact of different hyperparameters especially the weight parameter \u03b2 on the performance of InfoTS Such an analysis could provide valuable insights for practitioners when employing the method on new datasets.  Review Summary The paper introduces an important contribution to the domain of time series analysis by proposing InfoTS a method leveraging a metalearning mechanism for adaptive data augmentation selection in contrastive learning. The novel approach is grounded in a robust theoretical framework and demonstrates significant improvements over baseline models in forecasting and classification tasks. Despite the increased complexity and the need for further exploration on various aspects (e.g. performance under high noise conditions detailed hyperparameter analysis) the work undoubtedly advances the field offering a promising direction for future research. The thorough evaluation and clear presentation of the methodology and results provide strong support for the effectiveness of InfoTS. Thus I recommend the acceptance of this paper considering its significant contributions and the potential impact on advancing contrastive learning applications in time series analysis.", "xVGrCe5fCXY": " Review of \"Denoising Diffusion Gamma Models for Improved Image and Speech Generation\"  Summary This paper introduces the Denoising Diffusion Gamma Model (DDGM) which incorporates Gamma noise into the diffusion process for generative modeling. The motivation behind this approach is to explore an alternative to the conventional Gaussian noise used in Denoising Diffusion Probabilistic Models (DDPMs) with the aim of improving the performance in image and speech generation tasks. The authors argue that the Gamma distribution with its two parameters (shape and scale) provides more modeling flexibility than the singleparameter Gaussian distribution. They demonstrate that this added complexity allows the DDGM to more effectively model the distribution of data leading to faster convergence and improved results in terms of image quality and speech perceptibility. The paper provides a comprehensive framework for integrating Gamma noise into the diffusion process including a discussion on the theoretical background the training and inference algorithms and a variational lower bound for the Gamma diffusion model. It concludes with experimental evaluations on standard datasets showing the DDGMs superior performance over Gaussianbased models.  Strengths 1. Novelty: The paper introduces an innovative approach by incorporating Gamma noise in diffusion generative models expanding the modeling capacity of these models beyond what is possible with traditional Gaussian noise. 2. Theoretical Depth: The authors provide a thorough theoretical foundation for the Gamma diffusion process including derivations of the diffusion and reverse processes and the variational lower bound for the DDGM. 3. Empirical Validation: The experiments conducted offer solid evidence of the models superior performance across both image and speech generation tasks. The improvements in FID scores for images and PESQ STOI scores for speech generation are particularly compelling. 4. Reproducibility: The paper includes a detailed reproducibility statement with provided code clear explanations of hyperparameter choices and proofs in the appendices facilitating followup research and practical applications.  Weaknesses 1. Complexity vs. Benefit: The introduction of an additional hyperparameter \\( \\theta0 \\) and the more complex noise structure could increase the difficulty of model tuning and training. It would be beneficial to discuss the tradeoffs involved in more detail including how sensitive the models performance is to this hyperparameter. 2. Comparison with Related Work: While the paper does compare the DDGM to Gaussianbased DDPMs it would be interesting to see a broader comparison with other nonGaussian approaches or advances in generative modeling providing a more comprehensive context for the models performance. 3. Limitations and Challenges: The discussion could benefit from a deeper analysis of the limitations and potential challenges. For instance the effect of the Gamma distributions properties (e.g. being bounded at zero) on the diversity of generated samples or on the convergence properties of the model. 4. Scalability: The experiments are persuasive but somewhat limited in scope. Additional experiments on larger datasets or more diverse tasks could help establish the scalability and generalizability of the DDGM.  Questions and Feedback to the Author 1. Hyperparameter Sensitivity: Can the authors elaborate on how sensitive the models performance is to the choice of \\( \\theta0 \\) Is there a systematic way to select this parameter 2. Comparison with Other NonGaussian Models: Are there other nonGaussian noise models that were considered or compared against How does DDGMs performance stack up in such a broader landscape 3. Limitations Discussion: Could the authors discuss any observed or potential limitations of the DDGM particularly in the context of generating highdimensional or diverse data samples 4. Future Directions: What are the authors views on future directions Could incorporating other types of distributions (beyond Gamma and Gaussian) offer further benefits  Review Summary The paper presents a significant contribution to the field of generative modeling by introducing and rigorously developing the Denoising Diffusion Gamma Model (DDGM). By carefully justifying the choice of Gamma distribution for noise modeling and providing solid theoretical and empirical backing the authors convincingly demonstrate the potential of this approach for improving generative model performance. Moreover the provision of code and detailed methodological explanations enhances the papers impact by aiding in reproducibility and future work. While the exploration of the models limitations and comparative analysis could benefit from further expansion the overall work is robust innovative and represents a valuable step forward in generative model research.", "qhAeZjs7dCL": " Review of \"Learning GeneralPurpose Visual Representations from Generative Models\"  1. Summary The paper explores the potential of learning generalpurpose visual representations not from direct datasets but from samples generated by offtheshelf generative models (IGMs) like GANs without access to the original training data. The authors posit that as generative models have become highly efficient at creating realistic samples they may serve as a compressed and more manageable version of large datasets with added benefits such as privacy preservation and bias reduction. The study focuses on the application of contrastive learning methods to these generative samples where multiple \"views\" of the same content are created through latentspace transformations to facilitate learning. The findings suggest that contrastive learning from generative models can achieve or surpass the performance of learning from real data given certain conditions and methodologies are followed particularly regarding the sampling strategy and transformation methods used.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper addresses a novel and timely question in the field of representation learning by leveraging generative models as data sources which could revolutionize how visual representations are learned and applied.  Comprehensive Evaluation: The experimental setup comparing different generative models and representation learning techniques provides a thorough understanding of the potential and limitations of learning from generative samples.  Practical Implications: By demonstrating that generative models can substitute for real datasets in certain scenarios the paper outlines a future where model sharing becomes more feasible than dataset sharing with significant benefits for privacy and accessibility. Weak Points:  Lack of Diverse Domains: The study primarily focuses on visual representation from image data. The generalizability of these findings to other data types (e.g. text audio) remains an open question.  Finite Diversity: While the paper touched on the issue of generative models having finite diversity there might be a need for more indepth exploration of how this limitation affects representation quality as modelgenerated datasets grow larger.  Implementation Details: The paper could benefit from a clearer description of the training procedures for the different representation learning methods applied which would aid in replicability and further experimentation. Questions and Feedback:  Could the authors elaborate on the potential limitations of their approach when applied to rapidly changing or evolving datasets where generative models might lag behind the data they need to simulate  It would be interesting to see a more detailed analysis of the computational efficiency of learning from modelgenerated data versus real datasets especially considering the potential need for generating large volumes of synthetic data.  3. Review Summary The paper presents a compelling investigation into the feasibility and efficiency of learning visual representations from generative models offering an intriguing alternative to traditional datasetbased learning approaches. The findings that representations learned from generative data can rival or outperform those from real datasets are particularly promising suggesting new directions for efficient and privacypreserving representation learning. However the studys focus on image data and the potential impact of finite diversity in generative models are areas that warrant further exploration. Despite its limitations the paper makes a significant contribution to the field by opening up new avenues for research and application in representation learning. The proposed method of leveraging generative models for learning visual representations not only addresses practical concerns around data privacy and accessibility but also challenges conventional notions about the necessity of large real datasets for effective machine learning. This work sets a strong foundation for future research in this direction and provides valuable insights for both academic and industry practitioners.", "xo_5lb5ond": " 1. Summarize the paper The paper introduces a novel convolutional neural network (CNN) pruning method named LongEstchAiN (LEAN) that improves computational efficiency by selectively pruning convolutional filters based on their collective relevance rather than individually. Pruning is accomplished by interpreting the CNN as a graph where nodes represent operators edges symbolize the convolutional filters connecting these operators and edge weights correspond to the operator norm of the filters. This graphcentric approach allows LEAN to identify and preserve the most critical paths through the network ensuring minimal loss of accuracy while significantly reducing the number of convolutional filters. The method was tested on image segmentation tasks using various datasets including CamVid and a realworld Xray CT dataset. The experiments demonstrate that LEAN can achieve comparable accuracy with a substantially reduced number of filters resulting in faster computation and lower memory usage compared to conventional pruning techniques.  2. Strong and weak points of the paper Questions and Feedback to the author  Strong Points  Innovative Approach: The innovative use of graphbased algorithms to model the interdependency of convolutional operators offers a fresh perspective on CNN pruning addressing the limitations of traditional methods.  Comprehensive Testing: The thorough evaluation across multiple datasets and network architectures provides strong evidence of the methods general applicability and effectiveness.  Significant Performance Improvements: Demonstrating both theoretical speedups (in FLOPs reduction) and practical speedups during inference is a considerable achievement offering direct benefits to realworld applications.  Weak Points  Limited Consideration of Nonlinear Operators: The paper primarily focuses on linear operators with only a brief mention of handling nonlinear operations like ReLUs. This could limit the applicability or optimality of the method for architectures heavily reliant on nonlinearities.  Lack of Comparison with Recent Pruning Methods: While the paper compares LEAN to several established pruning techniques it does not benchmark against the most recent stateoftheart pruning methods potentially overlooking advancements in the field.  Complexity and Implementation Challenges: The paper hints at the complexity of implementing the proposed method but does not provide sufficient details on the challenges faced or how they were overcome. Practical applicability could be affected for users without a strong background in graph algorithms.  Questions and Feedback  How does LEAN handle networks with significant nonlinear components and what impact do these have on the methods effectiveness  Could the authors compare LEAN pruning not just against classical methods but also against very recent pruning techniques to highlight its competitive edge  Providing more insights or guidelines on implementing the proposed method could greatly benefit practitioners. Can the authors elaborate on this aspect  3. Review Summary The paper presents an innovative and effective approach to CNN pruning that addresses the inherent limitations of traditional pruning methods by considering the interdependence of convolutional operators within a network. The LongEstchAiN (LEAN) method significantly outperforms conventional pruning techniques in reducing computational costs while maintaining high accuracy levels across various datasets and network architectures. This achievement marks a substantial advancement in the field and has direct implications for deploying efficient CNNs in resourceconstrained environments. However the papers focus is primarily on linear operations and its treatment of nonlinear components is relatively superficial. Given the importance of nonlinear operations in CNNs a more detailed exploration of this aspect would strengthen the papers contributions. Additionally comparing LEAN with more recent pruning methods could provide a clearer picture of its standing in the current stateoftheart. Finally given the methods reliance on graph algorithms providing more detailed implementation guidance could broaden its accessibility and adoption. In conclusion the LEAN method represents a significant step forward in neural network pruning promising both theoretical and practical improvements in network efficiency. Further refinement and exploration particularly regarding nonlinear operators and broader comparisons could enhance its value to the field.", "nioAdKCEdXB": " Peer Review of \"A Novel Computational Framework for Schr\u00f6dinger Bridge with Applications in Generative Modeling\"  Summary The paper introduces a computational framework that bridges Schr\u00f6dinger Bridge (SB) models with Scorebased Generative Models (SGM) through the application of ForwardBackward Stochastic Differential Equations (FBSDEs). The authors successfully establish a theoretical connection between the optimization principles of SB and the likelihood objectives of SGM offering a novel perspective that allows SB models to adopt modern generative training techniques akin to those used in SGMs. By applying this framework named SBFBSDE to generate images on datasets like MNIST CelebA and CIFAR10 the paper demonstrates the potential of SB models to produce highquality generative outputs comparable to current leading methods. This work significantly extends the applicability of SB models in the field of deep generative modeling providing new insights and methodologies for training these models with likelihood objectives.  Strong Points 1. Theoretical Contribution: The paper makes a significant theoretical contribution by linking the optimization principles of SB to the loglikelihood training popular in modern generative modeling. The application of FBSDEs is both innovative and intellectually stimulating providing a fresh lens to view SB models. 2. Empirical Results: The experimental results showcasing the generation of realistic images on various benchmarks are convincing and show the practical relevance of the proposed framework. The comparison with existing models and methods further emphasizes the competitiveness of SBFBSDE in the realm of generative modeling. 3. Novel Optimization Principle: The introduction of an SB model training principle that incorporates advanced techniques from SGMs without sacrificing the mathematical rigor and flexibility of SB is noteworthy. This holistic approach could inspire future research in designing more efficient and robust generative models.  Weak Points 1. Explanation of Methodological Steps: While the paper is mathematically rigorous the explanation of some steps in the proposed framework remains dense for readers not highly familiar with FBSDEs or SB theory. The paper could benefit from a more detailed exposition or intuitive summaries of key methodological steps. 2. Breadth of Experimental Validation: Although the experimental results are solid the paper primarily focuses on image generation tasks. Exploring additional applications or demonstrating the frameworks performance on other types of generative modeling tasks could further validate the frameworks versatility. 3. Discussion on Limitations: The paper briefly mentions the computational burden and memory complexity as limitations but lacks a detailed discussion on potential tradeoffs scalability issues or challenges in applying the proposed method to more diverse or largerscale problems.  Questions and Feedback to the Author 1. Could the authors provide additional insights or intuitive explanations on how the FBSDEs theory specifically enables the connection between SB and SGM especially for readers less familiar with these advanced mathematical concepts 2. Would the authors consider extending their experimental validation to nonimage data or other generative tasks to showcase the versatility of the SBFBSDE framework 3. The paper mentions computational burden and memory complexity as limitations. Could the authors elaborate more on how these challenges might be mitigated in future work or when scaling the method to larger datasets  Review Summary The paper presents a groundbreaking computational framework that unifies Schr\u00f6dinger Bridge models with contemporary generative training techniques through the sophisticated application of ForwardBackward Stochastic Differential Equations. This theoretical innovation opens up new avenues for generative modeling offering a principled alternative capable of producing highquality results on par with leading methods. Despite its strengths the paper could improve on elaborating the methodological details broadening the scope of experimental validations and discussing potential limitations more thoroughly. Nonetheless the significance of the theoretical contribution and the promising empirical results strongly supports the potential of the proposed framework in advancing the field of deep generative modeling.", "ySQH0oDyp7": " Review of \"QDROP: Enhancing PostTraining Quantization with Activation Dropping for Neural Network Compression\" 1. Summary of the paper This paper introduces QDROP a novel approach for enhancing the performance of posttraining quantization (PTQ) in neural networks. The authors identify a critical gap in previous PTQ methods which tend to fail under extremely lowbit settings due to not incorporating activation quantization into the PTQ reconstruction process. Through both empirical and theoretical analyses the paper demonstrates that properly incorporating activation quantization\u2014especially in a partial and randomized manner\u2014significantly improves model accuracy. This is attributed to the enhanced flatness of the optimized lowbit model on both calibration and test data. The proposed method QDROP randomly drops quantization of activations during PTQ leading to a new stateoftheart performance in various tasks across both computer vision and Natural Language Processing (NLP) domains. Extensive experiments showcase the superiority of QDROP over existing PTQ methods achieving remarkable accuracy improvements in lowbit activation quantization settings. 2. Strengths and weaknesses Strong points:  Novelty: The paper addresses a significant gap in existing PTQ methods by pioneering the study of activation quantizations impact on model accuracy. The proposed QDROP method stands out for its simplicity and effectiveness.  Theoretical and Empirical Validation: The authors provide a robust theoretical framework to explain the impact of activation quantization on weight tuning and empirically validate its benefits. This dual approach strengthens the credibility of the proposed method.  Comprehensive Experimentation: The method is tested across a wide range of tasks and models demonstrating its versatility and effectiveness in enhancing PTQ performance under extremely lowbit settings.  Practicality: QDROP is presented as a plugandplay module compatible with various neural networks including CNNs and Transformers making it accessible for widespread realworld application. Weak points:  Lack of Indepth Comparative Analysis: While the paper compares QDROP with existing PTQ methods it could benefit from a more indepth analysis of how these methods differ in their approach to handling activation quantization. This would provide readers with clearer insights into why QDROP outperforms its counterparts.  Potential Overfitting Concerns: The paper briefly mentions the risk of overfitting with Case 2 without delving into how QDROP mitigates this risk effectively especially under different calibration and test datasets.  Hardware Implementation Insights Lacking: For practitioners insights into the practicality of deploying QDROP in realworld hardware scenarios including potentially increased computational overhead or compatibility issues would be beneficial. Questions and Feedback:  Can the authors provide more insights into how QDROP mitigates potential overfitting risks especially when there is a significant mismatch between calibration and test datasets  It would be interesting to see a comparative analysis of the computational overhead introduced by QDROP particularly in edge devices where computational resources are limited.  Could the paper discuss potential limitations or scenarios where QDROP might not perform as expected 3. Review summary The paper presents QDROP an innovative and effective method for enhancing PTQ of neural networks by incorporating activation quantization in a novel manner. QDROPs simplicity coupled with its theoretical and empirical validation serves as its main strength pushing the boundaries of PTQ to new heights and establishing a new state of the art. However the paper could be improved by addressing potential overfitting concerns more thoroughly providing deeper insights into comparative methodologies and discussing hardware implementation aspects. Overall I recommend the acceptance of this paper given its significant contribution to the field of neural network compression and its practical implications. The proposed QDROP method not only advances the understanding of activation quantizations role in PTQ but also provides an effective solution applicable to a variety of tasks and models. Further elaboration on the points mentioned would make this paper an even more valuable resource for both researchers and practitioners in the field.", "i--G7mhB19P": " Peer Review  Summarize the paper The investigated paper delves into the behavior and implications of natural gradient descent (NGD) as compared to Euclidean gradient descent (EGD) in the context of deep learning models focusing on its inductive biases and generalization capacities. The authors concentrate on deep linear models and several settings including logistic loss for separable classification and deep matrix factorization contrasting the outcomes with those traditionally observed in EGD. Through theoretical analysis and empirical evidence the paper demonstrates instances where NGD fails to generalize as effectively as EGD attributed to NGDs reparameterization invariance which while eliminating the role of parameterization in optimization also dispenses with the beneficial implicit regularization seen with EGD. The paper is structured around introducing the topic and its relevance reviewing pertinent literature detailing their theoretical contributions and experiments and discussing the implications of their findings in the broader context of gradientbased optimization and deep learning at large.  Strong and Weak Points Questions and Feedback Strong Points: 1. Theoretical Contributions: The paper offers concrete theoretical insights into how NGD behaves under logistic loss in separable classification and matrix factorization scenarios contributing valuable knowledge to the field. 2. Empirical Evidence: The authors successfully bridge theory with practice by providing experimental results that substantiate their theoretical claims which is commendable for its methodological rigor. 3. Indepth Literature Review: The thorough review and discussion of related work bolster the papers relevance and situates it effectively within its research context demonstrating the authors mastery over the subject matter. Weak Points: 1. Generalizability of Findings: While the paper provides compelling findings within the scope of deep linear models its conclusions may not be easily generalizable to more complex or nonlinear scenarios without further empirical validation. 2. Experimentation Details: There is a lack of detailed discussion regarding the experimental setup and potential limitations or biases which could affect the reproducibility and interpretation of the results. 3. Implications for Practice: The practical implications of choosing NGD over EGD especially in realworld applications are not extensively explored. It would be beneficial to provide more guidance for practitioners. Questions and Feedback: 1. Could the authors clarify if there were any constraints or limitations encountered during their experimentation that could influence the interpretability of their results 2. It would be enriching to include a discussion on potential methodologies or approaches that could mitigate the shortcomings of NGD as highlighted through the paper\u2019s findings. 3. Feedback: Consider expanding the scope of the paper to include nonlinear models and realworld applications to enhance the practical applicability of the findings.  Review Summary The paper offers significant theoretical and empirical insights on the behavior and effectiveness of natural gradient descent in certain machine learning scenarios. By focusing on deep linear models the authors successfully demonstrate the potential pitfalls of NGDs invariance to reparameterization especially in relation to generalization capabilities compared to EGD. While the paper is wellconstructed with strong theoretical underpinnings and supportive empirical evidence it faces limitations in terms of generalizability beyond the studied models and lacks a comprehensive discussion on the realworld implications of its findings. Feedback and questions aimed at addressing these concerns could enrich the paper further making it a more comprehensive resource for both theoreticians and practitioners in the field of machine learning and deep learning optimization techniques. Overall the paper is a valuable contribution sparking crucial dialogue on the optimization paths chosen in model training and their impacts on model performance and generalization.", "srtIXtySfT4": "Summarize the paper: The paper introduces Neural Parameter Allocation Search (NPAS) a novel task aimed at optimizing the distribution of parameters within a neural network given a fixed parameter budget. The approach deviates from traditional parameter sharing strategies by accommodating networks of arbitrary architectures and parameter sizes without preassumptions. The paper presents Shapeshifter Networks (SSNs) which automates parameter sharing across a network to fit within any specified parameter budget. SSNs adapt to both lowbudget regimes where the focus is on minimizing parameters (LBNPAS) and highbudget regimes which allow networks to utilize more parameters than standard architectures for performance gains without increasing computational effort (HBNPAS). SSNs demonstrate effectiveness across different networks and tasks including image classification and language processing improving performance metrics while reducing parameter counts. The methodology involves training steps for parameter mapping and weight generation offering a comprehensive framework that also enhances performance when combined with other techniques like knowledge distillation and parameter pruning. Strong Points: 1. Innovation: NPAS and SSNs address the critical challenge of parameter efficiency in neural networks innovatively. They offer a generalizable solution that adapts to any given parameter budget enhancing the flexibility and applicability of parameter sharing strategies. 2. Comprehensiveness: The paper covers an extensive range of experiments across different tasks and architectures demonstrating the versatility and broad applicability of the SSNs. 3. Performance Improvement: Results highlight notable improvements in error rates and model performance against benchmarks especially in lowbudget and highbudget NPAS scenarios demonstrating the effectiveness of the proposed method. 4. Combination with Other Techniques: SSNs compatibility with existing methods like knowledge distillation and pruning for additional performance gains embodies a holistic approach to model optimization. Weak Points: 1. Complexity and Accessibility: The sophisticated nature of SSNs may pose implementation challenges for practitioners. Simpler explanations or opensourced tools can enhance accessibility. 2. Computational Overhead: While SSNs reduce parameters the paper could explore more deeply the tradeoffs in computational overhead especially in the context of the added steps in parameter mapping and weight generation. 3. Lack of Discussion on Limitations: The paper could benefit from a more detailed discussion on potential limitations or scenarios where SSNs may not provide the expected benefits. Questions and Feedback to the Author: 1. Can the authors elaborate on the computational overhead associated with utilizing SSNs particularly during the parameter mapping and weight generation phases 2. How do SSNs handle scenarios where network layers have significantly diverse parameter requirements Is there a risk of underperformance in some layers due to the generalized sharing approach 3. It would be beneficial if the authors could discuss the limitations or potential challenges in implementing SSNs especially in complex or unconventional network architectures. 4. Could the authors provide insights or preliminary findings on how SSNs might integrate with emerging neural network designs such as spiking neural networks or networks designed for quantum computing platforms Review Summary: This paper introduces an innovative framework for optimizing parameter distribution in neural networks through Neural Parameter Allocation Search (NPAS) and Shapeshifter Networks (SSNs). This approach addresses the growing demand for computational efficiency by enabling networks to adapt to a fixed parameter budget enhancing flexibility across various architectures and tasks. The demonstrated performance improvements across a wide array of network types and tasks underscore the effectiveness and potential of SSNs in advancing parameter sharing methods. However the paper could further refine its contribution by addressing computational overhead concerns offering a deeper discussion on limitations and providing clearer guidance on implementing SSNs across different network architectures. Overall the paper makes a significant contribution toward efficient neural network design and with further refinements it can offer valuable insights and tools for the broader machine learning community.", "qw674L9PfQE": " Peer Review  1. Summary of the Paper This paper introduces a novel contrastive learning method titled \"Contrastive Leave One Out Boost\" (CLOOB) which integrates modern Hopfield networks to enhance learning with the InfoLOOB objective. The motivation behind CLOOB stems from limitations associated with the InfoNCE objective used in contrastive learning particularly its ineffectiveness for high mutual information scenarios and tendency to saturate. Conversely while the InfoLOOB represents an upper bound that theoretically overcomes these limitations it suffers from high variance and stability issues. CLOOB addresses these challenges by utilizing modern Hopfield networks to stabilize the InfoLOOB objective through the retrieval of embeddings thus enriching the embeddings by emphasizing correlations and reducing noise. The efficacy of CLOOB is demonstrated through its superior performance in zeroshot transfer learning tasks compared to CLIP across various datasets and architectural setups.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The paper presents a creative approach by integrating modern Hopfield networks with the InfoLOOB objective addressing a significant limitation in existing contrastive learning methods.  Theoretical Foundation: The authors provide a solid theoretical background for the proposed method including a discussion on the properties of InfoLOOB and InfoNCE which enriches the readers understanding.  Empirical Validation: Comprehensive experiments demonstrate CLOOBs effectiveness in zeroshot transfer learning tasks outperforming the established CLIP model across various datasets.  Practical Relevance: The ability of CLOOB to deliver improved performance in zeroshot transfer learning tasks indicates its potential impact on realworld applications requiring high adaptability to new data. Weak Points:  Complexity and Accessibility: The incorporation of modern Hopfield networks might increase the complexity of the model potentially impacting its accessibility and ease of implementation for some practitioners.  Computational Resources: The paper does not thoroughly discuss the computational efficiency and resource demands of CLOOB especially in comparison to base models like CLIP.  Limited Exploration of Failure Cases: While the paper showcases the strengths of CLOOB it would benefit from a more detailed discussion on scenarios where CLOOB may underperform or face challenges. Questions and Feedback to the Author: 1. Could the authors expand on the computational efficiency and resource demands of CLOOB in comparison to CLIP and other baseline models 2. It would be beneficial to see more analysis on the conditions or data characteristics under which CLOOB might struggle or underperform to better understand its limitations. 3. How does the stability of the InfoLOOB objective when boosted by modern Hopfield networks vary with different types of data (e.g. more chaotic or less structured datasets) 4. Can the authors discuss potential modifications or extensions to CLOOB that could mitigate its identified weaknesses or expand its applicability further  3. Review Summary The paper presents a significant contribution to the field of contrastive learning by addressing a wellknown limitation of the InfoNCE objective with the innovative integration of modern Hopfield networks through the proposed CLOOB method. The theoretical underpinnings are robust and the empirical results convincingly demonstrate CLOOBs superiority in zeroshot transfer learning tasks across various settings. Despite the strengths the complexity of the model and the lack of a thorough discussion on computational demands and potential limitations in certain scenarios might hinder its broader adoption without further elucidation. It would highly benefit the community if future work could address these points potentially making CLOOB more accessible and adaptable. Overall given its novel approach and demonstrated improvements in learning efficacy CLOOB represents an important step forward in the domain of selfsupervised learning warranting further exploration and adaptation in broader contexts.", "w-CPUXXrAj": " Peer Review  1) Summary of the Paper The paper investigates the limitations of multimodal variational autoencoders (VAEs) particularly focusing on mixturebased models when used for weaklysupervised data. Despite the advantages of weak supervision the authors identify a critical gap in generative quality between multimodal and unimodal VAEs which they attribute to the subsampling of modalities inherent in the training of multimodal VAEs. By formally proving that subsampling enforces an upper bound on the multimodal evidence lower bound (ELBO) they demonstrate how this limitation hinders the models ability to accurately approximate joint distributions especially with increased modalityspecific information. Through extensive experiments on synthetic and real datasets they empirically validate their theoretical findings showing a decline in generative quality with the addition of modalities and highlighting the tradeoffs between generative quality and coherence. The paper ends by discussing implications for the utility of multimodal VAEs in realworld applications and suggesting directions for future research to overcome these identified limitations.  2) Strong and Weak Points Question and Feedback to the Author Strong Points:  The paper addresses a significant gap in the literature by systematically analyzing the limitations of multimodal VAEs especially in the context of weak supervision.  The formal proof that subsampling modalities limits the multimodal ELBO is a solid theoretical contribution that helps to clarify the observed disparity in generative quality between multimodal and unimodal VAEs.  The comprehensive set of experiments on different datasets including both synthetic and real data provides strong empirical evidence supporting the theoretical claims.  The discussion section thoughtfully considers the implications of the findings for realworld applications and outlines potential directions for future work to improve multimodal learning. Weak Points:  While the paper thoroughly investigates the impact of subsampling modalities it focuses less on potential strategies for overcoming this limitation. More concrete suggestions or preliminary experiments on alternative approaches could further strengthen the paper.  The paper could benefit from a more detailed discussion on the limitations of its own methodologies and assumptions particularly regarding the choice of datasets and how they might influence the generalizability of the findings.  Although the work is foundational it might appear somewhat incremental to experts already familiar with the nuances and challenges of VAEs in multimodal learning environments. Question to the Author:  Have you considered the impact of using more sophisticated priors or more complex decoder architectures on the identified limitations Could such modifications potentially mitigate the negative effects of subsampling modalities Feedback:  It would be beneficial to see more exploration or discussion around the potential pathways for addressing the limitations raised including any novel architectural innovations or training strategies that could circumvent the issues with subsampling modalities.  A clearer elucidation of how these findings contrast with existing approaches to multimodal learning outside the domain of VAEs might provide additional context for the significance of your contributions.  3) Review Summary This paper makes a valuable contribution to the field of multimodal learning by uncovering and formalizing key limitations inherent in mixturebased multimodal VAEs. The authors articulate a nuanced problem \u2013 the upper bound enforced by subsampling modalities \u2013 and provide compelling theoretical and empirical evidence to support their claims. Their work sheds light on why multimodal VAEs despite their potential may fall short in realworld applications urging the research community to rethink current practices and explore new solutions. The impact of this paper lies in its diagnostic of an important yet underexplored issue prompting further investigation into more effective multimodal learning models. However the paper could be enriched by offering more concrete directions or experimental evaluations of potential strategies to overcome the highlighted limitations. Despite this the research represents a significant step forward providing foundational insights that could guide future innovations in the field. In summary this papers strengths notably outweigh its weaknesses presenting a thoughtful wellresearched contribution to understanding the challenges of multimodal VAEs. Its recommendations for future research directions are particularly valuable paving the way for advancements in designing more robust and effective generative models for multimodal data.", "zf_Ll3HZWgy": " Review  1) Summary of the Paper The paper explores the effectiveness of CLIP (Contrastive LanguageImage Pretraining) as a visual encoder for various VisionandLanguage (VL) tasks offering an alternative approach to traditional models that rely on encoders trained with manuallyannotated data. It investigates two primary scenarios: direct taskspecific finetuning and integration with VL pretraining for downstream task enhancement. The study presents empirical evidence that CLIP significantly improves performance across multiple VL tasks including Visual Question Answering Image Captioning and VisionandLanguage Navigation achieving stateoftheart results in some cases.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The papers exploration of CLIP for VL tasks challenges the traditional reliance on visually annotated data proposing a scalable and less laborintensive model training method.  Comprehensive Evaluation: Rigorous testing across multiple VL tasks and extensive comparison with existing models demonstrate the versatility and strength of integrating CLIP as a visual encoder.  Practical Implications: Achieving competitive or superior results on numerous VL tasks hints at the realworld applicability of this approach potentially impacting future designs of VL systems. Weak Points:  Limited Analysis on Failure Cases: While the paper discusses successes theres insufficient detail on where and why CLIP might underperform particularly in complex scenarios not covered by the experiments.  Generalization Across Datasets: The paper could benefit from an evaluation of how well models trained with CLIP generalize across unseen datasets especially given its zeroshot learning capabilities.  Computational Efficiency: The paper lacks a detailed discussion on the computational costs and training efficiency of using CLIP versus traditional encoders an important factor for practical adoption. Questions and Feedback:  Could the authors elaborate on the challenges and potential solutions for integrating CLIP with other components of VL models particularly concerning model tuning and interaction complexity  The paper would benefit from a deeper investigation into the failure cases of CLIP as a visual encoder and whether certain task characteristics consistently predict these failures.  Feedback is provided to consider extending the analysis to include how the models performance with CLIP varies across different languages and cultural contexts given the diversity inherent in webcrawled imagetext pairs.  3) Review Summary Overall the paper presents a compelling argument for using CLIP as a visual encoder in VL tasks backed by strong empirical evidence across several benchmarks. Its innovative approach addresses a significant challenge in the field\u2014reliance on manuallyannotated visual data\u2014by leveraging a scalable source of imagetext pairs with minimal human annotation. This not only enhances model performance but also opens up new avenues for efficient model training and deployment. However the paper could be improved by addressing its weak points such as providing a more detailed analysis of the models performance in diverse datasets its computational efficiency and an indepth discussion on the scenarios where CLIP may not be as effective. Furthermore exploring the generalization capabilities of models trained with CLIP across different datasets and languages could enrich the papers findings. In conclusion the research makes a significant contribution to the VL field proposing an alternative that may redefine how visual encoders are trained for these tasks. Nonetheless further exploration in the aforementioned areas could provide a more holistic view of its applicability and limitations. The recommendation is for acceptance assuming the authors can address the feedback and questions to further strengthen the paper.", "gKLAAfiytI": "Review of \"Equivariant SelfSupervised Learning (ESSL): A Framework for Enhanced Representation Learning via NonTrivial Equivariance\" 1. Summary: The paper presents a novel framework in selfsupervised learning (SSL) titled Equivariant SelfSupervised Learning (ESSL) that extends traditional invariant SSL approaches by introducing a principle of nontrivial equivariance for certain transformations while maintaining invariance for others. This hybrid approach is designed to improve the semantic quality of learned representations by allowing them to transform in a predictable manner corresponding to specific applied transformations on the input data. The proposed framework integrates an additional pretraining objective that aims at predicting these transformations. Through comprehensive experiments across various benchmarks in computer vision such as CIFAR10 and ImageNet and beyond into regression problems in photonics science the paper showcases ESSLs ability to leverage both invariance and equivariance for improved feature learning. Details on experimental setups methodologies and findings are elaborated with the key contribution being the demonstration of ESSLs effectiveness in enhancing representation quality thereby extending the applicability of SSL methods across domains. 2. Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses a critical and often overlooked aspect of selfsupervised learning by distinguishing between useful invariance and beneficial sensitivity to transformations effectively combining these into a single framework (ESSL).  It demonstrates wide applicability across domains including computer vision and photonics science underscoring the generalizability and potential impact of ESSL.  The empirical validation is thorough leveraging standard datasets and tasks and provides a clear comparison between ESSL and established SSL methods illustrating significant improvements.  Providing code datasets and pretrained models enhances reproducibility and facilitates future research based on ESSL. Weak Points:  While the frameworks benefits are clear in the context of the experiments conducted the paper lacks a deeper theoretical exploration of why nontrivial equivariance contributes positively towards the semantics of the learned representations.  The discussion on the choice of transformations that benefit from being treated equivariantly vs. invariantly could be expanded. Specifically guidelines or insights into choosing transformations for new datasets or tasks are somewhat limited.  Experiments primarily focus on visual tasks and one scientific domain. Additional experiments in other domains (e.g. natural language processing audio processing) could further validate the frameworks versatility. Questions to the Author:  Can the authors provide further insights or theoretical underpinnings that might explain the observed improvements when employing nontrivial equivariance in SSL  How does ESSL perform with complex or compound transformations and what strategies do you recommend for selecting transformations for new unseen datasets  Are there plans to extend ESSLs evaluation to domains beyond computer vision and photonics science potentially to natural language processing or other areas Feedback:  Exploring the theoretical aspects of equivariance in SSL specifically within the proposed ESSL framework could enhance the papers impact by bridging empirical findings with theoretical understanding.  Further elaboration on the decisionmaking process for choosing certain transformations as targets for equivariance or invariance would be valuable for researchers looking to apply ESSL to new problems.  Expanding the experimental validation beyond the current scope to include additional domains could substantiate the proposed frameworks general applicability and resilience to diverse data characteristics. 3. Review Summary: The paper presents Equivariant SelfSupervised Learning (ESSL) a promising advancement in SSL methodologies emphasizing hybrid invariance and equivariance to improve the semantic quality of learned representations. The comprehensive empirical evaluation demonstrates ESSLs effectiveness across several benchmarks and domains though the theoretical understanding of why such an approach is beneficial remains somewhat underexplored. Despite this ESSL marks a significant step forward in SSL research providing a versatile framework potentially applicable across a broad spectrum of tasks and domains. The transparency in sharing resources for reproducibility and future research is commendable further solidifying the papers contribution to the field. Given the strengths and the potential impact of ESSL the paper presents a significant advancement in representation learning warranting high regard within the academic community. However further theoretical insights and broader domain evaluations could enhance its impact and understanding.", "swrMQttr6wN": " Summarize the paper The paper presents a novel framework Learning to Map (L2M) designed for objectgoal navigation in unseen environments by leveraging the generation of semantic maps beyond the immediate field of view of an agent. This framework actively learns contextual semantic priors through an ensemble of segmentation models that predict both occupancy and semantic features of areas not directly observed by the agent. By utilizing model uncertainty as both a training signal for active sample selection and a guide for goaldirected navigation L2M aims to enhance the agents ability to navigate towards objects in unknown spaces effectively. The approach is validated using the Matterport3D dataset and the Habitat simulator demonstrating superior performance in objectgoal navigation tasks compared to existing baselines. Additionally the paper discusses ethical considerations related to the application of this technology and emphasizes reproducibility by sharing code models and trainingtesting protocols.  Strong and Weak Points of the Paper Strong Points: 1. Novelty: The paper introduces a unique approach to objectgoal navigation by predicting semantic maps beyond the agents direct observation and utilizing uncertainty in those predictions for navigation. This represents a significant advancement in robotic navigation and spatial understanding in unseen environments. 2. Performance: The experimental results show that L2M outperforms competitive baselines on the objectgoal navigation task within the Matterport3D dataset validating the effectiveness of the proposed method. 3. Ethical and Reproducibility Considerations: The authors not only consider the ethical implications of deploying their technology in realworld scenarios but also place a strong emphasis on reproducibility. Providing detailed appendices code and data ensures that other researchers can validate and build upon their work. Weak Points: 1. Generalization to Diverse Environments: While the method performs well on the provided dataset there is limited discussion on its generalization capabilities to environments vastly different from North American and European homes such as those with significantly different architectural styles or cultural settings. 2. Hardware Limitations and RealWorld Applicability: The paper does not discuss the hardware requirements or limitations for deploying this method on physical robots. Understanding the computational and sensory requirements would be crucial for realworld applications. 3. Lack of Comparative Analysis on Uncertainty Estimation Techniques: The paper utilizes ensemblebased uncertainty estimation but does not compare this choice with other methods such as Bayesian neural networks which might offer insights into the tradeoffs of different approaches. Questions and Feedback to the Author: 1. Can the authors provide insight into how L2M might perform or adapt to environments distinctively different from the training set such as commercial or industrial settings 2. It would be beneficial to discuss the computational and sensory requirements for implementing L2M on physical robotic platforms. Can the authors comment on these aspects 3. A comparative analysis of the chosen ensemblebased uncertainty estimation with alternative methods (e.g. Bayesian approaches) could enrich the discussion. Could the authors share any thoughts or preliminary results in this direction  Review Summary This paper proposes a significant advancement in objectgoal navigation through the introduction of the Learning to Map framework which innovatively utilizes semantic map predictions and model uncertainty for navigating in unseen environments. The methodological novelty combined with demonstrated superior performance over existing baselines and a strong commitment to ethical considerations and reproducibility positions this work as a meaningful contribution to the field of robotics and autonomous navigation. However the paper could be strengthened by addressing the generalization capabilities of the method to a wider variety of environments discussing the practical requirements for deployment on physical robots and providing a broader analysis of the chosen uncertainty estimation technique compared to alternatives. Engaging with these considerations would not only enhance the papers rigour but also its applicability and impact on future research and realworld implementations in the domain of autonomous navigation and robotics.", "pMQwKL1yctf": " 1. Summary The paper introduces Time Control (TC) a novel language model designed to tackle the existing challenges of generating coherent long texts by modern language models like GPT2. The authors identify the primary issue as the nexttokenonly language modeling objective which fails to plan ahead and capture longrange text dynamics. To address this TC employs a latent stochastic process where text dynamics within a document are mapped to those of a Brownian bridge process enabling the model to implicitly plan and generate text consistent with a latent document plan. The model significantly outperforms baseline methods in various tasks including text infilling discourse coherence and long text generation validated through both quantitative analysis and human evaluations.  2. Evaluation Strong Points:  Innovative Approach: The introduction of latent stochastic processes specifically the Brownian bridge for modeling text dynamics is novel and addresses the planning issue in long text generation effectively.  Comprehensive Evaluation: The paper presents a thorough evaluation across multiple domains comparing Time Control with domainspecific methods and GPT2 finetuning across text infilling discourse coherence and long text generation tasks. The detailed ablation study further strengthens the claim that each component of TC is crucial.  HighQuality Experimental Work: Extensive experiments and human evaluations provide strong evidence for the models capabilities. The experiments are welldesigned and the results are convincingly presented. Weak Points:  Complexity and Comprehensibility: The intricate details of the model and the heavy reliance on stochastic processes might make it difficult for readers not familiar with these concepts to fully grasp the methodology and its implications.  Potential Bias in Human Evaluation: The paper could further elaborate on the human evaluation process specifically how evaluators were selected and whether they had any domain expertise that could bias the results.  Lack of Discussion on Limitations and Failure Modes: While the model shows considerable improvements a discussion on its limitations and potential failure modes particularly in different genres or styles of text would provide a more balanced view. Questions and Feedback:  How does the model handle highly structured documents with nonlinear narrative flows such as nonfiction with numerous crossreferences or fiction with multiple plot lines  Is there a significant computational overhead associated with training and deploying TC compared to traditional models like GPT2 A comparison in terms of computational resources would be beneficial.  Could the authors elaborate on how the model could be adapted to other sequential data types such as audio or video What challenges do they anticipate in such adaptations  3. Review Summary This paper presents an innovative approach to improving long text generations coherency and structural integrity by integrating latent stochastic processes into the language model design. The Time Control model demonstrates superior performance across multiple evaluation metrics and domains marking a significant advancement in the field. However the complexity of the approach might limit its accessibility and a more detailed discussion on its limitations and broader applicability could enhance the papers value. The experimental work is thorough and wellexecuted providing strong evidence supporting the models effectiveness. Given these considerations I recommend acceptance contingent on addressing the feedback regarding the models potential limitations computational efficiency and its adaptability to other data forms.", "xFOyMwWPkz": " Review of the Paper: \"Quantitative Identification of Unit Status in CNNs via Topological Feature Entropy\"  1. Summary This paper proposes a novel approach to quantitatively assess the status of individual units in Convolutional Neural Networks (CNNs) utilizing algebraic topological tools. The core idea revolves around the concept of feature entropy a measure derived from the degree of chaos in the global spatial patterns represented by a unit for a given category. A unit with high feature entropy is deemed effective implying its capacity to capture and represent pivotal features from the input images of a particular class. The authors argue that this measure is rescalinginvariant and universally applicable regardless of the CNN architecture. Through experiments using the VGG16 network on the ImageNet dataset the paper demonstrates that feature entropy decreases with deeper layers and correlates closely with training loss suggesting its potential as a robust indicator of unit status and model generalization capabilities.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Uniqueness: The application of algebraic topology for quantifying unit status in CNNs is innovative offering a fresh perspective compared to traditional magnitudebased methods.  Rescaling Invariance: Addressing the issue of rescaling sensitivity in conventional indicators is a significant strength making feature entropy a more reliable metric.  Correlation with Model Training and Layer Depth: The observed trends of feature entropy during training and across different layers provide insightful evidence for its potential usefulness in understanding CNN mechanisms. Weak Points:  Universality and Applicability: While the paper claims universal applicability the experiments are limited to the VGG16 architecture on ImageNet. It remains to be seen how feature entropy behaves across a broader spectrum of network architectures and datasets.  Complexity and Interpretability: The methods reliance on algebraic topology might pose challenges in terms of computational complexity and ease of understanding for practitioners not familiar with topological data analysis.  Empirical Validation for Generalization Discrimination: The experimental validation of feature entropy as a discriminator of network generalization is compelling but could benefit from further elaboration on how it compares or complements other generalization measures. Questions and Feedback:  Scalability: How does the computation of feature entropy scale with the size of the network and dataset Are there computational constraints that might limit its application in larger more complex scenarios  Comparison with Existing Techniques: Could you provide more detailed comparisons with existing unit status indicators possibly including metrics not limited to CNN architectures  Impact on Network Design and Pruning: How can feature entropy be leveraged in the practical tasks of network design optimization or pruning Are there implications for guiding the architectural choices or identifying redundant units for pruning  3. Review Summary The paper introduces an innovative and compelling approach to assessing the status of units in CNNs grounding its methodology in algebraic topology. The concept of feature entropy is both novel and promising especially its invariance to rescaling and potential correlation with network generalization abilities. However the paper would benefit from additional clarity on scalability broader applicability across various network architectures and tasks and deeper comparative analysis with existing methods. While the theoretical foundation is solid and the preliminary experimental results are encouraging further exploration and validation could enhance the robustness and appeal of the proposed method. Overall the paper makes a valuable contribution to the field and with further development the proposed method could become an essential tool for CNN analysis and optimization.", "u6s8dSporO8": " Peer Review  1. Summary of the Paper This paper introduces Group Equivariant Neural Posterior Estimation (GNPE) a method designed to incorporate equivariances under joint transformations of parameters and data into simulationbased inference specifically in the context of solving inverse problems. Equivariances or the property that certain transformations of inputs lead to predictable transformations of outputs are common in scientific models but have been challenging to integrate directly into the architectures of expressive inference networks like normalizing flows. The authors present GNPE as an architectureindependent alternative that can handle both exact and approximate equivariances. They apply GNPE to the task of inferring astrophysical binary black hole systems from gravitational wave observations demonstrating stateoftheart accuracy with significantly reduced inference times compared to traditional methods. The paper covers the theoretical framework behind GNPE including its integration with neural posterior estimation (NPE) and its application to both a toy example and realworld gravitational wave data.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Impact: The paper addresses a significant gap in incorporating geometric properties such as equivariances into simulationbased inference offering a novel approach with broad applicability to scientific problems. The application to gravitational wave data analysis is particularly impactful.  Generality of the Method: GNPE is presented as architectureindependent making it potentially useful for a wide range of problem settings and network architectures.  Efficiency: The method achieves a dramatic reduction in inference times which is crucial for analyzing highdimensional and computationally expensive models like those encountered in astrophysics. Weak Points:  Accessibility to a Broader Audience: The paper is dense with technical detail which while necessary might make it less accessible to readers outside the field of simulationbased inference or those not already familiar with concepts like normalizing flows or equivariances.  Validation on a Broad Range of Applications: While the application to gravitational wave data analysis is compelling further evidence of the methods effectiveness across a broader range of scientific problems would strengthen the claims of generality. Questions and Feedback:  Scope for Explicit Equivariance Integration: Could the authors comment on any potential for integrating GNPE with explicit equivarianceenforcing network architectures in the future to potentially further improve performance  Handling of Approximate Equivariances: The methods ability to handle approximate equivariances is intriguing. Could the authors elaborate on the range of approximations that can be handled effectively and whether there are limitations to this approach  Comparison with StateoftheArt Alternatives: It would be beneficial to see a more detailed comparison of GNPE against other stateoftheart methods specifically in terms of simulation efficiency and accuracy across a diverse set of problems.  Interpretability and Analysis: Can GNPE offer insights into the data or the inference process itself beyond efficiency gains For example can the transformed data under GNPE provide new perspectives on the problem being studied  3. Review Summary The paper introduces an innovative and potentially transformative method GNPE for incorporating equivariances into simulationbased inference demonstrating significant improvements in efficiency and accuracy in a challenging realworld problem. The methods architecture independence and applicability to both exact and approximate equivariances are significant strengths. However the paper could benefit from a broader demonstration across diverse scientific problems and a clearer exposition for readers less familiar with the underlying technical concepts. Further discussion on the potential for integrating GNPE with explicitly equivariant architectures the limits of handling approximate equivariances and a detailed comparison with other stateoftheart methods would also enhance the paper. In conclusion GNPE represents a significant advancement in the field of simulationbased inference with promising implications for a wide range of scientific applications. The methods efficiency versatility and the demonstrated application to gravitational wave data analysis are commendable. With some enhancements as suggested this work has the potential to set a new standard in the field.", "vdbidlOkeF0": " Peer Review for \"Scaled Density Ratio Estimation for Machine Learning\"  1. Summary of the Paper This paper presents a novel approach to estimate the discrepancy between two probability densities an important problem in machine learning. Traditional methods estimate this discrepancy by modeling the ratio of the given densities. However accurately estimating these ratios when the densities are well separated is challenging. The paper first critiques the limitations and theoretical inconsistencies of current stateoftheart methods especially in dealing with wellseparated densities. It then introduces a new framework based on scaledBregman divergence which involves scaling the densities by an additional density and estimating the log ratio of the scaled densities. This approach termed Scaled Density Ratio Estimator (SDRE) is shown to resolve issues present in existing methods providing a theoretically sound and computationally efficient solution. The effectiveness of SDRE is demonstrated through extensive empirical evaluation on synthetic and real datasets across different tasks such as density ratio estimation mutual information estimation and representation learning. The paper also illustrates how SDRE can be adapted to enhance modelfree density ratio estimators.  2. Strengths and Weaknesses Strengths: 1. Theoretical Contribution: The paper introduces a novel theoretical framework for density ratio estimation that overcomes the limitations of existing approaches. By employing scaledBregman divergence the method provides a solid theoretical grounding for the problem at hand. 2. Comprehensive Evaluation: The authors provide a thorough empirical evaluation of their method across various tasks and datasets. The inclusion of both synthetic and realworld data along with comparison to stateoftheart methods convincingly demonstrates the effectiveness of SDRE. 3. Generalization to ModelFree Methods: The ability of SDRE to improve modelfree density ratio estimators is a significant contribution broadening the applicability of the proposed method. Weaknesses: 1. Complexity in Understanding: The mathematical and technical depth of the paper while a testament to its rigor may make it inaccessible to a broader audience. Simplifying the explanations or providing more intuitive examples could help in this regard. 2. Comparative Analysis: While the paper presents comparisons with existing methods a more detailed analysis of the performance differences and the specific conditions under which SDRE excels could provide deeper insights into its advantages and limitations. 3. Scalability Concerns: The paper does not adequately address scalability issues. Considering the computational complexities involved in multiclass logistic regression with multiple scaling densities discussing the method\u2019s scalability to very large datasets would be beneficial.  3. Questions and Feedback  Could the authors provide more intuitive explanations or visualizations to help readers better understand the technical aspects of scaledBregman divergence and how it facilitates density ratio estimation  In the empirical evaluation section were there any settings or conditions under which SDRE did not perform as expected Acknowledging such scenarios would offer a more balanced view of the method\u2019s applicability.  How does the choice of the scaling density affect the performance of SDRE and are there any guidelines for selecting an appropriate scaling density for a given problem  Can the authors discuss the scalability of their method in more detail especially concerning computational resources and time when applied to very large datasets  Review Summary The paper introduces SDRE a promising new approach for estimating the discrepancy between two densities addressing limitations of current stateoftheart methods especially in scenarios where the densities are well separated. The theoretical foundation is solid and the empirical evaluations across different tasks demonstrate the methods effectiveness convincingly. However the paper could improve by making the technical content more accessible providing a deeper comparative analysis with existing methods and discussing scalability concerns. Based on the strengths and contributions of the paper I recommend its acceptance provided that the authors address the feedback and questions raised to refine and improve the clarity and depth of their work.", "wQ7RCayXUSl": " Summarize the paper: The paper presents Mixture of Support Gradient (MSG) a novel offlinebatch reinforcement learning (RL) method that addresses the limitations of existing \"support constraint\" approaches. These traditional methods enforce policies to stay within the observed datas support based on the assumption that actions outside this support could lead to the worst outcomes. MSG challenges this by incorporating uncertainty estimation into decisionmaking allowing for actions beyond the datasets support in a controlled manner. MSG employs an ensemble of Qfunctions updated independently to estimate the uncertainty around action values. This approach diverges from common ensemble techniques in RL which typically share target values across the Qfunctions in the ensemble. By acting conservatively according to a lowerconfidence bound on the uncertainty estimates MSG is able to both achieve theoretical improvements (backed by literature on infinitewidth neural networks) and outperform stateoftheart methods in empirical tests including challenging antmaze domains from the D4RL benchmark. Despite MSGs reliance on computationally expensive deep ensembles the paper also explores the efficacy of more efficient ensemble approximations and finds that while some can surpass current methods they do not fully match the robustness and performance of the deep ensembles used in MSG.  Strong Points: 1. Novelty and Theoretical Contribution: MSG provides a significant conceptual and methodological advancement in offline RL by incorporating uncertainty estimates into policy decisionmaking. The unique use of independently updated ensembles based on theoretical insights is a compelling contribution. 2. Empirical Validation: The extensive empirical evaluations across various domains especially the challenging antmaze tasks demonstrate MSGs superiority over welltuned stateoftheart methods. 3. Insight into Ensembles: The paper provides valuable insights into how ensembles should be employed in RL underlining the importance of independence among ensemble members for more accurate uncertainty estimation.  Weak Points: 1. Computational Efficiency Concerns: MSGs reliance on deep ensembles poses substantial computational and memory costs which could limit its applicability in largerscale problems or environments with constrained resources. 2. Lack of Diverse Domains: Although MSG shows impressive performance on D4RL benchmarks its generalization to a wider range of tasks particularly those with different characteristics (e.g. highdimensional observation spaces realworld tasks) remains to be explored. 3. Insufficient Discussion on Limitations: The paper could further discuss the theoretical and practical limitations of MSG such as potential degradation in performance under certain conditions or the impact of ensemble size and architecture choice on uncertainty estimation quality.  Questions and Feedback to the Author: 1. Regarding Computational Efficiency: Is there ongoing or planned work to address the computational demands of MSG perhaps through novel efficient ensemble methods tailored for RL 2. Generalization to Other Domains: Can the authors provide insights or preliminary results on applying MSG to environments outside the D4RL benchmarks especially in realworld applications 3. Impact of Ensemble Configuration: How sensitive is MSGs performance to the configuration of the ensemble (e.g. number of members architecture specifics) Could there be a \"sweet spot\" that balances performance with computational efficiency  Review Summary: The paper introduces MSG an innovative approach in offline RL that significantly advances the field by integrating uncertainty estimates for action values which allows for actions beyond the datasets support in a controlled manner. The theoretical underpinnings notably the concept of using independently updated ensembles for uncertainty estimation are solid and represent a meaningful contribution. Empirically MSG demonstrates superior performance across several benchmarks validating its effectiveness and potential impact. However the computational demands of using deep ensembles raise questions about MSGs scalability and applicability in more resourceconstrained environments or more complex tasks. Additionally while the empirical results are impressive extending the evaluation to a broader range of tasks would further strengthen the case for MSGs general applicability. In summary MSG is a significant step forward in offline RL offering a promising direction for future research particularly in refining the approach to enhance computational efficiency and exploring its application to a wider array of RL problems.", "fpU10jwpPvw": " Peer Review of \"Improving Generative Adversarial Networks with Folded Hamiltonian Monte Carlo\"  Paper Summary This paper introduces a novel Bayesian formulation for unsupervised and semisupervised Generative Adversarial Network (GAN) learning. By incorporating Folded Hamiltonian Monte Carlo (FHMC) it markedly improves the generative models ability to sample the weights of both generators and discriminators. This method promises to produce higher quality more diverse samples while efficiently managing the complexities of highdimensional data spaces. Through theoretical formulation mathematical proof and empirical validation the paper convincingly demonstrates that FHMC can outperform established methods across several metrics including autocorrelation of generated samples test error rates and both Inception Score (IS) and Frechet Inception Distance (FID) scores on multiple datasets.  Strong Points 1. Innovative Approach: The introduction of FHMC as a sampling method for GANs is novel and addresses critical challenges in existing models including mode collapse and the inability to efficiently sample highdimensional spaces. 2. Comprehensive Theoretical Backing: The paper offers a robust theoretical framework and mathematical proof supporting the efficacy of FHMC enhancing the credibility of the proposed model. 3. Empirical Validation: A wide array of experiments covering synthetic multimodal data and natural image benchmarks (CIFAR10 SVHN and ImageNet) provides strong empirical evidence of the models performance improvements. 4. Clarity and Structure: The paper is wellorganized and clearly written making complex concepts and methodologies accessible to readers with foundational knowledge in the field.  Weak Points 1. Hyperparameter Sensitivity: While the paper touches on the subject it lacks a detailed exploration of the sensitivity of FHMC to its hyperparameters and how this might impact its deployment in different scenarios. 2. Comparison with Latest Methods: Although the paper compares FHMC with stateoftheart methods it may benefit from a broader comparison including more recent advances in GANs that were not covered. 3. RealWorld Applications Limitation: The paper primarily focuses on benchmark datasets. Additional insights into applying the proposed method to realworld problems and data could enhance its practical value.  Questions and Feedback to the Author 1. Hyperparameter Sensitivity: Could the authors elaborate on how sensitive FHMCs performance is to its hyperparameters A deeper analysis could help practitioners in finetuning the model for specific applications. 2. Scalability: How does the parallel composition of FHMC scale with the dimensionality and complexity of the data Are there limitations in terms of computational resources or execution time 3. RealWorld Applications: Can the authors discuss potential realworld applications where FHMCs advantages would be particularly beneficial Insights into practical deployments could highlight the models utility beyond benchmark datasets. 4. Comparative Analysis: Could a future iteration of this work include a comparison with more recent GAN methodologies particularly those that might have emerged post the development of FHMC  Review Summary The paper introduces Folded Hamiltonian Monte Carlo (FHMC) a significant advancement in GAN learning for unsupervised and semisupervised scenarios. Its novel approach to sampling generator and discriminator weights promises to mitigate longstanding challenges such as mode collapse and inefficiency in highdimensional spaces. Backed by strong theoretical foundations and comprehensive empirical evidence the model demonstrates superior performance across multiple metrics and datasets. While the innovation and contributions are clear the paper could be further strengthened by addressing its sensitivity to hyperparameters expanding comparisons to include more recent GAN advancements and providing deeper insights into realworld applicability. Despite these areas for improvement the work presents a compelling case for the use of FHMC in GAN models marking a potentially transformative addition to the field of generative models. In summary the paper is a valuable contribution to the deep learning community particularly for researchers and practitioners focusing on generative models. Its advancements hold the promise of broader applicability and improved performance in generating highfidelity samples across various data types and domains.", "gdegUuC_fxR": " Review of \"Accelerated GradientBased MCMC by HighResolution Dynamics\"  Summary This paper presents an innovative Markov Chain Monte Carlo (MCMC) method termed HFHR aimed at accelerating gradientbased sampling by incorporating key insights from Nesterov\u2019s Accelerated Gradient (NAG) method designed for optimization. The primary contribution lies in reconceptualizing NAGSC an optimizer for strongly convex functions into a sampling algorithm through a novel HessianFree HighResolution ODE enabling controlled injection of noise and discretetime acceleration. The authors propose an efficient discretization strategy for the HFHR dynamics and empirically demonstrate its superior convergence properties compared to traditional underdamped Langevin dynamics especially in the context of logstronglyconcave target distributions. The paper also tests the proposed algorithm through numerical experiments including application to a Bayesian Neural Network showcasing the practical relevance and potential of the HFHR method.  Strengths 1. Innovative Approach: The idea of leveraging optimization algorithms particularly NAGSC for accelerated sampling is both novel and potentially impactful for a wide range of applications in machine learning and statistical inference. 2. Theoretical Foundation: The paper provides a solid theoretical analysis of the proposed HFHR dynamics including proofs of additional acceleration over underdamped Langevin dynamics and a dedicated discretization method. 3. Empirical Validation: Numerical experiments including on a highdimensional and nonconvex example of Bayesian Neural Networks not only ground the theoretical findings in practical scenarios but also illustrate the methods applicability to complex models. 4. Thorough Discussion: The detailed exposition of the HFHR construction from the continuoustime reformulation to noise injection and subsequent discretization offers a comprehensive view of the methods development.  Weak Points 1. Comparison Baseline: While the paper includes numerical comparisons with underdamped Langevin dynamics it might benefit from a broader set of baselines including more recent accelerated MCMC methods for a comprehensive evaluation of the HFHR algorithms performance. 2. Complexity and Implementability: The transition from NAGSC to HFHR involving the addition of noise and discretization seems technically complex which may pose challenges for implementers. Further clarification on implementation details or provision of pseudocode might aid in understanding and adoption. 3. Discussion on Limitations: The manuscript could improve by discussing the limitations of the HFHR method including potential scenarios where HFHR may underperform or the circumstances under which its theoretical guarantees may not hold.  Questions and Feedback to the Author 1. Comparative Analysis: Can the authors expand on how HFHR compares to the latest accelerated MCMC methods beyond underdamped Langevin dynamics in both theoretical and practical aspects 2. Implementation Challenges: Could the authors elaborate on the potential challenges in implementing the proposed method especially when dealing with highdimensional problems or in cases where gradient evaluations are expensive 3. Limitations and Failure Modes: It would be beneficial for the readers if the authors could discuss any identified limitations of their approach or situations where HFHR might not be the optimal choice.  Review Summary The paper makes a significant contribution by introducing the HFHR algorithm an accelerated gradientbased MCMC method inspired by the optimization domain specifically NAGSC. The comprehensive theoretical analysis coupled with empirical validations demonstrates the methods potential to outperform existing sampling methods notably in acceleration and efficiency. However the manuscript could be strengthened by incorporating broader comparative analyses offering more guidance on implementation aspects and discussing limitations more thoroughly. Overall given the novelty theoretical grounding and demonstrated empirical performance the paper signifies a noteworthy advancement in the field of accelerated sampling methods.", "ucASPPD9GKN": " Peer Review for \"Revisiting the Performance of Graph Convolutional Networks on Heterophilous Graphs\"  1) Summary of the Paper This paper challenges the widely held belief that Graph Neural Networks (GNNs) particularly Graph Convolutional Networks (GCNs) inherently require homophily (the principle that similar nodes are connected) to perform effectively on semisupervised node classification (SSNC) tasks. Through empirical findings the authors demonstrate that GCNs can indeed perform competently on heterophilous graphs (where dissimilar nodes are connected) under certain conditions. They provide a nuanced understanding of heterophily differentiating between \"good\" and \"bad\" heterophily based on whether nodes within the same class share similar neighborhood patterns. Through theoretical analysis and empirical observation the paper elucidates under what circumstances GCNs can achieve satisfactory performance on heterophilous graphs. This is further substantiated with experimental results on commonly used benchmark datasets demonstrating that GCNs can outperform both MLP models and specific heterophilyoriented GNN architectures under certain heterophilous conditions.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Perspective: The work challenges the prevailing notion about the limitations of GCNs on heterophilous graphs offering a fresh perspective that enriches the current understanding of GNNs capabilities and limitations.  Comprehensive Analysis: The paper provides both empirical evidence and theoretical backing for its claims enhancing the credibility of its findings. The inclusion of toy examples realworld benchmark analysis and a nuanced differentiation between types of heterophily are particularly commendable.  Practical Implications: By identifying conditions under which GCNs can perform well on heterophilous graphs this study has practical implications for the design and application of GNNs across a wide range of tasks and datasets where homophily may not be pronounced. Weak Points:  Generalizability of Findings: It is uncertain how generalizable the findings are across all types of graphs and GNN architectures. While GCNs are foundational the landscape of GNNs is vast with many variants designed to address limitations of earlier models.  Impact of Hyperparameter Tuning: The paper mentions that careful hyperparameter tuning is needed for GCNs to excel on heterophilous graphs. However it does not thoroughly discuss the sensitivity of performance to hyperparameter choices or the potential overhead this tuning introduces. Questions and Feedback:  How do the findings extend to other GNN architectures especially those designed with heterophily in mind A comparison with more recent GNN variants could provide a clearer picture of GCNs relative performance.  The paper could benefit from a more detailed discussion on the scalability of the proposed approach. How do computation and storage requirements change when adapting GCNs to work on heterophilous graphs especially with largescale datasets  A deeper exploration into the impact of hyperparameter tuning on model performance across diverse graph types would be valuable. Providing insights or guidelines on efficient parameter search strategies could enhance the practical utility of the findings.  3) Review Summary The paper provides a significant contribution to the field of graph representation learning by challenging and refining the understanding of the role of homophily in GNN performance. The combination of empirical evidence theoretical analysis and practical insights offers a comprehensive examination of the conditions under which GCNs can effectively handle heterophilous graphs. While the findings open up new avenues for applying GNNs to a broader range of tasks further exploration into the generalizability of these results across different GNN architectures and scalable settings is needed. Additionally addressing the implications of hyperparameter tuning on model performance could further enhance the works relevance to practitioners. Overall this study represents a valuable step towards expanding the applicability of GNNs and deepens our understanding of their operation and limitations.", "o-1v9hdSult": " PeerReview:  Summarize the paper: This paper addresses the critical area of AI interpretability and the necessity for AI systems to provide understandable explanations of their decisions. Focusing on sequential decisionmaking tasks the work introduces a framework for generating contrastive explanations using userspecified vocabulary. The mechanism involves building partial symbolic models representing an approximation of the AI systems task model which accurately capture userdefined concepts and can be used to explain decisions in terms understandable to the user. The paper demonstrates the application of its methods through user studies and computational experiments on popular Atari game Montezumas Revenge and Sokoban evaluating the utility of the explanations generated.  Strengths and Weaknesses: Strong Points: 1. Innovative Approach: The paper introduces a novel framework for generating explanations in sequential decisionmaking contexts using userspecified concepts addressing a significant gap in interpretability research primarily focused on oneshot classification tasks. 2. Empirical Validation: The approach is empirically tested through user studies and computational experiments providing evidence of its effectiveness in aiding user understanding and preference for explanations grounded in their own vocabulary. 3. Comprehensive Analysis: The paper provides a thorough investigation of related works setting a solid theoretical foundation for their approach and effectively situating their contribution within the broader research context. Weak Points: 1. Potential Scalability Concerns: While the paper presents promising results it is not clear how the methods would scale to more complex or realworld environments where the vocabulary and dynamic interactions significantly increase. 2. Classifier Dependency: The systems reliance on classifiers to map internal states to userspecified concepts raises questions about robustness and performance impacts due to classifier inaccuracies. 3. Limited Discussion on Limitations and Future Work: Although the paper briefly discusses future work and limitations a more indepth exploration of the challenges in broader applicability including dealing with stochasticity and partial observability could enhance its contribution. Questions and Feedback to the Author: 1. How do the authors envision scaling this approach to more complex or less structured environments where the vocabulary needed for explanations might be significantly larger or harder to define 2. Can the authors elaborate on the robustness of their system in scenarios where the classifiers have low accuracy or when concepts may overlap or be ambiguous 3. Feedback: It would be beneficial for the paper to include a more detailed discussion on limitations and plausible approaches to address these potentially broadening the scope of the frameworks applicability and fostering further research in this direction.  Review Summary: This paper contributes significantly to the field of AI interpretability by introducing a novel framework for generating userunderstandable explanations in sequential decisionmaking tasks. Its focus on userspecified vocabulary and the empirical validation of its approach are strong points clearly demonstrating potential to enhance humanAI collaboration. However the scalability of the method and its reliance on classifiers pose certain challenges that warrant further investigation. Despite these concerns the paper presents a vital step forward in making AI systems decisions more transparent and understandable to users opening avenues for future research to build upon its findings. Overall the strengths of the paper including its innovative approach and comprehensive analysis outweigh its limitations making a compelling case for its importance to the research community and its applicability in realworld AI systems.", "j-63FSNcO5a": " Peer Review of \"Disentanglement via Contrast (DisCo): A Contrastive Learning Framework for Disentangled Representation Learning in Pretrained Generative Models\"  1) Summary of the Paper This paper introduces Disentanglement via Contrast (DisCo) a novel framework aimed at learning disentangled representation using pretrained generative models without requiring additional disentanglement regularization terms. The approach leverages the inherent disentanglement capabilities observed in the latent spaces of generative models like GANs VAEs and Flows. DisCo consists of a Navigator for identifying candidate traversal directions in the latent space and a \u0394Contrastor to model image variations and apply contrastive learning in what is referred to as the Variation Space. Key innovations include an entropybased domination loss and a hard negatives flipping strategy aimed at improving the extraction of disentangled representations. The paper presents a comprehensive evaluation of DisCo across several datasets and generative models showing superior performance in disentangled representation learning compared to existing methods.  2) Strong and Weak Points Questions and Feedback Strong Points: 1. Innovation and Significance: DisCo creatively combines contrastive learning with disentangled representation extraction from pretrained generative models. The method addresses the tradeoff between disentanglement and image quality without requiring disentanglementspecific regularization. 2. Technical Soundness: The proposed approach is technically solid incorporating unique solutions like the entropybased domination loss and a hard negatives flipping strategy which contribute to its effectiveness in learning disentangled representations. 3. Comprehensive Evaluation: The extensive experiments across multiple generative models and datasets along with comparisons to stateoftheart methods provide strong evidence of DisCos superior performance in disentangled representation learning. Weak Points: 1. Generality of the Approach: While impressive the performance of DisCo might be limited by the quality and characteristics of the pretrained generative models. The paper could further discuss implications for models that do not have inherently meaningful latent spaces. 2. Complexity and Computational Requirements: Although the method outperforms existing approaches the paper does not discuss the computational requirements or potential scalability issues especially in more complex or largerscale scenarios. 3. Exploration of Parameters: While the paper presents an ablation study a more indepth exploration of the models sensitivity to hyperparameters and their optimization could be beneficial for reproducibility and practical application. Questions and Feedback: 1. Impact of Pretrained Model Quality: How does the quality or the way a generative model is pretrained influence the performance of DisCo Are there specific characteristics of pretrained models that are more conducive to disentanglement using DisCo 2. Adaptability to Other Domains: Could DisCo be adapted for tasks beyond image generation such as audio or text What modifications if any would be necessary 3. Comparative Analysis: The paper could benefit from a deeper comparative analysis with baseline methods particularly in discussing why DisCo outperforms these methods beyond the metric comparisons. What are the intrinsic qualities of contrastive learning that make it superior in this context  3) Review Summary and Recommendation The paper \"Disentanglement via Contrast (DisCo): A Contrastive Learning Framework for Disentangled Representation Learning in Pretrained Generative Models\" presents a novel and technically sound approach to learning disentangled representations without the need for specific disentanglement regularization. The methods innovation lies in its use of contrastive learning and unique strategies like entropybased domination loss and hard negatives flipping which collectively lead to stateoftheart performance in disentangled representation learning. Despite its strengths the paper could be improved by addressing the limitations related to the methods generality computational requirements and parameter sensitivity. Further elucidating on how the quality of the pretrained model affects DisCos performance potential applications beyond image generation and a deeper comparative analysis with baseline methods would enhance the papers contribution. Overall the paper represents a significant advancement in the field of representation learning. With minor improvements and additional clarifications it holds the potential to influence future research on disentangled representation learning and contrastive learning methodologies. Thus I recommend the paper for publication contingent upon the consideration and incorporation of the feedback provided.", "uSE03demja": "Summarize the paper: This paper proposes a novel method for inferring the dynamic parameters of physical systems directly from videos under unknown rendering conditions. The approach combines domain randomization state estimation and rendering gradients within a RenderingInvariant State Prediction (RISP) network. This network learns to transform image differences into state differences effectively making its predictions independent of rendering configurations like lighting and material properties. To train this predictor a new loss function leveraging gradients from differentiable rendering is introduced alongside an efficient method for computing these gradients. The approach is evaluated across various tasks in simulation environments and a realworld application showing superior performance in inferring system dynamics directly from video with better generalizability to unknown rendering conditions. Strong and weak points Questions and Feedback: Strong Points: 1. Innovativeness: The paper introduces a novel concept of combining domain randomization with rendering gradients to predict state information from images despite unknown rendering configurations. This is a significant advancement in the field of dynamic system parameter inference from video data. 2. Comprehensive Evaluation: The authors conduct thorough experiments across different environments and tasks\u2014including state estimation system identification imitation learning and visuomotor control\u2014to demonstrate the efficacy and generalizability of their approach. The inclusion of a realworld application further strengthens the evaluation. 3. Theoretical Contribution: The paper presents a novel loss function incorporating rendering gradients and elaborates on an efficient secondorder method for computing this loss contributing valuable insights and tools to the research community. Weak Points: 1. Assumptions Limitations: The methods effectiveness is predicated on the availability of a differentiable simulator and knowledge of camera parameters limiting its applicability to scenarios where such conditions are met. 2. Computational Efficiency: While the paper presents an efficient method for computing gradients of the proposed loss function the overall computational requirements especially for realtime applications are not thoroughly discussed. It remains unclear how scalable this approach is to more complex or largerscale systems. 3. Lack of Comparison with StateoftheArt (SoTA): The paper could benefit from a more detailed comparison with SoTA methods including a discussion on the advantages and potential limitations of the proposed approach compared to existing techniques. QuestionsFeedback: 1. Scalability and RealTime Application: Could the authors discuss the computational efficiency of their approach in more detail How feasible is it to apply this method in realtime applications or on larger more complex systems 2. Generalization Across Diverse Physical Systems: How well does the proposed method generalize across a wider range of physical systems particularly those not evaluated in the paper such as fluid dynamics or highly deformable bodies 3. Impact of Camera Parameter Assumptions: The assumption of known camera parameters might limit the methods applicability in general realworld scenarios. Could future iterations of the model incorporate camera parameter estimation to overcome this limitation Review Summary: The paper presents a pioneering approach towards inferring dynamic parameters of physical systems from videos with unknown rendering conditions. By innovatively integrating domain randomization with rendering gradients in a RISP network the authors address significant challenges in the field showcasing their methods efficacy through comprehensive evaluations. While the proposal marks a substantial step forward the reliance on specific conditions like differentiable simulators and known camera parameters slightly limits its broader applicability. Furthermore the computational demands of the method and its scalability to more complex systems remain areas needing further elucidation. Nonetheless the novel loss function and efficient gradient computation method are significant theoretical contributions that pave the way for future research. Recommendation for publication is strong provided the authors address the scalability concerns and explore the potential for realtime application. Enhancing generalizability by incorporating broader physical systems and alleviating stringent assumptions could further elevate the works impact.", "nsjkNB2oKsQ": " Review of \"Deep Reinforcement Learning Algorithms with Delayed Rewards\"  Summary This paper addresses the challenges associated with delayed rewards in deep reinforcement learning (RL) where rewards for actions taken by an agent in a given state are not instantly available or clearly defined. The study begins by formalizing the concept of Delayed Reward Markov Decision Processes (DRMDPs) that contend with both nonMarkovian and delayed reward issues presenting a largely unexplored challenge in RL. The authors propose a novel offpolicy RL framework that modifies the traditional Qfunction to accommodate delayed rewards offering theoretical convergence guarantees. This modification includes a decomposition technique (HCdecomposition) for the Qfunction enhancing the frameworks efficiency and stability in highdimensional tasks. Comprehensive experiments demonstrate the advantages of the proposed method over current leading algorithms showcasing significant improvements in training robustness and efficiency.  Strong Points 1. Theoretical Contribution: The paper introduces a strong theoretical foundation for handling delayed rewards in RL. The definition of PastInvariant Delayed Reward MDPs and the formalization of a new Qfunction to address nonMarkovian reward issues are significant contributions to the RL domain. 2. Practical Methodology: The HCdecomposition technique presents a practical solution to optimize the proposed framework in highdimensional state spaces. This approach not only addresses the computational complexity but also improves the learning efficiency and stability making it a valuable tool for realworld applications. 3. Experimental Validation: The authors conduct extensive experiments to validate their claims. The comparison with stateoftheart algorithms and the exploration of different HCdecomposition implementations provide solid evidence of the proposed methods superior performance in handling delayed rewards.  Weak Points 1. Limited Exploration of Future Work: While the conclusion mentions future work possibilities it would benefit from a more detailed exploration of how the proposed framework could evolve. Specifically discussing potential improvements or extending the framework to handle other complexities within RL could enrich the paper. 2. Generalization of Results: The experiments focus on continuous control PIDRMDP tasks in simulated environments. Additional experiments in diverse domains or with realworld data could further validate the robustness and generalizability of the proposed approach. 3. Technical Clarity: Certain sections of the paper especially those discussing the theoretical underpinnings and algorithmic nuances could benefit from a clearer and more intuitive explanation. Enhanced visualizations or simplified examples might help in making the concepts more accessible to a broader audience.  Questions and Feedback 1. Generalization to Discrete Tasks: How well does the proposed HCdecomposition framework perform in discrete action spaces or RL tasks with a fundamentally different nature than continuous control 2. Scalability Concerns: Given the complexities introduced by the HCdecomposition in highdimensional spaces are there any scalability limits or computational concerns that could hinder practical applications 3. Future Work on Algorithmic Enhancements: Could the authors elaborate on potential algorithmic enhancements or adaptations of the framework for tasks with more complex reward structures  Review Summary The paper represents a substantial contribution to the field of reinforcement learning especially in the niche area of handling delayed rewards. The theoretical framework introduced is robust with practical implementations through HCdecomposition showing promising results against existing methods. While the research is comprehensive and wellexecuted expanding upon the future work section and broadening the scope of experiments could further reinforce the papers significance. Considering the strong theoretical foundation practical relevance and demonstrated performance improvements the paper provides valuable insights and advancements in delayed reward RL. Thus it should be considered a significant contribution to the field with potential realworld applications and further research avenues to explore.", "oVfIKuhqfC": " Peer Review of \"Generative Modeling through Diffusion Processes without TimeReversal\"  Summary The paper under review proposes a novel approach to generative modeling through diffusion processes diverging from the traditional denoising diffusion probabilistic modeling (DDPM) frameworks by circumventing the necessity for a timereversal argument. This paper introduces diffusion bridge mixture transport (DBMT) a mechanism that achieves the transport from an arbitrary initial distribution to the desired data distribution directly. The methodology employs a combination of diffusion bridges and a bivariate mixture thereof allowing for an exact construction of diffusion processes targeting the desired distribution with enhanced flexibility regarding the dynamics of the underlying diffusion. This approach leverages novel training objectives adapted for neural network approximation and extends the class of Stochastic Differential Equations (SDEs) suitable for more realistic computer vision applications. Through a comprehensive theoretical framework and practical considerations for implementation the authors set out to establish a generalized framework for generative modeling based on diffusion processes.  Strong Points 1. Novel Methodology: The paper presents an innovative approach to generative modeling by directly constructing diffusion processes that target the desired data distribution bypassing the common constraints and approximations associated with timereversal dynamics in DDPM. 2. Comprehensive Theoretical Framework: The theoretical underpinnings of DBMT and the associated drift adjustments are thoroughly developed providing deep insights into the mechanics of diffusionbased generative models and establishing a solid foundation for the proposed methodology. 3. Enhanced Flexibility in Model Dynamics: The proposed DBMT approach allows for significant flexibility in choosing the underlying diffusion dynamics enabling the model to cater to a wide range of distributions which is a meaningful departure from the constraints observed in traditional DDPM frameworks. 4. Application to Realistic Computer Vision Tasks: The extension of the SDE class to accommodate more complex and realistic diffusion transitions marks a significant advancement in applying diffusionbased generative models to practical computer vision problems. 5. Novel Training Objectives: The introduction of new training objectives designed to approximate the drift adjustments via neural networks presents an effective solution to the computational challenges inherent in the traditional DDPM approach.  Weak Points and Questions 1. Implementation and Computational Cost: While the theoretical framework is sound and the methodology innovative the paper lacks detailed discussion on the practical implementation especially concerning the computational cost and the scalability of the proposed approach to larger datasets and more complex models. 2. Empirical Validation: The paper could benefit from more comprehensive empirical studies or benchmarks against existing models to quantitatively demonstrate the advantages of the DBMT approach in terms of generative modeling performance and efficiency. 3. Comparison with Existing Approaches: A more detailed discussion comparing the proposed method with existing generative modeling approaches beyond the general DDPM framework would provide readers with a clearer understanding of the DBMT\u2019s positioning within the broader research landscape. 4. Robustness and Generalizability: Questions remain regarding the robustness and generalizability of the proposed method across different domains and data distributions. How does the method perform in scenarios with highdimensional complex data distributions  Review Summary The paper introduces an innovative and theoretically sound approach to generative modeling through diffusion processes without relying on timereversal presenting diffusion bridge mixture transport (DBMT) as a viable and flexible alternative to traditional DDPM frameworks. The strong theoretical foundation coupled with novel training objectives and the potential for application in realistic computer vision tasks positions this work as a significant contribution to the field of generative modeling. However the efficacy and practicality of the proposed method would benefit from further empirical validation and comparisons with existing models. Questions regarding the implementation computational cost robustness and generalizability also need addressing to fully ascertain the advantages and potential limitations of the DBMT approach. Overall this paper represents a meaningful step forward in the research on diffusionbased generative modeling warranting further investigation and development. Recommendation: Accept with Minor Revisions. The revisions should focus on addressing the weak points highlighted particularly around empirical validation computational considerations and comparison with existing models.", "mTcO4-QCOB": " Review of the Paper on \"Explainer Astuteness: Connecting the Robustness of Explainability Methods to the Smoothness of Blackbox Functions\"  1. Summary This paper addresses a crucial aspect of the reliability of explainers used in machine learning which is that similar data inputs should yield similar explanations. Specifically the paper introduces and defines the concept of explainer astuteness which measures the likelihood that an explainer provides closely related explanations for closely related data points. This concept is inspired by probabilistic Lipschitzness tying the smoothness of a blackbox function to the reliability of explanations provided by various explainer models like SHAP RISE CXPlain and PredDiff. The authors prove theoretically that smoother blackbox functions (measured through Lipschitzness) result in more astute explanations and verify these claims with empirical data from simulated and real datasets.  2. Evaluation  Strong Points  Novelty and Relevance: The paper addresses a relatively underexplored but increasingly important aspect of machine learning explainability  the reliability and consistency of explanations across similar inputs. This is a significant contribution to the field given the growing reliance on complex and opaque models.  Solid Theoretical Foundation: The introduction of explainer astuteness based on the mathematical foundation of probabilistic Lipschitzness provides a robust framework to assess and guarantee the reliability of explanation methods. The theoretical proofs linking smoothness of prediction functions to explainer astuteness are rigorous and logical.  Empirical Validation: The authors have done an excellent job at empirically validating the theoretical propositions using a mix of simulated and realworld datasets. This not only demonstrates the practical relevance of their findings but also strengthens the case for considering astuteness as a measure of explainer reliability.  Weak Points  Assumption of Uniform Behavior Across Features: The paper seems to assume that all features in a dataset behave uniformly in terms of their influence on the prediction function\u2019s Lipschitzness and by extension the explainer\u2019s astuteness. Realworld data might not always conform to this assumption and features could exhibit varying degrees of influence.  Limited Diversity of Explainers and Functions Tested: While the paper tests its theories on a commendable variety of dataset types and explainer models the exploration of more diverse types of blackbox functions and explainer methods could provide a deeper understanding and broader applicability of the findings.  Focus on Removalbased Feature Attribution Explainers: The restrictiveness to removalbased feature attribution explainers may limit the generalizability of the findings to other types of explainer models such as those based on model internals or surrogate models.  Questions and Feedback to the Author  Could the authors elaborate on how the concept of explainer astuteness would extend or adapt to explainer models not based on feature removal  Given the empirical testing involved several assumptions (e.g. parameter settings for classifiers and explainers) how robust do the authors consider their findings against changes in these assumptions  Are there practical guidelines the authors would recommend for practitioners to either select or modify blackbox models to enhance the astuteness of explainers in application scenarios  3. Review Summary In conclusion the paper makes a valuable contribution by connecting the robustness of explainability methods to the mathematical property of Lipschitzness in blackbox functions. The introduction of explainer astuteness as a formal measure is both novel and significant for advancing the reliability of machinelearning explanations. Despite some limitations in the scope of tested methods and an inherent assumption about feature behavior the theoretical and empirical analysis conducted provides a solid foundation for future exploration and application. Improvements could be made by expanding the types of explainers and prediction functions tested and by considering the varying influences of different features. Overall the papers strengths in novelty theoretical depth and empirical validation make a compelling case for the importance of considering explainer astuteness in the development and evaluation of machine learning explainability methods.", "yfe1VMYAXa4": " Review of \"OntoProtein: Integrating Gene Ontology into Protein Language Models for Improved Representation\"  Summary This paper introduces OntoProtein a pioneering framework designed to enrich protein language models (PLMs) with external biological knowledge from the Gene Ontology (GO) using a novel largescale knowledge graph. The approach cleverly integrates structured biological knowledge into protein representation by employing a hybrid encoder for text and protein sequences along with contrastive learning and knowledgeaware negative sampling for the optimization of both knowledge graph and protein embeddings during pretraining. The authors argue that this knowledgeinfused approach leads to significantly improved performance across several proteinrelated tasks such as proteinprotein interaction prediction protein function prediction and more according to the TAPE benchmark. They demonstrate OntoProteins superiority over stateoftheart methods in their experiments.  Strong Points 1. The proposal of a novel framework that integrates external knowledge from GO into protein pretraining is pioneering and addresses a critical gap in the existing literature. This integration could provide a new direction for research in bioinformatics and computational biology. 2. Extensive experimental validation across multiple proteinrelated tasks provides a strong empirical basis for the claims of improvement over stateoftheart methods due to the incorporation of Gene Ontology knowledge. 3. The creation and release of ProteinKG25 a novel largescale knowledge graph dataset is a valuable resource for the community and promotes further research in the area of protein language pretraining.  Weak Points 1. While the paper presents a significant advancement in methodological development there seems to be a lack of deep discussion on the theoretical foundations specifically on how exactly the integration of GO improves protein embeddings qualitatively. 2. The evaluation lacks a detailed ablation study to pinpoint which components of the proposed framework contribute most significantly to its performance gains. Understanding the impact of each component would provide deeper insights into OntoProteins success and guide future improvements. 3. The mention of \"modelagnostic\" capability is interesting but not thoroughly explored. A detailed examination on how OntoProtein performs when plugged into different existing PLM architectures would have added substantial value to its claims of wide applicability.  Questions and Feedback for the Authors 1. Could the authors elaborate on the theoretical reasoning behind the specific choice of contrastive learning with knowledgeaware negative sampling How does it compare with other potential methods for integrating knowledge into PLMs 2. The paper mentions OntoProteins modelagnostic design. Could you provide more insight or empirical evidence on how it performs when applied to various other PLM architectures 3. In terms of future work the paper suggests extending the approach to protein design. Could the authors speculate on potential challenges and directions in adapting OntoProtein for sequencegenerating tasks  Review Summary The paper presents OntoProtein an innovative and promising approach that integrates Gene Ontology knowledge into protein language models aiming to improve protein representation for a variety of tasks. The novel framework extensive experiments and the contribution of a new largescale KG dataset are notable strengths that significantly advance the field. However a deeper theoretical discussion on the intrinsic workings and benefits of incorporating GO a detailed ablation study and further exploration of the modelagnostic claims are areas that could enhance the manuscript. Overall the work is a significant step forward opening up new directions for research in protein analysis and design warranting attention and potential adoption by the community.", "kSwqMH0zn1F": " Summarize the paper This paper introduces PipeGCN a new method for distributed Graph Convolutional Network (GCN) training that significantly increases efficiency and maintains accuracy compared to traditional training methods. The primary innovation of PipeGCN is its pipeline design that allows for overlapping the communication of node featuresgradients among distributed partitions with intrapartition computation effectively reducing the communication overhead that is a major bottleneck in distributed GCN training. The authors conduct a convergence analysis to address the potential challenge of using stale features and gradients due to the pipeline process. They demonstrate that PipeGCN achieves a convergence rate close to that of standard distributed GCN training while significantly boosting training throughput (1.7\u00d7\\xe2\\x88\\xbc28.5\u00d7) without sacrificing accuracy. To mitigate errors incurred from staleness a smoothing method is also proposed. The performance of PipeGCN is validated through extensive experiments and ablation studies showing its superiority over conventional distributed GCN training methods and stateoftheart fullgraph training methods.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Pipelining Mechanism: The paper presents a novel approach to pipelining communication with computation in distributed GCN training effectively tackling the significant communication overhead challenge. 2. Comprehensive Theoretical Analysis: Theoretical convergence analysis adds depth to the research offering a solid theoretical foundation for the proposed PipeGCN methodology. 3. Extensive Experimental Validation: The extensive experiments ablation studies and comparative analysis with stateoftheart methodologies thoroughly validate the effectiveness and efficiency of PipeGCN. 4. Practical Usability: The availability of the implementation code enhances the papers utility for the research community and facilitates reproducibility. Weak Points: 1. Assumptions on System Homogeneity: The paper seems to assume a relatively homogeneous computing environment. Diverse computational resources and network capabilities can impact the performance gains of PipeGCN an aspect not thoroughly explored in the study. 2. Limited Discussion on Scalability Limits: While the paper demonstrates scalability it does not deeply investigate or discuss the upper limits of scalability and potential performance bottlenecks at extremely large scales. 3. Staleness Management: The management of staleness although innovative through the smoothing method might still be a concern for certain types of graphs or data distributions. More nuanced insight into when and how staleness significantly impacts model learning would be beneficial. Questions and Feedback: 1. Heterogeneous Environments: How well does PipeGCN adapt to heterogeneous computing environments where computational resources and network bandwidth greatly vary 2. Upper Scalability Limits: Can the authors discuss any theoretical or empirical insights into the scalability limits of PipeGCN especially concerning very large graphs beyond the scope of the current evaluation 3. Impact of Staleness in Diverse Scenarios: Could more detail be provided on how staleness impacts training in various scenarios such as graphs with highly dynamic structures or those exhibiting heavytailed distributions 4. Comparison with Asynchronous Methods: It would be interesting to see a comparison with asynchronous distributed training methods especially in terms of handling staleness and communication overhead. 5. Evaluation on Realworld Applications: While the method is validated on benchmark datasets an evaluation of PipeGCN on realworld largescale applications could further demonstrate its practical utility and limitations.  Review Summary PipeGCN represents a significant step forward in efficient distributed training of GCNs by cleverly pipelining communication and computation to reduce overheads backed by a solid theoretical convergence analysis. The methods ability to maintain accuracy while significantly boosting training throughput addresses a critical challenge in the field. While the paper is strong on innovation validation and theoretical grounding it could be enriched by deeper investigations into its performance in heterogeneous environments the practical limits of scalability and the nuanced impacts of feature and gradient staleness. Additionally exploring PipeGCNs efficacy in realworld applications and its comparative performance against asynchronous methods could offer valuable insights. Despite these areas for further exploration PipeGCN undoubtedly makes a valuable contribution to the field of graph neural networks offering a robust efficient solution for largescale GCN training.", "shbAgEsk3qM": " Peer Review Report  1) Summary of the Paper This paper delves into the theory of function approximation in reinforcement learning (RL) particularly focusing on the role of deep models in valuebased RL. By exploring overparameterized linear representations it reveals different convergence behaviors among classical updates like temporal difference (TD) learning fitted value iteration (FVI) and residual minimization (RM) in overparameterized settings. The paper transitions from theoretical analysis to practical algorithmic advancements by developing two novel regularizers that enhance the stability and generalization of deep RL algorithms. These regularizers are empirically tested showing promising results in improving the performance of TD FVI and RM in deep learning contexts. This study bridges a gap between theoretical insights and practical applications in reinforcement learning particularly in understanding and leveraging overparameterized models for better value estimation.  2) Strong and Weak Points of the Paper Strong Points:  Theoretical Contributions: The paper provides a rigorous analysis of the behavior of classic value estimation methods (TD FVI RM) in the overparameterized regime. The theoretical insights regarding the convergence to different fixed points the unified interpretation through constrained optimization and the development of generalization error bounds are valuable contributions to the field.  Practical Impact: By translating theoretical insights into practical regularizers and integrating them into deep RL algorithms the paper successfully bridges theory and application. The empirical justification of the proposed regularizers in improving stability and generalization is a significant step forward in designing more robust RL systems.  Novelty: The approach to managing the stabilitygeneralization tradeoff in overparameterized models by introducing novel regularizers based on theoretical insights is innovative. It addresses a critical challenge in deep RL and opens up new avenues for algorithmic improvements. Weak Points:  Generalizability of Results: The empirical studies focus on specific algorithms and environments. It is unclear how generalizable these findings are across a broader range of RL scenarios models or tasks. Additional experiments covering more diverse conditions would strengthen the papers claims.  Comparison with Existing Solutions: While the paper introduces novel regularizers it lacks a detailed comparison against other existing methods designed to improve stability and generalization in RL. This comparison would help clarify the unique advantages or potential tradeoffs associated with the proposed approach.  Theoretical Assumptions: The analysis primarily assumes linear value function approximation which might limit the direct applicability of the findings to scenarios where deep models with highly nonlinear representations are used. Discussing how these insights extend to more complex model architectures would be beneficial. Questions and Feedback to the Author:  Could the authors elaborate on how the proposed regularizers might be adapted or extended to work with more complex nonlinear function approximation schemes commonly used in deep RL  How do the proposed regularizers perform in dynamic environments where the characteristics of the task might change over time and how might this impact their generalization capabilities  Are there any considerations or modifications needed to apply the proposed regularizers in a multiagent learning context or when dealing with partial observability  3) Review Summary This paper provides an insightful analysis into the behavior of classic value estimation methods in the overparameterized regime of function approximation in reinforcement learning. Its theoretical contributions lay a solid foundation for understanding different convergence behaviors and for developing practical tools to improve the stability and generalization of deep RL algorithms. The introduction of novel regularizers based on this theoretical framework is an innovative step towards bridging the gap between theory and practice in RL. While the paper presents substantial advancements there remain areas for improvement chiefly in broadening the empirical evaluation providing clearer comparisons with existing methods and discussing the applicability of the findings to more complex scenarios beyond linear approximations. Further exploration in these areas could enhance the papers impact and provide clearer guidance for leveraging overparameterized models in RL effectively. Overall the paper makes a notable contribution to the field of RL by shedding light on how overparameterization affects the convergence of classical updates and by presenting actionable strategies to enhance learning algorithms performance. The findings have the potential to influence future research directions and algorithm development in reinforcement learning making it a valuable contribution to the literature.", "twv2QlJhXzo": " Review of \"Advisoraugmented Imitation Learning from Observations for Transition Model Disparity\"  1. Summary The paper introduces an innovative methodology for imitation learning from observations (ILO) that effectively addresses the challenge of transition dynamics mismatch between expert and learner environments\u2014an issue common in realworld applications. Traditional adversarial ILO methods strive to match statetransition distributions of the expert and learner however these approaches falter when discrepancies in transition dynamics exist. The proposed algorithm Advisoraugmented Imitation Learning from Observations (AILO) overcomes this by creating an intermediary policy referred to as the advisor in the learners environment. The advisor acts as a surrogate expert and its training focuses on minimizing the crossentropy distance between the stateconditional nextstate distributions of itself and the expert. This concept is supported by integrating ideas from distribution support estimation specifically the RED algorithm to make the advisors policy training scalable and practical. Comprehensive experiments conducted on five MuJoCo locomotion tasks underscore AILOs superior performance relative to existing ILO methods particularly in scenarios with dynamics mismatch.  2. Strong and Weak Points Questions and Feedback Strong Points:  The proposed AILO framework innovatively addresses the challenge of transition dynamics mismatch which is a significant and common problem in realworld ILO applications highlighting the papers high relevance and potential impact.  Incorporating the concept of an advisor policy as a surrogate expert is a novel approach that enriches the literature on ILO and opens avenues for further research on tackling dynamics discrepancies.  The integration of distribution support estimation (RED) to train the advisor policy is both innovative and technically sound showcasing the authors adeptness at combining existing theoretical frameworks to solve new problems.  Experiments are welldesigned covering a range of tasks and dynamics mismatch scenarios which effectively demonstrate AILOs superiority and robustness compared to baseline methods. Weak Points:  The paper assumes that the state space between the expert and the learner environments is shared which may not always be the case in practical applications. Handling completely different or partially overlapping state spaces could significantly challenge the proposed methods applicability.  The evaluation primarily focuses on simulation environments with quantitative metrics for performance. An inclusion of realworld tasks or further qualitative analysis might have bolstered the argument for AILOs practical utility.  While the algorithm shows promise the computational complexity and scalability especially in highly complex environments are not thoroughly discussed. Insights into computational requirements could aid in assessing AILOs feasibility for largerscale applications. Questions and Feedback: 1. How does AILO perform in scenarios where the state space is not entirely shared between the expert and learner environments Are there extensions or adaptations of AILO that could address such discrepancies 2. Could the authors elaborate on the computational resources needed for training AILO especially in comparison with the baseline ILO methods Understanding the tradeoff between performance gains and additional computational demands is crucial for realworld applications. 3. The paper mentions potential future enhancements to improve skill transfer in challenging setups (e.g. Ant (light) and Humanoid (heavy)). Could the authors speculate on what these enhancements might entail or what directions might be promising  3. Review Summary The paper presents a significant contribution to the field of imitation learning from observations (ILO) by addressing the challenge of transition dynamics mismatch through the novel AILO algorithm. The approach is both innovative leveraging an advisor policy as a surrogate expert and practical incorporating distribution support estimation for scalable training. The comprehensive experiments carried out in simulation environments convincingly demonstrate AILOs effectiveness and superiority over existing methods. However the paper could benefit from a deeper exploration of AILOs applicability to diverse state spaces and more expansive discussion on computational requirements which are pertinent to assessing its feasibility in realworld situations. Furthermore investigating potential adaptations or enhancements for more complex or qualitatively different environments could enrich the paper and broaden AILOs applicability. Overall the strengths of the manuscript significantly outweigh its weaknesses indicating that it represents a valuable addition to the body of knowledge on ILO. The proposed AILO method has the potential to spur further research in addressing dynamics mismatches and extending ILOs practical applications.", "uc8UsmcInvB": " Summarize the paper The paper introduces a new concept in the understanding and analysis of neural network architectures termed Statistically Meaningful (SM) approximation. This framework emphasizes the importance of not just being able to express a target function within a neural network architecture but also ensuring that the architecture can learn the function effectively with a realistic amount of training data. The authors argue against the traditional lens of infinite precision and unrealistic configurations that inflate the expressivity of architectures which doesnt translate well into practical learnability. To this end two specific case studies are presented: one showing that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity scaling polynomially with the circuit size and another showing that transformers can SM approximate Turing machines with a sample complexity that depends polynomially on the sizes of the Turing machines state space and alphabet as well as logarithmically on its computation time. Furthermore the paper introduces novel tools for analyzing generalization claiming to provide tighter sample complexities than typical VCdimension or normbased bounds which might be of independent interest beyond the discussed cases.  Strong and Weak Points Questions and Feedback to the Author Strong Points: 1. Novel Framework: The concept of Statistically Meaningful (SM) approximation is novel and addresses a significant gap between theoretical expressivity and practical learnability of neural networks. 2. Practical Relevance: The case studies presented have strong practical relevance showing that certain complex function classes can be approximated with realistic training data sizes. 3. Generalization Analysis Tools: The introduction of new tools for analyzing generalization could significantly impact how neural networks are studied and understood beyond the scope of this paper. Weak Points: 1. Complexity of Implementation: The paper while theoretically rich does not provide practical guidelines or examples of how these SM approximations can be implemented or verified in realworld scenarios. 2. Lack of Empirical Validation: The paper strongly focuses on theoretical constructs without empirical validation of the proposed approaches. It would be beneficial to see experiments that confirm the theoretical findings. Questions and Feedback: 1. Can the authors provide practical guidelines on implementing SM approximations for realworld problems For example how would one go about designing a neural network architecture based on the principles outlined for SM approximation 2. Are there specific domains or types of neural network architectures where SM approximation might not be applicable or effective It would be helpful to understand any limitations of this approach. 3. Empirical evidence supporting the theoretical claims would strengthen the papers impact. Can the authors include experiments or simulations that showcase the effectiveness of SM approximation in practice 4. The connection between the novel generalization analysis tools and existing techniques in the literature was briefly mentioned. Can the authors elaborate on how these new tools compare with traditional methods perhaps with examples  Review Summary This paper makes a significant theoretical contribution to the field of neural network research by proposing the concept of Statistically Meaningful (SM) approximation. This approach provides a more realistic lens for studying and understanding the capabilities of neural network architectures focusing on both expressivity and learnability. The case studies on boolean circuits and Turing machines demonstrate the utility and potential of SM approximation in designing neural networks with practical sample complexity bounds. However the papers lack of empirical validation and practical implementation guidelines limits its immediate utility for practitioners in the field. Further work that includes these aspects could greatly enhance the overall impact and applicability of the presented concepts. The introduction of new tools for analyzing generalization is another notable strength of this paper which could have implications beyond SM approximation. Clarification on the practical application of these tools and empirical evidence supporting the theoretical claims made would make a valuable addition to future versions of this work.", "xWRX16GCugt": " Review of \"Sequoia: A Unifying Framework for Continual Learning Research\"  Summary This paper addresses the challenges in the field of Continual Learning (CL) mainly stemming from the diversity of assumptions evaluations procedures and methodological solutions that hampers progress by making research efforts disjointed. To tackle this the authors introduce a taxonomy of CL settings based on a set of assumptions describing them which leads to a hierarchical organization allowing for inheritance among settings. This structure implies that methods developed for a general setting can be easily adapted to more specific ones making research more modular and reusable. The conceptual contribution is materialized through Sequoia a software framework that instantiates the proposed taxonomy. Sequoia supports a wide variety of Continual Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) settings along with methods that can be easily extended and customized. By showcasing Sequoias capabilities with extensive experiments and encouraging contributions through an opensource model the authors aim to unify and propel CL research forward.  Strong Points 1. Innovative Approach: The hierarchical organization of CL settings based on the inheritance of assumptions is a novel and powerful concept. It provides a systematic way to dissect and comprehend the landscape of CL fostering a more structured method of research. 2. Comprehensive Framework: Sequoia is not just a conceptual model but a fully implemented software framework. Its design addresses specific issues in CL research such as evaluation challenges reproducibility problems and the gap between CSL and CRL. 3. Extensive Validation: The authors support their proposal with exhaustive empirical studies demonstrating Sequoias utility across a broad range of settings and methods in both CSL and CRL realms. This level of validation is commendable and highlights Sequoia\u2019s practical relevance.  Weak Points 1. Barrier to Adoption: While Sequoias comprehensive nature is a strength its complexity and the initial learning curve might pose barriers to adoption for new researchers or those looking for lightweight solutions. 2. Performance Overhead: The paper does not discuss the computational overhead introduced by the framework. Given the added abstraction layers and the unified handling of diverse tasks there might be nonnegligible performance implications that users must consider. 3. Limited Explorations Outside CL: Although the authors hint at the potential applicability of their framework principles beyond CL they provide little insight or guidance on how this could be achieved. Expanding the discussion to include potential extensions or adaptations to other research fields would have been enriching.  Questions and Feedback for the Author 1. Adaptability: How easily can Sequoia integrate future CL advancements particularly those challenging the proposed taxonomys assumptions Is there a designed process for updating its hierarchical structure 2. Performance Considerations: Can the authors comment on the computational overhead introduced by Sequoia Have there been any benchmarks comparing the performance of methods implemented traditionally versus within Sequoia 3. Beyond CL: Could the authors elaborate on how the principles behind Sequoia could be adapted for other research areas Are there specific fields they believe would particularly benefit from a similar framework  Review Summary The manuscript presents a significant step forward in organizing Continual Learning research through a wellthoughtout taxonomy and a comprehensive software framework Sequoia. The innovative approach addresses key challenges in the field and promotes reuse and systematic development of CL methods. While the complexity and potential performance implications of the framework might present challenges the thorough validation and open invitation for community contributions mitigate these concerns. The paper could benefit from a more indepth discussion on adaptability performance overhead and applicability beyond CL. Nevertheless the conceptual and practical contributions of this work are substantial making it a valuable asset for both new and experienced researchers in the Continual Learning domain.", "ofLwshMBL_H": " Peer Review  Summary of the Paper This paper introduces a novel approach Task Conditional Neural Network (TCNN) to address the catastrophic forgetting problem in continual learning frameworks. The authors target the issue of conventional deep learning models inability to sequentially learn multiple tasks without forgetting previously acquired knowledge highlighting a significant limitation in achieving general artificial intelligence. The proposed TCNN model incorporates a mixture of experts (MoE) approach combined with a probabilistic layer that helps in estimating the task likelihood. This design allows TCNN to learn and adapt automatically to new tasks without prior knowledge of these tasks during the inference phase fundamentally improving upon existing dynamic network approaches that require explicit task information for neuron activation. The effectiveness of TCNN is demonstrated through experiments on the MNIST and CIFAR100 datasets showing superior performance in continual learning scenarios by reducing interference between tasks and efficiently managing neuron resources.  Strong Points 1. Novel Approach: The introduction of TCNN addresses a significant challenge in continual learning. By automatically identifying tasks through a probabilistic layer it overcomes the need for explicit task information a common requirement in many conventional methods. 2. Theoretical Soundness: The integration of MoE with probabilistic neural networks for task likelihood estimation is theoretically wellfounded. This combination leverages the strengths of both approaches providing a strong underpinning for TCNNs design. 3. Empirical Validation: The experimental results on MNIST and CIFAR100 datasets provide solid evidence of TCNNs effectiveness. The model not only outperforms existing stateoftheart models in terms of accuracy but also demonstrates an enhanced ability to learn new tasks while retaining old ones.  Weak Points 1. Scalability Concerns: Despite the proposed model showing promise in handling a limited number of tasks it remains unclear how TCNNs performance and resource efficiency would scale with an exponentially growing number of tasks. 2. Lack of Comparative Analysis: The paper could benefit from a more comprehensive comparison with recent approaches especially in terms of computational efficiency and actual memory usage to better understand TCNNs position among existing solutions. 3. Limited Domain Testing: Testing on MNIST and CIFAR100 is a good start but the application of TCNN in more complex or realworld continual learning scenarios remains unexplored. These could potentially include domains with more variability and less clear task delineation.  Question and Feedback to the Author 1. Scalability: How does TCNNs performance and resource management scale with a significantly large number of tasks Could there be a threshold beyond which the performance might degrade or memory consumption becomes a critical issue 2. Broader Application: Have the authors considered testing TCNN in more complex realworld settings or tasks that are less defined than those in MNIST or CIFAR100 Such exploration could greatly enhance the papers impact. 3. Parameter Sharing and Reusability: The paper briefly touches upon the potential for reusing neurons and parameters across tasks. Could the authors elaborate more on this aspect potentially demonstrating its effectiveness with experimental results  Review Summary The paper introduces a significant improvement in the field of continual learning through Task Conditional Neural Network (TCNN) particularly in addressing the catastrophic forgetting problem. The novel integration of MoE with probabilistic layers for task identification without requiring prior task information marks a potentially groundbreaking shift towards more adaptive and scalable learning models. While the experimental results are promising showcasing TCNNs superior performance in handling sequential task learning there remain areas for further exploration to fully understand the models capabilities and limits. Specifically concerns regarding how TCNN scales with an increasing number of tasks its comparative efficiency against recent methods and its applicability in more complex realworld scenarios could deepen the papers contribution. Further research on parameter sharing and neuron reusability could also provide valuable insights into making TCNN even more efficient and adaptable. Overall the paper presents a compelling case for TCNN but would benefit from addressing these areas to solidify its claims and potential impact in the field of continual learning.", "pgkwZxLW8b": " Peer Review  1) Summarize the Paper This paper introduces a novel federated learning approach termed federated sampled softmax (FedSS) aimed at efficiently learning image representations in a decentralized data scenario with large numbers of classes. It addresses the computational and communication challenges posed by the softmax crossentropy loss in federated learning (FL) environments by proposing a clientdriven negative class sampling mechanism. This mechanism allows clients to sample a subset of classes and optimize the model parameters related to those classes only effectively approximating the global full softmax objective with a fraction of the computational cost. The authors provide empirical evidence showing that FedSS achieves comparable performance to full softmax methods while significantly reducing the number of parameters that need to be transferred and optimized by client devices. The methodology experimental setup and analysis are presented thoroughly covering various aspects of this federated sampled softmax approach its implementation and its impacts on multiclass image classification and image retrieval tasks.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper tackles a relevant and challenging problem in federated learning and presents a solution that is both innovative and practical.  The proposed FedSS methods dual benefits of reducing computational load on client devices and minimizing communication costs while maintaining competitive model performance are significant contributions to the FL field.  The thorough experimental evaluation with detailed comparison to baseline methods including full softmax and centralized training provides a clear demonstration of FedSSs efficacy. Weak Points:  The paper does not discuss in depth the potential limitations or challenges in implementing FedSS in realworld heterogeneous federated learning environments where devices may vary widely in their computational capabilities.  While the paper mentions the unbiasedness of the sampled softmax gradient estimator there is little discussion on the potential impact of bias in the context of FL and how it might affect model convergence and performance across different setups. Questions and Feedback:  Could the authors elaborate on how FedSS could be adapted or extended to address potential biases introduced by negative class sampling especially in highly skewed data distributions  It would be beneficial if the paper could discuss potential strategies for selecting the optimal size of the negative class sample set (`Nk`) for different federated learning scenarios and datasets.  Feedback: Consider including a comparative analysis or discussion on how FedSS might perform relative to other FL optimization techniques that aim to reduce communication overhead such as model compression or quantization methods.  3) Review Summary The paper presents a significant contribution to the federated learning field with the FedSS algorithm addressing the challenges of learning image representations with large numbers of classes in a decentralized setting. The methodology is sound and the empirical results convincingly demonstrate the benefits of the proposed approach in terms of computational and communication efficiency without sacrificing performance. This work importantly advances the possibility of applying federated learning to more complex and largescale learning tasks. However the paper could be strengthened by addressing the potential biases introduced by negative class sampling and discussing the realworld applicability of FedSS in heterogeneous federated environments. Also elaborating on strategies for optimizing the negative class sample size could provide valuable insights for practitioners aiming to implement FedSS. Overall this paper is a valuable contribution to the area of federated learning offering a practical solution to a complex problem and opening avenues for further research in efficient distributed learning methods.", "wTTjnvGphYj": " Review of \"Learnable Structural and Positional Encodings for Graph Neural Networks\"  1. Summary This paper introduces a novel framework named LSPE (Learnable Structural and Positional Encodings) aimed at enhancing the representation power of Graph Neural Networks (GNNs) by decoupling structural and positional node representations. The motivation stems from the observation that most existing GNN architectures fail to differentiate between structurally identical but positionally distinct nodes due to the absence of canonical positioning in arbitrary graphs. To address this the authors propose a mechanism to inject learnable positional encodings (PE) at each layer of the GNN in addition to the structural representation. These PEs are initially set using random walk or Laplacian based methods and then refined through learning. Extensive experimental evaluations demonstrate significant performance improvements across various molecular graph datasets when LSPE is applied to both sparse and fullyconnected (Transformerlike) GNNs.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The idea of decoupling and learning positional and structural representations within GNNs is novel and addresses a wellknown limitation of current GNN models effectively.  Performance Improvement: The paper shows substantial performance improvements across several benchmark datasets reinforcing the effectiveness of the proposed architecture.  Theoretical and Practical Balance: The work is wellgrounded in theoretical intuition while providing practical utility with an openly available codebase for replication and future research exploration. Weak Points:  Generalization Beyond Molecular Graphs: While the paper presents positive results on molecular datasets it is less clear how this architecture performs on nonmolecular graphs or graphs representing different types of data structures.  Complexity and Scalability: Incorporating learnable PEs at every layer might substantially increase the model complexity and computational demand but the paper does not discuss these aspects thoroughly.  Lack of Theoretical Analysis on the Choice of PE Initializations: The paper uses Laplacian and random walkbased methods for initializing PEs but does not provide deep theoretical insights into why these are the best choices or how they compare to other potential methods. QuestionsFeedback to Authors: 1. Generalization: Can the authors provide insights or results on how the proposed LSPE framework performs on nonmolecular graphs 2. Complexity and Scalability: How does the computational complexity of the proposed architecture scale with the size of input graphs especially for fullyconnected Transformerlike GNNs 3. PE Initialization Choices: Is there a theoretical grounding for choosing Laplacian and random walkbased methods for PE initialization and were other methods considered or evaluated 4. Impact of Hyperparameters: The paper introduces new hyperparameters (e.g. for tuning the positional loss). How sensitive are the models performance improvements to these hyperparameter settings  3. Review Summary The paper presents a significant contribution to the field of GNNs by addressing a critical limitation\u2014lack of canonical positioning of nodes in arbitrary graphs\u2014through a novel architecture that learns both structural and positional representations. The experimental results demonstrate the effectiveness of this approach displaying significant improvements over stateoftheart models on several benchmarks. However there are areas such as complexity scalability and generalization beyond molecular graphs where the paper could benefit from additional elaboration or experimentation. Theoretical insights into the choice of initialization methods for PEs and the impact of new hyperparameters introduced would also enhance the papers contribution. Despite these concerns the work represents a significant step forward in the development of more expressive and powerful GNN architectures. The combination of theoretical novelty practical usefulness and demonstrated performance improvements supports a strong recommendation for publication assuming the authors can address the posed questions and feedback adequately in their revision.", "yql6px0bcT": "Summary of the Paper This paper introduces a novel variation of the CrossEntropy Method (CEM) for modelbased reinforcement learning (MBRL) named Decentralized CEM (DecentCEM). Traditionally CEM utilizes a centralized approach where a single sampling distribution is updated based on a global ranking of all samples. The authors argue that this centralized scheme is prone to local optima and detracts from sample efficiency especially in multimodal optimization tasks. In contrast DecentCEM decentralizes the process by running multiple independent CEM instances each updating its own sampling distribution based on local sample ranking. Through a onedimensional multimodal optimization task and several continuous control benchmark environments DecentCEM demonstrates improved consistency in finding global optima and enhances sample efficiency without increasing the sample budget. The paper further extends DecentCEM to include policy networks for even better performance in MBRL settings. Theoretical analysis shows that DecentCEM converges almost surely to a local optimum under the same assumptions as standard CEM. Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Approach: The decentralization of the CEM provides an inventive solution to the problems of local optima and low sample efficiency that plague centralized CEM methods. 2. Improved Performance: DecentCEM not only shows superior performance in locating global optima in multimodal optimization tasks but also enhances sample efficiency in MBRL continuous control environments. 3. Theoretical Soundness: The paper presents a solid theoretical framework that validates the convergence of DecentCEM providing confidence in its applicability and robustness. 4. Integration with Policy Networks: By extending DecentCEM to utilize policy networks the paper addresses the scalability challenge in highdimensional solution spaces demonstrating improved practicality in MBRL applications. Weak Points: 1. Lack of Comparative Analysis: Although the paper presents benchmark comparisons it could benefit from a deeper analysis of how DecentCEM performs against other decentralized optimization methods outside the CEM framework. 2. Computational Complexity: The paper does not fully delve into the computational costs associated with running multiple independent CEM instances especially when scaling to environments with highdimensional action spaces. 3. Generalization: While promising results are shown in specific environments the paper could further explore the generalizability of DecentCEM across a wider range of tasks and settings in MBRL. Questions and Feedback: 1. Can the authors provide insight into the computational tradeoffs involved in increasing the number of independent CEM instances in DecentCEM 2. How does DecentCEM handle environments with sparse rewards or extremely highdimensional action spaces Are there any limitations to its applicability in such cases 3. It would be informative to see a comparison with stateoftheart decentralized optimization methods outside of the CEM framework. Could such a comparison be included to contextualize DecentCEMs performance further 4. Feedback: Incorporating strategies to control the computational complexity such as adaptive instance management or integration with efficient policy networks could further enhance the appeal and applicability of DecentCEM. Review Summary This paper makes a significant contribution to modelbased reinforcement learning by introducing DecentCEM a decentralized variation of the CrossEntropy Method that addresses key limitations of traditional CEM approaches. The proposed method demonstrates improved efficiency in locating global optima and enhancing sample efficiency in continuous control tasks supported by both empirical results and theoretical analysis. Despite its strengths the paper could benefit from a deeper exploration of computational costs a broader comparison with other methods and an examination of its generalizability across more diverse MBRL environments. Overall DecentCEM represents a promising advancement in the field with potential implications for a wide range of optimization and control applications in reinforcement learning.", "iaxWbVx-CG_": "Review of \"Hierarchical Cross Contrastive Learning (HCCL) for SelfSupervised Visual Representation\" 1. Summary: The paper introduces Hierarchical Cross Contrastive Learning (HCCL) a novel approach aimed at enhancing the efficacy of selfsupervised learning (SSL) in computer vision through the utilization of hierarchical projection heads. By extracting and contrasting latent features from multiple layers of the projection head across different views HCCL capitalizes on the untapped potential of hidden layer information which has been overlooked by traditional contrastive loss methods. This crosslevel contrastive learning approach not only enforces invariance across multiple hidden layers but stimulates a deeper generalization in the visual representations obtained. HCCL\u2019s flexibility allows for its application across a variety of SSL frameworks and showcases superior performance in rigorous testing outperforming previous methods across standard benchmarks in classification detection segmentation and fewshot learning tasks. 2. Evaluation: Strong Points:  Novelty: The paper addresses a significant gap in SSL methodologies by proposing a novel use of hierarchical projection heads which is distinct from the traditional approaches that focus solely on optimizing contrastive loss at the output layer.  Robust Evaluation: The experimental setup including a diverse range of tasks and benchmarks provides a comprehensive validation of HCCL\u2019s efficacy and generalizability. Achieving stateoftheart results on these benchmarks underlines the method\u2019s effectiveness.  Methodological Detail: The detailed explanation of the HCCL framework including its implementation and optimization specifics is commendable. The level of detail supports reproducibility and provides a clear understanding of the innovation. Weak Points:  Complexity Analysis Missing: While the paper touches upon the computational aspects briefly a detailed analysis comparing the computational complexity and efficiency of HCCL in comparison to existing methods is lacking.  Limited Discussion on Failure Cases: The paper would benefit from a more thorough discussion on potential limitations or scenarios where HCCL may underperform providing a more balanced view of the method\u2019s applicability.  Broader Impacts and Ethical Considerations: The paper does not discuss the broader impacts especially the ethical implications of deploying such SSL methods in realworld scenarios. Questions and Feedback to the Author: 1. Could the authors provide insights into the computational overhead introduced by the hierarchical projection heads compared to singlelayer projection heads used in traditional methods 2. It would be valuable to include a section discussing the limitations of HCCL and potential scenarios where it might not perform as expected. 3. Considering the significant advancements HCCL offers are there specific realworld applications the authors envision this could have a major impact on 4. How might HCCL be adapted or extended to nonvisual data and are there plans to explore such directions 3. Review Summary: The submission presents a significant contribution to the field of selfsupervised learning with its novel Hierarchical Cross Contrastive Learning method. By leveraging untapped potential in the hidden layers of projection heads for contrastive learning HCCL showcases remarkable improvements in a variety of visionrelated tasks. The proposed method not only advances the theoretical foundations of SSL but also demonstrates practical efficacy underscored by superior performance in standard benchmarks. Despite its strengths the paper could be enriched by addressing its computational implications exploring limitations and considering broader impacts and ethical aspects of deployment. In summary the innovative approach robust evaluation and detailed methodology presented make a compelling case for the adoption of HCCL in advancing SSL capabilities warranting a strong recommendation for publication with minor revisions to address the highlighted areas for improvement.", "zNHzqZ9wrRB": "Summarize the paper: The paper introduces TorchMDNET a novel Transformerbased architecture for predicting quantum mechanical properties with a specific focus on molecular dynamics (MD) simulations. Leveraging the principle of equivariance and employing a modified attention mechanism TorchMDNET achieves stateoftheart performance on standard benchmarks such as MD17 ANI1 and QM9. The paper extensively discusses its architecture which incorporates quantum mechanical approximations and demonstrates through experiments the models superior accuracy and computational efficiency. Additionally an analysis of attention weights provides insights into the models decisionmaking process and highlights the importance of training on offequilibrium conformations for accurate molecular potential predictions. Strong and Weak Points Questions and Feedback: Strong Points: 1. Innovative Architecture: The introduction of an equivariant Transformer architecture represents a significant advance in the field of machine learning potentials particularly for its application in predicting quantum mechanical properties efficiently and accurately. 2. Comprehensive Benchmarking: The authors conduct extensive evaluations against established benchmarks (MD17 ANI1 and QM9) demonstrating clear improvements over existing methods. Such thorough benchmarking lends credibility to the proposed models effectiveness. 3. Insightful Attention Weight Analysis: The analysis of attention weights not only demystifies the decisionmaking process of the network but also offers valuable insights into the behavior of molecular conformations guiding future research directions in the field. Weak Points: 1. Complexity of Implementation: Given the advanced nature of TorchMDNETs architecture including its modified attention mechanism and the application of equivariance the complexity of implementation and adaptability might pose challenges for other researchers or practitioners. 2. Limited Discussion on Failures: The paper significantly emphasizes the successes of TorchMDNET but lacks a detailed discussion on any limitations or instances where the model underperforms which is crucial for a balanced and comprehensive evaluation. 3. Scalability Concerns: While the computational efficiency is highlighted there is minimal exploration of the models scalability with respect to extremely large datasets or molecules which could be crucial for practical applications in material science or drug discovery. Questions and Feedback: 1. Adaptability to Other Molecular Systems: How adaptable is TorchMDNET to predict properties of larger or more complex molecules than those present in the evaluated datasets 2. Comparison with Classical Methods: Could the authors provide more insights into how TorchMDNET compares with classical quantum chemistry methods especially in terms of computational resources and accuracy on a wider range of molecular properties 3. Impact of Dataset Diversity: The importance of offequilibrium conformations is wellnoted. Could the authors elaborate on the implications of dataset diversity for training robustness and how this might affect transferability to novel molecules not present in the training set 4. Future Directions: The paper opens various avenues for future work. It would be beneficial if the authors could discuss potential extensions of TorchMDNET particularly concerning the integration of temporal dynamics in MD simulations and the prediction of timedependent properties. Review Summary: TorchMDNET is a remarkable progression in the field of using machine learning for predicting quantum mechanical properties particularly in molecular dynamics simulations. The introduction of an equivariant Transformer architecture addresses critical challenges in the accurate and efficient prediction of molecular potentials setting new benchmarks across standard datasets. The authors also provide valuable insights into the models behavior through attention weight analysis underscoring the importance of dataset diversity. However discussions on the models limitations adaptability and scalability are somewhat lacking which are essential for a comprehensive evaluation and for understanding the models practical applicability in future scenarios. Specifically the adaptability of TorchMDNET to diverse and larger molecular systems remains an open question that future work could benefit from exploring. Overall the paper makes significant contributions to the field and its findings are poised to influence future research directions. The proposed architecture not only enhances the prediction accuracy for quantum mechanical properties but also offers a new perspective on designing machine learning models for chemistry and material science applications.", "kcrIligNnl": " Review of \"Direct Generation of Molecular Conformations without Interatomic Distances\"  Summary of the Paper The paper introduces an innovative approach for molecular conformation generation aiming to bypass the limitations encountered in predicting interatomic distances by directly generating the 3D coordinates of all atoms in a molecule. The authors utilize a variational autoencoder (VAE) framework that stacks multiple blocks each refining the output conformation from the preceding one to iteratively refine conformations. This method importantly maintains rototranslation equivariance by adopting a loss function invariant to molecular rotation and translation. Tested on four public datasets the authors report that their approach outperforms strong baselines demonstrating its effectiveness and potential advantages over traditional twostage distance prediction methods.  Strong and Weak Points Questions and Feedback Strong Points 1. Innovative Approach: The direct generation of molecular conformations is a novel approach that addresses the inherent limitations of distancebased methods particularly the issue of conflicting interatomic distances. 2. RotoTranslation Equivariance: The handling of rototranslation equivariance through an invariant loss function is a significant strength ensuring that the generated conformations are independent of the molecules orientation and position. 3. Performance: The methods superiority in generating molecular conformations on several datasets reaffirms its potential to advance bioinformatics and pharmacology applications significantly. 4. Open Source: Releasing the code is commendable and facilitates reproducibility and further research. Weak Points 1. Complexity in Understanding: The paper is dense with technical details which might be challenging for readers not deeply familiar with the field. Simplifying the explanation or providing a higherlevel overview could improve accessibility. 2. Comparison with StateoftheArt: While the paper benchmarks against several methods a more detailed discussion on how the proposed method compares with the very latest (including AlphaFold 2 which was briefly mentioned) would offer a clearer picture of its standing. 3. Evaluation Metrics: The choice of evaluation metrics is appropriate but the paper could benefit from including realworld case studies or applications where the direct generation approach provides a distinct advantage. Questions and Feedback 1. Generalization Capability: How well does the proposed method generalize to molecules with unusual or highly complex structures Are there any limitations observed 2. Scalability: The paper mentions scalability as a strength. Could the authors elaborate on the performance implications both in terms of computational resources and accuracy when scaling to datasets significantly larger than those tested 3. Future Directions: Considering the advancement in deep learning approaches what are the possible improvements or extensions to this method that the authors anticipate could lead to even more accurate conformation predictions  Review Summary and Recommendation The paper presents a compelling new method for generating molecular conformations by directly predicting atomic coordinates avoiding the limitations inherent in distancebased approaches. Its innovation lies in addressing the issue of conflicting interatomic distances and maintaining rototranslation equivariance leading to its impressive performance on benchmark datasets. However the paper could benefit from simplifying its presentation to reach a broader audience offering more detailed comparisons with the latest methods in the field and expanding on its evaluation metrics with realworld applications. Addressing these points would strengthen the papers impact and utility. Given the methods demonstrated effectiveness and potential for broad applications in bioinformatics and pharmacology I recommend this paper for publication with minor revisions as outlined in the feedback section. The direct generation approach significantly advances molecular conformation prediction and with further development and clarification it could become a cornerstone in the field.", "wxVpa5z4DU1": " Peer Review Report  1) Summary of the Paper The paper investigates the tradeoff between accuracy and privacy in the context of deep ensemble learning particularly in the face of membership inference (MI) attacks. Deep ensembles which enhance classification accuracy by averaging the outputs of multiple independently trained neural networks are shown to also increase the risk of privacy breaches through MI attacks. The study empirically examines this tradeoff across various datasets and deep learning architectures highlighting that an improvement in accuracy through ensembling can lead to more effective MI attacks. It delves into factors like prediction confidence and model agreement to understand the underlying reasons for this phenomena. Furthermore the paper evaluates different defense mechanisms such as regularization and differential privacy to mitigate MI attack effectiveness but notes the accompanying decline in ensemble accuracy. Advanced ensembling techniques like snapshot ensembles and diversified ensemble networks are also explored revealing a similar accuracyprivacy tradeoff.  2) Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Analysis: The paper offers a thorough empirical study across a diverse set of datasets and model architectures providing a wellrounded understanding of the tradeoff between accuracy and privacy in deep ensemble learning.  Insightful Observations: Through its examination of prediction confidence and model agreement the paper contributes valuable insights into the factors that potentially exacerbate the risk of MI attacks in deep ensembles.  Evaluation of Defense Mechanisms: Discussing various defenses against MI attacks including their limitations is beneficial for researchers working on enhancing privacy in machine learning models. Weak Points:  Lack of Theoretical Foundation: While the empirical results are compelling the paper lacks a theoretical framework that explains the observed phenomena. A deeper theoretical analysis could strengthen the findings.  Limited Scope of Defense Mechanisms: The analysis primarily focuses on generalizationbased defenses without fully exploring confidencemasking approaches which might offer potential avenues for mitigating privacy risks.  Generalizability of Results: The paper does not fully address how the findings might vary concerning different data types or more complex models which could affect the generalizability of the results. Questions and Feedback to the Author:  Theoretical Underpinnings: Can the authors provide any theoretical insights or models that could explain the observed tradeoff between accuracy and privacy in deep ensembles  ConfidenceMasking Defenses: Were confidencemasking defenses omitted from the study due to inherent limitations or could future work explore their potential in addressing the outlined privacy concerns  Impact of Data Complexity: How might the complexity of datasets or the use of significantly more complex models alter the observed tradeoff between accuracy and privacy  3) Review Summary The paper presents a thorough empirical investigation into the tradeoff between accuracy and privacy in deep ensemble learning specifically in the context of membership inference attacks. While it brings valuable insights and extensively analyzes various factors influencing this tradeoff the study could benefit from a stronger theoretical foundation and a broader exploration of defense mechanisms. Questions remain regarding the generalizability of these findings across different data complexities and model architectures. Nonetheless the paper makes a significant contribution to understanding the nuances of balancing accuracy with privacy concerns in ensemble learning methodologies. Future works inspired by this study could explore the outlined weak points and unanswered questions to further advance the field.", "m22XrToDacC": " Review of \"Distributionally Robust Recourse Action Framework\"  1. Summary This paper addresses a key limitation in the development of recourse actions within algorithmic decisionmaking particularly in the context of machine learning models subject to shifts over time. The authors introduce the Distributionally Robust Recourse Action (DiRRAc) framework designed to generate recourse actions that remain valid and effective despite potential changes in the underlying model parameters due to data distribution shifts. By leveraging a robust optimization approach that contemplates a mixture of possible model shifts the authors aim to improve the stability and reliability of recourse actions. The paper includes a comprehensive theoretical framework supported by numerical experiments on both synthetic and realworld data.  2. Strengths and Weaknesses Strengths:  Novel Framework: The proposal of a distributionally robust recourse action framework is novel and addresses a significant gap in existing literature regarding the dynamic nature of machine learning models.  Theoretical Depth: The paper provides a thorough mathematical foundation for the framework including a reformulation of the DiRRAc problem into a finitedimensional optimization problem and a projected gradient descent algorithm for its solution.  Empirical Validation: The authors conduct comprehensive numerical experiments that not only demonstrate the effectiveness of the DiRRAc framework but also extend its application to realworld datasets showcasing practical utility. Weaknesses:  Implementation Complexity: The complexity of the proposed framework and its reliance on projected gradient descent might pose challenges for practitioners in terms of implementation and computational resources.  Lack of Comparative Analysis: While the paper compares DiRRAc with three stateoftheart methods a deeper comparison or analysis of how DiRRAc outperforms these methods specifically in diverse and more complex scenarios would strengthen the claims.  Assumption Clarity: The assumptions regarding the mixture distribution and the operationalization of the Gelbrich distance and its implications could benefit from a more detailed explanation to enhance the readers understanding. Questions and Feedback:  Could the authors provide more insight into the practical implementation challenges that might arise when adopting the DiRRAc framework  How does the complexity of the DiRRAc frameworks optimization problem scale with the dimensionality of the input data and the number of model shifts considered  It would be beneficial if the authors could discuss the limitations or potential pitfalls of using the Gelbrich distance in defining the ambiguity set and its implications on the robustness of the recourse actions.  3. Review Summary The paper presents an important advancement in the field of explainable AI and algorithmic decisionmaking by introducing a framework designed to generate robust recourse actions in the face of model shifts. The theoretical foundation is solid and the empirical evidence supports the effectiveness of the DiRRAc framework. However the paper could benefit from addressing the highlighted weaknesses particularly in simplifying the explanation of assumptions and providing a more indepth comparative analysis with existing methods. Despite these areas for improvement the paper makes a significant contribution to improving the reliability and practical utility of recourse actions in dynamic machine learning environments. Recommendation: Accept with minor revisions to address the feedback regarding comparative analysis assumptions clarity and practical implementation challenges.", "nhN-fqxmNGx": " Peer Review of \"Comparing Variable Selection Methods in HighDimensional Linear Models: A Focus on Hamming Error\" 1) Summary: This paper conducts a detailed theoretical comparison of variable selection methods in the context of highdimensional linear models particularly when faced with challenges posed by moderate to strong correlations among variables. The study\u2019s cornerstone is the evaluation of Lasso and five alternative strategies: Elastic net SCAD forward selection thresholded Lasso and forward backward selection. The performance of these methods is analyzed theoretically focusing on expected Hamming error under a specific blockwise diagonal model for the design matrix and assuming regression coefficients drawn from a twopoint mixture. By exploring the rates of convergence of the Hamming errors and the corresponding phase diagrams the paper presents an insightful analysis of the strengths and performance tradeoffs of each method under varying conditions of sparsity signal strength and design correlations. 2) Strong and Weak Points: Strong Points:  The paper addresses a crucial aspect of highdimensional data analysis i.e. variable selection under conditions of moderate to strong correlations among variables which is a realistic and challenging setting.  The use of expected Hamming error as a metric for method comparison is appropriate and highly relevant for practical applications such as genomewide association studies and genetic regulatory networks where false discovery rate and power are of significant concern.  Comprehensive theoretical analysis provided including the construction of phase diagrams for each method under investigation significantly contributes to our understanding of the relative performances of these methods under various conditions. Weak Points:  The paper focuses almost exclusively on theoretical aspects with relatively less emphasis on empirical validation. While theoretical analysis is invaluable additional empirical studies could enhance the practical relevance of the findings.  The complexity of the theoretical frameworks and the assumptions made (e.g. the blockwise diagonal structure of the Gram matrix) may limit the direct applicability of the results to realworld situations where such assumptions may not hold.  While the comparison across methods is thorough the paper does not provide sufficient guidance for practitioners on how to select between these methods based on the characteristics of their specific application. Questions and Feedback to the Author: 1. The authors mention that the performance of the Elastic net is inferior to Lasso under the studys specific setting. Could the authors elaborate on scenarios where Elastic net might outperform Lasso and the other methods compared in this study 2. The paper provides a rich theoretical foundation but seems to lean less on empirical evidence. Are there plans to supplement the theoretical findings with more extensive empirical analysis Specifically realworld data applications would be highly beneficial. 3. Given the complexity of the theoretical analysis could the authors include a section or a framework suggesting practical guidelines for choosing among these methods based on the features of the data at hand (e.g. level of sparsity signal strength correlations) 3) Review Summary: The manuscript makes a significant theoretical contribution to the field of variable selection in highdimensional linear models particularly under the challenge of correlated variables. By focusing on expected Hamming error and thoroughly comparing six variable selection methods the paper provides insightful analyses into their relative performances. However the paper could be enhanced by including more empirical analyses to complement the theoretical results and by offering practical guidelines for method selection in realworld applications. Given the strengths of the theoretical analysis but balancing the areas that could be improved my recommendation would lean towards minor revisions where the authors could address the concerns raised about empirical validation and applicability to realworld data. Overall the paper is a valuable contribution to the literature offering deep theoretical insights that pave the way for future research and application in the field.", "t8O-4LKFVx": " Peer Review  Summary of the Paper The paper introduces Conformal Training (ConfTr) a novel approach that integrates conformal prediction (CP) directly into the training process of deep learning models. CP a technique that provides formal guarantees on the inclusion of the true class within predicted confidence sets at a specified probability typically operates as a posthoc adjustment to an already trained model. This disconnection limits the models capacity to inherently account for the conformal wrapper potentially resulting in suboptimal confidence set predictions. ConfTr addresses this gap by simulating conformal procedures within minibatches during training enabling endtoend learning that not only adheres to CPs coverage guarantees but also allows for efficient and purposeful shaping of confidence sets. Experimental outcomes demonstrate that models trained with ConfTr achieve smaller confidence set sizes\u2014indicating an advance in efficiency\u2014than those obtainable through traditional posttraining conformal methods. These findings underscore ConfTrs enhancement in managing classspecific inefficiencies and in customizing confidence set compositions without compromising the probabilistic assurances of CP.  Strong and Weak Points Strong Points: 1. Innovative Approach: The paper innovatively integrates CP within the training loop offering a significant conceptual leap that potentially enhances both the performance and utility of CP in highstakes applications. 2. Comprehensive Experimentation: A wide array of datasets and metrics were used to evaluate ConfTr showcasing its versatility across different contexts and its capability to consistently improve upon existing methods. Weak Points: 1. Limited Insight into Model Behavior: While ConfTr demonstrates improved efficiency and the ability to shape confidence sets there is sparse analysis on how this influences the models predictive behavior. For instance the impact on false positivesnegatives or on specific potentially rare categories within datasets is not discussed. 2. Generalization Concerns: The paper predominantly focuses on the benefits of incorporating ConfTr but lacks a rigorous examination of potential drawbacks such as the scenarios under which ConfTr might not provide the anticipated improvements or might even degrade model performance. 3. Computational Efficiency Not Addressed: There is no discussion on the computational implications of integrating CP into the training process. Given that CP operates on minibatches during training there could be significant increases in computational overhead which could limit practical adoption.  Question and Feedback to the Author 1. Impact on Model Predictive Behavior: How does the use of ConfTr influence the models overall predictive behavior particularly regarding false positivesnegatives and classifications of rare events or classes Insight into these aspects could help in understanding the tradeoffs involved in using ConfTr. 2. Scalability and Efficiency: Could the authors comment on the computational efficiency of ConfTr compared to traditional posthoc CP application Specifically how does the added complexity of integrating CP into the training loop affect training time and resource requirements 3. Generalization to Other Models: While the paper shows the efficacy of ConfTr with deep learning classifiers its applicability to other models such as treebased methods or unsupervised learning algorithms remains unexplored. Could ConfTr be adapted for such models and if so what modifications would be necessary  Review Summary The paper presents a novel and compelling methodology\u2014Conformal Training (ConfTr)\u2014that seamlessly integrates conformal prediction within the training process of classifiers achieving more efficient and tailored confidence sets without compromising probabilistic guarantees. This represents a substantial step forward in the operationalization of CP particularly relevant for critical domains like medical diagnostics. However the paper could be strengthened by addressing potential impacts on predictive behavior discussing computational implications and considering generalizability beyond the models and scenarios evaluated. Despite these areas for further exploration the methodological advancements and thorough experimental validation provided make a compelling case for the value of ConfTr in enhancing the reliability and applicability of conformal prediction in highstakes settings. The work opens up promising avenues for future research particularly in understanding the balance between efficiency and predictive accuracy and in expanding the approach to a broader array of model types and applications.", "fStt6fyzrK": " Peer Review of the Paper on \"MRTAdapt: ModelBased Robust Adaptive Training for Semantic Image Segmentation\"  1) Summary of the Paper The paper presents a novel training algorithm MRTAdapt to improve the robustness of deep neural network (DNN)based semantic segmentation methods against natural variations such as changes in weather or lighting conditions. The approach integrates modelbased robust training algorithms with generative adversarial networks (GANs) to minimize the effects of natural variations from both image and label sides. This is achieved through a detailed training procedure that simulates the worstcase scenarios of natural variations ensuring the models performance remains high under diverse realworld conditions. The paper validates the effectiveness of MRTAdapt through extensive experiments on realworld and synthetic datasets comparing it against several stateoftheart models and demonstrating superior performance under various natural variation conditions.  2) Strengths and Weaknesses Strong Points:  Innovative Approach: The papers proposal to address natural variations in the input data by focusing on both image and label sides using modelbased robust training and GANs is novel and valuable for the field.  Comprehensive Experimental Validation: The authors provide extensive experimental results on both realworld and synthetic datasets showing the effectiveness of MRTAdapt over stateoftheart methods under multiple conditions.  Practical Relevance: Addressing robustness in semantic segmentation especially for safetycritical applications like autonomous driving and medical imaging is of significant practical importance. Weak Points:  Lack of Detailed Comparison: While the paper compares MRTAdapt with other approaches it lacks a detailed analysis of why MRTAdapt performs better. A deeper dive into comparative analysis could provide clearer insights.  Limited Focus on Computational Efficiency: The paper does not address the computational efficiency of MRTAdapt which is crucial for realworld applications especially those requiring realtime processing.  Generalization Across Datasets: There is limited discussion on the algorithm\u2019s ability to generalize across more varied datasets beyond Cityscapes and Synthia especially those with more severe natural variations. Questions and Feedback to the Author:  Could you provide more insights into why MRTAdapt outperforms the baseline methods in depth Understanding the key factors contributing to its superior performance could be enlightening.  Has computational efficiency been considered or measured For realworld applications the algorithms practicality heavily relies on its efficiency.  How well does MRTAdapt generalize to other datasets or extreme natural variations not covered in the experiments Some discussion or preliminary results in this direction could enhance the paper\u2019s value.  Including ablation studies to understand the contribution of each component of MRTAdapt (modelbased robust training versus GANs) could be beneficial. Could you provide such analysis  3) Review Summary The paper introduces MRTAdapt a promising approach towards improving the robustness of DNNbased semantic segmentation against natural variations leveraging both modelbased robust training and GANs. The extensive experiments conducted across multiple datasets establish MRTAdapt\u2019s superiority over current stateoftheart methods making a compelling case for its application in safetycritical systems. However the paper could be significantly strengthened by addressing its computational efficiency providing a deeper comparative analysis with baseline methods and offering insights into its generalization capabilities across a broader range of datasets and conditions. Additionally an exploration of the individual contributions of MRTAdapts components could enrich the understanding about its inner workings and potential areas for further improvement. Given the papers innovative approach practical significance and the detailed experimental validation provided I recommend acceptance provided that the authors consider the feedback and questions raised to refine the paper further. Addressing these points will not only bolster the papers contributions but also increase its utility for researchers and practitioners aiming to deploy robust semantic segmentation models in realworld applications.", "xLfAgCroImw": " Review of \"EnergyBased Learning for Cooperative Games with Applications to Valuation Problems in Machine Learning\"  1) Paper Summary This paper introduces a novel approach to solve valuation problems in machine learning through an energybased model (EBM) framework for cooperative games. The authors argue that gametheoretic valuation problems  crucial in applications like feature interpretation and model valuation  can benefit from probabilistic treatments. Notably they present a method to construct probability distributions over game coalitions that maximizes entropy effectively assuming minimal prior information about unknown distributions. By employing meanfield variational inference within this EBM framework the paper intriguingly shows that classical gametheoretic valuation criteria like Shapley and Banzhaf values can be derived as special cases of their proposed method. Furthermore they introduce the \"Variational Index\" a criterion evaluated through an iterative fixedpoint optimization process aimed at improving upon traditional metrics by reducing decoupling errors. Theoretical discussions are underpinned by empirical testing on both synthetic and realworld datasets demonstrating the Variational Indexs superior performance in decoupling error reduction and valuation accuracy.  2) Strong and Weak Points Questions and Feedback Strengths:  Theoretical Novelty: The paper presents a unique perspective by linking energybased models with gametheoretic valuation problems providing a fresh probabilistic viewpoint on valuation criteria.  Connection with Classical Valuation Criteria: Demonstrating that Shapley and Banzhaf values emerge naturally from their framework offers compelling support for these traditional methods while proposing an alternative interpretation and generalization.  Empirical Validation: By conducting experiments on both synthetic scenarios and realworld datasets the paper solidly validates the theoretical advantages of the Variational Index over conventional criteria. Weaknesses:  Complexity and Accessibility: The mathematical density and lack of intuitive explanation may make the paper less accessible to practitioners or researchers from adjacent fields.  Computational Efficiency Concerns: While the paper briefly touches upon the computational aspects it does not provide a comprehensive analysis of the scalability or computational efficiency compared to existing methods.  Limited Discussion on Practical Implications: There is a gap in discussing the direct practical applicability of the Variational Index especially in realworld scenarios beyond the datasets tested. Questions and Feedback:  How does the computational cost of calculating the Variational Index compare to traditional methods especially for large games in practical settings  The paper would benefit from a clearer exposition of how practitioners can implement the proposed methods possibly including more detailed algorithmic descriptions or pseudocode.  Can the authors comment on the sensitivity of the Variational Index to the choice of the temperature parameter \\(T\\) and provide guidance on its selection  Including case studies or examples of practical applications outside the tested datasets could enhance the papers appeal and demonstrate the Variational Indexs versatility.  3) Review Summary This paper is a valuable contribution to the intersection of machine learning and cooperative game theory. It introduces an innovative energybased framework for valuation problems successfully linking it to wellestablished gametheoretic concepts and empirically demonstrating its potential advantages. Despite its strengths the paper could be improved by addressing concerns related to computational efficiency enhancing accessibility for a broader audience and detailing practical implementation aspects. Overall the innovative theoretical contributions and solid empirical validation support a positive recommendation contingent upon addressing the highlighted concerns to increase its practical relevance and accessibility.", "lyLVzukXi08": "Summary of the Paper This paper introduces Neural Variational Dropout Processes (NVDPs) a Bayesian metalearning method for improving multitask fewshot learning. Unlike previous methods that either adapt neural network (NN) weights or model with latent representations NVDPs extend the idea of variational dropout to configure taskspecific dropout rates based on a few observed contexts. This approach employs a novel lowrank product of Bernoulli experts metamodel for efficient mapping of dropout rates and utilizes a special prior conditioned on whole task data optimizing the dropout posterior in the amortized variational inference framework. The authors claim that NVDPs can solve underfitting and posterior collapsing issues prevalent in current methods and demonstrate superior performance over existing metalearning approaches across various fewshot learning tasks including 1D stochastic regression image inpainting and classification. Strengths: 1. Innovative Approach: NVDPs cleverly extend variational dropout a method traditionally used for avoiding overfitting in NNs to conditionally model taskspecific dropout rates for improving fewshot learning. 2. Efficiency in Parameter Mapping: The introduction of a lowrank product of Bernoulli experts metamodel for the dropout posterior is notable for its memory efficiency an important aspect when dealing with numerous tasks and limited data. 3. Robustness to Functional Ambiguities: The novel prior formulation allows NVDPs to robustly approximate taskspecific dropout rates addressing the challenge of functional ambiguities and uncertainties in fewshot learning. 4. Extensive Evaluation: The paper features comprehensive experiments across various tasks and datasets with NVDPs outperforming several established benchmarks showcasing the models versatility and effectiveness. Weaknesses: 1. Complexity of Implementation: The novel contributions while effective add layers of complexity to the implementation of NVDPs potentially limiting their accessibility and adaptability. 2. Lack of Theoretical Analysis: The paper could benefit from a more rigorous theoretical analysis of the proposed method particularly concerning the convergence properties and the specific conditions under which NVDPs outperform other methods. 3. Limited Discussion on Limitations: While the paper mentions NVDPs superior performance there is scant discussion on potential limitations or scenarios where NVDPs might not be the optimal choice. 4. Dependence on TaskSpecific Data: The efficiency and efficacy of NVDPs heavily rely on the taskspecific dropout rates learned from context data. This might reduce the methods applicability in extremely datasparse scenarios. Questions and Feedback to the Author: 1. Implementation Complexity: Could the authors provide insights or potential simplifications to make NVDPs more accessible for wider applications 2. Theoretical Underpinnings: Can the authors expand on the theoretical analysis of NVDPs particularly addressing their convergence and efficiency compared to other metalearning approaches 3. Comparative Study with Deep Learning Models: How would NVDPs perform in comparison with deep learning models specifically designed for fewshot learning beyond the traditional metalearning frameworks 4. Impact of Data Sparsity: How sensitive are NVDPs to the amount and variability of context data provided for each task Is there a threshold below which NVDPs performance significantly deteriorates Review Summary: The paper presents a significant contribution to the field of fewshot learning with the introduction of Neural Variational Dropout Processes (NVDPs). By efficiently modeling taskspecific dropout rates and incorporating a novel prior for robust training NVDPs address critical challenges in fewshot learning such as underfitting posterior collapsing and functional ambiguities. The thorough evaluation across multiple tasks showcases NVDPs superior performance and highlights their potential to advance the stateoftheart in metalearning. However the complexity of the proposed method and the lack of comprehensive theoretical analysis are areas that could be improved to further strengthen the paper. Additionally understanding the limitations and potential scenarios where NVDPs might not perform optimally would provide a more balanced view of the methods applicability. Further exploration into these aspects would not only enhance the current work but also inspire future research in this promising direction.", "w1UbdvWH_R3": " Review of \"On the Dynamics of Neural Collapse Under Mean Squared Error Loss\"  1) Summary of the Paper This paper provides a comprehensive theoretical and empirical investigation into the phenomenon known as Neural Collapse (NC) which emerges when training deep neural networks with Mean Squared Error (MSE) loss. Neural Collapse refers to a series of simplifications in the geometry of the lastlayer features and classifiers during the terminal phase of training deep classification networks. Specifically it involves withinclass variability collapse convergence to a Simplex Equiangular Tight Frame (ETF) for both features and classifiers selfduality between classifiers and classmeans and simplification of the decision rule towards a nearest class mean. Previous works have outlined NC primarily in the context of CrossEntropy (CE) loss. The authors of this paper expand upon this by empirically demonstrating that NC also occurs under MSE loss leveraging three canonical networks and five benchmark datasets. They then delve deeper by decomposing the MSE loss into components directly interpretable through the lens of NC and provide a theoretical framework to analyze the dynamics of NC leading to closedform predictions of NC behavior under MSE loss.  2) Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Analysis: The paper excels in providing a detailed theoretical analysis complemented by extensive empirical evidence supporting the emergence of NC under MSE loss. The decomposition of the MSE loss and the introduction of the central path are particularly novel contributions that deepen the understanding of NC.  Reproducibility: By providing PyTorch code in a Google Colab notebook the authors ensure that their experiments are reproducible which enhances the credibility and utility of their work.  Mathematical Rigor: The papers mathematical formalism in decomposing the loss analyzing the renormalized gradient flow and deriving the exact dynamics is rigorous and adds a significant theoretical contribution to the literature on deep learning optimization. Weak Points:  Limited Exploration of Practical Implications: While the paper thoroughly establishes the occurrence and dynamics of NC under MSE loss it could delve deeper into the practical implications of these findings particularly how they might influence the design and training of neural networks for improved performance or efficiency.  Scalability and Generalization: The paper focuses on canonical networks and benchmark datasets but does not explicitly address how these findings scale to larger more complex datasets or network architectures beyond the three examined. Further exploration in this area could enhance the generalizability of the results.  Comparison with CE Loss: The paper could benefit from a more detailed comparison between the dynamics and practical outcomes of NC under CE versus MSE loss helping to contextualize the significance of the findings within the broader landscape of loss function choices in deep learning. Questions and Feedback:  Could the authors discuss any potential strategies for leveraging the understanding of NC under MSE loss to improve network training protocols or architectures  How might the central path concept influence the development of new regularization techniques or loss functions that explicitly promote or harness the properties of NC for better generalization or efficiency  Are there specific network architectures or problem domains where the authors anticipate the effects of NC under MSE loss to be particularly pronounced or beneficial  3) Review Summary The paper makes a significant contribution to the understanding of Neural Collapse in the context of MSE loss expanding upon previous work that had largely focused on CE loss. Through rigorous theoretical analysis and comprehensive empirical validation the authors convincingly demonstrate the emergence and dynamics of NC under MSE loss across a range of networks and datasets. The novel decomposition of MSE loss into components interpretable through NC and the exploration of closedform dynamics under this framework are particularly noteworthy contributions. However the paper could be strengthened by a more extensive exploration of the practical implications of these findings for network design and training as well as a clearer delineation of the differences and relative advantages or disadvantages of NC dynamics under CE versus MSE loss. Further research could also explore the scalability of these findings to more complex architectures and datasets and investigate the potential for new optimization strategies informed by an understanding of NC. Overall this paper represents a significant theoretical advancement in our understanding of deep learning optimization with promising avenues for future research and practical applications in network training and architecture design.", "uydP1ykieNv": " Review of \"EnsembleinOne: Adversarial Robustness Improvement via Random Gated Networks\"  1) Summary of the Paper The paper introduces a novel method named EnsembleinOne (EIO) aiming to enhance the robustness of Convolutional Neural Networks (CNNs) against adversarial attacks while maintaining high scalability and efficiency. The strategy involves constructing a Random Gated Network (RGN) by substituting parameterized layers in a CNN with multipath Random Gated Blocks (RGBs). This setup allows the instantiation of numerous submodels by controlling the gates across these blocks effectively creating an ensemble within a single network structure. The method is designed to diversify vulnerabilities across various paths within the RGN significantly improving robustness and computational overhead compared to existing ensemble and adversarial training methods. Experimental results on benchmark datasets using ResNet20 and VGG16 networks demonstrate the superior performance of EIO in terms of enhancing adversarial robustness without compromising clean accuracy significantly.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The idea of constructing an RGN by incorporating RGBs to enable the instantiation of a large number of submodels within a single network framework is novel and promising for enhancing adversarial robustness.  Scalability and Efficiency: The method resolves the scalability issue commonly associated with ensemble models by ensuring that the computational and memory overhead does not substantially increase with the scaling up of the ensemble size.  Comprehensive Evaluation: The authors carry out thorough experiments to benchmark the proposed method against existing ensemble methods and adversarial training approaches demonstrating superior performance in adversarial robustness while maintaining comparable clean accuracy. Weak Points:  Generalization Beyond CNNs: The paper primarily focuses on the application of EIO to CNNs. It would be interesting to see how this approach generalizes to other types of neural networks such as Recurrent Neural Networks (RNNs) or Transformers.  Analysis of Failure Cases: The paper lacks an indepth analysis of scenarios where EIO might underperform or fail which would be useful for understanding the methods limitations. Questions and Feedback:  Adapting to Dynamic Threats: How adaptable is the proposed EIO method in scenarios where the nature of adversarial attacks dynamically changes over time  Impact on Inference Time: While the paper discusses computational efficiency in terms of training overhead it would be beneficial to elaborate on the impact of deploying RGNs on inference time especially in realtime applications.  Exploration of Gate Mechanisms: The use of binary gates is crucial for the EIO approach. Could exploring other gating mechanisms possibly incorporating attention or stochasticity offer further improvements in robustness  3) Review Summary The paper introduces a compelling and innovative approach EnsembleinOne (EIO) for improving the adversarial robustness of CNNs through a scalable and efficient mechanism. By leveraging a novel Random Gated Network structure the method addresses the scalability issues of traditional ensemble models and provides a significant improvement in adversarial defense without compromising too much on clean accuracy. Despite its strengths further exploration is warranted to assess the generalization of EIO to other neural network architectures and to understand its limitations and potential scenarios of underperformance. Questions regarding the adaptability of the model to dynamic adversarial environments and the exploration of alternative gating mechanisms could pave the way for extending the applicability and efficacy of the proposed method. Overall the paper presents a significant contribution to the domain of adversarial robustness in deep learning proposing a novel and promising direction for future research.", "xkjqJYqRJy": " Review of \"Exploring Bayesian Neural Network Priors for Enhanced Performance on Image Classification Tasks\"  Summary The paper undertakes an empirical study to challenge the conventional application of isotropic Gaussian priors in Bayesian Neural Networks (BNNs) and suggests alternative priors based on the empirical weight distributions observed in neural networks trained using Stochastic Gradient Descent (SGD). The authors demonstrate that fully connected neural networks (FCNNs) exhibit heavytailed weight distributions whereas convolutional neural network (CNN) and ResNet weights show significant spatial correlations. Through various experiments across image classification datasets it is shown that employing heavytailed priors for FCNNs and spatially correlated Gaussian priors for ResNets and CNNs can mitigate the cold posterior effect observed in FCNNs and paradoxically enhance it in ResNets thus potentially leading to improved model performance. The study comprises extensive empirical analysis including the exploration of weight histograms quantilequantile (QQ) plots and performance evaluations of the adapted priors against standard benchmarks in Bayesian inference tasks. The papers contributions are substantiated by a comprehensive set of experiments and a publicly available library to encourage further research in optimizing BNN priors.  Strong and Weak Points Strong Points: 1. Novel Empirical Analysis: The paper offers a novel empirical investigation into the weight distributions of neural networks trained with SGD providing a compelling foundation for rethinking the priors used in Bayesian inference for neural networks. 2. Comprehensive Experimental Evaluation: The experiments are diverse covering different architectures (FCNNs CNNs ResNets) and datasets thereby broadening the paper\u2019s implications. The detailed analysis of performance metrics (error rates log likelihood calibration outofdistribution detection) across different priors provides a valuable resource for the community. 3. Contribution to the Bayesian Deep Learning Field: By challenging and providing alternatives to the widelyused isotropic Gaussian priors the paper pushes the boundary of Bayesian deep learning enriched by the introduction of a GitHub library for the research community. Weak Points: 1. Limited Discussion on Practical Implementations: While the paper offers insights into improving Bayesian inference through alternative priors it lacks a detailed discussion on the practical implications and computational costs associated with implementing these alternative priors especially in realworld largescale scenarios. 2. Assumption Validity: The paper assumes that better performance in image classification tasks directly correlates with the adequacy of the prior. However the broader applicability of this assumption across different tasks and data modalities remains unchecked. 3. Lack of Theoretical Justification: The empirical findings although significant would benefit from a deeper theoretical analysis explaining why certain weight distributions lead to better performance with specific priors. Question and Feedback to the Author: Can the authors provide further insights into the computational implications of adopting the proposed priors in practice particularly concerning training time and resource requirements Additionally extending the empirical analysis to include theoretical perspectives on why these specific weight distributions exhibit improved performance might enrich the papers narrative.  Review Summary The paper presents a valuable empirical exploration into the optimality of isotropic Gaussian priors for Bayesian neural networks offering alternative priors based on the empirical weight distributions of networks trained via SGD. The findings that heavytailed and spatially correlated Gaussian priors can enhance performance on various image classification datasets shed significant light on the potential misalignment between the conventional isotropic Gaussian priors and the actual distributions beneficial for Bayesian inference. Nevertheless the paper would greatly benefit from a deeper discussion on the practical implementation challenges and the theoretical underpinnings of the observed empirical phenomena. Overall the study marks a substantial contribution to the Bayesian deep learning domain encouraging further investigations into the role and optimization of priors in Bayesian neural networks.", "qyzTEWWM0Pp": " Review of \"Multiresolution Equivariant Graph Variational Autoencoders (MGVAE)\"  1. Summary This paper introduces the Multiresolution Graph Network (MGN) and its generative counterpart the Multiresolution Graph Variational Autoencoder (MGVAE) as novel architectures for learning and generating graphs in a hierarchical manner which maintains permutation equivariance throughout the process. By leveraging higherorder message passing these models partition and coarsen input graphs to create a hierarchy of latent spaces from which graphs can be variationally decoded at multiple resolution levels. The approach is tested on a range of applications including both molecular and general graph generation showing competitive performance against existing methods.  2. Strengths and Weaknesses Strengths:  Innovative Approach: The introduction of a hierarchical and equivariant method for graph generation is novel and addresses the limitations of previous models that either lack hierarchical representations or equivariance thereby improving on the capacity to model complex graph structures.  Broad Applications: Demonstrating the models effectiveness across a wide range of graph generation tasks (e.g. molecular generation link prediction) showcases its versatility and potential impact on various domains.  Detailed Experimental Evaluation: The paper provides a comprehensive analysis of model performance on several benchmarks comparing it against established methods which helps in understanding its competitive advantages. Weaknesses:  Lack of Theoretical Analysis: The paper primarily focuses on empirical results with limited theoretical discussion on why and how the hierarchical equivariant approach leads to better or competitive performance compared to other methods.  Complexity and Scalability: Given the multiresolution and higher order message passing nature of MGVAE there might be concerns regarding the computational complexity and how scalable this approach is to very large graphs.  Limited Insight into Failure Cases: While the paper highlights competitive results there is sparse discussion on conditions under which MGVAE might underperform or on the limitations encountered during its application on different datasets. Questions and Feedback to the Author: 1. Could the authors provide a more detailed theoretical background or intuition behind the benefits of incorporating multiresolution and equivariant properties especially in how they contribute to the models performance 2. It would be beneficial if the authors discussed the computational complexity of MGVAE in more detail especially in comparison to other graph generative models to understand its practical scalability. 3. Insights into any observed failure cases or limitations of MGVAE would enrich the discussion and help in setting the right expectations for its applicability.  3. Review Summary The paper presents MGVAE as a pioneering approach in the domain of graph generation aiming to capture the complex multiscale nature of graphs through a hierarchical equivariant generative model. The stated strengths lie in its innovative approach broad applicability and comprehensive evaluation which collectively underscore the potential of MGVAE to advance the field of graph neural networks. However the paper would benefit from a deeper theoretical exploration into why the proposed model succeeds and discussion around its computational feasibility on larger graphs. Additionally addressing potential limitations and failure scenarios could provide a more balanced view of the models capabilities and scope. Overall the introduction of MGVAE represents a significant contribution to the ongoing development of graph neural networks promising to enhance their expressiveness and applicability across diverse tasks. Given its robust performance across multiple datasets and clear advancements over existing methods it is recommended that the paper be considered favorably provided that the authors can clarify the raised points regarding theoretical depth computational complexity and discussion on limitations.", "gJcEM8sxHK": " Review of \"Investigating the Grounding Capabilities of Large Language Models\"  1. Summary The paper presents an exploration into how large language models (LMs) specifically GPT2 and GPT3 can be taught to map word forms learned from text corpora onto grounded conceptual worlds such as spatial directions and colors with minimal examples. This investigation stems from the critique that textonly LMs may lack grounding which refers to the models ability to connect learned language representations to actual referents in the physical world. The authors tested models abilities to generalize learned concepts to unseen instances and related unseen concepts through structured prompts within \"textual grid worlds\" and color representations using RGB codes. The study found that while smaller models (like smaller variants of GPT2) struggled with these tasks the largest model GPT3 demonstrated significant capability to learn and generalize grounded concepts with minimal examples. This suggests that large LMs might possess a conceptual structure that can facilitate grounding in the nonlinguistic world offering a new perspective on how to approach the development of grounded language models.  2. Strengths and Weaknesses Strengths:  Innovative Approach: The paper tackles the challenge of grounding in LMs from a novel angle suggesting that incontext learning with few examples can serve as an alternative to building models that learn grounded representations from scratch.  Comprehensive Analysis: Extensive experiments across different model sizes and conceptual domains provide a thorough understanding of how model capability scales with size and how different types of concepts might be more or less challenging to ground.  Implications for Future Research: The findings open up exciting avenues for further exploration particularly in adapting textbased models to incorporate nontextual information and grounding more complex or abstract concepts not directly expressible through text. Weaknesses:  Limited Scope of Grounded Concepts: While acknowledging it as a limitation the focus on concepts that can be easily represented in text (spatial directions and colors) might not fully address the challenges of grounding more complex multidimensional concepts.  Dependence on Large Model Sizes: The clear performance gap between smaller and larger models raises questions about the scalability of this approach for practical applications considering the resourceintensive nature of training and deploying models like GPT3.  Question and Feedback to the Author:  Could you elaborate on potential methods to extend the grounding capabilities of smaller models given the substantial resource requirements for larger models  How might the approach presented here adapt to grounding abstract concepts that cannot be straightforwardly represented in text (e.g. emotions abstract ideas)  Feedback: Incorporating a broader range of conceptual domains and further investigating the mechanistic reasons behind the success of larger models in grounding could strengthen the paper\u2019s conclusions and applicability.  3. Review Summary This paper provides valuable insights into the grounding capabilities of large language models challenging the notion that effective grounding necessitates models to learn from multimodal data. By demonstrating that LMs especially GPT3 can map learned word forms to conceptual representations in a grounded world with minimal examples the authors present an alternative pathway toward developing grounded language models. However the limits posed by focusing on simpler grounded concepts and reliance on large model architectures suggest the need for further research to explore the sustainability and expansibility of these findings to more complex domains and practical applications. Overall the paper contributes significantly to the discussion on how to achieve grounding in language models and opens up numerous avenues for future work to build upon. Recommendation: Accept with considerations for addressing the limitations and questions raised.", "wu5yYUutDGW": " Review of \"Boundaryaware Selfsupervised Learning for Video Scene Segmentation\"  1. Summary The paper presents a novel selfsupervised learning framework titled Boundaryaware SelfSupervised Learning (BaSSL) focused on video scene segmentation. This task involves temporally localizing scene boundaries within a video an important step towards highlevel video understanding. The primary challenge addressed is the dependency of video scene segmentation on contextual relationship modeling between shots which is not effectively captured by existing selfsupervised learning methods aimed at learning indomain representations for other tasks like action classification. To overcome this the authors propose BaSSL which leverages pseudoboundaries identified through dynamic time warping (DTW) to define boundaryaware pretext tasks. These tasks include ShotScene Matching (SSM) Contextual Group Matching (CGM) and Pseudoboundary Prediction (PP) designed to maximize intrascene similarity interscene discrimination and identify transitional moments respectively. An additional task Masked Shot Modeling (MSM) inspired by masked frame modeling aims to further aid in learning temporal relationships between shots. The combined pretraining through these tasks showed significant performance improvements on the MovieNetSSeg benchmark for video scene segmentation achieving a new stateoftheart.  2. Strong Points  Novel Approach: The paper introduces an innovative use of selfsupervised learning for the video scene segmentation task leveraging pseudoboundaries for pretraining. This approach is both novel and relevant addressing the gap in existing literature regarding the application of selfsupervised learning to video scene segmentation.  Comprehensive Methodology: The proposed boundaryaware pretext tasks and the overall BaSSL framework are wellthoughtout and rigorously defined. The use of DTW for pseudoboundary discovery and the design of the pretext tasks are particularly commendable for their creativity and potential applicability.  Solid Experimental Validation: The experiments are thorough comparing BaSSL against a variety of stateoftheart methods. The use of a largescale dataset (MovieNetSSeg) and the achievement of new stateoftheart performance lend credibility and significance to the findings.  3. Weak Points  Limited Discussion on Failures: The paper would benefit from a more detailed discussion on the limitations and potential failure cases of the proposed method. Understanding where and why it might not work is crucial for future improvements and applications.  Computational Resource Requirement: Theres a brief mention of the computational resource requirement in the Ethics Statement but a detailed analysis of the computational cost including training time and resource usage is missing. Given the selfsupervised nature and the dynamic programming aspects the computational requirements could be significant.  Generalization across Domains: While the paper showcases impressive results on the MovieNetSSeg dataset its unclear how well the method generalizes to other domains or less structured videos. Insights into its applicability across various domains would be valuable.  Questions and Feedback to the Authors 1. Robustness and Generalization: How does BaSSL perform in more challenging scenarios or datasets with less structure than MovieNetSSeg Is there evidence of the methods robustness across different domains or in the presence of significant shottoshot variation 2. Computational Efficiency: Can the authors elaborate on the computational efficiency of BaSSL particularly during pretraining Are there strategies in place to manage the computational cost or suggestions for future research to enhance efficiency 3. Adaptation and Extension: How adaptable is the BaSSL framework to other videorelated tasks beyond scene segmentation such as event detection or summarization Could the boundaryaware pretext tasks be modified for these applications  Review Summary BaSSL introduces an innovative boundaryaware selfsupervised learning framework for the task of video scene segmentation addressing a significant gap in the existing literature. The methods design grounded in the use of pseudoboundaries and complemented by wellthoughtout pretext tasks demonstrates significant potential as evidenced by the results on the MovieNetSSeg benchmark. Despite its strengths the paper could be improved by providing a deeper discussion on its limitations computational cost and generalization capabilities. Overall the presented work is solid with clear contributions to the field of video understanding and selfsupervised learning. The positive aspects outweigh the identified weak points marking this paper as a valuable addition to the current body of research.", "mk8AzPcd3x": " Peer Review for \"A Novel Graph Shortest Distance Embedding Method Using Betweenness CentralityBased Distance Resampling (BCDR)\"  1) Summary of the Paper The paper introduces Betweenness Centralitybased Distance Resampling (BCDR) an innovative method for embedding graphs that significantly enhances the accuracy and efficiency of Shortest Distance Queries (SDQs). The necessity for such advancements is underscored by the growing importance of SDQs in various applications such as social network analysis and biomedical structure prediction where traditional approaches are hampered by their computational complexity in largescale graphs. The key innovation lies in employing betweenness centrality to guide random walks enabling coverage of a broader range of node distances and employing a novel distance resampling strategy before optimization which effectively preserves distance relations in the embedding space. The paper thoroughly compares BCDR against existing methods demonstrating its superior performance across diverse realworld datasets.  2) Strong and Weak Points Questions and Feedback Strong Points: a. Innovative Approach: The integration of betweenness centrality with distance resampling is a novel contribution that addresses the limitations of existing embeddingbased methods for SDQs such as their inability to adequately capture longrange node dependencies and preserve shortest distance relations effectively. b. Theoretical Foundation: The methodological development of BCDR is wellsupported by theoretical arguments and mathematical proof providing a solid foundation for the proposed approach. c. Empirical Validation: Comprehensive experiments using a variety of realworld graph datasets demonstrate the efficacy of BCDR with the method outperforming existing approaches in terms of accuracy while maintaining competitive computational efficiency. Weak Points: a. Complexity and Scalability Concerns: Despite proving more efficient than other methods the paper could explore in greater depth the scalability of BCDR to extremely large datasets and its performance in distributed computing environments. b. Lack of Qualitative Analysis: The paper predominantly focuses on quantitative evaluation. A qualitative analysis demonstrating specific instances where BCDR succeeds (or fails) compared to existing methods could provide deeper insights. c. Parameter Sensitivity: The paper mentions several hyperparameters (e.g. \u03b1 and \u03c4) but lacks a detailed discussion on their sensitivity and how they affect the performance of the embedding which is crucial for practical application and reproducibility. Questions and Feedback: 1. Scalability: How does BCDR scale with graph size beyond the datasets tested particularly in graphs with billions of nodes or edges Are there potential optimizations for distributed computing environments 2. Hyperparameter Selection: Can you elaborate on the process for selecting and tuning the hyperparameters of the BCDR method particularly the weight decay coefficient (\u03b1) and the use of smoothed normalization (\u03c4) 3. Qualitative Insights: Could the authors provide case studies or qualitative examples demonstrating the practical implications of the enhanced SDQ accuracy on specific applications such as social network analysis or biomedical predictions 3) Review Summary and Recommendation The paper presents a significant advancement in graph embedding methods with the introduction of BCDR addressing critical limitations in the field of graph analysis particularly for SDQs. The integration of betweenness centrality with a novel distance resampling strategy represents an innovative approach that substantially improves both the accuracy and efficiency of shortest distance predictions in complex graphs. The empirical evaluation is robust offering clear evidence of BCDRs superior performance compared to the stateoftheart. However for a stronger case the paper would benefit from a more thorough analysis of scalability to very large datasets an indepth discussion on hyperparameters tuning and more qualitative insights into the practical impact of the proposed method. Addressing these points could significantly enhance the papers contribution and applicability. Based on the strengths and with consideration of the highlighted areas for improvement I recommend the paper for publication contingent upon minor revisions that address the described concerns. The contribution of BCDR to the field is clear and valuable promising to advance graphbased analysis and applications significantly.", "fvLLcIYmXb": " Peer Review of \u201cAn Axial Shifted MLP Architecture for Vision\u201d  1. Summary This paper introduces an Axial Shifted MLP (ASMLP) architecture aimed at enhancing the local feature extraction capabilities of MLPbased architectures for visual tasks. The ASMLP modifies the traditional MLPMixer approach by incorporating an axial shift operation allowing for the modeling of local dependencies similar to those in CNN architectures without the need for convolution operations. This is achieved by axially shifting channels of the feature map in both horizontal and vertical dimensions which facilitates local information flow between spatially adjacent features. The architecture demonstrates superior performance compared to existing MLPbased models and competitive results against transformerbased models on the ImageNet1K dataset achieving a top1 accuracy of 83.3 with 88M parameters. Furthermore ASMLP marks the first application of an MLPbased architecture to downstream tasks such as object detection and semantic segmentation achieving competitive results on the COCO and ADE20K datasets respectively.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The axial shift mechanism is an inventive solution for incorporating local dependencies within MLPbased architectures without relying on convolutional or selfattention mechanisms underscoring the papers contribution to novel architectural developments.  Strong Empirical Results: The model achieves impressive performance on benchmark datasets notably outperforming all MLPbased models and providing competitive results against transformerbased models signifying its practical effectiveness.  Applicability to Downstream Tasks: Demonstrating the model\u2019s versatility by extending its use to object detection and semantic segmentation tasks is a significant achievement as it underscores the models adaptability and potential for broader application beyond image classification. Weak Points:  Lack of Theoretical Insight: The paper does not provide a thorough theoretical explanation or analysis on why axial shifting specifically benefits the feature extraction process or how it compares theoretically to convolution and selfattention mechanisms regarding capturing local dependencies.  Comparative Analysis: While the paper compares ASMLP with other architectures it lacks a detailed comparative analysis with existing models that specifically focus on enhancing MLPbased architectures with local feature extraction capabilities.  Scalability and Efficiency Discussions: The paper does not sufficiently discuss the scalability of the architecture to larger datasets or its efficiency in terms of computational resources compared to other stateoftheart models which is crucial for practical applications. Questions and Feedback:  Could the authors provide more theoretical insights or explanations on the efficacy of the axial shift mechanism in capturing local dependencies compared to convolution and selfattention mechanisms  It would be beneficial if the authors could include a more detailed comparative analysis with other similar attempts to enhance MLPbased models with local feature extraction capabilities discussing the advantages and possible limitations of ASMLP in these contexts.  A discussion on the models scalability to larger datasets and its computational efficiency compared to other models would be valuable for understanding its practical applicability and potential limitations.  3. Review Summary The paper proposes a novel Axial Shifted MLP architecture that introduces an innovative method for incorporating local dependencies in MLPbased models for visual tasks. The empirical results are impressive demonstrating significant performance improvements over existing MLPbased architectures and competitive results against transformerbased models on benchmark datasets including its successful application to downstream tasks such as object detection and semantic segmentation. However the paper could be strengthened by providing more theoretical insights into the mechanism of axial shifting and including a more detailed comparison with related works that attempt to introduce local dependencies in MLPbased architectures. Further a discussion on the scalability and computational efficiency of the ASMLP model would enhance the paper by addressing its practical applicability and potential limitations in broader contexts. In conclusion the proposed ASMLP architecture represents a significant advancement in the development of MLPbased models for visual tasks offering new perspectives on the design of neural network architectures that are capable of effective local feature extraction without traditional convolutional or selfattention mechanisms.", "rRg0ghtqRw2": " Review for \"Adversarially Compounding Complexity by Editing Levels (ACCEL) in Unsupervised Environment Design for Reinforcement Learning\"  1) Summary: This paper addresses the challenge of training Reinforcement Learning (RL) agents that can generalize to unseen environments by introducing a novel method named Adversarially Compounding Complexity by Editing Levels (ACCEL). The method aims to evolve a curriculum of training environments that begin simple and gradually increase in complexity. ACCEL builds on the framework of Unsupervised Environment Design (UED) and utilizes an evolutionary approach to edit existing levels rather than generating new ones from scratch or solely relying on random sampling. This strategy is designed to keep the training environments at the frontier of the agents capabilities thereby offering a more effective and theoretically robust way of scaling the complexity of training environments. The paper evaluates ACCELs performance in gridworld environments demonstrating its ability to efficiently train agents with superior transfer capabilities to complex outofdistribution environments compared to existing methods.  2) Strong and Weak Points: Strong Points:  Novelty in Approach: ACCEL introduces an innovative approach to evolve training environments by editing levels which distinguishes it from other methods that rely on generating environments from scratch or handcrafted heuristics.  Theoretical Foundation: The paper maintains the theoretical robustness guarantees seen in prior works providing a solid theoretical foundation for ACCELs approach to curriculum evolution.  Empirical Results: The experimental evaluation shows that ACCEL outperforms existing baselines in training agents capable of zeroshot transfer to more complex environments substantiating its effectiveness in a challenging scenario.  Focus on Generalization: By targeting the ability of RL agents to generalize to unseen environments the paper addresses a critical challenge in applying RL to realworld problems demonstrating a clear understanding of the fields current limitations. Weak Points:  Environment Specificity: The papers experiments are limited to gridworld environments. The applicability of ACCEL to more diverse or complex settings particularly those with different dynamics or higherdimensional state spaces remains unclear.  Editor Function Details: While the concept of evolving training levels through editing is central to ACCEL the paper provides limited details on how the editor function is implemented especially concerning the criteria for making specific edits to the levels.  Limited Discussion on Scalability: There is sparse discussion on the computational costs and scalability of ACCEL especially in comparison to other UED and evolutionary methods that might require significant resources. Questions and Feedback: 1. Can the authors provide more insight into the implementation details and decisionmaking process of the editor function in ACCEL Understanding how edits are chosen and executed could greatly benefit the readers. 2. How does ACCELs performance and computational efficiency compare to other methods when applied to more complex or highdimensional environments beyond gridworld settings 3. Further discussion on the scalability of ACCEL particularly in comparison to traditional UED methods and other evolutionary approaches would be valuable. Can the authors comment on the expected computational costs and how ACCEL might scale to larger or more complex design spaces  3) Review Summary: The paper presents ACCEL an innovative method for evolving a curriculum of training environments in RL focusing on improving agents generalization to unseen environments. The approach is theoretically grounded and empirically validated through experiments demonstrating superior performance in scenariospecific challenges. Despite its strengths some areas require further clarification and exploration particularly regarding the editor functions implementation details the methods applicability to more diverse environments and considerations on scalability and computational efficiency. Given the papers significant contributions and minor shortcomings I recommend accepting this paper contingent on addressing the provided feedback particularly the need for more detailed information on the editor function and discussions on scalability and application breadth.", "k32ZY1CmE0": " Review of the Paper: \"Exploring the Relationship Between RNN Dynamics and Loss Gradients: A Lyapunov Perspective\"  1. Summary The paper provides a thorough theoretical analysis of how the intrinsic dynamics of Recurrent Neural Networks (RNNs) influence the behavior of loss gradients during training particularly in challenging contexts involving chaotic data. The authors methodically prove that RNN models which converge to stable dynamics (e.g. fixed points or cycles) have bounded gradients. In contrast the gradients of RNNs modeling chaotic dynamics are inherently prone to divergence a phenomenon that stems from the chaotic nature of many realworld time series. Leveraging insights from Lyapunov exponents the paper introduces a novel training technique dubbed sparsely forced Backpropagation Through Time (BPTT) which mitigates the exploding gradients problem by periodically resetting training trajectories based on the systems maximal Lyapunov exponent. The technique offers a simplistic yet effective way of training RNNs on chaotic data without compromising on longterm dependencies and dynamics. Empirical evaluations across various chaotic systems and realworld datasets validate the theoretical assertions and demonstrate the superiority of the proposed training method over traditional approaches such as gradient clipping and architectural modifications aimed at tackling the exploding and vanishing gradient problem (EVGP).  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Theoretical Depth: The mathematical rigor employed in proving the relationship between an RNN\u2019s dynamical behavior and its loss gradients properties is commendable. The use of Lyapunov exponents to theoretically ground the discussion on RNN training dynamics is particularly innovative and adds to the literature.  Practical Relevance: By focusing on the ubiquitous problem of chaotic dynamics which are prevalent in many significant realworld applications the study addresses a gap in current RNN training methodologies. It brings attention to the necessity of incorporating dynamical systems theory into machine learning training practices.  Novel Training Technique: The introduction of sparsely forced BPTT as a mechanism to control gradient explosion in chaotic time series is both novel and practical. It shows potential for wide applicability in various domains where standard training methods fall short. Weak Points:  Generalization of Empirical Results: While the empirical evaluations are thorough additional experimentation across a more diverse set of chaotic systems and datasets especially with varying degrees of chaos could strengthen the claims.  Comparison with Other Methods: The paper focuses predominantly on gradient clipping and simple architectural adjustments as baselines. Incorporating contemporary alternative approaches such as adaptive learning rate methods or other gradient normalization techniques could provide a more comprehensive analysis of the proposed methods effectiveness.  Hyperparameter Dependence: The effectiveness of the proposed training technique seems to hinge significantly on the correct estimation of the Lyapunov exponent and the optimal setting of the learning interval \u03c4. More insights into the robustness of training performance to deviations in these parameters would be beneficial. Questions and Feedback:  How does the proposed training method perform in the context of very noisy time series data where the true underlying dynamics might be harder to capture  Given the reliance on Lyapunov exponents could you elaborate on the practical challenges of estimating these values accurately in realworld datasets and any potential impacts on the training process  Have alternative dynamical system characteristics been considered for guiding the training process aside from the Lyapunov spectrum  3. Review Summary The paper makes a significant contribution to the field of machine learning by bridging the gap between dynamical systems theory and RNN training practices especially in the context of chaotic time series. Its major strengths lie in its theoretical rigor and the practical utility of the proposed training technique which marks a departure from conventional approaches to handling the EVGP. However to solidify its contributions further exploration regarding the generalization of empirical results comparative analysis with more diverse methods and the resilience of the proposed technique under varying conditions is advisable. Overall the paper represents a valuable step forward in understanding and optimizing RNN training dynamics warranting publication with minor revisions addressing the above points.", "nWlk4jwupZ": " Peer Review of \"ScheduleNet: A Reinforcement Learningbased Decentralized Scheduler for Multiagent Coordination\"  1. Summary The paper introduces ScheduleNet a novel Reinforcement Learning (RL)based decentralized scheduling framework designed for multiagent systems aimed at minimizing the total completion time of tasks (makespan). By treating multiagent scheduling problems (mSPs) as eventbased Markov Decision Processes (MDPs) with episodic rewards ScheduleNet derives a decentralized decisionmaking policy using reinforcement learning. It integrates a typeaware graph attention mechanism (TGA) for extracting relational features among agents and tasks thereby facilitating optimal task assignment. The effectiveness of ScheduleNet is demonstrated through its application to multiple traveling salesmen problem (mTSP) and jobshop scheduling problem (JSP) where it outperforms existing heuristic and deep RL approaches especially on larger and practical problems. Additionally ScheduleNets ability to solve online vehicle routing problems is evidenced showcasing its realworld applicability and flexibility.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation in Decentralized Scheduling: The approach of leveraging a decentralized RL policy for scheduling in multiagent systems addresses the scalability and efficiency issues inherent in centralized methods. The integration of TGA for feature extraction further enhances its novelty.  Broad Applicability and Transferability: The demonstrated effectiveness of ScheduleNet across different types of scheduling problems (mTSP JSP) and its capability to adapt to online routing scenarios underline its broad applicability and strong transferability.  Performance Improvement: The empirical results show that ScheduleNet consistently outperforms standard heuristic solvers and existing RLbased methods providing a significant step forward in solving mSPs efficiently. Weak Points:  Sparse Rewards Challenge: The paper identifies the challenge of training decentralized policies with sparse and delayed rewards but doesnt deeply explore alternative reward shaping or curriculum learning strategies that could potentially improve learning efficiency and stability.  Limited Exploration of Failure Modes: While the paper presents comprehensive success stories of ScheduleNet there is less discussion on the scenarios where ScheduleNet might fail or underperform which could provide insights for future improvements.  Hardware and Computational Costs: There is minimal discussion regarding the computational resources and time required for training and inference which are critical factors for realworld deployment. Questions and Feedback:  Adaptability to Highly Dynamic Environments: How well does ScheduleNet adapt to environments with highly frequent and unpredictable changes Could you elaborate on potential limitations and suggest improvements  Comparison with Centralized Methods: A detailed comparison with stateoftheart centralized scheduling methods including performance and computational efficiency aspects would offer a clearer picture of ScheduleNets positioning in the field.  Scalability with Number of Agents and Tasks: It would be valuable to understand more about ScheduleNets scalability limits. How does performance change as the number of agents and tasks increases significantly  Feedback: Incorporating discussions on computational efficiency exploring advanced reward structures and detailing limitations could enhance the papers depth and applicability insights.  3. Review Summary ScheduleNet represents a significant advancement in decentralized scheduling for multiagent systems demonstrating remarkable performance improvements across multiple scheduling problems while showcasing broad applicability and transferability. Its innovative use of the typeaware graph attention mechanism for feature extraction and the reinforcement learning approach to tackle the mSPs are commendable. However the paper would benefit from a deeper exploration of alternative reward structures to address the sparse rewards challenge a comprehensive comparison with centralized methods and a critical discussion on computational costs and scalability limits. Overall ScheduleNet holds promising potential for realworld applications signifying a substantial contribution to the field of multiagent systems and decentralized scheduling. The recommendation is to accept this paper contingent upon addressing the feedback regarding reward strategy exploration scalability discussions and computational efficiency analysis.", "zq1iJkNk3uN": " Summary of the Paper This paper proposes DeCLIP a novel training paradigm aimed at enhancing the data efficiency of Contrastive LanguageImage Pretraining (CLIP). By leveraging selfsupervision within each modality multiview supervision across modalities and nearestneighbor supervision from similar pairs DeCLIP effectively learns generic visual features with significantly less data. The model based on a ResNet50 and Transformer architecture demonstrates an impressive 0.8 improvement in zeroshot top1 accuracy on ImageNet over CLIPResNet50 using 7.1\u00d7 fewer data. Extensive experiments across multiple visual datasets underscore DeCLIPs enhanced performance and efficiency in various downstream tasks. The authors also highlight the scalability of their model indicating successful application with larger models and datasets.  Strong and Weak Points Questions and Feedback Strong Points: 1. Enhancement of Data Efficiency: A significant strength is DeCLIPs ability to achieve superior performance using substantially less data compared to standard CLIP addressing one of the crucial limitations of largescale pretraining models. 2. Innovative Approach to Supervision: The introduction of selfsupervision multiview supervision and nearestneighbor supervision as mechanisms to exploit data potential marks a novel approach to improving the learning efficiency in multimodal settings. 3. Comprehensive Evaluation: The paper brings forth a thorough evaluation of the DeCLIP model demonstrating its efficacy on a variety of visual datasets supporting the versatility and transferability of the learned representations. Weak Points: 1. Complexity in Implementation: The models increased complexity in terms of multiple supervisions could pose challenges in implementation and might increase training time although the paper briefly touches on the training cost. 2. Limited Discussion on the Impact of Different Data Modalities: While the model shows compatibility with various modalities theres a lack of detailed discussion on how different types of data or data quality impact the effectiveness of the proposed supervisions. 3. Scalability and Computational Resources: Despite mentioning scalability the paper does not provide extensive insights into the computational resources required especially when scaling up the model which could be vital for replication and further development. Question and Feedback:  Could the authors elaborate on how the choice of data (quality and type) influences the efficacy of the proposed additional supervisions particularly the nearestneighbor supervision  It would be beneficial if the paper could provide more detail on the computational overhead introduced by implementing the three additional supervisions especially in terms of training time and memory requirements.  Feedback would be to explore and discuss the applicability and adaptations required for DeCLIP in lowresource settings or with datasets of varying quality and domain specificity providing broader applicability insights.  Review Summary The DeCLIP model introduces a highly innovative and efficient approach to scale down the data requirements of CLIP through enhanced supervision techniques. The results presented convincingly demonstrate the models superior performance in zeroshot recognition and transferability to downstream tasks marking a significant contribution to the field of multimodal learning. However the complexity of the model and a detailed analysis of computational resources warrant further exploration to fully comprehend the scalability and wider applicability of DeCLIP. Addressing the impact of data variability and providing deeper insights into the computational implications would greatly augment the strength of this work. On balance the compelling advantages and novel contributions of DeCLIP strongly endorse its significance and potential for driving future research in efficient multimodal pretraining paradigms.", "ms7xJWbf8Ku": " 1) Summary of the paper This paper introduces efficient methodologies to significantly speed up the pretraining of BERT (Bidirectional Encoder Representations from Transformers) models by addressing the inefficiencies associated with the processing of padding tokens in NLP datasets specifically the Wikipedia dataset used for pretraining. The authors identify that padding tokens comprise 50 of the dataset at a sequence length of 512 limiting the training speed due to unnecessary computation. By eliminating padding through the introduction of two novel packing algorithms the paper claims a 2x improvement in sequences processed per second. The proposed algorithms ShortestPackFirst HistogramPacking (SPFHP) and NonNegative Least Squares HistogramPacking (NNLSHP) operate under the assumption that sequences within a dataset can be interchanged allowing for packing based on the histogram of sequence lengths with linear complexity in dataset size. SPFHP is highlighted for its quick determination of packing order while NNLSHP though slower is noted for its depth efficiency in creating nearoptimal packs by combining a maximum of three sequences into one sample. Adjustments to the BERT model and hyperparameters are required to accommodate the packed dataset format yet these modifications are described as straightforward and minimally impactful on performance. The paper demonstrates that using these packing algorithms with the Wikipedia dataset for pretraining BERTLarge does not compromise model convergence while achieving the desired computational efficiency.  2) Strong and weak points questions and feedback to the author Strong Points:  Innovative Approach: The paper successfully addresses a prevalent issue in NLP model training. The introduction of SPFHP and NNLSHP as solutions for efficiently packing datasets represents a significant contribution to computational efficiency in pretraining large models.  Comprehensive Testing: Extensive experiments and comparisons not only validate the proposed algorithms but also provide a deep dive into their efficiency processing time and impact on model performance.  Detailed Adjustments: The adjustments to the model and hyperparameters for accommodating the packed dataset are wellexplained aiding reproducibility and application in other scenarios or with different models. Weak Points:  Complexity for Practitioners: While the paper claims the adjustments to the model and hyperparameters are straightforward the practical application may still represent a barrier for some industry practitioners or smaller research teams without extensive resources.  Limited Scope of Datasets: The focus is predominantly on the Wikipedia dataset for pretraining BERT. Its unclear how the algorithms would perform on datasets with different characteristics or in nonNLP contexts.  Potential for Overfitting: The paper does not discuss the implications of training with packed sequences on model generalizability or the risk of overfitting given the altered sequence distribution. Questions and Feedback:  Could the authors elaborate on potential challenges and solutions when applying these algorithms to datasets significantly different from Wikipedia both in size and content distribution  The study primarily focuses on BERT and NLP datasets. Do the authors foresee any limitations or adjustments needed to apply these techniques to other types of models particularly nonlanguage models  It would be beneficial to discuss the impact of these packing strategies on model generalizability and whether theres an increased risk of overfitting due to altered sequence distributions.  3) Review Summary The paper creatively addresses the inefficiency of padding in pretraining NLP models by introducing two novel packing algorithms thereby significantly enhancing computational efficiency without sacrificing accuracy or convergence rates. The methodologies are thoroughly tested and the required model adjustments are clearly outlined marking a substantial advancement in the field. However the practical complexities involved in implementing these strategies the potential overfitting risks and the papers limited focus on BERT and similar models highlight areas for improvement. It is recommended that further research explore the applicability of these algorithms across a broader range of datasets and model architectures as well as examine the longterm effects on model performance and generalizability. Given the strong theoretical foundation significant computational gains demonstrated and the detailed guidance provided for reproducing the methods the paper is a valuable contribution to the field. It is recommended that this work be published with the suggestion that future work aims to address the identified limitations and unanswered questions.", "lu_DAxnWsh": " Review of \"Exploring the Necessity of Intermediate Steps in Neural Networks for System 2 Tasks\"  Summary The paper presents an innovative approach to enhancing neural networks capability of solving \"System 2\" tasks\u2014tasks that involve conscious multistep reasoning as characterized by Daniel Kahneman. The authors argue that classical neural network training paradigms which focus on learning direct mappings from inputs to outputs (successful in \"System 1\" tasks that rely on unconscious quick problemsolving) are insufficient for System 2 tasks. They propose a training methodology that includes intermediate steps in the learning process akin to the steps a human would take when manually solving algorithmic tasks. The paper proves that a 1layer 1head Transformer can in principle compute any finite function given enough intermediate steps and empirically demonstrates that this type of transformer as well as a Frozen Pretrained Transformer can successfully learn binary addition\u2014a prototypical System 2 task\u2014only when trained using these intermediate steps. The work stands in contrast to methods that attempt to bridge the System 1 and System 2 gap via architectural modifications instead showcasing the potential of using existing architectures with modified training data.  Strong Points 1. Novel Approach to System 2 Learning: The work pioneers in extending existing neural models specifically Transformers to tackle algorithmic System 2 tasks without necessitating architectural changes underscoring the versatility of these models. 2. Compelling Empirical Evidence: The experiments particularly on binary addition clearly show how the inclusion of intermediate steps facilitates learning providing concrete data to back the authors claims. 3. General Applicability: The method of incorporating intermediate steps can be universally applied regardless of underlying architecture expanding its utility beyond specialized tasks.  Weak Points 1. Scalability and Complexity: While promising the proposed methods scalability to more complex or realworld problems remains uncertain. Future work needs to address how intermediate steps are determined and generated for tasks not as cleanly decomposable as binary addition. 2. Comparison with Alternative Methods: The paper could benefit from a more indepth comparison with alternative methods focusing on System 2 tasks including those involving architecturespecific enhancements to contextualize the proposed methods advantages and limitations more clearly.  Questions and Feedback  How does the method scale with the complexity of tasks beyond binary addition Is there a systematic way to identify and generate intermediate steps for a broad range of tasks  Have the authors considered how the incorporation of intermediate steps impacts the training efficiency or the compute resources required given that more complex problems may necessitate a significantly larger number of intermediate stages  The paper would benefit from a more detailed discussion on potential limitations or tradeoffs introduced by the reliance on intermediate steps particularly regarding the potential for overfitting or reduced model generalization.  Review Summary This paper presents an important step forward in the domain of neural network research especially concerning the longstanding challenge of applying these models to System 2 tasks effectively. By demonstrating that neural networks including transformers can be trained to solve algorithmic tasks such as binary addition through the inclusion of intermediate steps in training data the authors highlight a crucial pathway for future pursuits in neural network capabilities for multistep reasoning. This approach leverages the inherent strength of current models without necessitating architectural changes which is a major advantage. Nonetheless the scalability of this method to more complex tasks and its efficiency in terms of computation and training resources remain open questions that the paper currently does not address in depth. Additionally a more detailed comparative analysis with other methods targeting System 2 tasks would enrich the discourse and elicit a clearer understanding of the proposed methods standing. Overall this work is of high significance for advancing our understanding and capabilities of neural networks in executing complex multistep cognitive tasks. It opens up numerous avenues for future research particularly in devising systematic methods for determining and generating useful intermediate steps across a wider array of tasks.", "y1PXylgrXZ": "Peer Review for \"Certifying Robustness of Deep Equilibrium Models using Interval Bound Propagation\" 1) Summary of the Paper: This paper introduces a novel approach to certifying adversarial robustness in deep equilibrium models (DEQs) which have shown promising performance across various benchmarks but have largely remained unexplored in the context of robustness certification. The authors present the IBPMonDEQ layer inspired by interval bound propagation (IBP) to compute certifiable upper and lower bounds on the output of DEQ layers under adversarial perturbations. They achieve this by reformulating the DEQ\u2019s fixedpoint equilibrium in terms of an IBPinspired augmented fixedpoint equation. This innovative layer ensures a unique fixedpoint solution due to a specific parameterization of the layer\u2019s weights. Theoretical insights indicate that these models can be viewed as infinitely deep weighttied networks for which IBP bounds remain stable. Empirically the authors demonstrate that IBPMonDEQ layers can achieve competitive certified robustness compared to traditional explicit models while also providing a foundation for further exploration of DEQs in adversarially robust machine learning. 2) Strong and Weak Points Questions and Feedback: Strong Points: a) Novel Approach: The integration of IBP with DEQs to ensure certifiable robustness is innovative and addresses a significant gap in the literature surrounding the safety and security of DEQ models in adversarial settings. b) Theoretical Contributions: The paper offers substantial theoretical insights particularly the existence and uniqueness of the fixedpoint solution for the IBPMonDEQ layer which enrich the understanding of DEQs behavior under adversarial conditions. c) Practical Implications: By demonstrating comparable robustness of IBPMonDEQ layers to traditional explicit models the paper makes a strong case for the practical applicability of DEQs in contexts where adversarial robustness is critical. Weak Points: a) Model Complexity: The complexity associated with implementing and training IBPMonDEQ layers could be a barrier for adoption especially in applications where simplicity and efficiency are paramount. The paper could have discussed the computational implications more thoroughly. b) Empirical Validation Scope: While the empirical comparisons are compelling the evaluation primarily focuses on `\\8` certified robustness. Expanding these evaluations to include other norms or adversarial settings could provide a more rounded view of the models robustness capabilities. c) Exploration of Parameterization: The specific parameterization required for the unique fixedpoint solution might restrict the expressivity of the model. A deeper investigation into the tradeoffs introduced by this parameterization would be valuable. Questions and Feedback: 1. Could the authors elaborate on the potential computational overhead introduced by the IBPMonDEQ layer especially in comparison to traditional DEQs and explicit models 2. The paper focuses on `\\8` perturbations. How would the IBPMonDEQ layer perform under other types of adversarial attacks especially those not bounded by `\\8` norms 3. The unique fixedpoint guarantee is tied to a specific parameterization of the layer\u2019s weights. Could the authors discuss the impact of this requirement on the models flexibility and expressiveness Review Summary: The paper presents a significant advance in the field of deep equilibrium models by proposing a method to certify their robustness against adversarial attacks addressing a critical concern for their deployment in safetycritical systems. The theoretical contributions namely the guaranteed existence and uniqueness of the fixedpoint solution are substantial and pave the way for future research in this domain. The empirical evaluations while focused convincingly demonstrate the capability of IBPMonDEQ layers to achieve competitive certified robustness. However the complexity of the proposed approach and its computational demands warrant further discussion. Additionally expanding the empirical validation beyond `\\8` perturbations and exploring the expressivity restrictions imposed by the required parameterization would greatly enhance the papers contributions. Given the novelty of the approach its theoretical significance and promising empirical results I recommend acceptance. The paper opens up exciting avenues for research in the robustness of DEQs and contributes valuable insights to the broader conversation around secure and reliable machine learning models.", "ptxGmKMLH_": " Review Report  1) Summary of the Paper The paper presents a detailed examination of the efficiency and underlying mechanisms of prototype classifiers in fewshot learning scenarios contrasting them with linear classifiers trained on pretrained models without incorporating metalearning. The authors elaborate on the limitations encountered when directly applying prototypes constructed from the features extracted by pretrained models pointing out significant performance discrepancies compared to both prototypical networks and linear classifiers. To tackle these issues the paper introduces a novel generalization bound that indicates the variance of the norm of feature vectors as a pivotal component for enhancing prototype classifiers performance. Through experimental validation the paper advocates for the adoption of specific normalization methods specifically L2 normalization and embedding space transformation to minimize norm variance and equate the performance of prototype classifiers with that of existing methods without requiring the retraining of classifiers or the use of metalearning techniques.  2) Strong and Weak Points Questions and Feedback Strong Points: a. Theoretical Contribution: The derivation of a new generalization bound provides a significant theoretical advancement in understanding the impact of feature vector norm variance on the performance of prototype classifiers. This contribution not only fills a gap in existing literature but also paves the way for practical implications in the improvement of fewshot learning methods. b. Empirical Validation: The comprehensive experimental evaluation across multiple datasets (miniImageNet tieredImageNet CIFARFS FC100) and settings (1shot and 5shot) strengthens the papers claims. The comparison of various feature transformation methods against standard approaches including linear evaluation and prototypical networks provides robust evidence to support the authors hypotheses. c. Practical Implications: The finding that performance comparable to traditional methods can be achieved through specific feature transformation methods without the need for retraining classifiers or metalearning is immensely valuable. This insight offers a simpler potentially more efficient path to fewshot learning making the approach more accessible to practitioners. Weak Points: a. Lack of Clarification on Implementation Details: While the paper provides a summary of datasets and protocols used in experiments it lacks detailed clarification on several key aspects such as the exact architecture details of pretrained models and the specifics of the feature transformation implementations. This omission could hinder reproducibility. b. Discussion on Limitations and Challenges: The paper could benefit from a more thorough discussion on the limitations and potential challenges of the proposed approach. For instance understanding the conditions under which the performance gain from feature transformation diminishes or exploring the scalability of the approach to a larger set of classes would provide a more balanced view. Questions and Feedback: 1. Can the authors elaborate on how the proposed feature transformation methods particularly ESTL2norm might scale with an increase in the diversity and complexity of fewshot learning tasks 2. The paper briefly mentions that the variance normalization performed poorly in experiments. Could the authors speculate on potential adjustments or alternative approaches that might make variance normalization more effective 3. It would be insightful if the authors could provide a discussion or future work section addressing how their findings could be generalized or adapted to other forms of fewshot learning tasks beyond image classification such as text or speech recognition.  3) Review Summary The paper offers a significant theoretical and empirical contribution to the domain of fewshot learning by investigating the efficacy of prototype classifiers sans the traditional reliance on retraining or metalearning. The novel generalization bound focused on the variance of the norm of feature vectors constitutes a notable theoretical advancement while the extensive experimental validation effectively supports the proposed hypotheses. Despite certain weaknesses regarding the clarity of implementation details and the discussion of limitations the paper sets a firm groundwork for future research in simplifying and enhancing fewshot learning methodologies. The practical implications of the findings potential for scalability and the simplicity of the suggested approach substantially outweigh the outlined weaknesses. Thus its recommended that with minor revisions addressing the mentioned concerns this paper would make a valuable addition to the literature on fewshot learning.", "z8xVlqWwRrK": " Review of \"Eventbased Variational Distributions for Exploration (EVaDE) in ModelBased Reinforcement Learning\"  1. Summary This paper introduces Eventbased Variational Distributions for Exploration (EVaDE) a novel approach designed to enhance exploration in ModelBased Reinforcement Learning (MBRL) for objectbased domains. EVaDE integrates three unique convolutional layers\u2014noisy event interaction noisy event weighting and noisy event translation\u2014into deep neural network models to facilitate variational Thompson sampling. These layers employ Gaussian dropout to induce variational distributions over model parameters stimulating diversified exploration through perturbations in object interactions event importance and objectevent positions. The efficacy of EVaDE is demonstrated through empirical tests on a randomly selected suite of 12 Atari games displaying significant improvements in performance compared to existing models including CURL and baseline SimPLe. Particularly EVaDEequipped Simulated Policy Learning (SimPLe) agents achieve a mean Human Normalized Score (HNS) of 0.78 indicating robust exploration capabilities in environments with sparse interactions.  2. Strengths and Weaknesses Strengths: a. Novelty: The introduction of eventbased variational distributions is a significant contribution to the reinforcement learning community especially for exploration in MBRL with objectbased domains. b. Exploration Efficiency: EVaDE\u2019s ability to direct exploration using perturbations on object interactions and event significances is innovative addressing the challenges in exploring highdimensional state spaces efficiently. c. Empirical Performance: The experimental results demonstrating superior performance over several baselines in a challenging subset of Atari games showcase the practical effectiveness of the proposed method. Weaknesses: a. Transferability  Generality: The paper primarily focuses on objectbased domains particularly Atari games. Its unclear how EVaDE would perform in nonobjectbased environments or realworld applications beyond games. b. Computational Cost: The use of multiple noisy convolutional layers to induce variational distributions might introduce a significant computational overhead especially in larger models. The paper does not thoroughly discuss the computational implications. c. Lack of Theoretical Analysis: While empirical results are promising the paper lacks a deep theoretical foundation explaining why and how EVaDE outperforms its counterparts particularly concerning the explorationexploitation balance. Questions and Feedback: 1. Can the authors provide insights or preliminary results on how EVaDE performs in nonobjectbased domains or realworld scenarios outside of gaming 2. A deeper discussion on the computational cost associated with implementing EVaDE especially in terms of training time and resource consumption would be beneficial. 3. It would be valuable if the authors could offer a more comprehensive theoretical analysis or justification for the exploration mechanisms introduced by EVaDE\u2019s variational distributions.  3. Review Summary The paper presents a significant contribution to the field of MBRL by proposing EVaDE an innovative approach for enhancing exploration in objectbased domains through eventbased variational distributions. The empirical validations underscore its potential to outperform existing models in specific settings (Atari games) providing convincing evidence of its effectiveness. However the applicability of EVaDE in broader scenarios its computational demands and the absence of thorough theoretical backing are areas that call for further exploration. Given the strengths in novel exploration strategies and empirical achievements coupled with the identified weaknesses and areas for enhancement my recommendation would be to accept this paper with minor revisions. Addressing the questions and feedback provided could significantly strengthen the work enhancing its impact on the reinforcement learning community and extending its applicability beyond the demonstrated domains.", "lbauk6wK2-y": " Review of \"Continuous Learning of ObjectCentric Representations for Visual Learning and Understanding\"  Summary The paper introduces an innovative framework designed for the continual learning of objectcentric representations from visual data. This framework named Object Pursuit significantly contributes to the field by addressing the challenges faced by existing objectcentric representation learning methods which either depend on supervision for individualizing objects in a scene or struggle with the unsupervised disentanglement of complex realworld scenes. Through leveraging interactions for data sampling the proposed method effectively collects diverse variations of objects and their associated training signals. This is achieved by using a convolutional hypernetwork to generate discriminative weights for learned objects enabling efficient and robust representation learning while preventing catastrophic forgetting. The authors perform an extensive study to explore the frameworks key features and demonstrate its potential for enhancing label efficiency in downstream tasks.  Strong Points 1. Innovative Approach: The paper presents a novel approach to objectcentric representation learning leveraging a hypernetworkbased framework that significantly reduces the annotation burden and improves the learning of complex scenes. 2. Comprehensive Evaluation: The authors conduct thorough experiments to evaluate the proposed framework including its ability to reidentify objects learn base object representations and its impact on label efficiency for downstream tasks. This thorough evaluation provides a solid understanding of the frameworks capabilities and limitations. 3. RealWorld Applicability: The paper demonstrates the potential of the proposed framework to improve label efficiency in tasks requiring visual understanding highlighting its applicability to realworld challenges.  Weak Points 1. Comparative Analysis: While the paper provides extensive evaluation of the proposed framework it lacks a detailed comparison with established methods in objectcentric representation learning. A comparative analysis would have provided clarity on the advantages and limitations of the proposed method relative to existing approaches. 2. Complexity and Scalability: The implementation details and scalability of the proposed framework especially when dealing with very large datasets or extremely complex scenes are not thoroughly discussed. Given the innovative use of a hypernetwork it would be beneficial to understand any potential computational constraints or scalability challenges. 3. RealWorld Data Testing: Despite a section discussing an initial attempt on realworld data the paper does not provide extensive results or analysis on the performance of the proposed framework in natural environments. This would be crucial for understanding how the method translates from synthetic or controlled datasets to realworld applications.  Questions and Feedback to the Author 1. Comparative Analysis: Could the authors provide more details on how the proposed framework compares with existing objectcentric representation learning methods particularly in terms of performance on complex scenes 2. Scalability and Computational Requirements: Could the authors elaborate on the computational requirements of their framework and its scalability to larger datasets or more complex realworld scenes 3. RealWorld Data Performance: It would be insightful to have additional information or preliminary results on the performance of the framework when applied to realworld datasets. Is there any ongoing work or planned future research in this direction  Review Summary The paper presents a highly innovative framework for learning objectcentric representations addressing significant challenges in the field through an approach that allows for continuous learning from interactions. The proposed method shows promise in easing the annotation burden and improving the representation of complex scenes with substantial evaluation conducted to demonstrate its capabilities. However the paper could be strengthened by including a comparative analysis with existing methods addressing questions regarding scalability and computational requirements and providing more insights into realworld data performance. Supporting arguments for recommending this paper include its novel approach comprehensive evaluation and potential realworld applicability. Further work in the suggested areas could substantially enhance the papers contribution to the field of visual learning and understanding.", "zNR43c03lRy": " Review of \"Learning to Annotate with Gradient Matching for SemiSupervised Part Segmentation\"  Summary The paper presents a novel method for semisupervised part segmentation that leverages the power of pretrained Generative Adversarial Networks (GANs) to generate highquality images and uses an automatic annotator to label these generated images. The key innovation lies in formulating the training of the annotator as a learningtolearn problem solved with gradient matching. This approach allows for efficient annotator learning without the need for manual labeling of generated images thus overcoming a significant limitation of previous methods that rely on \"human in the loop\" for annotating synthetic images. The authors demonstrate that their method can work with various types of labeled data including real images synthetic images and even analytically rendered images and show significant performance improvements over existing semisupervised methods especially in scenarios with extremely limited labeled data.  Strong Points 1. Innovation: The paper introduces a novel gradientmatchingbased learningtolearn framework for annotator learning which significantly reduces the dependency on humanannotated data and enables the use of a broader range of labeled data types. 2. Broad Applicability: The method is tested across different datasets and scenarios (real images synthetic images outofdomain data) showcasing its versatility and broad applicability. 3. Performance: The proposed method outperforms existing semisupervised learning methods in part segmentation tasks particularly in settings with a limited amount of labeled data according to comprehensive experiments.  Weak Points 1. Complexity and Interpretability of Method: The nestedloop optimization and gradient matching are conceptually complex and the paper could benefit from a more detailed explanation or intuitive breakdown of these concepts especially for readers not deeply familiar with learningtolearn frameworks. 2. Dependence on Pretrained GANs: The performance of the method is inherently tied to the quality of the pretrained GANs. Although this is stated as a limitation further discussion on potential solutions or ways to mitigate this dependency would be beneficial. 3. Scalability and Generalization to Complex Scenes: The focus on singleclass images and avoidance of multiobject scenes are significant limitations. A discussion on potential pathways or future research directions to extend the method to more complex scenes would add value.  Questions and Feedback to the Author 1. Method Interpretability: Could the authors provide a more detailed explanation or intuitive example of how the gradient matching works in the context of annotator learning 2. Mitigating GAN Dependency: What strategies do the authors propose or foresee to lessen the dependence on the quality of pretrained GANs especially considering the rapid evolution of generative models 3. Extension to Complex Scenes: Are there preliminary ideas or ongoing work on adapting this method to handle multiobject scenes or more complex datasets  Review Summary This paper introduces a significant advancement in semisupervised learning for part segmentation tasks by proposing a method that efficiently leverages synthetic images generated by pretrained GANs. The formulation of annotator learning as a learningtolearn problem solved via gradient matching is both novel and demonstrates substantial performance improvements over existing methods. While the approach shows great promise especially in settings with extremely limited labeled data it also presents challenges related to the complexity of the methods explanation dependency on GAN quality and generalization to complex scenes. Addressing these concerns particularly through clearer methodological explanations and discussions on scalability and extending applicability could greatly enhance the papers impact. Despite these challenges the methods demonstrated effectiveness and broad applicability represent a significant contribution to the field of semisupervised learning warranting further investigation and development.", "pgKE5Q-CF2": "Review of the Paper: \"NeuronEnhanced Autoencoder Based Collaborative Filtering (NEAECF)\" 1. Summary: The paper introduces NEAECF a novel collaborative filtering approach designed to enhance recommendation systems. Unlike traditional autoencoder models that use a linear activation function in the decoder output NEAECF introduces an additional elementwise neural network that learns an adaptive activation function for the autoencoder\u2019s output layer. This setup aims to better approximate the nonlinearity in users rating patterns. The paper provides a comprehensive theoretical analysis demonstrating the improved generalization capability of NEAECF over traditional methods. Specifically it is shown that the added neural network reduces the upper bound of prediction errors and that data sparsity rather than being a hindrance actually contributes positively to prediction performance. The approach is evaluated on benchmark datasets such as Movielens 100k and 1M demonstrating superior performance when compared to existing models. 2. Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Approach: The introduction of an additional elementwise neural network to adaptively learn the activation function in the output layer of an autoencoder is novel and addresses the limitations of linear activation functions in capturing the complex useritem interaction patterns in rating data.  Theoretical Analysis: Providing theoretical insights into how NEAECF reduces prediction error upper bounds and why data sparsity might be advantageous adds valuable understanding to the field of AI and recommendation systems.  Empirical Validation: The method is rigorously tested against benchmark datasets showing significant improvements over stateoftheart methods which bolsters the papers claims. Weak Points:  Model Complexity: While NEAECF shows superior performance the additional complexity through the elementwise neural network might increase training time. The paper does not discuss computational efficiency or scalability to larger datasets.  Limited Comparisons: The paper focuses primarily on RMSE for performance evaluation. Including additional metrics such as precision recall or F1score could offer a more rounded view of performance. Questions and Feedback:  Could the authors comment on the computational complexity of NEAECF in comparison to traditional autoencoder models or other baseline models used in the evaluation  It would be valuable to see how NEAECF performs not just in terms of RMSE but also in capturing the diversity and novelty of recommendations. Has there been any analysis in this direction  The theoretical analysis is comprehensive but assumes certain conditions like the Lipschitz constants of activation functions. How sensitive is NEAECF\u2019s performance to these assumptions in practical scenarios 3. Review Summary: The paper presents NEAECF an innovative collaborative filtering approach that notably outperforms existing methods on benchmark datasets. The incorporation of an additional elementwise neural network to learn a nonlinear activation function adaptively is both novel and demonstrably effective offering a promising direction for recommendation system models. The theoretical and empirical evidence provided is compelling although the paper could be further strengthened by addressing the computational implications of the added complexity and extending the evaluation to include metrics beyond RMSE. Given the significant performance improvement and theoretical contribution the paper is of high value to the field of AI and recommendation systems. However to fully substantiate its claims addressing the computational efficiency of NEAECF and broadening the scope of performance metrics would be beneficial. Overall this work represents an important step forward in collaborative filtering research and with further development and analysis NEAECF has the potential to make a lasting impact on practical recommendation systems.", "p-BhZSz59o4": " Review of \"BEIT: Bidirectional Encoder Representations from Image Transformers\"  Summary The paper proposes BEIT a novel selfsupervised pretraining method for vision Transformers inspired by BERT named after its task of bidirectional encoder representations from Image Transformers. Applying a masked image modeling (MIM) task the authors introduce a twoview approach for each image involving image patches and visual tokens leveraging the notion of \"tokenizing\" images into discrete tokens obtained via a discrete variational autoencoder (dVAE). This method significantly diverges from previous pixellevel autoencoding approaches by emphasizing the recovery of visual tokens rather than raw pixels aiming to circumvent the limitations of focusing on shortrange dependencies and highfrequency details. Extensive experiments on downstream tasks such as image classification and semantic segmentation show that BEIT not only outperforms previous selfsupervised learning methods but also exhibits compatibility with supervised pretraining leading to further performance improvements. The paper also highlights the models ability to learn semantic regions and object boundaries without human annotations attributing it to the selfattention mechanism of BEIT.  Strengths  Innovative Approach: The paper introduces a novel selfsupervised pretraining task that addresses the challenges of applying BERTstyle pretraining to vision Transformers. The masked image modeling (MIM) approach emphasizing visual tokens over raw pixels is notable for its potential to overcome the limitations of shortrange dependencies.  Extensive Evaluation: The authors provide comprehensive experiments to validate the effectiveness of the proposed method across multiple tasks and settings including comparisons with stateoftheart methods demonstrating significant improvements in performance.  Theoretical Insights: The discussion around the perspective of variational autoencoders (VAEs) and how BEITs pretraining can be viewed through this lens is insightful providing a solid theoretical foundation for the approach.  Weaknesses  Complexity and Reproducibility: The methodology involves multiple complex components including the use of discrete VAEs for tokenization and specialized procedures for blockwise masking. This complexity might challenge reproducibility and practical applicability without comprehensive details or released code.  Potential for DomainSpecific Limitations: While the paper demonstrates impressive results in general domains (e.g. image classification semantic segmentation) it does not explore the applicability or performance in more specific or challenging domains leaving open questions about the universal applicability of BEIT.  Lack of Discussion on Failure Modes: The paper presents largely positive results but lacks a discussion on potential limitations or scenarios where BEIT might not perform as expected. Such insights could be valuable for future research and applications of the method.  Questions and Feedback  Generalization Across Domains: Could the authors provide insights or preliminary results on how well BEIT generalizes to more specific domains or tasks with unique characteristics (e.g. medical imaging)  Comparison with NonTransformer Models: While comparisons are made with other Transformerbased methods could the authors discuss how BEIT compares to stateoftheart nonTransformer selfsupervised learning methods  Future Work on Failure Modes: Are there identified scenarios or types of data where BEITs performance is known to degrade and if so what future work is planned to address these limitations  Review Summary The paper presents BEIT a compelling advancement in selfsupervised learning for vision Transformers. By introducing a novel pretraining task that efficiently leverages masked image modeling with visual tokens the authors address significant challenges in the field and demonstrate notable improvements over existing methods across various tasks. The thorough experimental validation along with insightful theoretical discussions provide strong support for the effectiveness of BEIT. However the complexity of the method and the lack of discussion on potential limitations or failure modes present areas for further exploration. Overall the paper makes a significant contribution to the literature promising to influence future research in selfsupervised learning for vision tasks.  Recommendation Given the significant contributions innovative approach and comprehensive evaluation I recommend the acceptance of this paper. Future work addressing the highlighted weak points and questions could further solidify the impact and applicability of BEIT in the broader research community.", "mZsZy481_F": " Peer Review for \"Fewshot ROBust (FROB) Model for Classification and OoD Detection\"  1. Summary The paper presents a novel framework termed Fewshot ROBust (FROB) designed to address the challenges associated with classification and OutofDistribution (OoD) detection in fewshot settings. Considering the critical nature of accurate OoD detection in safety security and defense applications the study emphasizes the need for models that can effectively differentiate between indistribution and OoD samples even when very few examples are available. FROB integrates selfsupervised learning with generative and discriminative models to generate a support boundary around the normal class distribution coupled with a mechanism for fewshot Outlier Exposure (OE). The method aims to enforce lower confidence levels on OoD samples lying close to or on this boundary including adversarially generated samples. The paper claims that FROB not only achieves stateoftheart performance in fewshot robustness and OoD detection but is also capable of generalizing to unseen anomalies and maintaining performance levels independently of the fewshot sample size.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The combination of a selfsupervised learning boundary and improved Outlier Exposure for fewshot settings addresses a significant gap in the literature on fewshot OoD detection.  Generalization: The ability of FROB to generalize to unseen data and maintain performance regardless of the fewshot sample size is particularly noteworthy and valuable for realworld applications.  Comprehensive Evaluation: The evaluation of FROB across multiple datasets and comparison with benchmarks provides a solid foundation for understanding its effectiveness and advantages over existing methods. Weak Points:  Method Complexity: The paper introduces a complex architecture involving both generative and discriminative elements as well as a novel loss function. Greater detail on the model architecture and a clearer explanation of its components would be beneficial for reproducibility and understanding.  Lack of DomainSpecific Applications: While the methodology is demonstrated across various datasets specific case studies or domainfocused applications could showcase the practical benefits and limitations of FROB in realworld settings.  Hyperparameter Sensitivity: The models performance may be sensitive to the choice of hyperparameters such as the boundary generation process and the weighting of the loss components. Discussion on this aspect and guidance on hyperparameter tuning would enhance the paper. Questions and Feedback: 1. Can the authors provide additional insights into the computational complexity of FROB especially in comparison to the benchmarks 2. How does the choice of the fewshot OE set impact the final model performance and what strategies can be employed to select or generate effective fewshot OE samples 3. The selfsupervised nature of the boundary generation process is intriguing. Could the authors elaborate on how this process is initiated and controlled particularly in the absence of labeled OoD samples 4. Feedback to the authors includes the suggestion to present case studies or realworld applications where FROB has been applied to underline its practical utility further. 5. Including a discussion section on potential improvements or extensions to the FROB methodology would be valuable perhaps highlighting avenues for incorporating additional modalities of data or adapting the model for timeseries analysis.  3. Review Summary The proposed FROB model introduces a noteworthy approach to addressing the challenges associated with fewshot OoD detection. Its novel combination of selfsupervised confidence boundary generation with fewshot outlier exposure for improved robustness is a significant contribution to the field. While the model demonstrates strong performance across various datasets and benchmarks greater clarity on its computational demands adaptability to different domains and sensitivity to hyperparameters would be beneficial for readers and prospective users. Exploring realworld applications and discussing potential refinements could further underscore FROBs value and applicability. Overall the paper makes a compelling case for its methodology although some areas could benefit from additional detail and elaboration. The strength of the empirical evidence coupled with a clear focus on a challenging problem area supports a recommendation for publication. Nonetheless addressing the highlighted feedback could significantly enhance the papers impact and its utility to the research community.", "xP3cPq2hQC": " Peer Review for \"GromovWasserstein Imitation Learning (GWIL)\"  1) Summary of the Paper The paper introduces GromovWasserstein Imitation Learning (GWIL) a novel approach in the field of crossdomain imitation learning. It addresses the challenge of leveraging expert demonstrations from one domain to train an imitation agent in a different domain especially focusing on scenarios where the expert and imitation agents have different embodiments or morphologies making direct comparison of trajectories and states difficult. The core of GWIL is the use of the GromovWasserstein distance which allows for the alignment and comparison of states in different spaces offering a solution to the challenge of crossdomain imitation without requiring proxy tasks. The paper also delves into the theoretical underpinnings of the approach presenting a formal characterization of scenarios wherein GWIL can preserve optimality. Empirical validation is provided through experiments in continuous control domains demonstrating the ability of GWIL to learn optimal behaviors from a single demonstration across domains with varying degrees of transformation.  2) Strengths and Weaknesses Strengths: 1. Innovative Approach: The paper proposes a novel methodology for crossdomain imitation learning addressing a key limitation in the field by eliminating the need for proxy tasks and offering a generalized approach for aligning and comparing stateaction spaces across different agents. 2. Strong Theoretical Framework: The theoretical analysis including the formal characterizations and theorem regarding GWILs capability to preserve optimality across domains provides a deep understanding of the methodologys foundational principles. 3. Empirical Validation: The series of experiments conducted offer solid empirical evidence supporting the efficacy of GWIL in various nontrivial continuous control settings illustrating the methods practical applicability. Weaknesses: 1. Limited Evaluation on NonIsometric Transformations: While the paper shows strong results in environments with isometric transformations it provides less rigorous exploration in settings where transformations between domains are nonisometric or where stateaction spaces are of significantly different dimensions. 2. Lack of Comparative Analysis: The experiments primarily demonstrate the success of GWIL but lack a comprehensive comparison with existing crossdomain imitation learning methods especially those not relying on proxy tasks which could better contextualize GWILs performance. 3. Scalability Concerns: There is limited discussion on the scalability of the method to more complex environments or to multiagent settings which are common in realworld applications. Questions and Feedback to the Author: 1. Could the authors provide more insight into how GWIL might perform or be adapted in highly nonisometric transformations possibly through additional experiments or theoretical discussion 2. A comparison with existing methods particularly those that also aim to bypass the requirement for proxy tasks would be valuable. Can the authors comment on how GWIL might compare in terms of efficiency scalability and applicability to complex domains 3. The practical scalability of GWIL to more complex environments or realworld applications is not extensively discussed. Could the authors elaborate on potential limitations and how they might be addressed  3) Review Summary The paper presents GromovWasserstein Imitation Learning (GWIL) an adaptive approach for crossdomain imitation learning that cleverly utilizes the GromovWasserstein distance for comparing and aligning disparate stateaction spaces without needing proxy tasks. This represents a significant step forward in the field addressing a crucial limitation in current imitation learning paradigms. The theoretical grounding provides a solid foundation for understanding GWILs capabilities and potential applications while the empirical evaluations demonstrate its efficacy in diverse control tasks. However the research could be strengthened by extending the analysis to more challenging nonisometric transformations and by providing a more thorough comparative study with other stateoftheart methods. Additionally considerations regarding the scalability of GWIL to more complex or realworld settings would enhance the papers practical relevance. Overall the contributions of GWIL to the field of imitation learning are substantial presenting a promising method for overcoming the challenges associated with crossdomain imitation learning.", "hm2tNDdgaFK": " Summarize the Paper The paper introduces a novel method designed to enhance molecular representation in graph neural networks (GNNs) by accounting for molecular stereochemistry through the encoding of 3D bond distances bond angles and torsion angles. The model named Chiral InterRotoInvariant Neural Network (ChIRo) specifically addresses the challenge of representing tetrahedral chirality and conformational flexibility in molecules. By leveraging the unique arrangement of torsion angles around chiral centers and incorporating invariance to internal bond rotations directly into the architecture ChIRo aims to diminish the need for conformerbased data augmentation. This is validated across four benchmarks contrasting it with both 2D and 3D GNNs and demonstrating its superior ability to learn chiralsensitive functions from molecular structures.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach to Chirality: The paper adeptly navigates the problem of representing molecular chirality a critical aspect in drug design and molecular property prediction which has been inadequately addressed by previous 2D and 3D GNN architectures. 2. Novelty in Architecture: By integrating a unique torsion encoder and achieving invariance to internal bond rotations ChIRo significantly advances the field in modeling molecules more accurately reflecting their realworld flexibility and stereochemical characteristics. 3. Comprehensive Validation: The model is rigorously tested across four tasks specifically designed to emphasize the importance of chirality wherein ChIRo outperforms stateoftheart models. This not only showcases the models effectiveness but also its practical applicability. 4. Public Dataset Contribution: The authors make their datasets for contrastive learning RS classification and docking tasks publicly available thereby facilitating further research and benchmarking in this domain. Weak Points: 1. Limitation to Tetrahedral Chirality: The study focuses solely on tetrahedral chirality leaving out other forms of stereoisomerism such as cistrans isomerism and atropisomerism. This limits the generalizability of the model to other forms of stereochemistry. 2. Computational Efficiency Not Addressed: There is no discussion on the computational efficiency or scalability of ChIRo compared to other models which is crucial for its application in highthroughput virtual screening and largescale datasets. 3. Lack of RealWorld Validation: Although the model is tested against benchmarks realworld applications such as predicting the efficacy of chiral drugs in biological systems are not explored leaving the practical impact of ChIRo somewhat uncertain. Questions and Feedback: 1. Future Directions: Will there be an extension of the model to cover other forms of stereochemistry beyond tetrahedral chirality Such expansion could significantly broaden the utility of ChIRo. 2. Comparison with Other Domains: Has there been any consideration to test or compare the models performance in learning chirality with approaches used in other domains such as 3D pose estimation in computer vision 3. RealWorld Application: Could the authors elaborate on potential collaborations with pharmaceutical research to apply ChIRo directly in drug design and discovery processes  Review Summary The paper presents ChIRo a cuttingedge model that represents a significant leap in modeling molecular stereochemistry for GNNbased molecular property prediction. Its novel approach towards encoding torsion angles to capture tetrahedral chirality and its ability to model conformational flexibility without relying heavily on data augmentation mark considerable advancements in the field. The comprehensive validation across four tasks uniquely designed to highlight the relevance of chirality is commendable and demonstrates the models superiority over existing technologies. However the exclusion of other forms of stereochemistry and the absence of a discussion on computational efficiency present areas for future improvement. Moreover exploring practical applications especially in drug discovery through collaborations with pharmaceutical research could significantly enhance the models impact and utility. Overall ChIRos contribution to representing molecular chirality in GNNs is undeniably valuable paving the way for more accurate and reliable molecular property prediction essential for drug design and beyond.", "xf0B7-7MRo6": " Review of \"AIRNet for Matrix Completion: Adaptive and Implicit Regularization Networks\"  Summarize the Paper The paper proposes an innovative model AIRNet for the matrix completion (MC) problem a fundamental challenge in fields such as computer vision and recommender systems. Unlike traditional methods that rely on explicit priors like lowrank or sparsity AIRNet employs adaptive and implicit lowrank regularization facilitated by deep neural networks (DNNs) to dynamically capture the complex prior information of the target matrix during the recovery process. The model utilizes a neural network to adaptively parameterize the Laplacian matrix enabling the regularization to evolve based on the recovered matrix thus addressing the limitations of static regularization methods. Theoretical analyses demonstrate that AIRNet not only enhances the inherent lowrank preference of implicit regularization but also circumvents the saturation issue typical of explicit regularization approaches. Numerical experiments on diverse datasets showcase AIRNets superior performance particularly in scenarios with nonrandomly distributed missing elements.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovativeness: The paper introduces a novel approach to MC with the implementation of a dynamic regularization framework\u2014a significant deviation from the static explicit regularization methods. 2. Theoretical Foundation: The authors provide a rigorous theoretical analysis elucidating how AIRNet enhances lowrank preference and avoids saturation which are notable contributions to understanding the models effectiveness. 3. Empirical Validation: Extensive experiments across different data types and missing patterns demonstrate the models versatility and superior performance over traditional methods solidifying the practical viability of AIRNet. Weak Points: 1. Model Complexity: The implementation of AIRNet may entail a higher computational complexity due to the need for training deep neural networks and parameterizing the Laplacian matrix potentially limiting its applicability in resourceconstrained scenarios. 2. Generalization Concerns: While the paper demonstrates impressive results across various datasets there is limited discussion on the generalization ability of AIRNet to unseen data and its robustness against overfitting in diverse practical settings. 3. Hyperparameter Sensitivity: There is scarce information on the sensitivity of the models performance to hyperparameter settings notably the regularization coefficients and neural network architectures which could affect the reproducibility and tuning of the model. Questions and Feedback: 1. Could the authors elaborate on the computational efficiency of AIRNet compared to traditional MC methods particularly in terms of computational resources and time 2. It would be beneficial if the paper could discuss the generalization capabilities of AIRNet possibly supported by crossvalidation results or performance on entirely unseen datasets. 3. More details on the hyperparameter selection process and its impact on the models performance would be appreciable helping readers to better understand and replicate the experiments.  Review Summary This paper presents AIRNet a pioneering approach to solving the matrix completion problem using adaptive and implicit regularization facilitated by neural networks. The introduction of a dynamic regularization method marks a significant advancement in the field potentially overcoming the limitations of existing MC methods that rely on static priors. The theoretical underpinning provided by the authors sheds light on how AIRNet enhances the lowrank property and avoids explicit regularizations saturation issue an invaluable contribution to the literature. The empirical results further support the models efficacy across various data types and missing patterns illustrating AIRNets potential as a versatile tool for MC applications. However the computational complexity and the broader implications of model generalization and hyperparameter sensitivity remain areas that could benefit from deeper exploration. Overall the strengths of the paper\u2014namely its innovative approach solid theoretical framework and strong empirical performance\u2014outweigh the highlighted weaknesses. With further investigation into the mentioned aspects AIRNet promises to be a highly valuable asset for addressing complex MC problems. I recommend this paper for publication providing that the authors consider the feedback regarding computational efficiency model generalization and hyperparameter sensitivity.", "xwAw8QZkpWZ": " Peer Review of \"SAFety skill pRiors (SAFER): Accelerating Safe Reinforcement Learning in Complex Control Tasks\"  1) Summary of the Paper This paper introduces SAFER (SAFety skill pRiors) a novel behavioral prior learning algorithm specifically designed to enhance policy learning in complex control tasks under safety constraints. It emphasizes the limitations of current behavioral priors in ensuring safety during reinforcement learning (RL) particularly because they fail to differentiate between safe and unsafe actions thus potentially promoting hazardous behaviors. SAFER addresses this by utilizing a contrastive training approach on both safe and unsafe data to learn a latent safety variable. This variable not only encodes safety requirements but also enables the extraction of safe primitive skills needed for different scenarios. During inference SAFER composes policies that are both successful and safe by mapping abstract actions through learned safety skills based on the inferred safety variable. The authors validate SAFERs effectiveness through experiments on robotic grasping tasks inspired by the game Operation showing it outperforms baseline methods in terms of both safety adherence and task success rate.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper progresses the field of safe RL by introducing a novel method that addresses the shortcomings of current behavioral priors concerning safety. Its use of a contrastive training method to learn safety variables and safe primitive skills is particularly noteworthy.  Clear Demonstration and Validation: The experimental setup inspired by the Operation game is both relevant and challenging. The clear superiority of SAFER in these tasks over baseline methods showcases its practical value and effectiveness.  WellDefined Problem and Solution: The authors provide a comprehensive background and articulate the problems with current behavioral priors in RL clearly. The proposed solution SAFER is logically presented with a stepbystep methodology that is easy to follow. Weak Points:  Transferability and Scalability: While the results on the robotic grasping tasks are impressive the paper does not extensively discuss the transferability of SAFER to other domains or its scalability to more complex or different types of tasks.  Comparison with Other Safety Methods: The paper compares SAFER mainly with behavioral prior methods. Given the focus on safety including more extensive comparisons with a broader range of existing safe RL methods could strengthen the argument for SAFER\u2019s superiority.  Details on Failure Cases: While the paper emphasizes the successes of SAFER a more detailed discussion on its limitations or potential failure modes would provide a fuller picture of its applicability and robustness. Questions and Feedback:  How does SAFER perform in a dynamic environment where safety constraints may evolve over time Is there a mechanism for SAFER to adapt to such changes  The paper could benefit from a deeper discussion on the implications of the choice of contrastive loss and the design of the latent safety variable. How sensitive is SAFERs performance to these design choices  Have the authors considered the computational efficiency of SAFER especially when scaling to larger datasets or more complex environments Insights into its computational requirements would be valuable.  3) Review Summary and Recommendation The paper introduces a significant advancement in the sphere of safe reinforcement learning by proposing SAFER a method that not only ensures rapid policy learning in complex control tasks but does so with an inherent focus on safety. The methodological rigor innovative approach and convincing validation underscore the papers contribution to the field. However extending the discussion on scalability broader applicability and computational efficiency as well as a more comprehensive comparison with a wider range of safe RL methods would further solidify the papers impact. Supporting Arguments:  SAFER\u2019s novel approach to distinguishing between safe and unsafe actions using a contrastive training method addresses a critical gap in the existing literature.  The experimental validation provides strong evidence of SAFERs effectiveness in enhancing both the safety and success of policies in complex control tasks.  The discussion on the limitations of current behavioral priors and how SAFER overcomes these challenges is wellarticulated and logically structured. Given the strengths of the proposed approach and the quality of the experimental validation I recommend this paper for publication. The suggested areas for improvement and expansion could be considered for future work or as potential enhancements to further demonstrate the robustness and applicability of SAFER across a broader range of scenarios.", "pjqqxepwoMy": " Review of \"Variational Latent Oracle Guiding for Deep Reinforcement Learning\" 1. Summary The paper presents a novel framework called Variational Latent Oracle Guiding (VLOG) for enhancing deep reinforcement learning (DRL) with oracle observations\u2014information not accessible during online decisionmaking but available during offline training. It tackles a significant but underexplored challenge in DRL: leveraging external hindsight information (oracle observation) to improve learning and decisionmaking. By introducing a Bayesian formulation VLOG integrates oracle observations into the learning process promising better decision models that rely solely on executor observations (information available during task execution). The authors demonstrate VLOG\u2019s effectiveness across various tasks including video games and the complex tilebased game Mahjong. Additionally they contribute a benchmarking Mahjong environment and dataset to encourage further research in this area. 2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Novelty of Approach: The Bayesian formulation for integrating oracle observations into DRL is innovative. VLOG addresses the challenge of harnessing hindsight information to refine policies a relatively unexplored aspect that could significantly impact the field.  Empirical Validation: The comprehensive experiments across different domains particularly the challenging Mahjong game convincingly demonstrate VLOG\u2019s effectiveness. The comparison with existing heuristic methods further highlights its superiority.  Practical Contribution: By releasing the Mahjong environment and corresponding RL dataset the paper provides valuable resources for the RL community facilitating further research in oracle guiding and beyond. Weak Points:  Scalability and Computational Costs: While VLOG shows promise the paper does not fully address scalability and the computational overhead of incorporating oracle observations especially in complex or realworld scenarios. Understanding these aspects is crucial for evaluating the frameworks practicality.  Hyperparameter Sensitivity: The paper introduces the taskagnostic latent bottleneck control method to adjust the \u03b2 coefficient adaptively but does not extensively discuss how sensitive VLOGs performance is to other hyperparameters which could impact its ease of use and generalizability.  Comparison with a Broader Range of Methods: The study primarily compares VLOG with heuristic methods and a few variational approaches. Including comparisons with a wider array of stateoftheart DRL methods might have provided a more detailed understanding of VLOG\u2019s position within current research. Questions and Feedback:  Could the authors expand on the computational implications of implementing VLOG especially concerning scalability to more complex or realworld tasks beyond the ones tested  How sensitive is VLOG\u2019s performance to its hyperparameters including the \u03b2 coefficient and others across different tasks More insight into this could help understand the robustness and flexibility of VLOG.  Including broader comparisons with stateoftheart DRL methods might better contextualize VLOGs performance. Can the authors comment on how they expect VLOG to perform against such methods particularly in tasks not covered in the paper 3. Review Summary This paper introduces VLOG a novel framework that innovatively uses a Bayesian approach to leverage oracle observations in deep reinforcement learning. Its strong empirical validation across various tasks notably including a challenging tilebased game Mahjong demonstrates its effectiveness and potential impact on the field. Additionally the practical contribution of releasing relevant resources encourages further research. However the paper would benefit from a more detailed discussion on scalability computational costs and hyperparameter sensitivity to fully assess VLOGs practicality and robustness. Expanding the range of comparisons to include a broader array of current DRL methods could also enhance the paper by providing a more comprehensive evaluation of VLOGs performance. Overall the paper adds significant value to the current literature by addressing an underexplored but crucial aspect of DRL promising to inspire future developments in the field.", "kOu3-S3wJ7": " Review of \"Graph Recurrent Imputation Network for Multivariate Time Series\"  Summary of the Paper The authors address the challenge of imputing missing values in multivariate time series (MTSI) coming from realworld applications a common yet complex problem when dealing with data from interconnected sensors or networks. They highlight the limitations of stateoftheart deep learning imputation methods which largely fail to capture nonlinear time and space dependencies and do not fully exploit relational information available in the data. To tackle these limitations the authors propose a novel architecture named Graph Recurrent Imputation Network (GRIN) based on graph neural networks (GNNs). GRIN models multivariate time series as sequences of graphs where relations among different channels are treated as edges in a graph allowing the exploitation of spatiotemporal dependencies through message passing. They claim that GRIN outperforms existing methods significantly in several realworld datasets showing improvements in mean absolute error of over 20.  Strong Points 1. Innovative Approach: The integration of GNNs with recurrent neural network (RNN) concepts to handle MTSI represents a novel approach addressing the relational and structural aspects often overlooked in traditional methods. 2. Comprehensive Evaluation: The empirical analysis is thorough covering multiple realworld datasets with different characteristics. This diversity in testing scenarios adds to the credibility of the results presented. 3. Flexibility in Application: The ability of GRIN to deal with variable dimensions in input data is a considerable advantage offering greater utility across a range of practical applications.  Weak Points 1. Model Complexity and Interpretability: The complexity of the GRIN model while beneficial in capturing intricate relationships may also hinder its interpretability and ease of implementation in realworld settings. 2. Assumptions on Data Stationarity: The assumption of stationarity in missing data distribution is a significant limitation particularly when dealing with realworld time series data that often exhibit nonstationary behaviors. 3. Computational Efficiency and Scalability: Given the advanced architecture there are concerns about the computational efficiency and scalability of GRIN especially when applied to very large datasets or when running on limited computational resources.  Questions and Feedback to the Author 1. Interpretability Concerns: How does the models complexity impact its interpretability and are there ways to mitigate this without sacrificing performance 2. Handling NonStationary Data: Could the authors elaborate on potential extensions or modifications to GRIN to handle nonstationary time series data effectively 3. Computational Resource Requirements: Can the authors provide insights into the computational resources required for training and deploying GRIN particularly in comparison to the baselines  Review Summary The paper presents a compelling approach to tackling the longstanding problem of missing value imputation in multivariate time series data through the innovative use of graph neural networks. GRINs architecture is wellsuited to exploit the spatiotemporal and relational aspects inherent in such data as evidenced by its superior performance across several benchmarks. While the model introduces significant improvements over existing methods there are valid concerns regarding its complexity assumptions on data stationarity and computational efficiency that merit further exploration. Addressing these concerns could enhance the models applicability and feasibility in a broader range of realworld scenarios. Overall given the solid presentation of the novel idea extensive empirical validation and the significant improvements demonstrated over stateoftheart methodologies this work represents a valuable contribution to the field of multivariate time series imputation. Future work addressing the limitations identified could further solidify its utility and adoption in practical applications.", "lgOylcEZQgr": "Summarize the Paper The paper introduces an innovative unsupervised learning model designed for online visual representation and category learning in environments where data distribution is nonstationary and class labels are sparse or nonexistent. The model termed the online unsupervised prototypical network (OUPN) leverages a prototypebased memory network and contrastive loss to learn and update visual representations and class prototypes from a stream of input data. This approach allows the model to form new class prototypes with minimal examples and adapt to imbalanced data distributions naturally observed in realworld settings. The authors demonstrate the superiority of OUPN over stateoftheart selfsupervised learning methods through experiments on the RoamingRooms RoamingOmniglot and RoamingImageNet datasets showcasing its robustness against noniid data sequences and class imbalances. Strong Points 1. Innovative Approach: The paper proposes a novel method for unsupervised learning in an online setting which is a significant departure from traditional batchbased iidfocused methods. This approach is more aligned with realworld learning scenarios. 2. Robust Handling of Imbalance and Nonstationarity: OUPNs ability to dynamically form and adjust class prototypes in response to new examples without prior class labels represents an advanced capability to deal with the class imbalance and nonstationary distributions often found in natural environments. 3. Significant Empirical Results: The model outperforms existing stateoftheart selfsupervised methods on several benchmarks particularly in noniid settings highlighting its effectiveness in more realistic learning scenarios. Weak Points 1. Scalability Concerns: While the method shows promising results in the experimental settings provided the scalability of the prototypebased memory network to a larger number of classes or higher dimensional data remains a concern not fully addressed in the paper. 2. Dependence on Hyperparameters: The models performance might be highly sensitive to the choice of hyperparameters such as decay factors loss weights (\u03bbent \u03bbnew) and thresholds for creating new prototypes. The paper lacks a detailed exploration of the impact of these hyperparameters on the learning outcomes. 3. Lack of Broader Applicability Discussion: Although the paper conveys the models effectiveness in visual category learning it omits a discussion on the applicability of the proposed method to other domains (e.g. audio text) or tasks beyond category recognition (e.g. object detection segmentation). Questions and Feedback to the Author 1. Scalability: Can the authors provide insights or experimental results on how the model scales with the increase in the number of classes or higher data dimensionality 2. Hyperparameter Sensitivity: How sensitive is the models performance to the selection of hyperparameters It would be helpful to include a sensitivity analysis or guidelines for choosing these parameters effectively. 3. Transferability to Other Domains: Have the authors considered or tested the applicability of OUPN in other data modalities or learning tasks Insights on whether the proposed methods principles can be generalized would be valuable. Review Summary This paper presents a noteworthy advance in the field of unsupervised learning by addressing the challenging problem of learning visual representations and categories online in natural nonstationary environments. The proposed OUPN model demonstrates a significant aptitude for dynamically forming categories based on minimal examples and adjusting to imbalanced data distributions outperforming stateoftheart selfsupervised methods in realistic learning scenarios. Despite these strengths questions remain regarding the models scalability the impact of hyperparameter selection on performance and the broader applicability of the method to other domains or tasks. To bolster the findings and utility of the proposed approach further investigations on these aspects would be beneficial. Overall the paper contributes a valuable perspective to the ongoing dialogue on learning in complex realworld settings promising avenues for future research in unsupervised learning methodologies.", "kAa9eDS0RdO": " Review of the paper titled \"ConceptTransformer: Enhancing Deep Learning Models with HighLevel ConceptBased Interpretability\"  1. Summary of the paper This paper introduces the ConceptTransformer (CT) a transformerbased module intended to bridge the gap between the raw featurebased interpretability of deep learning models and humanunderstandable highlevel conceptual interpretations. The CT is designed to work within existing deep learning architectures serving as a classifier head that relates input features to highlevel domainrelevant concepts using a crossattention mechanism. The authors argue that by focusing on these highlevel concepts which are more aligned with human cognitive processes the CT not only maintains the models performance but also enhances its interpretability. The CT builds explanations based on attention weights between model inputs and predefined concepts ensuring that these explanations are both plausible (convincing to humans) and faithful (accurate reflections of the models reasoning). The paper validates the approach by applying the CT to three different image datasets proving that it can improve model interpretability without sacrificing performance. Furthermore it claims that incorporating domain knowledge via highlevel concepts can potentially boost model accuracy particularly in datascarce scenarios.  2. Strong and weak points Questions and Feedback Strong Points:  The paper tackles an important issue in AI: the tradeoff between model performance and interpretability. The proposed ConceptTransformer is an innovative solution that doesnt compromise the models accuracy for the sake of making it interpretable.  Validation on multiple datasets and comparison with existing methods add credibility and demonstrate the versatility of the CT across different use cases.  The focus on highlevel concepts is welljustified with a solid theoretical foundation effectively addressing the limitations of traditional featurebased interpretability approaches. Weak Points:  Despite the claimed benefits there could be concerns regarding the scalability of the method especially when dealing with a large number of highlevel concepts or complex relationships between concepts and inputs.  The paper could benefit from a deeper exploration of the limitations and potential drawbacks of the CT including cases where it might not be applicable or its explanations could be misinterpreted.  The evaluation focuses primarily on image datasets expanding this to other modalities (e.g. text audio) could further validate the method\u2019s applicability and versatility. Questions and Feedback:  Could the authors expand on how the CT would manage scenarios where concepts are not welldefined or are subjective For instance how does it handle ambiguities in concept definitions  How does the CT adapt when new domain knowledge becomes available For instance if new relevant concepts are discovered posttraining how can the model incorporate these without extensive retraining  A comparative analysis of computational overhead introduced by integrating the CT into existing models would be informative. Can the authors comment on the expected impact on training time and resource requirements  3. Review Summary The paper presents a novel and promising approach to enhancing the interpretability of deep learning models through highlevel conceptbased explanations. The ConceptTransformer designed to act as a classifier head that maps input features to humanunderstandable concepts represents a significant step forward in making AI models more transparent and their decisions easier to interpret for humans. By ensuring that model explanations are both plausible and faithful the CT addresses critical challenges in AI ethics and applicability in domaincritical scenarios. However the approachs scalability the potential limitations in its applicability across different data modalities and the computational overhead it introduces warrant further investigation. Expanding the evaluation to include diverse data types and more complex scenarios could strengthen the papers claims and demonstrate the CTs full potential. Given the current enthusiasm for interpretable AI and the pressing need for transparent trustworthy models this paper contributes significantly to the field. With further development and rigorous validation the ConceptTransformer could become a key tool in bridging AI capabilities and humancentric interpretability making it a vital component of future AI systems across various application domains.", "jT5vnpqlrSN": " 1. Summarize the paper The paper introduces a novel framework for Graph Neural Networks (GNNs) named Graph Inference Representation (GIR) designed to implicitly encode position information within graph structures by leveraging anchorbased positioning combined with a specialized messagepassing mechanism. Traditional GNNs struggle to capture positional information due to their reliance on the messagepassing neural network (MPNN) pattern leading to limitations in distinguishing nodes based on their positions within the graph. Existing solutions precompute distances between nodes as a twostage process which the paper argues lose out on the potential interaction between distance computation and encoding stages. GIR aims to address this by employing a simple anchor indication strategy alongside a more specialized MPNN architecture allowing for the implicit tracking of shortest paths and better encoding of node positions. This is theoretically explored and empirically tested showing that GIR outperforms existing positionaware GNN methods on tasks involving positionaware datasets.  2. Strong and weak points questions and feedback Strong Points:  Novelty: The GIR framework introduces a unique approach to embedding position information implicitly within GNNs which is a significant advancement in overcoming the limitations of existing MPNN models.  Theoretical and Empirical Support: The paper provides a solid theoretical foundation for the GIR framework alongside comprehensive empirical evidence showcasing its superiority over current positionaware GNN methods.  Practical Relevance: The ability to encode position information more effectively is critical for a wide range of applications enhancing the utility and applicability of GNNs in realworld problems. Weak Points:  Complexity and Scalability: The paper lacks a detailed discussion on the computational complexity and scalability of the GIR model particularly when applied to larger and more complex graphs.  Comparative Analysis: While the paper claims superiority over existing methods a more granular comparison or case study highlighting specific scenarios where GIR significantly outperforms others would add value.  Implementation Details: More insights into the challenges of implementing GIR in different settings (e.g. dynamic graphs) and potential solutions or guidelines would be beneficial. Questions and Feedback to the Author:  Could the authors elaborate on the computational complexity of the GIR framework especially in comparison to existing positionaware GNN methods  It would be interesting to see a deeper analysis or a case study where GIRs approach to embedding position information is critically important. Can the authors provide such examples  The authors discuss the theoretical motivations behind GIR however insights into practical challenges encountered during implementation and possible mitigation strategies would be highly valuable.  3. Review Summary The manuscript presents a significant contribution to the field of Graph Neural Networks by introducing the Graph Inference Representation (GIR) framework which innovatively addresses the limitation of existing GNNs in capturing node position information. The proposal is wellfounded theoretically and substantiated by empirical evidence highlighting GIRs effectiveness over traditional positionaware methods. Despite the strong foundation and advantageous results the paper would benefit from a more detailed discussion on computational aspects a thorough comparative analysis under different scenarios and elaboration on practical implementation challenges. Supporting arguments for the recommendation include the frameworks novel approach to integrating distance information implicitly the combined theoretical and empirical validation of GIRs efficacy and its potential to enhance GNN applications in diverse realworld scenarios. Additional information on the areas highlighted for improvement would make the contribution even more robust and insightful for both the academic community and practical applications.", "xCVJMsPv3RT": "Summarize the paper: This paper introduces DroQ an advancement in computational and sample efficiency for reinforcement learning (RL) specifically targeting continuousaction benchmarks. Responding to the limitations of Randomized Ensembled Double Qlearning (REDQ) in terms of computational demand DroQ innovates by utilizing a reduced ensemble of Qfunctions that incorporate dropout and layer normalization. This approach not only preserves the sample efficiency achieved by REDQ but also significantly enhances computational efficiency making it comparable to the Soft ActorCritic (SAC) method. The paper thoroughly evaluates DroQ against relevant baselines through experiments in the MuJoCo benchmark confirming its double efficiency. Additionally it provides insights on the synergy between dropout and layer normalization and reevaluates the application of dropout in high updatetodata ratio settings. Strengths and Weaknesses: Strong Points: 1. Innovative Approach: The introduction of DroQ as a computationally efficient variant of REDQ by integrating dropout and layer normalization into the Qfunctions presents a novel contribution to the field of reinforcement learning. 2. Comprehensive Evaluation: The paper rigorously evaluates DroQ across multiple aspects including sample efficiency computational efficiency and bias reduction against established benchmarks. 3. Practical Insights: The authors offer valuable engineering insights on applying dropout in RLespecially in the context of high UTD ratios which can guide future research and applications. 4. Clear and Reproducible Methodology: The simplicity of the DroQ implementation as highlighted by the authors facilitates reproducibility and experimentation by practitioners. Weaknesses: 1. Limited Discussion on Generalization: The paper could benefit from a more extensive discussion on how DroQ generalizes across different environments or tasks beyond the MuJoCo benchmark. 2. Potential Overfitting Concerns: While dropout is beneficial for model uncertainty theres a lack of analysis regarding its potential for causing overfitting within the small ensemble setting especially given the high UTD ratios. 3. Comparison with Other StateoftheArt Methods: The paper focuses on REDQ and SAC for comparisons. Including a broader range of stateoftheart methods could provide a more comprehensive assessment of DroQs performance. Questions and Feedback: 1. Generalization Capability: Can the authors provide insights or additional results on DroQs performance in environments or tasks not covered by the MuJoCo benchmark 2. Overfitting Analysis: Could the authors discuss any measures taken or observed effects concerning overfitting particularly due to the combined use of dropout and high UTD ratios 3. Potential for Further Reduction in Computational Load: Are there any identified avenues for further reducing the computational load perhaps through optimization of the dropout rate or ensemble size Review Summary: The paper makes a significant contribution to the field of reinforcement learning by proposing DroQ a method that addresses the computational inefficiency of REDQ without compromising sample efficiency. The thorough experimental analysis showcases DroQs strengths offering a viable solution for applications where computational resources are limited. Despite its many strengths including simplification of implementation and new insights into the use of dropout the paper could be strengthened by addressing the potential for overfitting and expanding the comparative framework to include a wider range of stateoftheart methods. Additionally discussions on the generalizability of DroQ to a broader range of tasks and environments would enhance the papers impact. Overall the proposed method represents a valuable advancement in developing efficient and effective RL strategies warranting further exploration and application in diverse RL challenges.", "uqBOne3LUKy": " Peer Review for \"Importance Weighting and PolynomiallyTailed Losses in Overparameterized Neural Networks\"  1) Summarize the paper The paper investigates the efficacy of importance weighting in the context of overparameterized neural networks under distribution shifts. The authors challenge the prevailing notion that importance weights are ineffective with overparameterized models especially when gradient descent minimizes exponentiallytailed losses such as logistic or crossentropy losses. They argue that this ineffectiveness stems not from overparameterization itself but from the choice of loss function. By introducing polynomiallytailed losses they theoretically and empirically show how importance weighting can effectively adjust for distribution shift even in overparameterized models. Theoretical analysis is provided for the behavior of gradient descent on these polynomiallytailed losses demonstrating advantages in label shift scenarios including test error reduction and improved performance over reweighted crossentropy loss in practical experiments on neural networks with label and subpopulation shifts.  2) Strong and Weak Points Questions and Feedback to the Author Strong Points:  Innovative Approach: The paper presents a novel argument that the failure of importance weights in dealing with distribution shifts in overparameterized models can be addressed by changing the loss function from an exponentiallytailed one to a polynomiallytailed one.  Theoretical and Empirical Evidence: It provides both strong theoretical underpinning and empirical evidence supporting the effectiveness of polynomiallytailed losses in conjunction with importance weighting for correcting distribution shifts.  Practical Value: The experiments showcase significant improvements in test accuracy on datasets with label and subpopulation shifts suggesting practical applicability in realworld scenarios. Weak Points:  Assumption of Linear Separability: The theoretical analysis primarily focuses on linearly separable data for gradient descent behavior characterization. This assumption might limit the generalization of the findings to scenarios where data is not linearly separable.  Complexity of PolynomiallyTailed Loss Implementation: The implementation and optimization of polynomiallytailed losses could be more complex than standard loss functions a point not thoroughly addressed in the discussion.  Lack of Broad Comparison: While the empirical evaluation includes comparisons to stateoftheart methods it would be beneficial to see a broader range of benchmarks including different types of neural network architectures and a wider range of distribution shift scenarios. Questions and Feedback:  On Generalization Beyond Linear Separability: Can the authors clarify how the proposed method might perform or be adapted for datasets that are not linearly separable  Addressing Implementation Complexity: Could the paper elaborate on any potential challenges in implementing and optimizing polynomiallytailed losses in practice Tips or best practices for practitioners would be valuable.  Further Empirical Studies: It would enrich the paper to include further empirical comparisons or studies on more complex neural network architectures or in scenarios involving more nuanced types of distribution shifts.  3) Review Summary (Supporting Arguments for Recommendation) This paper stands out for challenging a prevailing belief regarding importance weightings efficacy in overparameterized models by pointing to the role of loss functions. The introduction of polynomiallytailed losses as a solution is both theoretically intriguing and practically valuable as demonstrated by the empirical results showing improved performance in adjusting for distribution shifts. The theoretical analysis is robust providing a significant contribution to understanding gradient descent behavior on these losses. However the papers discussion could be enhanced by addressing the complexity of implementing and optimizing polynomiallytailed losses and expanding its empirical evaluation to a broader range of scenarios and network architectures. Additionally the assumption of linear separability might limit the applicability of the theoretical findings which warrants further exploration. Overall the paper provides a compelling argument supported by solid theoretical and empirical evidence making it a valuable contribution to the field. It opens up interesting avenues for future research on loss function design and importance weighting in overparameterized models under distribution shifts. Therefore I recommend this paper for publication with minor revisions to address the highlighted points.", "uoBAKAFkVKx": " Summarize the paper The paper presents a novel method Hypothesistest Driven Coordinate Ascent (HDCA) for black box optimization in learning policies for stochastic environments within the context of Reinforcement Learning (RL). HDCA advances the application of coordinate ascent optimization combined with hypothesis testing as a gradientfree approach making it significantly efficient in distributed and parallel computing environments. The technique was empirically tested on various tasks from the MuJoCo physics simulator and OpenAI Gym framework showcasing its ability to achieve results comparable or superior to existing RL benchmarks without reliance on gradient estimation. The method breaks down the policy optimization problem into smaller subproblems optimizing each using a simple random search strategy that evaluates cumulative rewards of policy perturbations through hypothesis testing. This accounts for the stochastic nature of RL environments more effectively than previous methods.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: Integrating hypothesis testing with coordinate ascent optimization for policy learning is novel and addresses the limitations of gradientbased methods in stochastic environments. 2. Scalability: HDCAs design is inherently parallelizable leveraging distributed computing resources effectively and improving convergence speed. 3. Empirical Validation: The paper presents thorough experiments across multiple environments providing concrete evidence of HDCAs efficacy. 4. Independence from Gradients: By not relying on gradient information HDCA simplifies the optimization process and avoids issues related to gradient estimation errors. Weak Points: 1. Hyperparameter Sensitivity: The paper does not extensively discuss the sensitivity of HDCA to its hyperparameters such as the block size or the significance level \u03b1 in hypothesis testing. Understanding this sensitivity is crucial for practical deployment. 2. Comparison Depth: While the paper presents comparisons with other methods a deeper analysis of why HDCA outperforms these methods in specific scenarios could provide more insightful conclusions. 3. Large Network Policies: The authors mention future work on extending HDCA to larger networks but do not present initial findings or a preliminary framework for such extensions. Questions: 1. How does HDCAs performance and efficiency scale with the complexity of the RL environment particularly in highdimensional state or action spaces 2. Can the authors provide more details on the choice and impact of hyperparameters especially in different environmental setups 3. Are there any theoretical guarantees for the convergence of HDCA or scenarios where it might fail to converge Feedback:  It would enrich the paper to include a deeper analysis of hyperparameter impact on HDCAs performance perhaps through ablation studies or sensitivity analysis.  Expanding the comparison section to include qualitative explanations or theoretical considerations might offer readers more nuanced insights into HDCAs advantages.  Discussing potential strategies for applying HDCA to larger possibly nonlinear policy networks could bridge the identified gap for future research and practical applications.  Review Summary This paper introduces an innovative approach HDCA which significantly contributes to gradientfree optimization techniques in RL especially in environments where dealing with stochasticity is crucial. The method stands out for its effective use of parallel computing resources demonstrating superior or equivalent performance to established methods across tested environments. Despite its strengths the paper could benefit from a more indepth discussion on hyperparameter sensitivity theoretical convergence guarantees and broader applicability to complex policy networks. Overall the presented work is both significant and promising with clear areas for future exploration that could further solidify its practical utility and theoretical importance. The paper is recommended for publication with minor revisions to address the aforementioned points for an even stronger impact on the field of reinforcement learning.", "vkaMaq95_rX": " Summarize the paper The paper introduces \"EXACT\" a memoryefficient framework for training Graph Neural Networks (GNNs) with compressed activations to address the high memory usage challenge of training GNNs on large graphs. EXACT utilizes two compression techniques: quantization and random projection applied sequentially to the node embeddings (activations). This method substantially reduces memory footprint\u2014up to 32\u00d7\u2014with a minimal accuracy drop (0.20.5) and a manageable increase in training time (1025). EXACT is implemented as an extension for both Pytorch Geometric and Pytorch showcasing its feasibility by enabling training on hardware with significantly lower memory requirements (e.g. reducing from a 48GB GPU to a 12GB) without severely compromising performance. The paper systematically evaluates the tradeoffs between memory saving computational overhead and model accuracy through extensive experiments across different models and datasets.  Strong points 1. Innovative Approach: The paper tackles the wellknown problem of high memory usage in GNN training from a novel angle focusing on compressing activation maps instead of reducing the number of nodes processed. This broadens the scope of possibilities for memory efficiency in GNNs. 2. Comprehensive Evaluation: The extensive experiments across various models and datasets provide a thorough understanding of the tradeoffs involved in using compressed activations. This strong empirical foundation helps in establishing the viability of EXACT in realworld applications. 3. Practical Utility: By enabling the training of GNNs on GPUs with significantly lower memory EXACT directly addresses a major barrier facing researchers and practitioners with limited access to highend computing resources. This enhances the accessibility and adoption of GNN models. 4. Open Source Contribution: Making the code available as an extension for popular libraries like Pytorch Geometric extends the reach and potential impact of EXACT allowing for easy adoption and further development by the community.  Weak points 1. Overhead Analysis: While the paper mentions time overhead a deeper analysis of the types of overheads incurred (e.g. computational vs. memory access overheads) and how they scale with graph size could offer more insights into the practical limitations of EXACT. 2. Comparison with StateoftheArt: The paper could benefit from a more direct comparison with existing stateoftheart memory reduction techniques for GNNs. Such an analysis would help in positioning EXACT within the broader landscape of memoryefficient GNN training. 3. LongTerm Impact on Model Performance: The study briefly touches on the accuracy drop and time overhead but a more extended discussion on how these factors could affect longterm model performance especially in dynamic graphs or when models are continually trained with new data would be valuable. 4. Generalization across Different GNN Architectures: While the paper tests EXACT on several popular GNN models further exploration into how EXACT performs with newer or less common GNN architectures could widen its applicability.  Questions and Feedback to the author 1. Could you elaborate on how EXACTs time overhead scales with graph size and complexity Understanding this scaling would be crucial for largescale applications. 2. Have you considered integrating EXACT with other memory reduction techniques (e.g. node sampling layerwise training) to see if there\u2019s a compounding effect on memory efficiency 3. How does EXACT handle dynamic graphs where node embeddings might change frequently Is there a significant additional overhead in recalculating compressed activations 4. Given the minimal accuracy drop reported could further compression (beyond the maximum explored in your experiments) be feasible without critically impacting model performance  Review Summary The paper presents a valuable contribution to the field of GNN training by introducing a novel approach to reduce memory usage through compressed activations. The authors have demonstrated the effectiveness of EXACT across various scenarios showing it to be a promising solution for training GNNs on hardware with limited memory resources. The provision of an opensource implementation further enhances its value. However the paper would benefit from a more detailed analysis of time overheads a direct comparison with stateoftheart techniques and an exploration of its longterm impact on model performance. Addressing these points could strengthen the papers contributions and offer deeper insights into the practical applicability and scalability of EXACT.", "l9tb1bKyfMn": " Review of \"Exploring a Lightweight SelfAttention Mechanism for Efficient Transformer Models in Visual Tasks\"  1) Paper Summary This study presents a novel lightweight selfattention mechanism termed Lowrelation Mutilhead SelfAttention (LMSA) aimed at reducing the computational complexity inherent to the selfattention mechanism widely used in Transformer networks. The new approach diverges from traditional selfattention by altering the dimensional consistency between the Query Key and Value components subsequently lowering computational demands and storage needs. Through experiments utilizing Swin and Cswin models as backbones the authors demonstrate that LMSA not only reduces the computational resources required for selfattention but can also improve accuracy on image classification tasks such as CIFAR10 all while decreasing the models parameter count and storage size significantly. The findings challenge the necessity of dimensional consistency in selfattention mechanisms and suggest that LMSA can serve as an efficient alternative to conventional methods.  2) Strengths and Weaknesses Questions and Feedback Strengths:  Innovative Approach: The paper introduces a novel less resourceintensive version of the multihead selfattention mechanism tackling a pressing issue in the field of computer vision regarding the high computational cost of standard Transformer networks.  Comprehensive Experiments: The authors conducted extensive experiments with both controlled and realworld datasets (CIFAR10 and Caltech) to validate the effectiveness of LMSA. Their methodical approach provides convincing evidence of the proposed mechanisms efficacy.  Significant Improvements: The results showing improved accuracy in image classification tasks alongside substantial reductions in computational resources and model parameters are highly promising. This suggests that the LMSA could be pivotal in deploying Transformerbased models in more resourceconstrained environments. Weaknesses:  Lack of Theoretical Exploration: The paper falls short in providing a thorough theoretical justification for why breaking the dimensional consistency in selfattention mechanisms leads to improved performance and efficiency.  Limited Scope: The studys experiments focus primarily on image classification tasks. A broader investigation across various domains within computer vision and beyond could provide a fuller understanding of LMSAs applicability and limitations.  Comparison with Related Work: While the paper mentions current efforts to improve selfattention mechanisms efficiency a more detailed comparative analysis highlighting the specific advantages of LMSA over these alternatives would strengthen its argument further. Questions and Feedback:  Could the authors elaborate on the potential theoretical underpinnings that enable LMSA to maintain or even improve accuracy despite the reduction in dimensional consistency  How does LMSA compare to other recent efforts aimed at reducing the computational demands of selfattention mechanisms such as Linformer or Performer in terms of both efficiency and accuracy  Expanding the scope of experiments to include more diverse tasks and datasets within computer vision and perhaps other fields applying Transformer networks could underline the general applicability and robustness of LMSA.  3) Review Summary and Recommendation The paper introduces a compelling and innovative approach to reducing the computational complexity of selfattention mechanisms within Transformer networks a notable improvement in the field of computer vision. The proposed Lowrelation Mutilhead SelfAttention mechanism is demonstrated to not only reduce computational and storage demands but also potentially improve model accuracy. However the paper could benefit from a deeper theoretical discussion on the principles enabling these advancements a broader range of experiments across different tasks and domains and a more detailed comparative analysis against related work. Considering the significant contributions and the promising results obtained I recommend this paper for publication contingent upon the authors addressing the outlined weaknesses especially the provision of additional theoretical background and broadening the experiment scope. This study indeed advances our understanding of efficient Transformer models and proposes a viable path toward more resourceefficient implementations in computer vision and potentially other areas employing Transformer architectures.", "xENf4QUL4LW": " 1) Summarize the paper This paper proposes a novel approach to deal with the challenge of learning with noisy labels by focusing on the sample selection process. Traditional methods select smallloss examples for training assuming they are likely correctly labeled. However this paper argues that such methods do not account for the uncertainty associated with largeloss examples which may either be mislabeled or belong to underrepresented groups. To address this issue the authors propose using interval estimation of losses specifically the lower bounds of confidence intervals derived from distributionfree concentration inequalities for sample selection. This method allows for the inclusion of largeloss but less frequently selected data thus enabling the model to better distinguish between mislabeled and underrepresented correctly labeled data. The proposed method is tested on synthetic and realworld noisy datasets demonstrating superior performance and robustness to various types of label noise compared to existing baselines.  2) Strong and weak points Questions and Feedback to the author Strong Points:  Innovative Approach: The paper introduces a novel mechanism for sample selection by using interval estimation a significant shift from the conventional point estimation methods. This allows for a better handling of the inherent uncertainty in the training data.  Comprehensive Experimental Evaluation: The experiments cover a wide range of scenarios including synthetic balancedimbalanced noisy datasets and realworld noisy datasets. This extensive evaluation showcases the robustness and versatility of the proposed method.  Theoretical Justification: Theoretical analyses along with proofs and concentration inequalities derived for the robust mean estimator provide a solid foundation for the proposed method enhancing the confidence in its effectiveness. Weak Points:  Complexity and Implementation Details: While the method clearly outperforms the baselines the paper could benefit from a more detailed discussion on the computational complexity and practical implementation details. For instance information on the overhead introduced by calculating interval estimates and selecting samples based on them would be valuable.  Discussion on Limitations: The paper could further improve by discussing potential limitations or scenarios where the proposed method might not perform as expected. Understanding these boundaries can guide future research and application scopes.  Hyperparameter Sensitivity Analysis: While the paper mentions tuning hyperparameters (\u03c3 and \u03c4min) with a noisy validation set a deeper analysis regarding the sensitivity of the models performance to these hyperparameters would be informative. Questions and Feedback: 1. Computational Complexity: Could the authors elaborate on the computational complexity introduced by the interval estimation and sample selection process compared to conventional methods 2. Limitations: Are there any specific scenarios where the proposed methods performance might degrade How would the method perform with extremely high levels of noise or highly imbalanced datasets 3. Hyperparameter Sensitivity: How sensitive is the models performance to the hyperparameters \u03c3 and \u03c4min Would the models robustness be significantly affected by suboptimal values  3) Review Summary The paper presents a significant advancement in handling noisy labels within weaklysupervised learning frameworks by introducing an innovative sample selection strategy based on interval estimation of losses. This approach not only addresses a critical gap in existing methodologies by factoring in the uncertainty associated with largeloss examples but also showcases remarkable improvements in performance across various datasets and noise types. The theoretical foundations and comprehensive experimental validations add credibility and depth to the research. However the paper could be enhanced by providing more insights into the computational implications of the proposed method discussing potential limitations and conducting a sensitivity analysis of key hyperparameters. Addressing these aspects would make the paper more comprehensive and guide future research endeavours effectively. Overall the proposed method represents a valuable contribution to the field opening new avenues for tackling the challenge of learning with noisy labels. It is recommended that with slight improvements particularly in addressing the highlighted weak points the paper could have an even more significant impact.", "sMqybmUh_u8": " Peer Review of \"Learning Hierarchical Structures for Reinforcement Learning through MetaReinforcement Learning\"  Summary The paper presents an innovative approach to Hierarchical Reinforcement Learning (HRL) through a metareinforcement learning framework. The authors address the gap in existing HRL methods specifically the reliance on preconstructed hierarchies or heuristics without provable guarantees. By focusing on a tabular setting with latent hierarchical structures embedded in transition dynamics the paper introduces \"diversity conditions\" and an optimismbased algorithm that allows for sampleefficient recovery of these structures. Key contributions include the development of a tractable hierarchylearning algorithm proof of sample efficiency improvements over bruteforce methods and theoretical regret bounds that provide insights into the benefits of HRL in practice. The paper notably illustrates its claims through the analysis of problem settings such as Gated FourRoom and the Alchemy benchmark underlining the practical relevance of the theoretical framework proposed.  Strong Points 1. Novelty and Theoretical Contribution: The paper establishes a novel framework for learning hierarchical structures in reinforcement learning tasks without the need for prestructured hierarchies. The theoretical results including diversity conditions and regret bounds are significant contributions to the field of HRL and metaRL. 2. Practical Relevance: The use of intuitive examples such as the Gated FourRoom environment and the Alchemy benchmark effectively demonstrates the practical relevance of the proposed method. This connection between theory and practice is a strong aspect of the paper. 3. Comprehensive Analysis: The paper provides a thorough theoretical analysis including definitions of various prerequisites for the algorithm to work detailed steps of the algorithm and a complete view of how the hierarchy is utilized in metatest tasks for regret minimization.  Weak Points 1. Assumptions and Limitations: The reliance on certain assumptions (e.g. tabular setting specific \"diversity conditions\") might limit the generalizability of the results to more complex or continuous environments encountered in many realworld applications of RL. 2. Complexity and Accessibility: The high level of complexity and dense theoretical exposition could make the paper less accessible to a broader audience. More intuitive explanations or visualizations of key concepts could improve comprehension. 3. Empirical Validation: Despite the theoretical contributions the paper lacks empirical validation of the proposed method. Experimental results demonstrating the efficiency and effectiveness of the hierarchylearning algorithm in practical settings would strongly bolster the papers claims.  Questions and Feedback for the Author 1. Generalizability to NonTabular Environments: How do the authors envision extending the proposed method to nontabular or continuous state and action spaces What modifications or additional considerations would be necessary 2. Impact of Assumptions on RealWorld Applicability: Could the authors discuss how the assumptions made in the paper (e.g. tabular setting the structure of diversity conditions) might impact the applicability of their method in realworld RL tasks Are there potential workarounds for these limitations 3. Empirical Evidence: While the theoretical analysis is comprehensive could the authors provide insights into potential empirical experiments that could validate the proposed method Any preliminary results or observations from such experiments would be highly informative.  Review Summary This paper makes a significant theoretical contribution to the field of HRL and metaRL by proposing a novel framework for learning hierarchical structures without preconstructed hierarchies. The introduction of diversity conditions and an optimismbased algorithm for sampleefficient hierarchy recovery represents a notable advance. Additionally the theoretical results are grounded in practical relevance through illustrative examples. However the papers appeal is tempered by its complexity and limited accessibility along with a lack of empirical validation to support the theoretical claims. Moving forward addressing the generalizability of the proposed method to more complex environments and providing empirical evidence of its effectiveness would be beneficial. Overall the paper represents a promising step forward in HRL research with potential for significant impact if the highlighted concerns are addressed.", "rTAclwH46Tb": " Peer Review for \"Eigencurve: Learning Rate Schedulers for Optimal SGD Convergence on Skewed Hessian Spectrums\"  1. Summary The paper introduces \"Eigencurve\" a novel family of learning rate schedules designed for optimizing stochastic gradient descent (SGD) convergence rates on quadratic objectives. The authors argue that existing schedules such as step decay and cosine decay do not achieve optimal convergence especially when dealing with objectives where the Hessian matrixs eigenvalue distribution is skewed\u2014a common scenario in practice. Leveraging the eigenvalue distribution Eigencurve claims minimax optimal convergence rates for skewed Hessian spectrums and showcases its superiority over step decay through experiments on CIFAR10 and ImageNet datasets. Additionally the paper proposes simplified learning rate schedulers inspired by Eigencurve suggesting that these can achieve comparable or superior performance to cosine decay in certain contexts.  2. Strengths and Weaknesses  Strengths:  Novelty: The introduction of an eigenvalue distributionbased learning rate scheduler is innovative. It addresses a gap in SGD optimization by considering the eigenvalue distributions role in achieving optimality.  Theoretical Foundation: The paper provides a robust theoretical basis for the proposed method including thorough analyses and proofs that establish the optimality of Eigencurve under certain conditions.  Empirical Evidence: The experimentation on CIFAR10 and ImageNet provides solid empirical support for the theoretical claims showing Eigencurves effectiveness in practical settings.  Weaknesses:  Generalization: It remains unclear how the proposed scheduler performs across a broader range of tasks beyond quadratic objectives or image classification. The specificity to skewed Hessian spectrums may limit its applicability.  Complexity in Application: The requirement to precompute or estimate the Hessians eigenvalue distribution can be a significant hurdle for practical adoption especially in largescale models or datasets.  Comparative Analysis: While the paper includes comparisons with step and cosine decay a broader comparison with other stateoftheart learning rate schedulers could provide a clearer picture of where Eigencurve stands.  Questions and Feedback for the Authors:  Could you elaborate on the potential challenges or limitations in estimating the Hessians eigenvalue distribution for realworld largescale problems and how one might overcome these  How does Eigencurve perform with nonquadratic objectives or in tasks beyond image classification and what adjustments if any are necessary for it to be effective in these scenarios  Would it be possible to simplify the eigenvalue estimation process making Eigencurve more accessible for practical use without significant concessions on performance  3. Review Summary The paper presents an impactful contribution to the domain of SGD optimization by proposing Eigencurve a learning rate scheduler that skillfully incorporates the eigenvalue distribution of the Hessian matrix to achieve optimal convergence rates under specific conditions. The theoretical underpinnings are robust and commendably tied to empirical evidence demonstrating superior performance over commonly used schedulers in image classification tasks. However the specialized focus on skewed Hessian spectrums and the operational complexity due to the prerequisite of eigenvalue distribution estimation may limit its broad applicability and ease of adoption. Further exploration and clarification on these fronts could enhance the papers significance and practical value. Overall Eigencurve is a promising development in the optimization landscape warranting attention for its novel approach and the potential to refine and extend its application scope.", "s3V9I71JvkD": " Review  1. Summarize the paper The paper proposes a novel method SemiSupervised Meta ActorCritic (SMAC) to address the challenge of distributional shift in offline MetaReinforcement Learning (MetaRL). Traditional MetaRL enables efficient adaptation to new tasks with minimal data but requires extensive online data during metatraining making it impractical for realworld applications. Offline MetaRL attempts to solve this by training on a static dataset but struggles with a distributional shift when the policy adapted from offline data is applied to new onlinecollected data. To mitigate this SMAC utilizes offline data for metatraining and introduces a mechanism for selfsupervised online finetuning without requiring reward labels for the online data. This approach bridges the gap between the training distribution and the actual task distribution. The paper evaluates SMAC against other metaRL methods and shows significant improvements in performance on various benchmark tasks particularly in situations where generalization to new tasks is required.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The integration of unsupervised online data collection with offline metaRL to address the distributional shift is novel and addresses a significant gap in applying metaRL to realworld scenarios.  Comprehensive Evaluation: The paper presents a thorough comparison with existing methods across multiple challenging domains demonstrating the efficacy of SMAC in improving adaptation and generalization to new tasks.  Practicality: By leveraging unlabeled online data the proposed approach considerably reduces the barrier to deploying metaRL in environments where acquiring labeled data is costly or impractical. Weak Points:  Assumption of Online Data Collection: The assumption that agents can collect additional online data without reward labels might not always be practical due to environmental constraints or safety concerns.  Generalization to HighDimensional Tasks: While the paper demonstrates success in simulated robotic tasks it remains unclear how well SMAC would perform in more complex highdimensional realworld environments. Questions and Feedback:  Scalability: How does the performance of SMAC scale with the complexity and dimensionality of the task Would the same benefits be observed in environments with significantly higher state and action space dimensions  Comparison with Purely Online or Offline Methods: The paper could benefit from a more direct comparison with stateoftheart purely online and purely offline methods to better understand the tradeoffs involved with the semisupervised approach.  Impact of Online Data Quality: How sensitive is SMAC to the quality of the online collected data It would be insightful to know how variations in the exploratory behavior influence the adaptation process and overall performance.  3. Review Summary The paper presents an important contribution to the field of metareinforcement learning by introducing the SemiSupervised Meta ActorCritic (SMAC) algorithm which uniquely addresses the distributional shift issue inherent in offline metaRL through selfsupervised online finetuning. The proposed method is wellmotivated technically sound and demonstrates notable improvements in adaptation and generalization capabilities across several benchmark tasks making it a significant step towards practical metaRL applications. However the assumption that additional online data can always be affordably and safely collected may limit the applicability of SMAC in certain realworld scenarios. Future work exploring the scalability of the approach to more complex environments and its sensitivity to the quality of online data collection would be valuable. Despite these considerations the strengths and contributions of the paper outweigh its limitations and the proposed method represents a meaningful advancement in the field.", "qO-PN1zjmi_": " Peer Review of \"Ensembles with Regularized Disagreement for SemiSupervised Novelty Detection\"  1. Summary of the Paper This paper proposes a novel approach named Ensembles with Regularized Disagreement (ERD) aimed at improving the detection of outofdistribution (OOD) samples by deep neural networks particularly focusing on near OOD data which shares similarities with indistribution (ID) data but represents novel classes not seen during training. The ERD method leverages an ensemblebased procedure that only uses a mixture of unlabeled ID and OOD samples for semisupervised novelty detection (SSND). It introduces regularization to encourage diversity in the ensembles predictions on OOD samples while maintaining agreement on ID samples. This method is tested across various standard and medical image data sets demonstrating significant improvements in detecting near OOD samples with minimal computational cost increase compared to stateoftheart SSND methods.  2. Strong and Weak Points Questions and Feedback Strong Points:  Originality: The paper addresses a highly relevant and challenging problem in the field of deep learning particularly in OOD detection. The proposed ERD method introduces an innovative way to utilize unlabeled data and ensemble learning with regularization to achieve enhanced performance on near OOD detection.  Practicality: Considering the significant improvement in detecting near OOD samples without a considerable increase in computational costs the approach is not only theoretically sound but also practically viable for realworld applications especially in critical fields like medical imaging.  Comprehensive Experiments: The authors conducted extensive experiments across multiple datasets including standard image and medical image datasets thereby providing strong empirical evidence to support the effectiveness of the ERD method. Weak Points:  Assumption on OOD Samples in the Unlabeled Set: The effectiveness of the ERD method relies on the assumption that the unlabeled set used during training contains OOD samples representative of the OOD distribution during test time. This might not always be feasible especially in anomaly detection scenarios where OOD samples are rare.  General Applicability Concerns: It is unclear how the method performs when the OOD data at test time significantly differs from the OOD samples in the training set. The authors partially address this by a practical scenario (medical center example) but a more thorough exploration would be beneficial. Questions and Feedback:  Could the authors elaborate on the potential impact of the size of the unlabeled set on the performance of ERD especially in cases where such a set might be limited or highly imbalanced concerning ID and OOD samples  The paper mentions that ERD requires OOD samples in the unlabeled set to be representative of the test OOD distribution. How does the method perform under the presence of multiple distinct OOD distributions in the unlabeled set  A comparison with approaches that do not necessitate unlabeled OOD samples for training would be insightful. Are there scenarios where such methods might outperform ERD  3. Review Summary The introduction of the ERD method represents a significant contribution to the field of semisupervised novelty detection addressing the critical challenge of detecting near OOD samples without requiring extensive labeled data. The innovative use of ensembles with regularized disagreement leverages unlabeled ID and OOD samples in a way that notably enhances the detection of near OOD samples across diverse datasets offering substantial implications for domains where accurate and timely detection of such samples is crucial like medical diagnosis. Despite its merits the methods dependence on the representation of the OOD distribution in the unlabeled set and its applicability across varying OOD distributions at test time warrant further investigation. The paper could benefit from a more detailed exploration of the methods performance under diverse scenarios concerning the availability and composition of the unlabeled set as well as comparative analysis with methods reliant on different data modalities or without the need for unlabeled OOD samples. Overall the ERD method contributes a valuable tool for improving the reliability and functionality of deep neural networks in OOD detection with commendable practical relevance and theoretical backing. Future work could potentially expand on the method\u2019s versatility and applicability solidifying its standing in the field.", "yjMQuLLcGWK": " Peer Review of \"FPDETR: Fully Pretrained EncoderDecoder Transformers for Robust Object Detection\"  Summary The paper introduces FPDETR a novel approach that reformulates the training paradigm for detection transformers to leverage the benefits of largescale pretraining for enhanced robustness and generalization in object detection tasks. The crux of FPDETR lies in its ability to fully pretrain an encoderonly detection transformer which is then finetuned for object detection tasks using a task adapter that effectively bridges the gap between the upstream task (ImageNet classification) and the downstream task (object detection). The authors argue that the conventional training approaches limit the robustness and generalization capabilities of detection transformers because of the separate pretraining of the CNN backbone ignoring the transformer encoder and decoder. By treating query positional embeddings as visual prompts and introducing a task adapter for finetuning FPDETR aims to address these issues showing promising results on the COCO dataset superior robustness against common corruptions and improved generalization to smaller datasets.  Strong Points 1. Novel Approach: FPDETRs fully pretrained encoderonly architecture represents a significant shift from traditional pretraining methods which commonly neglect the transformer component of detection models. This approach is both innovative and highly relevant given the increasing importance of transformers in the computer vision domain. 2. Robustness and Generalization: The paper presents compelling evidence that FPDETR is more robust to common corruptions and generalizes better to small datasets than its counterparts. Such properties are crucial for practical applications where models encounter varied and unpredictable conditions. 3. Utilization of Visual Prompts: The adaptation of promptbased learning from NLP to object detection treating query positional embeddings as visual prompts is an intriguing concept. It opens up new avenues for research in how transformers can be adapted and finetuned for specific tasks beyond their initial pretraining.  Weak Points 1. Complexity in Adaptation: The paper could provide more details on the challenges faced when adapting the model from pretraining to finetuning phases especially considering the structural differences between tasks. 2. Comparison with StateoftheArt: While FPDETR shows superior performance in terms of robustness and generalization a more comprehensive comparison with stateoftheart methods including those outside the detection transformer domain would provide a clearer perspective on its standing. 3. Explanation of Task Adapter: The role of the task adapter in modeling the contextual relation between object query embeddings is novel but needs further elaboration. Specifically how it modifies the visual prompts for better attention targeting during finetuning could be more clearly detailed.  Questions and Feedback 1. Model Adaptation: How does FPDETR maintain its structural integrity and performance efficiency during the adaptation from pretraining to finetuning considering the significant differences between the tasks 2. Comparison Metrics: What specific metrics were used to evaluate robustness against common corruptions and generalization Including such information can help in replicating and further investigating the claims. 3. Extension to Other Tasks: Can the concept of visual prompts and the task adapter be applied to other computer vision tasks beyond object detection such as semantic segmentation or action recognition  Review Summary FPDETR presents a significant advancement in detection transformer training methodologies introducing a conceptually novel approach by fully pretraining an encoderonly transformer model. It addresses key limitations in existing models related to robustness and generalization by leveraging visual prompts and a specialized task adapter for effective finetuning on object detection tasks. Despite its promising performance further clarification on adapting the model between vastly different tasks and a more comprehensive comparison with a broader range of existing methods would strengthen the paper. Also exploring the application of its core concepts to a wider array of computer vision tasks could highlight the versatility and impact of FPDETR. Based on the strong empirical results and the innovative approach I recommend this paper for publication contingent on addressing the outlined concerns and questions.", "vQmIksuciu2": " Review of the Proposed Dynamic Pruning Architecture Utilizing Explainable AI for CNN Acceleration  1. Summary This paper presents a novel approach to dynamically prune filters in Convolutional Neural Networks (CNNs) aimed at reducing both the average and longestpath latency of inference. Unlike traditional methods which focus on static pruning or dynamic pruning based on sample difficulty this work introduces an architecture that employs early coarse prediction through a simple branch trained for topk classification and filter importance determination using explainable AI (XAI). The coarse prediction branch decides whether to make an early prediction or to narrow down the possible output classes subsequently enabling the dynamic pruning of filters relevant to these classes. The filter importance is predetermined for each class using XAI techniques. This approach allows for model deployment on hardware with minimal overhead. The paper outlines the development and evaluation of this dynamic pruning architecture detailing its components training and implementation followed by an evaluation using commonly used image classification models and datasets on CPU and GPU platforms.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty of Approach: The integration of explainable AI with early coarse prediction for dynamic filter pruning is a unique contribution addressing both the average and longestpath latency issues in existing pruning methods.  Comprehensive Evaluation: The paper provides a thorough evaluation using different datasets (CIFAR10 and CIFAR100) models (VGG11 VGG16 and ResNet20) and hardware platforms demonstrating the methods versatility and effectiveness.  Practicality for Deployment: The approachs compatibility with different hardware without significant memory overhead along with straightforward hardware deployment addresses a critical aspect of CNN implementation in realworld applications. Weak Points:  Lack of Comparison with StateoftheArt Dynamic Pruning Methods: While comparisons with static pruning methods are presented the paper does not thoroughly compare the proposed method with the latest dynamic pruning techniques which could further establish its superiority or areas for improvement.  Limited Discussion on the Impact of Filter Selection Criteria: Although the paper mentions using DeepLIFT for selecting filter importances it does not deeply explore how different XAI methods might affect the pruning results or model performance.  Scalability Concerns: The paper could benefit from a discussion on the scalability of the proposed method especially in terms of handling very deep networks or networks with a huge number of classes. Questions and Feedback: 1. Comparison With StateoftheArt: Could the authors expand on how this method compares with other contemporary dynamic pruning techniques in terms of both performance and computational efficiency 2. Impact of XAI Method Choice: How sensitive is the pruning performance to the choice of XAI method Would other methods like Integrated Gradients or SHAP provide significantly different pruning outcomes or efficiency 3. Scalability Discussion: How well does this architecture scale with network depth and complexity Are there diminishing returns in latency reduction as networks grow larger 4. Hardware Constraints Consideration: Was the method tested across a range of hardware capabilities including lowerend devices The paper mentions negligible computational overhead but it would be beneficial to understand its performance on devices with limited processing capabilities.  3. Review Summary The paper presents a significant contribution to the field of CNN acceleration through a novel dynamic pruning method leveraging explainable AI for filter importance determination and early coarse prediction. This method addresses critical challenges in reducing inference latency without substantial loss in accuracy or increased computational overhead. The evaluation demonstrates its effectiveness across various models datasets and hardware platforms affirming its potential for broad application and deployment in realworld scenarios. However the paper would greatly benefit from a more extensive comparison with current dynamic pruning methods offering a clearer perspective on its performance relative to the stateoftheart. Additionally exploration into the impact of different XAI methods on pruning decisions and further discussion on scalability issues could enhance the papers contribution. Overall the proposed architecture represents a promising direction in CNN optimization striking a balance between performance and efficiency that is crucial for deployment on hardware platforms. Further investigation into the aforementioned areas could solidify its position and utility within the deep learning community.", "nkaba3ND7B5": "Summary This paper addresses the challenge of transitioning reinforcement learning (RL) algorithms developed for episodic simulated environments to realworld applications where learning occurs in a continuous nonepisodic setting. The authors introduce Autonomous Reinforcement Learning (ARL) a framework where the agent learns autonomously without human intervention for resets between trials. They propose a benchmark EARL1 featuring diverse simulated tasks designed to highlight the challenges faced in ARL such as learning without resets. The study evaluates standard episodic RL algorithms and existing approaches in resetfree learning demonstrating their limitations in the ARL setting and underscoring the need for algorithms that focus on greater autonomy. Strong Points: 1. Innovative Framework: The introduction of the ARL framework is a novel contribution that bridges a significant gap in the reinforcement learning domain. It moves towards more realistic learning scenarios emphasizing learning continuity and autonomy. 2. Comprehensive Benchmark: EARL1 is a welldesigned benchmark that effectively captures the challenges of autonomous reinforcement learning. It includes a variety of tasks that test different aspects of ARL such as manipulation and locomotion in environments with irreversible states and limited interventions. 3. Thorough Evaluation: The comparative analysis of standard RL algorithms and existing autonomous learning approaches using the EARL benchmark provides valuable insights into their performance in nonepisodic settings. This assessment highlights the gap in current methodologies and the need for development in autonomous learning algorithms. Weak Points: 1. Lack of New Algorithms: While the paper identifies the limitations of existing RL algorithms in ARL settings it does not propose new algorithms or modifications to address these challenges. Developing or suggesting directions for creating algorithms suited for ARL would have added substantial value. 2. Limited RealWorld Applicability Discussion: Although the paper motivates the need for autonomous learning in realworld settings there is a lack of discussion on how the proposed benchmark and insights could be directly applied or adapted to actual robotic or reallife tasks. 3. Exploration Strategies not Fully Explored: The paper mentions the necessity of directed exploration for autonomous learning especially in sparse reward environments. However it doesnt delve deeply into how exploration strategies could be optimized for ARL missing an opportunity to address a critical aspect of autonomous learning. Questions and Feedback to the Author: 1. Future Directions for Algorithm Development: Could the authors suggest specific characteristics or features that future ARL algorithms should possess to overcome the identified challenges 2. RealWorld Application Scenarios: How do the authors envision the transition of ARL from simulated benchmarks to realworld applications Are there specific domains where they see ARL having immediate impact 3. Exploration in ARL: Can the authors elaborate on potential exploration strategies that would be particularly effective in ARL settings especially in environments with sparse rewards or where the agent must generate its own goals Review Summary: The paper presents a significant contribution to the field of reinforcement learning by introducing the concept of Autonomous Reinforcement Learning (ARL) and establishing a comprehensive benchmark EARL1 to evaluate the performance of RL algorithms in continuous nonepisodic settings. The need for such a framework is welljustified given the limitations of episodic learning when applied to realworld scenarios where human intervention for resets is impractical. The evaluation of existing RL approaches within this new context highlights a crucial gap in the current state of the field  the inadequacy of these approaches to handle the complexities of autonomous learning. However the paper stops short of proposing new algorithms or solutions to address these challenges which marks a significant missed opportunity. Additionally the discussion on the practical application of ARL in reallife scenarios is relatively surfacelevel and a deeper exploration into specific domains or tasks where ARL could be beneficial would have provided more concrete directions for future research. To strengthen the paper further future work should focus on developing new algorithms tailored for ARL considering the unique challenges such as the need for directed exploration in environments with sparse rewards. Also a more detailed examination of how ARL can be applied to realworld problems would enhance the papers relevance and impact. Overall this study marks an important step towards more realistic and autonomous learning systems highlighting areas for further research and development.", "per0G3dnkYh": " Review of \"Improving Tail Modeling in Normalizing Flows with Marginal TailAdaptive Flows (mTAF)\"  1. Summary This paper addresses the critical challenge of modeling the tail behavior of distributions using normalizing flows\u2014a class of deep generative models. It specifically tackles the limitations of normalizing flows in capturing the variance within the tail regions of a distribution effectively particularly for distributions with mixed tail behaviors (heavy and light tails). The authors propose a novel approach marginal tailadaptive flows (mTAF) that incorporates learnable base distributions influenced by datadriven permutations and extreme value theory estimators. Through theoretical insights and empirical analysis on synthetic data the paper demonstrates how mTAFs outperform traditional flow models in robustness efficiency and the ability to generate tail samples accurately.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Theoretical Contribution: The paper makes significant theoretical contributions by demonstrating how the marginal tailedness can be preserved and controlled in normalizing flows through judicious selection of base distributions. The introduction of marginal tailadaptive flows (mTAF) and the underlying theory are novel and potentially impactful. b. Practical Relevance: The application areas for accurately modeling heavytailed distributions are vast ranging from finance to natural language processing. Therefore the proposed method has a wide appeal and practical utility. c. Empirical Validation: The comprehensive experimental setup and the comparison of mTAF with existing models on synthetic data effectively validate the proposed methods superiority in modeling tail behavior. Weak Points: a. Complexity and Understandability: The paper is dense and technically complex which could impact its accessibility to a broader audience. A more detailed exposition or tutorial element particularly on the implementation of mTAF could enhance its understandability. b. Realworld Application: Despite robust validation on synthetic data the paper lacks empirical results on realworld datasets. This inclusion could significantly strengthen the papers contributions by demonstrating the practical effectiveness of mTAF in realworld scenarios. c. Comparison with Nonaffine Models: The paper primarily focuses on affine models and somewhat neglects the comparison and potential integration with stateoftheart nonaffine models. Questions and Feedback: 1. Extension to Realworld Data: Can the authors provide insights or preliminary results on applying mTAF to realworld datasets particularly in domains known for heavytailed phenomena 2. Integration with Nonaffine Architectures: Given the papers focus on affine models how do the authors envision integrating mTAF with more flexible nonaffine models like NSF and SOS 3. Computational Efficiency: What are the computational implications of employing mTAF especially considering the learnable base distributions and permutation schemes  3. Review Summary The paper introduces and validates a novel approach marginal tailadaptive flows (mTAF) which improves the modeling capability of normalizing flows especially in capturing the tail behavior of distributions. The theoretical development backed by empirical analysis marks a significant step forward. However the paper would benefit from addressing the complexities and broader implications of its method especially in realworld scenarios and integration with nonaffine models. Given these considerations the paper presents compelling findings but would be strengthened by deeper exploration into its applicability and comparison with a broader range of models. In conclusion the papers contributions are both novel and meaningful opening up new avenues for future research in deep generative modeling. With further development and clarification the proposed mTAF model has the potential to significantly advance the field.", "nwKXyFvaUm": " Peer Review for \"Federated Averaging with Diverse Client Selection (DivFL)\"  1) Summary of the Paper This paper introduces a novel approach to client selection in Federated Learning (FL) named Federated Averaging with Diverse Client Selection (DivFL). The core concept revolves around selecting a small diverse subset of clients based on their gradient information to approximate the aggregate updates from a larger pool of clients. This approach aims to address inefficiencies in convergence learning effectiveness and fairness observed in conventional random client selection methods. The authors propose leveraging a submodular facility location function over gradient space to maximize diversity in client selection ensuring a representative sample of updates are communicated to the server. They present thorough theoretical convergence analysis under heterogeneous data distribution settings and validate their method empirically on both synthetic and real datasets. Results show that DivFL outperforms existing client selection strategies in terms of convergence speed fairness and learning efficiency. Furthermore DivFL incorporates a communicationefficient variant that maintains these performance benefits with reduced communication overhead.  2) Strengths and Weaknesses Strong Points:  Originality and Relevance: The novel approach to client selection in FL by maximizing a submodular function to ensure diversity is both original and highly relevant to current challenges in FL efficiency and fairness.  Theoretical Foundation: The paper provides a solid theoretical basis for the proposed method including a thorough convergence analysis under practical assumptions like data heterogeneity and partial device participation.  Empirical Validation: The extensive empirical evaluation on both synthetic and real datasets showcases the effectiveness of DivFL across multiple metrics significantly contributing to the papers credibility.  Communication Efficiency: The development of a communicationefficient version of DivFL addresses practical concerns in FL implementations making the proposed method relevant for realworld applications. Weak Points:  Complexity of Implementation: The paper does not fully address the potential complexities or computational overheads associated with implementing the proposed diverse client selection mechanism especially in largescale FL systems.  Assumptions Limitations: While the paper makes practical assumptions for convergence analysis there is limited discussion on how deviations from these assumptions in realworld settings might affect the performance of DivFL.  Analysis of Fairness Impact: Although the paper reports improvements in fairness a deeper exploration or quantitative analysis of how diverse client selection directly impacts various fairness metrics would strengthen the conclusions. Questions and Feedback to the Author: 1. Can the authors provide more insights into the computational overhead of implementing the diverse client selection in largescale FL systems with thousands of clients 2. How sensitive is DivFLs performance to deviations from the theoretical assumptions used in the convergence analysis especially in highly heterogeneous realworld data distributions 3. It would be beneficial if the authors could elaborate more on the mechanisms of fairness improvement observed with DivFL. Specifically how does the diverse subset selection contribute to uniform performance across clients and are there scenarios where this improvement might not hold  3) Review Summary The paper \"Federated Averaging with Diverse Client Selection (DivFL)\" presents a significant advancement in the field of federated learning by introducing a method that efficiently selects a diverse subset of clients to improve convergence fairness and learning efficiency. The theoretical underpinnings extensive empirical validation and consideration for communication efficiency are commendable aspects of this work. However the paper could benefit from a more detailed discussion on the practical implementation aspects the impact of assumption deviations and a deeper analysis of fairness improvements. Despite these areas for further exploration the contributions made by this paper are substantial and promising for the advancement of federated learning methodologies. The innovative approach to client selection posited by DivFL has the potential to significantly enhance the practicality and effectiveness of FL across various applications.", "pWBNOgdeURp": " Peer Review of the Paper on Koopman Operator TheoryBased Pruning  Summary of the Paper The paper proposes a novel approach to deep neural network (DNN) pruning by leveraging recent advances in dynamical systems theory particularly Koopman operator theory. The authors introduce a new class of pruning algorithms theoretically motivated by the Koopman operator framework aiming to unify and provide deeper insights into existing pruning methods including magnitudebased and gradientbased pruning. The paper begins by addressing the importance and challenges of DNN pruning motivated by the practical necessity to reduce computational resources and the theoretical interest in understanding sparse network structures through the lens of the Lottery Ticket Hypothesis (LTH). The authors provide a comprehensive background on DNN pruning the Koopman operator theory and its application to dynamical systems. By applying the Koopman mode decomposition to the parameter dynamics of DNNs the study introduces two pruning methods: Koopman Magnitude Pruning (KMP) and Koopman Gradient Pruning (KGP). Experiments demonstrate the efficacy of these methods in achieving comparable or superior performance to existing pruning techniques particularly highlighting circumstances where KMP and KGP either align with or surpass conventional magnitude and gradient pruning.  Strong Points 1. Theoretical Novelty: The paper bridges a significant gap between DNN pruning and dynamical systems theory introducing the Koopman operator theory into the realm of neural network pruning in an innovative way. 2. Comprehensive Background: It provides an extensive background that not only contextualizes the problem within current research but also makes the concept of Koopman operator theory accessible to readers unfamiliar with dynamical systems theory. 3. Empirical Validation: The conducted experiments using MnistNet and ResNet20 demonstrate the practical effectiveness of the proposed Koopmanbased pruning algorithms underscoring their potential as alternatives to traditional pruning methods. 4. Unification of Pruning Methods: By showing the equivalence between KMP KGP and existing pruning methods under certain training regimes the paper offers a unifying framework that could streamline future research efforts in the field.  Weak Points and Questions to the Authors 1. Generalizability Concerns: The experiments focus on two particular DNN architectures (MnistNet and ResNet20) and datasets (MNIST and CIFAR10). How do the proposed pruning methods perform across a wider range of architectures and dataset complexities 2. Computational Efficiency: The manuscript touches on the computational demands of performing Koopman mode decomposition but doesnt comprehensively compare this aspect to traditional pruning methods. Can the authors elaborate on the scalability and computational efficiency of the Koopmanbased pruning algorithms 3. Insight on Pruning Time: The discussion concludes that timing in the \"static\" to \"active\" transition for pruning could be crucial. Could the authors provide more specific guidelines or insights on optimizing the timing of applying KMP or KGP to maximize pruning efficiency or performance 4. Robustness and Sensitivity: How sensitive are the proposed methods to initialization and hyperparameter settings especially in comparison to the robustness exhibited by magnitudebased pruning  Review Summary This paper presents a compelling intersection between dynamical systems theory and neural network pruning offering fresh theoretical insights and practical pruning methods. The introduction of Koopman operator theory to this realm is both innovative and theoretically appealing providing a novel lens through which to view and unify existing pruning strategies. The empirical results support the potential of Koopmanbased pruning algorithms to match or exceed the performance of traditional methods highlighting their relevance to both the academic and practical facets of the field. However the paper would benefit from addressing specific concerns regarding the generalizability of the proposed methods their computational demands and the practical guidelines for their application. Addressing these points would significantly strengthen the papers contribution making the proposed Koopmanbased pruning algorithms more applicable and appealing to a broader audience within the machine learning and neural network optimization communities. Given the strength of the theoretical contributions the novelty of applying Koopman operator theory to DNN pruning and the promising initial results this paper represents a valuable contribution to the field. With further clarification and expansion on the raised points it has the potential to spearhead a new direction in neural network pruning research merging theoretical insights from dynamical systems theory with practical algorithm development and implementation.", "zRb7IWkTZAU": " Peer Review of \"Zeroshot Reward Learning through Semantic Grounding of Language Goals in Vision Models\"  Summary of the Paper The paper introduces a novel approach to reinforcement learning (RL) by integrating grounded natural language descriptions with spatial relationship rules to instruct and reward an agent using raw pixels for input. Traditional RL relies heavily on explicitly defined reward structures or requires laborious collection of demonstrations or goal images. This work circumvents these requirements by employing CLIP a pretrained visionlanguage model to interpret goal states from textual descriptions in combination with spatial relationship heuristics to generate a zeroshot reward signal. The authors successfully distilled policies from training on various tasks into a unified languageconditioned policy capable of zeroshot generalization to new unseen tasks in robotic manipulation settings.  Strong Points 1. Innovative Approach: The combination of CLIP for semantic grounding and spatial relationship rules for defining goal states presents a cuttingedge method. It elegantly solves the problem of generating rewards from raw pixels without the need for state access or extensive data collection. 2. Zeroshot Generalization: The ability to distill learned tasks into a single policy that can then be applied to new tasks without further training is impressive. This demonstrates a significant advancement in task generalization within RL. 3. Robust Testing: The paper provides thorough experimental results showing the effectiveness of their approach across different tasks and comparing it against baselines and existing methods. The use of manipulation tasks in different robotic simulation environments lends credence to the practical applicability of the method.  Weak Points 1. Lack of Detail on Spatial Relationship Rules: While the use of spatial relationship heuristics is a cornerstone of this method the paper could benefit from a more indepth explanation or examples of these rules. It is unclear how versatile these rules are across diverse tasks. 2. Limited Task Domains: The presented work focuses on robotic manipulation tasks. While the results are promising expanding the experiments to include other domains or more complex tasks would further validate the approach\u2019s versatility and efficacy. 3. Ethical Considerations on Bias: The paper mentions potential biases in largescale vision and language models but does not delve deeply into how these biases are addressed or mitigated in their system.  Questions and Feedback to the Author 1. Spatial Relationship Rules: Can the authors provide more examples or a detailed explanation of how spatial relationship heuristics are designed and their limitations 2. Expansion to Other Domains: Are there plans to test this method in domains outside robotic manipulation or to tackle more complex tasks that could challenge the methods current capabilities 3. Bias Mitigation: Could the authors elaborate on specific strategies or techniques employed to assess and mitigate biases inherited from the CLIP model  Review Summary The paper presents a compelling and innovative methodology for reward learning in reinforcement learning tasks by leveraging natural language descriptions and spatial relationship cues. This approach not only reduces the effort needed to define rewards or collect demonstration data but also showcases impressive generalization capabilities to novel tasks. Despite the strengths more detailed discussion on the design and limitations of spatial relationship heuristics broader applicability beyond robotic manipulation tasks and clearer strategies for mitigating inherited biases would strengthen the paper. Nonetheless the methodology offers a promising direction for future research in RL particularly in environments where explicit reward signals are hard to define or collect. The experimental results support the papers claims making a strong case for the potential of languagevision grounding in developing flexible and generalizable RL systems.", "wkMG8cdvh7-": " PeerReview of \"Understanding and Enhancing the Robustness of Graph Neural Networks against Graph Injection Attacks\"  1) Summary of the Paper The paper focuses on the exploration of Graph Injection Attack (GIA) strategies on Graph Neural Networks (GNNs) compared to Graph Modification Attack (GMA). It has been established theoretically and empirically that GIA poses a larger threat to GNNs due to its high flexibility in attack strategies. However this flexibility results in significant damage to the homophily distribution of the graph making these attacks detectable and mitigable by homophilybased defenses. To address this limitation the authors introduce the concept of homophily unnoticeability and present a novel objective called Harmonious Adversarial Objective (HAO) that enables GIA to perform attacks without altering the homophily distribution significantly. Through extensive experimentation across various datasets and defense models they demonstrate that GIA with HAO can effectively bypass homophilybased defenses and significantly outperform existing GIA methods.  2) Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Foundation: The paper solidifies its arguments with theoretical findings (Theorems 13) substantiating the potential harm GIA can cause over GMA and how homophily damage can be mitigated by HAO.  Novelty and Practicality: Introducing the concept of homophily unnoticeability and the Harmonious Adversarial Objective (HAO) represents a significant contribution to adversarial attack strategies on GNNs with practical implications for testing the robustness of GNNs against more sophisticated attacks.  Extensive Experimentation: The empirical verification across multiple datasets and against 38 defense models underscores the papers comprehensiveness in evaluating the proposed methods. Weak Points:  Lack of InDepth Analysis on Homophily Metrics: While the paper successfully introduces a novel way to maintain homophily distributions during attacks it does not delve deeply into how different homophily metrics might affect the optimization process and outcome of HAO.  Assumptions on Attack Scenarios: The paper primarily focuses on specific attack models (ejection and inductive settings with blackbox access). It would be beneficial to discuss the effectiveness and limitations of HAO under different adversarial settings or assumptions such as whitebox attacks. Questions and Feedback:  Impact of Network Structure: Could the effectiveness of HAO vary significantly based on the underlying structure or type of networks (e.g. social vs. biological networks) If so how could HAO be adapted for different network characteristics  Comparison with Other Adversarial Objectives: How does HAO compare with other adversarial objectives that might not focus on homophily but could potentially enforce similar unnoticeability constraints  Potential Defenses against HAO: Are there any insights into how future defenses could evolve to detect or mitigate the effects of GIA with HAO considering its design to bypass current homophilybased defenses  3) Review Summary This paper provides a compelling investigation into the comparative effectiveness and pitfalls of Graph Injection Attacks (GIA) against Graph Neural Networks (GNNs) further developing a novel approach Harmonious Adversarial Objective (HAO) to circumvent homophilybased defenses. The theoretical and empirical rigor presented alongside innovative methodologies represents a significant leap in the understanding and application of adversarial attacks on GNNs. Despite its comprehensive coverage the study leaves room for further exploration particularly in the realm of how varying network structures might interact with HAO and potential avenues for future defensive measures against such advanced attack tactics. Given the substantial novelty theoretical backing and empirical validations provided the paper is a valuable addition to the domain of adversarial machine learning research specifically targeting the robustness of GNNs. Future investigations should aim to broaden the applicability and understanding of the impact of HAO in diversified adversarial contexts and against emerging defensive mechanisms.", "uwnOHjgUrTa": " Peer Review on \"DNN Quantization with Attention (DQA): A Training Methodology for Lowbit Quantization of Deep Neural Networks\"  1. Summary This paper introduces a novel training methodology named DNN Quantization with Attention (DQA) to improve the performance of lowbit quantized Deep Neural Networks (DNNs). DQA is fundamentally an attention mechanismbased approach that dynamically varies the bitwidth of quantization during the training process starting from a mix of high medium and lowbit quantizations and focusing on a lowbit precision towards the end. This approach is designed to mitigate the accuracy degradation often associated with lowbit quantization methods by smoothing the loss function and aiding the convergence process. The paper evaluates DQA across several computer vision tasks using popular datasets such as CIFAR10 CIFAR100 and ImageNet ILSVRC 2012 employing different quantization methods like minmax StatisticsAware Weight Binning (SAWB) and Binary Weight Network (BWN). The experiments demonstrate that DQA can achieve superior accuracy compared to existing stateoftheart quantization techniques while maintaining the same computational complexity and memory footprint.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The proposed DQA methodology is a fresh take on the problem of quantization in DNNs utilizing an attentionbased mechanism to dynamically adjust quantization levels. This is a novel approach that has not been extensively explored in the literature.  Comprehensive Experiments: The paper contains a thorough experimental evaluation across various datasets and network architectures providing a robust validation of the proposed approach.  Clear Improvement Over Baselines: The results demonstrate clear improvements in accuracy over both the fullprecision networks and previous quantization methods particularly in lightweight DNN architectures where compression is more challenging. Weak Points:  Lack of Theoretical Justification: The paper largely focuses on empirical results with limited theoretical analysis on why the attention mechanism leads to better convergences and performance in quantized networks.  Complexity of Implementation: The implementation of DQA which involves attention mechanisms and temperaturecontrolled softmax functions might be viewed as more complex than traditional quantization methods potentially limiting its adoption.  Limited Evaluation of Training Time: Although accuracy and performance are wellstudied there is limited discussion on the training time or computational overhead introduced by the DQA method compared to other quantization methods. Questions and Feedback:  Can the authors provide more insight into the theoretical underpinning of why the attention mechanism works well for quantization  How does the training time of DQA compare to conventional quantization methods especially given the added complexity  The paper mentions that DQA can be used with any existing quantizer. Have the authors considered evaluating the method with quantizers not mentioned in this work such as PACTSAWB or even more recent quantization approaches  3. Review Summary The paper \"DNN Quantization with Attention (DQA): A Training Methodology for Lowbit Quantization of Deep Neural Networks\" presents a compelling approach to address the accuracy degradation problem associated with DNN quantization. By leveraging an attention mechanism that dynamically shifts the focus between different bitwidth quantizations during training DQA achieves significant accuracy improvements across several benchmarks. The thorough experimental evaluation showcases DQAs compatibility with various stateoftheart quantization methods and its effectiveness in improving performance particularly on challenging datasets and lightweight DNN architectures. However the paper could benefit from deeper theoretical analysis explaining why the attention mechanism facilitates better convergence in quantized networks. Additionally understanding the tradeoffs in terms of computational overhead or increased training time would provide a more comprehensive view of the DQA methodologys practicality. In conclusion DQA represents a significant step forward in the quantization domain demonstrating the potential of attention mechanisms to enhance lowbit quantized DNN training. With further exploration and refinement DQA could pave the way for more efficient and accurate deployment of DNNs in resourceconstrained environments.", "z1-I6rOKv1S": " Review of \"Autoregressive Quantile Flows for Flexible Probabilistic Modeling\"  Summary The paper introduces Autoregressive Quantile Flows (AQFs) an innovative approach to probabilistic modeling that leverages a novel learning objective based on proper scoring rules. This technique addresses the computational challenges involved with calculating determinants of Jacobians in traditional normalizing flow models. By avoiding these computationally expensive operations the authors propose a method that is both versatile and efficient supporting new neural architectures for fast training and sampling. The paper demonstrates the utility of AQFs in various domains including time series forecasting and object detection showcasing improved probabilistic predictions. Furthermore it explores the benefits of autoregressive quantile flow regression for predictive uncertainty estimation offering a significant advancement over maximum likelihood estimation methodologies.  Strong Points 1. Innovative Objective Function: The introduction of a learning objective based on proper scoring rules is a notable advancement. It elegantly circumvents the need for computing Jacobian determinants facilitating the use of a broader array of neural architectures in flow models. 2. Versatile Application: The paper successfully applies AQFs to multiple domains ranging from time series forecasting to object detection demonstrating the models versatility and effectiveness in capturing complex probability distributions. 3. Improved Uncertainty Estimation: By directly parameterizing predictive distributions with flows AQFs noticeably enhance uncertainty estimates in probabilistic models. This is a significant contribution to fields requiring accurate uncertainty quantification such as financial forecasting and medical diagnosis. 4. Empirical Validation: The comprehensive experimental evaluation across different datasets and comparisons with existing methods strengthens the papers claims. The reported performance metrics conclusively showcase the advantages of AQFs over traditional models.  Weak Points 1. Lack of Theoretical Analysis: While the empirical results are compelling the paper lacks a thorough theoretical analysis of the properties of AQFs. An exploration of the theoretical underpinnings could provide deeper insights into the conditions under which AQFs perform optimally. 2. Complexity and Scalability: The paper does not sufficiently address the computational complexity and scalability of the proposed models especially when applied to extremely highdimensional data. A discussion on these aspects would be helpful for practitioners evaluating the feasibility of AQFs for largescale applications. 3. Comparison with StateoftheArt: Although the paper presents comparisons with traditional methods it does not benchmark AQFs against the latest advancements in probabilistic modeling such as recent variants of generative adversarial networks (GANs) or variational autoencoders (VAEs). Such a comparison could better position AQFs within the current research landscape.  Questions and Feedback 1. Can the authors provide a theoretical analysis or justification for the choice of proper scoring rules as the objective function Specifically how do these rules compare to alternative objectives in terms of model performance and computational efficiency 2. How do AQFs perform on extremely highdimensional datasets and what is the computational complexity of training these models in such scenarios Are there any strategies to mitigate potential scalability issues 3. The paper could benefit from a direct comparison with the latest stateoftheart probabilistic models. Have the authors considered evaluating AQFs against advanced GANs or VAEs Such a comparison could elucidate the relative strengths of AQFs.  Review Summary The paper presents a significant contribution to the field of probabilistic modeling with the introduction of Autoregressive Quantile Flows. By leveraging a novel learning objective and enabling the use of new neural architectures the authors offer a method that not only simplifies implementation but also enhances model performance across various applications. Despite its strengths the paper would benefit from a deeper theoretical analysis further discussion on computational complexity and direct comparisons with the latest probabilistic models. Overall given the innovative approach comprehensive experimentation and demonstrated improvements over existing methods I recommend the publication of this paper. Future work that addresses the mentioned weaknesses could further solidify AQFs position as a valuable tool for probabilistic modeling in diverse application domains.", "zXM0b4hi5_B": " Summary The paper investigates the intricate relationship between the probability distribution of data perceptual distances and unsupervised learning with a focus on the human visual systems processing of images and its implications for machine learning models especially autoencoders. It presents evidence that perceptual distances which are highly predictive of human psychophysical responses correlate with the likelihood of images based on a recent probabilistic model. This link suggests that modeling perception in unsupervised learning might lead to representations that are more aligned with the way humans perceive visual information. The paper explores how distances measured by autoencoders trained on natural images correlate not just with the training datas probability distribution but also with human perception challenging the conventional use of Euclidean distance for these models. Surprisingly the incorporation of perceptual distances in the optimization of autoencoders often results in models that prioritize highdensity data regions due to a \"double counting\" effect which could explain the marginal performance gains in some applications. However in scenarios with limited data perceptual distances can act as valuable regularizers.  Strong Points 1. The connection between the behavior of the human visual system probabilistic modeling and machine learning representations is compelling and represents a significant interdisciplinary integration. 2. Comprehensive experimental setup including the use of preexisting models and datasets (e.g. PixelCNN TID 2013) to explore the papers hypotheses offers solid evidence for the claims made. 3. The novel concept of a \"double counting effect\" presents an insightful explanation for the observed phenomena regarding the limited advantages of perceptual distances in certain contexts. 4. The ramifications of the findings for unsupervised learning particularly in the case of data scarcity or absence open up new avenues for using perceptual distances as an implicit model of data distribution.  Weak Points 1. The paper focuses heavily on autoencoders and image processing tasks which might limit its applicability to broader machine learning or other types of data. 2. While the experiments are thorough there is a lack of discussion on the potential impact of these findings beyond image processing such as audio processing or natural language understanding. 3. The discussion on the double counting effects implications is insightful but could benefit from a deeper exploration particularly regarding how this effect might influence training dynamics and model generalization beyond autoencoders.  Questions and Feedback 1. Could the authors discuss if and how their findings could be extended or applied to other domains beyond image processing such as audio or text data 2. Have the authors considered the potential for their findings to inform the development of new perceptual distance measures that mitigate the double counting effect 3. It would be interesting to see a more detailed discussion on how these findings impact our understanding of the human visual systems efficiency in processing natural images and what implications this might have for computational models of vision.  Review Summary This paper offers significant insights into the relationship between the statistical properties of data perceptual similarities and unsupervised learning presenting a thorough examination of how humanlike perceptual distances can influence machine learning models. The discovery of the double counting effect reveals a nuanced understanding of why perceptual distances may not always outperform Euclidean distances in machine learning tasks despite their closer alignment with human perception. Furthermore the implications of this for scenarios with sparse or nonrepresentative data provide a valuable perspective on the potential of these distances to act as regularizers. However the papers focus is relatively narrow primarily centered on image processing and autoencoders. Expanding the discussion to other domains and considering the broader implications of the findings could make the contributions even more impactful. Nonetheless this work significantly advances our understanding of the intersection between human perception and machine learning posing intriguing questions for future research.", "qXa0nhTRZGV": " Peer Review  Summarize the paper This paper explores SharpnessAware Minimization (SAM) a training method that emphasizes worstcase weight perturbations to significantly enhance model generalization across different settings. The study delves deeply into the operational mechanics of SAM particularly its implicit bias over diagonal linear networks and its efficacy under nonconvex objectives. The researchers demonstrate through theoretical analysis and empirical evidence that SAM tends to navigate towards solutions that showcase superior generalization capabilities compared to standard gradient descent methods. Additionally the investigation extends to settings marred by noisy labels asserting that SAM not only mitigates overfitting but when coupled with a robust loss framework further elevates model performance. Furthermore the paper draws an interesting parallel between the challenges of noisy label overfitting and adversarial training suggesting a broader applicability of noise label literature techniques to bolster robust generalization.  Strong Points 1. Thorough exploration of SAM\u2019s Implicit Bias: The paper offers a substantial theoretical framework that unveils the implicit bias of SAM particularly accentuating its inclination towards solutions with better generalization properties. This nuanced understanding provides a more solid rationale behind the success of SAM in various settings. 2. Comprehensive Empirical Validation: A strong empirical component accompanies the theoretical insights with experiments demonstrating SAMs effectiveness across different networks and settings. This blend of theory and practice significantly enhances the papers credibility. 3. Novel Insights into Noisy Label Overfitting: The papers exploration of SAM in noisy label scenarios and its ability to substantially reduce overfitting through gradient reweighting and integration with robust loss frameworks offers new pathways for dealing with label noise issues.  Weak Points 1. Limited Discussion on Computational Efficiency: While the paper extensively covers the theoretical and empirical strengths of SAM theres a relatively sparse discussion on the computational costs or efficiency considerations associated with implementing SAM especially in largescale applications. 2. Ambiguity on Practical Implementation Details: The paper sometimes glosses over specific practical implementation details for SAM particularly how the worstcase perturbations are identified and applied in realworld scenarios which could affect reproducibility and applicability. 3. Potential for Bias under Different Data Conditions: While the paper argues for the general applicability of SAM across various settings theres insufficient exploration of how SAM performs under highly imbalanced data conditions or with extreme outlier presence which could bias or limit its applicability.  Questions and Feedback to the author 1. Computational Cost: Could the authors elaborate on the computational costs associated with SAM especially in comparison to standard training methods and suggest ways to mitigate any increased costs 2. Implementation Details: Can more detailed guidance be provided on implementing SAM in practical settings including choices around hyperparameters and addressing potential challenges 3. Performance under Extreme Conditions: How does SAM perform under extreme data conditions such as with highly imbalanced datasets or significant outliers Can its robustness and generalization capabilities be maintained in such scenarios  Review Summary The paper provides a comprehensive analysis of SharpnessAware Minimization (SAM) offering valuable theoretical insights into its implicit bias and demonstrating its effectiveness across various settings through empirical evidence. The exploration of SAMs application in noisy label scenarios and its potential to mitigate overfitting is particularly noteworthy promising new directions for future research. Despite its strengths the paper could benefit from a more detailed discussion on the computational implications of implementing SAM and clearer guidance on practical implementation details. Additionally further exploration into SAMs performance under varying and potentially extreme data conditions would enhance the robustness of the findings and their applicability. Overall the paper makes a significant contribution to the understanding of SAM and its potential to improve model generalization warranting attention from the machine learning research community. However addressing the outlined weaknesses and considerations could further solidify the papers impact and application in broader contexts.", "jgAl403zfau": " Review of \"HardwareAware Latency Pruning (HALP) for Efficient Network Simplification\"  Summary The paper introduces HardwareAware Latency Pruning (HALP) an innovative approach towards structural pruning in neural networks that prioritizes latency reduction while balancing the accuracy tradeoff. The methodology is formulated as a global resource allocation optimization problem focusing on maximizing accuracy under a predefined latency budget. This is achieved by creating a latency lookup table for estimating the potential latency reduction and employing a global saliency score to gauge the likely impact on accuracy from pruning. These metrics are integrated into an augmented knapsack solver that guides the pruning process making HALP both efficient and effective. The study demonstrates the algorithms superiority over existing methods by testing it on varied architectures for tasks such as image classification and object detection using the ImageNet and VOC datasets respectively.  Strengths 1. Innovative Approach: The proposal of structuring the pruning process as a resource allocation optimization problem is novel and enables a more intuitive understanding of the tradeoffs involved in network simplification. 2. HardwareAwareness: Capturing the hardwareimposed latency characteristics via a lookup table and incorporating these into the pruning decisions is a significant stride towards making neural network deployment more practical and efficient in realworld scenarios. 3. Efficiency and Effectiveness: The empirical results particularly the improvements in throughput for ResNet and SSD architectures with minimal or acceptable loss in accuracy underscore the methods practical efficacy. The augmented knapsack approach for optimizing the pruning process further contributes to HALPs efficiency. 4. Comprehensive Evaluation: The thorough evaluation across different architectures and tasks and the comparison with stateoftheart methods provide a strong empirical foundation for the claims of superiority made by the authors.  Weaknesses 1. Generalizability Concerns: While the paper outlines HALPs effectiveness on the tested architectures and tasks there is limited discussion on its performance across a broader spectrum of neural network architectures and application domains. 2. Latency Lookup Table Overheads: The method relies on precomputed latency lookup tables specific to the target hardware which might not be readily available or easily generated for all potential deployment scenarios. The added step of generating these tables could be seen as a barrier to easy adoption. 3. Lack of Theoretical Insight: The paper leans heavily on empirical evidence and while it is convincing there seems to be a lack of theoretical backing or insight into why the method works particularly in comparison to others. A deeper theoretical analysis or discussion could have added value. 4. Implementation Details and Reproducibility: While the paper promises code release upon acceptance the current lack of access to implementation details may hinder immediate reproducibility and independent verification of the results by peers.  Questions and Feedback to the Author 1. Can the authors provide more insight into how the method might perform or be adjusted for newer potentially more complex network architectures that were not tested in the study 2. The reliance on latency lookup tables is a critical component of your method. Could the authors discuss the practicality of generating these tables across different platforms and the possible overheads involved 3. Could the authors elaborate on the theoretical underpinnings of the HALP approach particularly how it differs from or improves upon other optimization methods used in network pruning  Review Summary The paper presents a compelling and innovative approach to structured network pruning that is both practical and effective as demonstrated through comprehensive evaluations. The hardwareaware aspect of HALP through latency consideration represents a significant advancement in making neural network deployment more efficient. However there are areas where the paper could improve such as exploring the methods generalizability across a wider range of architectures and tasks providing more theoretical insight into the algorithms efficacy and addressing the potential overheads associated with generating and utilizing hardwarespecific latency lookup tables. Despite these concerns the strengths of the paper particularly its novel approach and demonstrated efficacy across several benchmarks support a strong recommendation for its acceptance contingent upon addressing the mentioned weaknesses and considerations.", "gKprVaCyQmA": " PeerReview  1) Summarize the paper The paper introduces the \u201cThere are Free Lunches\u201d (TAFL) Theorem as a counterpoint to the wellknown NoFreeLunch (NFL) Theorems which claim that all optimization algorithms perform equally across all possible problems. The TAFL Theorem posits that certain algorithms can optimally solve all tasks given that these tasks are presented in a specific order and it further suggests that the difficulty of new tasks decreases as the number of previously solved tasks increases. The paper delineates this theorem with a formal presentation followed by an analysis of task order and its implications for reducing the computational load of learning algorithms. A practical example illustrating the theorem\u2019s application in supervised learning specifically in 2dimensional binary classification tasks is provided. The authors conclude by discussing broader implications of TAFL for AI research emphasizing its potential to streamline the process of solving new tasks by utilizing models from previously solved tasks.  2) Strong and Weak Points Questions and Feedback to the Author Strong Points:  Novelty: The TAFL Theorem presents a novel perspective against the backdrop of the NFL Theorems suggesting an innovative approach to problemsolving in AI through task sequencing and leveraging past solutions.  Practical Implications: The paper does well to outline the practical implications of TAFL particularly in how previous tasks\u2019 solutions can accelerate solving new tasks which is a significant contribution to the field of AI and machine learning.  Clarity of Presentation: The paper provides a clear and structured presentation of the TAFL Theorem including necessary preliminary information detailed explanation of the theorem and a practical example that aids comprehension. Weak Points:  Empirical Validation: While the paper presents a theoretical framework and a preliminary example the lack of comprehensive empirical validation or comparison against existing algorithms limits the papers persuasive power.  Assumption on Task Order: The paper assumes a specific order of tasks is known a priori which might not always be practical or possible in realworld applications. This assumption needs further scrutiny or exploration of how such an order could be determined.  Scalability and Generalization: The discussion on the applicability of TAFL across different AI domains (beyond supervised learning) is limited. Further exploration on scalability and how TAFL could be generalized across domains would strengthen the paper. Questions:  How does the proposed theorem account for or adapt to scenarios where the task order cannot be precisely determined in advance  Can the authors provide more insight into how the TAFL Theorem compares with other approaches in machine learning such as transfer learning or metalearning  What are the computational implications of continuously increasing the set of models from solved tasks in terms of storage and retrieval Feedback: The paper presents an intriguing concept that challenges existing beliefs in optimization and problemsolving within AI research. To further strengthen the argument it would be beneficial to include more empirical evidence supporting the theorems efficacy compared to traditional methods. Additionally exploring the mechanisms for determining task order and addressing potential scalability issues would provide a more rounded argument and contribute significantly to the papers impact.  3) Review Summary The paper introduces the TAFL Theorem as a stark contrast to the NFL Theorems proposing that with a specific order of tasks some algorithms can optimally solve all tasks and that solving new tasks becomes less challenging with an increasing count of previously solved tasks. This concept is both revolutionary and promising suggesting a new path towards achieving more efficient AI problemsolving strategies. However the paper falls short in empirical evidence practical feasibility regarding task order determination and discussions on scalability and generalization across AI domains. Addressing these aspects through comprehensive empirical studies exploring algorithms or methodologies for determining task order and widening the discussion on applicability across different AI domains would make a more compelling argument for the TAFL Theorem. In its current state while the TAFL Theorem is an exciting and novel concept the paper requires further development to fully establish its practicality and superiority over existing methods. The inclusion of more robust empirical data and a detailed exploration of the theorems implications in realworld settings would significantly enhance the papers contributions to the field of AI and machine learning.", "lzupY5zjaU9": " Paper Summary This paper introduces COMPRESS a novel metaprocedure designed to enhance the efficiency of thinning algorithms used in distribution compression. The primary aim is to reduce the computational complexity of these algorithms without significantly compromising on the error margin. Thinning algorithms like kernel thinning (KT) are essential for summarizing a probability distribution P by a small set of representative points crucial in various machine learning and statistical inference tasks. COMPRESS is innovative in that it can accelerate any given thinning algorithm as demonstrated by its application to quadratictime algorithms such as kernel halving and kernel thinning improving their runtime from quadratic or superquadratic to nearlinear O(n log n) with modest space requirements and without significantly degrading accuracy. The paper thoroughly evaluates COMPRESS through empirical benchmarks including highdimensional Monte Carlo samples and Markov chains for differential equation posteriors showing that COMPRESS either matches or closely approximates the accuracy of the input algorithms within a significantly reduced computation time.  Strong Points 1. Novelty and Practicality: COMPRESS addresses a critical bottleneck in the application of thinning algorithms by significantly reducing their computational requirements making it practically feasible for large datasets and complex distribution summarization tasks. 2. Rigorous Theoretical Foundation: The paper provides a comprehensive theoretical analysis of COMPRESS including runtime and error guarantees. The integration of these guaranties ensures that COMPRESS not only accelerates thinning algorithms but also maintains their integrity within a welldefined error margin. 3. Empirical Validation: The extensive experiments conducted offer solid evidence of COMPRESSs efficiency and accuracy highlighting its potential advantages over traditional thinning methods especially in highdimensional applications and critically expensive tasks such as computational cardiology.  Weak Points and Feedback 1. Error Margin Discussion: The paper mentions that COMPRESS incurs at most a factor of 4 in error. It would be beneficial to have a more detailed discussion on the practical implications of this error inflation especially in sensitive applications where precision is paramount. Question: Can the authors provide more insights into how the factor of 4 error impacts the practical utility of COMPRESS in realworld applications 2. Comparison with Contemporary Techniques: While the comparisons made are comprehensive including a performance evaluation against the original KT and kernel halving algorithms a broader comparison with other contemporary distribution compression or summarization techniques could provide a clearer understanding of COMPRESS\u2019s positioning in the current research landscape. Feedback: A comparative analysis involving a wider range of algorithms including those that may not primarily focus on thinning but offer compression capabilities would enrich the papers discussion on the practical superiority and novelty of COMPRESS. 3. Scalability and Dimensionality: The paper successfully demonstrates COMPRESS\u2019s efficacy across a range of scenarios. However more insights into its scalability concerning higher dimensionality and even larger datasets could be instrumental for readers to gauge its limits. Feedback: It would be valuable to include experiments or theoretical analysis on the impact of increasing dimensionality and sample size on COMPRESSs performance and error characteristics.  Review Summary COMPRESS presents a significant advancement in the efficiency of thinning algorithms for distribution compression offering substantial reductions in computational time with only modest increases in error. The theoretical rigor combined with comprehensive empirical evaluations validates its effectiveness and practicality. However a deeper analysis concerning the error inflation broader comparative evaluations with existing methods and implications of scalability on performance could further strengthen the papers contributions. Overall COMPRESS represents an innovative and valuable tool for researchers and practitioners dealing with highdimensional data summarization tasks but with room for additional clarity and exploration in certain areas.", "on54StZqGQ_": " Review of \"Degradation Attacks Against Certifiably Robust Neural Networks\"  Summary This paper investigates a relevant and underexplored aspect of adversarial robustness in neural networks focusing on certifiably robust classifiers. The researchers identify and demonstrate that existing certified runtime defenses are excessively cautious which inadvertently leads to the unnecessary rejection of nonadversarial inputs. Through a series of thoughtfully designed experiments and novel attack methodologies labeled as degradation attacks the authors empirically show that these overcautious defenses significantly diminish the utility of stateoftheart models like GloRo Nets and randomized smoothed models by forcing them to erroneously reject a large portion of valid inputs. The paper is wellstructured thoroughly presenting the limitation of current certified runtime defenses offering a new perspective on attacking certifiably robust neural classifiers not by misclassification but through utility reduction.  Strengths 1. Novelty and Relevance: The identification of a new kind of attack (degradation attack) against neural networks that does not aim to misclassify but reduce the utility of the model is both novel and relevant especially given the increasing reliance on machine learning models in critical applications. 2. Experimental Rigor: The authors present a comprehensive experimental setup that convincingly demonstrates the vulnerability of certifiably robust classifiers to degradation attacks. The empirical evidence supported by a variety of stateoftheart models and datasets solidifies the papers claims. 3. Practical Contributions: By revealing a previously unexplored vulnerability of certifiably robust models and suggesting potential defenses against these attacks the authors contribute valuable insights that can guide both future research and the design of more resilient machine learning systems. 4. Reproducibility and Ethics Considerations: The authors have made a commendable effort to ensure the reproducibility of their results by committing to share their code and using publicly available models and training protocols. Additionally the ethics statement reflects a thoughtful consideration of the potential negative repercussions of their findings.  Weaknesses 1. Defense Strategies Limited Exploration: While the paper sketches proposed defenses against degradation attacks the exploration and evaluation of these defenses seem rudimentary compared to the thorough analysis of the attack itself. Particularly the defense via an Mmembership oracle is left largely unexplored. 2. Theoretical Foundation: The paper could benefit from a deeper theoretical analysis of why certifiably robust defenses are prone to overcautiousness. A theoretical framework could offer insights into designing inherently more resilient defenses. 3. Evaluation Against Diverse Attack Configurations: The evaluation focuses on attacks with certain assumptions (e.g. normbounded perturbations). An exploration of how degradation attacks perform under varied attack configurations could provide a more nuanced understanding of the threat landscape.  Questions and Feedback to the Author 1. On Defense Strategies: Could the authors elaborate on potential methodologies for implementing the Mmembership oracle and its practical challenges Also are there other defense strategies currently under consideration 2. Theoretical Insights: Can the authors provide more theoretical insights or hypotheses on the intrinsic reasons behind the overcautious nature of certifiable defenses Such analysis could enrich the discussion and help in the formulation of more robust defense methodologies. 3. On Broader Impact: Given the potential for degradation attacks to be employed maliciously how do authors propose the research community should address the ethical implications of disclosing such vulnerabilities  Review Summary This paper makes a significant contribution to the field of machine learning security by uncovering and analyzing a novel form of vulnerability in certifiably robust neural networks. The discovery of degradation attacks and the empirical demonstration of their impact on model utility are both compelling. The paper is wellorganized provides a thorough empirical evaluation and suggests directions for future defenses thus setting a solid foundation for subsequent research in this area. However the exploration of defense strategies could be expanded and a deeper theoretical understanding of the vulnerabilitys root causes would significantly enhance the papers value. Furthermore broader discussions on the potential misuse of such attacks and more elaborate defense mechanisms would be beneficial. Considering the strengths and opportunities for improvement the paper is a valuable contribution to the literature on adversarial robustness and neural network security. It opens up new avenues for research particularly in developing defenses against utility degradation attacks and understanding the limitations of current certifiably robust models.", "hGXij5rfiHw": " Paper Summary The paper introduces a novel strategy named Discovering Invariant Rationale (DIR) aimed at enhancing the intrinsic interpretability of Graph Neural Networks (GNNs) through the identification of causal features within graph data. Unlike existing rationalization models that heavily draw on data biases and shortcut features for prediction leading to poor generalization on outofdistribution (OOD) data DIR proposes a methodology for constructing intrinsically interpretable GNNs that focus on causal rather than spurious or noncausal patterns. This is achieved by generating various interventional distributions from the training set and leveraging these distributions to distinguish between causal and noncausal subgraphs for robust model prediction. The effectiveness of DIR is validated through experiments on both synthetic and realworld datasets exhibiting superior interpretability and generalization abilities in graph classification tasks compared to stateoftheart baselines.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty: The DIR approach introduces a novel perspective by integrating causal intervention into the learning process of GNNs addressing the critical challenge of model interpretability and generalization. 2. Comprehensive Experiments: The paper presents a thorough evaluation of DIR on both synthetic and realworld datasets offering substantial evidence of its superior performance relative to leading baselines. 3. Practical Relevance: The approach addresses a realworld need for GNNs that can generalize well on OOD data while providing intrinsic interpretability directly applicable to critical fields such as protein structure prediction. Weak Points: 1. Complexity and Scalability: The methodologys complexity particularly in constructing and managing various interventional distributions might pose challenges in terms of computational resources and scalability to very large datasets. 2. Limited Discussion on Theoretical Guarantees: While theres some discussion on theoretical justification the paper could benefit from a deeper theoretical analysis concerning the conditions under which DIR guarantees superiority in invariant rationale discovery. 3. Potential for Bias Introduction: Theres an implicit assumption that interventions can perfectly distinguish causal from noncausal subgraphs which may not always hold true. Potential biases in intervention design or implementation could inadvertently affect results. Questions and Feedback: 1. On Scalability: How does DIR perform in terms of computational efficiency and scalability when applied to very large graph datasets Are there strategies in place to mitigate potential computational bottlenecks 2. On Theoretical Grounding: Could the authors expand on the theoretical grounding of DIR particularly on conditions or assumptions necessary for its optimal performance This addition could strengthen the methods reliability. 3. Potential Biases: How does the paper address potential biases introduced through the intervention process Are there safeguards or validation steps to ensure that interventions accurately identify causal patterns  Review Summary The paper presents an innovative and practically relevant approach to enhancing the interpretability and generalization capabilities of GNNs through causal rationale discovery. The proposed DIR strategy signifies a significant advancement in addressing longstanding challenges associated with data biases and the reliance on shortcut features in GNN prediction models. The comprehensive experimental validation coupled with theoretical discussions convincingly illustrates DIRs effectiveness over existing models. However the paper would benefit from deeper exploration into its theoretical underpinnings a clearer exposition of potential challenges related to scalability and computational efficiency and a more critical examination of potential bias introduction through the proposed interventions. Clarification and expansion on these points could substantially strengthen the papers contributions and applicability. Despite these areas for improvement the papers innovative approach combined with its demonstrated success across varied datasets marks a valuable contribution to the field. It paves the way for future research focused on creating more reliable interpretable and generalizable GNN models especially in critical realworld applications requiring robust and transparent decisionmaking processes.", "vEIVxSN8Xhx": " Review of \"Expanding Local Receptive Fields with LogPolar Space Convolution\"  Summary: The paper introduces a novel convolution technique termed LogPolar Space Convolution (LPSC) which innovates the design of convolution kernels from a geometric perspective. Departing from the standard square grid LPSC adopts an elliptical shape for the convolution kernel arranging its parameters in a logpolar coordinate system. This design allows the layer to provide exponentially growing local receptive fields (LRF) while keeping the parameter count constant addressing the quadratic increase in parameters seen with traditional methods of enlarging the LRF such as the use of larger kernels or multilayer small kernels. Implemented through logpolar space pooling the proposed LPSC layer can be integrated into existing CNN architectures like AlexNet VGGNet ResNet and others to replace conventional convolutions. The paper demonstrates LPSCs effectiveness across various tasks and datasets presenting it as a tool to improve feature extraction without the typical tradeoffs.  Strengths: 1. Innovation in Convolution Design: LPSC represents a significant departure from traditional convolution methods by leveraging a geometric restructuring of the convolutional kernel. This innovation not only addresses the limitation of local receptive field sizes but does so without the penalty of increased parameters. 2. Compatibility and Integrability: The methods ability to be implemented with conventional convolution operations through logpolar space pooling ensures that it can be readily integrated into existing CNN architectures offering a straightforward enhancement path. 3. Empirical Validation: The extensive experiments conducted on multiple datasets and task types (classification and segmentation) using different network architectures and the comparative analysis against dilated convolutions and conventional methods underscore the LPSCs practical effectiveness and versatility.  Weaknesses: 1. Hyperparameter Sensitivity: The introduction of three new hyperparameters (Lr L\u03b8 and g) might complicate the model tuning process. While the paper suggests typical ranges for these parameters their influence on different tasks and datasets could vary necessitating further exploration. 2. Memory Overhead: The implementation via logpolar space pooling despite its innovative approach introduces significant memory overhead. This could limit the practical application of the LPSC in resourceconstrained environments or when applied at larger scales. 3. Lack of Theoretical Analysis: The paper primarily focuses on empirical results with limited theoretical examination of why LPSC performs better than other methods. An analysis of how LPSC influences the learning dynamics or affects gradient flow could provide deeper insights into its effectiveness.  Questions and Feedback for the Authors: 1. Could the authors provide more insights or analyses into how LPSC kernels adapt to different object scales and orientations in practice Are there scenarios where LPSC could perform suboptimally due to its geometric assumptions 2. In terms of implementation complexity how does the logpolar space pooling and parameter regularization impact the training time or inference speed compared to conventional or dilated convolutions 3. Given the memory overhead introduced by logpolar space pooling are there optimization techniques or recommendations the authors could suggest to mitigate this issue  Review Summary: The paper by introducing LPSC addresses a critical challenge in convolutional neural network design  enhancing the local receptive field without a proportional increase in parameters. This approach not only presents a novel geometric perspective on kernel design but also demonstrates compatibility with existing architectures and empirical effectiveness across tasks. However the introduction of new hyperparameters increased memory overhead and a focus on empirical results over theoretical exploration are areas where further work would be beneficial. Despite its weaknesses the merits of LPSC particularly its innovative approach and demonstrated practical value make it a significant contribution to the field of convolutional neural network research and design.  Recommendation: Based on the innovative approach compatibility with existing architectures and demonstrated effectiveness I would recommend accepting this paper contingent upon addressing concerns about memory overhead and providing deeper theoretical insights into LPSCs operation and advantages.", "w4cXZDDib1H": " Peer Review  1. Summary of the paper The paper presents a novel object detector Vision and Detection Transformers (ViDT) which integrates vision transformers (ViT) and detection transformers (DETR) to create a fully transformerbased endtoend object detection system without increasing computational load. ViDT introduces a Reconfigured Attention Module (RAM) to adapt the Swin Transformer for object detection maintains computational efficiency with a lightweight encoderfree neck architecture and incorporates a tokenbased knowledge distillation technique for performance improvement. The proposed system achieves stateoftheart performance on the Microsoft COCO benchmark dataset demonstrating the best tradeoff between average precision (AP) and latency among similar transformerbased models.  2. Strong and weak points questions and feedback Strong Points:  Innovative Integration: The seamless integration of ViT and DETR through a novel architecture delivers remarkable improvements in detection performance without heavily increasing computational costs.  Attention Mechanism Efficiency: The use of RAM to modify Swin Transformer for object detection addresses scalability issues related to the canonical ViT showcasing a linear complexity that enhances the model operability on larger images.  EncoderFree Neck Architecture: By eliminating the need for a transformer encoder in the neck ViDT directly extracts multiscale features apt for object detection further reducing computational demands.  Knowledge Distillation Approach: Introducing token matching for knowledge distillation is a novel method that effectively enhances the models performance utilizing the strengths of larger models to improve smaller models without compromising efficiency. Weak Points:  Limited Comparison Baselines: The paper primarily compares ViDT against transformerbased detectors. Including comparisons with stateoftheart CNNbased detectors could provide a more comprehensive performance analysis.  Implementation Details on Distillation: The paper could detail more on the practical implementation of knowledge distillation specifically how token matching is executed and its computational implications during training. Questions and Feedback: 1. Comparison with NonTransformerBased Detectors: How does ViDT compare with leading CNNbased detectors in terms of accuracy and computational efficiency Including such comparisons could underline the practical benefits of transformerbased architectures in realworld applications. 2. Generalization to Other Datasets: The evaluations are limited to the COCO dataset. Have there been any attempts to assess the models performance on other object detection datasets and if so what were the outcomes 3. Impact of Knowledge Distillation on Training Time: How does the knowledge distillation process influence the overall training time and computing resources required for ViDT models  3. Review Summary The paper on Vision and Detection Transformers (ViDT) presents a significant advancement in the use of transformers for object detection tasks demonstrating how to overcome the limitations of existing transformer models while preserving computational efficiency. Through innovative components such as the Reconfigured Attention Module and an encoderfree neck architecture the authors show a path forward for fully transformerbased detectors to achieve impressive performance metrics. The approach of combining multiscale feature exploitation with a lightweight decoder and enhancing model performance through a simplified tokenbased knowledge distillation strategy is both novel and impactful. While the paper sets a new benchmark in transformerbased object detection expanding the scope of comparison to include conventional CNNbased detectors and testing the models generalization capabilities across varied datasets would provide a more holistic view of its effectiveness. Additionally elaborating on the implementation and computational aspects of the knowledge distillation process could give readers a better understanding of its practical applications. Overall ViDT represents an important stride towards optimizing transformer models for complex vision tasks leveraging the benefits of transformers\u2019 flexibility and scalability. This work has the potential to guide future research in creating more efficient and powerful object detection systems and subsequent studies should aim to build upon its foundations to explore broader applications and enhancements.", "gI7KCy4UDN9": " Review of \"Integeronly Posttraining Quantization for Crossplatform Consistent Learned Image Compression\"  Summary of the Paper This paper tackles one of the foremost challenges in applying learned image compression techniques across different computing platforms: the lack of crossplatform consistency in the probability prediction caused by nondeterminism in computation. The authors propose a solution by implementing wellestablished posttraining quantization (PTQ) techniques to achieve integerarithmeticonly model inference mitigating the crossplatform inconsistency problems while maintaining competitive ratedistortion performance. The approach is validated with improvements in discretizing entropy parameters and extending deterministic inference to Gaussian mixture models (GMM). The method is tested on stateoftheart learned image compression models showcasing the potential for broader application and development in practical settings. This paper not only addresses the deterministic computing issue for learned data compression but also lays groundwork for future advancements by applying quantization techniques rigorously tested in other domains.  Strong Points 1. Novel Application of PTQ: The paper demonstrates an innovative use of posttraining quantization techniques to solve the nondeterminism problem in learned image compression. By leveraging PTQ which is less explored in this specific context the authors offer a credible solution to a critical issue potentially broadening the practical deployment of learned compression models. 2. Maintaining Performance: The solution manages to maintain marginal compression performance loss a significant achievement demonstrating that crossplatform consistency does not have to come at the expense of compression efficiency. This balance is crucial for the industrial application of learned image compression technologies. 3. Thorough Evaluation: The paper provides a comprehensive evaluation section including comparisons with stateoftheart BPG and VVC compression methods and a detailed discussion of ratedistortion performance. This thorough analysis adds credibility to the claims and provides a clear picture of the method\u2019s effectiveness.  Weak Points 1. Limitations Not Clearly Addressed: While the paper provides a robust solution to the nondeterminism issue it does not sufficiently discuss potential limitations or scenarios where the approach might not be as effective. Understanding these aspects is crucial for industrial stakeholders considering deploying these solutions. 2. Comparative Analysis: Though the paper compares its approach to existing methods it lacks a detailed comparative analysis against other quantization techniques especially in scenarios of extreme resource constraints or specific hardware capabilities. This comparison could have provided deeper insights into the methods utility spectrum. 3. Implementation Details: The paper could improve by providing more detailed implementation specifics particularly regarding the process of selecting quantization parameters and its impact on different architectures. This addition would aid in replicating the study and exploring the methodology\u2019s adaptability to various settings.  Questions and Feedback to the Author 1. Limitation Discussion: Could the authors elaborate on any limitations encountered during the application of PTQ in learned image compression Are there specific conditions under which this approach might be less effective 2. Impact on Training Process: How does the proposed PTQ approach impact the training process of compression models especially in terms of computational resources and training duration 3. Hardware Compatibility: Given the focus on crossplatform consistency have the proposed integeronly PTQ techniques been tested across a wide range of hardware specifications Feedback on any adjustments needed to accommodate different hardware capabilities would be beneficial.  Review Summary The paper presents an important contribution to the field of learned image compression by addressing the critical issue of crossplatform consistency through posttraining quantization. This novel application of PTQ not only solves the nondeterminism problem but also preserves the models ratedistortion performance marking a significant step forward in making learned compression techniques more viable in practical applications. While the paper lays a strong foundation it could be strengthened by a detailed discussion of potential limitations a comparative analysis with other quantization approaches and more comprehensive implementation details. Expanding on these points would provide a clearer path for future research and application development in this domain.", "fYor2QIp_3": " Peer Review  1. Summary of the Paper This paper addresses the challenge of Protein Function Prediction (PFP) by proposing a novel method that combines a pretrained Language Model (LM) for protein sequence encoding and a Graph Convolutional Network (GCN) to incorporate hierarchical features from Gene Ontology (GO) terms. The core innovation lies in enhancing GCNs ability to handle largescale hierarchical graphs by incorporating nodewise representations that capture the entire hierarchy of GO terms. Through extensive experiments notably on the CAFA3 dataset the authors demonstrate that their approach outperforms current stateoftheart models in PFP particularly in predicting biological process ontology which is acknowledged as the most challenging domain due to its complexity and scale.  2. Strong and Weak Points Questions and Feedback  Strong Points: 1. Innovation and Relevance: The paper addresses a significant problem in computational biology and proposes a method that is both innovative and relevant. Combining LM for sequence encoding with GCN for hierarchical feature representation is a promising approach. 2. Experimental Results: The extensive experimental analysis and comparison with baseline models demonstrate the effectiveness of the proposed method particularly its superior performance on the biological process ontology. 3. Methodological Clarity: The methodology from protein sequence encoding to hierarchical representation of GO terms and the prediction layer is described in a clear and understandable manner.  Weak Points: 1. Generalizability: While the results are promising the paper does not discuss the generalizability of the proposed method beyond the datasets and specific ontologies explored. 2. Interpretability: The paper lacks a discussion on the interpretability of the models predictions. For practical biomedical applications understanding why certain functions are predicted is almost as important as the predictions themselves. 3. Comparison with Very Recent Methods: Given the fastpaced advancement in the field it would have been beneficial to see comparisons with the very latest methods in PFP if any beyond those mentioned.  Questions: 1. How does the model perform on proteins with sparse or no annotations in the GO database Is its performance significantly affected by the quality of the annotations available 2. Were there any challenges in training the model especially given the largescale nature of the hierarchical GO graphs How do you manage overfitting if any 3. Can this model be extended to predict novel protein functions that are not yet annotated in GO If so what modifications would be necessary  Feedback:  It would enhance the paper to include a discussion on the potential impact of the model\u2019s predictions on realworld biological and medical research. How do the authors envision this model being used in the context of discovering novel protein functions or in drug discovery  Incorporating a section on model interpretability and generalizability would make the paper more comprehensive and appealing to a broader audience.  A deeper analysis of the models limitations and potential areas for future improvements would be beneficial.  3. Review Summary and Recommendation Overall the paper presents a significant contribution to the Protein Function Prediction field by introducing a novel method that creatively combines language modeling for sequence encoding with graph convolutional networks enhanced for hierarchical data. The methodological clarity coupled with robust experimental results supports the effectiveness of the approach. However to strengthen the paper further it would benefit from addressing the mentioned weak points especially focusing on the aspects of generalizability and interpretability. Additionally engaging with the most recent methodologies (if applicable) and discussing potential realworld implications and applications would enhance its relevance and impact. Given the clear strengths and the potential for addressing the highlighted concerns I recommend this paper for publication provided that the authors take into consideration the feedback and questions raised in the review to further refine and enhance their manuscript. The papers contribution to advancing PFP methodologies is clear and with revisions that deepen its discussion on implications limitations and future directions it could make a valuable addition to the field.", "sWqjiqlUDso": " Review of \"A Novel Causal GraphBased Fairness Framework for PathSpecific Causal Fairness\"  1. Summary This paper addresses the challenge of algorithm fairness through the lens of pathspecific causal fairness particularly focusing on the Scenario where sensitive attributes exert their influence on algorithm predictions through various pathways \u2013 some fair and some unfair. The authors propose a novel Causal Graphbased Fairness (CGF) framework that integrates graph structure learning with fair prediction to ensure unbiased outcomes by excluding unfair pathways in the causal graph. Unlike previous works that demand predefined causal graphs or grapple with the complexity of calculating pathspecific effects the CGF framework dynamically learns a causal graph that helps minimize unfair causal effects while maximizing prediction accuracy. What sets this framework apart is its ability to handle sensitive attributes as nonroot nodes in the causal graph addressing a gap left by previous studies. The paper provides theoretical underpinning for its methodology and validates its effectiveness using realworld datasets demonstrating improvements in prediction performance and fairness.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The integration of causal graph structure learning with fairness constraints is a significant contribution addressing a critical gap in handling nonroot sensitive attributes.  Theoretical Underpinning: The paper provides a thorough theoretical analysis ensuring a solid foundation for the proposed framework.  Empirical Validation: Extensive experiments on realworld datasets effectively demonstrate the frameworks ability to balance algorithm fairness and prediction performance. Weak Points:  Assumption of No Latent Confounders: The frameworks assumption that there are no latent confounders might limit its applicability as realworld data often contain unobserved variables that could affect the causal mechanism.  Complexity and Comprehensibility: The dense theoretical explanation and the complex structure of the proposed framework might be challenging for practitioners to grasp and implement effectively without sufficient guidance.  Scalability Concerns: There is limited discussion on the scalability of the proposed method especially in handling very large datasets or complex graph structures. Questions and Feedback: 1. Handling Latent Confounders: Could the authors elaborate on potential strategies to extend the CGF framework to account for latent confounders in the causal graph 2. Implementation and Scalability: Can the authors provide more insights into the computational efficiency of the CGF framework and its scalability to larger datasets or more complex graph structures 3. Interpretability of the Learned Causal Graph: How interpretable is the learned causal graph and can practitioners use it to gain insights into the causal relations and fairness considerations within their models  3. Review Summary The paper introduces an innovative approach to improving algorithm fairness by dynamically learning a causal graph structure that distinguishes fair and unfair pathways of sensitive attribute influences on predictions. This novel framework addresses a significant gap in current research notably the treatment of sensitive attributes as nonroot nodes and the simplification of pathspecific effect calculations. The theoretical analysis provided is robust and offers a solid foundation for the methodology which is further supported by compelling empirical evidence. However the paper assumes the absence of latent confounders which might not always hold true in realworld scenarios and could potentially limit the frameworks applicability. Moreover the explained methodology while comprehensive might pose practical challenges regarding comprehensibility and implementation for nonexperts. Lastly discussions on the frameworks scalability and computational efficiency especially regarding larger or more complex datasets would enrich the paper. The promise shown by the proposed CGF framework in balancing fairness and prediction performance is laudable. With further clarification on addressed weak points particularly around latent confounders and scalability the framework can significantly impact fair machine learning practices. I recommend publication with minor revisions addressing the critiques mentioned above which would undoubtedly make the papers contributions clearer and more accessible to a broader audience.", "kiNEOCSEzt": " Peer Review of \"Optimizing Recommender Systems to Avoid Manipulative Behaviors via Safe Shifts\"  1) Summary of the Paper: This paper tackles the intricate problem posed by the actions of recommender systems (RS) on user preferences. Specifically it explores how RS can influence shift or even manipulate user preferences through exposure to certain content thereby challenging the ethical use and design of RS. The authors argue that designers should estimate potential shifts in user preferences before deploying an RS to evaluate whether these shifts are undesirable or manipulative. They introduce a methodology involving the training of a user predictive model from historical user interaction data to anticipate how different RS policies would influence user preferences over time. Furthermore they propose evaluations and optimizations using metrics based on the notion of \u201csafe shifts\u201d\u2014a concept defining trust regions within which behavior shifts are considered ethically safe. Through simulated experiments the authors demonstrate that RS optimized for \u201csafe shifts\u201d can avoid manipulative behaviors while maintaining user engagement.  2) Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Approach: This work addresses an understudied yet critical aspect of RS design\u2014ethical considerations around shifting user preferences. The concept of \"safe shifts\" is a significant contribution.  Comprehensive Methodology: The methodology employs both predictive modeling and evaluation metrics to anticipate and evaluate preference shifts. This dual approach is thorough and wellarticulated.  Practical Implications: The research offers a practical solution to an ethical dilemma providing a framework for developing more ethically responsible RS. Weak Points:  Complexity in Implementation: The methodology while comprehensive seems complex and may challenge practitioners in terms of implementation and realworld application.  Evaluation Metrics: The paper proposes novel metrics for evaluation but it\u2019s unclear how universally applicable or easily interpretable these metrics will be across different domains or RS applications.  Generalization Concerns: The results derived from simulated environments raise questions about the methods applicability to realworld data and dynamics where user behavior and preferences might be more complex. Questions and Feedback to the Author:  Could the authors elaborate on how they see the \"safe shifts\" concept adapting to different domains with varying ethical considerations  Is there potential for the proposed methodology to incorporate feedback or corrections based on realworld deployment outcomes  The simulation assumes relatively simple dynamics for user behavior and preferences. How would the proposed method handle realworld complexities such as the dynamic nature of content availability or the diversity of user backgrounds and biases  3) Review Summary and Recommendation: The paper presents a timely and ethically important investigation into how the design and deployment of RS can inadvertently shift user preferences. The introduction of \"safe shifts\" as a concept to mitigate manipulative behaviors represents a novel and valuable contribution to the field. While the proposed methodology is comprehensive and demonstrates potential through simulated experiments the complexity of implementation and the generalizability of the findings to realworld scenarios remain areas for further exploration. Given the innovative approach and the importance of ethical considerations in RS design I recommend this paper for publication. However I encourage the authors to address the raised questions about domain adaptability realworld applicability and methodological complexity. Further it would be beneficial for future work to explore the implementation of the proposed framework in realworld scenarios to validate its effectiveness and refine its adaptability across different RS applications.", "rWXfFogxRJN": " 1) Summary of the Paper This paper introduces AdaAug a novel Automated Data Augmentation (AutoDA) method designed to generate adaptive classdependent and potentially instancedependent augmentation policies efficiently. The research highlights the limitations of existing AutoDA methods mainly their lack of adaptivity to specific dataset variations which commonly leads to suboptimal generalization on unseen data. AdaAug addresses this by employing a differentiable workflow that alternates between exploitation and exploration phases to optimize a datadriven augmentation policy that can adjust based on class or instancespecific features. The method demonstrates superior transferability to unseen datasets (Oxford Flowers OxfordIIT Pets FGVC Aircraft Stanford Cars) and achieves stateoftheart performance on benchmark datasets (CIFAR10 CIFAR100 SVHN) through extensive experimentation.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty: AdaAug\u2019s approach to learning class and instancedependent augmentation policies presents a significant advancement in the domain of AutoDA. The methodology of alternating between exploitation and exploration to refine these policies is both creative and logically sound.  Empirical Evidence: The comprehensive experiments across various datasets including transferability tests and benchmarking against stateoftheart AutoDA methods provide substantial evidence of AdaAug\u2019s effectiveness and versatility.  Efficiency: The efficient learning of adaptive policies through a differentiable workflow demonstrates a practical advantage over traditional methods that suffer from higher computational costs enhancing its applicability in realworld scenarios. Weak Points:  Generalization to NonImage Data: While AdaAug shows impressive performance on image datasets its applicability and effectiveness for other types of data (e.g. text audio) remain unexplored and thus a potential limitation.  Insight into Failure Modes: The paper could benefit from a deeper discussion on the scenarios where AdaAug might not perform optimally including an analysis of failure modes which can provide insights for further improvement.  Comparison with Generative Approaches: Given the mention of generative models for data augmentation a direct comparison between AdaAug and such GANbased methods could have elucidated the practical and performancerelated tradeoffs involved. Questions and Feedback:  Extension to Other Data Types: Can the AdaAug methodology be adapted for nonimage data types such as text or audio If so what modifications to the approach would be necessary  Impact of Hyperparameter Selection: The paper could offer more details on the sensitivity of AdaAugs performance to hyperparameter choices and the robustness of the method across different settings.  Insights into Learned Policies: Providing more qualitative insights or visualizations into the policies learned for various classes or instances (beyond the mentioned Appendix) could help understand what makes AdaAug effective and how it distinguishes between differing augmentation needs.  3) Review Summary and Recommendation The paper presents a compelling advancement in automated data augmentation through AdaAug effectively addressing the need for classand instancespecific adaptivity in augmentation policies. The extensive empirical validation alongside the novel approach that balances exploitation with exploration sets a strong foundation for its potential as a significant contribution to the field. However the paper could be enriched by addressing the broader applicability of AdaAug beyond image data detailing its potential limitations and providing more insights into the inner workings and decisions behind learned augmentation policies. Given the strengths of the proposed method and the comprehensive experiments that substantiate its claims the paper is recommended for acceptance. However I strongly suggest that the authors consider the feedback provided especially in broadening the discussion on the methods applicability its comparison to generative approaches and the provision of deeper insights into learned policies and possible failure modes. These additions would substantially enhance the papers contribution and provide clearer guidance for future work inspired by AdaAug.", "vr4Wo33bd1": " Review of \"Leveraging Unlabeled Data for SemiSupervised LongTailed Recognition\"  Summary This paper introduces a novel approach to address the challenges posed by the longtailed distribution of data common in realworld datasets through the utilization of unlabeled data in a semisupervised learning setting. The authors propose a semisupervised longtailed recognition setting that mirrors the actual process of data collection and annotation thereby presenting a more realistic and challenging scenario. The proposed alternate sampling framework decouples the training of the classifier and the feature embedding employing classbalanced sampling for the classifier to mitigate the bias towards head classes typical of longtailed distributions. Additionally a consistency loss mechanism is incorporated to utilize unlabeled data in updating the feature embedding while limiting its negative impact. This method is distinct in its combination of a classbalanced sampling strategy and iterative update process between the classifier and feature embedding showing significant improvements over competitive methods on CIFAR10SSLT and ImageNetSSLT datasets.  Strong Points 1. Realistic Scenario Assumption: The paper addresses a gap in current research by assuming a realistic scenario where labeled and unlabeled subsets of training data follow the same underlying longtailed distribution. This assumption is more representative of realworld data collection and annotation workflows. 2. Decoupling Strategy: The proposed alternate sampling framework that decouples classifier training from feature embedding training offers a novel approach to balance the training process in a semisupervised longtailed setting. This strategy effectively addresses the challenges of applying classbalanced sampling on unlabeled data. 3. Iterative Update Mechanism: The iterative update process for the classifier and feature embedding with the introduction of pseudo labels and a consistency loss for unlabeled data is an innovative addition. This mechanism allows for the progressive improvement of the model by leveraging unlabeled data more effectively.  Weak Points 1. Generalization and Scalability: While the paper demonstrates significant improvements on CIFAR10SSLT and ImageNetSSLT datasets it may have benefited from evaluating the method on a broader range of datasets to better establish the generalizability and scalability of the proposed approach. 2. Implementation Details: The paper could provide more detailed insights into the practical implementation challenges especially when dealing with extremely large datasets or datasets with a very high degree of imbalance. 3. Comparison with StateoftheArt: A more exhaustive comparison with the latest stateoftheart methods in both semisupervised learning and longtailed recognition would enhance the papers contribution further. Specifically understanding how the proposed method stands against purely supervised methods designed for longtailed recognition could provide more depth.  Question and Feedback to the Author 1. Could the authors elaborate on how the proposed method scales with extremely imbalanced datasets and if there are limitations to the size of the unlabeled dataset that can be effectively utilized 2. It would be beneficial if the authors could discuss any implementation challenges encountered especially in terms of computational efficiency and resource requirements when applying the proposed method to larger datasets. 3. Can the authors provide additional comparisons with recent stateoftheart methods particularly focusing on the aspects where the proposed method outperforms these approaches in semisupervised longtailed recognition  Review Summary The paper presents a thoughtful and innovative approach to addressing the semisupervised longtailed recognition problem by leveraging unlabeled data in a realistic data collection and annotation scenario. The proposed methods strengths lie in its realistic scenario assumption the decoupling strategy between the classifier and feature embedding training and the innovative iterative update mechanism. Despite its considerable contributions the paper could be strengthened through expanded generalization considerations more indepth discussions on implementation challenges and a broader comparison with the stateoftheart methods. Overall the paper provides a significant step forward in the semisupervised longtailed recognition domain demonstrating notable improvements over competitive methods and paving the way for further research in this area.", "ybsh6zEzIKA": " Review of \"IntrusionFree Mixup for Graph Neural Networks\"  Summary The paper introduces a novel approach for enhancing the generalization capability of Graph Neural Networks (GNNs) through an interpolationbased regularization technique named ifMixup (intrusionfree Mixup). This method is inspired by the Mixup technique which has been effectively used in image and text classification by creating synthetic samples through linear interpolation of randomly selected training sample pairs and their labels. However applying Mixup to graphs is challenging due to the arbitrary structure and topology of graph data which significantly influence their semantic meaning. The authors address these challenges by presenting the first input mixing schema designed for Mixup on graphs ensuring that the mixed graphs are free from manifold intrusion issues and can recover the source graphs therefore enhancing model generalization without introducing underfitting.  Strong Points 1. Innovation: The proposal of an input mixing schema for Mixup on graph classification is novel and addresses the unique challenges associated with graph data structure and semantics. 2. Theoretical Foundation: The authors provide a robust theoretical framework to support their methodology including proofs for the invertibility and intrusionfreeness of their mixing strategy. 3. Empirical Validation: The method is empirically validated with eight benchmark graph classification tasks across different domains showing superior performance compared to popular graph augmentation baselines and existing pairwise graph mixing methods.  Weak Points 1. Complexity for Large Graphs: While the methodology is sound the paper does not discuss the computational complexity or scalability of their approach especially for very large graphs or dense graphs. 2. Limited Discussion on Limitations: The paper could benefit from a more detailed discussion on potential limitations or scenarios where ifMixup might not perform as expected. 3. Comparison with Nonaugmentation Baselines: The comparison is mainly focused on augmentationbased strategies. It would be informative to see how ifMixup compares with other types of regularization or generalization techniques for GNNs.  Questions and Feedback 1. Scalability: Can the authors comment on the computational scalability of ifMixup especially when dealing with large or dense graphs 2. Impact of Graph Sparsity: How does the sparsity or density of the original graphs affect the performance of ifMixup 3. Generalization to Other Tasks: While the paper focuses on graph classification could this approach be adapted for other tasks such as link prediction or graph generation Any theoretical or practical challenges foreseen in such adaptations  Review Summary The paper presents an innovative and theoretically grounded approach for applying Mixup regularization to graph neural networks addressing the unique challenges posed by graph datas arbitrary structure and topology. The methodology is wellmotivated and the empirical results demonstrate its effectiveness in improving GNN generalization on various benchmark datasets. However the paper could be strengthened by addressing the computational aspects of the proposed method especially its scalability to larger graphs and by providing a broader comparison with other regularization strategies beyond augmentationbased methods. Overall the paper makes a significant contribution to the field of GNN regularization and opens up interesting avenues for future research in applying Mixup techniques to graph data.", "wbPObLm6ueA": " Peer Review of \"Shifty: HighConfidence Fairness Guarantees under Demographic Shift\"  Summary The paper introduces \"Shifty\" an innovative class of machine learning algorithms designed to maintain fairness guarantees in the face of demographic shifts\u2014changes in the demographic makeup of populations that models encounter between training and deployment phases. The authors argue that existing fairnessaware algorithms assume a static distribution between training and deployment phases an assumption that doesnt hold true in realworld scenarios. Shifty algorithms uniquely tackle the challenge of demographic shifts by providing highconfidence behavioral guarantees without needing deployment environment data during training. Two scenarios are considered: known demographic shifts (where new demographic proportions are precisely known) and unknown demographic shifts (where these proportions are bounded within known intervals). The effectiveness of Shifty algorithms is demonstrated through evaluations on the UCI Adult Census dataset and a dataset from the Brazilian university system regarding university entrance exams and student success showing that Shifty can successfully maintain fairness without significant accuracy loss.  Strong Points 1. Novel Algorithmic Framework: The introduction of Shifty represents a significant advancement in the design of fairnessaware machine learning algorithms. Its capability to handle demographic shifts without access to deployment data fills a crucial gap in the literature. 2. Comprehensive Evaluation: The paper includes thorough empirical evaluations using realworld data effectively demonstrating that Shifty can provide fairness guarantees under demographic shifts which prior models fail to uphold. 3. Practical Relevance: The problem of demographic shift is highly relevant in many realworld applications of machine learning making the solutions provided by Shifty of considerable practical importance. 4. Open Source Contribution: By releasing an opensource implementation of Shifty and their datasets the authors significantly contribute to the community enabling other researchers to replicate the studys results and build upon this work.  Weak Points 1. Assumption Transparency: While Shifty is innovative the paper could benefit from a clearer explanation of the assumptions underlying the models operation particularly concerning the distributions of demographic shifts and how these assumptions impact the applicability of Shifty in realworld scenarios. 2. Theoretical Basis: The paper provides a constructive proof of the guarantees offered by Shifty but a more detailed theoretical exploration of the conditions under which these guarantees hold or fail could strengthen the work. 3. Comparison with Existing Methods: Although the paper compares Shifty to previous algorithms a more detailed analysis of the performance differences including a broader range of metrics (beyond fairness and accuracy) would provide deeper insights into the tradeoffs involved. 4. Consideration of Alternative Approaches: Exploring and discussing alternative methodologies for addressing demographic shifts in fairness such as adaptive learning or continuous model retraining could offer a more rounded perspective on solving this challenge.  Questions and Feedback to the Author 1. Clarity on Assumptions: Can the authors clarify the assumptions made about the nature and predictability of demographic shifts and how these assumptions might limit the scenarios where Shifty can be effectively applied 2. Theoretical Foundation: Is it possible to provide a more detailed theoretical analysis or proofs regarding the conditions under which Shiftys fairness guarantees hold especially under extreme demographic shifts 3. Performance Metrics: Could the authors discuss other performance metrics (e.g. precision recall F1 score) that were considered or evaluated during their experiments and why they chose to focus on accuracy and fairness 4. Comparison and Alternative Approaches: How does Shifty compare to dynamic or adaptive learning approaches that continuously update models based on new data Could such approaches address demographic shifts in fairness more effectively or efficiently  Review Summary This paper presents a significant contribution to the field of fairness in machine learning by introducing Shifty an innovative algorithmic framework designed to maintain fairness guarantees in the presence of demographic shifts. The comprehensive evaluation using realworld datasets convincingly demonstrates Shiftys efficacy in achieving its design goals. However the paper could benefit from greater clarity on the underlying assumptions a deeper theoretical foundation more extensive comparisons with existing methods and a discussion of alternative approaches. Overall the paper makes a valuable contribution to fairnessaware machine learning with Shifty offering a practical solution to the critical challenge of demographic shifts. The release of an opensource implementation and datasets further enhances the papers impact enabling future research in this vital area.", "nf3A0WZsXS5": " Review of the paper \"A Novel Method for Learning DiseaseRelated Imaging Patterns via GAN: SurrealGAN\"  1. Paper Summary This paper introduces SurrealGAN an innovative machine learning approach that leverages Generative Adversarial Networks (GAN) for the semisupervised learning of diseaserelated imaging patterns. Unlike traditional methods that either overlook the heterogeneity of disease effects or provide noninterpretable nonlinear models SurrealGAN efficiently captures the disease heterogeneity in a clinically relevant manner. Utilizing crosssectional imaging data it constructs a continuous dimensional representation of disease severity which is then validated through semisynthetic experiments and applied to Alzheimers Disease (AD) imaging data for demonstration. The proposed method extends preexisting semisupervised clustering techniques introducing a way to model disease progression as a continuum rather than a dichotomous process. By learning transformation functions imposing constraints for pattern capture (such as monotonicity orthogonality and continuity) and implementing an inverse mapping function SurrealGAN can dissect disease heterogeneity and infer individual disease severity levels.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Innovation: The method introduces innovative ways to handle disease heterogeneity by modeling it as a continuous variable and leveraging semisupervised learning for enhanced interpretability. b. Clinical Relevance: The approach can capture clinically significant imaging patterns potentially aiding in the earlier diagnosis and tailored treatment of neurological diseases. c. Validation: Validation through semisynthetic data provides a robust groundwork for model effectiveness demonstrating its ability to capture known disease patterns and severities. d. Clinical Application: Application to Alzheimers Disease and the discovery of relevant imaging patterns confirms the models practical utility in realworld clinical scenarios. Weak Points: a. Complexity and Accessibility: The sophistication of the method might pose a barrier to adoption in clinical settings. The paper could benefit from a discussion on the models computational demands and ease of integration into existing clinical workflows. b. Generalizability: While the model shows promise its primarily validated on AD data. Its performance across a broader spectrum of neurological and neuropsychiatric disorders remains uncertain. c. Interpretability vs. Complexity: Despite the aim for interpretability the inherent complexity of the model and the highdimensional nature of the generated patterns might still pose challenges for clinical interpretation. Questions and Feedback: a. Generalization: How well does SurrealGAN generalize to other types of neurological disorders beyond Alzheimers Are there specific disease characteristics for which this model is particularly wellsuited or illsuited b. Model Complexity: Could the authors elaborate on the models computational requirements and suggest feasible strategies for its implementation in typical clinical settings c. Comparison with Clinical Practices: How do the imaging patterns identified by SurrealGAN compare with those currently utilized in clinical settings for AD diagnosis and prognosis  3. Review Summary SurrealGAN represents a significant advance in the use of machine learning to parse disease heterogeneity in neuroimaging data. Its novel approach to semisupervised learning capacity to model diseases as a continuum and initial application to Alzheimers Disease imaging data showcase both innovative methodology and practical applicability. The validation via semisynthetic data sets strongly supports the models efficacy. However questions around the models complexity generalizability to other neurological conditions and comparison with standard clinical diagnostic patterns warrant further exploration. The balance between computational complexity and clinical interpretability also requires additional attention ensuring that such advanced models can effectively transition from the research domain to impact patient care. Given the strengths noted in innovation validation and clinical relevance alongside addressable concerns about complexity and generalizability the recommendation is an acceptance of the paper. Future work should focus on expanding the validation to a wider range of conditions simplifying the models implementation in clinical workflows and enhancing the interpretation of generated patterns for endusers.", "iEx3PiooLy": " Peer Review  1) Summarize the paper This paper proposes a novel visionbased approach VATMART to generate actionable visual priors for the manipulation of 3D articulated objects by homeassistant robots. Unlike prior work that relies on abstract kinematic structures for manipulation VATMART predicts dense geometryaware interactionaware and taskaware affordances and trajectory proposals for object parts. The approach leverages a dualpathway framework incorporating reinforcement learning (RL) for exploring diverse interaction trajectories and a perception module that learns to generalize these explorations into actionable visual representations. The system was evaluated using the PartNetMobility dataset within the SAPIEN simulator environment demonstrating efficacy in generating visual priors that facilitate robot manipulation of objects like doors and drawers. The results show promising generalization to unseen objects and realworld settings.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The shift from traditional kinematic representations to dense visual priors for object manipulation addresses the intricate challenge of perceiving and interacting with diverse articulated objects.  Comprehensive Evaluation: The utilization of the largescale PartNetMobility dataset and SAPIEN environment for experimentation lends credibility to the findings. The systems favorable performance in generalizing to unseen shapes and object categories underscores its applicability in varied realworld scenarios.  TwoWay Learning Framework: VATMARTs interactionforperception learning paradigm involving a curiositydriven RL policy for exploration and a perception module for generalization is a creative and effective strategy for understanding complex object manipulations. Weak Points:  Dependence on Simulation: While promising the generalizability of the results to realworld data could be hindered by the reliance on simulated environments for training. The differences between simulated and actual environments could affect performance.  Limited Interaction and Articulation Types: The study primarily focuses on doors and drawers with 1DoF articulation. Future work could expand the range of interaction types and object complexities to enhance the systems versatility.  Computationally Intensive: The dualpathway learning process especially the RL exploration might be computationally demanding. The paper could benefit from a discussion on computational efficiency and potential optimization strategies. Questions: 1. How does the VATMART framework adapt to objects with higher degrees of freedom or more complex articulation mechanisms 2. Can the system cope with dynamic environments where object positions or layouts change over time Feedback:  To strengthen the paper it would be beneficial to include more detailed insights into the computational demands of the framework along with strategies to mitigate these demands in practical applications.  Exploring the application of VATMART to a wider variety of tasks and object types could vastly improve the systems utility and applicability.  An indepth discussion on the limitations imposed by simulationbased training and potential pathways to bridge the gap between simulated and realworld environments would add depth to the study.  3) Review Summary The paper presents a commendable and innovative approach to bridging the perceptioninteraction gap in robot manipulation of 3D articulated objects. The proposed VATMART frameworks emphasis on learning dense actionable visual priors represents a significant shift from traditional methods aligning with the practical complexities of realworld tasks. The experimental results underscore the frameworks potential showcasing its capabilities in understanding and generalizing across a variety of object manipulations. However the reliance on simulated environments for training limited scope of tested interaction types and computational demands pose challenges to the frameworks immediate adoption in practical scenarios. For a stronger argument in favor of VATMART further exploration of these aspects is warranted. Overall the study makes a valuable contribution to the field offering a promising direction for future research on robot manipulation in complex environments.", "yhCp5RcZD7": " Summarize the Paper The paper introduces a novel method for representing detailed 3D geometry through implicit displacement fields (IDF) inspired by the traditional computer graphics technique of displacement mapping. This methodology represents complex surfaces by disentangling them into a lowfrequency base surface and a highfrequency displacement field. The process is unsupervised and utilizes a uniquely tailored architecture that inherently creates a frequency hierarchy allowing for effective shape reconstruction and detail transfer. Through systematic evaluations the IDF approach demonstrates improved representational power training stability and generalizability compared to existing methods.  Strengths and Weaknesses Strong Points: 1. Innovative Representation: The idea of extending the classic displacement mapping into a continuous 3D domain and integrating it with neural implicit functions is highly innovative. This allows for detailed and highquality 3D geometry representations. 2. Unsupervised Learning: The unsupervised nature of geometric detail disentanglement without the need for predefined surface parameterizations or correspondence mappings makes the approach widely applicable. 3. Superior Training Stability: The architectural design incorporates SIRENs with distinctive frequency hyperparameters which ensures the models training stability\u2014an issue commonly encountered with highfrequency representations. 4. Excellent Generalizability: Demonstrated capability in detail transfer between different shapes showcases the models ability to generalize well across various applications. Weak Points: 1. PreAlignment Required: For detail transfer applications prealignment of shapes is necessary which might limit the methods flexibility in realworld scenarios where automated alignment is preferred. 2. Model Complexity: The layer of complexity introduced by combining two different frequencies in representation might pose practical challenges in model tuning and optimization for specific applications. Questions and Feedback: 1. Could the authors elaborate on potential strategies for automatic prealignment to enhance the usability of the IDF method in detail transfer applications 2. How does the models performance scale with increasing complexity and variability of input geometries Are there benchmarks on larger datasets 3. The paper mentions the robustness of parameter selection for \u03b1 and \u03bd however detailed guidelines or insights on selecting these parameters for new shapes or datasets would be beneficial.  Review Summary This paper proposes a groundbreaking approach to detailed 3D geometry representation utilizing implicit displacement fields (IDF) building on the principle of displacement mapping. The techniques ability to unsupervisedly distinguish and represent complex surfaces through a smooth base surface plus displacement shows considerable innovation. The designed architecture which results in stable training and excellent model generalizability is commendable. However the limitation regarding the need for prealigned shapes for detail transfer and the added complexity of managing dualfrequency representations suggests areas for improvement. In recommendation the strength of the proposed method in representing geometric details accurately its stability in training and generalizability far outweigh its weaknesses. The method stands out for its innovative approach and potential applications in 3D geometry processing. Further exploration into automatic shape alignment and more comprehensive guidelines for parameter selection would enhance the methods practical applicability and broaden its impact. The novel representation created by this work presents a significant step forward in the field of 3D geometry modeling potentially setting a new standard for future research and applications.", "nLb60uXd6Np": " Peer Review  Summary of the Paper This paper introduces a novel set of deep learning architectures designed to respect key symmetries in physical science data particularly for small point clouds in two or three dimensions. By leveraging geometric algebra and attention mechanisms the authors develop rotation and permutationequivariant neural network models. These models systematically combine vectors scalars and other geometric inputs to maintain invariance or covariance to rotation which is crucial for practical applicability in fields like physics chemistry and biology. The paper describes the theoretical foundations of these architectures their implementation and applications to three domainspecific problems: crystal structure identification molecular force regression and backmapping of coarsegraining operators. The proposed models show promise in solving complex scientific problems by inherently incorporating the symmetry and structure of the data they process.  Strong and Weak Points Questions and Feedback  Strong Points 1. Innovation in Architecture: The integration of geometric algebra with attention mechanisms for deep learning applications in physical sciences is highly innovative. This approach enables the handling of rotation and permutation equivariance in a mathematically rigorous way. 2. Clear Domain Relevance: The paper effectively demonstrates the relevance and application of the proposed architectures across multiple scientific domains showcasing their versatility and potential impact on various fields. 3. Theoretical Depth: There is a considerable depth to the theoretical foundation provided especially in explaining how geometric algebra is used to achieve rotation invariance and how attention mechanisms help with permutation equivariance. 4. Model Interpretability: By utilizing the primitives of geometric algebra and attention schemes the models lend themselves to interpretability which is crucial for scientific applications where understanding model decisions can be as important as the decisions themselves.  Weak Points and Feedback 1. Computational Complexity: The paper acknowledges the potential computational complexity and memory scaling issues but does not provide detailed benchmarks or comparisons regarding the computational efficiency of their models versus existing methods. Future work could benefit from a deeper exploration of these practical limitations and potential solutions. 2. Limited Comparison with Existing Methods: While the paper briefly discusses related work a more detailed comparison with existing methods (beyond a qualitative discussion) including a comparison of performance metrics would strengthen the argument for the proposed architectures novelty and effectiveness. 3. Data Homogeneity and Generalization: The discussion on model performance primarily focuses on training set error with limited analysis on the models generalization capability outside the training data. Additional experiments or discussions on validationtest set performance or crossvalidation strategies could address these concerns. 4. Question: Could the authors elaborate on potential strategies for managing the polynomial scaling behavior associated with generating tuples Are there specific heuristics or algorithms that could efficiently select relevant tuples without significantly compromising model performance  Review Summary The paper presents an original and theoretically rich approach to developing neural network architectures that inherently respect the symmetry and structure intrinsic to data from the physical sciences. By harnessing geometric algebra and attention mechanisms the authors propose a framework for building deep learning models that are rotation and permutationequivariant showcasing practical applications in identifying crystal structures predicting molecular forces and backmapping in coarsegrained models. The innovative integration of mathematical principles directly into the architecture design is commendable and represents a significant step toward more interpretable and scientifically relevant machine learning models. However the paper would benefit from a more thorough analysis of computational efficiency a detailed comparison with existing methods in terms of performance and a deeper investigation into model generalization capabilities. Addressing these areas would enhance the papers contribution to the field providing a clearer understanding of the proposed architectures practicality and comparative advantage. Nevertheless the presented work opens promising avenues for research and application in geometric deep learning for scientific domains advocating for models that are not only powerful but also inherently aligned with the underlying principles of the data they are designed to analyze.", "fWK3qhAtbbk": " Peer Review of \"Improving LSTM Timeseries Forecasting Accuracy with Exponential Partitioning\"  1. Summary The paper introduces an innovative approach to handling long time series data for LSTM (Long ShortTerm Memory) neural network models a common challenge in time series forecasting. Traditional methods often simplify this issue by uniformly partitioning the data into smaller intervals and applying aggregation functions like averaging to feed into the LSTM model. The authors argue that not only is this method suboptimal due to the uniform treatment of data regardless of its proximity to the prediction point but that other partitioning methods and aggregation functions can better capture the importance of more recent data thus improving forecasting accuracy. To overcome these limitations the paper suggests a novel method of Exponential Partitioning along with the exploration of other aggregation functions (median maximum) beyond the traditional averaging. The key hypothesis is that exponential partitioning which assigns increasingly larger bins as the data goes back in time coupled with an appropriate aggregation function can better represent the datas underlying patterns and improve the LSTM models forecasting accuracy. This hypothesis is tested across seven publicly available datasets showing a significant improvement in prediction accuracy ranging from 6 to 27.  2. Strong and Weak Points Questions and Feedback Strong Points:  Originality: The paper addresses a wellknown challenge in LSTM models with a novel solution that significantly departs from the standardized practice of uniform partitioning.  Comprehensive Evaluation: The authors test their approach using seven diverse datasets which adds credibility to their claims. The extensive experimentation reinforces the robustness of the proposed method.  Practicality: The provision of source code enhances the papers practical value facilitating reproducibility and further experimentation by other researchers or practitioners. Weak Points:  Limited Exploration of Other Models: While the focus on LSTM is justified given its prominence in time series forecasting the impact of the proposed exponential partitioning on other time series models (beyond the brief mention in the conclusion) warrants further investigation.  Lack of Theoretical Explanation: The paper could benefit from a more indepth theoretical discussion on why exponential partitioning and certain aggregation functions perform better. Including analyses on the influence of different data characteristics on the optimal partitioning strategy would provide deeper insight.  Generalization of Findings: The paper presents a compelling case for the improvement in LSTM performance using exponential partitioning and certain aggregation functions. However it lacks a discussion on the possible limitations or the contexts in which this approach may not offer significant benefits. Questions and Feedback:  How sensitive are the improvement percentages to changes in the LSTM models architecture (e.g. number of layers neurons)  Could the authors clarify if the improvement from exponential partitioning is consistent across different time horizons  It would be beneficial if future work could explore the potential tradeoffs between model complexity and forecasting accuracy when adopting this new partitioning strategy.  3. Review Summary The paper presents a significant contribution to the field of time series forecasting by proposing an innovative data preparation method that challenges the conventional wisdom of uniform partitioning for LSTM models. The empirical results across multiple datasets demonstrate a clear advantage of exponential partitioning coupled with nonaverage aggregation functions in enhancing forecasting accuracy. Despite its strengths the paper could further bolster its findings by providing a deeper theoretical foundation for its empirical observations extending its examination to other time series models and discussing the potential limitations or ineffectiveness of the proposed method under different conditions. Moreover exploring the models sensitivity to changes in architecture or partitioning strategy could add valuable insights for practitioners seeking to implement these findings. In conclusion the papers innovative approach substantial empirical validation and practical implications make a compelling case for its acceptance provided certain areas are expanded upon in future work. The suggested improvements would not only solidify its current contributions but also pave the way for further research in this promising area.", "s6roE3ZocH1": " Review of the Paper on Genetic Algorithm Featuring Constrained Molecular Inverse Design  Summary The paper introduces a novel genetic algorithm designed to address the challenges in molecular inverse design particularly focusing on maintaining structural constraints while optimizing pharmacological properties. Traditional methods in drug discovery including deep learningbased approaches often struggle with generating synthesizable molecules and optimizing nonlinear structureactivity relationships efficiently. The novel approach presented leverages a twophase optimization process incorporating graph and SELFIES molecular descriptors to ensure the generation of valid molecules that adhere to structural similarity constraints and achieve desired properties.  Strong Points 1. Innovative Approach: The paper proposes a unique method combining genetic algorithms with constrained molecular design addressing a significant gap in drug discovery research. This approach can potentially streamline the lead optimization process by inherently maintaining structural integrity while exploring chemical space. 2. TwoPhase Optimization Process: The introduction of a twophase optimization process including constraint satisfaction and biooptimization is well conceptualized and executed. This method logically progresses from ensuring structural similarity to optimizing for desired properties which appears effective based on the presented results. 3. Use of SELFIES Descriptors: Employing SELFIES descriptors for mutation operations enhances the validity and reliability of generated molecules. This innovative application potentially solves the common issue of invalid molecule generation in existing algorithms.  Weak Points 1. Comparison with StateoftheArt Methods: While the paper claims superiority over stateoftheart models a more detailed comparison with direct results from leading models would strengthen this claim. Including metrics other than penalized LogP applicable in different contexts could provide a more comprehensive analysis of performance. 2. Diversity of Optimized Properties: The focus on penalized LogP as the primary property for optimization limits the understanding of the algorithms applicability to other pharmacologically relevant properties. Expanding the scope to include multiple properties could highlight the algorithms versatility and applicability in realworld drug design scenarios. 3. Computational Cost and Scalability: The paper lacks discussion on the computational resources required and the scalability of the proposed method. Given the large search spaces in molecular design understanding the algorithms efficiency and practical limitations would be valuable for potential users.  Questions and Feedback to the Author 1. Can the authors provide comparisons with other stateoftheart methods using a diverse set of optimization targets beyond penalized LogP This would help in understanding the generalized performance of the algorithm across different drug discovery contexts. 2. Is there potential to adapt the algorithm for optimizing multiple pharmacological properties simultaneously Insights into potential extensions or modifications to accommodate multiobjective optimization would be beneficial. 3. How does the computational cost of the proposed algorithm compare with existing methods especially when scaling to larger molecular datasets An analysis or discussion on this aspect would help in evaluating the practical applicability of the approach.  Review Summary The paper presents a compelling approach to molecular inverse design addressing critical challenges in drug discovery with an innovative genetic algorithm. The methods emphasis on maintaining structural constraints while optimizing key properties showcases significant promise evident in the experimental results. However for a comprehensive evaluation of the algorithms potential further analysis comparing performance across a wider range of properties and against existing stateoftheart methods would be beneficial. Additionally considerations regarding the computational efficiency and scalability of the approach are necessary to assess its applicability in realworld scenarios. Overall the work contributes meaningfully to the field paving the way for further research and development in constrained molecular design.", "gxRcqTbJpVW": " Review of \"Orthogonality Preserving Pruning for Dynamic Isometry in Neural Networks\"  Summary This paper introduces a new structured neural network pruning technique termed Orthogonality Preserving Pruning (OPP) designed to maintain dynamical isometry\u2014a property facilitating ease of training and robustness of networks\u2014during the pruning process. By incorporating a regularization scheme that targets the Gram matrix of convolutional kernels and batchnormalization parameters OPP aims at preserving kernel orthogonality among significant filters while diminishing the impact of less important weights. The methods efficacy is demonstrated through empirical validation across several benchmarks including linear networks CIFAR datasets with ResNet56 and VGG19 architectures and more challenging the ImageNet dataset with modern deep networks like ResNets. The authors claim it is the first successful attempt at preserving dynamical isometry during pruning for largescale networks positioning OPP as a superior alternative to existing pruning methods in terms of maintaining high performance postpruning.  Strong Points 1. Originality: The idea of maintaining dynamical isometry through a regularizationbased approach during neural network pruning is novel and addresses a critical gap in the domain. The approach to regularizing both the weights and batchnormalization parameters is particularly innovative. 2. Comprehensiveness: The paper provides a thorough comparative analysis with existing pruning methods across various datasets and network architectures. This extensive evaluation adds credibility to the claims made about OPPs performance. 3. Practicality: Given the importance of pruning for deploying neural networks in resourceconstrained environments the proposed method has high practical significance. Its compatibility with modern largescale networks and various datasets enhances its applicability in realworld scenarios.  Weak Points 1. Implementation Details: While the paper mentions that the code and models will be made publicly available immediate provision or more detailed implementation nuances within the paper could help with the reproducibility of the results. 2. Theoretical Underpinning: The paper could benefit from a more detailed theoretical discussion on why exactly the method works including potential limitations of the approach. A deeper analysis of the relationship between dynamical isometry and neural network pruning could enrich the papers contribution. 3. Generalization: While the paper demonstrates impressive performance improvements insights into the methods performance outside the tested architectures and datasets could better establish the generalization capability of OPP.  Questions and Feedback 1. Could the authors provide more insights into the computational overhead introduced by the OPP method compared to conventional pruning techniques especially in contexts where computational resources are highly constrained 2. It would be beneficial to know whether there are specific types of neural networks or application domains where OPP might not perform as expected. 3. Feedback: Expanding the experimental section to include adverse cases or failures could provide a more balanced view of OPPs capabilities and limitations.  Review Summary The paper presents a pioneering approach to preserving dynamical isometry during the pruning of neural networks addressing a significant challenge in the field. The Orthogonality Preserving Pruning (OPP) methods ability to maintain model performance postpruning as demonstrated across various networks and datasets marks a notable advancement. However enhancing the paper with more detailed theoretical insights broader generalization discussion and clarifications on computational overhead could further strengthen its contribution. Given the strong empirical results and the innovative approach to a pertinent problem in neural network optimization I recommend the publication of this paper subject to minor revisions that address the aforementioned feedback.", "tHx6q2dM86s": " Review of \"Adversarial Attacks Using Homoglyph Against Natural Language Web Services in the Physical World\"  1. Summary of the Paper This paper presents an exploration into the security vulnerabilities of MachineLearningasaService (MLaaS) particularly focusing on the domain of Natural Language Processing (NLP) services offered by major cloud providers such as Amazon Google IBM and Microsoft. The authors introduce an attack mechanism named HYPOCRITE which stands for homoglyph adversarial examples designed to exploit the security gaps in NLP services. The cornerstone of HYPOCRITE involves replacing English characters in texts with visually similar international characters (homoglyphs) to mislead AI models into making incorrect sentiment classifications without being detected by human supervisors. The authors demonstrate the efficiency of HYPOCRITE through extensive testing against four realworld NLP services showing its superiority over a baseline method in both the success rate of attacks and the proportion of text that needs to be altered for the attack to be effective. They further assess the human detectability of such attacks through a user study involving Amazon Mechanical Turk.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The approach of using homoglyph adversarial examples in the context of attacking NLP services is quite innovative addressing a critical and less explored security concern within AI.  Comprehensive Evaluation: The paper provides a thorough evaluation of the attack model against multiple realworld NLP services and includes a user study to gauge human detectability thereby substantiating the practical relevance of the findings.  Significant Findings: Demonstrating that HYPOCRITE outperforms existing baseline methods for adversarial attacks in NLP adds value to the ongoing discussions about AI security. Weak Points:  Generalization of Results: The study is limited to sentiment analysis services. It would be beneficial to see if HYPOCRITE can be effectively applied to other types of NLP tasks or models.  Ethical Considerations: The paper could better address the ethical implications of such adversarial techniques including potential misuse and strategies for mitigation.  Technical Detail: For replication and further development the paper could provide more detail on the experimental setup including the specific parameters used in testing and more insight into the baseline model for comparison. Questions and Feedback: 1. Could the authors elaborate on any attempts to notify the affected services about these vulnerabilities and any response or mitigation strategies proposed by them 2. Is there a potential for adapting HYPOCRITE to attack NLP models beyond sentiment analysis such as translation or summarization services Further how might defenses be developed to counteract such attacks 3. Given the reliance on specific character replacements how might language diversity (e.g. nonLatin scripts) affect the applicability and efficacy of the HYPOCRITE attack  3. Review Summary and Recommendation The paper makes a valuable contribution to the field of AI security specifically within the domain of NLP. The novel attack vector introduced HYPOCRITE not only reveals significant vulnerabilities in widely used MLaaS but also challenges the existing understanding of how adversarial attacks can be perpetrated and detected in natural language contexts. The comprehensive evaluation methodology including a user study enhances the credibility and relevance of the findings. However to strengthen the paper further the authors could expand the scope of their evaluations to include a broader range of NLP services and models. Addressing the ethical considerations more rigorously would also highlight the responsible handling of such powerful adversarial techniques. Enhancing the technical details provided would aid in the reproducibility of the study and foster further research into countermeasures against such attacks. Given the significance of the findings and potential for broad impact within the AI and cybersecurity communities I recommend acceptance of this paper contingent on a minor revision addressing the highlighted feedback points. Improving the generality of the findings and clarifying ethical considerations would make this work a substantial contribution to the literature on AI security.", "vtDzHJOsmfJ": " Peer Review for \"Efficient Algorithms for Fair Supervised Learning with Equalized Loss\"  Summary: The paper addresses the challenge of mitigating bias in supervised learning algorithms towards protected social groups by focusing on the Equalized Loss (EL) fairness notion. It highlights the inherent nonconvexity of the optimization problem when aiming for EL fairness even with convex loss functions. To tackle this the authors propose two algorithms: ELminimizer which finds the global optimum by converting the nonconvex problem into a series of convex optimizations and a simpler computationally more efficient algorithm for finding suboptimal EL fair predictors. The paper validates the effectiveness of these algorithms through experiments on realworld datasets.  Strong Points: 1. Novelty and Relevance: The pursuit of fairness in ML algorithms is highly relevant in todays context. The ELminimizer is a novel approach that addresses a previously underexplored aspect of fair ML  Equalized Loss. 2. Theoretical Depth: The paper offers a thorough theoretical analysis including proofs and theorems that support the proposed algorithms. It provides a comprehensive mathematical foundation for understanding the EL constraints implications. 3. Empirical Validation: The authors have done a commendable job of testing their algorithms on realworld datasets offering empirical evidence of their efficacy. This significantly enhances the papers credibility. 4. Practicality of Algorithms: The algorithms proposed are not only theoretically sound but are also presented as practically implementable with discussions on computational complexity and generalization performance.  Weak Points: 1. Comparison with Existing Algorithms: While the paper claims the superiority of the proposed algorithms a more detailed comparison with existing methods\u2014especially in terms of computational efficiency and fairness achieved\u2014would have been valuable. 2. Discussion on Application Limitations: The paper could benefit from a more detailed discussion on the practical limitations or challenges in applying the proposed algorithms across different domains or dataset sizes. 3. Broader Impact and Ethical Considerations: Given the focus on fairness the paper might have explored more deeply the ethical implications of EL fairness including potential misuse or oversimplification of fairness concepts.  Questions and Feedback to the Author: 1. Scalability: How do the proposed algorithms scale with increasingly large and highdimensional datasets Are there any practical limitations to their scalability 2. Algorithm Generalizability: Given the specific focus on EL fairness how generalizable are the proposed algorithms to other notions of fairness or to multitask learning scenarios 3. Impact of Approximations: The paper utilizes approximations (e.g. suboptimal EL fair predictor). Could you discuss the potential impact of these approximations on the fairness and accuracy of the predictions  Review Summary and Recommendation: The paper makes a significant contribution to the field of fair machine learning by introducing efficient algorithms for achieving Equalized Loss fairness. The theoretical and empirical work is robust providing a strong foundation for future research in this area. However the paper could be improved by addressing the comparative effectiveness of these algorithms against existing methods discussing the potential practical limitations and elaborating on the ethical considerations of applying EL fairness in realworld scenarios. Overall the strengths of the paper outweigh its weaknesses. The novel approach combined with theoretical rigor and empirical validation presents a compelling case for the publication of this work. However incorporating the suggested feedback could make the paper even stronger. I recommend this paper for publication contingent upon the consideration of the provided feedback.", "uVXEKeqJbNa": " Peer Review of \"StiffnessAware Neural Network (SANN) for Learning Hamiltonian Dynamical Systems\"  1. Summary of the Paper This paper introduces a stiffnessaware neural network (SANN) designed to learn Hamiltonian dynamical systems with improved accuracy by addressing the inherent stiffness in complex physical systems. SANN achieves this through a novel stiffnessaware index (SAI) that effectively distinguishes between stiff and nonstiff portions of training data. This distinction enables the application of different integration strategies and step sizes tailored to each portions dynamical characteristics. The authors validate SANNs performance against stateoftheart methods on two systems: a threebody problem and a billiard model demonstrating superior stability and energy preservation.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Relevance: The paper addresses the challenge of learning stiff Hamiltonian dynamical systems a notable problem in datadriven modeling of physical processes. The introduction of SAI as a means to categorize stiff and nonstiff data segments is both innovative and pertinent.  Experimental Validation: The authors provide comprehensive experiments comparing SANN with existing methods (HNN and SRNN) on complex physical systems. The impressive improvements in energy preservation and accuracy substantiate the papers claims.  Practicality: By incorporating a resampling technique to balance training data and applying step size adaptation SANN enhances training efficiency and model accuracy showcasing a practical approach to dealing with stiff dynamics. Weak Points:  Limited Discussion on Implementation Details: The manuscript could benefit from a more detailed discussion on the implementation complexities and potential limitations when applying SANN in realworld scenarios or on a broader range of systems.  Performance Evaluation: While the comparison with HNN and SRNN is valuable additional benchmarks against other recent advances in learning dynamical systems could further contextualize SANNs performance.  Theoretical Analysis: The paper presents a thorough empirical analysis but lacks a deep theoretical exploration of why SANN outperforms other methods significantly especially in terms of energy preservation. Questions and Feedback:  How does SANN perform with noisy data or incomplete datasets which are common in realworld applications  Could the authors elaborate on how SANN might be adapted or extended to other types of stiff dynamical systems beyond the Hamiltonian framework  The paper briefly mentions the potential impact of implicit bias. Could further explanation be provided on how this bias might affect SANN and ways to mitigate it  3. Review Summary and Recommendation This paper presents a significant advancement in the field of learning Hamiltonian dynamical systems particularly in addressing the challenge of system stiffness. The introduction of SANN and the novel SAI metric for stiffness awareness demonstrates both innovative thinking and practical implications for physical system modeling. The empirical results provided offer convincing evidence of SANNs superior performance in terms of stability and energy conservation compared to existing methods. However the paper would benefit from a more exhaustive theoretical discussion on the underlying reasons for SANN\u2019s effectiveness broader benchmarking to situate its performance within the landscape of recent advances and a more detailed exploration of its application potential in realworld scenarios. Based on the strengths of the proposed approach and the satisfactory experimental validation I recommend this paper for publication. Nevertheless I encourage the authors to address the raised questions and feedback to enhance the papers comprehensiveness and appeal to a broader audience.", "rI0LYgGeYaw": " PeerReview  1) Summary of the Paper This paper presents an indepth study of dictionary learning specifically focusing on a sparse representation of data derived from noisy datasets. Alternating minimization (AM) has been identified as a conventional method for dictionary learning but its computational cost is prohibitive for large datasets. The authors propose an alternate approximation method based on algorithm unrolling and compare it with AM to find a balance between computational speed and precision. They explore the convergence rates and asymptotic behavior of gradient estimates in both methods. The paper demonstrates that unrolling is advantageous in the initial iterations and supports the inner problem solution well. The methods efficacy is further validated through application in magnetoencephalography (MEG) pattern learning leveraging a stochastic algorithm to demonstrate its superiority over a stateoftheart method in terms of computational efficiency.  2) Strong and Weak Points of the Paper Strong Points:  The paper addresses a significant issue in dictionary learning: the high computational cost of alternating minimization making it a critical contribution to the field.  It presents a thorough theoretical analysis including propositions and theorems that elucidate the asymptotic behavior and convergence rates of the discussed methods.  The application of unrolling to MEG pattern learning is innovative showcasing the practical benefits of the approach in realworld data analysis.  The inclusion of numerical simulations and realworld applications to validate the theoretical findings strengthens the papers arguments. Weak Points:  The paper is heavily theoretical which may limit its accessibility to a broader audience not wellversed in the technical aspects of dictionary learning and optimization algorithms.  While the application to MEG pattern learning demonstrates practical utility additional realworld examples across different domains could further showcase the methods versatility.  The comparison between the proposed method and AM primarily focuses on computational efficiency and convergence rates. A deeper analysis of the quality of the learned dictionaries in terms of signal recovery accuracy would be beneficial.  The discussion on the limitations of evaluating dictionary quality using the proposed metrics could be expanded. An examination of alternative evaluation methods might provide a more comprehensive assessment. Questions and Feedback to the Author:  Could the authors elaborate on the potential realworld applications of their method beyond MEG pattern learning It would be interesting to see its applicability in different domains.  The paper mentions numerical instabilities in gradient estimation with increased iterations of unrolling. Can the authors suggest possible approaches to mitigate these instabilities  How does the proposed method deal with overfitting given that it focuses on approximations in solving the Lasso problem  Is there a plan to explore the superefficiency phenomenon of DDL in early iterations further and provide a theoretical basis for it  3) Review Summary The paper presents a valuable contribution to the field of dictionary learning by proposing an approximate approach based on algorithm unrolling aimed at reducing the computational burden of alternating minimization. The theoretical analysis accompanied by practical application in MEG pattern learning convincingly demonstrates the methods effectiveness and potential for broad applicability. However the paper could be improved by addressing its accessibility to a wider audience expanding its realworld applications and providing a more detailed analysis of dictionary quality assessment. Questions regarding the mitigation of numerical instabilities overfitting and the unexplained superefficiency of DDL warrant further investigation to strengthen the papers contributions. Overall this work is a promising step forward in making dictionary learning more feasible for largescale datasets with a recommendation for acceptance contingent on addressing the outlined concerns.", "xNOVfCCvDpM": "1) Summary of the Paper This paper embarks on a rigorous investigation into the capability of post hoc model explanations\u2014namely feature attribution concept activation and training point ranking\u2014to identify models\u2019 dependencies on spurious signals within training data especially when such signals are unknown to the user. Through the creation of semisynthetic datasets embedded with predefined spurious artifacts the study quantitatively evaluates the performance of these explanation methods in signaling the reliance of models on these artifacts. The insights gleaned are further augmented by a user study analyzing the practical effectiveness of these tools in realworld scenarios. Despite the promising utility of post hoc explanations highlighted in prior studies the findings here present a stark contrast suggesting a considerable ineffectiveness in detecting spurious signals particularly when these are nonvisible such as background blur calling into question the reliability of these methodologies in critical applications. 2) Strong Points Weak Points Questions and Feedback Strong Points:  The paper addresses a highly relevant and underexplored area in the interpretability of deep neural networks focusing on the critical issue of spurious signals in training data.  The empirical methodology adopted is comprehensive using semisynthetic datasets to introduce known spurious signals which allows for a controlled evaluation of post hoc explanation methods.  The inclusion of a blinded user study provides valuable insights into how these methods perform in practical settings offering a wellrounded perspective on the issue. Weak Points:  The study focuses solely on imagebased tasks which might limit the generalizability of its conclusions to other data modalities such as text or timeseries.  While the paper evaluates a diverse range of explanation methods the rapidly evolving landscape of post hoc explanations suggests that there might be other novel approaches that could potentially perform better but were not considered in this study.  The paper could benefit from a more rigorous analysis of the impact of different types of spurious signals on the various post hoc explanation methods particularly in understanding why certain methods fail under specific conditions. Questions and Feedback:  Could the authors elaborate on whether the ineffectiveness of post hoc explanations in certain scenarios could be mitigated by combining multiple explanation approaches  The paper would gain from a discussion on potential methodological improvements or entirely new directions in post hoc explanations that could address the identified shortcomings.  In the light of these findings how should practitioners adjust their approach to using post hoc explanations especially in highstakes domains 3) Review Summary The paper presents a significant contribution to the field of machine learning interpretability by scrutinizing the capacity of post hoc explanation methods to detect reliance on spurious signals within training data. Its methodological rigor combined with the relevance of the research question underscores the importance of the study. Despite its strengths the researchs focus on image data and the exclusion of other models or explanation types may limit its broader applicability. The unexpected findings\u2014that post hoc explanations often fail to identify unknown spurious signals especially nonvisible ones\u2014pose crucial questions about the reliability and utility of such methods in practice. Based on these observations it appears essential for future research to explore alternative approaches or enhancements to current methodologies that could improve the detection of spurious dependencies. Furthermore the results indicate a pressing need for practitioners to exercise caution and consider the limitations of post hoc explanations when deploying models in critical settings. Moving forward addressing the identified gaps could pave the way for more robust generalizable and trustworthy post hoc explanatory methods. Recommendations for the authors include broadening the scope of data modalities considered and offering more nuanced insights into the performance of different explanation types across a wider spectrum of spurious signals.", "qj1IZ-6TInc": " Peer Review for \"Neural Voice Camouflage: RealTime Adversarial Attacks Against Automatic Speech Recognition Systems\"  Summary This paper addresses the rising concerns around privacy with automatic speech recognition (ASR) systems embedded in smart devices which potentially eavesdrop on conversations. The authors propose a novel method dubbed \"Neural Voice Camouflage\" that aims to protect user privacy by camouflaging voices in realtime to impede ASR systems without disrupting human conversation. This is achieved through predictive attacks that forecast effective adversarial noise based on a short segment of the speakers voice adding this noise to the speech signal in realtime. Their approach shows a significant increase in word error rate (WER) and character error rate (CER) on the DeepSpeech ASR system demonstrating effectiveness in both controlled environments and realistic settings. They also highlight the resilience of their method against common defense mechanisms like denoisers and adversarial training. The experimental verification includes robustness assessments and evaluations under realworld acoustic conditions.  Strong Points 1. Timeliness and Relevance: The work directly addresses growing privacy concerns in the era of ubiquitous smart devices with embedded listening capabilities. It provides a novel solution catering to users need to protect their conversations. 2. Innovative Approach: The paper introduces a realtime predictive adversarial attack against ASR systems a significant advancement over existing methods that either are not realtime or target specific words or phrases. 3. Comprehensive Evaluation: The authors present a thorough evaluation of their method across different metrics (WER and CER) conditions (realworld rooms) and against various baselines and defenses demonstrating the methods effectiveness and practical applicability. 4. Ethical Considerations: It is commendable that the paper addresses ethical considerations noting the potential adverse effects and limitations of their approach which underlines the authors responsibility towards the impact of their work.  Weak Points 1. Dataset and Language Limitations: The reliance on Western speech data and English language might limit the methods applicability across different cultures and languages. It would benefit from an evaluation of its effectiveness across diverse linguistic datasets to understand its broader applicability. 2. Potential Misuse: While the paper addresses ethical considerations theres a potential risk of misuse of such technology by malicious actors to evade surveillance legitimately warranted for security reasons. Discussion on safeguards or countermeasures against such misuse would have been beneficial. 3. Effect on HumantoHuman Communication: Despite claiming not to inconvenience human conversation there was limited quantitative evidence on the nonintrusiveness of the adversarial noise on human listeners especially in diverse acoustic environments. 4. Robustness Against Future ASR Improvements: The paper successfully demonstrates effectiveness against current ASR systems and defense mechanisms. However it does not delve into the anticipated evolution of ASR technologies and how future systems could potentially mitigate such adversarial attacks.  Questions and Feedback to the Author 1. Can the authors elaborate on potential measures to ensure that this technology is not exploited by individuals for malicious purposes effectively turning a privacypreserving tool into one that enables illicit activities 2. Given the improvement of ASR systems over time do the authors speculate on the longevity of their methods effectiveness How adaptable is it to counteract advancements in ASR technology 3. Will the presence of adversarial noise generated by this method impact the quality of human communication in settings with complex acoustics or for individuals with hearing impairments  Review Summary This paper presents an innovative and timely approach to safeguarding user privacy in the face of pervasive ASRenabled devices. It proposes a novel realtime method capable of obstructing ASR transcription processes while maintaining the intelligibility of human speech. The experiments convincingly demonstrate the methods efficacy under various conditions against different ASR systems and in the face of common defense strategies. The inclusion of ethical considerations underscores the authors\u2019 awareness of the broader impact of their work. However the paper could benefit from a more indepth discussion on the methods scalability across languages its potential for misuse and its adaptability to future advancements in ASR technologies. Overall this work marks a significant contribution to the field of privacypreserving techniques in AI offering a solid foundation for further exploration and development.", "gEynpztqZug": " Summarize the paper The paper introduces Mako a novel semisupervised Lifelong Machine Learning (LML) framework designed to address the challenge of acquiring labeled data for each task in continual learning. Mako acts as a wrapper for existing supervised LML tools enabling them to perform semisupervised learning by leveraging a combination of a small amount of labeled data and a larger pool of unlabeled data. The key innovation lies in the application of data programming to automatically generate labels for the unlabeled data thus significantly reducing the reliance on expensive labeled datasets. Mako is tested across various LML task sequences derived from wellknown image classification datasets (MNIST CIFAR10 and CIFAR100) and demonstrates close performance to fully supervised LML methods in terms of task accuracy and the prevention of catastrophic forgetting without additional knowledge base storage overhead. The paper compares Makos performance against both supervised LML methods and existing semisupervised LML tools showing notable improvements.  Strong and Weak Points Questions and Feedback to the Author Strong Points: 1. Novelty: Mako represents a significant advancement in the LML field by introducing a generic approach to semisupervised lifelong learning that does not impose extra constraints on the LML framework or require additional knowledge base storage. 2. Performance: The comprehensive experimental setup and the extensive evaluation show that Mako can achieve performance close to fully supervised methods which is impressive given the use of partially labeled data. 3. Generality: The ability to mount Mako on top of existing LML tools without specific adaptations enhances its applicability and makes it an attractive solution for a wide range of LML problems. Weak Points: 1. Complexity in Practical Implementation: While Makos design is described as modular and generally applicable the paper does not fully address the practical complexities of integrating it with various LML tools particularly in terms of computational costs and tuning efforts. 2. Limited Evaluation in Diversity of Tasks: The evaluation mainly focuses on image classification tasks. Lifelong learning encompasses a wider array of task types and data modalities. The effectiveness of Mako across these varied settings remains an open question. 3. Insufficient Discussion on Hyperparameter Tuning: The paper briefly touches on automatic hyperparameter search but lacks a detailed discussion on the challenges of hyperparameter tuning in the context of semisupervised lifelong learning and how Mako addresses them. Questions and Feedback: 1. Can the authors elaborate on the computational overhead introduced by Mako when it is mounted on existing LML frameworks Specifically how does the automatic hyperparameter tuning and label generation process impact the overall training time and resource consumption 2. The paper predominantly showcases experiments on image classification tasks. Can the authors discuss the applicability and potential limitations of Mako in dealing with other types of tasks such as sequential decisionmaking or language understanding tasks 3. In the context of data programming the generation of weak labeling functions is crucial. The paper mentions leveraging Snuba for this purpose. Would the authors be able to provide insights into how Mako handles scenarios where the weak labelers perform poorly especially in complex or noisy data environments Review Summary: The paper presents Mako a promising semisupervised LML framework that tactically addresses the challenge of expensive labeled data by leveraging data programming for automatic label generation. Its ability to closely match the performance of fully supervised LML methods with minimal additional computational overhead represents a significant contribution to the field. However the paper would benefit from a more detailed discussion on the integration complexity broader applicability across diverse learning tasks and the intricacies of hyperparameter tuning and its impact on performance. Nevertheless Makos innovative approach and promising results make it a substantial step forward in the domain of lifelong machine learning opening avenues for further exploration and optimization in semisupervised learning settings.", "hdSn_X7Hfvz": "Summarize the Paper This paper delves into the realm of probability estimation from highdimensional data using deep neural networks differentiating itself from conventional binary classification by focusing on estimating probabilities of outcomes rather than the outcomes themselves a crucial aspect for realworld applications marred by inherent uncertainties. To tackle the challenge of evaluating performance without groundtruth probabilities the authors introduce a synthetic dataset alongside utilising three realworld datasets from disparate domains: weather forecasting cancer patient survival prediction and vehicle collision prediction. They critically evaluate existing methodologies and introduce a novel approach Calibrated Probability Estimation (CaPE) that enhances model training for better consistency with empirical probabilities. Through comprehensive evaluations CaPE is demonstrated to outperform existing methods on both the synthetic and realworld datasets. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovativeness: The introduction of CaPE which enables calibration during model training shows potential in not just retaining calibration but also in enhancing the discriminative ability of models. This contributes significantly to the field of probability estimation in highdimensional data scenarios. 2. Comprehensive Evaluation: The papers methodology stands out with its thorough evaluation spanning synthetic and three diverse realworld datasets. This broad approach helps in establishing the efficacy and generalizability of the proposed method. 3. Practical Relevance: By focusing on fields with inherent uncertainties such as medical prognosis and vehicle collision prediction the research addresses realworld needs ensuring its findings are of practical significance. Weak Points: 1. Methodological Transparency: While the paper introduces CaPE and discusses its superiority it could benefit from a more detailed explanation or illustration of how the calibration loss and discrimination loss are balanced or traded off during the training process. 2. Comparison Depth: Although the paper compares CaPE to existing methods a deeper dive into the reasons behind the underperformance of these methods other than the lack of calibration during training could provide more insight into the nuances of probability estimation. Questions and Feedback: 1. Extended Applicability: Could CaPE be adapted or tested for multiclass probability estimation scenarios It would be interesting to see how it performs outside the binary context. 2. Impact of Data Imbalance: Given the challenge presented by the Skewed scenario how sensitive is CaPE to imbalanced datasets and are there specific strategies employed within CaPE to mitigate such issues 3. Computational Efficiency: How does the computational efficiency of CaPE compare to existing methods especially when applied to largescale datasets Review Summary This paper makes a significant contribution to probability estimation in uncertain realworld scenarios by introducing a new method Calibrated Probability Estimation (CaPE) which is shown to outperform existing stateoftheart techniques on both synthetic and realworld data. The comprehensive evaluation across different domains and the papers focus on practical applications underscore the potential impact of this research. However the paper could benefit from elaborating on the practical implementation of balancing calibration and discrimination losses during training and from an extended discussion on the adaptability and efficiency of CaPE in different or more challenging scenarios. Overall the papers strengths considerably outweigh its weaknesses presenting a promising direction for future research in probability estimation with deep learning models. The recommendation is for acceptance with minor revisions addressing the feedback and questions.", "jGmNTfiXwGb": "Peer Review of \"A General Methodology for Approximating Offline Algorithms in Online Settings\" 1) Summary of the paper: The paper introduces a novel methodology for transforming offline algorithms to function in online settings through an endtoend differentiable multitask learning model. The core idea is to capture and predict the behavior of offline algorithms using a graphstructured labeling scheme allowing for the approximation of these algorithms future predictions based on past and present data. The authors apply their methodology to both synthetic data and historical stock market data showcasing its ability to bridge the gap between offline analysis and realtime prediction. By focusing on the behavioral output of offline algorithms rather than direct time series forecasting this work aims to offer a generalizable approach to making the vast suite of offline algorithms available for online applications. 2) Strong and Weak Points Questions and Feedback:  Strong points: 1. Novelty and Generality: The approachs novelty lies in its general application to any offline algorithm making it a significant contribution to both theoretical and applied machine learning. 2. EndtoEnd Differentiability: The methodologys design as an endtoend differentiable system allows for seamless integration with modern ML tools enhancing its adaptability and efficiency. 3. Practical Demonstrations: The application to synthetic data and historical stock market data effectively illustrates the methodologys potential offering concrete examples of its utility. 4. Reproducibility: Making all code data and configuration scripts available under the MIT License is commendable promoting transparency and facilitating further research.  Weak Points: 1. Scope of Experiments: The experiments are limited to 1D settings and a specific offline algorithm (`1 trend filtering). While the methodology is proposed as general additional demonstrations in multidimensional settings and with various algorithms might solidify its applicability. 2. Performance Benchmarking: The paper lacks a comparative analysis with existing methods that attempt to tackle similar problems. Such comparisons could provide a clearer view of the proposed methodologys advantages or areas for improvement. 3. Detailed Explanation of the Multitask Model: While the model architecture is described a deeper discussion on the choice of architecture especially the decisionmaking behind the number of layers their sizes and the choice of tasks would be beneficial for understanding and potential replication.  Questions and Feedback for the Author: 1. Could you provide more insight into the selection process for the multitask learning models architecture How might the methodology adapt to different architectures or models 2. The paper states its applicability to any offline algorithm. Could you elaborate on how the methodology might handle the complexity of multidimensional or multimodal data given that the demonstration was confined to 1D settings 3. Regarding synthetic data could the addition of varying degrees of noise or irregularities help test the methodologys robustness further 4. Could future work explore automatic identification of the most relevant decision points without prior knowledge of the specific offline algorithm being approximated This could potentially enhance the methodologys generality. 3) Review Summary: The paper presents an innovative and potentially impactful methodology for approximating offline algorithms in online settings addressing a significant gap in current machine learning applications. Its generality highlighted by its application to both synthetic and stock market data positions it as a valuable tool for various fields including finance healthcare and beyond. While the paper is strong in its innovative approach clarity of exposition and potential for wide applicability it falls short in benchmarking against existing methods and demonstrating its effectiveness in multidimensional settings. The authors are encouraged to address these limitations in future work by expanding the scope of their experiments offering a comparative analysis with other methodologies and providing a deeper theoretical and practical explanation of the chosen machine learning models. Additionally questions regarding the choice of model architecture and its adaptability suggest that further details could enhance the replicability of the research and its application to a broader array of algorithms and data types. Overall this work represents a significant step forward in the effort to bring the analytical power of offline algorithms into the realm of online processing. With further development and elaboration this methodology has the potential to become a cornerstone in the field of predictive modeling.", "tCx6AefvuPf": " Review of \"Learning Differentially Private Graph Neural Networks with NodeLevel Privacy\"  1. Summary The paper addresses the significant challenge of learning Graph Neural Networks (GNNs) while ensuring that sensitive information at the node level both features and connectivity is preserved. The authors recognize the limitations of current privacy preserving techniques such as Differentially Private Stochastic Gradient Descent (DPSGD) when applied to GNNs owing to the inherent aggregation of data from neighboring nodes in GNNs. To combat this they propose a novel approach that adapts DPSGD for GNNs ensuring nodelevel privacy by extending the privacy by amplification technique and conducting a sophisticated sensitivity analysis. Their empirical evaluations demonstrate the effectiveness of their proposed DPGNN method against standard nonprivate methods and private MLP methods on standard benchmarks making a case for GNNs feasibility in privacycritical applications.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper addresses a significant gap in the field of privacypreserving machine learning by focusing on nodelevel privacy in GNNs. Their novel adaptation of DPSGD to GNNs presents a significant advancement.  Solid Empirical Evaluation: The comprehensive experimental results provided offer convincing evidence that the DPGNN model outperforms both nonprivate GNNs and private MLP models in accuracy while ensuring privacy.  Broad Application Potential: The discussion on the implications of nodelevel privacy in GNNs across various domains (e.g. social networks organizational data) is insightful highlighting the broad applicability of the proposed method. Weak Points:  Assumptions on Graph Structure: The paper assumes a directed graph structure for their methods privacy guarantees. It remains unclear how their approach would adapt to undirected graphs which are common in many applications.  Scalability and Computational Efficiency: While the empirical evaluations are thorough there is limited discussion on the scalability and computational efficiency of the proposed method especially for very large graph datasets.  Impact on Rare Classes: The paper briefly mentions that their method like other differentially private models performs worse on lowfrequency classes. This aspect could significantly impact the practical deployment of the proposed method warranting a more detailed investigation. Questions and Feedback: 1. Adaptation to Undirected Graphs: How would your approach adapt to undirected graphs Are there significant modifications required in the sensitivity analysis or the privacy by amplification technique 2. Computational Efficiency: Could you elaborate on the computational efficiency and scalability of your method especially when applied to largescale graphs Are there any optimization techniques that could be employed to improve scalability 3. Performance on Rare Classes: Given the observed performance drop on lowfrequency classes are there any mitigation techniques or model adaptations you are considering to improve the performance on these classes without compromising privacy 4. Extension to Multilayer GNNs: Your study focuses on 1layer GNNs. What challenges do you foresee in extending your method to multilayer GNNs and how might these challenges be addressed  3. Review Summary The paper presents a highly relevant and innovative approach to learning GNNs with nodelevel privacy addressing a crucial gap in privacypreserving machine learning. The proposed method represents a significant advancement over existing techniques by ensuring the privacy of sensitive information at the node level in graphstructured data while maintaining accuracy. The empirical evaluations are particularly compelling demonstrating the methods effectiveness against both standard nonprivate models and private MLP alternatives. However the paper could benefit from a more detailed discussion on its limitations and potential areas for improvement. Specifically it would be valuable to address the methods adaptability to undirected graphs its computational efficiency and scalability and strategies to mitigate the reduced performance on rare classes. Exploring these aspects further could help in making the proposed DPGNN method more universally applicable and robust. Additionally extending the method to multilayer GNNs could significantly expand its applicability. Overall the work is a promising step toward making the deployment of GNNs feasible in privacysensitive scenarios offering solid theoretical foundations and empirically demonstrating practical effectiveness. Future work that addresses the outlined questions and feedback could make substantial contributions to both the field of graph neural networks and privacypreserving machine learning.", "ovRQmeVFbrC": "Peer Review of \"PARS: PseudoLabel Aware Robust Sample Selection\" 1) Summarize the paper: This paper introduces PARS a novel framework for learning with noisy labels (LNL) by leveraging the concept of pseudolabel aware robust sample selection. The key innovation of PARS lies in its hybrid approach that integrates sample selection noise robust loss functions and label correction methods in a unified training framework. The method exploits both original noisy labels and selfgenerated pseudolabels segregates training samples into ambiguous and noisy subsets based on confidence levels and applies tailored loss functions to each subset to handle label noise effectively. Extensive experiments on CIFAR10 CIFAR100 and Clothing1M datasets demonstrate that PARS significantly outperforms existing stateoftheart LNL methods especially in highnoise and lowresource settings. 2) Strong and weak points questions and feedback: Strong Points: a. Comprehensive Approach: PARS integrates the benefits of existing LNL strategies into a singular cohesive framework addressing their limitations and enhancing overall performance on noisy datasets. b. Significant Performance Gains: The paper reports substantial improvements over current methods particularly in high noise scenarios and lowresource settings which is a noteworthy achievement. c. Extensive Evaluation: The method is thoroughly evaluated across multiple benchmarks noise types and settings including a challenging realworld dataset providing strong empirical evidence of its effectiveness. Weak Points: a. Complexity and Implementation: The paper could have provided more details on the computational complexity and the practical challenges of implementing PARS especially given its hybrid nature. b. Comparative Analysis: While the paper compares PARS to stateoftheart LNL methods a deeper analysis or a qualitative comparison highlighting why PARS outperforms these methods in specific scenarios could enrich the findings. c. Broader Applicability: The focus is primarily on image classification tasks. Discussions on the potential adaptability of PARS to other domains such as NLP or audio albeit suggested for future work could have made the paper more comprehensive. Questions and Feedback: 1. Implementation Details: Could the authors provide more insights into the implementation complexities including computational resources and runtime comparisons with other methods 2. Feature Representation Learning: How does PARS handle feature representation learning in noisy label scenarios Is there a mechanism to prevent feature overfitting to noisy labels 3. Domain Adaptability: The authors mention future work in audio and NLP domains. Can they speculate on potential challenges and necessary adjustments for adapting PARS to these domains 4. Threshold Sensitivity: How sensitive is the models performance to the threshold parameters used in robust sample selection Is there a guideline for setting these thresholds 3) Review Summary: The paper presents PARS an innovative and comprehensive framework for learning with noisy labels across various noise levels and dataset sizes. By masterfully integrating and building upon the strengths of existing LNL techniques PARS achieves significant performance improvements highlighting its efficacy and potential as a robust solution for noisy label challenges. The empirical evidence across multiple datasets especially under extreme conditions strongly supports the claims made by the authors. However the paper could be improved by addressing certain weaknesses such as the lack of detailed discussion on implementation complexity a more nuanced comparison with existing methods and a brief exploration of the applicability in domains beyond image classification. Addressing these points would enhance the papers contribution and provide readers with a more rounded understanding of PARS. Overall the strengths of the paper outweigh its limitations making it a significant contribution to the field of learning with noisy labels. With its promising results and innovative approach PARS sets a new benchmark for future research in this area.", "m7zsaLt1Sab": " Peer Review on \"Understanding the Representation of Context in Contextualized Word Embeddings: A Category Theory Approach\"  1. Summary This paper presents a novel perspective on contextualized word embedding by focusing on how models represent context rather than the conventional tokencentric approach. The authors argue that current \"tokencentric\" understandings do not adequately explain model representations of context. They introduce a rigorous method to define and analyze context representation using category theory aiming at a deeper understanding of how contextual information is encoded and represented. They apply manifold learning methods to probe and characterize the representation of context in BERT examining changes in context representation through the models attention and FFN components. The paper contributes to the field by addressing the gap in understanding context representation in contextualized embedding models providing a theoretical framework and offering empirical insights into the behavior of BERT.  2. Strong Points and Weak Points Strong Points: a. Innovative Theoretical Framework: The application of category theory to formalize the representation of context is novel and intellectually stimulating. This theoretical advancement helps in moving beyond traditional tokencentric analyses. b. Comprehensive Methodology: The papers methodology combining theoretical formulations with empirical analysis using manifold learning is thorough and wellstructured. It illustrates a nuanced approach to dissecting the complex operations capturing context. c. Insightful Empirical Results: The case studies and experimental results provide valuable insights into how BERT encodes context offering a more grounded understanding of Transformerbased models behaviors. The findings on the balancing act between attention and FFN layers are particularly intriguing. Weak Points: a. Complexity and Accessibility: The heavy theoretical focus particularly the use of category theory might be a barrier for readers not wellversed in abstract mathematics. Simplifying the exposition or providing more intuitive explanations could enhance the papers accessibility. b. Experimental Validation: While the empirical analyses are insightful the paper could benefit from a wider range of experiments or datasets. This could bolster the generalizability of the findings. c. Connection to Practical Applications: The paper is more theoretical and could be strengthened by discussing the implications of its findings on downstream applications or suggesting potential uses of the proposed context representation analysis. Questions and Feedback: 1. Could the authors provide more intuitive explanations or illustrations of the category theory concepts for readers not familiar with this mathematical area 2. Are there specific downstream NLP tasks where the insights from this paper could be directly applied to improve model performance or interpretation 3. How do the authors envision their theoretical framework being operationalized in practical model development or tuning scenarios  3. Review Summary The paper makes a valuable contribution to the understanding of contextual representation in neural language models presenting a novel and rigorous approach grounded in category theory. Its strength lies in the innovative theoretical framing and the detailed empirical investigation into how BERT encodes context. The findings offer fresh perspectives on the interactions between different components of Transformerbased models and their implications for context representation. However the papers heavy reliance on complex mathematical formulations may limit its accessibility. Future iterations could benefit from simplifying these concepts and providing more intuitive explanations. Expanding the empirical evaluations and explicitly connecting the theoretical insights to practical applications or specific NLP tasks could also enhance the impact of the work. Overall this paper is an intellectually rich and significant step forward in understanding contextualized embeddings promising to spur further research in the area. Its recommendations for leveraging category theory and manifold learning to probe language models are wellsupported and pave the way for more nuanced analyses of context in NLP.", "rl8jF3GENq": " Review of the Paper on Wavelet Packetbased Deepfake Analysis and Detection  Summary The paper introduces a novel methodology for detecting synthesized fake images specifically targeting those generated by Generative Adversarial Networks (GANs). It leverages a multiscale wavelet packet representation which enables localized analysis in both spatial and frequency domains to differentiate between natural and GANgenerated images. This approach fills a gap in existing literature by combining spatial and frequency information for image source identification providing a more comprehensive analysis tool. The authors evaluate their model on standard datasets including FFHQ CelebA and LSUN demonstrating competitive or superior performance in separating GANgenerated images from real ones. Additionally the forensic classifiers built on this method are efficient with a small network size facilitating quick learning and potentially broader application.  Strong Points 1. Innovative Approach: The use of wavelet packets for analyzing GANgenerated content is a novel contribution that enhances the detection framework by integrating spatial and frequency information. 2. Comprehensive Evaluation: The method is thoroughly tested on several benchmarks (FFHQ CelebA LSUN) comparing its performance not only in detecting GANgenerated images but also in distinguishing between different GAN architectures. This thorough evaluation demonstrates the robustness and versatility of the approach. 3. Efficiency and Practicality: The models small size and the efficiency of its learning process make it practical for a wide range of applications. This could significantly impact realworld deployment where computational resources might be limited. 4. Opensource Contribution: Releasing the source code and providing a publicly available wavelet toolbox significantly contribute to the research community enabling further research and application of the developed methods.  Weak Points 1. Boundary Wavelets Implementation: While the paper discusses the implementation of boundary wavelets to manage the finite size of images it does not delve deeply into the potential limitations or drawbacks of this approach compared to alternatives (e.g. symmetrization periodic extension). 2. Realworld Applicability: While the method shows promising results in a laboratory setting its applicability and effectiveness in realworld scenarios especially against the latest and potentially unreleased GAN architectures are not sufficiently discussed. 3. Lack of Comparison to StateoftheArt: Although the paper compares with DCTbased approaches a comparison with the latest stateoftheart methods in deepfake detection including those leveraging deep learning or hybrid models is missing. 4. Impact of Hyperparameter Tuning: There is limited discussion on how different hyperparameters (e.g. wavelet levels types of wavelets) impact the models performance and the rationale behind the chosen configurations.  Questions and Feedback to the Author 1. Boundary Wavelets Implementation Details: Could the authors provide more insight into the choice of boundary wavelets over alternative methods for handling image borders A discussion of the tradeoffs involved would be valuable. 2. RealWorld Deployment: How would the proposed method perform in detecting GANgenerated images in a realworld scenario where images might not be as standardized as those in controlled datasets 3. StateoftheArt Comparison: Could the authors elaborate on how their method compares with the latest stateoftheart deepfake detection techniques especially in terms of detection accuracy and computational requirements 4. Hyperparameter Sensitivity Analysis: It would be enlightening to see an analysis of the model\u2019s sensitivity to various hyperparameters including the type and levels of wavelets used. How might these choices affect the methods applicability to different types of image data  Review Summary The paper presents a highly innovative and potentially impactful approach to the timely issue of detecting GANgenerated images by utilizing wavelet packet representations. This methodology provides a significant advance by incorporating spatial and frequency information. The evaluation demonstrates the methods robustness across different datasets and settings highlighting its potential for diverse applications. However further clarification on its realworld applicability comparison to stateoftheart techniques and the impact of hyperparameter choices would strengthen the paper. The provision of source code and a wavelet toolbox is commendable and supports the communitys further exploration and application of this work. Overall the paper makes a valuable contribution to the field of deepfake detection with areas for enhancement mostly centered around broadening the discussion on its application scope and comparative effectiveness.", "r9cpyzP-DQ": " Review of \"Learning Ordinary Differential Equations via Diffeomorphism and Base ODE Modeling\"  Summary The paper presents a novel methodology for learning ordinary differential equations (ODEs) from data by incorporating diffeomorphisms and base ODE models to enhance the integration speed and stability. The authors address the challenge of slow and unstable integration in learning systems of ODEs from long sequences by proposing an approach that views the target ODE dynamics as a vector field related to a simpler base vector field via a diffeomorphism. The base ODE is chosen to be either linear or modeled by a neural network to ensure ease of integration. Key contributions include a new paradigm for ODE learning significant integration speedups\u2014especially with linear base ODEs and a method to ensure asymptotic stability of learned ODEs. The effectiveness of this approach is demonstrated through training and evaluating benchmark ODE systems and continuousdepth neural network models showing speedups of up to two orders of magnitude.  Strengths and Weaknesses Strong Points: 1. Innovative Approach: Proposing a novel framework that combines diffeomorphisms with simpler base ODE models is innovative and addresses a significant challenge in learning complex ODE systems. 2. Significant Speedups: The paper demonstrates substantial speed improvements in integrating learned ODEs especially when using linear base ODEs which could be a significant advancement for timecritical applications. 3. Asymptotic Stability: Incorporating a methodology to ensure the asymptotic stability of the learned ODEs is a strong point contributing to the robustness and reliability of the models. Weak Points: 1. Generalizability Concerns: There is a lack of discussion on the limitations regarding the types of ODEs that can be effectively modeled using this approach. Its unclear how well the method would perform on extremely chaotic or complex systems beyond the presented benchmarks. 2. Implementation Details: The paper could benefit from more detailed explanations or pseudocode on the implementation of the diffeomorphism learning via invertible neural networks aiding reproducibility. 3. Comparison with StateoftheArt: While the paper shows improvements over traditional differentiable integrators a more comprehensive comparison with stateoftheart methods in ODE learning including benchmarks on a wider variety of systems could strengthen the claims. Questions and Feedback to the Author: 1. Generalizability to Chaotic Systems: Can the authors comment on the methods performance and limitations when applied to highly chaotic systems or systems with unknown instabilities 2. Impact of Dimensionality: How does the method scale with the dimensionality of the system being modeled especially when using neural networks for base ODE modeling 3. Tradeoffs in Choosing Base ODE Model: Could the authors provide more insight into the tradeoffs between using linear models and neural networks as the base ODE How should one decide which to use for a given problem  Review Summary This paper introduces a promising approach to learning ODEs by leveraging diffeomorphisms and simpler base ODE models to enhance integration speed and stability. The methodology presents an innovative solution to the slow and unstable integration of complex ODE systems and the demonstration of significant speedups and ensured asymptotic stability are convincing strengths of this work. However the paper could be improved by addressing concerns regarding the generalizability of the method to a broader range of complex systems providing more detailed implementation guidance and offering a more comprehensive comparison with other stateoftheart approaches. Overall the paper makes a valuable contribution to the field of ODE learning and continuousdepth models potentially impacting a wide range of applications in natural sciences engineering and deep learning.", "zRJu6mU2BaE": " Review Summary  1) Summarize the paper The paper introduces a novel framework ConFeSS (Contrastive Learning and Feature Selection System) for tackling the challenge of large domain shift in fewshot learning. Unlike traditional fewshot learning methods that perform poorly on datasets with significant domain differences from the training set ConFeSS utilizes a threestep approach: unsupervised pretraining of a backbone network using contrastive loss learning a masking module to select relevant features for the target domain and finetuning both the pretrained backbone and a classifier with these relevant features. The proposed method aims to improve generalization on novel categories across distinct domains. The framework was tested on a crossdomain fewshot learning benchmark showing competitive performance against other metalearning approaches and recent crossdomain methods.  2) List strong and weak points of the paper Question and Feedback to the author Strong Points:  Novel Approach: The introduction of a contrastive learning phase and a feature masking module for fewshot learning across domain shifts is innovative. It tackles the challenge of generalizing from source to significantly different target domains effectively.  Comprehensive Evaluation: The framework is evaluated on a challenging benchmark that includes a wide range of domain shifts demonstrating its effectiveness across various datasets.  Theoretical Insights: The paper provides indepth theoretical motivations for the choice of contrastive learning and feature selection which strengthens the overall argument for the proposed method. Weak Points:  Lack of Detailed Comparison: Although the paper presents comparisons with other methods it could provide a more indepth discussion on why certain methods perform poorly on the benchmark and how ConFeSS specifically addresses these shortcomings.  Dependency on Domain Similarity: While the paper showcases improvement across different domains there\u2019s a limited exploration into how domain similarity impacts the effectiveness of the feature selection mechanism.  Generalization to Other Tasks: The paper focuses on classification tasks within fewshot learning. Expanding the evaluation to include other types of tasks (e.g. detection or segmentation in fewshot settings) could further validate the framework\u2019s utility. Questions and Feedback:  How does the feature masking module handle extremely dissimilar domains where almost none of the source domain features are relevant Could you elaborate on the mechanism\u2019s robustness in such scenarios  Can you provide insights into how the method would scale with the increasing complexity of the target domain especially when moving beyond image data to more structured or sequential data types  It would be beneficial to see a sensitivity analysis of the method\u2019s performance with respect to the choice of hyperparameters particularly those related to the feature masking module and the finetuning step.  3) Review Summary (Supporting arguments for recommendation) Overall the paper presents a significant advancement in the field of crossdomain fewshot learning by introducing the ConFeSS framework. The novel integration of contrastive learning for unsupervised feature extraction and a feature masking module for domainspecific relevance determination addresses the critical challenge of domain shift in fewshot learning. The comprehensive experimentation and theoretical motivations strongly support the effectiveness of the proposed method. However to strengthen the paper further the authors should consider expanding their comparison and analysis section. A deeper dive into why existing methods fall short and a more detailed sensitivity analysis on hyperparameters would provide readers with greater insights into the method\u2019s robustness and adaptability. Moreover exploring the framework\u2019s applicability to different tasks and data types could significantly increase its relevance to broader applications in the field. Given the strong performance of ConFeSS on a challenging benchmark and the novelty of the approach the paper makes a valuable contribution to the field. With some enhancement in the comparative analysis and a broader evaluation scope this work could serve as a cornerstone for future research in crossdomain fewshot learning.", "m8uJvVgwRci": " Summarize the paper This paper introduces the concept of Weak Indirect Supervision (WIS) a novel paradigm in machine learning that addresses the challenge of creating labeled training sets using indirect supervision sources with different output label spaces. The authors propose a Probabilistic Label Relation Model (PLRM) that leverages userprovided label relations to synthesize training labels from these indirect sources. They theoretically establish the models capability to distinguish unseen labels and provide a generalization error bound. The effectiveness of PLRM is demonstrated through experiments on image and text classification tasks and an industrial application outperforming adapted baselines.  Strong points 1. Innovative Approach: The concept of leveraging indirect supervision sources for synthesizing training labels is novel and addresses a significant bottleneck in the application of machine learning models. 2. Comprehensive Theoretical Foundation: The paper thoroughly establishes the theoretical underpinnings of the PLRM including a generalization error bound and a test for label distinguishability which adds to its credibility and utility. 3. Practical Relevance: Demonstrating the models effectiveness in realworld scenarios especially in an industrial setting underscores the practical relevance and potential impact of this research. 4. Improved Performance: The reported improvement in model performance over the baselines highlights the efficacy of the proposed approach.  Weak points 1. Dependency on UserProvided Relations: The success of the model relies on the availability of accurate label relations the creation of which can still demand significant manual effort and domain knowledge. 2. Complexity of Model: The introduction of a new probabilistic model along with label relation dependencies could complicate the understanding and implementation for practitioners. 3. Scalability Concerns: Although the paper demonstrates the models effectiveness on selected datasets and an industrial application the scalability of the approach to a wider range of problems or larger datasets is not clear. 4. Adaptation Versus Innovation: While the introduction of WIS is innovative the adjustments to baseline models and the related work suggest incremental improvements over existing weak supervision frameworks.  Questions and Feedback to the author 1. Could you elaborate on strategies for efficiently generating or obtaining the necessary label relations with minimal human intervention 2. How does the PLRM handle scenarios where userprovided label relations may be inaccurate or incomplete 3. Considering the complexity introduced by PLRM are there simplified versions or tools planned to facilitate adoption by practitioners 4. It would be interesting to see a scalability analysis of the model possibly through stresstesting the approach with increasingly larger datasets or more complex label spaces. 5. Further insights into the comparative analysis between direct and indirect supervision models in varying contexts (beyond the scope of the experiments conducted) would be beneficial.  Review Summary The paper presents a significant advancement in the field of machine learning by introducing Weak Indirect Supervision and proposing a Probabilistic Label Relation Model to efficiently create labeled training datasets from indirect supervision sources. The theoretical groundwork laid for PLRM and its demonstrated empirical effectiveness represent major strengths of this work. However the reliance on userprovided label relations model complexity and concerns regarding scalability and practical implementation suggest areas for improvement and further research. The innovative approach notably advances the realm of weak supervision by broadening the applicability of machine learning models in scenarios where direct supervision is not feasible. Despite its promising results the paper would benefit from addressing the outlined limitations and questions particularly around the scalability automation of label relations and simplification for practical utility. Overall this research opens new avenues for leveraging indirect supervision sources promising significant reductions in manual annotation efforts. Its contributions are both theoretically significant and practically relevant suggesting a strong potential for widespread impact in the machine learning domain.", "wronZ3Mx_d": " Peer Review for \"Deep Kernel Gaussian Process with Landmark Metafeatures for Efficient Hyperparameter Optimization\"  1. Summary of the Paper This paper introduces a novel method dubbed Deep Kernel Gaussian Process with Landmark Metafeatures (DKLM) designed to improve Hyperparameter Optimization (HPO) by leveraging transfer learning and a unique approach to metafeature extraction. The authors identify the negative transfer phenomenon as a critical issue in Bayesian HPO and propose conditioning Gaussian Process (GP) surrogates on datasetspecific metafeatures to tackle this issue. Unlike previous methods DKLM employs a deep neural network to generate these metafeatures directly from the data avoiding the need for manual feature crafting by domain experts. The method is metalearned on a collection of source tasks enabling efficient transfer to new unseen tasks. The efficacy of DKLM is demonstrated through extensive experiments on the OpenML platform showcasing superior performance against a range of stateoftheart HPO methods.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The paper presents a unique approach to conditioning GP surrogates on metafeatures extracted via a deep neural network addressing the challenge of negative transfer in HPO.  Comprehensive Evaluation: The experimental validation on a largescale metadataset consisting of 3.4 million hyperparameter evaluations over 101 datasets provides robust evidence of the methods effectiveness.  Practical Impact: By enabling more efficient HPO via transfer learning DKLM has the potential to significantly reduce the computational resources required for deploying machine learning models which is both economically beneficial and environmentally friendly. Weak Points:  Complexity and Accessibility: The methods reliance on deep learning adds layers of complexity that might make it inaccessible or impractical for smallerscale projects or teams with limited computational resources.  Generalization Across Different Domains: While the method shows empirical superiority on the tested datasets the paper lacks discussion on its applicability and performance across a wider range of domains including those with significantly different data characteristics.  Experimental Comparisons: While the paper compares DKLM against a variety of baseline methods it does not include comparisons against simpler metalearning or fewshot learning approaches that could provide a different perspective on the methods relative advantages. Questions and Feedback:  How does the methods performance vary with the complexity of the target task It would be helpful to understand if there are specific types of tasks or domains where DKLM is particularly effective or less beneficial.  The paper would benefit from a deeper analysis of the computational costs associated with training and transferring the proposed model including comparisons with the baselines to give readers a clearer view of the tradeoffs involved.  Could the authors elaborate on potential limitations or challenges in scaling DKLM to even larger datasets or when working with realtime or streaming data environments  It would be enlightening to see an exploration of the interpretability of the learned metafeatures. Understanding what characteristics of the metafeatures contribute to successful transfer could provide valuable insights into the methods inner workings.  3. Review Summary The paper addresses a critical and challenging issue in the domain of HPO by proposing a novel and promising approach that leverages landmark metafeatures for efficient transfer learning. The methods empirical validation demonstrates superior performance over a comprehensive set of baselines marking a significant advancement in the field. However questions remain regarding its complexity scalability and generalizability across diverse domains which the authors could explore in future work. Moreover a comparative analysis with simpler metalearning approaches and an examination of the computational tradeoffs involved would provide readers with a more comprehensive understanding of the methods practical implications. Overall the paper represents an important contribution to the machine learning community particularly in improving the efficiency and effectiveness of HPO processes.", "z2zmSDKONK": " Peer Review for \"Training Robustness of Distributional RL in Noisy Environments\"  1. Summary This paper investigates the robustness of distributional reinforcement learning (distributional RL) against state observation noises during training compared to expectationbased reinforcement learning (expectationbased RL). The authors introduce a StateNoisy Markov Decision Process (SNMDP) model to analyze both random and adversarial state observation noises. Through theoretical analysis and extensive experiments they demonstrate that distributional RL exhibits better robustness in noisy state observation settings compared to expectationbased RL. Key contributions include the introduction and analysis of SNMDP characterization of the vulnerability of expectationbased RL and the robustness advantages of distributional RL under noisy state observations and empirical evidence supporting theoretical claims.  2. Strong Points  Novelty and Practical Relevance: The paper addresses an important and realistic problem of state observation noises in RL offering insights into the robustness properties of distributional RL compared to expectationbased RL. This is highly relevant for deploying RL agents in realworld applications.  Solid Theoretical Foundation: The authors present a comprehensive theoretical analysis including the introduction of the SNMDP model proofs of contraction of both expectationbased and distributional Bellman operators and the derivation of bounded gradient norms for distributional losses.  Empirical Validation: An extensive suite of experiments provides solid backing for the theoretical claims. The comparison between expectationbased and distributional RL algorithms across different game environments under noisy conditions enriches the papers findings.  3. Weak Points  Assumptions and Generalizability: The paper might benefit from a discussion on the generalizability of the findings beyond the tabular setting and specific noise models considered. Clarifying the assumptions and exploring potential extensions could enhance the impact.  Analysis on the Choice of Distributional Losses: Although histogrambased distributional loss is analyzed other possible distributional loss functions like Wasserstein or Cram\u00e9r distance are mentioned but not explored. Insight into how different choices of distributional losses impact robustness would be valuable.  Complexity and Accessibility: The paper is rich in technical content which while thorough might be daunting for readers unfamiliar with the nuanced aspects of RL theory. It could benefit from a more extensive explanation or simplification of key theoretical contributions for broader accessibility.  4. Question and Feedback to the Author  On Generalization: Could the authors comment on how their findings might generalize to continuous action spaces or other forms of state observation noise not covered in this analysis  On Distributional Loss Choices: Why was the histogrambased distributional loss specifically chosen for analysis How might the use of other distributional losses affect the training robustness of distributional RL  Methodology Clarification: Can the authors elucidate the decision behind selecting the games for experimentation How representative are these games of the broader domain of potential RL applications  Review Summary The paper provides significant contributions to understanding the robustness of distributional RL in the presence of noisy state observations crucial for practical deployments of RL systems. The theoretical analyses are robust and empirical evidence strongly supports the claims made. While it excels in depth it could be improved by addressing assumptions more directly considering a broader set of distributional losses and enhancing accessibility for nonspecialist readers. Overall the work is a valuable addition to the RL literature especially for research focused on the practical and safe application of RL algorithms in noisy realworld environments.", "qjN4h_wwUO": " Review of \"Maximizing Gradient Norms for Efficient Neural Network Growth\"  Summary This paper introduces a novel approach dubbed Gradient Maximizing Growth (GradMax) to expand the architecture of neural networks dynamically without the need for costly retraining. Contrary to traditional methods that optimize network architecture and parameters independently GradMax focuses on growing the network by adding new neurons in a manner that does not affect already learned information thereby circumventing the extensive computational resources typically required. This is achieved by ensuring that the addition of new neurons maximizes the gradient norm of newly added weights with the optimal initialization determined through Singular Value Decomposition (SVD). The effectiveness of GradMax is demonstrated across various vision tasks and architectures suggesting it as a practical solution to the problematic enlargement of network architectures.  Strong Points 1. Innovation: The GradMax method introduces a distinct approach to growing neural networks focusing on the dynamic expansion of the architecture that preserves the learned information and enhances training dynamics. 2. Methodological Rigor: The use of SVD to find the optimal initialization of new neurons is mathematically robust ensuring that the gradients of the new weights are maximized effectively. This approach is wellsupported by theoretical underpinnings detailed in the paper. 3. Broad Applicability: Demonstrations of GradMaxs effectiveness across various vision tasks and architectures signify its versatility and potential as a generalized solution for efficient network growth. 4. Open Source Code: The decision to opensource the implementation of GradMax is commendable allowing for community validation improvements and adaptations to different problems.  Weak Points 1. Limited Exploration of Activation Functions: The paper mentions the requirement for activation functions to map zero to zero and have a nonzero derivative at the origin which may not hold for all functions. This limitation restricts the applicability of GradMax across different network designs. 2. Comparison with Other Methods: While the paper outlines the advantages of GradMax over previous approaches a more detailed comparative analysis with contemporary methods including a quantitative evaluation would offer a clearer understanding of its relative performance. 3. Impact on Overfitting: Theres a relative lack of discussion on how growing the network with GradMax affects the models generalization capabilities and whether it introduces a higher risk of overfitting especially in scenarios with limited data. 4. Computational Efficiency: Although the paper claims GradMax avoids costly retraining it does not provide a comprehensive analysis of the computational savings in comparison to traditional growth methods or the potential increase in computational demand due to SVD calculations during training.  Questions and Feedback 1. Activation Function Limitation: Can the authors elaborate on potential solutions or workarounds for the limitation posed by specific activation functions that do not meet the zeromapping criterion 2. Detailed Comparative Analysis: Could the authors provide a more exhaustive comparison with existing network growth methodologies including metrics such as computational efficiency scalability and impact on model generalization 3. Overfitting Concerns: How does GradMax impact the model\u2019s ability to generalize to unseen data Are there any mechanisms within GradMax to mitigate potential overfitting as the network grows 4. Computational Efficiency Clarification: Could the authors offer more detailed insights or empirical evidence on the computational efficiency of GradMax especially concerning the SVD calculations  Review Summary GradMax presents an innovative and theoretically sound approach to growing neural network architectures efficiently by maximizing the gradient norm of new weights. Its application across various tasks demonstrates its potential as a versatile solution for dynamically expanding neural networks without the extensive computation typically associated. However the paper could benefit from a more detailed exploration of its limitations especially concerning activation functions that do not conform to GradMaxs requirements and a comprehensive comparative analysis with existing methods. Additionally addressing potential impacts on model generalization and providing clearer insights into computational efficiency would enhance the papers contribution to the field. Overall GradMax represents a promising direction in neural network architecture optimization warranting further investigation and refinement.", "o9DnX55PEAo": " Peer Review for \"Distilling Pretrained Language Models into Efficient Matrix Embeddings for NLP\"  1) Summary of the Paper This paper addresses the challenge of deploying large pretrained language models (PreLMs) in resourceconstrained environments. It proposes a novel method for distilling PreLMs into a more efficient architecture named CMOWCBOWHybrid which combines matrix and vector embeddings to capture word order effectively. This model is enhanced with bidirectionality and a separate encoding scheme for improved handling of sentence pairs in downstream tasks like natural language inferencing. The authors present a comprehensive experimental analysis showing that their method yields competitive performance with existing sizereduced models like DistilBERT on the GLUE benchmark while offering significant gains in inference speed and parameter efficiency. Importantly this work also demonstrates the potential of crossarchitecture distillation moving beyond the confines of distilling into smaller versions of the same architecture as the teacher model.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper tackles a highly relevant problem in NLP by offering a novel solution to make large PreLMs more accessible for smallerscale research settings and devices with limited computational resources.  The proposed CMOWCBOWHybrid model is innovative combining the strengths of matrix embedding to capture word order with the efficiency of vector embeddings. This hybrid approach is welljustified and clearly explained.  An extensive experimental evaluation demonstrates the effectiveness of the proposed distillation method across several tasks of the GLUE benchmark. The comparison with established benchmarks like ELMo and DistilBERT helps underscore the competitive performance and efficiency of the proposed model. Weak Points:  The paper focuses predominantly on GLUE benchmark tasks for evaluation which might not fully reveal the potential limitations or weaknesses in handling other types of NLP challenges such as opendomain question answering or more nuanced text generation tasks.  While the paper mentions the environmental and economic benefits of smaller models it does not provide a detailed analysis of energy consumption or cost reductions. A quantitative analysis could strengthen these claims.  The discussion on limitations primarily focuses on performance comparison and does not delve deeply into potential architectural or conceptual limitations of the CMOWCBOWHybrid approach itself. Questions and Feedback:  The authors mention that linguistic acceptability remains a challenge. Could incorporating a small attention mechanism as suggested in future work potentially compromise the efficiency gains How do the authors envision balancing complexity and performance  Can the authors elaborate on how the proposed model might be adapted or extended for nontext sequence encoding tasks as briefly suggested towards the end of the paper  Feedback: It would be beneficial to include more nuanced discussions or preliminary studies on the models performance outside the GLUE benchmark or in more computationally constrained settings to strengthen its generalizability claims.  3) Review Summary In conclusion this paper presents a compelling approach to making PreLMs more accessible and efficient through an innovative distillation process into a CMOWCBOWHybrid model. Its contributions are significant offering a potent blend of competitive performance with marked improvements in inference speed and parameter efficiency. This work not only advances the current stateoftheart in language model distillation but also opens new avenues for future research in crossarchitecture distillation and beyond. While it stands strong on its methodological rigor and experimental validation expanding its evaluative horizons and addressing potential conceptual limitations would further its impact. The potential environmental and economic benefits highlighted are noteworthy though a more detailed quantitative analysis could strengthen this argument. Nonetheless this paper constitutes an important step forward in making advanced NLP models more universally accessible and environmentally sustainable.", "kl8flCo98nm": " Review of \"Parameter Estimation for ReLU Neural Networks in the Presence of Outliers\"  1) Summarize the paper This paper addresses the significant problem of estimating the parameters (weight matrix and bias vector) of a singlelayer neural network (NN) with ReLU activation in the presence of arbitrary outliers under the unsupervised learning framework. The authors propose a twostep algorithm that estimates the norms and angles related to the NNs parameters by utilizing gradient descent (GD) combined with median or trimmed meanbased filters to lessen the effect of outliers. The main theoretical contribution is the calculation of sample complexity showing that the algorithm can achieve estimation within a specific error margin with high probability depending on the uncorrupted sample probability the number of samples and the problem dimensionality. The paper also includes an empirical validation of the proposed method demonstrating its effectiveness in robustly estimating NN parameters against the presence of outliers.  2) Strong and Weak Points Questions and Feedback to the Author Strong Points: a) Novelty and Practicality: The formulation of the problem itself is novel targeting the realistic scenario where datasets are contaminated with outliers. The approach of combining GD with robust statistical methods (median or trimmed mean filters) for dealing with outliers is practical and innovative. b) Theoretical Depth: The derivation of sample complexity for parameter estimation in the presence of arbitrary outliers adds significant theoretical value. It provides a comprehensive understanding of how robust estimation depends on various factors such as the probability of a sample being uncorrupted sample size and problem dimensionality. c) Empirical Validation: The empirical results corroborate the theoretical findings and showcase the robustness of the proposed method against outliers. The comparison with benchmark schemes and the observation of the effects of different parameters (e.g. sample being uncorrupted probability) on estimation accuracy are thoroughly analyzed. Weak Points: a) Assumptions Limitation: The assumption about the nonnegativity of the bias vector could limit the applicability of the algorithm to a broader range of problems where this condition does not hold. b) Method Complexity and Computation Time: While the paper provides a comparison of runtime between the proposed and existing methods an indepth analysis of the computational complexity and potential scalability issues to larger networks or datasets is missing. c) Extensions to Multilayer NNs: The paper only addresses singlelayer NNs. While the authors mention extending the results to multilayer NNs as future work an initial discussion or theoretical insight into the challenges and potential approaches for such extensions would have been beneficial. Questions and Feedback: 1. Could the authors provide more insight into the impact and theoretical justification of the nonnegativity assumption on the bias vector How much does this restriction limit the methods applicability 2. Is there a theoretical or empirical comparison indicating how the chosen filters (median or trimmed mean) perform relative to other outlierrobust estimation techniques 3. Regarding the future extension to multilayer NNs could preliminary ideas or observed challenges in extending the proposed methodology be shared  3) Review Summary and Recommendation The paper makes a significant contribution by proposing a novel algorithm for estimating the parameters of a ReLU NN in the presence of outliers backed by rigorous theoretical analysis and empirical validation. The intention behind addressing a realistic problem scenario where datasets are often contaminated with outliers is commendable and of high relevance to the machine learning community. The thorough theoretical formula of sample complexity and the insightful empirical investigations are major strengths of the work. However the limitation due to the assumption on the bias vectors nonnegativity lack of detailed computational complexity analysis and the focus only on singlelayer NNs are areas that could be improved upon or addressed in future work. Given the novelty theoretical contributions and practical implications I recommend the acceptance of this paper. Nevertheless addressing the pointedout weaknesses and questions could further enhance the value and applicability of the research.", "zBhwgP7kt4": " Peer Review of \"Efficient Dynamic LeastSquares Regression\"  1. Summary of the Paper: This paper addresses the need for efficient maintenance of linear models in situations where data evolves over time specifically focusing on dynamic leastsquares regression (LSR). The authors explore the possibility of maintaining an exact or approximate solution to the LSR problem as new data is incrementally added without the need to recompute the model from scratch. They present a novel dynamic data structure that provides an \u03b5approximate solution with an amortized update time that closely matches the best static solution times while being practically implementable. Their approach is validated through experiments on both synthetic and realworld datasets.  2. Strong Points Weak Points Questions and Feedback: Strong Points:  The paper tackles a highly relevant problem in modern data analysis and machine learning contexts where data is continuously updated.  The introduction of a dynamic data structure that approaches the input sparsity time of static sketchandsolve methods represents a significant advancement.  The theoretical foundations are solid with comprehensive proofs supporting the claims regarding time complexity and efficiency.  Experiments demonstrate the practicality of the proposed method showing superior performance compared to existing strategies. Weak Points:  While the paper presents a significant theoretical contribution the readability and accessibility for a broader audience might be hindered by the dense mathematical exposition.  The experiments albeit insightful focus on a limited number of datasets. Expanding the empirical evaluation could further solidify the practical relevance of the proposed method.  The realworld applicability of the algorithm may vary significantly depending on the specific characteristics of the data and the computational environment which the paper does not fully address. Questions and Feedback:  The paper could benefit from a more detailed discussion on the potential limitations of the approach when applied to extremely large datasets or data with peculiar characteristics (e.g. high dimensional sparse data).  Could the authors elaborate on how their method scales with dimensionality and whether there are particular constraints on the dataset size or feature space  It would be valuable to include a comparison with more recent methods in dynamic data analysis that may offer competitive performance or insights.  3. Review Summary: This paper presents a highly relevant and theoretically sound approach to solving the dynamic leastsquares regression problem. The authors successfully tackle a longstanding issue by proposing an efficient and practical dynamic data structure that achieves an approximate solution with nearly optimal time complexity. Their contributions are significant for the field of data analysis and machine learning especially in applications where data continuously evolves. However the paper could be improved by addressing readability for a broader audience expanding empirical evaluations and providing more details on the practical applicability and scalability of the proposed method. Additionally including a discussion on limitations and potential future directions would enhance the completeness of the work. In conclusion the paper is a valuable contribution to the literature offering both theoretical insights and practical implications for dynamic data analysis. With some refinements it has the potential to impact a wide range of applications in datadriven research and development.", "tlkHrUlNTiL": " Review of \"Deep Linearly Gated Networks: Disentangling and Interpreting Deep Neural Networks\"  Summary This paper aims to address the interpretability issue in deep neural networks (DNNs) by proposing a novel framework named Deep Linearly Gated Networks (DLGNs). It presents an innovative disentangled computational model capable of splitting the entangled linear and nonlinear operations in DNNs with rectified linear units (ReLUs). The authors build upon the concept of the gating property of ReLUs and extend the dual view paradigm highlighting the importance of gates in learning. Through theoretical insights and experimental evaluations the paper demonstrates that DLGNs can maintain a significant portion of the performance of stateoftheart DNNs while providing a clearer computational mechanism. The authors investigate the roles of convolution with global pooling and skip connections in providing rotational invariance and ensemble structure to the neural path kernel (NPK). The paper thoroughly compares DLGNs with conventional DNNs and other related architectures evidencing DLGN\u2019s capability to recover over 83.5 of the performance of standard DNNs on CIFAR10 and CIFAR100 datasets.  Strengths and Weaknesses Strengths: 1. Novelty: The proposal of DLGN introduces an innovative approach to disentangling and interpreting the computations within DNNs addressing a wellknown issue of their black box nature. 2. Theoretical Depth: The paper provides profound theoretical insights into the gating mechanisms importance in DNNs and introduces the concept of dual view and NPK which are crucial for understanding DLGN\u2019s performance. 3. Empirical Evaluations: The authors conduct comprehensive experiments to validate their claims effectively comparing DLGN with traditional DNNs and demonstrating substantial retention of performance. Weaknesses: 1. Complexity in Practical Applications: While DLGN offers a more interpretable framework the complexity and novelty of the approach might pose challenges in practical and industrial applications where simplicity and efficiency are often prioritized. 2. Generalization Across Diverse Datasets: The experiments focus primarily on CIFAR10 and CIFAR100 datasets. Additional evaluations on diverse and more complex datasets would strengthen the argument regarding DLGNs applicability and performance generalizability. 3. Insufficient Discussion on Limitations: The paper could benefit from a more detailed discussion on potential limitations and scenarios where DLGN may not perform as expected or where its interpretability advantages do not hold. Questions and Feedback to the Author: 1. Could the authors provide more insights into how DLGNs would perform on more complex tasks such as natural language processing or object detection which involve more intricate patterns and relationships 2. How does the computational efficiency of DLGN compare with traditional DNNs especially in terms of training time and resource requirements 3. The paper mentions several future directions. Can the authors prioritize these based on what they believe would most significantly impact the field  Review Summary This paper introduces an intriguing concept of Deep Linearly Gated Networks (DLGNs) targeting the wellknown interpretability issue in deep learning. By disentangling and rearranging the computations in DNNs the authors pave the way for a deeper understanding while preserving a notable portion of performance. The novelty and theoretical insights presented are commendable offering a fresh perspective on DNNs inner workings. However the practical feasibility and broader applicability of DLGNs remain areas where further exploration and evidence are desirable. While the empirical results are promising expanding this work to a wider range of tasks and datasets along with a thorough examination of computational efficiency will be crucial steps forward. Overall this paper makes a significant contribution to deep learning research offering a foundation for future investigations aimed at demystifying DNNs while maintaining their powerful capabilities.", "vpiOnyOBTzQ": " Summary of the Paper The paper presents a novel approach to improve the outofdistribution (OOD) generalization and longterm stability of dynamical system predictions using deep neural networks. The authors propose the supervised disentanglement of domain parameters from the dynamics in the latent space of generative models particularly focusing on Variational Autoencoders (VAEs) and a hybrid model incorporating both stochastic and deterministic components. The technique is tested on lowdimensional phase space dynamics of three systems and the highdimensional video rendering of a swinging pendulum. The results indicate that disentangled representations can considerably enhance the adaptability and prediction accuracy of models both in indistribution settings and more notably in OOD scenarios hence potentially contributing valuable insights into timeseries modeling and dynamical system prediction.  Strong Points 1. Innovative Approach: The proposal to use supervised disentanglement for improving OOD generalization and longterm stability in dynamical system predictions is novel and addresses a significant challenge in the field. It leverages the potential of generative models and sophisticated neural network architectures for a crucial application area. 2. Comprehensive Experiments: The experiments are welldesigned and thorough covering both lowdimensional phase space and highdimensional video dynamics. Such a diverse set of experiments provides a solid foundation for the claims regarding the effectiveness of the proposed approach. 3. Detailed Analysis: The analysis of results is detailed comparing the performance of different models and highlighting the benefits of disentanglement especially in OOD situations. This gives a clear understanding of the advantages and potential applications of the proposed method.  Weak Points 1. Generalization of Findings: While the paper presents promising results on the datasets tested it may benefit from a discussion on the generalizability of the approach to other dynamical systems and the challenges that might be encountered. 2. Limitations and Future Work: The paper mentions limitations and leaves certain aspects like the inspection of disentangled representations for future work. A more detailed discussion on how these limitations could be overcome and the potential implications for further research would strengthen the paper. 3. Comparison with StateoftheArt: The paper could benefit from a direct comparison with the latest stateoftheart models in dynamical system prediction particularly those not using disentanglement to more precisely quantify the improvements offered by the proposed method.  Questions and Feedback to the Author 1. Generalizability: How well do the authors believe their approach would generalize to more complex dynamical systems or those with a larger number of domain parameters Are there specific conditions under which their method would not perform well 2. Realworld Data: The use of simulated data is essential for the development and validation of the approach. However could the authors elaborate on how their method could be adapted or extended to work with realworld data where privileged information might not be readily available 3. Potential for Further Disentanglement: The paper focuses on disentangling domain parameters from dynamics. Would there be potential benefits in further disentangling different aspects within the domain parameters or dynamics themselves 4. Impact of Noise: The datasets were corrupted with Gaussian noise for robustness. Could the authors comment on how different types or levels of noise affect the models performance and disentanglement capability  Review Summary The paper introduces a compelling technique for enhancing OOD generalization and longterm stability in dynamical system predictions through supervised disentanglement. The methodology is solid and the experimental results support the claims made. However the paper could be enhanced by addressing the generalizability of the approach providing a broader context of its applicability through comparisons with stateoftheart models and delving deeper into its limitations and potential areas for further research. Overall the work is a significant contribution to the field offering a novel perspective on using disentanglement in generative models for dynamical system prediction. The potential benefits in terms of OOD robustness and prediction accuracy are particularly noteworthy and represent a valuable direction for future investigations.", "t5EmXZ3ZLR": " Peerreview of \"SOSP: Efficiently Capturing Global Correlations by SecondOrder Structured Pruning\"  Summary This paper introduces a novel approach for structured pruning of neural networks to improve inference time and memory costs without compromising the accuracy of the model. The authors target the pruning of coarsegrained structures such as feature maps to leverage advantages on standard hardware configurations. They propose two saliencybased methods for secondorder structured pruning (SOSP) named SOSPH and SOSPI which account for global correlations across all structures of a network. The SOSPH method stands out by employing a secondorder approximation that allows for Hessianvector product evaluations enabling it to scale efficiently similar to firstorder methods while considering the full Hessian. The paper validates the efficiency scalability and performance of SOSPH against SOSPI and other stateoftheart pruning methods through various experiments on medium to largescale vision tasks. Additionally it explores the use of pruning methods in detecting and widening architectural bottlenecks to further increase network performance.  Strong Points 1. The paper addresses an essential challenge in the field of neural network pruning by proposing a scalable global secondorder pruning method that captures the full Hessian without requiring significant computational resources. This approach is innovative and promising for efficient structured pruning. 2. SOSPHs ability to scale similar to firstorder methods thanks to the novel Hessianvector product approximation is a significant advancement. It ensures that the method can be applied to largescale networks which is pivotal for practical adoption. 3. The comprehensive evaluation against various benchmarks and datasets illustrates the methods effectiveness in reducing computational costs without compromising accuracy highlighting its competitive edge. 4. Diagnosing and widening architectural bottlenecks present a novel application of pruning methods showcasing the authors effort to explore beyond traditional pruning objectives and improve neural network performance from an architectural standpoint.  Weak Points 1. While the paper provides a theoretical rationale and empirical evidence supporting the superiority of secondorder pruning methods there is a lack of deeper insight into the scenarios where these advantages are most pronounced. Specifically the paper could benefit from a more detailed discussion on the conditions or network architectures that might limit the effectiveness of SOSP methods. 2. The computational complexity analysis is wellpresented but the paper could provide more details on the practical implications of employing SOSPH for different network sizes and types. Specifically insights into memory requirements and parallelization opportunities could enhance the papers usefulness for practitioners. 3. While the results are promising the comparative analysis could be further strengthened by including more diverse datasets especially those representing nonvision tasks to demonstrate the generalization capacity of SOSP methods. 4. The explanation of how SOSP methods account for global correlations is somewhat abstract. A more intuitive explanation or visualization of how correlations are captured and utilized for pruning decisions could make the paper more accessible to a broader audience.  Questions and Feedback 1. Could the authors provide more insights into the limitations of SOSP methods particularly in scenarios where traditional firstorder methods might outperform or where SOSP methods may face scalability or efficiency issues 2. How do SOSP methods compare to recent pruning techniques that also consider structural and functional aspects of networks such as attentionbased pruning or methods leveraging graph theory concepts 3. The idea of using pruning to diagnose and widen architectural bottlenecks is compelling. Could the authors discuss potential automation strategies for this process to facilitate its adoption in realworld applications  Review Summary The paper \"SOSP: Efficiently Capturing Global Correlations by SecondOrder Structured Pruning\" introduces a significant advancement in the field of neural network pruning through its innovative secondorder methods particularly SOSPH. The methods scalability efficiency and performance are rigorously evaluated making a strong case for its adoption in practice. The exploration of using pruning to improve network architecture further enriches the paper\u2019s contributions. However the paper could benefit from addressing the mentioned weak points and questions to bolster its impact and applicability. Overall the advancements presented in this work are promising and represent a valuable contribution to the field of neural network optimization.", "inSTvgLk2YP": " Peer Review  1. Summary The paper introduces MeshInversion a novel approach leveraging a pretrained Generative Adversarial Network (GAN) for the reconstruction of textured 3D mesh from a single view. The method aims to address the challenges in singleview 3D object reconstruction particularly the absence of 3D or multiview supervision by exploiting the generative prior encapsulated in a pretrained GAN. MeshInversion relies on finding the latent code within the GAN manifold that best recovers the target 3D object from the input image through iterative updating. The method proposes two key innovations to effectively manage common reconstruction issues: Chamfer Texture Loss and Chamfer Mask Loss. These losses enable robust texture reconstruction despite reprojection misalignment and aim to retain geometric details against discretizationinduced information loss. The paper reports that MeshInversion achieves stateoftheart results in terms of the perceptual metric (FID) and offers generalization capabilities for complex shapes and occlusion scenarios.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: Leveraging a pretrained GAN for singleview 3D reconstruction is a novel and promising approach potentially sidestepping the limitations of existing datasetdriven methods.  Robustness to Occlusion and Articulation: The demonstrated ability of MeshInversion to handle occlusions and less common shapes (e.g. birds with extended tails) is impressive and addresses a significant challenge in the domain.  Quantitative and Qualitative Validation: The comprehensive evaluation strategy including both qualitative and quantitative metrics (e.g. IoU FID) effectively illustrates the methods superiority over existing baselines. Weak Points:  Complexity and Performance Tradeoffs: The paper does not discuss the computational requirements or inference times compared to existing methods which is crucial for understanding the practical applicability of MeshInversion especially in realtime applications.  Lack of Detailed Discussion on Limitations: While the paper briefly mentions future work and potential for improvement a more thorough examination of the current limitations (e.g. handling of highly dynamic scenes or extremely fine details) would have strengthened the paper.  Dependency on the Quality of Pretrained GAN: The success of the approach is inherently tied to the quality of the pretrained GAN but there is limited discussion on the implications if the GAN does not adequately capture the diversity of realworld objects. Questions and Feedback:  Have the authors considered extending their approach to dynamic scenes or moving objects and if so what challenges do they anticipate  It would be beneficial to include a discussion on how the approach could be adapted or extended to reconstruct objects from less structured or more diverse categories than those presented (e.g. furniture urban scenes).  A comparison in terms of computational requirements and inference times with existing methods would be informative providing insight into the practical deployment of the proposed method.  3. Review Summary MeshInversion represents a significant advancement in singleview 3D object reconstruction demonstrating superior performance in reconstructing complex shapes and handling occlusions. The innovative use of a pretrained GAN to capture the generative prior of objects and the introduction of Chamferbased losses for texture and mask optimization are compelling features of the method. However the paper could benefit from a deeper exploration of the computational aspects of the approach and a more detailed discussion on its limitations and dependency on the quality of the pretrained GAN. Overall the strengths of MeshInversion particularly its ability to generate realistic and detailed 3D reconstructions from a single view outweigh its limitations marking it as a promising direction for future research in the field.", "k7efTb0un9z": "Summarize the paper: The paper introduces a novel learning rate scheduler named Graph Networkbased Scheduler (GNS) designed to dynamically adjust learning rates using reinforcement learning by understanding the structure of the target optimization problem in deep neural networks. Unlike prior methods that rely on predefined rules or limited representations of the state of training GNS leverages the intrinsic directed graph structure of neural networks using graph message passing to encode rich state information and train an agent to control learning rates. The scheduler shows improved performance on benchmark datasets such as FashionMNIST CIFAR10 and GLUE for tasks in image classification and language understanding outperforming popular baselines and demonstrating strong generalization across various scales of problems and network architectures. Strong Points: 1. Innovative Approach: The introduction of a graphbased method to encode the state of the neural network for learning rate scheduling is novel and innovative. This approach allows for a richer state representation that captures the dynamics of intermediate layers which is a significant advancement over traditional methods and simple reinforcement learning approaches that operate with limited state inputs. 2. Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed GNS across multiple benchmarks (FashionMNIST CIFAR10 and GLUE) and network architectures (CNNs and Transformers) demonstrating its effectiveness and generalization capability. The comparison with multiple baselines including both handdesigned schedulers and learningbased methods strengthens the validity of the results. 3. Generalization Capability: The ability of GNS to generalize across different datasets and network structures is noteworthy. The use of graph neural networks enables the scheduler to manage varying input dimensions and network complexities effectively making it a versatile tool for a wide range of applications. 4. Efficient Reward Collection: The design of an efficient reward collection procedure to accelerate training is a practical contribution. It addresses the computational overhead typically associated with reinforcement learning methods making the approach more feasible for realworld applications. Weak Points: 1. Complexity and Scalability: While the paper discusses the efficiency of the reward collection it does not thoroughly address the overall computational complexity of the GNS framework especially when applied to very largescale tasks like BERT pretraining. The scalability of the method in terms of computational resources required for larger datasets or more complex networks remains a concern. 2. Lack of Theoretical Analysis: The paper predominantly focuses on empirical results with limited theoretical insight into why the graphbased approach is effective for learning rate scheduling. A deeper analysis or theoretical justification could provide more clarity on the methods strengths and potential limitations. 3. Limited Comparison with Adaptive Learning Rate Methods: Although GNS is compared against several baselines there is a missed opportunity in not extensively comparing it with adaptive learning rate optimizers like AdaGrad RMSProp or the latest variations of Adam. Such a comparison could further establish the utility of GNS over existing adaptive methods. Questions and Feedback to the Author: 1. Can the authors provide more details on the computational complexity of GNS especially when applied to largescale networks and datasets 2. It would be beneficial to include a theoretical analysis or justification of why the graphbased state encoding works so effectively for learning rate scheduling. Can the authors elaborate on this aspect 3. Could the paper benefit from a more comprehensive comparison with stateoftheart adaptive learning rate methods to showcase the advantages of GNS more clearly Review Summary: The paper presents a significant contribution to the field of stochastic optimization for deep learning by proposing the Graph Networkbased Scheduler (GNS) a novel approach to dynamic learning rate scheduling leveraging graph neural networks. The methods ability to encode rich state information from the neural networks structure allows for improved performance and generalization across different tasks and architectures. However there are concerns regarding the computational complexity and scalability of the method along with a lack of theoretical analysis on its effectiveness. Despite these limitations GNS represents a promising direction in learning rate scheduling with its innovative use of graph neural networks and efficient reward collection mechanism. The comprehensive empirical evaluation strengthens the case for GNS showcasing its potential as a versatile tool for enhancing the performance of deep learning models. Further investigations into the computational requirements and theoretical underpinnings of GNS could make this approach even more valuable for a broader range of applications.", "kUGYDTJUcuc": " Peer Review of the Paper on a Unified Visual Attention Model  1) Summary of the Paper The paper presents a new model for recurrent visual attention in computer vision tasks that integrates both topdown and bottomup mechanisms. The authors propose to utilize image pyramids and Qlearning for selecting regions of interest in a topdown approach guiding the policy search in a bottomup fashion. The model adds two constraints over the bottomup recurrent neural networks to enhance exploration. It is trained in an endtoend reinforcement learning framework and evaluated on visual classification tasks. The experiments demonstrate that the proposed model outperforms both convolutional neural networks (CNNs) and existing bottomup recurrent attention models on datasets such as MNIST CIFAR 10 and SVHN.  2) Strengths and Weaknesses Strong Points:  Innovation: The paper addresses recurrent visual attentions high variance and instability issue by innovatively combining topdown and bottomup attention mechanisms. This approach is novel and addresses a significant challenge in the field.  Comprehensive Evaluation: The authors conduct extensive experiments to validate their models effectiveness across multiple datasets demonstrating its superiority over existing CNNs and bottomup models.  Detailed Methodology: The explanation of the methodology including the integration of image pyramids Qlearning and constraints for better exploration is thorough and clear. This detail aids in understanding the models functionality and potential replicability. Weak Points:  Comparative Analysis: While the paper discusses the performance improvements over CNNs and RAM models it lacks a detailed comparative analysis with other stateoftheart models in visual attention which might have showcased its relative performance more comprehensively.  Impact of Each Component: The paper introduces multiple novel components in the model but does not individually analyze the impact of each component such as the image pyramids or the two added constraints on the overall performance.  Robustness and Generalization: The discussion on the models robustness to adversarial attacks is valuable but there\u2019s limited exploration of the models generalization across diverse and more complex scenarios beyond the datasets evaluated. Questions and Feedback to the Author: 1. Can the authors provide more insights on how the model performs relative to other stateoftheart visual attention models 2. It would be beneficial if the authors could dissect the contribution of each novel component (image pyramids Qlearning the two constraints) towards the overall performance improvements. 3. How does the model generalize to more complex realworld scenarios beyond MNIST CIFAR 10 and SVHN datasets Are there limitations in its applicability  3) Review Summary The paper contributes significantly to the computational vision community by introducing a unified model for recurrent visual attention that blends topdown and bottomup mechanisms. The approach is innovative addressing the inherent variance and instability issues in existing recurrent attention models. The results as demonstrated across various datasets underscore its effectiveness and potential superiority over traditional CNNs and bottomup models. However to bolster the contribution further it would be eminent to provide a comparative analysis with other leading models in this domain assessing the unique contributions of the proposed models components individually. Furthermore expanding on the models generalization capabilities and adaptability to more complex datasets would enhance the papers impact. In conclusion the paper stands out for its novel approach and significant improvements in visual classification tasks. With additional analyses and exploration of the models applicability to diverse scenarios it has the potential to make a profound impact on the development of future visual attention models.", "m4BAEB_Imy": " 1) Summarize the paper The paper presents a novel approach to prune Binary Neural Networks (BNNs) to significantly reduce memory and computation requirements while maintaining high accuracy levels on the MNIST and CIFAR10 datasets. The proposed method iPrune specifically addresses the challenges associated with implementing BNNs on hardware accelerators such as Field Programmable Gate Arrays (FPGAs) by reducing the number of inputs per neuron through a magnitudebased unstructured pruning technique. By comparing their results with existing fullprecision models and other recent pruning techniques the authors demonstrate substantial improvements in reducing memory and computation needs by up to 2200x and 70x respectively. They also explore the impact of applying iPrune to randomly initialized networks suggesting potential in reducing computational costs from the outset. The paper concludes with suggestions for future research directions including the exploration of other forms of quantization and the potential for porting pruned models to FPGAs to further examine energy and inference time benefits.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a critical and timely challenge in deploying complex AI models on resourceconstrained hardware providing a solution that significantly reduces memory and computation without substantially sacrificing accuracy.  By focusing on Binary Neural Networks the study leverages an already efficient model architecture and further enhances it making it a valuable contribution to the field of efficient AI computations.  The comparison of iPrune against fully connected full precision and other contemporary pruning methods is thorough offering clear evidence of its advantages. Weak Points:  The evaluation focuses primarily on the MNIST and CIFAR10 datasets which are relatively simple. Its effectiveness on more complex or realworld datasets remains unclear.  While the paper suggests future work on porting to FPGAs it would have strengthened the current work if preliminary evaluations on hardware implications were included.  The discussion on the implications of neuron input reduction on model robustness or adaptability to new or shifted data distributions is lacking. Questions: 1. Could the authors clarify if there were any specific challenges encountered when applying iPrune on convolutional layers compared to dense layers considering their structural differences 2. Is there an expected tradeoff between the reduction of inputs per neuron and the models ability to generalize across more complex or diverse datasets 3. How does the iPrune method adapt to dynamic datasets or scenarios where model retraining is frequent Feedback:  It would greatly benefit the paper to include a section discussing potential limitations or the algorithms performance in varied contexts offering a more balanced view.  Considering the impressive reductions in memory and computation a deeper exploration into the models performance on hardware even if speculative or based on modeling could provide valuable insights into realworld applicability.  Expanding the evaluation to include more complex datasets or tasks could significantly strengthen the papers claims about iPrunes effectiveness and versatility.  3) Review Summary The paper introduces iPrune an innovative pruning method for Binary Neural Networks that significantly reduces memory and computational requirements on constrained hardware while preserving high accuracy levels. This work is particularly relevant for the deployment of AI in resourcelimited environments and contributes to the broader effort of making AI more environmentally sustainable. The authors provide a compelling comparison with existing techniques showcasing the superior efficiency of iPrune. However the paper could be improved by broadening its empirical evaluation to more complex datasets and tasks providing a more indepth discussion on potential limitations and the practical implications of significantly reducing neuron inputs especially regarding model robustness and generalization. Additionally offering more insights into the expected performance and challenges of porting such models to specialized hardware like FPGAs would enhance the papers relevance and applicability. In summary while the paper presents a significant advancement in pruning techniques for BNNs a more comprehensive evaluation and discussion on practical implementation aspects could make it an even more valuable resource for the AI research community and industry practitioners alike.", "hq7vLjZTJPk": " Peer Review  Summary of the Paper The paper introduces a novel algorithm for distributed training of deep neural networks particularly focusing on Federated Learning (FL) environments. The authors address the challenge of slow convergence in training models like RNNs and LSTMs due to the exploding gradient problem by proposing a communicationefficient distributed stochastic local gradient clipping algorithm (CELGC). This algorithm utilizes a relaxedsmoothness assumption to enable gradient clipping on local machines while reducing the frequency of intermachine communication. The authors prove that CELGC achieves linear speedup and reduced communication complexity leading to faster convergence rates in practice. They validate the effectiveness of their algorithm through experiments on various benchmark datasets demonstrating significant improvements in speed and efficiency over existing methods.  Strong and Weak Points Strong Points: 1. Innovative Algorithm Design: The CELGC approach addresses a gap in current literature by extending the gradient clipping technique to a distributed FL setting which is novel and practically significant. 2. Theoretical Contributions: The paper presents rigorous theoretical analysis showing how CELGC enjoys linear speedup and better communication efficiency under certain conditions. These contributions provide a solid theoretical foundation for the proposed algorithm. 3. Empirical Validation: The authors conduct comprehensive experiments demonstrating the algorithms effectiveness across multiple datasets and scenarios. This empirical evidence strengthens the papers claims about the practical benefits of CELGC. Weak Points: 1. Assumption Transparency: While the paper mentions making certain assumptions (e.g. relaxed smoothness bounded gradient norm) it could benefit from a more detailed discussion on the implications and limitations of these assumptions in practical settings. 2. Comparison with a Broader Range of Algorithms: The experiments mainly compare CELGC against a \"naive parallel version\" as a baseline. Including comparisons with a wider array of current FL and distributed training algorithms could provide a clearer picture of CELGCs positioning in the field. 3. Impact on Final Model Performance: While speedup and efficiency are critical the paper does not thoroughly discuss the impact of CELGC on the final model performance compared to other methods. An analysis of any potential tradeoffs between speed and accuracy would be valuable. Question and Feedback to the Author:  Could the authors provide more insight into the practical applicability of the CELGC algorithm especially concerning the assumptions made (e.g. relaxedsmoothness) How would these assumptions hold in highly nonideal realworld datasets and models beyond RNNsLSTMs  Adding to the experiments it would be beneficial if the authors could include comparisons with more recent FL or distributed learning algorithms. How does CELGC perform against these alternatives in terms of not only efficiency and speed but also model accuracy and robustness  Review Summary This paper introduces a significant advancement in distributed training for deep learning particularly addressing the challenges in Federated Learning setups. The CELGC algorithm proposes a novel approach to leverage gradient clipping effectively in a distributed setting achieving notable improvements in convergence speed and communication efficiency. The provided theoretical analysis and empirical validations strongly support the algorithms efficacy and applicability in practical scenarios. However further clarifications on the practical implications of the algorithms underlying assumptions and comparisons with a broader set of existing methods could enhance the papers impact. Additionally a deeper exploration of the tradeoffs between speed efficiency and final model performance would provide a more comprehensive understanding of the algorithms benefits and limitations. Overall the paper represents a valuable contribution to the field with its innovative approach strong theoretical grounding and practical relevance warranting a positive recommendation. Further addressing the discussed points could make it an even stronger candidate for publication and application in realworld distributed deep learning tasks.", "qwBK94cP1y": "Summarize the Paper: This paper presents a novel approach to causal discovery focusing on identifying causal relationships in bivariate data through a dynamicalsystem perspective on Functional Causal Models (FCMs). It leverages the connection between FCMs and optimal transport to propose a framework that transcends the limitations imposed by traditional model assumptions on FCMs. By integrating optimal transport with the dynamics of FCMs under specific constraints the paper introduces a new criterion for causal direction determination based on the divergence of velocity fields in volumepreserving pressureless flows\u2014an analogy drawn from Additive Noise Models (ANMs). The proposed optimal transportbased algorithm DIVOT is showcased to perform exceptionally well on both synthetic datasets and realworld causal discovery benchmarks establishing a groundwork for a more flexible and robust approach to causal inference. Strong Points: 1. Innovative Integration: The paper successfully integrates concepts from dynamical systems optimal transport and functional causal models to address causal direction determination which is a significant stride toward reconciling causal inference with complex realworld data structures. 2. Robust Algorithm (DIVOT): DIVOT stands out for its efficiency flexibility and robustness to model specification errors. Demonstrating stateoftheart performance on both synthetic and real datasets is a testament to its effectiveness. 3. Theoretical Contribution: The authors provide a thorough theoretical analysis connecting ANMs to volumepreserving pressureless flows which is a meaningful contribution to the causal inference literature. Weak Points: 1. Assumptions Clarity: While the paper claims the approach is robust to model assumptions it could benefit from a more detailed discussion on any underlying assumptions implications and limitations. 2. RealWorld Applications: The paper focuses on synthetic and benchmark datasets for performance demonstration. Insights into the application of the proposed method for complex realworld causal scenarios could enhance its practical relevance. 3. Comparison with Existing Methods: The paper could strengthen its position by providing more comprehensive comparisons including detailed discussions on the advantages of DIVOT over existing methods under various conditions. Questions and Feedback to the Author: 1. Can the authors clarify if there are any hidden or unexplored assumptions that might limit the applicability of their proposed method in certain types of causal inference problems 2. How does the performance of DIVOT compare to existing methods in scenarios with highdimensional data or in the presence of multiple confounders 3. Could the authors discuss any potential challenges in extending the proposed framework to multivariate causal discovery tasks 4. Feedback: It would be beneficial to include more realworld application examples or case studies demonstrating how the DIVOT method can address complex causal inference challenges outside of benchmark datasets. Review Summary: This paper represents an impressive leap forward in causal discovery methodologies particularly in bivariate cases through a wellfounded dynamicalsystem approach to understanding and modeling causal relationships. The integration of FCMs with optimal transport under the banner of dynamical systems theory not only bridges a gap in the existing literature but also offers a versatile and robust algorithm DIVOT with excellent performance metrics. While the theoretical contributions are significant and the proposed algorithm demonstrates superior performance the paper could gain further strength from a detailed examination of its underlying assumptions a broader comparison with existing methods and more explorations into its applicability in complex realworld scenarios. In conclusion this work opens up new avenues for research in causal discovery and has the potential to significantly influence future work in this domain.", "tYRrOdSnVUy": " Summarize the paper: This paper introduces a novel approach titled NonTransferable Learning (NTL) to protect the intellectual property of deep learning models by restricting their generalization ability to specific domains. NTL is designed to address two primary concerns in AI as a Service (AIaaS): ownership verification and usage authorization. The approach leverages domaindependent feature learning to ensure that models trained with NTL demonstrate robust performance in a source domain while significantly degrading in performance when applied to auxiliary or unauthorized domains. This paper presents two variations of NTL namely TargetSpecified NTL and SourceOnly NTL and demonstrates their effectiveness through extensive experimental analysis across various datasets including digits CIFAR10  STL10 and VisDA. The results highlight NTLs superiority in resisting stateoftheart watermark removal techniques for ownership verification and its capability in enforcing datacentric applicability authorization.  Strong and Weak Points Questions and Feedback: Strong Points: 1. Innovative Approach: The concept of NTL is a significant departure from traditional techniques focused on enhancing model generalization. By intentionally restricting generalization NTL offers a novel perspective on model protection. 2. Comprehensive Evaluation: The paper presents a thorough experimental analysis across multiple datasets showcasing NTLs effectiveness in both ownership verification and usage authorization. 3. Robustness Against Watermark Removal: A standout feature of NTL is its resistance to advanced watermark removal techniques making it a more reliable method for verifying model ownership. Weak Points: 1. Applicability Beyond Image Classification: The paper is primarily focused on image classification tasks. Its applicability and effectiveness in other domains such as NLP or video analysis remain unclear. 2. Impact on Model Performance: While the paper discusses the restricted generalization ability to protect IP there is less emphasis on the potential negative impact this restriction might have on the overall utility and performance of the model in legitimate use cases. 3. Complexity and Overhead: The design and implementation of NTL especially the generative adversarial augmentation framework for SourceOnly NTL might introduce significant complexity and computational overhead. The paper could elaborate more on these aspects. Questions and Feedback: 1. Extension to Other Domains: How transferable is the NTL approach to tasks outside of image classification particularly in domains where data augmentation and domain distinction might be less straightforward 2. Impact on Legitimate Users: Could the restriction on model generalization negatively affect legitimate users especially in scenarios where slight domain variations are common in the intended use case 3. Optimization Tradeoffs: Can the authors discuss any observed or potential tradeoffs between strengthening IP protection through NTL and preserving the models utility for authorized purposes  Review Summary: The paper presents a novel and promising approach for protecting the intellectual property of deep learning models through NonTransferable Learning (NTL) which strategically restricts model generalization to designated domains. This approach is both innovative providing a new direction away from traditional model protection methods and effective as demonstrated by extensive experimental evidence highlighting its resilience to watermark removal methods and capability for datacentric applicability authorization. However questions remain on its broader applicability beyond image classification potential impacts on model utility for legitimate purposes and the complexity it introduces. Overall the paper makes a valuable contribution to the field of machine learning offering a viable solution to pressing concerns around the protection of intellectual property in AIaaS. With further exploration and refinement NTL has the potential to become a standard in model protection strategies especially if its application can be extended and optimized for a wider range of tasks and domains.", "lQI_mZjvBxj": " Peer Review of \"Designing Model Agnostic Federated Learning Protocols via Knowledge Distillation: A Theoretical and Empirical Investigation\"  Summary This paper investigates the possibility of designing a universal API for federated learning that allows adhoc groups of dataholders (agents) to collaborate and perform federated learning without sharing raw data. It aims to develop modelagnostic federated learning algorithms by leveraging Knowledge Distillation (KD) as the primary tool. The paper introduces a new theoretical framework named Federated Kernel Ridge Regression to analyze the performance degradation observed in KDbased federated learning protocols under conditions of model and data heterogeneity. The authors validate their theoretical findings through experiments with neural networks confirming the limitations of KD under data heterogeneity.  Strong Points 1. Innovative Theoretical Framework: The paper introduces a compelling theoretical framework Federated Kernel Ridge Regression that effectively captures both model and data heterogeneity addressing a significant gap in understanding KDbased federated learning protocols. 2. Comprehensive Analysis: The detailed analysis of alternating knowledge distillation (AKD) and its failure to converge due to data heterogeneity provides valuable insights into the limitations of KD in federated settings expanding our understanding of KDs applicability. 3. Empirical Validation: The experiments using realworld neural network models to validate the theoretical predictions add empirical strength to the study ensuring that the theoretical findings have practical relevance and applicability. 4. Novelty in Approach: The use of knowledge distillation for model agnostic federated learning is novel and the approach to analyze and address its limitations through a theoretical framework and practical implementations is commendable.  Weak Points 1. Complexity of Proposed Solution: While the ensembled knowledge distillation (EKD) scheme is innovative its requirement for potentially large ensembles to match centralized performance might limit its practical applicability. This complexity and potential computational overhead could deter adoption in realworld scenarios. 2. Lack of Depth in Practical Evaluation: Although the paper presents an initial empirical evaluation there is a scope for more extensive realworld experimentation including more diverse datasets and federated learning scenarios to fully understand the practical limitations and possibilities of the proposed methods. 3. Limited Discussion on Alternatives: The paper could benefit from a more indepth comparison with existing federated learning protocols outside of KD providing a broader context and justification for the focus on KDbased approaches. 4. Assumptions and Simplifications: Some of the assumptions and simplifications particularly around the data distribution and model architecture heterogeneity may not fully capture the complexity of realworld federated learning scenarios potentially limiting the generalizability of the findings.  Questions and Feedback for the Authors 1. Could the authors provide more insights into how the proposed EKD scheme could be optimized for practical implementation particularly in reducing the number of models required for effective performance 2. Are there specific domains or applications where the authors believe their proposed approach would be most beneficial or applicable 3. How do the authors envision overcoming the limitations of data heterogeneity beyond the EKD scheme presented in the paper 4. Could the paper benefit from a more detailed comparison with nonKDbased federated learning approaches helping to place the proposed method in the broader landscape of federated learning research  Review Summary The paper makes a valuable contribution to the field of federated learning by introducing and carefully analyzing a novel theoretical framework for understanding the limitations of knowledge distillation in the presence of model and data heterogeneity. While the proposed solutions bring innovation their complexity and the potentially extensive computational resources required may limit practical applicability. Further the paper would benefit from broader empirical evaluation and a more detailed contextualization within the wider federated learning research landscape. Despite these limitations the work opens new avenues for research in modelagnostic federated learning protocols and presents a solid theoretical foundation for further exploration. My recommendation would be to accept this paper with minor revisions particularly focusing on enhancing the practical evaluation and discussing alternative federated learning approaches for a more rounded contribution.", "oDFvtxzPOx": " Review  1) Summary of the Paper This paper introduces a novel deep learning framework for feature selection specifically designed to tackle the challenges of limited labeled data and correlated features prevalent in fields such as medicine and biology. The approach named Selfsupervision Enhanced Feature Selection (SEFS) leverages unlabeled data through a selfsupervised learning phase where the network learns informative representations from partial feature sets. A gating procedure that incorporates the correlation structure of input features is then introduced guiding the network to prioritize the selection of the most relevant features. The proposed method is validated across various datasets demonstrating superior performance in feature selection and prediction accuracy compared to current stateoftheart methods.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The integration of selfsupervised learning with a gating procedure that accounts for feature correlations is a novel and theoretically motivated approach to address the twin challenges of feature selection with limited labeled data and high feature correlation.  Practical Importance: The paper tackles a highly relevant problem in fields where data is either expensive or difficult to label and features are often correlated making it significantly impactful.  Experimental Validation: The extensive experiments conducted on synthetic and realworld datasets including clinical and genomics data provide solid evidence for the efficacy of the proposed method.  Alignment with Domain Needs: The method\u2019s ability to identify biologically relevant features as corroborated by the medical and scientific literature demonstrates its practical utility and alignment with the needs of the targeted domains. Weak Points:  Generalizability of SelfSupervised Tasks: While the paper presents a solid case for the chosen selfsupervised tasks its unclear how the selection of other tasks might affect the performance. The generalizability of the selfsupervised learning phase across different task types remains to be explored.  Complexity and Interpretability: The complexity introduced by the model especially with the gating mechanism and selfsupervised learning phase might impact its interpretability and ease of use. An analysis of the interpretability of the selected features and the models decisions would add value. Questions:  Would the model perform equally well in domains outside of medicine and biology where data characteristics might differ significantly  How sensitive is the model to the choice of pretext tasks in the selfsupervision phase Could other tasks potentially improve or worsen the performance Feedback:  A deeper discussion on the interpretability of the model\u2019s decisions and the selected features would enhance the paper\u2019s relevance especially for realworld applications where justification of model decisions is crucial.  Including a sensitivity analysis of the models performance with respect to different selfsupervised learning tasks could provide insights into the robustness and adaptability of the approach.  3) Review Summary The paper presents a significant advancement in the domain of feature selection through its innovative use of selfsupervised learning and a clever gating procedure that accounts for feature correlations. This method not only addresses practical challenges in fields like medicine and biology but also demonstrates superior performance over existing methods. While the paper stands out for its novel approach and solid experimental validation areas such as the interpretability of the model and the generalizability of the selfsupervised tasks could be further explored to strengthen the findings. Nonetheless the demonstration of practical utility especially in identifying biologically relevant features backed by literature underscores the potential impact of this work. Considering both its strengths and opportunities for further exploration I recommend this paper for publication highlighting its contribution to advancing feature selection methods in highdimensional and limited label data scenarios.", "ptZfV8tJbpe": "1) Summary of the Paper: The paper introduces a novel approach to MultiLabel Text Classification (MLTC) that addresses the challenge of modeling complex label correlations. Unlike conventional methods that explicitly model label correlations often introducing inflexible inductive biases this study proposes employing latent label encodings. These encodings are concatenated with text tokens and fed into BERT whereafter the contextual representations of these latent labels are mapped to actual labels. This methodology shifts away from the explicit modeling of label interdependencies towards an implicit flexible and adaptive strategy theoretically allowing for a more nuanced understanding of label correlations. Experiments on benchmark datasets demonstrate superior performance of this approach over stateoftheart methods especially highlighting its effectiveness in leveraging label correlations to improve classification outcomes as opposed to merely enhancing document representation. 2) Strengths and Weaknesses: Strengths:  Innovativeness: The approach of using latent label encodings to implicitly model label correlations is novel and addresses limitations of previous models that rely on explicit modeling of label dependencies which can introduce rigid inductive biases.  Empirical Evidence: The method outperforms existing stateoftheart models on widelyused benchmark datasets which is a strong testament to its effectiveness. The comprehensive experiments and feature studies further validate the advantages of using latent label embeddings.  Insightful Analyses: The paper provides thorough analysis and discussions including the utility of latent label embeddings the limitations of pretraining these embeddings and error analyses comparing the proposed model with the previous stateoftheart thereby offering deep insights into why and how this model works effectively. Weaknesses:  Lack of Theoretical Foundation: While the paper discusses the intuition behind using latent embeddings a more rigorous theoretical foundation or analysis on how and why implicit modeling of label correlations with latent labels works could enhance the readers understanding.  Generalizability Concerns: The experiments are conducted on two benchmark datasets. Including more diverse datasets particularly those from different domains or with different characteristics (e.g. label sparsity) could help in evaluating the generalizability of the proposed method.  Comparison with Baseline Methods: While the paper compares its method with a range of existing approaches it would be beneficial to see comparisons or discussions on how this new approach performs relative to other recent deep learning models that might not strictly fall under the MLTC category but are capable of multilabel classification. Questions and Feedback: 1. Can the authors provide more theoretical insights or justifications on why latent label representations are particularly effective for MLTC tasks 2. How does the proposed method perform on datasets with a higher degree of label sparsity or datasets from radically different domains compared to the ones used in the experiments 3. The paper mentions that pretraining latent label embeddings could sometimes degrade performance. Could the authors delve deeper into when and why this occurs possibly providing guidelines on when to use pretraining 3) Review Summary: The paper presents a novel method for MultiLabel Text Classification that utilizes latent label encodings to implicitly model label correlations showcasing a significant departure from traditional explicit modeling approaches. This approach not only addresses the limitations imposed by rigid inductive biases but also demonstrates superior performance on benchmark datasets underscoring its effectiveness in leveraging label correlations for classification tasks. The methodological innovation backed by comprehensive experiments and insightful analyses constitutes a major strength of this work. However the paper could benefit from a more robust theoretical foundation concerning the underlying principles of latent label usage and a broader evaluation scope to ascertain its generalizability across diverse datasets. Furthermore an indepth exploration of the noted limitations concerning the pretraining of latent label embeddings could enrich the papers contributions. In conclusion the proposed method marks a significant step forward in the field of MultiLabel Text Classification suggesting a promising new direction for future research on modeling complex label interdependencies. The paper is wellpositioned to stimulate further investigations into the implicit modeling of label correlations and its application across a broader range of classification tasks and domains.", "m5EBN92vjN": " Peer Review of \"Attention Aware Network (AASeg) for Real Time Semantic Image Segmentation\"  1) Summary of the paper The paper introduces Attention Aware Network (AASeg) a novel architecture for realtime semantic image segmentation. The proposed network leverages Spatial Attention (SA) and Channel Attention (CA) modules for capturing spatial and semantic information effectively. Additionally it utilizes a Multi Scale Context (MSC) module for dense local context information across multiple scales. The authors claim AASeg outperforms most previous architectures providing improved Mean IOU values while maintaining high FPS on the Cityscapes ADE20K and Camvid datasets.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Design: The integration of SA and CA modules with MSC for realtime segmentation is commendable addressing both spatial and semantic feature enhancement efficiently.  Comprehensive Evaluation: The paper presents extensive experiments including an ablation study and compares the network against multiple datasets showcasing its versatility and robustness.  Balance of Speed and Accuracy: Achieving a balance between segmentation accuracy and inference speed is critical in realtime applications. AASeg\u2019s performance metrics indicate a significant advancement in this area. Weak Points:  Lack of Discussion on Computational Complexity: While the paper mentions the reduction of computational cost due to the attention mechanisms theres a lack of detailed discussion or comparison of the computational complexity with existing models.  Limited Qualitative Analysis: The paper could benefit from a more indepth qualitative analysis including visual comparisons of segmentation results to better understand where AASeg performs well or falls short.  Absence of Realworld Application Testing: Despite the mention of applications like autonomous driving there\u2019s no realworld testing or demonstration provided which could strongly support the practical applicability of AASeg. Questions and Feedback:  Could the authors elaborate on the computational efficiency of AASeg compared to other stateoftheart models in terms of parameters and actual runtime on standard hardware  A more detailed analysis of how AASeg handles challenging scenarios such as objects with fine details or scenes with complex textures would be beneficial.  Feedback to the author would include the suggestion to incorporate realworld application scenarios or testing to validate the network\u2019s performance and applicability further.  3) Review Summary The paper presents a promising approach towards realtime semantic image segmentation through the Attention Aware Network (AASeg) showcasing a novel integration of spatial and channel attention mechanisms with a multiscale context module. The strong performance metrics reported on several datasets underline the potential of AASeg to set a new bar in the balance of speed and segmentation accuracy. Nevertheless the paper could be strengthened by addressing certain areas such as computational complexity analysis extending qualitative analysis for a better understanding of the models behavior in various conditions and demonstrating the networks efficacy in practical applications. Overall the innovative approach combined with comprehensive quantitative analysis makes a compelling case for AASegs effectiveness in realtime semantic segmentation. However to solidify its place as a new standard further discussion and exploration in the highlighted areas would be beneficial.", "msRBojTz-Nh": " Peer Review  1) Summary of the Paper This paper discusses the development and evaluation of a machine learning (ML)based model designed for the simulation of turbulent dynamics at low spatial and temporal resolutions which historically has been a challenging task due to the chaotic nature of turbulence. The authors propose a learned simulator model referred to as Dilated ResNet (DilResNet) that outperforms classical numerical solvers in capturing highfrequency information and accurately simulating turbulent dynamics across various relevant statistical measures at comparably low resolutions. The model is trained endtoend and leverages dilated convolutional neural networks (CNNs) and residual networks (ResNets) to capture a range of challenging chaotic and turbulent dynamics without the need for specialized knowledge of the domain. A notable advancement is the models ability to produce stable rollouts through the tuning of training noise and temporal downsampling which addresses common instability issues found in learned simulators. The authors evaluate the models performance across several turbulent domains demonstrating its superiority over traditional solvers and other architectures from the learned turbulence simulation literature. Additionally the paper touches upon outofdistribution generalization challenges and suggests methods to improve this such as training noise dataset augmentation and added loss constraints.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a critical challenge in turbulence simulation by reducing the need for highresolution grids pushing forward the efficiency of simulations in scientific and engineering domains.  It presents a comprehensive evaluation of the proposed model across various turbulent domains showcasing its broad applicability and effectiveness in accurately capturing turbulent dynamics at low resolutions.  By successfully incorporating strategies like training noise and temporal downsampling the paper provides a valuable solution to the common problem of instability in learned simulators. Weak Points:  The paper could benefit from a more detailed discussion of the failures or limitations encountered when the model attempts to generalize beyond the training distribution providing specific examples or scenarios where the performance drops significantly.  While the paper suggests methods to improve outofdistribution generalization it does not fully explore or empirically evaluate these suggestions leaving questions about their effectiveness unanswered. Questions and Feedback:  How does the performance of the DilResNet compare to classical solvers and other architectures in terms of computational efficiency and running time especially when leveraging GPU acceleration  Could the authors elaborate on the potential implications of using DilResNet for realworld applications in aeronautics meteorology or other fields where turbulent dynamics are crucial Specifically how does the model\u2019s lower resolution simulation impact the accuracy and reliability of predictions in practical scenarios  Feedback: It would be beneficial to include a deeper analysis of the models limitations and potential strategies for mitigating issues related to outofdistribution generalization. Experimenting with various approaches and providing empirical results would significantly strengthen the claims about improving model robustness.  3) Review Summary The paper presents a significant advancement in the field of turbulence simulation by introducing a machine learning model capable of accurately simulating turbulent dynamics at low resolutions. Its comprehensive evaluation across multiple domains alongside its ability to address the instability often seen in learned simulators solidifies its contribution to the field. However the exploration of outofdistribution generalization remains somewhat superficial with the paper providing recommendations but lacking empirical evaluation of these strategies. In conclusion while the DilResNet model showcases promising results in efficiently simulating turbulent dynamics further research into its limitations and the effective generalization of the model is necessary to fully understand its capabilities and applicability in realworld scenarios.", "xUdEO_yE-GV": "Summarize the paper: The paper introduces a novel method to enhance the training of deep networks for segmenting road networks in aerial images and neuronal processes in microscopy scans by leveraging Persistent Homology (PH) to compare topological features of datasets. The core improvement proposed is a new filtration technique that merges thresholdingbased filtrations and height functions enabling the locationcentered computation of persistence diagrams. This method outperforms existing topological and nontopological approaches by producing segmentations that preserve the connectivity of the original structures more effectively. It is validated through experiments on various datasets showing its superiority over other methods in ensuring topological correctness of segmentation masks. Strong and weak points Question and Feedback to the author: Strong Points: 1. Innovative Methodology: The combination of thresholdingbased filtrations with height functions to compute persistence diagrams that include the location of topological features is novel and addresses a clear gap in the existing literature. 2. Broad Applicability: The approach is applicable for both 2D and 3D images increasing its utility across different fields particularly valuable for biomedical applications. 3. Comprehensive Evaluation: The paper conducts extensive experiments across multiple datasets and compares performance using multiple metrics which solidly validates the effectiveness of the approach. Weak Points: 1. Complexity of Implementation: The paper introduces a method that may be significantly complex for implementation especially in terms of computational requirements. More discussion around the computational efficiency or potential optimization strategies would be beneficial. 2. Sparse Gradient Issue: The authors identify the issue of sparse gradients in their proposed loss function but do not provide a solution within the scope of the current paper. Future work is proposed in this direction but initial ideas or direction would be valuable. Question:  Given the identified limitation regarding sparse gradients in the proposed loss function can the authors speculate on potential approaches or methods that could be explored to address this issue Feedback: 1. Clarification on Computational Efficiency: The paper would greatly benefit from a more detailed discussion on the computational efficiency of the proposed approach including comparisons with existing methods in terms of computational resources and time required for training and inference. 2. Exploration of Sparse Gradients Issue: While it is understandable that addressing the sparse gradients issue is considered future work providing preliminary insights or theoretical directions for addressing this limitation would enrich the discussion and potentially guide future research. Review Summary: The paper presents a significant advancement in the use of Persistent Homology for training deep networks to achieve topologically accurate segmentations of road networks and neuronal processes. The novel approach of a modified filtration technique that incorporates the location of topological features addresses a critical gap in existing methods demonstrating superior performance across multiple datasets. Despite its strengths the paper could be improved by addressing the computational efficiency of the method more explicitly and providing initial insights into the identified issue of sparse gradients in the proposed loss function. My recommendation is a strong accept contingent upon the authors considering the provided feedback to enhance the clarity and impact of their contributions. The innovative methodology broad applicability and rigorous validation make this study a valuable contribution to the field with promising implications for both image segmentation and the broader application of machine learning to biomedical imaging.", "wIzUeM3TAU": " Peer Review on \"Characterizing the Separation Power of Graph Neural Networks through Tensor Languages\"  Summarize the Paper This paper introduces a novel framework to analyze the separation power of Graph Neural Networks (GNNs) using a specifically defined tensor language which in turn is connected to the WeisfeilerLeman (WL) tests. The authors propose a tensorbased procedural language that facilitates the analysis of arbitrary GNN architectures by simplifying their expression focusing on the number of indices and the depth of summation in these expressions. By doing so it ties the GNNs separation ability directly to the kdimensional WL tests providing bounds on what GNNs can or cannot distinguish in terms of graph structure. New insights into designing GNN architectures for enhanced separation power are presented along with general approximation results for GNNs. The method is applied to extract bounds on the separation power of various known GNN models and addresses open problems in the field.  Strong and Weak Points of the Paper Strong Points: 1. Originality and Technical Contribution: The paper contributes significantly to the field by offering a tensor language approach to understand and analyze the separation power of GNNs an area where comprehensive tools were previously lacking. The use of tensor languages for this purpose is novel and provides a solid theoretical foundation for assessing and designing GNN architectures. 2. Unified Framework: The introduction of a generalized framework to encapsulate the separation power of various GNN models is a substantial strength. By doing so the paper streamlines the analysis process for GNNs making it accessible to architectures that have not been specifically analyzed in the scope of WL tests. 3. Impactful Insights: The insights offered on bolstering the separation power of GNNs are valuable for researchers and practitioners aiming to design more powerful GNN architectures. Specifically the connections made between tensor language expressions kWL bounds and graph separation tasks are profound and could guide future GNN architecture designs. 4. Broad Applicability: The tensor languagebased analysis is applicable to a wide range of GNN architectures not just specific models. This broad applicability enhances the utility and reach of the presented method. Weak Points: 1. Accessibility and Complexity: The technical depth and the use of specialized language may make the paper less accessible to practitioners or researchers outside the specialized domain of GNN architecture design and theory. 2. Empirical Validation: While the paper presents a solid theoretical framework the inclusion of more empirical studies showcasing the practical impact of using the tensor language to design or optimize GNNs would enhance the paper. Validation of the theoretical findings through experiments can provide additional support for the claims. 3. Comparison with Existing Methods: The paper could benefit from a more detailed comparison with existing methods or tools used for analyzing GNNs. Explicit discussion on the advantages and potential limitations of the proposed method relative to existing approaches could provide a clearer perspective on its contribution. Questions and Feedback to the Author: 1. Complexity and Scalability: How does the computational complexity of analyzing GNNs through the proposed tensor language compare with that of direct analyses based on WL tests or other graph algorithms 2. Practical Implications: Can the authors provide more insights or potential guidelines on applying the theoretical findings of this paper to actual GNN design or optimization Specifically how can practitioners implement the insights on increasing separation power in designing new GNN models 3. Future Directions: What are the potential limitations of the proposed tensor language approach and how might future research overcome these challenges Furthermore how does the framework adapt to evolving GNN architectures or novel graph learning tasks  Review Summary The paper makes significant technical contributions by introducing a novel tensor language framework to analyze the separation power of Graph Neural Networks (GNNs). It successfully ties the analysis to WeisfeilerLeman (WL) tests offering a unified method to understand the structural discrimination capabilities of various GNN models. The insights into enhancing the separation power of GNNs are impactful and could guide future architecture designs. However the paper could be strengthened by including empirical validations expanding on practical guidelines for GNN designers based on the theoretical findings and providing a more detailed comparison with existing analytical methods. Overall this work represents a significant advance in the theoretical understanding of GNN capabilities and their relationship with graph algorithms like WL tests offering valuable tools and insights for the design of more powerful GNN architectures.", "g1SzIRLQXMM": "Summarize the paper This paper explores strategies to significantly reduce the number of supervised synaptic updates required for training deep neural networks to replicate the primate visual object recognition system. Existing models necessitate trillions of these updates which is impractical compared to the biological development of the visual system. The authors present three complementary methods to alleviate this issue: (1) they demonstrate that only 2 of the typical number of updates are needed to achieve around 80 of a fully trained model\u2019s alignment with adult ventral stream functionality (2) they utilize an improved random distribution of synaptic connectivity to achieve good model performance without any training suggesting evolutionary encoding could provide a functional \u201cat birth\u201d state and (3) they show that training merely about 5 of model synapses can still attain nearly 80 of the brain predictivity score. By combining these strategies the authors propose models that require orders of magnitude fewer updates yet maintain significant alignment with primate visual processing. Pretrained models and code are shared for further exploration. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach to Reduce Supervised Learning: The use of three distinct strategies to drastically reduce the necessity for supervised synaptic updates is a considerable achievement in making deep learning models more biologically plausible. 2. Comprehensive Benchmarking: The extensive benchmarking against the BrainScore metrics provides a solid basis for evaluating the models effectiveness demonstrating their capability to mimic primate visual processing closely. 3. Resource Efficiency: The reductions in required training data epochs and synapse training are likely to result in decreased computational resources and energy requirements making largescale neural network training more accessible. Weak Points: 1. Lack of Biological Plausibility in Weight Compression: While the authors achieve significant reductions in synaptic updates the method of deriving \"at birth\" weight distributions from a pretrained network does not offer a biologically plausible mechanism for synaptic connectivity established through evolution. 2. Generalization to Other Modalities or Complexities: The paper focuses on the visual object recognition system and its unclear how well these methods would generalize to other sensory systems or more complex cognitive tasks. Questions and Feedback: 1. Expand on Biological Plausibility: Can you explore or propose methods that might offer more biologically plausible alternatives to the weight compression technique perhaps inspired by processes observed in actual neural development 2. Generalization of Methods: How well do you anticipate these training reduction techniques would generalize to other models or neural network architectures particularly those designed for tasks beyond visual object recognition 3. Impact on Learning Dynamics: Reducing the number of trained synapses significantly alters the learning dynamics of the network. Can the authors comment on any observed changes in how the network converges or in its sensitivity to initial conditions and hyperparameters Review Summary The paper presents a compelling study on reducing the supervised synaptic updates necessary for training neural networks to model the primate visual object recognition system. The strategies proposed here represent a significant step forward in the quest for more biologically plausible and resourceefficient neural network training methodologies. While the improvements in training efficiency and reduction in computational resource requirements are noteworthy the exploration into more biologically plausible means for synaptic connectivity and the potential for these methods to generalize across tasks and modalities are areas ripe for further research. The paper is wellgrounded in current research and contributes valuable insights and tools for the field notably the openly shared pretrained models and code which encourage reproducibility and further exploration. My recommendation is for publication provided the authors can address the concerns regarding the biological plausibility of weight compression and potentially expand on the implications of their findings for broader applications beyond visual object recognition.", "uxxFrDwrE7Y": " Peer Review of the Paper on Complementary Learning SystemExperience Replay (CLSER) Method for Continual Learning  Summary The paper presents the Complementary Learning SystemExperience Replay (CLSER) a novel dualmemory method for Continual Learning (CL) in Deep Neural Networks (DNNs). It draws inspiration from the complementary learning system (CLS) theory in neuroscience which highlights the importance of the interplay between rapid instancebased learning and slow structured learning for knowledge accumulation and retention. CLSER incorporates shortterm and longterm semantic memories which interact with episodic memory to mitigate catastrophic forgetting by aligning the decision boundaries of the working model with those of the semantic memories. This mechanism enables the model to acquire new knowledge while maintaining a balance between plasticity (ability to learn new tasks) and stability (retaining old knowledge). The authors claim that CLSER operates without depending on task boundaries or assumptions regarding data distribution positioning it as a versatile tool for general continual learning. The paper evaluates CLSER across various benchmark tasks and demonstrates its superiority over existing stateoftheart methods in CL scenarios.  Strengths 1. Novelty and Theoretical Foundation: CLSERs design inspired by the CLS theory in neuroscience represents a significant methodological innovation. This foundation provides a plausible mechanism for achieving a balance between learning new tasks and retaining old information a notorious challenge in CL research. 2. Generalization across Tasks: The authors successfully demonstrate that CLSER can be applied to a variety of learning tasks without specific adjustments for task boundaries or data distribution which is a notable advantage over specialized approaches. 3. Empirical Validation: Comprehensive experiments demonstrate CLSER\u2019s effectiveness compared to several baseline methods across various datasets and settings including classincremental domainincremental and more complex general incremental learning scenarios.  Weaknesses 1. Lack of Detail on Computational Costs: The paper does not fully address the computational and memory overheads associated with maintaining and updating dual semantic memories. Given that resource efficiency is crucial in many applications a detailed analysis in this area would have been beneficial. 2. Limited Insight into Failure Modes: While the paper showcases impressive performance gains it provides limited discussion on the conditions or scenarios under which CLSER might underperform or fail. Understanding these limitations is vital for practical applications and future improvements. 3. Comparison with Biological Systems: The paper briefly mentions the inspiration from neuroscience but does not delve deeply into how closely CLSER mimics actual human learning processes. A more thorough comparison could strengthen the theoretical grounding and potentially highlight areas for further enhancement.  Questions and Feedback to the Author 1. Could the authors elaborate on the computational demands and memory requirements of implementing CLSER particularly in comparison with other CL methods 2. The paper demonstrates CLSER\u2019s superior performance across several benchmarks. Are there specific types of tasks or data distributions where CLSER\u2019s approach may be less effective 3. How closely do the mechanisms of CLSER align with the current understanding of human learning and memory systems Are there aspects of the CLS theory that have not been incorporated but could potentially improve the model further 4. The paper mentions the use of mean teacher approach for accumulating knowledge in semantic memories. Is there a specific reason for choosing this strategy over other potential methods for knowledge distillation or aggregation  Review Summary CLSER represents a significant advancement in the field of continual learning providing an innovative and effective approach to the longstanding problem of catastrophic forgetting in neural networks. By drawing inspiration from neuroscience the authors propose a dualmemory system that balances the acquisition of new information with the retention of previously learned knowledge. The method\u2019s general applicability across various settings without the need for taskspecific adjustments is particularly commendable. However the paper could be improved by addressing the computational and memory requirements of the proposed system and by providing a more nuanced discussion of its limitations and potential failure modes. Additionally a deeper comparison with biological learning systems could offer valuable insights and future directions for making neural networks learn more like humans. In conclusion despite the mentioned areas for improvement the strong empirical results and the method\u2019s grounding in neuroscience theory make CLSER a noteworthy contribution to the field of continual learning. The paper successfully addresses crucial challenges in CL and sets a new benchmark for future research.", "uHv20yi8saL": " Summarize the paper This paper addresses the challenge of optimizing decentralized policies in cooperative MultiAgent Reinforcement Learning (MARL) under nonstationary transition dynamics. It introduces a theoretical analysis that substantiates a monotonic improvement guarantee for two recent actorcritic methods Independent Proximal Policy Optimization (IPPO) and MultiAgent PPO (MAPPO). These methods employ independent ratios for computing policy probabilities and were previously without a theoretical foundation. The paper demonstrates that despite the nonstationarity caused by independent ratios a monotonic improvement guarantee can still be achieved by enforcing a centralized trust region constraint over joint policies. The study further shows how this constraint can be pragmatically applied by bounding independent ratios according to the number of agents providing a basis for proximal ratio clipping. Empirical results validate that the strong performance of IPPO and MAPPO can be attributed to enforcing this trust region constraint with the sensitivity of hyperparameters to the number of agents aligning with theoretical predictions.  Strong and Weak Points Questions and Feedback Strong Points: 1. Theoretical Contribution: The paper fills a significant gap in the literature by providing a theoretical foundation for the strong empirical performance of IPPO and MAPPO. It extends the monotonic improvement guarantee to decentralized policies in MARL with nonstationary dynamics a notable advancement. 2. Practical Implications: Through demonstrating the importance of the trust region constraint and how it can be enforced via ratio clipping the paper contributes practical insights for designing and tuning MARL algorithms. 3. Empirical Validation: The study includes empirical results that not only support the theoretical analysis but also offer actionable insights into how hyperparameter settings should be adjusted based on the number of agents involved. Weak Points: 1. Generalizability of Results: The empirical validation is primarily conducted on the StarCraft MultiAgent Challenge (SMAC). Additional experimentation across a broader range of environments could strengthen the findings. 2. Detail on Hyperparameters Sensitivity: While it is mentioned that hyperparameters are highly sensitive to the number of agents a deeper exploration into this relationship and how it might affect scalability and generalization across tasks would be valuable. 3. Comparison with Other MARL Methods: The paper focuses on comparing IPPO and MAPPO with each other and previous methods they outperform. Including comparisons with other stateoftheart MARL methods especially those emerging concurrently or after could provide a more comprehensive view of their relative strengths and weaknesses. Questions and Feedback: 1. Can the authors elaborate on how the theoretical findings might inform the choice of hyperparameters for new MARL problems Specifically how should one approach setting these parameters in significantly different environments from SMAC 2. Is there potential for the trust region analysis to be extended or modified to improve the scalability of IPPO and MAPPO to larger numbers of agents or more complex environments 3. Feedback: A future direction could involve exploring how the theoretical framework and findings might apply or be adapted to competitive or mixed cooperativecompetitive MARL settings. Addressing the broader applicability of these insights could significantly enhance their impact.  Review Summary This paper presents a significant theoretical contribution to cooperative MARL by analyzing the performance guarantees of IPPO and MAPPO two recent actorcritic algorithms under nonstationary transition dynamics. By establishing a foundational analysis for the monotonic improvement guarantee and demonstrating how a trust region constraint enforced through ratio clipping supports strong empirical performance the work advances both theoretical and practical understanding in the field. The studys strengths notably outweigh its limitations. It not only advances theoretical knowledge but also provides practical guidance that can inform the development and tuning of MARL algorithms. However expanding the empirical validation to a wider array of environments and providing deeper insight into the sensitivity and scalability relationship of hyperparameters could enhance the robustness of the findings. Given the papers contributions and the opportunities it presents for future research it represents a valuable addition to the literature on cooperative MARL. Recommendations for future work include exploring the generalizability of these findings across varying MARL settings and further investigating the implications of hyperparameter sensitivity on algorithm scalability and adaptability.", "qsZoGvFiJn1": " Peer Review on \"Leveraging Historical LiDAR Data for Improved Detection in Autonomous Driving\"  1. Summary of the paper The paper introduces a novel framework called HINDSIGHT which enhances the performance of 3D object detection in autonomous driving systems. Taking advantage of historical LiDAR scans collected from past traversals of the same scene the framework aims to address the issue of detecting small faroff and occluded objects  a common challenge in autonomous driving due to the sparsity of data points for such objects in LiDAR point clouds. The authors propose to encode these historical scans into a compact easilyqueryable data structure named SQuaSH (SpatialQuantized Sparse History features). During operation the current LiDAR point cloud can be enriched with contextual information from SQuaSH thereby improving the object detection capabilities of the vehicle. The framework is designed to be endtoend trainable and is demonstrated to work well with several modern 3D detection architectures showing impressive improvements in detection accuracy especially for the mentioned challenging cases. This is validated through experiments on two largescale realworld autonomous driving datasets.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The idea of utilizing historical LiDAR data to improve current 3D object detection seems both innovative and practical. It effectively leverages previously discarded data providing a costeffective way to enhance perception accuracy.  Compatibility: The ability of HINDSIGHT to integrate with existing 3D detection architectures without requiring specialized training or additional labels is a significant advantage. It offers a plugandplay improvement to current systems.  Significant Performance Improvements: The reported improvements especially in detecting challenging objects (over 300 in some cases) are commendable. This could greatly enhance the safety and reliability of autonomous vehicles. Weak Points:  Generalization to Different Environments: The paper focuses on improvements under the assumption of repeated traversals in similar environments. It would be valuable to understand how this approach generalizes to environments that are not frequently traversed or have highly dynamic conditions.  Deployment Challenges: While the framework appears to reduce the latency and storage concerns practical deployment challenges including managing and updating the historical data repository in a realworld operational vehicle fleet are not deeply discussed.  Adversarial Conditions: The performance of the proposed method under adverse weather conditions significant seasonal changes or drastic alterations in the urban layout is not clear. Questions and Feedback: 1. How does HINDSIGHT handle scenarios where significant environmental changes have occurred between past traversals and the current scenario (e.g. construction sites new road layouts) 2. Could the authors elaborate on the potential privacy concerns with storing and processing historical traversal data While the data is heavily quantized could there be scenarios where sensitive information is inadvertently captured or inferred over time 3. A comparison with systems utilizing highdefinition (HD) maps for context would be insightful. How does HINDSIGHT compare in terms of accuracy latency and storage requirements  3. Review Summary Overall the paper presents a compelling approach to enhancing 3D object detection for autonomous vehicles by utilizing historical LiDAR data. The novel idea substantial performance improvements and compatibility with existing detection systems are the strongest aspects of this work. However questions remain regarding its generalization across varied environments handling environmental changes deployment challenges and privacy concerns. The substantial improvement in detection accuracy particularly for challenging objects is a significant achievement that could have a profound impact on the safety and reliability of autonomous driving systems. The innovative use of previously discarded historical data is both efficient and practical potentially opening new avenues for leveraging such data in autonomous driving and beyond. Given the strong results and contributions with considerations for addressing the outlined weaknesses and questions I recommend this paper for publication. It provides valuable insights into enhancing perception systems for autonomous vehicles and the proposed ideas could inspire further research in this vital area of study.", "okmZ6-zU6Lz": " Peer Review  Summary of the Paper The paper explores the challenge of understanding and measuring the controllability of largescale networked dynamical systems when complete and detailed knowledge of the networks structure is unavailable. It innovatively applies learningbased approaches to infer the systems controllability using coarse summaries of the network which typically represent available information about realworld systems such as power grids or brain networks. The authors delve into how community structures within these networks can be leveraged to assess controllability through a stochastic block model (SBM). They propose two methodologies: one building upon Model Order Reduction (MOR) techniques and another employing a direct learning approach. The work culminates in deriving and validating theoretical error bounds associated with both methods offering insights into when and how these approaches might be effectively applied in practice.  Strong Points 1. Novel Approach: The study is pioneering in its use of learningbased frameworks for inferring finescale network controllability from coarse summaries addressing a significant challenge in the control of complex systems. 2. Theoretical Contribution: The derivation of tight upper bounds on approximation and estimation errors is rigorous and offers valuable theoretical insights into the contingent factors affecting accuracy. 3. Relevance: The application context particularly the examples related to medical technology and brain networks underscores the high relevance and potential impact of this work on critical domains. 4. Clarity: Despite the complexity of the subject the paper is structured in a manner that guides the reader through the problem statement methodologies and findings with a clear exposition of theoretical underpinnings.  Weak Points 1. Assumption Limitation: The reliance on specific assumptions such as the network being generated by an SBM or conditions for stability and controllability may limit the generalizability of the findings to networks that do not meet these criteria. 2. Comparison with Existing Methods: While the paper positions its approach against traditional MOR it does not provide an extensive comparison with other existing methods for inferring controllability in complex networks especially those not based on model reduction or learning. 3. Practical Implications and Scalability: The paper could benefit from a deeper discussion on the practical implications of the findings including considerations of computational complexity and scalability for very large networks.  Questions and Feedback to the Author 1. Generalizability: How do the authors perceive the applicability of their methods to networks that do not conform to an SBM or similar structured models Are there possibilities for adapting the framework to accommodate a broader range of network types 2. Comparison with NonMOR Methods: Could the authors elaborate on how their learningbased approach compares with other nonMORbased strategies for understanding network controllability Insights into strengths weaknesses or complementary aspects would be valuable. 3. Practical Application: Are there ongoing or planned efforts to apply this theoretical framework to realworld networks particularly in contexts like brain network studies for neurological disorders Such insights could significantly enhance the papers impact.  Review Summary This paper introduces a novel and theoretically robust framework for estimating the controllability of finescale networked systems using coarse data representations a topic of significant importance in systems and control theory. It marks a critical step forward especially in applications where detailed network models are unattainable. The theoretical contributions regarding the derivation of error bounds are particularly noteworthy and lay a solid foundation for future research in this domain. However the paper could be strengthened by addressing its limitations regarding the assumptions generalizability providing a broader comparison with existing methods beyond MOR and discussing the practical implications and feasibility of applying the proposed methods to realworld networks. Addressing these points can significantly broaden the papers appeal and applicability making its insights more valuable for both the theoretical and applied research communities in network science and systems control.", "qHsuiKXkUb": " 1) Summary of the Paper This paper addresses the challenges associated with highresolution image generation using diffusion models focusing on theoretical and practical issues that emerge when working with high precision diffusion levels. The core problem lies in the unbounded nature of the data score as the diffusion time approaches zero which complicates the training of diffusion models due to instability and poor local sample fidelity. To overcome these difficulties the authors propose a novel approach that consists of three main improvements: 1. Alternative Parametrization: They introduce a new parametrization method that allows for the estimation of unbounded data scores by making the time argument extendable to infinity at t  0. This parametrization is applicable to various diffusion models leading to the development of High precision NCSN (HNCSN) and High precision DDPM (HDDPM). 2. Soft Truncation (ST) Trick: A practical optimization technique that mitigates extreme variations in score scales by partitioning the diffusion time for each minibatch update allowing for effective training across different diffusion times without sacrificing global fidelity. 3. Reciprocal Variance Exploding SDE (RVESDE): A new Stochastic Differential Equation (SDE) formulation that avoids the theoretical and practical limitations of existing diffusion processes by ensuring a geometric variance property thereby enhancing sampling precision at small diffusion levels. The paper validates these improvements through extensive experiments on various benchmark datasets demonstrating stateoftheart performances in highresolution image generation tasks such as CelebAHQ. An ablation study further confirms the individual contributions of the proposed methods towards performance enhancement.  2) Review: Strong Points Weak Points Questions and Feedback Strong Points:  Theoretical Rigor: The paper presents a theoretically sound approach to addressing the unbounded data score problem in diffusion models supported by mathematical proofs and a solid conceptual foundation.  Comprehensive Solution: The proposed improvements tackle both the theoretical and practical challenges in highresolution image generation offering a holistic solution that enhances both density estimation and sample fidelity.  Empirical Validation: Extensive experimental results including an ablation study effectively demonstrate the efficacy of the proposed methods across multiple benchmark datasets providing a convincing argument for their practical utility. Weak Points:  Complexity and Implementational Challenges: The paper introduces sophisticated technical innovations that may pose significant implementational challenges potentially limiting accessibility for some practitioners.  Computational Efficiency: While the paper focuses on improving model performance there is limited discussion on the computational costs associated with the proposed methods particularly regarding training time and resource requirements. Questions and Feedback: 1. Can the authors elaborate on the computational efficiency of their proposed methods Specifically how do the training time and resource requirements compare to existing approaches 2. How robust are the proposed improvements in contexts beyond highresolution image generation such as other types of generative tasks 3. The paper provides compelling theoretical and empirical support for its claims. However it would be beneficial to see a discussion on potential limitations or scenarios where the proposed methods might not perform optimally.  3) Review Summary The paper presents a significant contribution to the field of highresolution image generation with diffusion models. By addressing the unbounded data score problem through three innovative improvements the authors achieve stateoftheart performance across several benchmark datasets. The theoretical rigor coupled with extensive empirical validation underscores the robustness of the proposed approach. However considerations regarding computational efficiency and broader applicability warrant further exploration. Overall the strengths of the paper outweigh its weaknesses making it a valuable addition to the literature on generative modeling. The proposed methods not only advance our understanding of diffusion models but also open avenues for further research in highprecision generative tasks.", "rrWeE9ZDw_": " Summarize the Paper This paper presents a novel approach for learning objectcentric representations for planning in continuous highdimensional environments using minimal samples. By focusing on the commonality of objects across different tasks it develops a method that allows agents to transfer learned representations and efficiently learn models for new tasks. The approach is showcased through applications in a 2D crafting domain and Minecraft tasks demonstrating an ability to quickly form complex longterm plans based on representations learned directly from pixel data. The core of the methodology includes autonomously learning lifted typed objectcentric representations and integrating them with tasklevel planners. The contributions are significant as they provide a framework for scaling modelbased methods in reinforcement learning by exploiting object commonalities leading to more sampleefficient learning.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The paper proposes a unique method for leveraging objectcentric representations to achieve sampleefficient learning. Its especially commendable that the approach can generalize across objects and tasks showcasing the ability to learn from minimal samples. 2. Practical Utility: Applying the method to both a simplified crafting domain and complex Minecraft tasks demonstrates practical utility and scalability. Furthermore integrating these representations with existing tasklevel planners is a significant stride toward solving complex planning problems. 3. Sound Methodology: The paper outlines a clear and methodologically sound approach to learning abstract representations including generating propositional models lifting to typed models and taskspecific instantiation. This structured approach is a strong foundation for reproducibility and further research. Weak Points: 1. Assumption of Object Individuation: The method relies heavily on the agents ability to individuate objects within its environment which might not always be practical or possible in highly dynamic or densely cluttered environments. How does the system perform under such conditions 2. Computational Efficiency: While the learning approach is sampleefficient the paper does not clearly address the computational demands of processing highdimensional pixel data and learning objectcentric representations. Can the authors comment on the computational cost and its scalability 3. Lack of Comparative Baseline: The paper could be strengthened by including comparisons with other stateoftheart methods in similar domains. How does this approach perform in terms of efficiency and effectiveness compared to other modelbased or modelfree learning methodologies Questions and Feedback:  Could the authors elaborate on the limitations encountered when dealing with environments where object individuation is not straightforward  It would be beneficial for readers if the authors included insights or data on the computational resources required for implementing their methodology particularly when scaling to complex environments like Minecraft.  Incorporating a comparative analysis against other modern approaches in reinforcement learning could significantly bolster the papers impact. Can the authors provide empirical comparisons or justify the absence of such comparisons  Review Summary The paper introduces a compelling and innovative method for learning objectcentric representations in highdimensional environments aiming to improve sample efficiency in reinforcement learning tasks. By leveraging the consistency of object types across different tasks the authors have developed a framework that allows for the transferability and generalization of learned models enabling agents to quickly learn and plan for new tasks with minimal sample requirements. The methodology presented is robust with clear steps outlined for learning from pixel data generating propositional and lifted typed models and finally taskspecific instantiations. These efforts are commendably applied to both simplified and complex domains illustrating the approachs potential to advance planning and learning in continuous spaces. Nonetheless the paper would benefit from a deeper exploration into the limitations of object individuation in cluttered or dynamic environments providing insights into the computational demands of the approach and enhancing the work with comparisons to other stateoftheart methods. Addressing these points could elevate the papers contribution and offer a more comprehensive view of its position within the broader research landscape on modelbased reinforcement learning. The proposed method stands out for its innovative use of objectcentric abstraction and transferability positioning it as a significant step forward in tackling longterm sparsereward challenges in reinforcement learning. With some enhancements particularly in addressing the identified weak points this work has the potential to make a longstanding impact on the field.", "n6Bc3YElODq": " 1) Summarize the paper The paper introduces ModelBased Opponent Modeling (MBOM) an approach designed to enable an agent to adapt its policy effectively when interacting with various types of opponents in multiagent environments. MBOM utilizes environment modeling to simulate and imagine improving opponent policies through a process called recursive imagination allowing adaptation to fixed policy opponents na\u00efve learners and reasoning learners. The imagined opponent policies are then mixed based on their similarity to the opponents real behaviors updated via Bayesian rule. The MBOM approach was evaluated in competitive and cooperative tasks against different types of opponents showing superior adaptation capabilities compared to existing methods.  2) List strong and weak points of the paper Question and Feedback to the author Strong Points:  Novel Approach: The introduction of recursive imagination and Bayesian mixing for modeling opponent policies is original and addresses a significant gap in existing research. These features enable MBOM to adapt dynamically to opponents\u2019 policy improvements over time without assumptions regarding opponents learning algorithms.  Versatility: MBOM is tested across competitive and cooperative environments showcasing its robustness and versatility in handling various interaction dynamics and opponent types.  Empirical Evaluation: The method is thoroughly evaluated with strong baseline comparisons demonstrating MBOMs effectiveness in adapting to opponents more efficiently than current methods. Weak Points:  Scalability and Computational Overhead: The paper mentions the increased computation cost with the size of the joint opponent. This scalability issue is critical in multiagent environments with many agents and the paper does not provide a clear solution to this problem.  Assumption on Cooperation Among Opponents: MBOM implicitly assumes a fully cooperative relationship among opponents when treated as a joint opponent. This assumption may not hold in many realworld scenarios where opponents may have complex noncooperative relationships.  Lack of Theoretical Analysis: While the empirical results are promising the paper lacks a rigorous theoretical analysis of MBOM\u2019s convergence properties stability and the conditions under which it outperforms other methods. Questions and Feedback: 1. Can the authors expand on the computational overhead associated with MBOM specifically in scenarios with a large number of agents Is there a way to mitigate this overhead without compromising the modeling accuracy 2. The paper could benefit from a deeper exploration of scenarios where opponents have noncooperative relationships. How would MBOM adapt in such environments and what modifications if any would be required 3. A theoretical analysis highlighting MBOMs convergence stability and effectiveness compared to other modeling approaches could strengthen the paper. Are there specific conditions or settings under which MBOM is particularly advantageous or disadvantaged  3) Review Summary ModelBased Opponent Modeling (MBOM) presents a novel and effective approach to adapting an agents policy in multiagent environments by simulating opponent policies and using Bayesian mixing to adjust to various types of opponents. The method\u2019s robustness is demonstrated through comparative empirical evaluations across different environments and opponent types showing superior adaptation capabilities. However the paper falls short in addressing the scalability of the approach especially concerning the computational overhead in environments with many agents. Furthermore the assumption of fully cooperative relationships among opponents might limit the applicability of MBOM in more complex scenarios. Lastly the lack of a theoretical foundation leaves questions about when and why MBOM is effective compared to other methods. Despite these shortcomings MBOM represents a significant step forward in opponent modeling in multiagent systems. With further exploration into scalability noncooperative dynamics and theoretical underpinnings MBOM has the potential to contribute broadly to the field of reinforcement learning.", "hSktDu-h94": " Peer Review of \"Automatic Prediction and Optimization Connector (APOC) for PredictThenOptimize Problems\"  1. Summary of the Paper The paper introduces a novel framework named Automatic Prediction and Optimization Connector (APOC) aimed at improving predictthenoptimize (PTO) problems where combinatorial optimization decisions are made based on predicted parameters from side information. Recognizing the challenges arising from the mismatch between continuous prediction results and discrete optimization decisions particularly when using the conventional l2 loss the authors propose two new loss functions: the Total Group Preorder (TGP) loss and its differentiable variant the Approximate Total Group Preorder (ATGP) loss. These are designed to better align prediction losses with optimization goals in problems with a strong ranking property. The ATGP loss in particular leverages a family of DerivativeofGaussian wavelet functions to capture group comparison relationships offering a smoother approximation conducive to gradientbased optimization. A searching algorithm is then developed within the APOC framework to automatically identify suitable ATGP loss functions for varying PTO problems. Experimental validation is conducted across three classic combinatorial problems: ranking knapsack and shortest path problems with the proposed method demonstrating superior performance over existing approaches.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Theoretical Contributions: The introduction of TGP and ATGP losses as well as the APOC framework represents a significant theoretical advancement. These innovations address the challenge of aligning continuous prediction models with discrete optimization tasks in a novel and theoretically grounded manner.  Comprehensive Evaluation: The extensive experiments conducted on three different combinatorial problems provide strong empirical evidence supporting the effectiveness of the proposed method.  Reproducibility and Openness: Providing the source code is an excellent practice that promotes reproducibility and allows the community to further investigate and build on the proposed method. Weak Points:  Generalizability Concerns: While the paper presents results on three combinatorial problems the broader applicability of the APOC framework to a wider range of predictthenoptimize problems remains to be fully explored.  Complexity and Implementation Details: The paper could benefit from a more detailed discussion on the computational complexity of the APOC framework and practical considerations regarding its implementation and deployment in realworld PTO problems.  Lack of Comparative Analysis with Human ExpertDesigned Loss Functions: Given the emphasis on automating loss function design a comparison with expertdesigned loss functions in specific domains could have provided insights into the advantages and limitations of the automated approach. Questions and Feedback:  Could the authors elaborate on how the APOC framework might be extended to PTO problems beyond those with strong ranking properties  How does the computational cost of employing the APOC framework compare with traditional approaches particularly in terms of training time and resource requirements  The authors mention the potential for future work to reduce the searching time within the APOC framework. Could they speculate on possible strategies or methods that might achieve this goal  3. Review Summary The paper presents a significant contribution to the field of optimization and machine learning by addressing a critical problem at the intersection of prediction and optimization. The proposed APOC framework along with the novel TGP and ATGP loss functions offer a promising approach to improving the alignment between continuous predictions and discrete optimization decisions. Theoretical underpinnings combined with solid experimental validation across several PTO problems highlight the methods effectiveness and potential applicability. However further investigation into the generalizability of the approach its computational efficiency and comparisons with domain expertdesigned loss functions would enhance the papers impact. Overall I recommend the paper for publication considering its novel contributions and the potential for significant advancements in the predictthenoptimize problem space.", "mQDpmgFKu1P": " Peer Review  Summarize the paper The paper introduces a novel attention module termed \"implicit selfattention\" and incorporates it into a Legendre Memory Unit (LMU) based model for language modeling tasks. This model aims to address the computational and memory inefficiencies of transformer models particularly their quadratic growth in requirements with sequence length. By leveraging the LMU for temporal representation and compression of input sequences the authors achieve notably lower computational and memory complexities specifically O(n) for memory and O(n log n) (or better) for computation. Additionally the integration of global selfattention with the LMU architecture further improves performance. The study demonstrates through experiments that the LMUbased model even at a smaller scale offers loss improvements over traditional transformers comparable to the improvements transformers have over LSTMs thereby indicating superior data efficiency and scaling properties.  Strong and Weak Points Questions and Feedback Strong points: 1. Innovation in Architecture: The novel application of the LMU for temporal compression in combination with implicit selfattention is a significant contribution. This approach not only addresses the memory and computational bottlenecks of transformers but does so in a way that maintains if not enhances model efficacy. 2. Improved Efficiency: Demonstrating that the LMUbased model has linear memory requirements and n log n computational requirements represents a substantial improvement over traditional transformer models with significant implications for training larger models or deploying models in resourceconstrained environments. 3. Empirical Validation: The experimental results showing improved loss reductions and data efficiency strongly support the paper\u2019s claims providing a solid empirical foundation for the models advantages. Weak Points: 1. Lack of Detailed Analysis on Model\u2019s Limitations: While the paper extensively demonstrates the models strengths it could benefit from a deeper discussion on any limitations or scenarios where the LMU model might underperform. Understanding these aspects is crucial for guiding future research and applications. 2. Comparability with StateoftheArt Models: The comparison primarily focuses on traditional transformers and LSTMs. A broader comparison including stateoftheart transformer variants optimized for efficiency would provide a clearer picture of the proposed model\u2019s standing. 3. Exploration of Task Generalization: The focus is primarily on language modeling. Investigating the model\u2019s performance on a wider variety of NLP tasks (e.g. question answering machine translation) could further validate its utility and robustness. Questions and Feedback: 1. Could the authors elaborate on potential limitations or failure modes of the proposed model 2. Have there been any experiments on tasks other than language modeling to examine the models adaptability and performance across different NLP tasks 3. Is there potential for the implicit selfattention mechanism to be integrated into other neural architectures and if so what might those be  Review Summary This paper presents a significant advancement in the field of NLP through the introduction of a novel model architecture that combines LMUs with an implicit selfattention mechanism achieving superior computational and memory efficiency compared to traditional transformer models. The empirical results strongly support the model\u2019s efficacy showcasing its potential for scaling and data efficiency. However the discussion could be enriched by addressing the models limitations and extending the comparison to include a broader range of contemporary models. Additionally exploring the models adaptability to various NLP tasks beyond language modeling could further underscore its utility and potential impact on the field. Overall the paper contributes a highly valuable approach to addressing some of the key limitations of current transformer models marking a promising direction for future research in efficient NLP model design.", "qnQN4yr6FJz": " Peer Review for the Paper on Distributional Prediction with Incomplete Features  1) Summary of the Paper This paper addresses the challenge of predicting distributional outcomes when some feature data is missing  a common problem in practical applications like analyzing electronic health records. The authors draw a distinction between two traditional approaches: a generative approach which imputes missing values before prediction and a discriminative approach which modifies the regression procedure to accommodate missing data. Recognizing the limitations of both approaches the authors introduce a novel method that combines the strengths of both through the use of blackbox variational inference (VI). This method is shown to be broadly applicable across various machine learning models especially variational autoencoders (VAEs) and demonstrates superior performance in empirical tests.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper presents a novel approach (DIG) to handling incomplete feature vectors merging the benefits of discriminative and generative models through blackbox VI. This innovation addresses a longstanding dilemma in handling incomplete data.  Broad Applicability: The methods compatibility with modern machine learning architectures particularly VAEs is a significant advantage making it relevant to a wide audience in the machine learning field.  Empirical Validation: The inclusion of numerical experiments especially with VAEs and the comparison with other known methods (e.g. MICE Simple CVAE) effectively demonstrate the practicality and superiority of the proposed method. Weak Points:  Technical Complexity: The paper is densely packed with mathematical formulations and variational techniques which may limit its accessibility to readers who are not wellversed in these areas.  Experimental Scope: While the experiments demonstrate effectiveness they are limited to scenarios of synthetic and controlled missing data. Realworld datasets with naturally occurring missing data patterns could provide a more comprehensive validation.  Comparison with Stateoftheart: The paper could benefit from a more detailed comparison with the very latest techniques in handling missing data especially those that have emerged in parallel to traditional imputation and regression methods. Questions and Feedback:  Could the authors elaborate on the potential impact of their method on datasets with highdimensional feature spaces where missingness is not just random but potentially structured in complex ways  The methods scalability and computational efficiency particularly in comparison to existing methods are not deeply discussed. Can the authors provide insight into the computational overhead introduced by their hybrid approach  It would be beneficial if the authors could discuss any limitations or scenarios where their proposed method may not perform as expected. Are there conditions under which the methods advantages might be diminished  3) Review Summary The manuscript presents a commendable contribution to the literature on handling incomplete data for distributional prediction a challenge pervasive across many data analysis applications. By innovating a hybrid method that unifies the strengths of both generative and discriminative approaches through blackbox variational inference the authors address a critical gap in existing methodologies. The methods broad compatibility with contemporary machine learning models evidenced by empirical validation using VAEs is a significant strength underscoring the papers relevance and potential impact. However the papers technical density and the scope of the experimental validation present areas for improvement. Increasing the accessibility of the concepts to a wider audience and extending the empirical validation to encompass more diverse and naturally incomplete datasets could enhance the manuscripts contribution. Also a deeper comparison with the latest stateoftheart methods alongside a discussion on computational efficiency would provide a clearer understanding of the proposed methods positioning within the current landscape of solutions for incomplete data. In summary the paper is a valuable addition to the field proposing a methodologically sound and empirically effective solution to a wellestablished problem. With considerations for refining its accessibility and expanding its empirical demonstrations the paper is likely to be of great interest to both academic and industrial practitioners in machine learning and data analysis.", "oiZJwC_fyS": " Review  Summary This paper is an innovative attempt to bridge tropical geometry with neural network approximation and compression. The authors provide a novel theoretical framework that uses tropical geometry to establish a bound on the approximation error between two neural networks with one hidden layer and ReLU activations leveraging the Hausdorff distance of their respective tropical zonotopes. Building on this foundation the paper proposes two geometric neural network compression algorithms that utilize the Kmeans clustering algorithm to reduce the number of neurons in the fully connected layers of deep neural networks. These algorithms are then evaluated both theoretically and empirically with a focus on the compression of fully connected layers in convolutional neural networks. The experiments demonstrate the potential of the proposed methods to achieve competitive compression rates without significantly compromising network performance when compared to other existing techniques.  Strong Points 1. Theoretical Contribution: The paper provides a significant theoretical contribution by linking the approximation error of neural networks to the tropical geometry of their structures. This novel perspective opens up new avenues for understanding and optimizing neural network architectures. 2. Compression Algorithms: The development of two novel compression algorithms based on tropical geometry and Kmeans clustering presents an innovative approach to neural network compression. These methods offer a structured way to reduce network complexity with theoretical guarantees on error bounds. 3. Empirical Evaluation: The comprehensive evaluation of the proposed compression algorithms through experiments on wellknown datasets (MNIST FashionMNIST CIFAR) provides valuable insights into their practical utility and effectiveness in comparison to both tropical and nontropical compression methods.  Weak Points 1. Generalization to Deeper Networks: Although the paper provides a strong foundation for compression of networks with one hidden layer it is less clear how these methods generalize to deeper more complex networks. The extension and scalability of the proposed methods to architectures with multiple hidden layers or different activation functions need further exploration. 2. Comparison with a Broad Range of Methods: The comparison of the proposed methods primarily focuses on tropical geometrybased techniques and a few selected nontropical methods. Inclusion of a wider array of stateoftheart compression methods including those using pruning quantization or knowledge distillation could provide a more comprehensive understanding of the competitive landscape. 3. Detail on Computational Complexity: While the paper discusses the theoretical and empirical aspects of the proposed compression algorithms there is limited discussion on their computational complexity and efficiency. Given the use of Kmeans clustering understanding the computational demand and its scalability with network size and dimensionality would be beneficial.  Questions and Feedback 1. Extension to Deeper Networks: Can the authors discuss how the proposed methods could be extended or adapted to compress deeper neural networks with multiple hidden layers or different types of layers (e.g. convolutional layers) 2. Comparison Range: Could the authors provide insight into how their methods might compare with other types of network compression techniques such as pruning quantization or knowledge distillation which were not included in the experimental comparison 3. Computational Complexity: Could the authors provide details on the computational complexity of their algorithms especially in the context of larger and deeper networks  Review Summary The paper makes a notable contribution to the field by introducing a novel theoretical framework that applies tropical geometry to the compression of neural networks. The development of two geometric compression algorithms and their empirical evaluation against existing methods demonstrate the potential of tropical geometry as a tool for neural network compression. However the paper could be improved by addressing the generalization of these methods to more complex network architectures expanding the comparison with a broader range of existing compression techniques and providing a detailed analysis of the computational complexity. Overall the strengths of the paper particularly its theoretical contributions and novel compression algorithms are significant. With further development and clarification on the points mentioned the paper could make a substantial impact on the field of neural network compression.", "w01vBAcewNX": " Peer Review  Summary of the Paper The paper addresses the problem of learning from expert data with unobserved confounders in the context of both imitation and reinforcement learning (RL). It highlights the challenges when the context observed by the expert is not accessible during learning causing a covariate shift between the expert data and the environment where the learning occurs. Through a structured analysis the paper defines this problem within a contextual Markov Decision Process (MDP) framework. The authors rigorously analyze the imitation learning problem presenting theoretical impossibility results under certain conditions and then extend the discussion to reinforcement learning where additional rewards are available. They propose a novel algorithm that leverages corrective data sampling to counteract the effects of the unknown covariate shift with proofs of convergence to optimal solutions. This methodology is empirically validated on simulated tasks in assistive healthcare and recommender systems where it shows promise in efficiently utilizing confounded expert data. The work bridges concepts from causal inference and reinforcement learning to address a significant practical hurdle in learning from expert demonstrations.  Strong Points 1. Theoretical Foundation: The paper presents a solid theoretical analysis of the problems at hand delineating conditions under which imitation learning is possible and those that lead to challenges due to covariate shift. The possibility and impossibility results are particularly insightful adding depth to our understanding of the learning dynamics in these settings. 2. Algorithmic Contributions: The development of a new algorithm with corrective sampling to deal with unknown covariate shifts in RL is notable. The thoroughness in proving convergence and optimality of the proposed solutions adds significant value to the work. 3. Empirical Validation: The application and testing of the proposed methodologies on simulated tasks in two distinct areas (assistive healthcare and recommender systems) provide practical insights into their effectiveness and utility. The extensive experiments strengthen the papers claims and illustrate potential realworld applications. 4. Interdisciplinary Approach: By effectively bridging concepts from causal inference and RL the paper opens up new avenues for research in both fields. The causal perspective on learning from confounded expert data is particularly innovative and could inspire further interdisciplinary work.  Weak Points and Feedback 1. Practical Applicability: While the simulations are insightful the applicability of the proposed methods in realworld settings beyond the tested simulations could be further discussed. Information on computational demands and scalability would benefit practitioners evaluating the feasibility of these approaches. 2. Limitation Discussion: The paper could benefit from a more detailed discussion on limitations. For instance how sensitive are the proposed methods to the assumptions made about the bounds of sensitivity and covariate shifts 3. Comparison to Baselines: While the paper demonstrates the effectiveness of the proposed methods a comparison with existing stateoftheart approaches in both imitation and RL under similar conditions (i.e. confounded expert data) would provide a clearer picture of the contributions significance. 4. Extension to Other Forms of Learning: The focus is primarily on imitation and reinforcement learning. It would be insightful to discuss or hypothesize how the findings and methodologies might apply or extend to other learning paradigms or problem settings.  Review Summary The paper makes significant theoretical and algorithmic contributions to the domain of learning from confounded expert data in imitation and RL settings. By rigorously analyzing the effects of covariate shifts and proposing a novel sampling procedure to address these challenges the work advances our understanding of how to effectively utilize expert demonstrations even in the presence of unobserved confounders. The empirical validation further strengthens the papers contributions although discussions on practical applicability comparisons with existing methods and an exploration of limitations could provide a more holistic view. Overall this paper is a valuable addition to the literature offering new perspectives and tools for researchers and practitioners dealing with expert data in complex learning environments.", "qvUJV2-t_c": " Summary of the Paper The paper presents an innovative line search method tailored for Deep Learning optimization specifically aimed at improving the process of choosing optimal step sizes in stochastic gradient descent (SGD) without requiring handtuned learning rates. This technique named LargeBatch Parabolic Approximation Line Search (LABPAL) capitalizes on empirical observations indicating that the fullbatch loss locally resembles a parabolic shape in the direction of SGD update steps and that the optimal step size evolves slowly. LABPAL operates by approximating the fullbatch loss with a parabola estimated from multiple minibatches enabling efficient learning rate determination during training. This approach not only demonstrated superiority over SGD with manually tuned piecewise constant learning rates and other line search methods in various tests across different models datasets and batch sizes but also showed resilience to high gradient noise situations by adapting well to small batch sizes.  Strong and Weak Points of the Paper  Strong Points: 1. Novelty and Practicality: The introduction of a line search method that accommodates the stochastic nature of deep learning optimization is innovative. The practical implications of such a method which reduces the need for handtuning learning rates are profound for the deep learning community. 2. Empirical Validation: The extensive experiments conducted covering various models datasets and conditions (particularly high gradient noise scenarios) provide strong empirical evidence supporting the methods effectiveness. 3. Indepth Analysis: The paper provides a comprehensive analysis including performance hyperparameter sensitivity runtime and memory consumption which contributes significantly to understanding the methods practicality.  Weak Points: 1. Theoretical Foundation: The paper lacks a rigorous theoretical foundation to explain why the fullbatch loss behaves parabolically which leaves some uncertainty about the methods general applicability. 2. Limitation in Application Scope: The methods performance and effectiveness were mainly tested on classification tasks. Its applicability to other types of deep learning tasks (e.g. object detection segmentation) remains unclear. 3. Complexity and Overhead: Despite claims of low computational overhead the necessity to sample several minibatches to approximate the fullbatch loss might introduce significant computational complexity and slower training speeds especially in largescale applications.  Questions and Feedback for the Author: 1. Could the authors elaborate on the theoretical justification behind the observed parabolic behavior of the fullbatch loss 2. How does LABPAL perform in deep learning tasks beyond classification such as natural language processing or generative models 3. The discussion about adapting to varying gradient noise is interesting. Could the authors provide more insights into how they envisage the methods adaptability to realworld scenarios where computational resources are constrained  Review Summary This paper introduces a promising line search method LABPAL aiming to automate the adjustment of learning rates in deep learning leveraging the empirical observation that the fullbatch loss can be approximated as a parabola in the update direction. The experimental validation of LABPAL across a wide range of scenarios including its resilience to high gradient noise is commendable and establishes its potential as a practical tool for deep learning optimization. However the paper falls short in providing a theoretical basis for the observed empirical phenomena it relies on and its performance in tasks beyond classification remains uncertain. Additionally the computational overhead particularly regarding the necessity to sample large minibatches might pose practical challenges. Despite these concerns the papers strengths in introducing a novel optimization technique backed by extensive empirical testing outweigh its weaknesses. My recommendation is for acceptance contingent on the authors addressing the theoretical underpinnings of their method and clarifying its applicability and computational efficiency in broader deep learning tasks and scenarios.", "pC00NfsvnSK": "Summarize the Paper This paper introduces Generalized Similarity Functions (GSF) a novel selfsupervised learning algorithm designed to enhance zeroshot generalization in offline Reinforcement Learning (RL) by leveraging contrastive learning. The algorithm aggregates latent representations of observations by comparing their expected future behavior quantified through generalized value functions (GVF). This approach not only allows overcoming the limitations imposed by offline RL settings such as the reliance on a fixed dataset and the inability to explore new states actively but also improves upon existing stateoftheart methods in zeroshot generalization performance on a complex new benchmark dubbed offline Procgen. The paper thoroughly explores the theoretical underpinnings of GSF provides empirical evidence of its superiority over existing methods and discusses the impact of different hyperparameters and cumulant functions. Strong Points: 1. Novelty: GSF represents a significant innovation in offline RL and representation learning addressing a critical challenge of zeroshot generalization through a unique application of contrastive learning. 2. Theoretical Foundation: The paper provides a comprehensive theoretical analysis of GSF grounding its methodology in solid mathematical principles that support the proposed approach. 3. Empirical Validation: The extensive experimental comparison on the offline Procgen benchmark demonstrates GSFs effectiveness in improving zeroshot generalization in offline RL offering strong empirical evidence to back up the theoretical claims. 4. Relevance: Tackling zeroshot generalization is crucial for deploying RL in realworld applications. This work directly addresses this challenge contributing to the advancement of RL research and its applicability to complex problems. Weak Points: 1. Complexity: The algorithms theoretical grounding while robust is complex and might be challenging for readers unfamiliar with deep theoretical analysis in RL. 2. Generality of Results: While the performance improvements on the offline Procgen benchmark are convincing the paper could benefit from experiments in additional settings or benchmarks to verify the generality of GSFs effectiveness. 3. Hyperparameter Sensitivity: The discussion around the impact of hyperparameters and cumulant functions hints at potential sensitivity. A deeper exploration or guidance on selecting these for different scenarios would be beneficial. Questions and Feedback: 1. Could the authors expand on the potential applications of GSF in realworld scenarios Gaining understanding on this front could further highlight the importance of the proposed method. 2. Are there specific domains or problems where GSF might not perform as expected Any insights or speculative discussion on scenarios where GSF might face limitations could help future research navigate potential pitfalls. 3. How sensitive is GSF to the choice of cumulant functions While the paper briefly discusses this more detailed guidance or empirical insight on selecting appropriate cumulant functions could enhance replicability and applicability. Review Summary: This paper presents a compelling and novel approach to address one of the significant challenges in RL  zeroshot generalization in an offline setting. Through the introduction of Generalized Similarity Functions (GSF) the authors not only contribute a theoretically motivated solution but also demonstrate its effectiveness on a challenging new benchmark. The work stands out for its theoretical depth empirical validation and relevance to a critical area in RL research. Despite a few areas where additional clarification and exploration might enhance the paper the work represents a significant step forward in the field. The proposed methods ability to outperform existing stateoftheart approaches substantiates its value and opens up new avenues for research in offline RL and beyond. Consequently I recommend this paper for publication provided that the authors address the posed questions and consider expanding on the weak points mentioned.", "wIK1fWFXvU9": " Peer Review of \"Exploring the Interactions of Adversarial Training with Noisy Labels\"  1) Summarize the Paper The paper investigates the relationship between adversarial training (AT) and the presence of noisy labels (NL) in data proposing that AT can inherently correct for NL to some extent due to its smoothing effects. The authors introduce a new measure the number of projected gradient descent (PGD) steps required to generate adversarial examples (termed the geometry value \u03ba) to differentiate between correctincorrect and typicalrare data. The paper shows that AT not only distinguishes between correct and incorrect data better than standard training (ST) but also mitigates the negative effects of incorrect data thereby improving model generalization. Moreover they demonstrate the utility of the geometry value \u03ba for sample selection and provide two exemplar applications: developing a robust annotator for unlabeled data under adversarial manipulations and providing confidence scores for label annotations. Experiments on synthetic and realworld datasets validate the approachs effectiveness.  2) Strong and Weak Points Question and Feedback to the Author Strong Points:  Innovative Interaction Study: The paper pioneers in studying the interaction between adversarial training and noisy labels. This provides fresh insights into how adversarial training might inherently correct noisy labels due to its smoothing effects.  Introduction of Geometry Value \u03ba: Proposing the geometry value \u03ba as a new measure to filter incorrect data and further stratify data into typical and rare is innovative. This measure opens new avenues for research in sample selection and data annotation.  Practical Applications: The authors not only present theoretical findings but also develop practical applications such as a robust annotator and confidence scores for label annotations. This demonstrates the realworld applicability and potential impact of their research. Weak Points:  Limited Discussion on AT Limitations: While the paper emphasizes the benefits of adversarial training it may understate the known limitations of AT such as its computational cost and potential for reduced accuracy on nonadversarial examples.  Generalization Beyond Image Datasets: The experiments predominantly focus on image datasets like CIFAR10 and MNIST. It would be valuable to assess if the findings and the effectiveness of the geometry value \u03ba hold across different types of data or domains.  Comparison with Other Methods: The paper could benefit from a broader comparison with existing labelnoise correction techniques. This would place the contribution of the geometry value \u03ba in a wider context and further validate its utility. Question and Feedback to the Author:  Have the authors considered exploring the impact of adversarial training and the geometry value \u03ba in domains other than image classification such as text or audio This could help understand the measures robustness and applicability across different types of data.  It would be intriguing to see a detailed analysis of the tradeoffs involved in using adversarial training for noise correction especially in terms of computational resources and potential impacts on the learning of nonadversarial examples.  As future work investigating how the geometry value \u03ba could be integrated into or compared with other labelnoise correction methods might provide further insights into its strengths and limitations.  3) Review Summary This paper presents a novel exploration of the interaction between adversarial training and noisy labels proposing the innovative concept of the geometry value \u03ba as a measure to distinguish between correctincorrect and typicalrare data. The strong points of the paper lie in its pioneering approach to studying adversarial trainings inherent ability to correct noisy labels its practical applications and the introduction of a new measure for sample selection and data annotation. However the paper could be improved by addressing its limitations such as the need for a broader discussion on adversarial trainings drawbacks extending the experimentation to nonimage datasets and comparing the introduced measure with existing noise correction techniques. Questions raised about exploring other domains and analyzing tradeoffs further exemplify areas for potential improvement and future work. Overall the paper provides valuable insights into the benefits of adversarial training in the context of noisy labels and introduces a promising new measure for data analysis and model training. Its contributions are significant encouraging further research in this interdisciplinary area. I recommend accepting this paper conditional on the authors addressing the highlighted concerns to strengthen the studys impact and applicability.", "olQbo52II9": " Review of \"ECORD: Exploratory Combinatorial Optimisation with Rapid Decoding for GraphBased Problems\"  1) Summary The paper introduces a novel reinforcement learning (RL) algorithm ECORD designed to address the core challenges of combinatorial optimization (CO) on graphs particularly focusing on scalability and speed issues present in current stateoftheart approaches. The highlighted contributions demonstrate that ECORD substantially outperforms existing RL methods in solving the Maximum Cut (MaxCut) problem both in terms of scalability and computational efficiency making it a significant step forward in applying RL to CO problems. The background section effectively sets the stage by discussing the relevance and challenge of CO problems in various domains coupled with a critical examination of approximate and heuristic algorithms. The authors then pivot to how automated learning methods especially RL have emerged as promising avenues for devising novel heuristics without requiring preprocessed data. ECORDs methodology is rooted in the combination of a single preprocessing step using a Graph Neural Network (GNN) and a fast exploratory phase facilitated by a recurrent unit circumventing the scalability limitations faced by continuous application of GNNs in decisionmaking. Experimental results validate ECORDs superiority over stateoftheart RLGNN methods in terms of both performance on the MaxCut problem and computational efficiency. Furthermore the method maintains its performance edge across a range of graph sizes demonstrating impressive generalizability and scalability.  2) Strong and Weak Points Questions and Feedback Strong points:  Innovation and Relevance: ECORD addresses a critical bottleneck in applying RL to CO problems  the scalability issue due to the computational complexity of GNNs. By innovating on the process flow (single GNN preprocessing step followed by a fast exploratory phase) the paper targets a significant gap in the literature.  Experimental Validation: The comprehensive experimental setup and meticulous comparison with stateoftheart methods provide robust evidence of ECORDs effectiveness. The use of realworld applicable graph datasets for validation further underscores its practical utility. Weak points:  Algorithmic Complexity Analysis Missing: While the paper states the computational advantage of ECORD over existing methods a formal algorithmic complexity analysis comparing ECORD with baseline methods is lacking. Such an analysis would strengthen the claim of computational efficiency.  Limited Discussion on Generalization to Other CO Problems: The focus is predominantly on the MaxCut problem. While this is critical for proof of concept extending the discussion to how ECORD might adapt to or perform with other CO problems would offer a broader perspective on its applicability. Questions and Feedback:  Could the authors elaborate on the potential modifications or extensions needed for ECORD to cater to other NPhard CO problems beyond MaxCut  A deeper insight into the limitations faced by ECORD especially in scenarios where it might not perform as expected and potential strategies to mitigate such limitations would be beneficial.  Feedback includes a suggestion to incorporate a section on algorithmic complexity analysis comparing ECORD with the baseline methods to quantify the computational gains more formally.  3) Review Summary The paper introduces ECORD a pioneering algorithm leveraging RL for solving the MaxCut problem on graphs showcasing significant improvements in scalability and efficiency over existing RL approaches. The innovation lies in its technique to circumvent the computational overhead of continuous GNN utilization a considerable advancement in the field of CO. The experimental evaluations persuasively validate the superiority of ECORD in terms of performance and efficiency across various graph sizes marking a noteworthy contribution to leveraging RL in solving NPhard combinatorial optimization problems. However the paper could be strengthened by providing a formal analysis of ECORDs computational complexity and a broader discussion on its adaptability and performance implications across a wider array of CO problems. These enhancements would solidify the papers claims and provide a clearer roadmap for future applications and developments of ECORD. Based on the significant positive findings and the comparative minor gaps identified the recommendation would be to accept the paper with minor revisions addressing the mentioned areas for improvement.", "rczz7TUKIIB": " 1. Summary This paper introduces a novel approach named MetaLoss focusing on metalearning of loss functions specifically for regression tasks with a primary application in forecasting time series data. The study delves into the limitations of traditional fixed loss functions like Mean Absolute Error (MAE) and Mean Square Error (MSE) in regression tasks and explores how a metalearning model can dynamically learn and adapt loss functions to improve forecasting accuracy. MetaLoss operates by cotraining a primary regression neural network (predictor) and a secondary neural network that learns the optimal loss function based on realworld data. This approach is tested across several realworld datasets covering different domains such as mobile data traffic demand and energy consumption demonstrating significant improvements in forecasting accuracy over traditional loss functions. The paper also highlights how MetaLoss can deal with complex timecorrelated and nondifferentiable metrics showcasing its flexibility and potential in challenging forecasting scenarios.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The paper addresses a relatively unexplored area of loss function metalearning for regression tasks offering a novel solution that significantly enhances forecasting accuracy.  Practical Relevance: The application of MetaLoss to realworld datasets and practical scenarios such as mobile network traffic and energy consumption forecasting illustrates its relevance and potential impact.  Experimental Rigor: The authors have conducted thorough experiments with different datasets providing a detailed comparison between MetaLoss and traditional loss functions demonstrating the formers superiority. Weak Points:  Model Complexity and Overhead: The paper lacks a detailed discussion on the computational complexity and potential overhead introduced by implementing MetaLoss. For practitioners understanding the tradeoffs between performance gains and additional computational costs is crucial.  Generalizability and Limitations: While the paper presents impressive results in specific forecasting tasks it lacks a discussion on the limitations and scenarios where MetaLoss might not be the optimal choice. Understanding its applicability and boundaries is essential for broader adoption.  Comparative Analysis with Other MetaLearning Approaches: The paper could benefit from a broader comparison with other metalearning and loss function adaptation approaches to highlight MetaLosss unique advantages and situational preferences. Questions and Feedback: 1. Can the authors provide insights or empirical data on the computational overhead introduced by implementing MetaLoss compared to traditional loss functions 2. How does MetaLoss perform in scenarios with highly noisy or volatile data Is there a risk of overfitting and how could it be mitigated 3. It would be beneficial to include a more comprehensive analysis comparing MetaLoss to other metalearning or adaptive loss function approaches. Such a comparison could emphasize MetaLosss unique strengths and potential use cases.  3. Review Summary The paper presents an innovative and practically relevant approach to improving forecasting accuracy in regression tasks through metalearning of loss functions. The proposed MetaLoss model demonstrates significant performance gains over traditional loss functions across various realworld datasets underscoring its potential and applicability in essential forecasting domains. Nonetheless the paper could be strengthened by addressing its computational complexity broadening the discussion on its limitations and generalizability and providing a more comprehensive comparative analysis with related metalearning approaches. Overall the study makes a valuable contribution to the field of machine learning for regression tasks opening new avenues for research and application in metalearning of loss functions. My recommendation is acceptance with minor revisions provided the authors address the aforementioned feedback points to enhance the papers contribution further.", "lsQCDXjOl3k": "PeerReview 1) Summary of the paper: The paper introduces a novel concept named unconditional guidance to improve the sample quality of diffusion models without employing an extra trained classifier. This is an extension of an existing technique called classifier guidance which enhances the sample fidelity of a diffusion model using the gradient of an auxiliary classifier model. The paper argues that similar or better results can be achieved by combining the score estimates from jointly trained conditional and unconditional diffusion models thus bypassing the need for a separate classifier. The authors successfully demonstrate that their method unconditional guidance can trade off between FID (Fr\u00e9chet Inception Distance) and IS (Inception Score) similar to classifier guidance thereby illustrating the capability of pure generative models to synthesize highfidelity samples without opposing image classifiers. 2) Strong and weak points Question and Feedback to the author: Strong points:  The paper addresses a significant question in the field of diffusion models i.e. whether highquality sample generation can achieve without additional classifiers. This question is both relevant and timely given the increasing interest in diffusion models as a generative tool.  The methodology for unconditional guidance is well thought out and clearly articulated making a compelling case for its effectiveness. The use of jointly trained conditional and unconditional models is a clever approach that is likely to inspire future research in this area.  The experiments are welldesigned and convincingly demonstrate the efficacy of unconditional guidance especially the detailed exploration of the tradeoff between FID and IS across different guidance strengths and model configurations.  The reproducibility and ethics statements are commendable additions that show the authors commitment to open science and responsible AI development. Weak points:  While the paper does an excellent job at demonstrating the potential of unconditional guidance it could benefit from a deeper theoretical analysis on why removing the classifier does not detrimentally affect the performance. Some theoretical insights or a more detailed discussion on the phenomenon observed could enrich the paper.  The comparison with classifier guidance primarily focuses on achievable tradeoffs between FID and IS. It would be beneficial to also compare aspects such as computational efficiency adaptability to different data modalities and potential limitations of each approach.  The ethics statement mentions the possibility of biases but does not delve into specific examples or methods to quantify or mitigate such biases. Given the broad applicability of generative models a more detailed discussion on potential ethical implications would be beneficial. Question:  How does the performance of unconditional guidance scale with an increase in the complexity of data (e.g. higher resolution images more complex datasets beyond ImageNet) Feedback:  While the experimental results are promising expanding the discussion on theoretical foundations could make the paper more comprehensive. Potential directions include analyzing the differences in the learned distributions with and without classifier gradients.  Exploring the computational efficiency and scalability of unconditional guidance in more depth could make the case stronger especially for practical applications where resources are limited.  A more nuanced discussion on ethical considerations possibly supported by experiments or case studies would offer valuable insights into the responsible use of unconditional guidance in various domains. 3) Review Summary: The paper presents a significant advancement in the field of generative models by introducing unconditional guidance a method that enhances the quality of samples generated by diffusion models without the need for an auxiliary classifier. This addresses a key question in generative modeling and opens new avenues for research. The methodology is robust and the experimental results are convincing. However the paper could be strengthened by a deeper theoretical exploration a broader comparison with existing methods and a detailed discussion on the ethical implications of generative models. Overall the contribution is substantial and with some improvements the paper has the potential to influence future research in diffusion models and generative AI.", "r8S93OsHWEf": "Summarize the paper The paper proposes an innovative approach to enhance the generalization and robust accuracy of adversarially trained networks through selfsupervised testtime finetuning. It presents a meta adversarial training strategy inspired by the MAML framework aimed at finding an optimal starting point for testtime finetuning. This method embeds the finetuning process into the training phase strengthening the link between selfsupervised and classification tasks. The approach was tested on CIFAR10 STL10 and Tiny ImageNet datasets with different selfsupervised tasks showing consistent improvements in robust accuracy against various adversarial attacks including both whitebox and blackbox scenarios. Contributions include the selfsupervised testtime finetuning framework the meta adversarial training strategy and designing an adaptive attack strategy aware of the finetuning process. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The combination of selfsupervised learning with adversarial training is unique and tackles the critical issue of poor generalization in adversarially trained models. 2. Thorough Evaluation: The paper presents extensive experimental results across multiple datasets (CIFAR10 STL10 and Tiny ImageNet) and against various attack strategies providing a strong validation of the proposed methods effectiveness. 3. Practical Contributions: By introducing an adaptive attack aware of the testtime finetuning the paper not only contributes a novel defense mechanism but also advances research in formulating more realistic attack scenarios. Weak Points: 1. Dependency on Selfsupervised Tasks: The effectiveness of the method appears to rely heavily on the choice of selfsupervised tasks yet the paper does not offer a clear guideline on how to select or design these tasks for optimal performance. 2. Computational Cost: While the approach shows promising results the need for finetuning at test time and the bilevel optimization problem may imply a significant increase in computational cost which is not thoroughly discussed. 3. Generalizability Across Different Scenarios: The paper provides experimental results on image datasets however it lacks discussion on its applicability or necessary adjustments in other domains or more complex adversarial settings. Questions and Feedback: 1. Selection of Selfsupervised Tasks: Could the authors elaborate on how different selfsupervised tasks can be selected or tailored for various datasets or applications to maximize the effectiveness of the proposed method 2. Computational Efficiency: How do the additional computational costs of meta adversarial training and testtime finetuning compare to standard adversarial training and are there strategies to mitigate these costs 3. Extension to Other Domains: Can the proposed method be adapted for tasks beyond image classification such as natural language processing or speech recognition If so what modifications would be necessary Review Summary This paper introduces a promising method to address the challenge of poor generalization in adversarially trained networks through selfsupervised testtime finetuning and a novel meta adversarial training strategy. The comprehensive experimental evaluation across various datasets and against multiple attack strategies effectively demonstrates the approachs potential to significantly enhance robust accuracy. Despite its strengths the paper could be improved by addressing the dependency on the selection of selfsupervised tasks the computational costs involved and the generalizability of the method across different domains or more complex adversarial settings. Overall the novel contributions and the positive experimental results support the papers claims making it a valuable addition to the field of adversarial robustness. Further research should explore task selection guidelines efficiency improvements and adaptations to other domains to fully realize the proposed methods benefits.", "i7h4M45tU8": " Review of \"Neural Temporal Logic Programming for Composite Event Recognition in Temporal Data\"  Summary The paper introduces a novel approach to understanding complex temporal sequences by breaking down composite events into their constitutive atomic events and the logical temporal relations between them. It presents Neural Temporal Logic Programming (Neural TLP) a framework designed to learn implicit relations between atomic events and generate logic rules for composite events using only composite event labels for supervision. This method hinges on two main stages: parameter learning where the model infers temporal predicates between atomic events and structure learning where it selects the right combination of predicates to form rules that explain the observed composite events. The paper evaluates Neural TLP on synthetic video data and healthcare datasets highlighting its superiority in rule discovery over baseline methods.  Strengths 1. Innovative Approach: The introduction of Neural TLP is noteworthy for its novel method of extracting explicit temporal logic rules from raw temporal data without requiring detailed background knowledge or predefined relations among atomic events. 2. Robustness to Noisy Data: The framework exhibits a remarkable capability to handle noise in temporal data making it particularly suitable for realworld applications where data inconsistencies are common. 3. Improved Interpretability and Generalization: By focusing on the underlying structure of atomic events and their relations Neural TLP enhances the interpretability of model predictions and demonstrates better generalization to outofdistribution data compared to conventional models that directly predict composite events. 4. Comprehensive Evaluation: The experimental design including the application to both synthetic and realworld datasets provides a robust evaluation of Neural TLPs performance and utility.  Weaknesses 1. Computational Complexity: The introduction of a combinatorial search space for rule induction might lead to scalability issues especially as the number of atomic events increases. The paper could provide a more detailed discussion on the computational demands and potential optimizations. 2. Lack of Detailed Comparison: While baseline methods are discussed a more thorough comparative analysis against stateoftheart methods in temporal rule discovery beyond LSTM and Temporal MAP would strengthen the papers contributions. 3. RealWorld Applicability: The paper presents exciting results on synthetic video and clinical datasets however further validation in diverse realworld settings across different domains would help in understanding the frameworks broader applicability. 4. Model Interpretability: Although the method is designed to improve interpretability by learning explicit rules a deeper exploration into how these rules can be interpreted and utilized by practitioners in respective fields (e.g. healthcare professionals) is warranted.  Questions and Feedback to the Author 1. Can the authors elaborate on the computational requirements of Neural TLP especially concerning the combinatorial aspect of rule learning 2. How does Neural TLP compare to other stateoftheart methods in temporal logic programming or event prediction that were not mentioned in the paper 3. Are there specific limitations in the frameworks applicability across various domains (e.g. limitations in handling highly irregular time series or extremely sparse events) 4. It would be beneficial if the paper could discuss practical implications of the discovered rules particularly in the healthcare scenario to understand how these could potentially influence decisionmaking or diagnostics.  Review Summary The paper presents a significant advancement in extracting composite event rules from temporal data addressing a crucial gap in understanding complex time series through a logic programming lens. Neural TLP is a compelling framework that opens new avenues for research in temporal data analysis particularly through its emphasis on rule discovery and interpretability. Nonetheless discussions on computational efficiency broader model comparisons extended realworld validation and the practical utility of the induced rules could enhance the papers impact and applicability. Given the strengths of the proposed approach and the quality of experimental validation provided the paper is a valuable contribution to the field albeit with room for further exploration and expansion in future work.", "xQUe1pOKPam": " Peer Review of \"Graph MultiView Pretraining (GraphMVP) for Enhancing 2D Molecular Graph Representation with 3D Geometry\"  Summary The paper introduces GraphMVP a novel framework for molecular graph representation learning that addresses the challenge of incorporating 3D geometric information by leveraging consistency between 2D topological structures and 3D geometric views. The method is poised to improve drug and material discovery processes by providing a more discriminative model for molecular functionalities prediction. GraphMVP operates under a selfsupervised learning paradigm incorporating both contrastive and generative SSL tasks to infuse 3D geometric knowledge into a 2D molecular graph encoder enhancing its representation capabilities for downstream tasks absent of 3D information. The authors provide comprehensive theoretical insights and empirical evidence showcasing GraphMVPs superiority over existing SSL methods in molecular graph representation.  Strong Points 1. Novelty and Relevance:  The proposal of integrating 3D geometric information into the learning process of 2D molecular graph representation via selfsupervised learning is innovative and timely addressing a significant gap in the graph representation learning domain especially relevant to drug and material discovery. 2. Theoretical Justification and Insights:  The paper provides a strong theoretical foundation for the proposed GraphMVP including insights into how it maximizes mutual information between 2D and 3D views and the rationale behind using 3D geometry as privileged information. 3. Comprehensive Experimental Validation:  The authors present extensive experiments across various datasets showing consistent performance improvements over established SSL methods. This rigorous empirical evaluation significantly strengthens the papers contributions. 4. Open Source Contribution:  Making the code available on GitHub is commendable promoting transparency reproducibility and further research based on GraphMVP.  Weak Points 1. Broader Applicability and Generalization:  While the paper demonstrates remarkable results in molecular graph representation it is less clear how generalizable GraphMVP is across different domains or graph types outside molecular structures. It would be beneficial to explore or discuss its broader applicability. 2. Integration with Other Graph Learning Paradigms:  The paper focuses mainly on SSL without much discussion on how GraphMVP could integrate or benefit from supervised or semisupervised learning paradigms particularly in scenarios where limited labeled data might be available alongside large unlabeled datasets. 3. Computational Efficiency and Scalability:  The discussion lacks depth regarding the computational implications of implementing GraphMVP especially concerning the handle of 3D structures and the overall scalability of the framework to larger more complex molecular databases.  Questions and Feedback 1. Generalization and Applicability:  Can the authors elaborate on potential extensions of GraphMVP to other domains or graph types How would the performance compare in less structured environments or with graphs of significantly different characteristics 2. Integration with Other Learning Paradigms:  Could the authors discuss possible synergies between GraphMVPs SSL approach and other learning paradigms especially in contexts where some level of labeled data might augment the pretraining phase 3. Computational Considerations:  Given the significant role of 3D structures what are the computational demands of GraphMVP and how scalable is it across different hardware setups or larger datasets Are there any optimizations or strategies employed to mitigate potential computational challenges  Review Summary The paper \"Graph MultiView Pretraining (GraphMVP) for Enhancing 2D Molecular Graph Representation with 3D Geometry\" presents a significant advancement in molecular graph representation learning. Its innovative approach to integrating 3D geometric information through selfsupervised learning represents a notable contribution to the field especially relevant for drug and material discovery applications. The comprehensive theoretical backing and extensive experimental validations strongly support GraphMVPs effectiveness over existing methods. However discussions on the frameworks generalizability to other domains integration with other learning paradigms and computational efficiency could be further expanded to bolster the papers impact. Overall the novel insights combined with opensource code availability make a compelling case for the adoption and further investigation of GraphMVP in enhancing molecular graph representations.", "w8HXzn2FyKm": " Peer Review  1. Summary of the Paper The paper addresses a significant gap in existing research on multiagent reinforcement learning (RL) and distributed linear stochastic approximation algorithms. It introduces a novel approach to enable consensus among agents in a distributed RL setting even when communication is bidirectional and characterized by a stochastic matrix instead of a doubly stochastic one. This is particularly crucial for realworld applications where bidirectional communication cannot always be assured due to various constraints. By extending the applicability of distributed RL algorithms the paper contributes to advancing the effectiveness of such algorithms under more realistic and challenging conditions. The authors propose two main algorithms: a consensusbased distributed linear stochastic approximation algorithm and a pushbased variant to overcome the limitations posed by unidirectional communication. The paper presents a detailed theoretical examination of both algorithms providing proofs of their convergence finitetime bounds on meansquare error and a discussion on how these findings can directly apply to Temporal Difference (TD) learning. The technical innovation lies in the formulation of absolute probability sequences for handling stochastic matrices and their finitetime performance analyses.  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Innovative Approach: The paper introduces novel algorithms that advance the field of distributed RL by addressing practical challenges related to communication restrictions among agents.  Rigorous Analysis: The authors provide a comprehensive theoretical foundation for their proposed algorithms including convergence proofs and finitetime performance bounds.  Applicability to TD Learning: The direct applicability of the findings to TD learning is highly beneficial illustrating the practical relevance of the research. Weak Points:  Complexity of Constants: A significant limitation is the complexity involved in computing the constants used in the finitetime bounds. This may hinder practical implementations or require simplifications that could affect the preciseness of the outcomes.  Assumptions and RealWorld Applicability: While the paper addresses realistic communication constraints it does not thoroughly discuss how varying network dynamics in realworld applications might impact the performance and applicability of the proposed algorithms.  Empirical Validation: The paper lacks empirical validation of the proposed algorithms. Providing simulation results or case studies could significantly strengthen the claims about the algorithms effectiveness and applicability in realworld scenarios. Questions and Feedback:  Can the authors elaborate on how the complexity of computing the mentioned constants can be reduced or provide guidelines on approximating these for practical implementations  How do the proposed algorithms perform under dynamic network conditions such as changing topology or varying communication reliability which are common in realworld applications  The authors are encouraged to include empirical evidence or simulation studies to validate the theoretical findings and demonstrate the algorithms\u2019 realworld applicability and performance.  3. Review Summary The paper makes a valuable contribution to the field of distributed RL by presenting new algorithms that allow for consensus among agents under more restrictive and realistic communication conditions. The theoretical analysis is thorough and provides a solid foundation for understanding the algorithms properties and potential applications notably in TD learning. However the paper falls short in a few areas. The complexity of computing specific constants used in the analyses may limit the practical utility of the findings. More importantly the absence of empirical evidence to support the theoretical claims significantly weakens the papers impact. Realworld applicability is touched upon but not demonstrated through simulations or case studies which are crucial for validating the proposed methods under realistic conditions. In summary while the paper addresses an important gap and proposes theoretically sound algorithms further work particularly in empirical testing and simplification of the computational aspects is necessary to fully ascertain the algorithms effectiveness and feasibility in realworld scenarios.", "qPzR-M6HY8x": "Draft Review: 1) Summary of the Paper This paper tackles the challenge of label noise in multiclass classification specifically addressing the instancedependent noise (IDN) which manifests as the noisy label being dependent on the input instance. Recognizing that realworld datasets often contain aleatory label noise that varies significantly between instances the authors propose the instanceconfidence embedding (ICE) method as a novel approach. ICE utilizes a variational approximation to simplify the modeling of IDN employing a scalar confidence parameter and instance embedding to assign a trainable parameter per instance aiming to efficiently manage label noise and detect ambiguous or mislabeled instances. Through experiments on image and text classification tasks the authors demonstrate the merit of ICE in handling label noise and its potential in identifying problematic training instances. 2) Strong Points:  Novelty: The paper introduces a novel approach to model instancedependent noise in multiclass classification tasks which is significant considering the prevalence of IDN in realworld datasets. The introduction of instanceconfidence embedding (ICE) is a key innovation.  Relevance to RealWorld Challenges: The paper addresses a real and prevalent issue in machine learning models trained on largescale data collections which often include inaccurately labeled instances.  Experimental Verification: The authors conducted comprehensive experiments across various datasets and compared ICE against ten other methods showcasing its effectiveness in improving classification performance under label noise and detecting ambiguous or mislabeled instances. Weak Points:  Lack of Theoretical Analysis: The paper primarily focuses on empirical results and lacks theoretical analysis or guarantees regarding the performance of ICE compared to other methods especially in different noise regimes.  Scalability and Computational Cost: While the paper briefly mentions the computational cost theres insufficient analysis of the scalability of ICE especially for larger datasets given that it assigns a trainable parameter to each instance.  Generalizability to Unseen Instances: The methods reliance on instance embeddings could potentially limit its generalizability to unseen instances a point that is addressed but not substantially explored. Questions and Feedback:  Could the authors provide any theoretical insights or guarantees that support the variational approximation used in ICE especially concerning its ability to approximate IDN accurately  Considering the ICE method involves assigning a trainable parameter to each instance how does the model scale with extremely large datasets Is there a practical limit to the number of instances it can handle effectively  How does ICE perform on unseen instances especially given that the instance embedding does not generalize to new examples Can the authors comment on potential strategies to enhance generalizability 3) Review Summary: The paper presents a novel and practical approach to addressing the challenge of instancedependent noise in multiclass classification tasks. The introduction of instanceconfidence embedding (ICE) is both innovative and timely given the widespread issue of label noise in datasets derived from imperfect sources. The experimental results are compelling demonstrating ICEs ability to improve classification performance in the presence of label noise and to detect ambiguous or mislabeled instances effectively. However the paper would benefit from a deeper theoretical analysis to complement the empirical findings offering insights into why and under what conditions ICE outperforms other methods. Furthermore concerns regarding scalability and generalizability particularly to unseen instances warrant further attention. Considering the strengths and potential areas for improvement this paper makes a significant contribution to the field. It opens new avenues for research in handling label noise and provides a solid foundation for further exploration of instancespecific noise modeling techniques.", "fM8VzFD_2-": "Review of \"Mining Nosological Relations Among Neuropsychiatric Disorders Through Dual Utilization of Diagnostic Information in Conditional Variational AutoEncoders\" 1) Summary of the Paper This paper introduces a novel approach to understanding the complex nosological relationships among neuropsychiatric disorders by leveraging restingstate functional connectivity (FC) features and diagnostic information. The authors propose a novel type of conditional variational autoencoder (VAE) that utilizes diagnostic information in two distinct manners: (1) to regularize the learning of lowdimensional embeddings and ensure clustering around empirical cluster centers and (2) to encode diagnostic differences among disorders through contrastive learning. This dual utilization of diagnostic information aims to derive a more informed and optimal lowdimensional embedding space that preserves the diagnostic attributes of the represented disorders. The methods effectiveness is demonstrated through simulation studies and implementation on two empirical neuropsychiatric neuroimaging datasets revealing consistent nosological relations among autism spectrum disorder (ASD) major depressive disorder (MDD) and schizophrenia (SCZ). 2) Strong Points Weak Points Questions and Feedback Strong Points:  The paper addresses a significant challenge in computational psychiatry by proposing a method to uncover the complex continuous relation among diverse neuropsychiatric disorders which could have implications for diagnosis and treatment.  The dual utilization of diagnostic information in the conditional VAE framework is novel and wellmotivated potentially offering a more robust and meaningful way to capture the essence of neuropsychiatric disorders in lowdimensional embeddings.  Empirical validation on two realworld datasets and the findings related to the nosological relation among ASD MDD and SCZ contribute valuable insights to the field. Weak Points:  The paper would benefit from a more detailed discussion on the interpretability of the derived lowdimensional embeddings especially in the context of clinical utility and neuroscientific insights.  It is unclear how the proposed method compares directly to existing dimensionality reduction techniques or other methods specifically designed for similar nosological investigations. A comparative analysis could strengthen the claims of superiority.  The scalability of the method to larger more heterogeneous datasets and its performance in such scenarios have not been addressed. Questions and Feedback: 1. Could the authors elaborate on the potential clinical implications of their findings considering the current limitations in interpretability and the early stage of their approach 2. How might the proposed method be adapted or extended to incorporate additional types of diagnostic information such as genetic or cognitive data 3. The authors mention limitations regarding the discernment of novel subtypes or nosological overlaps between disorders. Could they speculate on future directions to overcome these limitations 3) Review Summary and Recommendation This paper makes a significant contribution to computational psychiatry by introducing a novel method for uncovering nosological relations among neuropsychiatric disorders using functional connectivity features and diagnostic information. The dual utilization of diagnostic information in a conditional VAE framework is an innovative approach that holds promise for deriving more meaningful lowdimensional embeddings of mental disorders. The empirical validation of the proposed method on realworld datasets adds value to the findings presenting a reliable nosological relation among ASD MDD and SCZ. However the study could be strengthened by addressing the comparative performance of their approach against existing methods expanding on the clinical relevance and interpretability of the findings and discussing the scalability of the method. Addressing these points would enhance the robustness and utility of the research. Given the novelty of the approach and the promising preliminary findings the paper is a valuable addition to the literature. However I recommend a minor revision to address the points mentioned above. With additional clarification and expansion on these aspects the paper has the potential to significantly impact our understanding of the nosological relations among neuropsychiatric disorders and inform future diagnostic and treatment approaches.", "qwULHx9zld": " Review of \"Spectral Behavior of Ternary Random Features and Kernel Matrices in HighDimensional Settings\"  1. Summary This article explores the spectral characteristics of random features kernel matrices generated from largedimensional data and introduces a new computationally efficient method for computing these kernels named Ternary Random Features (TRF). The focus is on kernel matrices K in forms of expected values of products of nonlinear transformations (\u03c3) of data points (xi xj) projected through random vectors w. The study is grounded in the context of Gaussian Mixture Models for the data proposing that in highdimensional spaces the crucial aspects of the kernel matrixs spectrum are largely independent of the randomness in w but are influenced by the statistical properties encapsulated in the generalized Gaussian moments of \u03c3. The TRF method minimizes computational and storage requirements by utilizing ternary values for projections and nonlinear transformations fundamentally altering the typical computational pathways (eliminating multiplication significantly reducing memory footprint). Empirical validations demonstrate that TRFs retain and somewhat enhance the efficacy of conventional random feature methods while offering substantial computational and storage efficiencies.  2. Evaluation Strong Points:  Innovative Approach: The paper introduces a novel technique (TRF) that notably reduces the computational complexity and memory requirements inherent to classical kernel approximation methods. This innovation could significantly expand the applicability of kernel methods in largescale and resourceconstrained environments.  Rigorous Theoretical Foundation: The paper thoroughly develops a theoretical framework that elegantly ties together random matrix theory kernel methods and highdimensional data analysis. The proofs particularly concerning the spectral equivalence of TRF and traditional random features are both robust and enlightening.  Practical Relevance: By addressing both the theoretical and practical aspects of scalability and efficiency in kernel methods the work aligns well with critical needs in machine learning applications. The experimental validation offers persuasive evidence of TRFs benefits enhancing the papers relevance to practitioners. Weak Points and Questions:  Generalization Beyond GMM Assumptions: The model assumes Gaussian Mixture Model (GMM) data which while broad does not encompass all types of distributional data characteristics encountered in practice. How might the findings extend to nonGMM including distributions lacking a welldefined mathematical structure or realworld data exhibiting heavier tails than Gaussian distributions  Impact of Sparsity Parameter (): The introduction of the sparsity parameter  influencing the ternary representations is intriguing but raises questions about optimal tuning and its impact on performance across different datasets and tasks. Could the authors discuss strategies for selecting and its effects more thoroughly  Comparison Against StateoftheArt: The empirical comparisons convincingly demonstrate TRFs efficiency and efficacy. However further clarity on how TRFs perform relative to the latest advancements in this domain including deep kernel methods and advanced quantization or pruning techniques for neural networks would be valuable for readers.  3. Review Summary The paper builds an important bridge between theoretical insights into the spectral properties of kernel matrices in high dimensions and practical solutions to computational challenges in kernel methods. The TRF approach represents a significant step forward offering potentially transformative efficiency gains. While the work is grounded in solid mathematical theory and supported by compelling empirical evidence its applicability to a broader range of data distributions and comparative analysis against the latest methods in this rapidly evolving field could be further elaborated. The contribution to understanding the structure of random features kernel matrices in highdimensional settings is noteworthy. However expanding the scope to address the weak points identified\u2014particularly concerning the methods performance beyond GMM assumptions and an indepth discussion on the sparsity parameter \u2014could bolster the papers value and impact. Nonetheless the innovative TRF method combined with rigorous theoretical underpinnings positions this work as a significant contribution to the machine learning literature particularly in the subdomains of kernel methods and highdimensional data analysis.", "ldkunzUzRWj": "Summarize the Paper The paper addresses the class imbalance problem prevalent in pairwise ranking models used in recommendations by proposing a novel sampling method dubbed Vital Negative Sampler (VINS). The authors argue that existing dynamic and stationary sampling methods fail to address vertexlevel imbalance leading to suboptimal embedding norms and affecting model performance due to vanishing gradients. VINS introduces a bias sampler with a reject probability that favors negative items with higher popularity relative to a given positive item thus efficiently mitigating both edge and vertexlevel class imbalances. Theoretical analysis and extensive experiments across several datasets and model architectures demonstrate VINSs efficacy in speeding up the training process by 30 to 50 while maintaining or even improving ranking quality. Strong and Weak Points of the Paper Strong Points: 1. Novel Approach: The introduction of VINS and the bias sampling strategy addresses the overlooked vertexlevel class imbalance issue providing a fresh perspective on improving pairwise ranking models. 2. Theoretical Analysis: The paper offers a thorough theoretical examination of the classimbalance problem supporting its argumentation and the design of VINS with solid analytical foundations. 3. Empirical Validation: Extensive experiments not only validate the efficiency and effectiveness of VINS compared to stateoftheart methods but also demonstrate its applicability to diverse models and datasets showcasing its robustness. 4. Practical Impact: By significantly reducing training times without compromising or even enhancing model performance the proposed method has a high potential for positive realworld impact especially for applications requiring frequent retraining or updating. Weak Points: 1. Comparison with Baselines: While the paper compares VINS with several existing methods it lacks a detailed discussion on how VINS performs relative to the very latest sampling techniques which could further assert its current stateoftheart status. 2. Scalability Discussion: There is limited discussion on the scalability of VINS when applied to extremely large datasets or in online (realtime) recommendation scenarios which would be valuable for practitioners in the field. Questions and Feedback for the Author: 1. Could the authors provide further insights into how VINS performs compared to other very recent sampling strategies not covered in the paper 2. The paper would benefit from a more detailed exploration of the impact of VINS on model interpretability especially considering its bias sampling strategy. 3. How does VINS handle extremely large datasets or work in online recommendation systems where realtime useritem interactions could potentially affect its performance Review Summary The paper introduces VINS a novel sampling method aimed at addressing the class imbalance issue in pairwise ranking models through bias sampling with reject probability. This approach strategically mitigates vertexlevel imbalance a largely unaddressed problem thereby enhancing the training efficiency and performance of recommendation models. The strength of this paper lies in its comprehensive theoretical analysis broad empirical validation and significant practical implications. However the paper could be improved by including comparisons with more recent sampling methods discussing its scalability and exploring the impact on model interpretability. Overall the proposed method represents a significant contribution to the field of recommendation systems offering a pragmatic solution to a wellrecognized problem and opening avenues for further research. I recommend publication provided the authors address the mentioned concerns particularly around scalability and comparison with recent advancements.", "q2DCMRTvdZ-": " Peer Review of the Paper on Differentiable Supernet NAS Evaluation  1) Summary This paper introduces an innovative approach toward evaluating differentiable Neural Architecture Search (NAS) algorithms specifically focusing on supernetbased methods. Traditional NAS algorithms have been challenged by the high computational costs and the complexity of comparative evaluations. To address this the paper presents a twostage search framework that decouples the training of the supernet from the architecture selection process aiding in the independent evaluation of each stage using newly proposed metrics. These metrics aim to assess the quality of the supernet\u2019s shared weights and the effectiveness of the learned architecture sampling distribution. By applying this evaluation framework the paper highlights the potential for designing more efficient NAS algorithms by combining different supernet training and architecture selection techniques. The empirical study including a case study on DARTS modifications and evaluations on the NASBench201 dataset demonstrates the utility of these metrics in understanding and improving supernetbased NAS algorithms.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Framework: The proposed twostage search framework is a significant advancement in the evaluation of NAS algorithms. It addresses a critical gap in the literature by allowing for the independent assessment of the training and architecture selection stages.  Comprehensive Metrics: The introduction of specific metrics to evaluate the quality of supernet training and architecture selection independently is noteworthy. These metrics provide a detailed understanding of the algorithms\u2019 performance beyond conventional accuracy measures.  Empirical Validation: The thorough experimental evaluation including a case study on DARTS variants provides solid evidence of the framework\u2019s utility. The use of NASBench201 for benchmarking further enhances the credibility of the results. Weak Points:  Generalizability Concerns: While the paper presents results on NASBench201 the generalizability of the proposed framework and metrics to other datasets and search spaces remains unclear.  Limited Scope of Stage2 Evaluation: The evaluation of Stage2 search algorithms primarily focuses on selection methods based on the supernet\u2019s output. Expanding this to include other selection techniques could provide a more rounded view.  Complexity for Practitioners: The division into two stages and the introduction of multiple metrics may increase the complexity for practitioners looking to apply these methods in realworld scenarios. Questions and Feedback: 1. Generalizability: How do the authors anticipate their framework and metrics will perform when applied to larger more complex datasets and search spaces beyond NASBench201 2. Impact of Hyperparameters: The training of supernets and the architecture selection process can be highly sensitive to hyperparameter settings. How do the proposed metrics account for or mitigate the impact of hyperparameter variability 3. Comparison with Existing Methods: Could the authors provide more insight into how their metrics and evaluation framework compare with existing NAS evaluation methodologies particularly in terms of predictive validity for realworld performance  3) Review Summary and Recommendation The paper represents a noteworthy advancement in the field of Neural Architecture Search particularly in the evaluation of NAS algorithms. The introduction of a twostage search framework accompanied by specific evaluation metrics addresses a crucial need for comprehensive and independent assessment of NAS algorithms\u2019 components. The empirical validation solidifies the frameworks utility offering valuable insights into improving NAS strategies. However there are areas for improvement particularly concerning the generalizability of the results the scope of Stage2 evaluation and the practical applicability of the proposed framework. Addressing these concerns either through additional empirical studies or a more detailed discussion on implementation considerations could significantly enhance the papers impact. Taking into account both the strengths and potential areas for improvement I recommend this paper for acceptance given that the authors consider incorporating feedback on expanding the evaluation scope and providing a detailed discussion on generalizability and practical applicability. The paper\u2019s contribution to the field backed by solid experimental evidence outweighs the identified limitations making it a valuable addition to the NAS literature.", "fVu3o-YUGQK": " Peer Review  1) Summary of the Paper This paper introduces an efficient selfsupervised vision transformer model termed EsViT focusing on visual representation learning. The authors address the efficiency and effectiveness of selfsupervised learning (SSL) for computer vision (CV) by proposing a combination of multistage architecture and a novel pretraining task called region matching. Traditional SSL approaches in CV primarily involve CNNs and focus on viewlevel learning. However the transformation of knowledge from NLPs success with Transformers to CV has been challenging due to the computational demand and the complexity of modeling longrange dependencies in images. EsViT utilizes a multistage architecture with sparse selfattentions to reduce computational complexity significantly complemented by a pretraining task that enhances the models ability to learn finegrained region dependencies. This approach addresses the loss of finegrained correspondence in multistage architectures and boosts the quality of learned vision representations. The paper demonstrates EsViTs superior performance on the ImageNet linear probe evaluation achieving 81.3 top1 accuracy and outperforming stateoftheart methods in both efficiency and effectiveness across several downstream tasks.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper successfully addresses a critical challenge in using Transformer models for SSL in CV by reducing computational complexity without compromising the learning of finegrained image features.  The introduction of the region matching pretraining task is innovative and effectively improves the models ability to capture detailed region dependencies which is a significant advancement over existing SSL methods.  The comprehensive empirical studies provide strong evidence of the proposed methods effectiveness and the extensive comparison with stateoftheart models showcases EsViTs superior performance and efficiency. Weak Points:  The transition from multistage architectures to the proposed solutions specific implementation details is not deeply explored. It would benefit from a more detailed discussion on the architectural design choices that led to improved results.  The paper focuses primarily on image classification tasks. While it briefly mentions the models transferability to other domains like detection and segmentation detailed analysis or results in these areas are limited.  The longterm impact of EsViT on tasks outside standard benchmarks particularly in more realworld applications or underresourced settings remains unclear. Further exploration in these areas would strengthen the papers contributions. Questions and Feedback:  How does EsViT handle highly variable scene complexities found in natural images and what implications does this have for its applicability to diverse visual tasks  The paper could benefit from a discussion on the potential limitations or challenges in scaling EsViT for larger more complex datasets than those tested. Feedback on this aspect could provide valuable insights for future work.  Given the impressive performance of EsViT it would be insightful to see a more detailed analysis of the models interpretability specifically how the region matching mechanism contributes to the learning of meaningful visual representations.  3) Review Summary EsViT represents a significant advance in the field of SSL for CV by combining a multistage Transformer architecture with a novel region matching pretraining task. This approach not only reduces computational demands but also enhances the models ability to learn finegrained features and dependencies within images as evidenced by its outstanding performance across several benchmarks. However the paper could be strengthened by addressing the weak points mentioned particularly by expanding the discussion on the architectural choices and their implications exploring the models performance on tasks beyond image classification and considering its applicability in more varied realworld settings. Additionally addressing questions related to scene complexity scalability and interpretability would provide a more comprehensive understanding of EsViTs capabilities and limitations. Overall EsViT is a promising step towards more efficient and effective SSL in computer vision and with further exploration and refinement it has the potential to significantly impact the field.", "swiyAeGzFhQ": " 1. Summarize the paper This paper introduces the ArchitectBuilder Problem (ABP) a novel interactive learning setting where two artificial agents an architect and a builder learn to solve tasks collaboratively without a shared communication protocol. The architect knows the goal but can only communicate through arbitrary messages while the builder unaware of the goal performs actions based on the architects instructions. The paper presents ArchitectBuilder Iterated Guiding (ABIG) as a solution leveraging selfimitation learning for the builder and modelbased guidance from the architect. The agents evolve a communication protocol through iterated interaction frames enabling task solutions and protocol generalization to unseen tasks. The efficacy of ABIG is demonstrated in a 2D construction task environment highlighting its potential for learning complex cooperative behaviors without predefined language or direct reinforcement signals.  2. Strong and Weak Points Questions and Feedback Strong Points:  The ABP framework is innovative tackling the challenge of learning cooperative behaviors without a shared language direct imitation or explicit reinforcement which is a significant advancement in multiagent and humanrobot interaction research.  Utilizing selfimitation learning and modelbased guidance for evolving a communication protocol is a novel approach that shows promising results in enabling effective cooperation between agents.  The paper presents a thorough evaluation of the proposed method including ablation studies which help understand the importance of shared intent and interaction frames in the learning process. Weak Points:  The specific setup and tasks used to evaluate the ABIG approach while effective in demonstrating its potential might limit understanding of its applicability to more complex or realworld scenarios.  The dependence on several iterations of interaction frames for successful learning raises questions about the data efficiency of ABIG which could be a limitation for scaling to more computationally intensive environments or realtime applications. Questions:  How would ABIG adapt to scenarios where the task complexity significantly surpasses the 2D construction tasks used in the evaluation Would the guiding principles need adjustments  Could the ABIG model be adapted or extended to facilitate more complex forms of communication such as compositional messages or hierarchical instructions Feedback:  Exploring and discussing the potential scalability of ABIG to more complex environments or tasks would add value to the paper. This could involve theoretical analyses or additional experiments in environments with increased complexity.  Investigating ways to increase the data efficiency of the learning process could make the approach more practical for a wider range of applications. Suggestions for future work or ongoing research in this direction would be beneficial.  3. Review Summary This paper presents a significant contribution to the field of machine learning and agent interaction by introducing a novel learning setting (ABP) and an effective solution (ABIG) enabling collaborative task solving without predefined communication protocols. The innovative approach of using selfimitation and modelbased guidance to evolve a shared language between interactive agents is both novel and promising demonstrated through compelling experiments. However the researchs focus on a somewhat simplified experimental setup may leave questions about ABIGs applicability to more complex realworld scenarios. Further exploration into the scalability of the proposed method and efforts to enhance its data efficiency will be crucial areas for future development. Overall the strengths of the paper including its original framework and the successful demonstration of emergent communication between agents outweigh its limitations. With additional research and refinement the ABIG approach has the potential to advance the capabilities of autonomous agents significantly facilitating more natural and dynamic humanmachine interactions and cooperation among machines.", "i3abvoMoeCZ": " 1. Summary of the Paper This paper provides a comprehensive approach to OutofDistribution (OOD) detection by focusing on distribution shifts specifically covariate shift and concept shift. The authors introduce new score functions derived from KLdivergence capturing these shifts to enhance OOD detection. They propose a geometric method Geometric ODIN to improve the models sensitivity to both shifts significantly. Moreover this method allows for stateoftheart calibration performance on indistribution and OOD data. This research also introduces CIFAR100 Splits a benchmark for assessing gradual concept shift further demonstrating the methods effectiveness across various distribution shifts.  2. Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Approach: The paper addresses the OOD detection problem from a novel perspective by focusing on distribution shifts. This allows for a more granular understanding and categorization of OOD instances.  Novel Score Functions: The derivation of new score functions from KLdivergence provides a wellfounded method to detect and calibrate OOD samples effectively.  Geometric ODIN: This innovative method not only improves model sensitivity to both covariate and concept shifts but also achieves stateoftheart performance in calibration and OOD detection without the need for hyperparameter tuning or extended training.  CIFAR100 Splits Benchmark: The introduction of this benchmark is valuable for the community allowing for a more nuanced evaluation of OOD detection methods against gradual concept shifts. Weak Points:  Reproducibility and Generalizability: The paper could benefit from a more detailed discussion on the reproducibility of their methodology across different datasets and models. It is unclear how generalized Geometric ODIN and the new score functions are beyond the experiments presented.  Analytical Insights vs. Empirical Observations: While the paper provides theoretical underpinnings for the score functions and Geometric ODIN it is somewhat lacking in empirical validation of these theoretical claims across diverse scenarios.  Comparison with StateoftheArt (SOTA): A deeper comparative analysis with existing SOTA models could strengthen the argument for the novel contributions effectiveness. This includes discussing the limitations of the proposed methods in comparison to others. Questions and Feedback to the Author: 1. Reproducibility and Generalizability: Could the authors elaborate on the generalizability of their methods across different datasets and neural network architectures Are there specific conditions under which Geometric ODIN may not perform as expected 2. Comparison with SOTA: Can the authors provide more detailed comparisons with current SOTA methods especially in scenarios where their method underperforms This could offer insights into potential areas of improvement. 3. Insights into Failure Modes: What are the potential failure modes of Geometric ODIN and the proposed scoring functions Discussing these could help in understanding the boundaries of these methods applicability.  3. Review Summary The paper introduces a novel methodology for OOD detection by focusing on distribution shifts establishing new score functions and proposing the Geometric ODIN method for enhanced detection and calibration performance. The comprehensive approach grounded in analytical derivation and supported by empirical evidence represents a significant contribution to the field of OOD detection. However further clarification on the methods reproducibility generalizability and comparison with existing SOTA methods could strengthen the paper. Additionally discussing the potential limitations and failure modes of the proposed methods would provide a more balanced view and suggest directions for future research. Overall the novel perspective on categorizing OOD instances combined with the methodological contributions makes this work an important step forward in understanding and improving OOD detection and calibration. Given the wellfounded theoretical basis and promising experimental results this paper is recommended for publication.", "ks_uMcTPyW4": " Peer Review of \"A Novel Approach to Solve the ExplorationExploitation Problem in Partially Observed Reinforcement Learning Tasks with CostEfficient Feature Acquisition\"  1. Summary of the Paper This paper presents a novel framework for tackling sequential decisionmaking problems under partial observability where information acquisition is costly such as in medical diagnostics. Specifically it addresses the challenge of learning effective policies that balance the explorationexploitation tradeoff by selectively acquiring the most relevant features for decisionmaking hence optimizing both task performance and cost efficiency. The authors propose a modelbased reinforcement learning framework integrated with a sequential variational autoencoder (VAE) that learns highquality representations from partially observed states. These representations guide the policy to make rewarding decisions with minimal feature acquisition costs. The efficacy of the framework is validated through experiments in a control domain and a medical simulator demonstrating superior performance and cost efficiency compared to conventional baselines.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper introduces a modelbased reinforcement learning integrated with a novel sequential VAE to solve the explorationexploitation problem efficiently under partial observability. This approach is particularly relevant in domains like healthcare where information acquisition is expensive and risky.  Comprehensive Evaluation: The proposed framework is evaluated on two different domains showcasing its versatility and the improvements it offers over traditional methods in terms of both performance and costefficiency.  Theoretical and Practical Contributions: The paper makes significant contributions by defining the problem in a wellstructured manner proposing a novel framework and empirically validating its benefits pointing towards practical applications. Weak Points:  Complexity in Implementation: The frameworks complexity especially integrating the sequential VAE with reinforcement learning may pose challenges in implementation and tuning for specific applications.  Limited Evaluation Domains: While the chosen domains for evaluation are relevant additional experiments across a broader range of applications could further validate the frameworks generalizability.  Discussion on Scalability: The paper lacks a detailed discussion on the scalability of the proposed method especially in scenarios with extremely highdimensional state spaces or feature sets. Questions and Feedback:  Scalability and Generalization: How does the proposed framework scale with the complexity and dimensionality of the task Are there limitations in terms of the size of the state space or the number of features it can efficiently handle  Comparison with StateoftheArt: How does the proposed method compare with the latest stateoftheart methods especially those not mentioned or very recently proposed in the domain of reinforcement learning under partial observability  Impact of Feature Cost Variation: How sensitive is the framework to variations in feature acquisition costs especially if these costs change dynamically during the task  3. Review Summary and Recommendation The paper presents a significant step forward in addressing the challenging problem of feature acquisition under partial observability in reinforcement learning tasks offering a novel and innovative framework. The integration of modelbased reinforcement learning with a sequential VAE represents a promising approach to efficiently balance the tradeoff between exploration and exploitation particularly in highstakes domains like healthcare. While the work is strong in its methodological innovation and the demonstrated improvements over traditional approaches further elaboration on scalability comparison with the latest methods and sensitivity to cost variations could strengthen the paper. Additionally extending evaluations to a wider array of domains would be beneficial for broader validation and generalizability. Given the papers contributions strengths and the potential impact of the proposed framework I recommend acceptance provided the authors address the aforementioned points to enhance the works clarity and applicability. The proposed framework represents a significant advancement in partially observed reinforcement learning tasks promising improvements in both performance and cost efficiency.", "vqGi8Kp0wM": " Review of the Paper on OneShot Domain Adaptation Using a PreTrained GAN  Summary The paper introduces a novel methodology for oneshot domain adaptation leveraging a single reference image from domain B to adapt a pretrained GAN capable of generating images in domain A. This method significantly improves visual quality and offers enhanced control over the domain adaptation process. The authors employ a pretrained StyleGAN for image generation and a CLIP model for semantic encoding proposing new regularizers and a finetuning process to optimize the pretrained StyleGANs weights. This adjustment allows it to produce images in the desired domain B while minimizing overfitting and preserving withindomain variance. The process involves identifying a domaingap vector finetuning the GAN with specially designed loss functions and applying style mixing for refined visual similarity. Comparative studies demonstrate the superior performance of this method over existing singleshot and fewshot domain adaptation techniques highlighted by visual evaluations and a user study.  Strong Points 1. Novelty and Technical Contribution: The paper presents a unique approach to oneshot domain adaptation which is both innovative and technically sophisticated. The usage of pretrained networks such as StyleGAN and CLIP combined with the development of new regularizers pushes the boundaries of current domain adaptation methods. 2. Quality of Results: The qualitative and quantitative results shared in the paper alongside the visual evaluations through figures demonstrate a noticeable improvement in reducing overfitting and maintaining the identity of images across domains. These outcomes significantly contribute to the domain adaptation field. 3. Practical Applications: The methodology not just offers a theoretical advancement but also provides practical applications in image editing tasks like pose lighting and expression adaptation among others. The reduced computational requirements further enhance its applicability in realworld scenarios.  Weak Points and Questions 1. Limitation on Domain Relatedness: The paper specifies that the method is applicable to related domains (e.g. different styles of faces) but may not extend well to dramatically different domains (e.g. faces to cars). How significant is this limitation in practical applications and is there a way to quantify the \"relatedness\" of domains for effective adaptation 2. Semantic Similarity Concerns: While the method advances in preserving visual identity across domains it is unclear how well semantic similarities beyond visual resemblances are maintained. Can the authors expand on how the proposed method ensures semantic consistency especially in complex images where multiple objects or entities are present 3. Evaluation Metrics: The paper could benefit from a more comprehensive discussion on the evaluation metrics used to ascertain visual quality and domain similarity. What specific metrics apart from user studies could objectively evaluate the effectiveness of the domain adaptation  Feedback to the Author 1. Broader Impact and Ethical Considerations: It would be beneficial if the authors included a section discussing the potential broader impacts both positive and negative of their work. In particular attention to ethical considerations surrounding the generated contents realism and potential misuse should be addressed. 2. Extension to Unrelated Domains: While the paper acknowledges the limitation regarding domain relatedness future work could explore methodologies or modifications to the proposed method that could bridge vastly different domains. Insights or preliminary findings in this direction would be of high interest. 3. Comparative Analysis: The paper compares the proposed method to a few notable works but could deepen the comparative analysis by including a wider range of existing techniques. Additionally a breakdown of the performance gains attributable to each novel component of the proposed methodology would enrich the discussion.  Review Summary The paper presents a significant contribution to the field of domain adaptation particularly in the context of oneshot adaptation using GANs. The innovative approach of optimizing a pretrained StyleGAN with the incorporation of a CLIP model for encoding domain gaps represents a notable advancement. The results demonstrate valuable improvements in visual quality reduction of overfitting and flexibility in domain control. Despite this the limitation to related domains and the need for further objective evaluation metrics are areas that warrant attention. Given the clear technical contributions positive outcomes and potential for wide applicability I recommend the acceptance of this paper. Future iterations would benefit from addressing the discussed weaknesses and exploring the proposed areas for further research.", "rhOiUS8KQM9": " Draft PeerReview  1) Summary This paper proposes a novel decoding algorithm named Beam Adaptive Tree Search (BATS) which serves as an alternative to the conventional beam search and reranking methods. BATS is a deterministic version of the Monte Carlo Tree Search (MCTS) adapted for text generation tasks particularly focusing on improving translation model outputs. The authors argue that current decoding practices like beam search impose detrimental biases on model outputs particularly affecting models ability to predict translation quality accurately. BATS is designed to be compatible with a wider range of models including those not adhering to autoregressivity or conditional independence by enabling the refinement of initial score estimates with progression in the search tree. Key innovations include a deterministic playout heuristic and a modified UCT criterion for node selection fostering a balance between exploration and exploitation. The authors claim BATS outperforms beam search in delivering higher quality translations according to BLEU scores across various models and settings attributing the improvement to BATSs reduced decoding biases compared to beam search.  2) Strong Points Weak Points Questions and Feedback Strong Points:  The paper addresses a significant issue regarding the limitation of current decoding practices by presenting a robust argument against the reliance on beam search due to its inherent biases problematic for model calibration and prediction of translation quality.  BATS is innovative leveraging the strengths of MCTS while overcoming its computational impracticalities for text generation by introducing a deterministic playout and modified selection criteria.  The empirical evaluation is comprehensive testing BATS across different model types and language pairs demonstrating its superiority over beam search in achieving higher translation quality as measured by BLEU scores. Weak Points:  The paper could benefit from a clearer elucidation of how BATS specifically mitigates the decoding biases compared to other methods. While its understood that BATS leads to better translations the mechanical explanation of why this occurs is somewhat vague.  There is a heavy reliance on the assumption that BLEU scores are an adequate measure of translation quality. Given emerging criticisms of BLEU expanding evaluation metrics could provide a more nuanced understanding of BATSs performance.  The adaptability and efficacy of BATS across a broader spectrum of text generation tasks outside of translation such as summarization or content generation are not addressed. Demonstrating its utility beyond translation could significantly enhance the paper\u2019s impact. Questions and Feedback:  Can the authors elaborate on the specific components of BATS that directly address the calibration issues associated with current models and how these differ from the existing methods  Given the deterministic nature of the playout in BATS is there a risk of converging to suboptimal translations especially in highly complex or ambiguous texts where stochastic methods might explore more diverse translation options  How does BATS perform in terms of computational efficiency and scalability especially when compared to beam search given its apparent complexity Some insights into computational overheads and practical deployment considerations would be valuable.  3) Review Summary This paper makes a significant contribution to the field of natural language processing particularly in the domain of machine translation. By introducing BATS the authors provide a compelling alternative to conventional decoding practices addressing critical limitations and biases inherent in beam search methods. The methodological innovations presented including a deterministic approach to MCTS tailored for text generation are laudable and represent a forward step in developing more accurate and reliable translation models. However to enhance the papers clarity and impact the authors are encouraged to provide a more detailed mechanical explanation of how BATS overcomes the identified limitations of existing decoding methods. Expanding the evaluation beyond BLEU scores as the primary metric of translation quality and demonstrating the utility of BATS across a wider range of text generation tasks could also significantly bolster the papers contributions. Overall this paper is wellpositioned to stimulate further research and development in decoding practices for text generation highlighting the need for continued exploration of methods that can harness the full potential of advanced translation models without being hindered by the biases of traditional decoding techniques.", "iim-R8xu0TG": " Summarize the Paper The paper introduces FitVid an innovative architecture for video prediction aimed at addressing the issue of underfitting prevalent in current video models. Unlike existing models that generally require increasing the number of parameters (and consequently computational resources) to improve prediction quality FitVid efficiently utilizes its parameters to fit benchmarks so effectively that it risks overfitting. This overfitting is managed through existing image augmentation techniques allowing FitVid to achieve stateoftheart results across various benchmarks and metrics without the need for complex training regimes. The work is significant because it shifts the focus from underfitting to managing overfitting in video prediction tasks opening new avenues for enhancing model performance through efficient parameter use and simple augmentation strategies.  Strong and Weak Points Questions and Feedback Strong Points: 1. Efficiency in Parameter Use: FitVids innovative architecture achieves a superior fit on current benchmarks using a similar number of parameters as existing models. This efficiency is particularly noteworthy because it marks a departure from the trend of increasing model size to improve performance. 2. Simplicity of Design: The models nonhierarchical convolutional structure void of complex components like attention mechanisms or curriculum learning makes it simpler and potentially more accessible for replication and further research. 3. Mitigation of Overfitting: The application of existing image augmentation techniques to manage overfitting is a practical approach that also contributes to the stateoftheart performance of FitVid across several datasets. Weak Points: 1. Potential for Overfitting: While the paper identifies and addresses overfitting through augmentation strategies the propensity for the model to overfit could pose challenges in scenarios not covered by the standard augmentation techniques or in highly dynamic environments. 2. Generalization to Novel Tasks: The paper hints at limitations in the models ability to generalize to tasks with significant differences from the training data such as the realrobot experiment. This suggests that despite its efficiency and performance there might be underlying challenges in applying FitVid to diverse realworld applications. 3. Dependence on Augmentation for Regularization: The reliance on data augmentation as a primary means for preventing overfitting may limit the models adaptability or performance in contexts where such augmentation techniques are less effective. Questions and Feedback to the Author: 1. Overfitting Management: Could you elaborate on the potential limitations of using image augmentation techniques to manage overfitting in video prediction tasks Are there scenarios where these techniques might not be enough to prevent overfitting 2. Generalization Across Domains: The realrobot experiment showed limitations in generalizing to new domains. What strategies do you envisage could enhance the models generalizability across significantly different tasks or datasets 3. Future Directions: Given the simplicity of FitVids design what other features or components do you consider promising for further improving the models performance and applicability in more challenging scenarios  Review Summary In summary FitVid represents a significant advancement in video prediction models by addressing the prevalent issue of underfitting through efficient use of parameters rather than merely scaling up the model size. The simplicity of its design and the practical approach to managing overfitting via augmentation are commendable strengths that set it apart from existing models. However the tendency towards overfitting and the models limitations in generalization raise questions about its applicability in diverse realworld settings. The reliance on augmentation techniques for overfitting prevention is both a strength and a potential weakness suggesting a need for further research into more intrinsic methods of regularization. Nonetheless the stateoftheart performance achieved by FitVid across various benchmarks demonstrates its potential especially for tasks closely aligned with the training contexts. Future work could explore enhancing model generalizability and reducing dependency on augmentation possibly by incorporating components like attention mechanisms or investigating new avenues for internal model regularization. While the paper effectively demonstrates FitVids superiority in fitting training data and managing overfitting a deeper exploration into its generalization capabilities and strategies to address its limitations would be important for broadening its applicability. The paper sets an encouraging direction for future research in video prediction highlighting the importance of efficient parameter use and simpler model architectures.", "oh4TirnfSem": " Summary The paper introduces Particle Filter Graph Neural Networks (PFGNNs) a novel approach that combines the expressive power of graph isomorphism solver techniques with the efficiency and scalability of GNNs. The authors address the limitation of message passing GNNs  their bounded expressive power by the 1WL test for graph isomorphism. To overcome this PFGNN guides the learning process with Individualization and Refinement (IR) techniques from exact isomorphism solvers but approximates the otherwise exponential growth of possibilities via a probabilistic particle filter methodology. This allows PFGNN to learn more discriminative graph representations with a linear increase in runtime showing significant performance improvements over existing GNN models on benchmarks for isomorphism detection and realworld datasets.  Strong Points 1. Innovative Approach: The combination of exact isomorphism solver techniques with a particle filterbased approximation is novel and addresses a significant bottleneck in GNN expressiveness without incurring prohibitive computational costs. 2. EndtoEnd Differentiability: The PFGNN framework preserves endtoend differentiability making it compatible with gradientbased optimization and easily integrable with existing GNN architectures. 3. Experimental Validation: The exhaustive evaluation on both synthetic benchmarks and realworld datasets convincingly demonstrates the superior expressiveness and practical utility of PFGNN over stateoftheart GNN models. 4. Detailed Theoretical Insights: The paper provides a comprehensive theoretical discussion with proofs (e.g. Theorem 1 and 2) elucidating the underpinnings of the proposed method enhancing the scientific rigor of the work.  Weak Points 1. Complexity and Implementation Details: While the paper states PFGNN only linearly increases runtime it would benefit from a more detailed discussion on computational complexity especially concerning varying graph sizes and structures. 2. Generalization Across Domains: The experimental evaluation although comprehensive could be expanded to include more diverse realworld scenarios to fully ascertain the models generalizability. 3. Hyperparameter Sensitivity: The method involves several hyperparameters (e.g. the number of particles depth T) and the paper could provide deeper insights into their sensitivity and optimization.  Questions and Feedback to the Author 1. Computational Complexity: Can the authors provide more detailed analysis or empirical evidence on the computational complexity of PFGNN when dealing with large graphs or graphs with complex structures 2. DomainGeneralization: How well does PFGNN generalize across different domains especially where graphs exhibit distinct characteristics not covered by the current evaluation 3. Hyperparameter Tuning: Could the authors discuss strategies for tuning the hyperparameters integral to PFGNN and are there any recommendations for adapting these based on specific types of graphs or tasks  Review Summary The paper presents PFGNN a cuttingedge approach that effectively enhances the expressive power of GNNs by incorporating isomorphism solving methods through a novel particle filter technique. This work significantly advances the field by addressing a fundamental limitation of GNNs  their inability to distinguish certain graph structures due to the bounds of the 1WL test. The empirical results highlight PFGNNs superiority in distinguishing nonisomorphic graphs and achieving competitive results on various benchmarks positioning it as a highly promising approach for learning on graphstructured data. Though the paper is strong on many fronts including its innovative methodology and comprehensive evaluation it could be further strengthened by addressing its computational complexity in more detail expanding its validation to more varied datasets for generalization assessment and providing deeper insights into the hyperparameter sensitivity and tuning process. In conclusion this work is a valuable contribution to the field of graph representation learning offering both theoretical and practical advancements. As such I recommend it for publication contingent upon addressing the feedback provided.", "u2JeVfXIQa": " Review of \"Adaptive CrossLayer Attention for Image Restoration\"  1. Summary The paper introduces a novel attention module CrossLayer Attention (CLA) targeting image restoration tasks. This method deviates from conventional nonlocal attention mechanisms by allowing query pixels to interact with key pixels across previous layers thereby harnessing the potential correlations across different scales of representation. The authors address the computational inefficiency typically associated with such hierarchical attention models by restricting each query to a small fixed number of keys from preceding layers. Building upon CLA the paper proposes an enhanced version Adaptive CrossLayer Attention (ACLA) which dynamically selects the number of keys for aggregation. Furthermore the paper utilizes Neural Architecture Search (NAS) to identify optimal positions for inserting ACLA modules within CNN architectures. The efficacy and efficiency of ACLA are demonstrated through extensive experiments across various image restoration tasks including superresolution denoising demosaicing and compression artifacts reduction.  2. Evaluation Strong Points:  The paper tackles the significant challenge of capturing feature correlations across different layers in CNNs an aspect often overlooked in traditional attention mechanisms.  Introducing a deformation mechanism to limit computational costs represents a novel approach within the context of attention modules in image restoration.  The adaptive nature of ACLA allowing dynamic key selection is particularly innovative. This adaptability likely contributes significantly to the modules efficiency and effectiveness.  Extensive experiments and comparative analysis provide a solid foundation for the claimed improvements over existing methods. Weak Points:  While the paper excellently details the computational efficiencies introduced by CLA and ACLA the actual impact on overall computational cost and memory requirements particularly in comparison to conventional methods is not clearly quantified.  The role of Neural Architecture Search in the optimization of ACLA insert positions is a notable feature however the paper could benefit from a deeper discussion on the NAS methodology especially regarding how it differentiates from or improves upon existing NAS approaches for CNN optimization.  Further clarification on the deformation mechanisms specifics and how it precisely contributes to computational efficiency would enhance the readers understanding. Questions and Feedback:  In the experiments section the improvement in image restoration tasks is evident however details regarding the failure cases or limitations of the proposed CLA and ACLA modules would provide a more balanced view.  How does the computational efficiency of ACLA compare to traditional nonlocal attention mechanisms when applied at scale say in scenarios with very highresolution images  The paper briefly mentions the potential of ACLA in neural networks for nonimage data. Could the authors elaborate on possible adaptations or challenges in extending this approach beyond image restoration tasks  3. Review Summary This paper presents a compelling advancement in attention mechanisms for CNNs through its CrossLayer Attention and Adaptive CrossLayer Attention modules addressing critical challenges in image restoration tasks. The innovative approach to dynamically selecting keys and leveraging Neural Architecture Search to optimize module positioning sets a new precedent in the field. However a comprehensive analysis detailing the computational tradeoffs and a more thorough discussion on NAS methodology could further solidify the papers contributions. Despite these areas for expansion the demonstrated effectiveness of ACLA across a variety of restoration tasks strongly supports the introduction of this method to enhance CNN performance in image restoration and potentially other domains.", "lNreaMZf9X": " Peer Review of \"Disambiguating Model Design Choices for Dynamics Learning in ModelBased Reinforcement Learning\"  1. Summary The paper rigorously investigates how various model learning design choices impact planning performance in modelbased Reinforcement Learning (RL). By establishing feedforward neural networks to learn dynamics models the authors examine four critical areas: deterministic vs. stochastic models singlestep vs. multistep loss optimization the utility of network ensembles and the influence of introducing input noise. Leveraging the DeepMind Control Suite environments for empirical analysis they assess these design choices based on their ability to enhance planning performance both quantitatively and qualitatively. The outcomes reveal nuanced insights into the optimal configurations for model learning proposing best practices for deploying feedforward networks within this domain. Their systematic approach sheds light on the complex interplay between different model learning strategies contributing to a more informed application of modelbased RL.  2. Strengths and Weaknesses Strengths: a. Comprehensive Evaluation: The paper conducts an extensive comparison across multiple design choices environments and applies both quantitative metrics and qualitative analysis. This depth of evaluation provides robust evidence for the papers conclusions. b. Practical Insights: By suggesting best practices and highlighting the nonsignificant impact of particular design choices in certain situations the study offers valuable guidelines for practitioners in modelbased RL. c. Extensibility: The modular approach to investigating each design choice in isolation and in combination with others lays a groundwork that future research can easily extend or build upon. Weaknesses: a. Limited Scope in Environments: While the DeepMind Control Suite provides a diverse set of challenges the studys insights might not generalize to environments with drastically different characteristics or those outside continuous control tasks. b. Humanoid System Failure: The inability to obtain meaningful results for the humanoid model flags a potential limitation in the current approach or the chosen design choices\u2019 scalability to more complex systems. c. Quantitative vs. Qualitative Metrics: The reliance on MSE as a quantitative indicator despite acknowledging its limitations in reflecting planning performance might mislead or insufficiently capture the models capability for actual tasks. Questions and Feedback: 1. Can the authors comment on potential methodologies to tackle the identified shortcomings in humanoid and other complex models 2. The paper suggests that MSE is not a sufficient indicator of planning performance. Could alternative metrics or evaluation frameworks provide a better alignment with planning success 3. Regarding the use of input noise for stochastic models how does the choice of noise amplitude influence the balance between exploration and exploitation and could there be an optimized way to adjust it dynamically based on the environment\u2019s feedback  3. Review Summary The paper represents a significant step forward in disentangling the effects of various model learning design choices on planning performance within modelbased RL frameworks. Its rigorous empirical methodology alongside thoughtful analysis provides a comprehensive examination of the domain yielding actionable insights for practitioners. While the study offers several key contributions including best practices and nuanced understanding of design choices impact it also leaves room for exploration particularly in extending these findings to more complex systems and identifying alternative evaluation metrics that better correlate with planning performance. The limitations identified do not detract from the papers overall value but instead highlight avenues for future research such as the exploration of more adaptable models capable of handling highdimensional systems like humanoids and the development of evaluation frameworks that more accurately reflect planning efficacy. Given these considerations the paper significantly contributes to the field paving the way for more efficient and effective modelbased RL applications. Thus I recommend this paper for publication provided the authors consider the feedback and questions raised to further strengthen their findings.", "z3Tf4kdOE5D": " Summarize the Paper The paper introduces FEDDISCRETE a federated learning (FL) framework that enhances privacy and security through a novel probabilistic discretization mechanism at the client side. This approach discretizes the clients model weight into a binary vector format maintaining an unbiased estimation of the model weight for the server thus promising robustness against various weightbased attacks including availability poisoning integrity backdoor and inference attacks. The paper thoroughly explores the utility robustness and convergence of the discretization mechanism both theoretically and empirically across different datasets and FL settings showing superior defense capabilities without significant compromise on model performance.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Defense Mechanism: The introduction of a clientside probabilistic discretization mechanism is innovative directly addressing the issue of privacy leakage and various attacks without requiring the server to perform additional computations. 2. Comprehensive Analysis: The paper provides a detailed theoretical analysis of the utility robustness and convergence of the proposed mechanism which is further complemented by empirical validation across multiple datasets. 3. Robustness Across Diverse Settings: FEDDISCRETE demonstrates effective defense against multiple attack strategies in various settings including i.i.d and noni.i.d data distributions highlighting its adaptability and strength in different FL environments. Weak Points: 1. Scalability Concerns: While the paper mentions the applicability of FEDDISCRETE in crossdevice settings with numerous clients it acknowledges limitations in crosssilo settings with fewer clients potentially limiting its scalability across different FL applications. 2. Limited Defense Capability Against Byzantine Attacks: The paper admits the proposed mechanism may not defend effectively against Byzantine attacks by itself suggesting a potential area for further enhancement. 3. Potential Overhead from Discretization: Despite claiming communication efficiency the paper does not deeply explore the potential overhead (e.g. computation time and energy consumption) introduced by the discretization and additional rounds of communication which might be significant in resourceconstrained client devices. Questions and Feedback:  How does FEDDISCRETE handle the scenario where attackers control a significant portion of the clients particularly in smaller federated networks  Could the authors elaborate on the potential performance impact (e.g. latency energy usage) on client devices due to the discretization process  It would be beneficial if future work could explore integrating robust byzantine defense mechanisms within the FEDDISCRETE framework to broaden its defense capabilities.  Review Summary The paper presents FEDDISCRETE a pioneering approach in the realm of federated learning that significantly enhances the security and privacy of federated models through a clientside discretization mechanism. Its strength lies in its innovative approach to defending against various attacks without imposing extra computational demands on the server. The extensive theoretical and empirical analyses validate its effectiveness demonstrating robustness against numerous attack vectors across different datasets and federated settings. However the framework exhibits limitations in terms of scalability and defense capabilities against certain types of attacks such as Byzantine attacks especially in federated settings with a limited number of clients. Additionally the implications of the discretization process on the computational overhead for client devices warrant further investigation to ensure the practicality of FEDDISCRETE in resourceconstrained environments. In sum FEDDISCRETE represents a significant advancement in federated learning security. Its successful defense against a broad spectrum of attacks backed by solid theoretical and empirical analyses highlights its potential. Nonetheless addressing its scalability concerns expanding its defense mechanism to encompass a wider range of attacks and assessing its impact on client devices constitute vital steps for future research. This work lays a strong foundation for developing robust privacypreserving federated learning systems.", "iulEMLYh1uR": " Peer Review for \"Contradictions in Cost Indicators for Model Efficiency in Deep Learning\"  1) Summary of the Paper The paper titled \"Contradictions in Cost Indicators for Model Efficiency in Deep Learning\" embarks on a comprehensive examination of the various metrics used to assess the efficiency of machine learning models. The authors argue that the assumption of a correlation among different cost indicators like floatingpoint operations per second (FLOPs) number of parameters and speed or throughput can be misleading. The study elucidates how relying on a limited subset of metrics for reporting can obscure the full picture of a models efficiency and lead to biased or incomplete conclusions regarding its practical applicability. The authors term this phenomenon the \"efficiency misnomer.\" They present a series of experiments that highlight how model efficiency comparisons can vary significantly based on the chosen cost indicator especially in scenarios involving parameter sharing sparsity or parallelizable operations. Additionally the paper provides a critical review of current reporting practices in the literature and offers recommendations to enhance the accuracy and completeness of future efficiency reporting.  2) Strong and Weak Points of the Paper Questions and Feedback Strong Points:  Comprehensiveness: The paper delivers a detailed and thoughtprovoking analysis of different cost indicators considering a variety of models and circumstances. This broad approach is commendable and contributes significantly to the discourse on model efficiency.  Practical Insights: Through experimental evidence and discussions the paper offers valuable practical insights into how different cost indicators can lead to varying interpretations of efficiency which is highly valuable for both practitioners and researchers.  Recommendations: The suggestions provided for improving the reporting of efficiency metrics are clear and actionable emphasizing the papers contribution to enhancing future research and deployment practices. Weak Points:  Lack of Empirical Validation: While the paper presents experiments to illustrate the efficiency misnomer it somewhat lacks a systematic empirical validation of the proposed recommendations across diverse realworld datasets and model architectures.  Limited Discussion on Automated Tools: The discussion about existing and potential automated tools that can help in accurately measuring and reporting these cost indicators seems lacking. Incorporating insights on this aspect could be beneficial for researchers and practitioners. Questions and Feedback: 1. Clarification on Scalability: How do the reported findings on cost indicators relate to model scalability especially in the context of very large datasets It would be helpful if the authors could elaborate on this aspect. 2. Impact on Architecture Design: The paper touches upon how choosing certain efficiency metrics can influence model architecture design. Could the authors provide insights into how their findings might guide the future development of more efficient architectures 3. Automated Measurement Tools: Could the authors suggest any frameworks or tools that might help in automating the measurement of these diverse cost indicators thereby facilitating more comprehensive and unbiased reporting  3) Review Summary The paper makes a noteworthy contribution to the field by challenging the conventional wisdom regarding the correlation among various model efficiency cost indicators. The identification and detailed exploration of the \"efficiency misnomer\" phenomenon offer crucial insights that can significantly impact both the academic research and practical application of machine learning models. The suggested recommendations for reporting efficiency metrics are poised to foster a more accurate and holistic understanding of model efficiency enabling betterinformed decisions in model selection and development. However the paper could be strengthened by providing more rigorous empirical evidence across a broader range of models and datasets. Additionally including a discussion on the role of automated tools in measuring and reporting cost indicators could enhance the papers utility for the community. Overall the papers strengths significantly outweigh its weaknesses making it a valuable contribution to the ongoing dialogue on evaluating and improving the efficiency of machine learning models.", "i4qKmHdq6y8": " Summarize the Paper This paper addresses the challenge of learning from datasets containing a significant proportion of uninformative or noisy data which is common in domains like finance and public health. The authors introduce a novel methodology that differentiates between informative and uninformative data during both training and inference phases. This is achieved through a process that involves a predictive model and a selector model. The selector model aims to identify and abstain from making predictions on uninformative data allowing the predictive model to focus on learning from the structured informative data. The paper provides theoretical guarantees for the effectiveness of this approach including a proof of concept through selective risk and selector error rates under certain conditions. An iterative algorithm is proposed to jointly optimize both models and empirical results on realworld datasets demonstrate the superiority of this method over existing baselines.  Strong Points 1. Novelty of Approach: The introduction of a methodology that can distinguish between informative and uninformative data especially in fields plagued with high noisetosignal ratios is both novel and crucial. 2. Theoretical Foundation: The paper establishes theoretical guarantees for the proposed method providing a solid foundation for the empirical success observed. This duality enhances the reliability and attractiveness of the approach. 3. Empirical Validation: The experiments conducted on realworld datasets using complex scenarios of mixed informative and uninformative data confirm the practical applicability and effectiveness of the proposed method. 4. Contribution to Selective Learning: By focusing on datasets where a majority of the data may be uninformative the paper opens up new directions in selective learning a relatively underexplored area with significant potential.  Weak Points 1. Assumption of Binary Informative Status: The method assumes each data point is either informative or not which simplifies the complexity of realworld data. This binary view may not capture the nuances of varying degrees of informativeness present in practical datasets. 2. Limited Experimental Settings: While the selected datasets are relevant the paper could benefit from a wider range of datasets especially those with closer ties to the highlighted domains of finance and public health to comprehensively validate the approach. 3. Complexity and Scalability: The paper does not sufficiently address the computational complexity and scalability of the proposed method particularly in comparison to simpler baselines or when applied to extremely large datasets. 4. Future Work Clarity: The extension to settings with continuous transitions between informative and uninformative data is mentioned as future work. More detail on potential challenges and the envisioned approach for tackling continuous informativeness would be valuable.  Questions and Feedback to the Author 1. Handling Varied Degrees of Informativeness: Could the authors elaborate on potential extensions of the method to handle varying degrees of informativeness in data rather than a binary classification 2. Comparative Complexity Analysis: Can more insight be provided into the computational complexity of the proposed method especially in scenarios involving massive datasets 3. DomainSpecific Applications: Are there plans to apply this methodology directly within the finance and public health domains mentioned using datasets inherently tied to these fields 4. Practical Implementation Details: Could further details be provided on the practical implementation of the iterative algorithm particularly regarding hyperparameter choices and optimization techniques  Review Summary The paper introduces an innovative approach to learning from datasets heavily laden with uninformative data delineating between informative and uninformative samples and only making predictions on the former. This strategy is not only relevant but necessary in fields like finance and public health where the noisetosignal ratio is inherently high. The combination of theoretical foundation empirical validation and clear applicability to realworld problems strongly supports the papers contributions to the field of selective learning. However the paper could benefit from addressing the nuances of data informativeness beyond a binary classification expanding experimental validation to a broader range of datasets directly relevant to the highlighted domains and providing a detailed analysis on the computational aspects of the proposed methodology. Questions regarding the adaptability of the method to varying degrees of informativeness its computational complexity domainspecific applications and practical implementation details remain and would greatly enhance the papers scope and applicability. Overall the strengths of the paper including its novel approach and successful empirical validation outweigh its limitations. With additional clarity and expansion in the mentioned areas the paper could make a significant contribution to the field offering a robust methodology for tackling the challenge of learning from predominantly uninformative datasets.", "g8NJR6fCCl8": " 1. Summary of the Paper The paper introduces a novel deep learning adaptation of Generalized Additive Models (GAMs) and Generalized Additive Models for Location Scale and Shape (GA2Ms) termed NODEGAM and NODEGA2M respectively. These models are designed to bridge the gap between the interpretability of traditional GAMs and the scalability and performance benefits of deep learning. The authors emphasize the significance of interpretability in highrisk settings such as healthcare where understanding model decisions is crucial. They demonstrate that NODEGAM and NODEGA2M retain interpretability while achieving or surpassing the performance of existing GAMs on large datasets. The introduction of selfsupervised pretraining further enhances their performance especially in scenarios with limited labeled data. The paper goes on to compare these models against various baseline GAM implementations and full complexity models showing their competitive or superior performance across a range of datasets. Several aspects of the model design including feature interactions data preprocessing and regularization are discussed in detail. The authors also provide insight into extracting interpretable graphs from these models showcasing their practical value in deriving meaningful insights from data.  2. Strengths and Weaknesses  Strengths:  Interpretability and Performance: The paper successfully addresses the challenge of combining the interpretability of GAMs with the performance and scalability of deep learning models. This is a significant contribution given the increasing demand for transparent AI in critical applications.  SelfSupervised PreTraining: The novel use of selfsupervised pretraining to improve model accuracy in scenarios with limited labeled data is both innovative and practical providing a pathway to leverage unlabeled data effectively.  Comprehensive Evaluation: The thorough comparison with various GAM implementations and fullcomplexity models across multiple datasets provides a robust validation of the proposed models effectiveness.  Novel Architectural Changes: The architectural modifications to enable interpretability while leveraging deep learning (such as the gating mechanisms and attentionbased GAMs) are well thought out and represent a key innovation in the field.  Weaknesses:  RealWorld Application and Validation: While the paper discusses the potential of NODEGAM and NODEGA2M in highrisk areas realworld application examples or case studies are limited. Inclusion of such validations could strengthen the practical significance of the findings.  Complexity and Usability: The complexity introduced by the novel gating mechanisms and the attentionbased model design may pose challenges for practitioners to understand and implement these models effectively. A discussion on usability and guidelines for practitioners could enhance the papers impact.  Comparison with Recent Advances: While the paper compares NODEGAM and NODEGA2M with several existing models comparisons with the latest advancements in interpretable machine learning that have emerged since the referenced works could provide a more current context.  Questions and Feedback to the Author: 1. Could you provide more insights or examples on the realworld application of NODEGAM and NODEGA2M especially in the highrisk settings mentioned 2. How does the complexity of the proposed models affect their usability and interpretability from a practitioners perspective 3. Have there been considerations or tests on the performance of NODEGAM and NODEGA2M against the most recent interpretable machine learning models not covered in the comparisons  3. Review Summary The paper makes a noteworthy contribution to the field of interpretable machine learning by introducing NODEGAM and NODEGA2M which effectively bridge the gap between traditional GAMs interpretability and deep learnings performance. The novel use of selfsupervised pretraining to leverage unlabeled data for improving accuracy in limitedlabeled data scenarios is particularly commendable and innovative. The comprehensive evaluation of the models featuring comparisons across various datasets and against multiple baselines robustly validates their effectiveness. However to maximize the papers impact and relevance further discussions or demonstrations of realworld applications particularly in the highrisk domains mentioned would be highly beneficial. Additionally addressing potential usability challenges arising from the models complexity could widen their appeal and accessibility to practitioners. Overall the proposed NODEGAM and NODEGA2M represent a significant step forward in developing scalable highperformance machine learning models without sacrificing interpretability. With further validation in realworld scenarios and enhancements in usability these models have the potential to make substantial contributions to various critical domains including healthcare finance and criminal justice.", "kEvhVb452CC": " Peer Review for \"Transitioning Convolutional Networks to Transformed CNNs via Reparameterization for Enhanced Performance and Robustness\"  Summarization of the Paper This paper introduces an innovative approach to bridging the performance gap between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) by proposing a reparameterization method that transitions pretrained CNNs into Transformed CNNs (TCNNs) utilizing Gated Positional SelfAttention (GPSA) layers. The core idea is to initially leverage the efficiency of CNNs for feature extraction and subsequently enhance their capability by integrating selfattention mechanisms aiming to improve both performance and robustness with minimal computational overhead. The authors demonstrate that after a relatively short finetuning period (50 epochs) TCNNs significantly outperform the original CNNs in terms of accuracy and robustness across a variety of benchmarks including robustness to common corruptions adversarial attacks and distribution shifts. This method retains the advantages of CNNs while addressing their limitations in capturing longrange dependencies all without the typical training and computational challenges associated with ViTs. Key contributions include the development of a seamless transition method from CNNs to TCNNs a thorough evaluation of the performance and robustness improvements and an analysis of the changes in learned representations.  Strong Points 1. Novel Reparameterization Approach: The method of converting CNN layers to GPSA layers for seamlessly integrating selfattention is innovative. It provides a clear path forward for leveraging existing pretrained models to achieve greater performance and robustness. 2. Comprehensive Evaluation: The range of datasets and metrics used to evaluate the TCNNs including robustness to a variety of corruptions and adversarial attacks provides a thorough understanding of the models improved performance and applicability in realworld scenarios. 3. Insightful Analysis of Representations: The exploration of how the representations within the TCNNs evolve especially the dissection of convolutional versus contentbased attention heads offers deep insights into why these models outperform traditional CNNs and how selfattention layers contribute to model robustness.  Weak Points and Questions 1. Cost of Reparameterization in Early Stages: The paper acknowledges the prohibitive cost of reparameterizing early CNN stages but does not provide detailed analysis or potential solutions for this limitation. Could the authors discuss how significant this cost is and explore strategies that might mitigate it 2. Comparison with EndtoEnd Transformers: While TCNNs are shown to outperform CNNs and match hybrid models the comparison with stateoftheart endtoend Transformers (e.g. DeiT) implies that there is still a gap especially in certain benchmarks like ImageNetC. Could the authors elaborate on the reasons behind this remaining gap and potential steps to close it 3. Generalization to Other Architectures: The paper mainly focuses on ResNet architectures. How broadly applicable is the proposed reparameterization method to other CNN architectures Could this method be applied to more recent CNN variants like EfficientNets or DenseNets with similar benefits  Feedback to the Author The presented work is a significant contribution to the field effectively demonstrating how to enhance the performance and robustness of CNNs through a novel reparameterization technique. To strengthen the paper further it would be beneficial to address the computational costs associated with the proposed method in more detail especially for reparameterizing early stages of CNNs. Additionally expanding on the discussion around the performance gap between TCNNs and endtoend Transformers and exploring broader applicability to other CNN architectures could provide clearer guidance for future work and adoption of this approach in practice.  Review Summary The paper introduces a compelling method for transitioning traditional CNNs to TCNNs by reparameterizing convolutional layers with selfattention mechanisms significantly improving both performance and robustness with minimal additional computational cost. The novel approach thorough evaluation and insightful analysis of learned representations substantiate the claimed benefits. However exploring strategies to mitigate the cost of reparameterization providing deeper comparisons with endtoend Transformers and assessing applicability to a wider range of CNN architectures could further enhance the papers impact and practical value. Overall this work represents a considerable advancement in leveraging the strengths of both CNNs and Vision Transformers with broad implications for future research and applications in computer vision.", "keQjAwuC7j-": " Review for \"TransDenoiser: Certified Robustness and Differential Privacy via Perturbation Transformation\"  1) Summary This paper introduces a novel framework TransDenoiser aimed at enhancing the security and privacy of deep learning models equipped with pretrained classifiers. The major challenge tackled by this work is the susceptibility of such models to adversarial attacks which deliberately introduce minor input perturbations to mislead the model and the potential leakage of sensitive information from training data. The proposed solution TransDenoiser employs a denoiser specifically an autoencoder trained on Gaussiannoiseperturbed input data. This not only provides certified robustness against adversarial examples by leveraging randomized smoothing but also secures training data privacy through differential privacy (DP) assurances via gradient perturbation. By transforming input perturbation into gradient perturbation the authors creatively utilize the inherent randomness introduced for robustness in serving a dual purpose: aiding in achieving DP. This transmutation is directed by a Multivariate Gaussian Mechanism (MGM) and further bolstered by additional gradient perturbation whisked together under a Mixed Multivariate Gaussian Analysis (MMGA) to tightly estimate DP. The method outshines existing benchmarks in terms of offering a tighter DP guarantee and improved utility under the same privacy constraints validated through extensive experiments on MNIST and CIFAR10 datasets.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovativeness: The paper pioneers in harnessing the randomness introduced for attaining certified robustness towards fulfilling differential privacy requirements in deep learning models. This amalgamation is not only novel but also practically significant.  Rigorous Analysis: The theoretical underpinnings including the perturbation transformation and privacy analysis via MGM and MMGA are thoroughly presented offering a solid foundation for the claims of enhanced privacy and utility performance.  Extensive Evaluation: The authors conducted exhaustive experiments to validate the effectiveness of TransDenoiser. The comparison with stateoftheart methods and within various ablation settings underscores the frameworks superiority in balancing privacy and utility effectively. Weak Points:  Complexity and Scalability: While TransDenoiser demonstrates remarkable efficacy the paper does not extensively delve into its computational complexity or scalability concerning data size and dimensionality. Considering the potential computational overheads of MGM and additional perturbations insights into these aspects would be valuable.  Generalization to Other Data Types: The experiments focus predominantly on image data (MNIST and CIFAR10). Given the diverse applications of deep learning understanding how TransDenoiser adapts to other data types such as text or timeseries would enhance its applicability.  Comparison with NonDP Methods: The paper primarily benchmarks against methods that offer both DP and robustness. A comparison with leading methods that focus singularly on robustness or DP could provide a clearer landscape of the tradeoffs involved. Questions and Feedback:  How does the performance of TransDenoiser scale with the increase in data dimensionality and diversity Would the method maintain its efficacy across datasets with higher complexity or varying noise characteristics  Could the authors elaborate on the potential impacts of model complexity (e.g. depth and width of the pretrained classifiers and denoisers) on the balance between privacy and utility  It would be insightful if the paper could discuss any limitations encountered during TransDenoisers development and application. Specifically are there scenarios where the current iteration of TransDenoiser might not be ideal and how might future work address these gaps  3) Review Summary The paper presents a commendable effort in addressing two paramount concerns in deploying deep learning models: adversarial robustness and privacy preservation. TransDenoisers unique approach of leveraging input perturbations traditionally aimed at enhancing model robustness to also serve differential privacy objectives marks a significant advancement in the field. The clarity of the methodology coupled with exhaustive experimental validation lays a strong case for the proposed frameworks efficacy. However considerations regarding computational demands generalizability across varied data modalities and a broader comparative analysis would further strengthen the manuscript. Despite these minor points TransDenoiser stands as a promising solution to the dual challenges of maintaining both privacy and robustness in deep learning applications. The papers contributions are welljustified making it a valuable read for researchers and practitioners alike in the domains of secure and private AI systems.", "rGg-Qcyplgq": " Review of \"Exploring Distributional Reinforcement Learning with Randomized Risk Criteria\"  1. Summary This paper introduces a novel approach to distributional reinforcement learning (DRL) that aims to explore and optimize policies in stochastic environments by employing a randomized risk criterion. Traditional DRL methods rely on descriptive statistics to handle uncertainty often leading to biased exploration and suboptimal performance due to persistence in riskaverse or riskseeking behaviors. To address these challenges the authors propose a method that perturbs the risk criterion used in action selection which leads to a more balanced and effective exploration strategy. This is achieved by introducing a perturbed distributional Bellman optimality operator that distorts the risk measure within a defined ambiguity set. The paper proves the convergence and optimality of this approach through theoretical analysis and demonstrates its superior performance over existing methods in various gaming environments including Atari games and LunarLanderv2.  2. Strong and Weak Points Strong Points:  Novel Approach: The paper presents a unique perspective on tackling the explorationexploitation dilemma in DRL by randomizing the risk criterion which is a significant departure from the conventional optimism in the face of uncertainty (OFU) principles.  Theoretical Contributions: The authors provide a robust theoretical framework including proofs of convergence and optimality for the proposed method. This strong theoretical backbone enhances the credibility of their approach.  Empirical Validation: Comprehensive empirical evaluations in different environments attest to the practical effectiveness of the proposed method. The results convincingly demonstrate improvements over traditional OFUbased approaches and QRDQN particularly in environments with intrinsic uncertainties. Weak Points:  Complexity and Comprehension: The methods theoretical aspects while robust are quite dense and might be challenging for readers to fully grasp without a strong background in the subject. A clearer exposition or a simplified explanation of the key concepts could improve accessibility.  Generalizability and Applicability: While the method shows promise in gaming environments its effectiveness in realworld problems or domains with different characteristics remains to be seen. Further studies could explore its applicability beyond simulated environments.  Comparison with StateoftheArt: The paper primarily compares the proposed method against OFUbased approaches and QRDQN. Including comparisons with other stateoftheart DRL methods especially those that incorporate different exploration strategies could provide a more comprehensive view of the proposed methods advantages and limitations. Questions and Feedback to the Author: 1. Could you provide more intuition or simplified explanations behind the key theoretical results to make the paper more accessible to a broader audience 2. Have the methods been tested in realworld scenarios or tasks with highdimensional action spaces If not do the authors anticipate any challenges in such settings 3. The paper demonstrates superior performance in gaming environments. Are there plans to apply this method to other domains such as robotics or autonomous driving where risk sensitivity and efficient exploration are critical  3. Review Summary The paper introduces an innovative approach to exploring distributional reinforcement learning by randomizing the risk criterion helping to avoid the pitfalls of biased exploration due to a onesided tendency towards risk. The methods theoretical underpinnings are robust and its effectiveness is empirically validated across various simulated environments. While the paper is a significant contribution to the field its complexity could be a barrier to broader understanding. Additionally exploring its applicability to other domains and its stand against other stateoftheart methods could further establish its value. Overall the proposed method is a promising step forward in the development of more effective exploration strategies in distributional reinforcement learning.", "hC474P6AqN-": " Review of the Paper on MultiType Continuous Disentanglement Variational Autoencoder  Summary The paper introduces a novel approach to solve a significant challenge in machine learning tasks associated with categorically labeled datasets. Traditional datasets annotated with categorical labels such as \"happy\" or \"sad\" in emotion recognition suffer from a lack of uniformity across different datasets making it difficult to merge them for supervised learning tasks. This work presents a multitype continuous disentanglement variational autoencoder (VAE) that aims to address this issue by extracting and disentangling the true dimensional generative factors behind each categorical label. By doing so it projects the data points into a unified latent space allowing for the integration of multiple categorically diverse datasets. The authors demonstrate the efficacy of their method through experiments on synthetic datasets showcasing a perfect correlation between the disentangled latent values and the genuine generative factors thus affirming the potential of their framework in enhancing explainability in neural networks.  Strong Points 1. Innovative Approach: The paper addresses a critical and often overlooked issue in machine learning with an innovative solution. The idea of using a variational autoencoder to disentangle and unify different categorical datasets is novel and can significantly impact how datasets are merged and used. 2. Solid Experimental Validation: The experiments conducted on synthetic datasets and the detailed comparison with existing models provide a strong foundation for the claims made in the paper. The perfect correlation observed between encoded and original values demonstrates the effectiveness of the proposed method. 3. Promises for Explainability and Applicability: The paper opens up new avenues for improving explainability in machine learning models. By making it possible to analyze and interpret the latent space it lays the groundwork for future research in the area. Moreover the frameworks applicability to any categorically labeled dataset makes it a versatile tool.  Weak Points and Questions 1. Lack of Realworld Dataset Evaluation: While the synthetic dataset experiments are compelling the absence of realworld dataset evaluation leaves a gap in the paper. How does the model perform when applied to complex realworld datasets with noise and outliers 2. Computational Requirements: The complexity of the proposed model might translate into significant computational requirements. Can the authors provide insights into the scalability of their approach especially when dealing with large datasets 3. Generalization Concerns: The paper claims the model can project any categorized dataset into a unified latent space. However given the diversity and complexity of realworld data how generalizable is this approach Are there limitations in terms of the types of categorical labels or datasets it can merge effectively 4. Comparison with Other Methods: Although the paper positions its approach as superior to existing methods a more detailed quantitative and qualitative comparison (especially with realworld datasets) would strengthen the argument.  Feedback to the Author  It is recommended that the authors extend their experiments to include realworld datasets to validate the frameworks effectiveness further and address potential challenges in practical applications.  Exploring and discussing the computational efficiency and scalability of the proposed method would enhance the papers applicability insights.  Providing a more indepth comparison with current stateoftheart approaches including limitations and potential areas for further improvement could deliver a more balanced perspective.  Review Summary The paper introduces a promising framework tackling the significant challenge of merging categorically diverse datasets by identifying the true dimensional generative factors behind each label. The novel use of a variational autoencoder for this purpose is both innovative and well substantiated through synthetic dataset experiments. Despite the impressive results the absence of realworld dataset applications and discussions on scalability and generalizability leaves room for questioning the frameworks practical implications. Therefore while the proposed method shows tremendous potential further validation and exploration are advised to ensure its effectiveness and applicability in realworld scenarios.", "oLYTo-pL0Be": " Review of \"Utilising Patient Data across Multiple Hospital Data Centres for Federated Machine Learning without Sacrificing Privacy\"  1. Summary The paper presents a federated learning framework tailored to the healthcare context aiming to leverage patient data across multiple hospital data centers while ensuring privacy. Through the inventive use of a scheduler and a studentteacher algorithm trained using metagradients and reinforcement learning respectively the authors propose a novel approach to machine learning that mitigates the need for direct access to patient data. This framework enables a central model to learn from distributed datasets by intelligently selecting which data batches to use at each learning step. In addressing the challenges of data distribution diversity and potential data poisoning this work contributes significantly to the federated learning field particularly in applications to healthcare data privacy. The methodology showcases overcoming traditional federated learning challenges including node poisoning through superior performance benchmarks against other stateoftheart models while also offering possibilities for transfer learning.  2. Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a highly relevant and sensitive issue in healthcare  patient data privacy and proposes a novel solution enhancing both privacy preservation and model performance.  The use of a studentteacher algorithm in conjunction with a scheduler trained via metagradients to direct learning effectively at different federated nodes is stimulating and creative. This potentially opens up new avenues for research in federated learning methodologies.  Experiments conducted across multiple datasets including patient data and standard machine learning datasets (CIFAR10 MNIST) provide a robust validation for the proposed methodology. Additionally the comparison with stateoftheart models and the resilience against data poisoning significantly contribute to the papers strengths. Weak Points:  While the paper is thorough in methodology and experimentation it does lack a clear discussion on the practical implementation challenges in realworld hospital environments such as the computational requirements and the network constraints.  There is an assumption of data homogeneity across hospitals which might not always be the case. The variability in data quality structure and representation was not fully addressed which might impact the generalizability of the model outside the tested environments.  The paper could benefit from a deeper analytical examination of the schedulers decisionmaking process. Understanding why certain nodes are preferred at different training stages could provide insights into further optimizing the federated learning process. Questions and Feedback:  Could the authors elaborate on how the federated learning framework would handle realtime data updates or continuous learning scenarios where patient data is incrementally added to hospital databases  It would be valuable to see a discussion on the potential scalability issues of the proposed model especially considering a scenario with a significantly larger number of federated nodes.  The defense mechanisms against data poisoning are compelling. However could there be potential vulnerabilities in the teacherstudent algorithm itself For instance what if the reinforcement learning process of the teacher is manipulated by adversarially crafted inputs  3. Review Summary This paper makes a significant contribution to the field of federated learning particularly within the critical context of healthcare data privacy. The innovative approach of leveraging a studentteacher model managed by a scheduler trained on metagradients represents a notable advancement in ensuring data privacy while achieving high model performance. The comprehensive experiments conducted and comparisons drawn with existing models not only validate the efficacy of the proposed solution but also underscore its potential to mitigate common federated learning challenges such as data poisoning. However for the paper to make a more grounded case for practical implementation it would benefit from an explicit discussion on the adaptability of the model in diverse realworld environments particularly focusing on the heterogeneity of data and the pragmatic challenges of deployment in hospital settings. Addressing these aspects could significantly enhance the papers relevance and applicability. Overall the paper presents a compelling case for a novel federated learning framework that prioritizes patient privacy without compromising on learning efficiency or model performance. With further exploration and clarification on some of the raised points this work has the potential to influence both the academic and practical realms of machine learning in healthcare.", "v-f7ifhKYps": " Review of \"Maximum Entropy Populationbased Training for Zeroshot HumanAI Coordination\"  1) Summary: The paper presents a methodology for improving zeroshot humanAI coordination by introducing Maximum Entropy Populationbased training (MEP). This approach aims to train a Reinforcement Learning (RL) agent to effectively coordinate with humans in cooperative tasks without prior exposure to human data. By leveraging maximum entropy RL MEP seeks to enhance diversity within a population of agents through a centralized population entropy objective. This diverse set of agents then serves to train a robust AI agent capable of adapting to a wide range of human behaviors. The effectiveness of the MEP approach is demonstrated through experiments in the Overcooked game environment and further validated by online experiments with real human participants.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: The paper addresses the crucial issue of zeroshot humanAI coordination with an innovative solution that balances individual and collective diversity in agent training.  Theoretical Framework: The derivation of a safe and computationally efficient surrogate objective population entropy grounded in maximum entropy RL provides a solid theoretical foundation for the approach.  Empirical Validation: Comprehensive experiments including comparisons with baseline methods and realworld human interaction offer strong empirical support for the proposed methodology.  Practical Relevance: The study addresses a significant challenge in humanAI collaboration presenting a method potentially applicable in various realworld scenarios. Weaknesses:  Complexity and Scalability: The methodology while innovative appears to be computationally intensive raising questions about scalability to more complex environments or larger populations.  Generalization Concerns: Despite the positive results in Overcooked and with recruited users it remains uncertain how well MEP generalizes across different types of games or realworld applications beyond cooperative cooking scenarios.  Limited Discussion on Limitations: The paper could benefit from a more detailed discussion on the limitations and potential failure modes of the MEP approach.  Questions about Human Data: The claim of zeroshot learning might be slightly overstated. The method does require a model (HProxy) trained on human data albeit not directly using this data to train the AI agent. Questions:  How does MEP scale with the complexity of the environment or the size of the agent population  Can the authors elaborate on how MEP could be adapted or extended to competitive settings or environments with nonstationary dynamics  Is there potential for integrating MEP with other techniques such as metalearning to further enhance its generalization capabilities Feedback:  It would be valuable to see an analysis of the computational costs associated with MEP and potential strategies for optimization.  Further investigation into the generalization of MEP across a broader range of environments would strengthen the case for its practical utility.  A more comprehensive discussion on the limitations and the circumstances under which MEP might not perform as expected would provide a balanced perspective.  Review Summary: This paper introduces MEP a novel approach to training RL agents for effective zeroshot coordination with humans. By fostering diversity within a population of agents the method strengthens the adaptability of the trained AI agent to various human behaviors. The theoretical underpinning based on maximum entropy RL and the robust empirical validation are significant strengths of this work demonstrating potential practical impact in humanAI collaboration tasks. However concerns regarding the scalability generalization across diverse settings and the somewhat limited discussion on limitations and human data usage suggest areas for further development and clarification. Overall the paper makes a valuable contribution to the field proposing an avenue for advancing humanAI coordination without direct human data dependency. The suggested questions and feedback aim to guide future research to address the current limitations and explore the full potential of MEP in diverse applications.", "fUhxuop_Q1r": " Peer Review on \"Characterizing Generalization in Reinforcement Learning via Contextual Decision Processes\"  Summarize the Paper The authors of the manuscript propose a novel framework for evaluating generalization in Reinforcement Learning (RL) by combining a tabular environment with observations from a supervised learning dataset specifically the MNIST dataset to create a contextual decision process (CDP). This approach allows for the assessment of an RL agents ability to generalize across states observations and actions within the same environment a task not readily achievable in most conventional RL environments. Through extensive experimentation with DQN and QRDQN algorithms the authors investigate the impact of common regularization methods like dropout and L2 regularization on generalization in RL. Interestingly the paper refutes some previous claims showing that dropout does not improve observation generalization but aids action generalization. It also corroborates that QRDQN outperforms DQN in observation generalization in the offline setting but not in state generalization.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty in Evaluation Framework: The paper introduces an innovative way to dissect RL generalization into state observation and action axes providing a more nuanced understanding of an agents generalization capabilities. 2. Rigorous Empirical Validation: The work rigorously evaluates previous claims regarding generalization mechanisms in RL using a setup that allows for the recovery of groundtruth optimal policies and value functions for a thorough investigation. 3. Comprehensive Experiments: The extensive experimental studies covering both online and offline learning settings offer exhaustive evidence to support the authors claims about the nuances of generalization in RL. Weak Points: 1. Limited Environment Diversity: While the use of MNIST with gridworld environments provides a clear framework for evaluating generalization the studys insights might be limited by the lack of diversity in environmental complexity and types. 2. Generalization to Realworld Scenarios: The paper could further discuss how their findings would translate to more complex and heterogenous environments typical of realworld applications of RL. 3. Discussion on Computational Resources: There is little mention of the computational resources required for these experiments which is critical for reproducibility and understanding the scalability of the proposed evaluation method. Questions and Feedback: 1. Extension to More Complex Environments: Can the authors provide insights or preliminary results on applying their evaluation framework to more complex RL environments such as those found in the Atari or OpenAI Gym How would the disentangled perspective of generalization adapt to environments with continuous action spaces 2. Impact on RL Algorithm Design: Given the findings about generalization mechanisms how do the authors foresee their work influencing the design of new RL algorithms Are there specific modifications to DQN or QRDQN that could enhance their generalization capabilities based on this study 3. Evaluation of Computational Resources: Could the authors comment on the computational efficiency and resources required for their experiments Understanding the computational demands is crucial for researchers attempting to replicate or extend this work.  Review Summary The manuscript offers a fresh lens to examine generalization in RL by introducing a novel evaluation framework that combines elements of supervised learning datasets with tabular environments. This approach permits an unprecedented depth of analysis into how RL agents generalize across different axes (states observations actions) filling a significant gap in the existing literature on RL generalization. The authors effectively challenge and refine the prevailing understanding of the effectiveness of common generalization mechanisms such as dropout in observation and action generalization. These nuanced findings emphasize the papers original contribution to the discourse on RL and underscore the necessity for careful consideration when evaluating generalization in RL. However the research could be augmented by addressing the environments diversity and discussing the implications of these findings for realworld applications and the design of future RL algorithms. Additionally details on computational resources would enhance the papers practical applicability and reproducibility. Overall the manuscript significantly advances our understanding of generalization in RL providing a robust foundation for future research endeavors aimed at developing more generalized RL algorithms. The combination of innovative methodological contributions and comprehensive empirical validation supports a strong recommendation for publication pending minor revisions to address the discussed limitations and questions.", "rX3rZYP8zZF": " 1. Summarize the paper The paper presents the development and evaluation of a recommender system named \"CareGraph\" for a digital health platform focusing on diabetes management. The system utilizes knowledge graph embeddings to address the challenges of coldstart and data sparsity in recommending personalized health nudges to users. A deep neural networkbased model is proposed leveraging the knowledge graph embeddings to enhance the recommendation of health nudges aimed at maximizing user engagement. The authors compare the performance of the CareGraph model against baseline models based on text similarity and a DeepandCross Network approach demonstrating an improved ClickThrough Rate prediction accuracy and better performance in coldstart situations.  2. List strong and weak points of the paper Question and Feedback to the author Strong Points: 1. Innovative Approach: The utilization of knowledge graph embeddings to solve the coldstart problem in the context of health nudge recommendations is novel and addresses a realworld issue in the digital health domain. 2. Comprehensive Evaluation: The authors conducted thorough performance evaluations comparing the proposed model with baseline approaches providing a clear demonstration of its effectiveness. 3. Relevance and Impact: This research addresses a highly relevant problem in the digital health space providing a tool that can potentially improve diabetes selfmanagement for users. 4. Clear Methodology: The paper explains the development process of the CareGraph model methodically including problem formulation methodology and experimental setup which allows for reproducibility. Weak Points: 1. Lack of Detailed Explanation for NonExperts: The paper could benefit from a more detailed explanation of knowledge graph embeddings and deep learning aspects for readers who may not be familiar with these concepts. 2. Limited Scope of Evaluation Metrics: The primary metric for evaluation is AUC. Including additional metrics such as F1score or precisionrecall could provide a more rounded assessment of model performance. 3. Consideration of User Privacy: There is little discussion on how user data privacy is handled which is crucial in healthrelated applications. Questions: 1. Were ethical considerations particularly regarding data privacy and user consent taken into account in the development and deployment of the CareGraph system 2. Could the authors elaborate on how the model might be adapted or extended to other chronic conditions beyond diabetes Feedback: 1. It would enhance the paper to discuss the ethical considerations and data privacy measures in more detail considering the sensitive nature of health data. 2. Adding a user study or qualitative feedback on the perceived usefulness and acceptance of the recommended nudges could provide valuable insights into user engagement beyond clickthrough rates.  3. Review Summary The paper introduces an innovative approach to addressing the coldstart problem in the domain of digital health platforms through the use of a knowledge graphbased recommender system CareGraph. This work is particularly relevant given the increasing shift towards digital health solutions and the need for personalized engagement strategies in chronic disease management. The proposed system demonstrates encouraging results outperforming baseline models in terms of prediction accuracy especially in coldstart scenarios. While the approach and findings are solid enhancing the paper with detailed discussions on ethical considerations expanding the scope of evaluation metrics and considering user privacy in depth would make the contribution even more robust. Additionally exploring the adaptability of the model to other contexts and including user perceptions of the recommendations could offer broader insights. Overall this work presents a significant step forward in personalized health recommendations with strong potential for impacting user engagement in digital health platforms. The clear methodology coupled with rigorous evaluation supports the efficacy of the CareGraph model offering promising avenues for future research and application in health technology enhancements.", "psQ6wcNXjS1": "Summarize the paper This work introduces novel strategies for training EnergyBased Models (EBMs) through the implementation of distinct Markov Chain Monte Carlo (MCMC) sampling trajectory lengths aimed at optimizing for three specific applications: image synthesis adversarial defense and density estimation. The core contribution involves the introduction of three new MCMC initialization methods that significantly enhance the performance of EBMs across these domains without altering the fundamental maximum likelihood (ML) learning objective or the architecture of the networks involved. These methods are validated through experiments that demonstrate superior performance in generating stateoftheart Frechet Inception Distance (FID) scores for unnormalized image densities on CIFAR10 and ImageNet datasets providing leading adversarial defense mechanisms on the same datasets and enabling scalable density estimation techniques with practical applications. Strong points: 1. Innovative Approach to Sampling Initialization: The papers fresh perspective on leveraging different lengths of MCMC trajectories through unique initialization strategies for diverse applications represents a significant contribution to the field of generative modeling. 2. Comprehensive Application Range: Addressing three critical areas (image synthesis adversarial defense density estimation) with a single unified approach illustrates the versatility and potential widespread impact of the proposed methods. 3. Significant Performance Gains: Demonstrating stateoftheart results across multiple benchmarks sets a new standard for EBMs and potentially for generative modeling as a whole. 4. Practical Implications: The paper not only advances theoretical understanding but also presents methods with immediate practical applications particularly in enhancing image synthesis quality and defending against adversarial attacks. 5. Clear Presentation and Experimentation: The clarity in the presentation of the methods alongside detailed experimentation and thorough comparisons with existing models make the findings of this paper particularly compelling. Weak Points and Feedback: 1. Computational Efficiency: It remains unclear how these initialization strategies impact the computational efficiency both in terms of training time and inference. Given that longer MCMC trajectories are computationally demanding elaboration on this aspect would enhance the papers relevance for practical applications. 2. Generalization Across Datasets: While CIFAR10 and ImageNet results are impressive the generalizability of these methods to other datasets or modalities (e.g. text audio) is not discussed. Insights into the models performance beyond image datasets would provide a more holistic understanding of its applicability. 3. Comparison with StateoftheArt Methods: The paper demonstrates superior performance compared to other EBMs but does not extensively compare its approaches with nonEBM stateoftheart methods across all tasks particularly in the domain of adversarial defense. 4. Longterm Impact on Density Estimation: While the paper addresses the issue of learning valid densities the longterm implications of these improvements (e.g. in sampling efficiency model interpretability) are not fully explored. 5. Theoretical Justification: A deeper theoretical analysis of why and how different MCMC initialization strategies explicitly contribute to performance gains in their respective applications would offer more rigorous support for the empirical findings. Review Summary: This paper represents a significant step forward in the domain of EnergyBased Models through its novel approach to MCMC sampling initialization tailored specifically for different lengths to optimize image synthesis adversarial defense and density estimation. By focusing on initialization strategies within a coherent maximum likelihood framework and achieving stateoftheart performance in several benchmarks the work sets new standards for EBMs and potentially generative models at large. While the study is robust in its current form addressing aspects such as computational efficiency broader generalizability and comparative analysis with nonEBM methods alongside a deeper theoretical justification of the proposed methods would make the contributions even more impactful. Given the clear presentation significant findings and the practical importance of the addressed problems this paper is recommended for publication. However addressing the highlighted feedback could further strengthen the paper.", "lycl1GD7fVP": " Peer Review of \"Generalization in Kernel Regression and Infinite Neural Networks: A New Theory\"  1. Summary The paper offers a significant contribution to the understanding of the generalization behavior of wide neural networks and kernel regression. It establishes a novel theoretical framework predicting generalization performance encompassing meansquared error (MSE) and both first and secondorder statistics of the learned function. Central to this theory is the introduction of a quantified measure of \"learnability\" and a fundamental \"nofreelunch\" theorem that underpins the tradeoff in inductive bias for kernel regression models. The authors demonstrate how their theory can explain and predict nuanced phenomena such as worsethanchance generalization on certain functions and nonmonotonic error curves. Through rigorous experimentation with both synthetic datasets and finite neural networks the paper validates the theoretical findings showing that their predictions hold remarkably well even outside the infinitewidth limit.  2. Strengths and Weaknesses Strengths:  Theoretical Novelty: The introduction of a \"learnability\" measure and a \"nofreelunch\" theorem for kernel regression are significant theoretical advancements. These contributions provide a robust framework for understanding the intrinsic tradeoffs in the generalization capabilities of kernel regression models.  Comprehensive Analysis: The paper presents a thorough analysis of the derived theory covering a wide spectrum of scenarios and elucidating first and secondorder generalization statistics. This comprehensive treatment significantly advances the understanding of kernel regression\u2019s inductive bias and learning behavior.  Empirical Validation: The experimental section effectively demonstrates the accuracy of the theoretical predictions even for finitewidth networks. This not only validates the theory but also shows its potential applicability to practical machine learning problems. Weaknesses:  Assumption of Infinite Width: While the theory is derived under the assumption of infinitewidth networks and experiments support its applicability to finite settings the exact conditions under which the predictions hold for finitewidth networks could be more clearly delineated.  Focus on FullyConnected Networks: The experiments primarily focus on fullyconnected architectures. Including other architectures such as convolutional and recurrent networks might provide a more comprehensive understanding of the theory\u2019s applicability across different types of neural networks.  Question and Feedback to the Author: How does the theory adapt or scale with the complexity of the learning problem particularly regarding the function spaces dimensionality It would be beneficial if the authors could discuss or provide insights into how the theorys predictions might vary with increasing data complexity or in the presence of highdimensional input spaces.  3. Review Summary The paper successfully advances the theoretical understanding of generalization in kernel regression and infinitely wide neural networks. The novel theoretical insights particularly the concepts of \"learnability\" and the \"nofreelunch\" theorem are significant contributions that elucidate the intrinsic tradeoffs in kernel regressions inductive bias. The empirical validation of the theory even in finitewidth networks is impressive and suggests broader applicability to practical machine learning settings. However refining the theory to address its assumptions more comprehensively and exploring its implications across a diverse set of neural network architectures could further enhance its impact. Overall the paper represents a substantial forward step in demystifying the generalization behavior of kernel regression and wide neural networks with strong theoretical and empirical backing supporting its conclusions.", "xbx7Hxjbd79": " Peer Review  Summary of the Paper This paper deals with optimization problems in the context of differentiable games such as Generative Adversarial Networks (GANs) and multiagent Reinforcement Learning (RL) emphasizing the Learning with OpponentLearning Awareness (LOLA) approach. The authors scrutinize the notion of consistency within LOLA critiquing the assumptions made about opponent behavior and the methods failure to preserve stable fixed points (SFPs). Previous methodologies like Competitive Gradient Descent (CGD) are discussed with a new proposal termed Consistent LOLA (COLA) that aims for mutual opponent shaping while addressing the drawbacks noted in LOLA. COLA is described as a learning model that avoids issues related to convergence and the calculation of highorder derivatives present in LOLA by using neural networks to learn consistent update functions. The paper extensively explores theoretical aspects offering proofs and propositions related to the consistency and convergence of COLA challenges the belief that consistency leads to the preservation of SFPs and compares the empirical performance of COLA with LOLA CGD and other algorithms across various game settings.  Strong Points 1. Innovative Approach: The paper introduces COLA a novel method addressing the consistency issue in learning with opponent awareness a significant step forward in the field of differentiable games and multiagent learning. 2. Comprehensive Theoretical Foundation: The authors provide a thorough theoretical analysis including formal definitions propositions and proofs that deepen the understanding of consistency in differentiable games. 3. Empirical Analysis: The study includes an extensive empirical comparison of LOLA CGD SOS and the newly proposed COLA across a variety of games showcasing the practical implications and performance of COLA. 4. Addressing a Gap: This work addresses a critical gap in understanding the inconsistency issue of LOLA and challenges the prevailing assumption about the relationship between consistency and the preservation of SFPs.  Weak Points 1. Complexity and Accessibility: The theoretical depth while a strength makes the paper quite dense and potentially challenging for readers not deeply versed in the mathematics of differentiable games. 2. Neural Network Specifics: The use of neural networks in COLA for learning update functions is innovative however the details regarding the architecture training procedure and parameter selection are not deeply discussed. 3. Scalability Considerations: Although the paper hints at the potential of COLA in GANs or Deep RL there is limited discussion on the scalability of COLA to highdimensional settings or more complex games beyond the twoplayer framework. 4. Comparison with Baselines: While the empirical analysis is strong additional baselines especially newer methods in the fastevolving field of differentiable games could provide a clearer picture of COLAs positioning.  Questions and Feedback 1. Neural Network Details: Could the authors provide more specifics on the neural networks used in COLA including architecture training details and any challenges faced in training 2. Scalability and Complexity: How does COLA scale with the complexity of the games and the dimensionality of the problem space Are there computational constraints that limit its application 3. Beyond TwoPlayer Games: The paper focuses on twoplayer games. How does COLA perform in multiplayer settings and what modifications if any are required to adapt it to more complex scenarios 4. Baseline Comparisons: Could the authors elaborate on the selection of baselines for the empirical analysis Including more recent methods as baselines could enhance the comparative analysis.  Review Summary The paper contributes significantly to our understanding and methodology for dealing with consistency in differentiable games especially in the context of opponentlearning awareness. Introducing COLA is a notable advancement addressing the consistency issue in LOLA and showing that consistency does not necessarily preserve SFPs. The theoretical analysis is both comprehensive and illuminating offering formal frameworks and proofs that deepen our understanding of these complex systems. Empirically the comparison of COLA with existing methodologies like LOLA CGD and SOS across various games demonstrates the practical benefits and challenges of this new approach. However the paper could be strengthened by providing more details on the neural network implementations discussing scalability and applicability to more complex or higherdimensional settings and expanding the range of baselines for comparison. Overall this work is a notable contribution to the field opening pathways for future research on multiagent learning and the design of algorithms for differentiable games.", "uPv9Y3gmAI5": "1) Summary of the paper The paper addresses the challenge of optimizing the compression of transformerbased language models without compromising their task accuracy. It critically analyzes the limitations of conventional Singular Value Decomposition (SVD) in model compression where SVD minimizes the reconstruction error of the compressed model without considering the importance of specific parameters for the models task performance. To tackle this problem the authors propose FisherWeighted SVD (FWSVD) a novel method that incorporates Fisher information to weight parameter importance in the compression process. By doing so FWSVD aligns the compression objective with the model\u2019s task accuracy effectively maintaining model performance with a higher compression rate compared to standard SVD. This method is extensively tested on transformerbased language models across a range of language tasks showing promising results in retaining taskspecific model accuracy despite significant parameter reduction. The paper also examines the effectiveness of FWSVD on already compact models demonstrating its potential to further reduce parameters with minimal impact on performance. 2) Strong and weak points Question and Feedback to the author Strong points:  The paper addresses a significant and previously unexplored problem in the field of model compression offering a fresh perspective on aligning factorization objectives with task accuracy.  The introduction of Fisher information as a criterion for weighting parameter importance in SVD is both innovative and theoretically sound supported by comprehensive empirical evidence.  The extensive experiments and analyses conducted across various language tasks and models provide a robust validation of FWSVD\u2019s effectiveness showcasing its superiority over standard SVD and other compact model strategies in maintaining or even improving task accuracy postcompression.  Implementation details and comparison methodologies are thoroughly described ensuring replicability and fairness in evaluation. Weak points:  The paper predominantly focuses on transformerbased language models which may limit the generalizability of the findings to other model architectures or tasks outside the NLP domain.  While FWSVD shows remarkable results in compressing taskspecific models the method\u2019s dependency on taskspecific data for computing Fisher information might restrict its applicability for compressing generic models not yet finetuned for specific tasks.  The simplification of using a rowwise importance matrix in FWSVD although effective might not fully leverage the granularity of information that could be provided by an elementwise approach. Questions and Feedback:  Could the authors elaborate on the potential impact of FWSVD on the deployment of compressed models in realworld applications particularly in resourceconstrained environments  It would be beneficial if future work could explore the adaptability of FWSVD to other model architectures beyond language models to assess the methods versatility and applicability across different domains.  Given the simplified importance matrix used in this study how significant do the authors believe the performance improvements could be if an elementwise factorization solution were developed and implemented 3) Review Summary This paper presents a significant advancement in the field of model compression addressing a critical gap in the alignment of compression objectives with model task accuracy. The introduction of Fisherweighted SVD as a method to ensure the compressed model maintains its performance by prioritizing the reconstruction of parameters critical to task accuracy is both innovative and impactful. The thorough empirical validation across a range of language tasks and models provides convincing evidence of the method\u2019s effectiveness showcasing its potential to redefine approaches to efficient model compression. Despite its strong contributions the focus on language models and the use of a simplified importance matrix may limit the scope of the findings. Nonetheless these limitations open avenues for future research that could further augment the versatility and efficacy of FWSVD. Overall the paper is a valuable contribution to the literature on model compression offering practical methods for maintaining high model performance postcompression. Its clear exposition rigorous analysis and substantial experimental results support its recommendations and highlight the potential of FWSVD for broader application in the AI field.", "mKsMcL8FfsV": "Peer Review of \"Learning SelfSupervised Representations through GradientDescent Based Ensemble Learning\" 1. Summarize the paper. This paper introduces a novel framework to improve the representation quality of convolutional neural networks (CNNs) pretrained in a selfsupervised manner through model ensembling and gradient descent at inference time. Unlike traditional methods that ensemble model outputs after training this work proposes a direct learning approach of representations across multiple selfsupervised models. The technique enables better nearest neighbor (NN) performance in downstream tasks both on the original dataset and in transfer learning scenarios. The paper also examines the application of this ensembling method on individual models demonstrating improvements echoing those found in selfdistillation practices. Moreover the study extends the use of multilayer perceptrons (MLPs) trained on one dataset to learn representations on new datasets without further training emphasizing the versatility and generalization capability of their approach. 2. Strong and Weak Points of the Paper. Strong Points:  The paper addresses a relatively unexplored area of ensembling selfsupervised models providing a novel method that enhances representation quality.  The approach is versatile as demonstrated by its application across various selfsupervised models and its adaptability to different datasets without requiring MLP retraining.  The methodology is wellsupported by empirical evidence showing significant improvement over baselines in NN accuracy across several datasets.  The use of gradient descent at inference time to adjust representations is both innovative and technically intriguing distinguishing this work from conventional ensembling techniques. Weak Points:  The paper does not provide a thorough theoretical foundation for why learning representations directly through gradient descent leads to better performance leaving the reader to infer the mechanisms at play.  While the method demonstrates improvement in NN accuracy its unclear how this translates to more complex downstream tasks beyond classification.  The computational cost of the proposed approach especially the utility of gradient descent at inference time is not thoroughly discussed which might be a concern for practical applications.  The comparison to existing methods primarily focuses on ensembling and selfdistillation potentially overlooking other relevant techniques in selfsupervised or unsupervised learning that could serve as benchmarks. Questions and Feedback to the Author:  Could the authors provide more theoretical insights or hypotheses on why gradient descent at inference time improves the representations learned by the ensemble of models  How does the proposed method perform on downstream tasks that are more complex than NN classification such as object detection or semantic segmentation  Can the authors elaborate on the computational efficiency of their method particularly the impact of gradient descent at inference time on computational requirements and latency  It would be beneficial to compare the proposed approach with a broader set of existing methods in selfsupervised learning. Have the authors considered including comparisons with newer or alternative techniques that also aim to improve representation learning 3. Review Summary. The paper introduces an innovative approach to enhancing the representation quality of selfsupervised convolutional neural networks through a novel method of model ensembling and gradient descent at inference time. This work stands out for its unique perspective on directly learning representations and its demonstration of versatility across different models and datasets. The empirical results strongly support the efficacy of the proposed method showing significant improvements in nearest neighbor accuracy. However the paper could benefit from a more comprehensive theoretical explanation of the observed improvements exploration of the method\u2019s performance on complex downstream tasks and a detailed discussion on computational efficiency. Additionally broadening the scope of comparison to include alternative selfsupervised learning techniques would strengthen the argument for the novelty and superiority of the proposed approach. Overall the paper presents a compelling contribution to the field of selfsupervised learning opening new avenues for research in model ensembling and representation learning. The proposed methods versatility and demonstrated improvements in multiple scenarios underscore its potential impact on future studies and applications in computer vision and beyond.", "fuaHYhuYIDm": " Summarize the paper The paper introduces MAGNEx a novel global and modelagnostic explainability algorithm designed to generate explanations for decisions made by blackbox models. Unlike existing methods that often focus on local instancebased explanations or are limited by model type MAGNEx applies to any pretrained model and aims to unveil the rationale behind its decisions en masse potentially offering more generalized and computationally efficient explanations. It achieves this through a neural network explainer trained to assess the importance of input features globally which alleviates hindsight bias and the computational burdens seen in perturbationbased methods. The results across various experiments in image classification sentiment classification and question answering show that MAGNEx outperforms popular explainability algorithms like LIME and Integrated Gradients in producing highquality faithful explanations more efficiently.  Strong and Weak Points Questions and Feedback Strong Points: 1. ModelAgnosticity and Global Nature: MAGNExs ability to apply broadly across various models and tasks along with its focus on global rather than local explanations addresses significant limitations in the field of explainable AI. 2. Computational Efficiency: The shift of the computationally intensive process of feature importance learning to the training phase rather than at inference presents a noteworthy advancement in generating explanations rapidly. 3. Alleviation of Hindsight Bias: By learning explanations across multiple instances MAGNEx is less likely to overfit to the peculiarities of individual examples thus reducing hindsight bias and potentially increasing the generality and reliability of the explanations. Weak Points: 1. Lack of HumanCentric Evaluation: Despite the significant achievements in model and computational efficiency the paper falls short in evaluating the human interpretability of the generated explanations. 2. Dependence on Quality of Training Data: As with many AI systems MAGNExs performance is likely contingent on the diversity and quality of the data it is trained on. This limitation albeit minor raises questions about its performance in lessdiverse or biased data scenarios. 3. Potential Overemphasis on Sparsity: While sparsity is often desired in explanations for simplicity there might be intricate decisions where a more nuanced or detailed explanation is necessary. Questions and Feedback: 1. Could the authors provide insights on how MAGNEx performs with datasets known to have biases or lesser diversity 2. Given the generalization benefits of global explanations are there instances or scenarios where MAGNExs explanations could potentially mislead due to overly generalized rationale 3. A humancentric evaluation of the explanations interpretability could significantly strengthen the papers claims. Are there plans to incorporate such an evaluation  Review Summary MAGNEx represents a notable step forward in the field of explainable AI particularly addressing the critical challenges of modelagnosticism computational efficiency and mitigation of hindsight bias. By leveraging a neural network to learn importance scores globally it shows superior performance in generating faithful explanations across various tasks and models more efficiently than existing methods. However the paper could benefit from exploring the implications of its approach on human interpretability and under varied data conditions further. Moreover while fostering sparsity and generalization theres a potential exploration gap regarding scenarios requiring depth over simplicity in explanations. Overall the proposed method significantly contributes to explainable AI with avenues for enriched inquiry into humancentric evaluation and application to diverse realworld datasets.", "rFJWoYoxrDB": " Review of \"Empirical PostHoc Analysis of CellBased Neural Architecture Search Spaces\"  Summary This study embarks on an empirical posthoc examination of the architecture within prevalent cellbased search spaces in Neural Architecture Search (NAS). Despite the significance of cellbased searches in NAS theres a deficit in analytical efforts regarding their efficacy. Through the analysis of architectures generated by a diverse array of search strategies the authors uncover that a significant amount of the existing search spaces is marked by redundancy. They showcase that alterations in large sections of cells minimally impact architectural performance. Moreover prevalent design choices such as the search for reduction cells add complexity but do little to enhance performance. The study reveals that impactful areas within cells often adhere to identifiable and simplistic patterns. By strategically imposing constraints to incorporate these patterns even randomly sampled architectures could achieve or surpass current stateoftheart performance levels. This revelation raises questions about the potential of discovering genuinely innovative architectures within the existing cellbased search spaces and suggests directions for refining NAS research. The study eloquently integrates explainable machine learning tools in examining architectures and introduces \u201cOperation Importance\u201d as a novel metric to quantify the significance of individual components.  Strengths 1. Attention to Detail: The meticulous operationlevel and subgraphlevel analyses are commendable. By evaluating the importances of operations and identifying redundant elements through sophisticated statistical methods the paper offers a holistic understanding of NAS complexities. 2. Implications for NAS Research: The findings significantly contribute to the narrative on NAS optimization. By highlighting the redundancies within cellbased searches and underscoring simple yet effective architecture patterns this study paves the way for a reevaluation of current NAS paradigms. 3. Accessible Tools and Models: Providing the code repository is a beneficial asset for the reproduction of results and further investigation by the community. Moreover the surrogate model built on NB301 predictions as a basis for analysis is innovative.  Weaknesses 1. Generalizability of Results: While the findings are insightful the generalizability across other search spaces or tasks beyond image classification remains uncertain. As NAS applications diversify understanding the implications across broader contexts is crucial. 2. Potential Bias Towards Known Architectures: The emphasis on patterns similar to those in classical architectures (e.g. ResNetstyle residual links) could imply an inherent bias in search strategies. This aspect might limit the exploration of truly novel architectural designs. 3. Computationally Intensive Validation Processes: Despite attempting to validate key findings through actual training of selected architectures the extensive computational requirements might limit the scope for widespread replication and examination in resourceconstrained environments.  Questions and Feedback for the Authors 1. Extending the Analysis: Could the authors elaborate on potential methods or preliminary ideas on how this analysis could be expanded to other NAS paradigms such as those involving Transformers or alternative tasks beyond image classification 2. Addressing Bias: Given the findings primarily reinforce patterns observed in classical architectures what strategies do the authors suggest to mitigate potential biases towards these known patterns in future NAS research 3. Impact of Advanced NAS Techniques: How do the authors envisage advanced NAS techniques such as metalearning or reinforcement learningbased approaches fitting into the landscape shaped by their findings  Review Summary This paper makes substantial contributions to the field of Neural Architecture Search by critically examining the redundancies and impactful patterns within popular cellbased search spaces. Its thorough analysis leveraging novel metrics and explainable machine learning tools offers a fresh perspective on optimizing NAS practices. While it highlights crucial areas for improvement in NAS research questions about the generalizability of findings and potential biases towards known architectural patterns need addressing. Furthermore the implications of these insights on the development and application of advanced NAS techniques merit further discussion. Overall the study is a significant step forward in the continual refinement of NAS research advocating for a more focused and efficient exploration of the architectural search space.", "gICys3ITSmj": " Review Summary 1. Summarize the paper: The paper explores the interconnection between contrastive learning and metalearning particularly in the context of selfsupervised learning (SSL). The authors argue that the learning mechanisms inherent in contrastive learning closely resemble the task distribution and adaptation strategies used in metalearning especially when it comes to generating and utilizing augmented data. They demonstrate that when metalearning techniques specifically taskbased data augmentation are applied to contrastive learning algorithms such as SimCLR and noncontrastive methods like BYOL significant improvements in performance can be observed. Their experiments show BYOL achieving 94.6 accuracy on CIFAR10 matching or surpassing levels attained by fully supervised methods. The paper substantiates these findings through comprehensive experimental validation on CIFAR10 and ImageNet datasets and proposes a metalearning framework that facilitates the application of these metaspecific improvements to SSL. Additionally the paper makes its Pytorch implementation available providing a practical resource for further exploration and research in the field. 2. Strong and Weak Points Questions and Feedback: Strong Points:  Interdisciplinary Approach: The paper excellently integrates insights and methodologies from metalearning into contrastive and selfsupervised learning fostering a crossdisciplinary approach that benefits both fields.  Empirical Validation: The thorough experimental setup and detailed comparison across different datasets and models lend strong support to the paper\u2019s claims.  Practical Contributions: By providing a Pytorch implementation the paper contributes valuable tools to the research community for replicating and building upon their findings. Weak Points:  Hyperparameter Settings: The paper mentions using the same hyperparameters as SimCLR due to computational constraints. This could limit the optimization of the proposed methods specifically tailored for their metalearning framework.  Comparative Analysis: While the paper does compare its approach to existing methods a more detailed analysis concerning the fundamental differences in feature learning between metalearning and contrastive learning approaches could provide deeper insights.  Broader Applicability: The focus is predominantly on visual representation learning. Discussions on the applicability of the proposed framework in other domains or task types (e.g. text audio) could broaden the impact and relevance. Questions and Feedback:  How would the metalearning inspired data augmentation strategy perform on smaller less varied datasets than CIFAR10 or ImageNet  Could the authors elaborate on potential limitations or challenges when implementing their proposed framework in a realworld resourceconstrained scenario  Feedback: It would be beneficial to include a more detailed discussion on the transferability of the learned features across vastly different downstream tasks that go beyond visual classification to further validate the robustness and versatility of the approach. 3. Review Summary and Recommendation: The paper presents a compelling case for the convergence of metalearning and contrastive learning methodologies particularly within the realm of SSL. By adopting and adapting metalearning strategies the authors demonstrate notable improvements in model performance on benchmark datasets. This interdisciplinary approach not only validates the effectiveness of combining insights from different learning paradigms but also pushes the envelope in the understanding and application of SSL. However for a more comprehensive impact the paper could benefit from a deeper exploration into the optimal hyperparameter settings specific to their proposed metalearning framework and a broadened discussion on its applicability and potential limitations in various settings and across diverse data types. Overall the strengths of the paper significantly outweigh its limitations. Its innovative approach coupled with strong empirical evidence and practical contributions make it a valuable addition to the fields of metalearning and SSL. I recommend the paper for publication suggesting that the authors consider the feedback provided to further enhance the breadth and depth of their findings.", "tBIQEvApZK5": " Review of \"Feature Kernel Distillation for Efficient Neural Networks in MultiView Data Settings\"  Summary This paper introduces Feature Kernel Distillation (FKD) a novel method leveraging the concept of feature kernels in Neural Networks (NNs) for Knowledge Distillation (KD). The authors build upon the theory initially proposed by AllenZhu  Li (2020) emphasizing the crucial role of feature kernels\u2014representations of data attributes learnt during training\u2014in improving NN generalization within multiview data settings. FKD focuses on distilling knowledge by aligning the students feature kernel with that of the teacher(s) diverging from traditional KD approaches reliant on prediction space overlap between teacher and student models. Theoretical analysis affirms FKDs superiority over vanilla KD and other feature kernelbased methods by proving enhanced test performance in multiview data contexts. Empirically the paper demonstrates FKDs efficacy across various standard architectures and datasets showcasing its ability to transfer knowledge between different datasets and outperform existing KD techniques.  Strong Points 1. Theoretical Foundation: The extension of AllenZhu  Li (2020)s work to incorporate the feature kernels role in NN learning and distillation presents a significant theoretical contribution. The paper skillfully connects the dots between the initial parameter bias towards learning specific data attributes and its effects on generalization providing a wellrounded explanation bolstered by mathematical analysis. 2. Novel Approach: FKD introduces a fresh perspective on KD focusing on feature kernels comparison rather than prediction outputs or logits. This approach not only addresses the limitations of traditional KD (like the need for matching teacherstudent output spaces) but also opens up avenues for distillation across diverse architectures and tasks. 3. Experimental Validation: The comprehensive experiments conducted offer strong empirical support for the theoretical claims. FKDs ability to outperform traditional KD and other feature kernelbased methods across several datasets and architectures solidifies its validity and potential utility in practical applications.  Weak Points 1. Limited Theoretical Generalization: The theoretical analysis while robust is primarily focused on a specific CNN architecture and the multiview data setting. This narrow focus might limit the direct applicability of the findings to other architectures or data scenarios not covered in the paper. 2. Implementation Details: The paper could benefit from more detailed implementation instructions or considerations especially regarding the chosen (pseudo)distance measure \\(D\\) in the FKD loss function. Although pseudocode is provided a deeper discussion on the choice of \\(D\\) and its implications on performance could enhance reproducibility and understanding. 3. Analysis of Computational Overhead: While FKD shows improvement in generalization and adaptability the paper does not thoroughly address the computational costs associated with calculating the feature kernels and applying FKD especially in comparison to traditional KD or when dealing with large ensemble sizes.  Questions and Feedback to the Author 1. Could the authors elaborate on how the chosen pseudodistance measure \\(D\\) influences the distillation process and by extension the final performance Are there any recommendations on selecting \\(D\\) based on the specific architecture or task 2. The paper focuses on a specific CNN architecture for theoretical analysis. Can the authors discuss how the proposed FKD might be adapted or its potential effectiveness across other prevalent architectures such as Transformers 3. Insights into the computational overhead of FKD compared to standard KD practices would be valuable. Could the authors comment on the scalability of FKD particularly in scenarios involving large models or extensive ensemble teachers  Review Summary This paper makes a commendable theoretical and practical leap in the domain of Knowledge Distillation by proposing Feature Kernel Distillation (FKD) grounded in the novel idea of leveraging feature kernels for efficient knowledge transfer. The theoretical underpinnings provide a clear mechanistic understanding of how FKD enhances generalization in multiview data settings by aligning student models with the feature kernels of teachers thereby improving upon traditional KD methods. Empirically FKDs superiority is well demonstrated across multiple datasets and architectures showing its potential for broad application. However further clarification on the generalization of these methods beyond the tested scenarios more detailed implementation guidance and an examination of computational costs could enhance the papers impact. Overall this work presents significant advancements in the field of Knowledge Distillation suggesting FKD as a promising approach for efficient model training and knowledge transfer.", "pIjvdJ_QUYv": " Review of \"PrivHFL: A Practical PrivacyPreserving Framework for Heterogeneous Federated Learning\"  1. Summary of the Paper The paper introduces PrivHFL a framework designed to address privacy and dataset availability challenges in Heterogeneous Federated Learning (HFL). HFL allows clients with diverse computational capabilities to collaboratively train models by sharing model predictions rather than raw data. However two main issues arise: the unrealistic assumption of accessible auxiliary datasets for model knowledge transfer and the exposure to privacy violations. PrivHFL overcomes these by introducing a dataset expansion technique eliminating the need for auxiliary datasets and incorporating lightweight cryptographic protocols for privacy preservation. The framework demonstrates efficacy through significant improvements in computational efficiency and model accuracy over existing methods.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Innovativeness: The introduction of a dataset expansion method as a solution to the dependency on auxiliary datasets is notable. This method is not only innovative but also practical aligning well with realworld applications.  Practicality in Privacy Protection: The application of lightweight additive secret sharing for privacy preservation demonstrates a deep understanding of the privacy challenges in HFL. This approach significantly enhances the papers contribution to the field.  Comprehensive Evaluation: The extensive experiments across different datasets and model architectures in both IID and NonIID settings provide a solid validation for the effectiveness and efficiency of PrivHFL.  Weak Points:  Scalability Discussion Lacks Depth: While the paper briefly mentions challenges related to scalability it would benefit from a more detailed examination of how PrivHFL can be optimized or adjusted for larger scale deployments.  Limited Discussion on Differential Privacy (DP) Integration: The paper touches upon the potential integration of DP mechanisms but lacks a detailed exploration or experimental results to understand the impact on privacyutility tradeoffs within PrivHFL.  Questions and Feedback:  Data Expansion Method Specifics: Can more details be provided on how the dataset expansion method compares with traditional data augmentation techniques in terms of model performance and privacy preservation  Future Directions on Scalability: Are there specific strategies that the authors are considering to address the scalability of PrivHFL particularly concerning the number of clients and model sizes  Integration with Differential Privacy: How do the authors envision the integration of differential privacy mechanisms with PrivHFL Would this integration necessitate significant structural changes to the framework  3. Review Summary and Recommendation The paper presents a significant advancement in addressing the challenges of privacy preservation and the reliance on auxiliary datasets within Heterogeneous Federated Learning (HFL). Through the innovative dataset expansion method and the incorporation of lightweight cryptographic protocols PrivHFL demonstrates remarkable improvements in efficiency and accuracy. The comprehensive evaluations reinforce the framework\u2019s practicality and effectiveness. However the paper could benefit from a more indepth analysis of its scalability and the detailed mechanism of integrating differential privacy to bolster its contributions further. Addressing these areas would not only enhance the papers comprehensiveness but also its applicability in broader contexts. Given the strengths of the paper and the potential impact of its contributions I recommend accepting this paper. Nonetheless I encourage the authors to consider the feedback provided especially in exploring scalability and the integration of differential privacy mechanisms to strengthen the papers relevance and applicability in realworld scenarios.", "w_drCosT76": "Summarize the paper: The paper introduces a novel approach termed Differentiable Scaffolding Tree (DST) for optimizing molecular structures. Traditional methods of molecular optimization have struggled with the discrete nature of chemical structures by either relying heavily on bruteforce methods or failing to model the discrete structures efficiently. DST addresses these issues by defining a differentiable representation of the molecular structure that enables gradientbased optimization through backpropagation. This representation is achieved utilizing a Graph Neural Network (GNN) to interpret the molecular graphs properties continuously. The study demonstrates that this method not only improves efficiency in identifying molecules with desired properties but also requires fewer oracle calls compared to existing methods. Moreover DST provides an interpretability aspect by correlating modifications in the molecular structure with changes in its properties. The empirical validation is conducted across various computational objective functions showcasing DSTs effectiveness and efficiency. Strong and Weak Points Questions and Feedback: Strong Points: 1. Novelty of the Approach: The introduction of DST as a means to transition from discrete to continuous modeling of molecular graphs is innovative. It allows for gradientbased optimization a leap forward in the field of molecular design and optimization. 2. Efficiency and Effectiveness: The empirical results showing DSTs ability to optimize molecular structures more efficiently and effectively with fewer oracle calls is a significant advantage. This could greatly expedite the process of drug discovery and other materials science applications. 3. Interpretability: Providing explanations for the models output by correlating specific structural changes with modifications in molecular properties is highly beneficial for domain experts. This feature enhances trust and understanding of the DST model. Weak Points: 1. Dependency on Oracle GNN: While reducing the number of oracle calls is a strong point the methods success is heavily contingent upon the performance of the Oracle GNN which models the molecules properties. The performance of DST is thus limited by how well this GNN is trained and how accurately it can simulate the oracle functions. 2. Scalability Concerns: The paper does not thoroughly discuss the scalability of DST especially as it pertains to very large molecular graphs and complex molecular properties. There could be concerns about the computational efficiency and memory requirements in these scenarios. 3. Generalization to Diverse Chemical Spaces: While DST demonstrates compelling results within the scope of the experiments its unclear how well DST generalizes to highly diverse or novel chemical spaces that are significantly different from the training datasets. Questions: 1. How does DST handle very large or complex molecular graphs Are there limitations in terms of graph size or complexity 2. Can DST be adapted for use in optimizing properties of polymers or biomaterials which might have different constraints or requirements than small molecules 3. How sensitive is DST to the initial structure of the molecule Is there a risk of local optima and how does DST mitigate such issues Feedback: The paper presents a promising methodology for molecular optimization by introducing differentiation at the substructure level. It would be beneficial to expand on the scalability and generalization capabilities of DST to bolster its applicability across different fields of chemistry. Additionally providing more insight into the limitations and potential workarounds for dealing with large or complex graphs could help in understanding the methods constraints. Finally exploring the adaptation of DST for broader classes of materials could significantly amplify its impact. Review Summary: The paper introduces a pioneering approach to optimize molecular structures using Differentiable Scaffolding Trees (DST) allowing for gradientbased optimization in what has traditionally been a challenge due to the discrete nature of chemical structures. This innovation not only promises to expedite the discovery and optimization processes in chemical sciences but also provides a means to reduce the dependency on oracle calls and adds a layer of interpretability to the modifications suggested by the model. While the method exhibits substantial improvements in efficiency and efficacy questions regarding the scalability generalization across diverse chemical spaces and dependency on the performance of the Oracle GNN remain. Addressing these concerns and providing deeper insight into DSTs adaptability to more complex scenarios could enhance the robustness and applicability of the proposed method. Thus the paper contributes a significant advancement in the field of molecular optimization with its novel approach despite some areas that could be further elaborated for a comprehensive understanding of DSTs potentials and limitations.", "nZOUYEN6Wvy": " Summarize the Paper This paper introduces GrIDNet a novel framework that extends the utility of Granger causality to dynamical systems represented as directed acyclic graphs (DAGs) focusing on its application to singlecell multimodal data for uncovering gene regulatory interactions. Leveraging graph neural networks with lagged message passing GrIDNet addresses gaps in existing methods by accounting for temporal lags between chromatin accessibility and gene expression. The paper demonstrates GrIDNet\u2019s superior performance in inferring regulatory links between genomic loci and genes within singlecell multimodal assays showing significant advancements over existing techniques through validation against independent genetic datasets.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: GrIDNet effectively addresses the limitations of traditional Granger causality enabling its application to DAGstructured dynamical systems. This opens up new avenues for causal analysis in complex biological processes like cell differentiation. 2. Significant Performance Improvements: By demonstrating up to 71 greater agreement with independent geneticsbased estimates compared to existing methods GrIDNet showcases a substantial advancement in identifying regulatory locus\u2013gene links. 3. Comprehensive Evaluation: The authors rigorously evaluate GrIDNet across multiple singlecell multimodal datasets benchmarking against existing techniques and validating predictions against eQTL and HiC data thereby showcasing the method\u2019s robustness and applicability. Weak Points: 1. Complexity and Accessibility: Given the sophisticated integration of graph neural networks the method might be challenging for practitioners outside the computational biology domain to adopt without substantial effort or further simplifications. 2. Potential for Overfitting: While the paper outlines impressive results there remains a concern about the potential for overfitting given the models complexity. Clarifying how the model is regularized or whether dropout techniques were employed could be beneficial. 3. Biological Validation Lacking: Despite strong computational validations the paper does not present empirical biological validation of the predicted regulatory relationships which would strengthen the evidence for GrIDNets utility in practical research settings. Questions and Feedback: 1. Scalability Concerns: Could the authors elaborate on how GrIDNet scales with increasingly large singlecell datasets Specifically what optimizations or future enhancements are considered to manage larger graphs 2. Interpretability of Results: How does GrIDNet provide insights into the biological significance of the inferred causal relationships Can the model offer explanations or features that illuminate the biological mechanisms underlying these relationships 3. Adaptability to Other Domains: Beyond the specific application to singlecell multimodal data how adaptable is GrIDNet to other domains or types of DAGstructured data Are there constraints or modifications needed for such extensions  Review Summary GrIDNet represents a significant step forward in applying Granger causality to complex dynamical systems represented by DAGs particularly in the context of singlecell multimodal data analysis. Its ability to infer regulatory links with higher accuracy than current methods addresses a crucial need in genome biology and offers a pathway towards understanding gene regulation at an unparalleled scale and resolution. The rigorous benchmarking against existing methods and validation with independent genetic data underscores the methods potential and reliability. However the complexity of the method poses challenges for broader adoption. A focus on improving accessibility alongside empirical biological validation of the inferred relationships could increase GrIDNets impact. Further exploration into the models scalability and interpretability would also enhance its utility and applicability across different domains. Overall GrIDNets contribution is noteworthy providing novel capabilities that can significantly advance causal inference in systems biology and beyond. Its further development and application promise to reveal insights into the dynamics of cellular differentiation and disease progression making it a valuable tool for the scientific community.", "l431c_2eGO2": " Peer Review of \"MixMaxEnt: Regularization of Neural Networks for Improved Uncertainty and Accuracy\"  1) Paper Summary The paper introduces MixMaxEnt a novel regularization technique for enhancing the performance and reliability of neural networkbased classifiers. This method essentially integrates an entropy maximization regularizer with the standard crossentropy loss. The regularizer operates on synthetic samples crafted by mixing pairs of images from different classes with the aim of increasing model uncertainty (and thus reducing overconfidence) in regions of the data manifold that are not wellrepresented in the training set. The authors provide comprehensive experiments on CIFAR10 and CIFAR100 datasets using ResNet and WideResNet architectures showing that MixMaxEnt outperforms existing approaches in terms of classification accuracy calibration and uncertainty estimation particularly in scenarios involving domain shifts and outofdistribution samples.  2) Strong and Weak Points Questions and Feedback to the Author Strong Points:  Innovative Approach: The simplicity and efficiency of the MixMaxEnt method which doesn\u2019t require modifications to the network architecture or tradeoffs in accuracy for improved uncertainty estimates is commendable.  Extensive Experimental Validation: The breadth of experiments including comparisons with both deterministic and Bayesian methods and the consideration of various realworld scenarios (like domain shifts) provides a solid validation for the method\u2019s effectiveness.  Practicality: The method\u2019s ease of implementation and the fact that it does not significantly add to the model\u2019s computational burden makes it highly practical for realworld applications. Weak Points:  Lack of Theoretical Underpinning: While the empirical results are strong the paper lacks a deeper theoretical discussion on why mixing features from different classes leads to improved uncertainty estimation and why the entropy maximization works effectively in the synthetic sample space.  Generalization Concerns: The paper focuses on image classification tasks with specific datasets and architectures. It leaves open the question of how well MixMaxEnt generalizes across different types of data tasks or more complex models.  Analysis of Failure Modes: There is little discussion on potential limitations or scenarios where MixMaxEnt might not perform as expected. Questions and Feedback:  Could the authors elaborate on any theoretical basis for why the convex combination of images from different classes should lead to an entropy barrier that accurately reflects regions of high uncertainty  It would be beneficial if the authors could provide some insights or early results on how MixMaxEnt performs on datasets from other domains (e.g. text or audio) or on tasks beyond classification (e.g. regression).  An analysis of the computational overhead introduced by the synthetic sample generation and entropy calculation would be helpful especially for very large datasets or more complex models.  Finally a discussion on potential limitations or scenarios where MixMaxEnt might not be as effective would make the conclusions stronger and guide future research directions.  3) Review Summary and Recommendation The paper \"MixMaxEnt: Regularization of Neural Networks for Improved Uncertainty and Accuracy\" presents a novel and practical approach to addressing a significant challenge in neural networkbased classification: the model\u2019s overconfidence. The method\u2019s simplicity combined with its impressive performance improvements on both classification accuracy and uncertainty estimation marks a significant contribution to the field. Furthermore the extensive experimental validation provides strong evidence for the method\u2019s effectiveness. However the paper could be strengthened by addressing theoretical underpinnings more thoroughly discussing the method\u2019s generalizability across domains and tasks and acknowledging potential limitations or failure modes. Addressing these points would provide a more rounded perspective and potentially increase the impact of the work. Despite these minor points for improvement the novel approach combined with empirical results makes this paper a valuable contribution to the research community. I recommend acceptance with a suggestion for the authors to consider the feedback provided to enhance the paper\u2019s breadth and depth.", "ziRLU3Y2PN_": " 1) Summarize the paper This paper introduces a novel waveletbased model for texture synthesis capable of generating natural textures with complex geometric structures. It bridges the gap between classical waveletbased approaches and stateoftheart CNNbased models. The methodology is rooted in leveraging nonlinear waveletbased representations specifically through the use of a generalized rectifier nonlinearity effectively constituting a simplistic onelayer CNN. This approach results in significant improvements over classical waveletbased texture synthesis approaches and achieves comparable results with contemporary models. The paper thoroughly examines memorization effects and the tradeoff between the quality and diversity of synthesized textures providing insights into the potential for decreasing the number of statistics without sacrificing visual quality.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper presents a unique blend between classical techniques and modern CNNbased approaches which is both innovative and effective. It expands the toolset available for texture synthesis by integrating wavelet transforms and phase harmonics making an important contribution to the field.  Improved Visual Quality: By integrating a generalized rectifier nonlinearity with wavelet transformations the model achieves substantial improvements in visual quality over classical wavelet models and competes well with stateoftheart models.  Comprehensive Experimental Evaluation: The paper includes a detailed experimental evaluation comparing the proposed model with various stateoftheart techniques across different texture complexity levels showcasing its versatility and efficiency. Weak Points:  Complexity and Interpretability: The transition from wavelet to more complex nonlinear statistics might introduce additional complexity and reduce the interpretability of the model which is not fully addressed in the discussion.  Model Overfitting and Memorization: While the paper discusses memorization effects it could benefit from a deeper exploration of strategies to mitigate overfitting especially in the context of very complex textures or nonstationary images.  Limited Exploration of Iterative Refinement: The paper mentions using the LBFGS optimization algorithm with restarts but does not explore the potential of iterative refinement techniques in depth which could further enhance the quality and diversity of the generated textures. Questions and Feedback:  Can the authors provide more insights on the tradeoff between the number of statistics included in the model and the resulting synthesis quality Specifically how does this tradeoff impact different types of textures (e.g. natural vs. artificial textures)  Is there potential for integrating learningbased approaches with the proposed model to dynamically adjust the number of statistics or model parameters based on the input texture  The paper would benefit from a more detailed discussion on the limitations and potential future improvements of the model especially regarding its application to nonstationary images and very complex textures.  3) Review Summary and Recommendation The paper represents a significant advancement in texture synthesis effectively merging classical waveletbased approaches with modern CNN methodologies through a novel waveletbased model. The introduction of a generalized rectifier nonlinearity allows for capturing complex geometric structures within textures presenting a competitive alternative to current stateoftheart models. The comprehensive experimental evaluation demonstrates the models effectiveness across a variety of textures. However the models complexity potential for overfitting and the limited exploration of iterative refinement techniques are areas that require further attention. Questions regarding the scalability of the approach to different types of textures and the possibility of integrating learningbased enhancements also remain. Given the innovative approach significant improvements in visual quality and thorough evaluation presented I recommend accepting this paper with minor revisions. Specifically the authors should aim to address the mentioned weak points and consider incorporating the feedback and answers to the posed questions to strengthen the papers contributions further. The integration of waveletbased and CNNmethodologies presents a promising direction for future research in texture synthesis making this work a valuable addition to the field.", "lvM693mon8q": " Peer Review for \"Compressed Vertical Federated Learning\"  1. Summary: The paper introduces a new framework called Compressed Vertical Federated Learning (CVFL) aimed at enhancing communication efficiency in federated learning setups involving vertically partitioned data. In vertical federated learning different parties possess data on the same entities but with unique nonoverlapping features. CVFL addresses the challenge of excessive communication costs by allowing parties to share compressed embeddings of data thus significantly reducing the amount of information that needs to be transmitted without greatly impacting model accuracy. The authors provide a theoretical foundation for the convergence of CVFL showing that with bounded compression error the training converges to a fixed point at a defined rate. They further specify bounds for convergence given common compression techniques such as scalar quantization and topk sparsification. Through empirical experiments using LSTMs on the MIMICIII dataset and CNNs on the ModelNet10 dataset the authors demonstrate that CVFL can achieve over 90 reduction in communication cost while maintaining model accuracy.  2. Strong and Weak Points Questions and Feedback: Strong Points:  Originality and Relevance: The paper addresses an important and timely problem in federated learning. The introduction of compression in vertical federated learning setups is innovative and addresses a significant bottleneck in distributed machine learning scenarios.  Theoretical Foundation: Providing a theoretical analysis for the convergence of CVFL adds substantial value. By proving convergence with nonconvex objectives and specifying conditions for convergence with common compression techniques the paper offers robustness to the proposed framework.  Empirical Validation: The experimental results using realworld datasets effectively demonstrate the practical benefits of the proposed method especially in terms of communication cost without significantly sacrificing accuracy. Weak Points:  Assumption Limitations: The bounded gradient assumption and the requirement for specific conditions for convergence might limit the applicability of the proposed method across all possible use cases. A discussion on the implications of these assumptions and possible extensions to relax these conditions would enhance the paper.  Comparative Analysis: While the experiments show that CVFL is effective compared to uncompressed VFL a comparison with other stateoftheart federated learning approaches that aim to reduce communication costs would offer a more comprehensive understanding of its relative performance.  Implementation Details and Reproducibility: Though the paper mentions compression techniques and parameters more detailed implementation insights or open source code would aid in the reproducibility of the results and foster further research in this area. Questions and Feedback:  Could the authors elaborate on the impact of different compression techniques on model convergence speed It would be insightful to understand how varying compressors influence the training dynamics beyond just the final model accuracy and communication cost.  The privacy considerations section briefly mentions potential attacks and defenses. Could the authors provide a deeper analysis of the privacy implications of embedding compression and the robustness of CVFL against specific inversion or reconstruction attacks  For future work the authors mention exploring adaptive compressors. Could they provide preliminary thoughts on how adaptive compression could be incorporated into CVFL and what benefits they foresee  3. Review Summary: This paper presents a novel and significant contribution to the field of federated learning by introducing a method to drastically reduce communication costs in vertically partitioned data scenarios without significantly impacting model accuracy. The theoretical analysis grounding the proposed CVFL framework and the empirical validation provide a strong case for the effectiveness and applicability of this method. However the paper could benefit from addressing the limitations related to the bounded gradient assumption providing a deeper comparative analysis with other techniques and offering more detailed implementation details for better reproducibility. Additionally expanding on the privacy implications of embedding compression would enhance the papers contribution to the field. Overall this paper offers a substantial contribution to federated learning presenting a promising direction for communicationefficient training over vertically partitioned data.", "iLHOIDsPv1P": " Summarize the Paper The paper presents a novel theoretical framework to address the longstanding issue of understanding the generalization ability of neural networks (NNs) in machine learning. It introduces a new information bottleneck the PACBayes Information Bottleneck (PIB) underpinned by the PACBayes theorem to guarantee theoretical generalization. The core idea revolves around the information stored in weights (IIW) of NNs proposing a method to efficiently approximate IIW and subsequently leveraging it to analyze and enhance the generalization capabilities of NNs. Through the PIB the authors were able to identify the transition from fitting to compression phase in NN training across various scenarios including different architectures noisy labels and batch sizes. Moreover they proposed a Markov Chain Monte Carlo (MCMC)based algorithm for sampling from the optimal weight posterior specified by PIB demonstrating its applicability in practical deep learning setups. Empirical experiments validate the approach showing it can explain NN behavior broadly and improve inference and training efficiency.  Strong and Weak Points of the Paper Strong Points: 1. Novel Theoretical Framework: The introduction of the PACBayes Information Bottleneck (PIB) and its application to understanding NN generalization is innovative and addresses a critical gap in machine learning theory. 2. Comprehensive Experimentation: The authors have conducted a wide range of experiments across various NN architectures activation functions and training conditions (e.g. noise in labels batch sizes) demonstrating the robustness and broad applicability of their method. 3. Practical Algorithm for IIW Approximation: Proposing an efficient algorithm to approximate the otherwise intractable Information Stored in Weights (IIW) is a significant contribution making it feasible to implement PIB in realworld applications. 4. Evidence of Improved Generalization: The empirical results convincingly show that optimizing NNs under the PIB framework can significantly enhance their generalization performance even in challenging conditions like noisy labels and overparameterization. Weak Points: 1. Assumption of Gaussian Distributions: The derivation of IIW and the subsequent algorithms assume Gaussian distributions for simplicity. This assumption may limit the applicability of the proposed methods in cases where the weight distributions significantly deviate from Gaussian. 2. Complexity of Implementation: While the paper presents novel theoretical insights and algorithms the practical complexity of implementing these methods especially the MCMCbased sampling for optimal posterior inference may limit their accessibility for a broader audience. 3. Lack of Comparison with StateoftheArt: The paper could benefit from a more detailed comparison with existing methods that aim to explain or improve NN generalization. Including baselines or recent approaches could provide clearer evidence of the superiority or novelty of the proposed method. Question and Feedback to the Author: Could you provide more insight into how the PIB framework might perform or adapt in nonGaussian weight distribution scenarios given the reliance on Gaussian assumptions in deriving IIW approximations Expanding on this could help understand the limitations and potential adjustments needed to apply PIB more broadly.  Review Summary The presented paper makes a significant theoretical and practical contribution to the field of machine learning by proposing the PACBayes Information Bottleneck (PIB) as a novel framework to understand and improve the generalization ability of neural networks. The authors successfully address a fundamental challenge by providing an efficient approximation for Information Stored in Weights (IIW) and demonstrating the utility of PIB in various practical NN training scenarios. The comprehensive experimentation across different settings underlines the robustness of their approach. However the reliance on Gaussian assumptions for IIW and the complexity of implementation especially regarding the MCMCbased algorithm might limit the frameworks applicability. Future work could aim to relax these assumptions and simplify implementation to broaden PIBs accessibility and impact. Overall this paper contributes valuable insights and tools for advancing our understanding of NN generalization paving the way for more principled and effective deep learning practices.", "morSrUyWG26": " Peer Review of \"AutoOED: Automated Optimal Experimental Design Platform Powered by Machine Learning\"  1) Summary of the Paper This paper introduces AutoOED an opensource platform designed for the optimal experimental design (OED) of multiobjective problems with a limited budget for experiments. Leveraging machine learning techniques specifically multiobjective Bayesian optimization (MOBO) AutoOED seeks to efficiently identify Pareto optimal solutions by executing data and timeefficient experiments. Notably the platform incorporates a novel asynchronous optimization strategy dubbed BelieverPenalizer (BP) to enhance the speed of batch optimizations without compromising performance. AutoOED distinguishes itself with a modular Python codebase an intuitive graphical user interface (GUI) for ease of use by nonexperts and compatibility with realworld hardware experiments for fully automated operation. The paper compares AutoOEDs features and performance with existing Python libraries demonstrating its robustness and effectiveness through benchmark problems and a realworld case study optimizing a PID heater controller.  2) Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Solution: AutoOED thoroughly addresses the need for an accessible efficient and flexible platform for multiobjective optimization making noteworthy contributions in data and time efficiency modularity and userfriendliness.  Innovative Asynchronous Optimization: The BP strategy is a significant advancement smartly integrating beliefs on certain predictions and penalizing uncertain ones to effectively manage the asynchronous optimization challenge.  Practical Utility: Demonstrations of realworld applications and an easytouse GUI significantly lower the barriers to applying complex MOBO methodologies in various fields potentially broadening the adoption of optimization techniques in experimental designs.  Benchmarking and Comparative Analysis: The extensive experimental section provides a clear and detailed comparison with existing methods and libraries showcasing AutoOED\u2019s competitive edge. Weak Points:  Limited Discussion on Scalability: While the paper presents AutoOED\u2019s effectiveness through benchmarks and a realworld experiment there is minimal discussion about the platforms scalability and performance in extremely largescale multiobjective optimization problems.  Lack of User Case Studies: Although the platform promises ease of use for nonexperts the paper does not include case studies or testimonials from this target user group which could strengthen the claim of userfriendliness.  Potential for Broader Algorithm Inclusion: While AutoOED supports a range of MOBO algorithms there seems to be room for incorporating more diverse or stateoftheart algorithms into the platform\u2019s modular framework. Questions and Feedback:  Scalability and Performance: Can the authors elaborate on AutoOED\u2019s scalability and its performance in handling very large or complex multiobjective problems compared to its counterparts  User Experience Insight: Have there been user studies or feedback sessions conducted with nonexpert users to validate the GUI\u2019s intuitiveness and overall user experience  Future Algorithm Support: Are there plans to periodically update AutoOED\u2019s library of algorithms to include emerging optimization techniques that may increase its effectiveness and applicability  3) Review Summary and Recommendation This paper successfully presents a promising opensource platform AutoOED designed to address the complex and crucial task of optimal experimental design in multiobjective optimization scenarios. With its novel BP strategy for asynchronous optimization a userfriendly GUI and a modular approach allowing for customizable experimentation AutoOED stands out as an accessible and powerful tool for both researchers and practitioners. The platforms capability to facilitate efficient design of experiments and its demonstration through realworld applications significantly contribute to lowering the barriers facing nonexperts interested in utilizing machine learning for optimization tasks. However to strengthen the paper further it would be beneficial to include more discussions on the scalability of AutoOED particularly in the face of extremely challenging optimization problems and to present case studies or feedback from nonexpert users to underline the platform\u2019s usability. Overall given AutoOED\u2019s demonstrated effectiveness flexibility and its potential to democratize the use of advanced MOBO techniques I recommend this paper for publication. Further enhancements addressing the discussed weak points and questions could make subsequent versions even more compelling and informative.", "sEIl_stzQyB": " Peer Review of \"GreedyBased Value Representation for Optimal Coordination in MARL\"  Summary The paper addresses the challenge of optimal coordination in fully cooperative multiagent reinforcement learning (MARL) due to the relative overgeneralization problem inherent in linear and monotonic value decomposition methods. The authors propose \"optimal consistency\" as a criterion for evaluating coordination optimality and introduce the TrueGlobalMax (TGM) condition to ensure optimal stable points are unique tackling the root cause of nonoptimal coordination. They develop the GreedyBased Value Representation (GVR) method incorporating inferior target shaping (ITS) and superior experience replay (SER) theoretically proven to ensure optimal consistency under sufficient exploration. The efficacy of GVR is demonstrated through various benchmarks showing significant performance improvements over stateoftheart baselines.  Strong Points 1. Novel Criterion and Condition: The introduction of \"optimal consistency\" and the TGM condition offers a fresh perspective on achieving optimal coordination in MARL addressing a longstanding issue in value decomposition approaches. 2. Theoretical Underpinning: The paper provides robust theoretical backing for the proposed GVR method including the expressions for the joint Q value function and stability conditions under ITS and SER enhancing the credibility of their approach. 3. Empirical Validation: Comprehensive experiments across different benchmarks (matrix games predatorprey tasks and SMAC) robustly validate the performance superiority of GVR over existing methods showcasing its practical applicability.  Weak Points 1. Complexity and Generality: The GVR method while effective appears to be relatively complex potentially limiting its accessibility and applicability to a broader range of MARL scenarios. The paper could benefit from a discussion on the generalizability and scalability of GVR. 2. Comparison with Baselines: While the paper shows GVR outperforms stateoftheart baselines a more detailed comparison showcasing the specific scenarios where GVR excels or struggles compared to each baseline could provide deeper insights. 3. Exploration Challenge: The mention that GVR may not ensure the TGM condition in hard exploration tasks points to a potential limitation. Further elucidation on the conditions or settings where GVRs performance might be hindered would be informative.  Questions and Feedback to the Author 1. Scalability and Complexity: Can the authors elaborate on the computational complexity of GVR and discuss its scalability to larger more complex MARL environments 2. Baseline Comparisons: Could additional insights be provided on the comparative analysis with baselines especially in scenarios where GVRs performance is close to that of other methods 3. Handling Hard Exploration: The paper notes a limitation of GVR in hard exploration tasks. Could the authors discuss potential strategies or adaptations to GVR that might overcome this challenge  Review Summary This paper makes significant contributions to addressing the challenge of optimal coordination in MARL by introducing a novel criterion optimal consistency and the associated TGM condition underpinning the GreedyBased Value Representation (GVR) method. GVRs theoretical foundation and empirical validation through extensive benchmarks underscore its effectiveness in ensuring optimal coordination beyond the capabilities of current stateoftheart methods. However the paper would benefit from deeper discussions on the complexity and generalizability of GVR detailed comparative analysis with baseline methods and strategies to address its limitations in hard exploration scenarios. Despite these suggestions for further clarification and exploration the papers contributions are noteworthy presenting a meaningful advancement in the pursuit of optimal coordination in MARL environments. The theoretical grounding combined with robust empirical results strongly supports the potential of GVR in advancing the field warranting a favorable recommendation for publication.", "saNgDizIODl": " Review of the Paper: A Fast and Scalable Method for Quantifying Uncertainty in Neural Network Predictions  Summary The paper addresses a critical challenge in deploying neural networks (NNs) in highstakes areas: the need for models to estimate the uncertainty of their predictions reliably. The authors introduce a novel method for uncertainty quantification (UQ) in deterministic NN models that distinguish between aleatoric and epistemic uncertainties without requiring changes to the network architecture or training procedure. This method operationalizes the NadarayaWatson nonparametric estimate within the NNs embedding space providing a scalable solution tested on various realworld image datasets. The papers contributions are fourfold: a rigorous uncertainty definition an estimate of this uncertainty via kernel density estimation implementation in the NNs embedding space and demonstration of scalability and effectiveness in experiments. The work culminates in the proposed Nonparametric Uncertainty Quantification (NUQ) method which shows superior performance in misclassification and outofdistribution (OOD) detection tasks across several datasets.  Strong and Weak Points  Strong Points 1. Originality and Relevance: The paper tackles an unresolved issue in machine learning with an innovative approach. The ability to quantify uncertainty without significant modifications to existing models is practically very valuable especially in critical applications like healthcare and autonomous driving. 2. Rigorous Theoretical Foundation: The method is grounded in established statistical theory providing a solid basis for the proposed UQ method. The distinction between aleatoric and epistemic uncertainties is particularly noteworthy. 3. Extensive Experiments: The authors comprehensively evaluate the proposed method across multiple datasets and against several benchmarks. This extensive testing not only validates the approach but also demonstrates its scalability and robustness. 4. Clarity and Reproducibility: The paper is clearly written with a detailed methodology and experiments section that allows for reproducibility. The decision to make the code available is commendable and aligns with the principles of open science.  Weak Points 1. Lack of Comparative Analysis with Nonimage Data: While the method is evaluated across multiple image datasets its performance on nonimage data remains unclear. Given the broad applicability of neural networks understanding how NUQ performs on textual or tabular data could enhance the papers impact. 2. Complexity and Computational Overhead: While the paper addresses scalability there is limited discussion on the computational complexity and runtime especially in comparison with baseline methods. Providing benchmarks on computational overhead could strengthen the argument for NUQs practicality. 3. Potential for Bias in Kernel Choice: The choice of kernel and bandwidth significantly influences the kernel density estimation. The paper could benefit from a more detailed discussion on how different choices affect the results and guidelines for making these choices in practice.  Questions and Feedback to the Author 1. Generalizability to Nonimage Data: Can the authors comment on the applicability of NUQ to nonimage datasets such as text or tabular data Are there any limitations or adjustments needed 2. Computational Costs: Could the authors provide more details on the computational costs associated with NUQ particularly in terms of runtime and resource usage 3. Kernel and Bandwidth Selection Process: How sensitive is NUQ to the choice of kernel and bandwidth Could the paper provide best practices or strategies for these choices  Review Summary The paper presents a significant contribution to the field of machine learning by providing a novel scalable method for quantifying the uncertainty of predictions made by deterministic neural network models. The approachs solid theoretical foundation extensive experimentation and clear presentation are its strongest assets marking a step forward in making NN models more reliable and trustworthy for highstakes applications. However elaboration on the methods applicability to nonimage data a detailed comparison of computational costs and guidance on kernel and bandwidth selection could further strengthen the paper. Overall I recommend acceptance provided the authors address the aforementioned points to ensure the methods broader applicability and practical implementation.", "qNcedShvOs4": "Review of \"EinSteinVI: A Composable Library for Stein Variational Inference in Probabilistic Programming Languages\" 1. Summary This paper introduces EinSteinVI an innovative library designed to enhance Stein Variational Inference (Stein VI) by integrating it into the probabilistic programming language (PPL) NumPyro. The paper outlines the challenges associated with Bayesian inference particularly the difficulty in computing the posterior distribution due to the intractability of the normalization constant. It highlights the advantages of Variational Inference (VI) methods in addressing these challenges with a special focus on Stein VI for its scalability and flexibility. EinSteinVI is presented as a solution that not only integrates the latest advancements in Stein VI into NumPyro but also introduces a novel algorithm ELBOwithinStein for efficient inference with custom guide programs. The library supports a wide range of kernels nonlinear scaling of repulsion forces and secondorder gradient updates showcasing its applicability through examples like Bayesian neural networks and Steinmixture deep Markov models. 2. Strong and Weak Points Questions and Feedback Strong Points:  Novel Algorithm: The introduction of ELBOwithinStein is a significant contribution simplifying the implementation of Stein mixtures while maintaining efficiency and accuracy.  Comprehensive Integration: EinSteinVI\u2019s seamless integration with NumPyro and support for various advanced methods in Stein VI makes it a valuable tool for researchers and practitioners in the field of Bayesian inference and machine learning.  Practical Demonstrations: The application of EinSteinVI to realworld problems including Bayesian neural networks and deep Markov models effectively demonstrates the librarys capabilities and potential benefits over existing methods. Weak Points:  Performance Benchmarking: While the paper presents comparative results a more comprehensive benchmark against a broader spectrum of existing techniques including computational performance metrics would strengthen the paper.  Theoretical Insight: A deeper theoretical discussion on the limitations and assumptions underlying the proposed ELBOwithinStein algorithm and its impact on inference accuracy would be beneficial.  User Experience: Insights into the usability of the EinSteinVI library including potential learning curves and integration challenges with existing pipelines are somewhat lacking. Questions and Feedback:  Can the authors clarify the tradeoffs involved in choosing the ELBOwithinStein algorithm over traditional methods particularly in terms of computational complexity and accuracy  How does the performance of EinSteinVI scale with the complexity and dimensionality of the models Are there any limitations in terms of model size or structure  Feedback: Incorporating user testimonials or case studies might provide valuable insights into the practical benefits and challenges faced by users of EinSteinVI further validating the librarys applicability in realworld settings. 3. Review Summary The presented paper makes a notable contribution to the field of Bayesian inference by proposing EinSteinVI a comprehensive library that enhances the integration of Stein variational inference methods into probabilistic programming languages. The introduction of the ELBOwithinStein algorithm is particularly noteworthy promising simplified implementation and improved performance in Stein mixture models. However the paper could benefit from a more detailed exploration of the algorithms theoretical underpinnings along with expanded benchmarking against existing methods to firmly establish its advantages. Additionally insights into the librarys user experience could provide a more rounded view of its practical impact. Overall EinSteinVI represents a significant advancement in making sophisticated Stein VI methods more accessible to researchers and practitioners with clear potential for broad applications in machine learning and beyond. Recommendations for publication are strong contingent upon addressing the highlighted concerns to further strengthen the papers contributions.", "ngjR4Gw9oAp": " Review of \"Soft ActorCritic with Inhibitory Networks for Efficient Retraining in Reinforcement Learning\"  1) Summary This paper addresses the problem of efficiently retraining deep reinforcement learning agents in scenarios where new constraints or goals emerge potentially conflicting with previously learned skills. The authors introduce the Soft ActorCritic with Inhibitory Networks (SACI) approach inspired by neuroscientific research on inhibitory control. SACI employs separate competing value functions (critics) that adapt according to the current task demands enabling agents to balance the exploitation of previously learned skills with the exploration of new ones more effectively. The approach includes novel elements such as episodic memory through selective replay buffers and automated dual entropy estimation. The effectiveness of SACI is demonstrated through experiments in modified OpenAI Gym environments specifically the LunarLanderContinuousv2 with a bomb scenario and a mixed version of BipedalWalkerHardcorev3 showcasing its superior performance in retraining over the standard Soft ActorCritic (SAC) method.  2) Strengths and Weaknesses Strengths:  Innovation: The introduction of inhibitory networks represents a significant innovation offering a novel way to address the retraining challenge in reinforcement learning. This approach is thoughtfully inspired by neuroscience providing an intriguing bridge between biological insights and computational methods.  Comprehensive Experiments: The authors provide detailed experiments demonstrating the advantages of SACI over standard SAC in scenarios requiring the simultaneous exploitation of known skills and exploration of new ones. Their use of modified OpenAI Gym environments for this purpose is commendable.  Clear Presentation: The paper is wellorganized with a clear exposition of the technical aspects of SACI and includes illustrative figures that aid in understanding the proposed approach and its benefits. Weaknesses:  Generalization and Scalability: While the demonstrated experiments are convincing within their scope the paper does not address the generalization of SACI to a broader range of tasks or its scalability to more complex environments.  Comparison with Related Work: Although the paper situates SACI within the context of existing work it lacks a deeper comparative analysis with closely related approaches particularly those employing different strategies for balancing exploitation and exploration during retraining.  Implementation Details: Some implementation details are scarce such as the specific configuration of the neural networks used and the exact conditions under which episodic memories are selected for replay. More explicit details would aid in reproducibility. Questions and Feedback:  How does SACI scale with the complexity of the environment especially in tasks with a much larger state space or more nuanced objectives  Could the authors provide more insight into the inhibitory policy network training process particularly how it decides between competing Q networks  It would be beneficial if the paper discussed potential limitations or challenges in applying SACI to realworld scenarios beyond the simulated environments.  3) Review Summary The paper presents a significant contribution to the field of reinforcement learning introducing the SACI approach to efficiently retrain agents in the face of newly emerging potentially conflicting goals. The innovative use of inhibitory networks inspired by neuroscience is a highlight of this work offering a compelling method to manage the exploreexploit tradeoff more effectively during retraining. The experimental results convincingly demonstrate the advantages of SACI over the standard SAC method in tailored environments. However the paper could benefit from a more detailed comparison with related approaches a deeper discussion of potential limitations and a clearer presentation of implementation specifics to enhance reproducibility and understand the approachs applicability to more complex or realworld tasks. Overall the paper is wellwritten and presents an interesting and novel contribution that could have a significant impact on the field particularly in applications where agents must frequently adapt to new tasks or constraints.", "tQ2yZj4sCnk": " Review Summary The paper introduces DMAC a novel divergenceregularized multiagent actorcritic framework for cooperative multiagent reinforcement learning (MARL) addressing the conventional issues of policy deviation caused by entropy regularization in MARL environments. The authors adeptly blend theoretical derivations with empirical evaluations presenting a comprehensive study that leads to the derivation of an offpolicy update rule guaranteeing monotonic policy improvement without bias from the regularization.  Paper Summary The paper presents a detailed exploration of applying divergence regularization to cooperative MARL to circumvent the limitations of entropy regularization. The authors propose DMAC an offpolicy cooperative MARL framework designed around the concept of divergence regularization. Theoretical advancements including divergence policy iteration and the derivation of an update rule for critics policy and target policy are laid out ensuring monotonic policy improvement and a lack of bias due to regularization. The experimental evaluation of DMAC involves a stochastic game and the StarCraft MultiAgent Challenge (SMAC) where DMAC is combined with existing MARL algorithms like COMA MAAC QMIX DOP and FOP. The results demonstrate substantial improvement over these algorithms in terms of performance stability and convergence speed substantiating the advantages of divergence regularization over entropy regularization in MARL settings.  Strong Points 1. Theoretical Contributions: The paper contributes significantly to the theoretical underpinnings of MARL by providing a rigorous foundation for the application of divergence regularization ensuring that the optimal policy converges without bias. 2. Empirical Validation: The extensive empirical evaluation provides strong evidence of the frameworks effectiveness across different cooperative MARL settings particularly with the improvements seen in adapted versions of COMA MAAC QMIX DOP and FOP when integrated with DMAC. 3. Addressing its Contribution in Context: The authors thoroughly discuss the papers contributions concerning existing work clearly identifying the novelty and significance of their approach in the broader MARL research landscape.  Weak Points 1. Limited Discussion on Computational Complexity: While the paper excels in theoretical explanation and empirical evaluation it lacks a detailed discussion on the computational implications of implementing DMAC which could be crucial for realworld applications. 2. Generalizability Concerns: The paper mainly focuses on cooperative MARL settings. Additional insights into how DMAC could be adapted or extended to competitive or mixed cooperativecompetitive environments would enhance its appeal and applicability. 3. Comparison with Baselines: Although the paper compares DMAC with stateoftheart MARL algorithms a deeper analysis contrasting it directly with baseline divergence regularization approaches in singleagent settings could highlight the benefits and any potential tradeoffs more clearly.  Questions and Feedback 1. Could the authors elaborate on the computational complexity of implementing DMAC especially in comparison to the MARL algorithms it seeks to improve 2. How does DMAC perform in competitive or mixed environments Are there additional tweaks required to apply DMAC in these settings effectively 3. Would it be possible to expand on the direct comparisons between DMAC and singleagent divergence regularization approaches to further clarify the improvements DMAC offers in the multiagent context  Review Summary The paper by the authors introduces a compelling approach to enhancing cooperative MARL algorithms through divergence regularization. The theoretical rigor and comprehensive empirical evaluations significantly argue for DMACs effectiveness. However addressing the computational complexity and exploring the frameworks adaptability to broader MARL environments could provide a more holistic view of DMACs practical applicability. Despite these minor weaknesses the papers contributions to the MARL field are substantial elucidating a promising direction for future research. This work certainly merits publication with the recommendation that the authors consider expanding on the computational aspects and generalizability of DMAC in future iterations.", "lf0W6tcWmh-": " Review of \"Understanding the Generalization Benefit of Momentum in Deep Learning\"  Summary The paper investigates the impact of adding momentum to the stochastic gradient descent (SGD) algorithm on generalization in deep learning. While previous works have focused on the optimization advantages of momentum this study shifts the focus towards understanding how momentum contributes to generalization improvements. The authors provide empirical evidence and theoretical analysis demonstrating that gradient descent with momentum (GDM) outperforms vanilla gradient descent (GD) in terms of generalization across several deep learning tasks. Specifically they construct a binary classification problem involving an overparameterized convolutional neural network (CNN) where it is proven that GDM leads to superior generalization compared to GD. The mechanism proposed is that momentum facilitates learning from examples with small margins by leveraging historical gradients a process termed \"historical feature amplification.\" This mechanism is further supported by empirical validations on modified CIFAR datasets where GDM exhibited a clear generalization advantage particularly on data with small margin characteristics.  Strengths 1. Comprehensive Treatment: This paper comprehensively addresses the phenomenon of momentumenhanced generalization through both theoretical and empirical lenses. It fills a significant gap in the literature by providing a theoretical framework that explains observed empirical benefits. 2. Novel Insight: The concept of \"historical feature amplification\" as the underlying mechanism for momentums generalization benefit is novel and insightful. It provides a plausible explanation that aligns with empirical observations. 3. Practical Implications: The empirical validation using CIFAR datasets demonstrates the practical relevance of the theoretical findings showing that the insights gained can translate to realworld settings. 4. Clarity: The paper is wellorganized and concepts are clearly presented. The division between theoretical analysis and empirical justification aids understanding.  Weaknesses 1. Assumptions and Generalizability: The theoretical framework relies on an overparameterized binary classification setup with specific data distribution characteristics. It is unclear how the findings generalize to other types of neural networks tasks or data distributions beyond the ones studied. 2. Empirical Validation Scope: While the empirical experiments support the theoretical claims they are limited to variations of the CIFAR dataset. Additional experiments on completely different datasets and tasks would strengthen the argument. 3. Comparison with Other Regularization Techniques: The paper does not provide a comparison of the generalization benefit of momentum with other regularization or optimization techniques. Such comparisons could contextualize the effectiveness of momentum relative to other methods.  Questions and Feedback 1. Generalizability: How do the authors envision extending the theoretical analysis to encompass a broader variety of tasks network architectures or data distributions 2. Momentum Parameter Sensitivity: The paper could benefit from a sensitivity analysis of the momentum parameter (\u03b3). How sensitive are the generalization benefits to variations in \u03b3 Are there diminishing returns or practical limits 3. Comparison with Other Methods: Can the authors provide insights or speculate on how momentums generalization benefit compares with other known regularization or optimization strategies such as dropout batch normalization or weight decay  Review Summary This paper makes a significant contribution by advancing the understanding of how the inclusion of momentum in SGD affects the generalization in deep learning. The introduction of \"historical feature amplification\" as a mechanism offers a compelling explanation for the observed benefits of momentum bridging a gap in the literature. However the studys limitations regarding the generalizability of its theoretical findings and the scope of its empirical validations suggest areas for further research. Despite these concerns the paper presents valuable insights into the role of momentum in deep learning supported by both theoretical analysis and empirical evidence. It opens new avenues for investigating the interplay between optimization techniques and generalization contributing to the broader understanding of deep learning training dynamics.", "famc03Gg231": " Review of \"Integrating HighOrder Optimization Methods with Machine Learning Techniques for Inverse Problem Solving in Physical Systems\"  Summary The paper presents a novel approach for solving inverse problems in physical systems by integrating higherorder optimization techniques with traditional machine learning (ML) methods. It identifies that current stateoftheart training techniques in machine learning are not always suitable for problems involving physical processes due to the variability in gradient magnitude and direction. To address this the authors propose a hybrid training approach where updates derived from higherorder or domainspecific optimizers referred to as physical gradients (PGs) are used in place of regular gradients during the training of deep learning models. This method not only allows embedding domain knowledge into the training process but also demonstrates substantial improvements in optimization and learning problems involving physical systems. The paper shows how this approach outperforms existing training schemes through various experiments involving canonical physical systems like Poissons equation the heat equation and the NavierStokes equations.  Strengths and Weaknesses Strengths: 1. Innovative Approach: The integration of higherorder optimization techniques with ML models is a significant advancement potentially opening new avenues for solving complex inverse problems in computational science and engineering. 2. Improved Performance: The experiments demonstrate that the physical gradients yield improvements by orders of magnitude over traditional training schemes highlighting the efficacy of the proposed method. 3. Applicability: The method is versatile showing effectiveness across different systems and equations which suggests its potential applicability to a wide range of inverse problems beyond those tested. Weaknesses: 1. Computational Complexity: While the paper mentions that the computational cost of the PG method is on par with regular gradient evaluation it does not provide detailed comparative analysis on computational resources and time which is crucial for practical applications. 2. Limited Exploration of MultiModality: The paper briefly mentions challenges associated with multimodal inverse problems but does not delve deeply into how the proposed method performs in such scenarios which are common in physical systems. 3. Lack of Broader Validation: Though the method shows promise in the experiments conducted the paper could benefit from additional validation across more diverse physical systems and with realworld data to fully assess its generalizability and limitations. Questions and Feedback for the Author: 1. Could you provide more insights into the computational cost and resource requirements of the PG method compared to traditional training techniques 2. How does the proposed method perform in the context of multimodal solution spaces that are typical of many physical inverse problems 3. Are there any specific conditions or scenarios where the proposed method might not perform as expected Additional discussion on potential limitations would be valuable. 4. Have you considered extending the validation of the method with realworld data or more complex systems to further establish its efficacy and applicability  Review Summary The proposed hybrid training approach represents a significant development in solving inverse problems involving physical processes by efficiently combining the strengths of higherorder optimization and machine learning. The demonstration of substantial accuracy improvements and the methods applicability to a range of systems underscore its potential impact. However more indepth analysis on computational efficiency behavior in multimodal solution spaces and broader validation across diverse systems and realworld data would strengthen the papers findings and conclusions. Overall the methodology introduces an important tool for computational science presenting a promising direction for future research and application in solving complex physical inverse problems.", "rbFPSQHlllm": " Review of \"Automated MultiObjective Mixer for Medical ImageBased Diagnosis using MLPMixer Architecture\"  1. Summary The paper introduces an innovative deep learning model the Automated MultiObjective Mixer (AutoMOMixer) tailored for medical imagebased diagnosis. Leveraging the MLPMixer architecture which uses multilayer perceptrons instead of convolutional layers the proposed model reduces the number of parameters significantly thus facilitating training on smaller datasets. Distinguishing itself from traditional approaches the AutoMOMixer considers both sensitivity and specificity as objective functions within its training phase aiming for a balanced model capable of minimizing false negatives and positives. This approach is achieved through a Paretooptimal model set selection guided by multiobjective optimization. Hyperparameter tuning is addressed via Bayesian optimization and an evidence reasoning (ER) approach is employed to fuse outputs from selected models increasing the reliability of the diagnosis. The models effectiveness is demonstrated through extensive experiments on two public medical datasets concerning brain tumors and COVID19 showing superior performance against standard MLPMixer and CNN models in terms of sensitivity specificity AUC and accuracy.  2. Evaluation Strong Points:  The innovative integration of sensitivity and specificity as simultaneous objective functions via Pareto optimization is commendable. It addresses a vital clinical need for balanced diagnostic models.  Employing Bayesian optimization for hyperparameter training enhances the models performance representing a thoughtful approach to dealing with the complexity of hyperparameters.  The fusion of Paretooptimal model outputs using the ER approach is a creative solution for ensuring robust and reliable diagnostic results.  Comprehensive experimentation on public datasets with detailed performance metrics effectively demonstrates the model\u2019s superiority in realworld applicability. Weak Points:  The paper does not sufficiently discuss the limitations or potential biases within the datasets used for experiments nor does it fully explore the impact of dataset diversity on model performance.  While the paper compares the proposed model with CNN and Mixer architectures the inclusion of comparisons with other stateoftheart medical imaging diagnosis models would have enriched the validity of the claims.  The computational complexity and runtime of the AutoMOMixer particularly concerning the training phase are not discussed. This information is crucial for practical applications. Questions and Feedback to the Author:  Could the authors provide insights into the models performance with datasets of varying sizes and compositions to better understand its scalability and adaptability  Is there an evaluation of the computational resources required for training and inference phases of the AutoMOMixer compared to traditional models  How might the model be adapted or extended to support multiclass classification tasks prevalent in medical diagnosis beyond binary outcomes (e.g. identifying multiple types of brain tumors)  3. Review Summary The paper presents a promising approach to medical imagebased diagnosis addressing key challenges in balance and reliability through an innovative use of MLPMixer architecture and multiobjective optimization. The employment of sensitivity specificity and Pareto optimality in model training is particularly noteworthy offering a nuanced approach to diagnosis that is critically lacking in many existing models. However for a thorough appraisal and to substantiate the models utility further exploration into the models performance under varied dataset conditions its computational demands and potential extensions for multiclass scenarios is necessary. Despite these areas for expansion the paper significantly contributes to the field of medical image diagnosis through deep learning with the AutoMOMixer showing considerable promise in enhancing diagnostic accuracy and reliability. Given the presented strengths and acknowledging the areas for further detail and clarification the recommendation is for acceptance with minor revisions focusing on addressing the posed questions and expanding on the discussion regarding the models scalability computational efficiency and versatility across diagnostic tasks.", "irARV_2VFs4": " Review of the Paper on \"A New Algorithm for Training with Group Annotations: Improving Minority Group Performance\"  1. Summary This paper introduces a novel algorithm Common Gradient Descent (CGD) aimed at addressing the limitations of existing training methods such as Empirical Risk Minimization (ERM) and Group Distributionally Robust Optimization (GroupDRO) in contexts where training data are stratified into groups with varied distributions. The primary concern with ERM and GroupDRO is their suboptimal performance on minority groups attributed to an overfitting on majority groups or focusing on groups with the highest losses potentially ignoring valuable intergroup interactions. CGD seeks to mitigate these issues by focusing on training groups that contribute to an overall reduction in average training loss across all groups hence encouraging the learning of shared characteristics among groups and improving performance on minority groups. The authors provide both theoretical and empirical evidence demonstrating CGDs superiority or competitive performance against benchmarks like ERM and GroupDRO across various datasets.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The CGD method offers an original solution to the problem of suboptimal performance on minority groups by leveraging insights from domain generalization and focusing on common gradients that benefit all groups.  Comprehensive Evaluation: The paper stands out by providing both theoretical proofs of CGDs convergence to firstorder stationary points and extensive empirical evaluation across seven realworld datasets showcasing its robustness and effectiveness.  Practical Relevance: By addressing the issue of performance disparity across groups CGD contributes to the development of fairer machine learning models which is highly relevant in many realworld applications. Weak Points:  Assumption of Group Annotations: The methodology relies on the availability of group annotations during training which might not always be available or could be noisy in realworld scenarios.  Analysis of Computational Complexity: The paper lacks an analysis of the computational complexity and scalability of CGD compared to its counterparts which is crucial for practical implementations.  Limited Discussion on Failure Modes: While the paper mentions that CGD outperforms or matches the performance of other methods a detailed discussion on potential failure modes or scenarios where CGD might underperform is absent. Questions and Feedback:  Robustness to Noisy Annotations: How robust is CGD to inaccuracies or noise in group annotations Some insight or empirical evidence on this aspect would enhance the papers contributions.  Comparison with Domain Adaptation Techniques: Could CGD be effectively compared with or complement domain adaptation techniques especially in scenarios with significant distribution shifts  Impact on Model Complexity: Does the implementation of CGD lead to any significant changes in model complexity or training time particularly when compared to GroupDRO or ERM  3. Review Summary This papers contribution Common Gradient Descent (CGD) represents a significant advancement in training models with groupannotated data focusing particularly on improving performance on minority groups. CGDs novel approach of leveraging insights from domain generalization and focusing training on groups that minimize overall loss is both inventive and empirically validated to outperform current methodologies in diverse contexts. Despite its strengths the reliance on group annotations absence of computational complexity analysis and discussion on potential limitations or failure modes are areas that could be improved upon. These aspects do not detract from the originality and value of the work but represent opportunities to bolster the findings and applicability of CGD in broader contexts. In conclusion the introduction of CGD addresses a crucial need for fair and robust model training methods. Its demonstrated effectiveness across various settings underscores its potential warranting further exploration and adoption in efforts to develop equitable machine learning systems. The paper is a commendable contribution to the field and the authors should strive to refine and expand their work addressing the indicated weaknesses and feedback.", "yKIAXjkJc2F": " Review of \"Invariant Imbedding Networks (InImNets) for ContinuousDepth Neural Networks\"  Summary The paper introduces a new class of Deep Neural Networks (DNNs) termed as Invariant Imbedding Networks (InImNets) which innovatively address the longstanding issue of interpreting the \"depth\" in DNNs specifically in the context of continuousdepth neural networks such as Neural Ordinary Differential Equations (ODEs). By leveraging the concept of imbedding networks of varying depth while keeping inputs constant the authors provide a novel insight into understanding DNNs as dynamical systems. They assert that their approach translates a nonlinear vectorvalued optimal control problem into a series of forwardfacing initial value problems supported by rigorous mathematical formulations and proofs. Through experimental evaluations including on highdimensional rotating MNIST and bouncing balls datasets the paper claims that InImNets offer competitive performance to existing stateoftheart models with increased computational efficiency. The work presents a promising avenue for explainable AI and the theoretical understanding of neural network depth via the explication of invariant imbedding principles.  Strong Points 1. Conceptual Innovation: The introduction of InImNets which redefine the depth of a network as a fundamental variable is a novel conceptual leap enhancing the understanding of continuousdepth DNNs. This provides a fresh perspective on the architecture of neural networks. 2. Theoretical Rigor: The paper is solid in its mathematical foundation clearly detailing the underpinnings of invariant imbedding methods and their application to solving nonlinear control problems within DNNs. Theorems and proofs presented are thorough. 3. Experimental Validation: The authors successfully validate their claims with compelling experimental results. Benchmarks on tasks like rotating MNIST and dynamics predictions like bouncing balls illustrate potential improvements in computational efficiency without sacrificing performance. 4. Open Source Codes: Making the accompanying code available on GitHub is highly beneficial for replicability and further research fostering open science principles.  Weak Points 1. Complexity for Practical Implementation: The mathematical rigor and intricacies of the proposed method while a strength might also pose a significant barrier to understanding and practical implementation by a wider segment of the machine learning community. 2. Limited Exploration of Practical Limitations: The paper could benefit from a deeper discussion on the limitations and potential downsides of InImNet architectures particularly in more diverse or noisy realworld applications beyond the datasets tested. 3. Scalability Concerns: Although the paper shows promising results on specific tasks it falls short of thoroughly addressing scalability issues especially when dealing with very largescale problems or datasets.  Questions and Feedback to the Author 1. Robustness and Generalization: How do InImNets perform in scenarios with significant data noise or where datasets have a higher degree of variability An analysis of robustness and generalization would add valuable insight. 2. Comparison with Traditional Methods: Could the authors elaborate on the comparative advantages of InImNets over conventional Neural ODE approaches and other continuousdepth models in terms of computational resources and training time 3. Potential for Scalability: How scalable are InImNets when applied to problems of larger magnitude or higher complexity beyond the experiments shown especially in terms of computational efficiency and memory usage  Review Summary The paper presents a highly innovative approach to understanding and implementing continuousdepth neural networks through the development of InImNets. The theoretical insights provided are commendable and represent a significant contribution to the field of machine learning research especially in explicating the role of \"depth\" in DNNs. The experimental results lend credibility to the claim of competitive performance and increased computational efficiency marking a notable step forward in the practical application of neural ODEs and related architectures. Despite its strengths the work could be enhanced by addressing questions around practical implementation barriers robustness to variability in datasets and scalability. Given these considerations I recommend this paper for publication provided the authors can clarify these aspects possibly through minor revisions to solidify the relevance and applicability of their work to a broader range of realworld problems and datasets.", "sWbXSWzHPa": " Review of \"Optimizing for the Worstoff Group through Partially Labeled Data: A Worstoff DRO Approach\"  Summary: This paper tackles the significant challenge in machine learning of learning invariant representations specifically aimed at reducing the bias neural networks might have due to spurious correlations present in datasets. This bias often manifests as disparities in performance across different groups particularly affecting minority groups adversely. The central question the paper addresses is how to train models that are invariant to group membership when only partially labeled data is available. The authors propose a novel framework based on distributionally robust optimization (DRO) which they refer to as Worstoff DRO. This framework is designed to optimize for the worstoff group assignments from a constrained set that is more likely to contain the true group memberships especially where labels are partially missing. Experiments conducted on both image and tabular datasets demonstrate the proposed methods efficacy in improving the performance of minority groups while maintaining overall accuracy.  Strong Points: 1. Novel Approach: The paper presents a unique perspective on handling partially labeled datasets for mitigating bias in machine learning models. The Worstoff DRO framework is a thoughtful adaptation of distributionally robust optimization to the context of partial label availability. 2. Theoretical Rigor: The authors provide a robust theoretical foundation for their approach including a high probability bound for group assignments and reducing the constraint set with a marginal distribution constraint. These contributions are both innovative and crucial for the method\u2019s success. 3. Empirical Validation: The paper offers comprehensive experimental results including comparisons with several baseline methods across different datasets. This extensive evaluation demonstrates the practical value of the proposed approach. 4. Relevance to Current Issues: Addressing bias and fairness in machine learning is of paramount importance. This work contributes significantly to this area by offering a method to improve the accuracy for minority groups which is a critical concern in deploying equitable and fair ML systems.  Weak Points: 1. Assumption of Label Missingness: The method assumes that group labels are missing completely at random (MCAR). This assumption might not always hold in realworld scenarios impacting the method\u2019s applicability. 2. Complexity of Implementation: While the paper provides a detailed explanation of the Worstoff DRO framework the complexity of the proposed algorithm and the need for marginal distribution information might pose challenges for implementation. 3. Analysis of Failure Modes: Although the paper presents extensive experimental results there is a lack of discussion on potential failure modes or limitations of the approach particularly in datasets where the MCAR assumption may not apply or where marginal distributions are hard to estimate.  Questions and Feedback for the Author: 1. Handling NonMCAR Data: Could the authors elaborate on how the Worstoff DRO approach could be adapted or extended to situations where group labels are not missing completely at random (MCAR) but follow other missingness patterns 2. Scalability Concerns: How does the Worstoff DRO approach scale with the size of the dataset and the dimensionality of the data Could high complexity hinder its applicability to largescale realworld problems 3. Robustness to Incorrect Marginal Distributions: The method relies on estimates of marginal distributions. How sensitive is the Worstoff DRO to inaccuracies in these estimates and are there strategies to mitigate potential impacts  Review Summary: The paper contributes a notable approach to addressing performance disparities across groups in machine learning models particularly when faced with partially labeled data. The Worstoff DRO framework is both theoretically sound and empirically effective as demonstrated through rigorous experiments. However the assumption of data missingness completely at random (MCAR) and the complexity of the proposed solution might limit its application in some realworld scenarios. Further exploration into the methods scalability its robustness to inaccuracies in marginal distribution estimates and adaptations for nonMCAR data could enhance its utility. Given the strong theoretical foundation experimental validation and relevance to critical issues in fairness and bias in machine learning this paper makes a significant contribution to the field. The proposed method presents a promising avenue for improving the performance of machine learning models on minority groups an area of great importance for building equitable and fair AI systems.", "xw04RdwI2kS": " Summarize the paper The paper addresses the challenge of understanding how decisionmaking specifically in healthcare for organ allocations evolves over time. It introduces the concept of Inverse Contextual Bandits (ICB) to model and analyze the nonstationary behavior of decisionmakers. Unlike traditional policy learning approaches that assume stationary decisionmaking ICB captures the dynamic evolution of decisionmakers knowledge and preferences. The paper outlines the development of two algorithms for ICB that provide interpretable and accurate representations of evolving decisionmaking policies using both parametric (Bayesian updates) and nonparametric methods. Applying these algorithms to realworld and simulated liver transplantation data the authors demonstrate the utility of ICB in interpreting and evaluating the changes in medical decisionmaking over time.  Strong and weak points of the paper Strong Points: 1. Novelty: The paper addresses a critical and previously underexplored problem of understanding how decisionmaking evolves over time especially in the context of healthcare and organ allocation policies. 2. Methodological Rigor: The authors introduce two concrete algorithms for ICB providing a methodological framework for capturing nonstationary decisionmaking processes through contextual bandits. The Bayesian approach for belief updates and the nonparametric method for modeling belief trajectories are both rigorously defined and implemented. 3. Applicability and Explainability: The application of these algorithms to realworld and simulated liver transplantation data not only showcases their practicality but also emphasizes their potential in yielding interpretable insights into decisionmaking policies over time. 4. Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed algorithms against several benchmarks and across various types of learning agents demonstrating their superiority in terms of belief accuracy and reward accuracy. Weak Points: 1. Assumptions: Some of the assumptions such as the linear combination of features for reward computation and the Gaussian distribution of rewards may limit the applicability of the proposed methods in more complex or nonlinear decision environments. 2. Generalization: While the focus on contextual bandits captures a wide range of applications extending the ICB framework to environments with arbitrary transition dynamics (T) remains an open question for future research. 3. Interpretability Criteria: Although the paper discusses interpretability extensively it could benefit from a more detailed framework or set of criteria to assess interpretability across different domains or applications beyond the specific case of organ allocations. Question and Feedback to the author: 1. Could the authors discuss the limitations of the assumptions regarding reward distributions and linearity in more depth and potentially explore alternative models that could address these limitations 2. In extending the applicability of ICB to environments with arbitrary transition dynamics what are the initial challenges the authors foresee and how might they approach solving these challenges 3. It would be insightful if the authors could elucidate their perspective on developing a broader set of interpretability criteria applicable across various domains to assess and compare the efficacy of policy learning methods in capturing nonstationary decisionmaking behavior.  Review Summary The paper presents a compelling and novel approach to understanding the evolution of decisionmaking in healthcare specifically in the context of organ allocation policies. By introducing the Inverse Contextual Bandits framework and developing both parametric and nonparametric algorithms the authors make significant contributions to the fields of computational and behavioral science with practical implications for policy evaluation and design in healthcare. The methodology is rigorously defined and the application to realworld and simulated data demonstrates both the applicability and explainability of the proposed methods. However the paper does have limitations particularly regarding the assumptions of linearity and reward distributions which could limit the general applicability of the methods. Furthermore while the paper makes strides toward explainability it could benefit from a broader discussion or development of interpretability criteria applicable across different domains. Overall the paper makes a substantial contribution to understanding nonstationary decisionmaking processes. Future work addressing the outlined limitations and questions could further enhance the generalizability and applicability of the ICB framework potentially extending its use beyond healthcare to other domains where understanding the evolution of decisionmaking is crucial.", "jJJWwrMrEsx": "Review of \"Truth Table Deep Convolutional Neural Networks for Formal Verification and eXplainability\" 1) Summary: This paper introduces Truth Table Deep Convolutional Neural Networks (TTDCNN) a novel architecture for Deep Convolutional Neural Networks (DCNN) that is SATencodable allowing for formal verification enhanced interpretability and human postprocessing integration. Unlike Binary Neural Networks (BNNs) which were previously the only type capable of being translated into SAT formulas but suffered from accuracy limitations due to binary constraints TTDCNNs incorporate realvalued weights and intermediate values. This breakthrough addresses several limitations of BNNs such as natural accuracy constraints interpretability and scalability issues related to the vast number of variables and clauses in the SAT conversion process. TTDCNNs are distinguished by their ability to extract all logical classification rules offering a better tradeoff between accuracy and formal verification capabilities improving verification time and reducing the complexity of SAT formulas. The paper demonstrates the architectures efficacy through rigorous experiments highlighting its potential in advancing the fields of explainable AI (XAI) and formal verification. 2) Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Approach: The introduction of TTDCNN as a SATencodable model with realvalued weights and intermediate values is novel and addresses critical limitations of previous models namely the interpretability scalability and natural accuracy of BNNs.  Enhanced Interpretability and Formal Verification: The methodological contributions particularly the interpretability through global rule extraction and the tractability of precomputing all possible model inputsoutputs are significant advancements for both XAI and formal verification fields.  Rigorous Experimental Validation: The experimentation includes comprehensive comparisons to existing stateoftheart models showcasing TTDCNNs superior tradeoff between natural accuracy and formal verification along with detailed analyses on robustness verification settings. Weak Points:  Complexity and Scalability Concerns: Despite improvements the preprocessing steps required for SAT encoding especially for multilayer and multifilter configurations might still pose challenges for scalability to very large or complex models.  Limited Exploration of Interpretability Features: While the paper significantly advances interpretability the exploration of how these features can be applied in practical AI systems for enhancing decisionmaking processes or user interaction remains limited.  Experimental Scope: The experiments are largely focused on MNIST and CIFAR10 datasets within specific noise configurations. Expanding the scope to more diverse datasets or realworld applications could further validate the models versatility and robustness. Questions and Feedback: 1. Scalability: How does TTDCNN performance and SAT conversion complexity scale with increasing network depth and dataset complexity beyond MNIST and CIFAR10 2. Application to Other Domains: The current focus is primarily on image classification tasks. Could you elaborate on potential applications in other domains such as natural language processing or video analysis 3. Interpretability Insights: Could the authors provide more insights or examples on how the extracted logic classification rules have been or could be utilized for practical AI system enhancements or humanAI interaction improvements 4. Future Directions: What are the anticipated challenges and future directions for integrating TTDCNNs into realworld applications particularly considering continuous inputs and the need for dynamic posttuning capabilities 3) Review Summary: The paper presents a groundbreaking approach to enhancing the explainability and formal verification capabilities of Deep Convolutional Neural Networks through the introduction of Truth Table Deep Convolutional Neural Networks (TTDCNN). The work significantly advances the stateoftheart by addressing key limitations associated with Binary Neural Networks particularly concerning interpretability and natural accuracy. The authors contributions in developing a SATencodable model with realvalued weights and a highly interpretable conversion method offer promising avenues for research and application in XAI and formal verification fields. Nonetheless further inquiries into the scalability of the proposed method its applicability across different domains beyond the tested datasets and the practical utilization of the interpretability features are recommended for future work. Expanding the experimental validation to include diverse datasets and providing deeper insights into the practical implications of the models interpretability and posttuning capabilities would strengthen the papers contributions. Overall this paper represents an important step forward in the intersection of neural network interpretability formal verification and SAT encoding. Its innovations not only offer theoretical advancements but also lay down a foundation for practical applications in making AI systems more transparent understandable and verifiable.", "s5lIqsrOu3Z": " Review of \"A Computational Framework for Learning Discriminative and Generative Representation for MultiClass MultiDimensional Data\"  1. Summary The paper presents a novel computational framework aimed at establishing a more efficient and effective method of learning representations for complex multiclass multidimensional realworld data. The method bridges the benefits of AutoEncoding (AE) and Generative Adversarial Networks (GAN) through a unique approach that frames the encoding and decoding process as a twoplayer minimax game where the equilibrium point serves as the optimal solution for encoding and decoding mappings. This setup leverages rate reduction as a utility function an informationtheoretic measure to dictate the dynamics between the encoder and decoder thus avoiding the computational challenge of directly minimizing arbitrary distribution distances in data or feature space. This framework is specifically designed to learn a Linear Discriminative Representation (LDR) that effectively maps data distributions onto linear subspaces facilitating both generative and discriminative utilities. The paper thoroughly demonstrates through extensive experiments on benchmark datasets how this model outperforms or competes closely with existing models based on GAN VAE or their combinations in terms of visual quality of generated data and classification performance.  2. Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Innovation: The approach of utilizing a minimax game to find the equilibrium point for optimal encoding and decoding mappings is innovative. It blends the discriminative power of AE with the generative capabilities of GAN within a unified framework.  Extended Applicability: The formulation\u2019s ability to handle complex multiclass multidimensional data is exceptionally beneficial considering the vast and varied nature of realworld data.  Empirical Evidence: The comprehensive set of experiments and the comparison with stateoftheart models provide strong empirical support for the proposed frameworks effectiveness. Weak Points:  Accessibility and Complexity: The theoretical concepts while innovative are quite complex and might be challenging for a broader audience to grasp fully.  Implementation Details: While the experimental results are impressive the paper could provide more insights into the implementation details making it easier for others to reproduce the results.  Discussion on Limitations: The paper could benefit from a more detailed discussion on potential limitations and how they might impact the application of the framework in realworld scenarios. Questions and Feedback: 1. Could the authors elaborate on how the proposed method scales with the dimensionality and complexity of data in computational terms 2. It would be beneficial if the paper could discuss potential strategies for overcoming any identified limitations especially in terms of computational requirements. 3. Feedback: Incorporating case studies or examples of realworld applications could significantly enhance the understanding and perceived utility of the proposed framework.  3. Review Summary and Recommendation The paper introduces a compelling and theoretically innovative framework that successfully integrates the benefits of AE and GAN models for learning representations of complex multiclass multidimensional data. The proposed minimax game approach and the use of rate reduction as a utility function are particularly noteworthy providing a fresh perspective on tackling the encodingdecoding challenge in data representation learning. The extensive empirical validations add significant credibility to the proposed methods effectiveness and competitiveness with existing models. However the paper could benefit from addressing the accessibility of its theoretical concepts providing more detailed implementation insights and discussing the frameworks potential limitations in greater depth. Addressing these points could make the paper more comprehensive and accessible to a wider audience including practitioners looking to apply these concepts in realworld scenarios. Recommendation: Considering the strengths outweigh the identified weaknesses which can be addressed with minor revisions I recommend this paper for acceptance. Its theoretical contributions and empirical validations are valuable additions to the literature on generative and discriminative representation learning.", "jFfRcKVut98": "Summary: The paper introduces Partial Group equivariant Convolutional Neural Networks (Partial GCNNs) a novel approach in the architecture of equivariant networks that enables the learning of partial and full equivariance directly from data at each layer of the network. By dynamically adjusting equivariance levels during training Partial GCNNs can switch between full and partial equivariance based on the data improving upon traditional GCNNs in tasks where full equivariance is not necessary or even detrimental. The approach is validated through both illustrative toy tasks and benchmark datasets showing that Partial GCNNs perform comparably to traditional GCNNs in tasks requiring full equivariance and outperform them in tasks that do not. Strong Points: 1. Novel Approach: The concept of learning partial equivariance to group transformations at each layer of a CNN is novel and highly relevant for tasks with varying degrees of symmetry. This flexibility allows for more efficient representations and could lead to advances in various applications. 2. Comprehensive Evaluation: The paper presents a thorough evaluation of Partial GCNNs across a range of tasks convincingly demonstrating their benefits over traditional GCNNs where full equivariance is not ideal. The use of both toy tasks and established benchmark datasets strengthens the validity of the claims. 3. Theoretical Foundation: The paper provides a solid theoretical framework for understanding partial group equivariance including detailed formulations and proofs. This theoretical clarity enhances the papers contribution to the field. Weak Points: 1. Complex Implementation Details: The papers complexity particularly around the implementation details of partial convolution and the Monte Carlo approximation might be challenging for readers not already familiar with group equivariant neural networks or those specifics. 2. Lack of Broader Application Exploration: While the paper convincingly demonstrates the effectiveness of Partial GCNNs in chosen tasks it could further explore applications outside of the presented domains considering its potential impact across various tasks that exhibit partial symmetries. 3. Comparison with Other Equivariance Methods: The paper could benefit from a broader comparison with other techniques aimed at learning equivariance or invariance such as methods using data augmentation for invariance learning to more clearly position Partial GCNNs within the landscape of equivariance learning. Questions and Feedback: 1. Could the authors provide more insight into the practical implications of implementing Partial GCNNs particularly regarding model complexity and computational resources compared to traditional GCNNs 2. How does the performance of Partial GCNNs scale with the complexity of the group structures (e.g. higherdimensional groups) and the size of datasets 3. The paper briefly mentions the potential instability in training discrete groups. Can the authors elaborate on the challenges faced and any solutions or workarounds that were considered or could be explored in future work Review Summary: The paper presents a significant advancement in the domain of equivariant neural networks by introducing Partial GCNNs which offer the flexibility to learn partial equivariances directly from data. This approach addresses a critical limitation in traditional GCNNs and opens doors for more efficient model representations in tasks with nonuniform symmetries. The theoretical foundation laid down combined with extensive experiments strongly supports the proposed methods efficacy. However the paper could benefit from a broader discussion on implementation implications a comparison with a wider range of equivariance learning methods and further exploration into the training stability of discrete groups. Overall this work represents a valuable contribution to the field promising for both theoretical insights and practical applications in equivariant learning.", "yOBqNg-CqB0": " Review of \"Reevaluating the Word Mover\u2019s Distance: Classical Baselines Are Competitive When Normalized Properly\"  Summary This paper undertakes the task of critically reassessing the performance of the Word Movers Distance (WMD) in document classification tasks suggesting that previous evaluations might have overstated its effectiveness. By applying appropriate normalization (L1 normalization) to classical baseline methods such as BagofWords (BOW) and TFIDF the authors argue that these simpler methods can perform comparably to WMD. The paper also draws a novel analogy between WMD and L1normalized BOW further demonstrating that when considering the high dimensionality of word embeddings the distance values and performance between WMD and BOW converge. Through a series of reevaluations the paper points out flaws in the original study of WMD and suggests that normalization plays a significant role in the perceived performance improvements of WMD over classical methods.  Strong Points 1. Critical Reevaluation: The paper conducts a thorough and critical reevaluation of WMD providing significant insights into the actual performance of WMD compared to classical methods when normalization is properly applied. This adds depth to the understanding of document classification methodologies. 2. Clarity and Novel Insights: Presenting a clear argument threaded with novel insights such as the analogy between WMD and L1normalized BOW enhances our understanding of why these methods may perform similarly under certain conditions. 3. Comprehensive Analyses and Experiments: The experiments are welldesigned and comprehensive employing various data sets and carefully considering factors such as normalization and the impact of highdimensional spaces on word embeddings. The inclusion of preprocessing scripts and code for replication is commendable. 4. Valuable Discussion on Dimensionality: The discussion on how the high dimensionality of word embeddings can affect the performance and perceived similarities between documents is insightful shedding light on the complexities of working with word embeddings.  Weak Points 1. Possible Overemphasis on Normalization: The paper strongly emphasizes normalization as the critical factor for the competitive performance of classical methods against WMD. While important this might oversimplify the nuanced differences between these approaches and other inherent advantages of WMD. 2. Limited Scope of Comparisons: The comparisons focus primarily on BOW and TFIDF with WMD. Including more recent or alternative document similarity methods could have provided a richer context for the evaluation. 3. Potential Underestimation of WMDs Nuances: The paper\u2019s analogy between WMD and BOW under normalization somewhat glosses over the unique capability of WMD to capture semantic similarities beyond mere word matching potentially underestimating the nuances of optimal transport in semantic spaces. 4. Generalization of Findings: The paper could strengthen its argument by discussing whether the findings generalize beyond document classification to other NLP tasks where WMD has been applied such as text generation or alignment tasks.  Questions and Feedback 1. Consideration of Computation Costs: How do the computation times of WMD and the normalized classical methods compare especially in practical largescale applications 2. Impact on Other NLP Tasks: Do the authors believe the findings regarding normalization and highdimensional spaces would similarly affect the performance of WMD in other NLP tasks 3. Exploration of Alternative Document Similarity Measures: Could there be interest in exploring or comparing the performance of other advanced document similarity measures that have emerged since the original WMD study  Review Summary The paper provides a comprehensive and insightful reevaluation of the Word Movers Distance for document classification challenging previous assertions about its superiority by demonstrating that classical methods when appropriately normalized can be highly competitive. Through a series of welldesigned experiments the paper sheds light on the crucial role of normalization and the effects of highdimensional word embeddings offering valuable insights for future research in document similarity and NLP more broadly. While the paper could benefit from a broader comparison with other methods and a deeper exploration of WMD\u2019s unique advantages its critical perspective thorough analyses and transparent reporting significantly contribute to the field. The findings prompt a rethinking of document classification methodologies and stress the importance of normalization potentially guiding new approaches in NLP research. Overall this paper is a significant contribution urging the community to reassess established beliefs about the efficacy of WMD and to consider classical methods more favorably when equipped with appropriate preprocessing.", "r5qumLiYwf9": "1) Summarize the paper The paper presents MaGNET a novel method aimed at producing uniformly distributed samples on the manifold learned by Deep Generative Networks (DGNs) such as GANs VAEs and other variants. The motivation behind MaGNET arises from the observation that DGNs tend to replicate biases present in the training data which could lead to unfair or imbalanced generated samples. MaGNET by modifying the latent space distribution ensures uniform sampling on the manifold without the need for retraining or relabelling effectively mitigating bias and enhancing diversity in generated data. The paper validates MaGNETs effectiveness through quantitative and qualitative evaluations across several stateoftheart DGNs and datasets showing improvements in distribution metrics reduction in bias and increased sample diversity. 2) List strong and weak points of the paper Question and Feedback to the author Strong Points:  Theoretical Foundation: The paper provides a solid theoretical underpinning for MaGNETs approach including detailed discussion on continuous piecewise affine (CPA) mappings and their manipulation to ensure uniform distribution on the learned manifold.  Practical Significance: By addressing the bias in DGNgenerated samples MaGNET tackles a critical issue in fair machine learning and data augmentation offering practical benefits for a wide range of applications.  Broad Applicability: MaGNETs design as a plugandplay solution that can be applied to any pretrained DGN model without the need for retraining is highly advantageous making it widely applicable and easy to adopt.  Extensive Evaluation: The paper includes a comprehensive set of experiments demonstrating the methods effectiveness in reducing biases and increasing the diversity of generated samples while maintaining or improving quality metrics. Weak Points:  Uniformity vs Semantic Diversity: While the paper discusses uniform sampling on the manifold there is less emphasis on how this translates to semantic diversity and fairness in terms of representation across various attributes. Uniform manifold distribution does not guarantee uniformity in semantic attributes which might be a critical aspect for certain applications.  Computational Overheads: Although MaGNET is designed to avoid the need for retraining DGNs the paper does not extensively discuss the computational costs associated with its application particularly in terms of the MonteCarlo approximation steps involved in obtaining uniform samples.  Potential Limitations in Complex Manifold Structures: The assumption of CPA mappings might limit MaGNETs effectiveness in cases where the data manifold exhibits highly complex topologies or discontinuities that are not well captured by such mappings. Questions and Feedback:  Can the authors provide more insights or empirical evidence on how MaGNETs uniform sampling on the manifold translates to semantic diversity and fairness across different attributes  A discussion on the computational overheads and potential strategies for optimizing MaGNETs efficiency would be beneficial especially for largescale applications.  Further exploration on MaGNETs applicability or limitations in handling complex manifold structures beyond CPA mappings could enhance the papers contribution to the field. 3) Write review summary (Provide supporting arguments for your recommendation) The paper introduces MaGNET an innovative approach for ensuring uniform sampling on the manifold of Deep Generative Networks addressing key challenges in fairness and diversity of generated data. The substantial theoretical grounding broad applicability and significant practical implications of MaGNET are notable strengths of the work. Through rigorous experimentation the authors successfully validate the methods effectiveness showcasing its ability to mitigate bias and enhance data diversity without the need for extensive retraining or labeling efforts. However the paper would benefit from a deeper exploration into the relationship between uniform manifold sampling and semantic diversity more detailed analysis on computational efficiency and investigation into the handling of complex manifold structures. Despite these areas for potential improvement the contributions of this paper are significant offering valuable insights and tools for advancing fair and diverse data generation practices in machine learning. The paper is wellwritten though addressing the highlighted feedback could further strengthen its impact and application scope. Overall I recommend this paper for publication contingent upon consideration and possible incorporation of the suggested revisions.", "vJb4I2ANmy": "1) Summary of the Paper This paper introduces Noisy Feature Mixup (NFM) a novel data augmentation method that aims to enhance model robustness and generalization by injecting noise into convex combinations of pairs of input and hidden layer features. NFM is posited as a generalization and effective combination of two existing approaches: mixup and noise injection thereby inheriting the merits of both. The method is designed to provide a substantial regularization effect intentionally smoothing decision boundaries for improved performance on outofdistribution data and robustness against various data perturbations. The authors back their proposal with both theoretical analysis demonstrating how NFM imposes stronger regularization effects and empirical results showing favorable outcomes on benchmark datasets across several architectures. The paper also delves into the robustness aspect of NFM providing insights into how it may contribute to a models resilience against adversarial attacks without compromising accuracy on clean data. 2) Strong and Weak Points Questions and Feedback Strong points:  Innovative Integration: The idea of combining features from both noise injection and mixup approaches to create NFM for data augmentation is innovative. It stands out as a compelling method for enhancing model robustness and smoothing decision boundaries.  Solid Theoretical Backbone: The paper presents a wellgrounded theoretical analysis including the formulation of NFM within the vicinal risk minimization framework and the demonstration of its implicit regularization effects.  Comprehensive Empirical Validation: The extensive empirical results across various datasets and model architectures provide strong evidence for the effectiveness of NFM compared to baseline methods and related data augmentation techniques.  Focus on Model Robustness: The explicit focus on improving model robustness especially against a range of data perturbations addresses a critical need in machine learning and deep learning research. Weak points:  Computational Overhead Concerns: Despite the claim of minimal computation overhead the paper lacks a detailed discussion on the computational costs involved in the implementation of NFM especially in comparison to simpler data augmentation techniques.  Limited Exploration of Parameter Sensitivity: There is some exploration of different noise levels however a more detailed analysis of the sensitivity to the hyperparameters of NFM (e.g. noise levels mixup parameters) could provide further insights into the robustness and performance tradeoffs.  Potential Overfitting to Perturbations: While NFM shows improved robustness to specific types of perturbations there\u2019s a concern whether models could become overly adapted or overfitted to the types of noise introduced during training potentially reducing generalizability. Questions and Feedback:  How does the computation time of training models with NFM compare with traditional mixup manifold mixup and other data augmentation techniques especially at scale or with larger models  The paper demonstrates NFMs robustness against several types of perturbations primarily visual. Could there be future work on NFM\u2019s applicability and effectiveness in nonvisual tasks such as natural language processing or time series analysis  Considering the varied settings for \u03b1 \u03c3add and \u03c3mult could the authors provide more guidance or heuristic rules for selecting these hyperparameters based on the dataset or task at hand 3) Review Summary The introduction of Noisy Feature Mixup (NFM) represents a significant contribution to the field of machine learning especially in the context of improving model robustness and generalization. The integration of mixup and noise injection into a single augmentation technique is both innovative and theoretically wellfounded. The comprehensive empirical analysis spanning various datasets and architectures convincingly demonstrates NFM\u2019s superior performance over established methods in enhancing robustness without sacrificing accuracy on clean data. However areas such as computational cost implications sensitivity to hyperparameters and potential overfitting to introduced noise types could benefit from further exploration. Despite these concerns the strengths of NFM particularly its theoretical grounding and demonstrated effectiveness across multiple empirical scenarios support a positive outlook on its applicability and potential for wide adoption. Future investigations into its computational efficiency parameter tuning and performance in domains beyond computer vision could further solidify NFMs standing as a valuable tool in the machine learning toolkit.", "q4tZR1Y-UIs": "Peer Review of \"Curriculum Self Play for GeneralPurpose Reinforcement Learning Agents\" 1. Summary The paper introduces Curriculum Self Play (CuSP) a novel method for automatic curriculum generation in reinforcement learning (RL). CuSP aims to efficiently train generalpurpose RL agents capable of solving a wide variety of goals by automatically generating a sequence of goals (goal curriculum) that progressively increase in difficulty and diversity while also preventing catastrophic forgetting of previously learned tasks. The approach is based on a symmetrized multiagent setup involving two offpolicy student learners and two regretmaximizing goal generators each pair dynamically balancing cooperation and competition to promote effective learning. CuSP leverages the Soft Actor Critic (SAC) algorithm for optimizing goal generators incorporating entropic goal coverage to ensure diversity and dynamically adjusting regret targets to account for the nonstationary nature of student learners. This methodology is claimed to outperform existing methods in generating goal curricula that lead to improved sample efficiency and generalization to novel outofdistribution (OOD) goals across various control tasks in robotics. 2. Strengths and Weaknesses Strengths:  Innovative Approach: CuSP introduces a unique combination of curriculum learning selfplay and regret optimization to tackle the challenges of automatic curriculum generation.  Comprehensive Empirical Evaluation: The paper performs extensive experiments across diverse tasks demonstrating CuSPs superiority over other methods in terms of sample efficiency and OOD generalization.  Theoretical Motivation: The work is wellfounded on established RL principles and extends prior work on asymmetric selfplay and curriculum learning providing a solid theoretical background. Weaknesses:  Limited Comparison: While the paper shows CuSP outperforms other methods it may lack an exhaustive comparison against a wider range of recent techniques in goal curriculum generation.  Complexity and Scalability: The methods dependency on maintaining four dynamically interacting agents (two learners and two goal generators) raises questions about its computational complexity and scalability to even larger more diverse task sets.  Goal Space Specificity: CuSPs reliance on a prespecified goal space may restrict its application to environments where defining such a space is nontrivial or when goals cannot easily be encoded in the environments state space. Questions and Feedback:  Can the authors elaborate on how CuSP could be adapted or extended to environments where a clear goal space is hard to specify or when goals are multidimensional and highorder  It would be beneficial if the authors could discuss the scalability of CuSP in more detail specifically addressing computational complexity and potential strategies for scaling to larger more complex environments.  Given the dynamic interaction between the agents and goal generators how sensitive is CuSP to the initial configuration of these entities and their hyperparameters Any insights or guidelines for practitioners would be valuable. 3. Review Summary The paper presents a significant contribution to the field of RL specifically in the area of automatic curriculum generation for generalpurpose reinforcement learning agents. CuSP is a theoretically sound and empirically validated method that creatively combines elements of selfplay curriculum learning and regret optimization. Its success in generating effective goal curricula as evidenced by improved sample efficiency and generalization capabilities is compelling. However the approachs practical applicability might be limited by its complexity scalability concerns and reliance on a predefined goal space. Future work could explore adaptations for more generalized goal settings scalability enhancements and further empirical validations against a broader set of benchmarks and recent methods. Overall the paper is wellwritten with clear contributions and a strong experimental section supporting its claims. While acknowledging the limitations and suggested areas for future research CuSP stands as a notable advancement in the quest for versatile and efficient RL agents.", "wClmeg9u7G": " Peer Review for \"Distributed Methods for Solving Variational Inequalities with Compressed Communication\"  1. Summary of the Paper This paper tackles the challenge posed by the demand for communication efficiency in distributed processing of machine learning models specially focusing on variational inequalities (VIs) and saddle point problems. Such problems are notably relevant in adversarial learning Generative Adversarial Networks (GANs) transport and robust optimization. The authors introduce two novel algorithms MASHA1 and MASHA2 that incorporate data compression techniques to reduce the volume of information exchanged during the distributed training of models without compromising on performance quality. These algorithms utilize unbiased (Randk) and contractive (Topk) compressors respectively and are theoretically grounded with provable convergence properties. The study further validates these algorithms through experiments on a bilinear minmax problem and largescale adversarial training of transformers demonstrating their effectiveness in reducing communication overhead while maintaining model performance.  2. Strong and Weak Points Question and Feedback Strong Points: a. Originality: The introduction of MASHA1 and MASHA2 which leverage communication compression within distributed methods for solving VIs and saddle point problems addresses a significant challenge in distributed ML systems\u2014communication overhead. This novel approach not only enriches the literature but also offers practical solutions. b. Theoretical Foundation: The paper provides a rigorous theoretical analysis of the proposed methods establishing convergence properties that are crucial for the adoption of such algorithms in practice. c. Experimental Validation: The empirical studies conducted provide valuable insights into the practical effectiveness of the proposed methods. The comparison with existing techniques highlights the advantages of MASHA1 and MASHA2 in terms of communication efficiency and model performance. Weak Points: a. Complexity of Implementation: While the paper provides a thorough theoretical background and empirical evidence of effectiveness there is little discussion on the implementation complexity or the potential challenges practitioners might face in deploying these algorithms. b. Lack of Broad Experimentation: The experiments are limited to a bilinear minmax problem and adversarial training of transformers. Additional experiments across varied domains and problem sets would enhance the robustness and generalize the applicability of the findings. Questions and Feedback: 1. Can the authors elaborate on the computational overhead introduced by the compression and decompression processes Specifically how do these costs compare to the savings in communication overhead 2. How do MASHA1 and MASHA2 perform in a network with highly variable bandwidth Is there a significant impact on convergence rates or model performance 3. The paper could benefit from a deeper discussion regarding the choice and impact of compression techniques (unbiased vs contractive compressors) on different types of variational inequalities and learning models. 4. It would be interesting to see a comparison or integration of these methods with other strategies aimed at reducing communication overhead in distributed learning such as federated learning techniques.  3. Review Summary This paper presents significant contributions to the field of distributed machine learning by addressing the critical issue of communication overhead in the training of models for variational inequalities and saddle point problems. The introduction of the MASHA1 and MASHA2 algorithms backed by solid theoretical foundations and empirical validations is both novel and valuable. The ability of these methods to leverage compression techniques to efficiently reduce the amount of information that needs to be communicated without sacrificing model quality is particularly compelling. However the paper could be strengthened by addressing the computational costs associated with the compression techniques expanding the range and diversity of experimental validations and elaborating on the implications of these methods in different network conditions and across various machine learning applications. Addressing these points would provide a more comprehensive understanding of the practical applicability and limitations of MASHA1 and MASHA2. Overall the paper makes a significant contribution to the literature proposing innovative solutions to a pressing problem in distributed machine learning. With further elaboration on the raised points this work has the potential to guide future research and practice in developing efficient scalable machine learning models.Dispatcher", "wYqLTy4wkor": " Review of \"CurriculumInduced Covariate Shift in Reinforcement Learning: A Solution via Unsupervised Environment Design\"  Summary This paper addresses a fundamental issue in adaptive curriculum learning for Reinforcement Learning (RL) known as CurriculumInduced Covariate Shift (CICS) where the distribution of training environments diverges from the test (or deployment) environment distribution potentially leading to suboptimal policies. The authors argue that this issue is particularly problematic in partiallyobservable or stochastic settings where optimal policies are contingent upon the groundtruth distribution of aleatoric parameters. To tackle this they introduce SampleMatched PLR (SAMPLR) a novel method under the Unsupervised Environment Design (UED) framework that aims to align advantage estimates to the groundtruth distribution thereby correcting the curriculuminduced bias. The efficacy of SAMPLR is demonstrated through experiments conducted in challenging environments based on the NetHack Learning Environment showcasing its ability to learn nearoptimal policies where standard methods fail.  Strong Points 1. Novelty of Addressing CICS: The paper successfully identifies and formalizes the problem of CurriculumInduced Covariate Shift (CICS) in RL an underexplored area that has significant implications for the generalization of learned policies to new environments. 2. Theoretical Foundation and Empirical Verification: The methodological approach is grounded in a solid theoretical framework including the derivation of Bayesoptimal guarantees for SAMPLR. The experimental results offer convincing evidence of SAMPLRs ability to mitigate the effects of CICS outperforming baseline methods in terms of generalization to unseen environments. 3. Relevance and Timeliness: Given the growing complexity of environments where RL is being applied the ability to generalize well to unseen scenarios is critical. This work makes a timely and relevant contribution by addressing a core challenge in curriculum learning for RL.  Weak Points 1. Scalability and Computational Overheads: The solution relies on blackbox access to simulators and the alignment of advantage estimates which might introduce significant computational overheads. The scalability of SAMPLR particularly for highly complex or realworld environments where simulation could be costly or slow is not thoroughly discussed. 2. Generalization of the Approach: While the paper presents a compelling case for the effectiveness of SAMPLR in the environments tested there is a lack of discussion on the methods applicability across a broader range of RL tasks and settings. Further the reliance on certain assumptions such as blackbox access to simulators may limit the general applicability of the proposed solution. 3. Comparison with Other Curriculum Learning Strategies: The paper could benefit from a more extensive comparison with other curriculum learning or environment design strategies beyond the mentioned PLR and domain randomization methods. A deeper analysis of how SAMPLR compares or contrasts with other contemporary approaches would add value.  Questions and Feedback to the Author 1. Could the authors elaborate on the potential scalability issues of SAMPLR especially in scenarios where simulation is expensive 2. How does SAMPLR fare in environments with a higher degree of complexity or in more dynamic multiagent settings Are there any limitations to its applicability 3. The paper relies on specific assumptions such as blackbox access to simulators. Could the authors discuss alternative scenarios where these assumptions may not hold and propose how SAMPLR could be adapted  Review Summary This paper makes a significant contribution to the field of reinforcement learning by identifying and addressing the issue of CurriculumInduced Covariate Shift (CICS). The proposed SAMPLR method not only provides a novel solution to align advantage estimates to the groundtruth distribution but also showcases effectiveness through rigorous theoretical backing and empirical testing. However challenges related to the scalability of the method the generalization of the approach to a wider array of RL tasks and comparisons with other curriculum strategies could be better addressed. Despite these areas for potential improvement the work presents valuable insights into optimizing curriculum learning methods for RL pushing the boundaries of how adaptive curricula can be designed to achieve robust generalization. Given the significance of the problem tackled and the promising results obtained this paper constitutes an important step forward in the domain of unsupervised environment design for RL.", "rhDaUTtfsqs": "Review of the Paper on Curriculum Learning in Pretraining of GPT2 Models 1. Summary: This paper investigates the challenges associated with training highcapacity autoregressive language models specifically GPT2 focusing on training instability and inefficiency. It presents a detailed study contrasting the effects of different training parameters on model stability and introduces a curriculum learningbased approach to address these issues. The authors demonstrate that curriculum learning significantly improves training stability and efficiency allowing for larger batch sizes and learning rates without the usual risks of training divergence. They provide extensive experimental results to back their claims showcasing reductions in required training tokens and wallclock time while maintaining or enhancing model performance on standard evaluation benchmarks. 2. Strong and Weak Points Questions and Feedback: Strong Points:  Theoretical and Practical Contributions: The paper provides both theoretical insights into the reasons behind training instabilities with large models and practical solutions to mitigate these issues. The identification of gradient variance as a key factor in training instability is a significant contribution.  Comprehensive Experimentation: The authors present a thorough set of experiments to validate their approach. The comparison between traditional training methods and their curriculum learningbased approach across various metrics (e.g. validation perplexity zeroshot evaluation) is wellexecuted.  Impact on Training Efficiency: The reported improvements in training efficiency (reducing the required number of tokens and wallclock time by up to 61 and 49 respectively for achieving the same validation perplexity) are impressive and hold substantial practical relevance for future work in the field. Weak Points:  Generalizability of Results: While the results are promising for GPT2 models its unclear how generalizable these findings are to other model architectures such as GPT3 or nonautoregressive models.  Lack of Discussion on Limitations: The paper could benefit from a more detailed discussion regarding potential limitations or downsides of the proposed curriculum learning approach especially in different training contexts or when applied to a broader range of tasks.  Comparison with Other Regularization Techniques: While curriculum learning is shown to outperform the baseline it would be informative to see how it compares with other stateoftheart regularization or training techniques designed to tackle similar issues. Questions and Feedback:  Generalizability to Other Models: Can the authors comment on the potential applicability of their curriculum learning approach to other large language models such as GPT3 or even to different architectures like BERT  Impact on Downstream Tasks: The paper focuses on pretraining efficiency and stability. It would be valuable to explore or discuss how the models trained with curriculum learning perform on a wider array of downstream tasks.  Curriculum Learning Design Choices: The choice of sequence length as the primary difficulty metric is welljustified. However are there other metrics that were considered or might complement the current approach to further enhance training efficiency 3. Review Summary: This paper tackles the significant challenge of training instability in large autoregressive models providing a novel and effective solution through a curriculum learning approach. The thorough experimentation and clear demonstration of benefits in terms of training stability and efficiency are strong aspects of this work. However the paper would benefit from a broader discussion on the applicability of the proposed method to other models and tasks potential limitations and a comparison with alternative stateoftheart techniques. Overall the contributions of this work are valuable presenting a compelling case for the use of curriculum learning as a regularization method in training large language models. The findings not only advance our understanding of training dynamics in such models but also offer practical insights that can significantly reduce the computational resources required for model training.", "rJvY_5OzoI": " Review of \"MultiCritic Actor Learning (MultiCriticAL) for MultiStyle Reinforcement Learning\"  1. Summary This paper introduces MultiCritic Actor Learning (MultiCriticAL) a novel framework for multitask reinforcement learning (MTRL) particularly focusing on multistyle learning scenarios. Traditional MTRL approaches employ a single critic function for value estimation across multiple tasks which can lead to negative interference and suboptimal policy learning. MultiCriticAL addresses this issue by assigning a separate critic for each task (or style) while maintaining a singular actor. This separation helps avoid value estimation interference and simplifies the learning process since critics no longer need to differentiate between tasks. The paper presents extensive evaluations of MultiCriticAL across various setups including synthetic environments classic video games and a realworld application with EA\u2019s UFC game demonstrating significant performance improvements over traditional singlecritic methods.  2. Strong Points  Novelty: MultiCriticAL introduces a unique approach to the MTRL problem by employing multiple critics with a single actor a methodology not extensively explored in existing literature. This innovation addresses the negative interference problem a significant bottleneck in MTRL performance.  Comprehensive Evaluation: The paper provides a wide range of experimental evaluations from synthetic environments to complex video games and a realworld application. These experiments convincingly demonstrate the flexibility and effectiveness of MultiCriticAL in various contexts.  Practical Relevance: By showcasing MultiCriticAL in a realworld scenario with the UFC game the paper underlines the practical applicability of its contributions suggesting it can serve immediate use in industry applications such as game development and AI behavior training.  Weak Points  Complexity and Resource Requirements: While the separation of critics per task might alleviate learning interference it potentially introduces a significant increase in computational resources and complexity especially as the number of tasks grows. The paper could provide a more thorough discussion on the scalability and computational cost implications.  Comparison with StateoftheArt: The baseline comparisons though extensive primarily focus on singlecritic MTRL frameworks. Including a broader range of stateoftheart MTRL approaches especially those employing novel policy optimization or valuefunction approximation techniques would provide a clearer picture of MultiCriticAL\u2019s positioning.  Generalization to NonStyle Based Tasks: The paper focuses on multistyle learning problems. It would be beneficial to understand how MultiCriticAL performs in more general MTRL problems where task distinctions are not necessarily stylebased.  Questions and Feedback to the Author 1. Scalability and Resource Usage: Can the authors provide insights into the scalability of MultiCriticAL especially concerning the number of critics and the associated computational overhead 2. Generalization: How well does MultiCriticAL generalize to nonstyle based MTRL problems Are there specific application domains where MultiCriticAL may not be as effective 3. Improvement on StateoftheArt: Could the authors clarify how MultiCriticAL compares with the latest stateoftheart MTRL methods not limited to singlecritic approaches  Review Summary MultiCritic Actor Learning (MultiCriticAL) represents a significant advancement in multitask reinforcement learning particularly for multistyle learning problems. By employing multiple critics with a singular actor the approach innovatively addresses negative interference a persistent challenge in MTRL. The comprehensive evaluation across diverse environments illustrates MultiCriticAL\u2019s efficacy and flexibility. However discussions on scalability comparison with a broader range of stateoftheart methods and generalization could be further expanded to bolster the papers contributions. The introduction of MultiCriticAL fills a notable gap in MTRL methodology potentially paving the way for more efficient and performant multitask learning solutions. The presented results substantiate the papers claims showing considerable improvements over singlecritic baselines. Despite the need for a more detailed discussion on certain aspects this work undoubtedly contributes valuable insights and tools to the RL community with immediate applicability in industry scenarios. Recommendation: Accept with minor revisions. The paper presents significant contributions to the RL field however addressing the highlighted questions and feedback will further strengthen its impact and clarity.", "kWuBTQmkO8_": " Review of \"MixRL: A Data Augmentation Meta Learning Framework for Regression Using Monte Carlo Policy Gradient Reinforcement Learning\"  Summary This paper introduces MixRL a novel data augmentation framework designed for regression tasks which overcomes the limitations of existing techniques predominantly tailored for classification tasks. The authors argue that the linear interpolation assumption underpinning Mixup techniques for classification does not hold in regression due to the continuous nature of label spaces leading to potentially incorrect labels when mixing examples with large data or label distances. To address this MixRL employs a more restricted assumption of linearity mixing examples only within specific data or label distances determined by learning how many nearest neighbors should be mixed for optimal model performance. This learning is achieved through a meta learning framework utilizing Monte Carlo policy gradient reinforcement learning based on the insight that mixing examples should be selective and tailored to individual examples. Experimental evaluations on both synthetic and real datasets demonstrate MixRLs ability to outperform stateoftheart data augmentation baselines supporting the frameworks potential for integration with Mixup techniques for classification to further enhance results.  Strong Points 1. Innovative Approach: The MixRL framework introduces an innovative approach to data augmentation for regression tasks addressing a significant gap in existing literature where most augmentation techniques are designed for classification. This unique focus is well justified supported by empirical evidence and likely to spur further research in this area. 2. Strong Theoretical Foundation: The theoretical underpinnings of MixRL including the identification of limitations in the direct application of Mixup techniques to regression and the rationale for selective example mixing are clearly articulated. The methodology for adaptively determining the number of nearest neighbors for mixing adds a layer of sophistication and seems appropriately solved using reinforcement learning. 3. Clear Demonstration of Effectiveness: The experimental design is comprehensive employing both synthetic and real datasets to test the MixRL framework. The use of baseline comparisons and detailed analysis of results (including RMSE and R2 scores) effectively demonstrate MixRLs superior performance in enhancing regression model accuracy.  Weak Points 1. Complexity and Scalability Concerns: While the authors address scalability to a degree the computational complexity inherent in the proposed approach\u2014especially given the reliance on reinforcement learning\u2014raises potential concerns. The impact of this complexity on the practical applicability in various realworld scenarios especially those with very large datasets or limited computational resources could be further clarified. 2. Generalizability of Results: The paper presents a strong case based on the datasets used however the generalizability of the MixRL framework to a wider range of regression tasks and data characteristics could benefit from further exploration. Particularly the performance impact in scenarios with highly irregular data distributions or extreme outliers remains to be understood. 3. Integration with Classification Techniques: While the authors briefly mention the potential for integrating MixRL with classification Mixup techniques this aspect is not deeply explored. Given the prevalence and success of Mixup in classification tasks a more detailed investigation into how such integration could be optimized would be valuable.  Questions and Feedback 1. Computational Complexity: Can the authors provide more details on the computational demands of MixRL especially in comparison to the baseline methods and discuss potential strategies to mitigate complexity in largescale applications 2. Implementation Details: Are there specific challenges to implementing the MixRL framework in practice particularly regarding the setting of hyperparameters (e.g. the selection of nearest neighbors) 3. Potential for Bias: The ethics statement raises the important issue of data augmentation possibly introducing bias. Could the authors expand on specific scenarios where MixRL might inadvertently introduce bias and suggest means of mitigation  Review Summary MixRL represents a significant advancement in data augmentation techniques for regression tasks addressing a noteworthy gap in the literature with a wellconceived and effectively demonstrated approach. The innovative use of reinforcement learning to tailor mixing strategies to individual examples is a particular strength of the paper yielding empirical results that highlight MixRLs potential to improve regression model performance markedly. Despite these strengths considerations regarding the computational demands of the framework and its generalizability across diverse regression problems warrant further exploration. Additionally while the possibility of integrating MixRL with classificationfocused augmentation techniques is intriguing a more thorough investigation in this direction could enhance the papers contributions. Overall the clear presentation solid experimental methodology and promising results support the potential of MixRL as a valuable addition to the domain of regression modeling.", "vXGcHthY6v": " Peer Review of \"Invariance through Inference\"  1. Summary The manuscript presents a novel method named \"Invariance through Inference\" aimed at improving testtime performance in reinforcement learning (RL) agents when deployed in environments exhibiting novel perceptual variations. Distinct from current methods that rely heavily on data augmentation to produce invariant visual features this approach tackles adaptation as an unsupervised learning problem at deployment. The core idea is to adapt the encoder to minimize distributional shifts between the source and target domain latent features without requiring paired data or deploymenttime rewards. Comparisons are made against stateoftheart methods in scenarios involving camera pose and lighting changes with the method demonstrating notable enhancements in various adaptation scenarios.  2. Assessment Strong Points:  The paper addresses a pressing issue in RL concerning the generalization of agents to unseen environments a scenario frequently encountered in practical applications especially robotics.  The methodology is novel leveraging unsupervised learning for testtime adaptation by specifically focusing on matching latent distributions without the necessity for deploymenttime rewards or paired data.  The experimental evaluation is thorough utilizing a challenging set of domains and comparing against both traditional and stateoftheart methods providing clear evidence of the effectiveness of the proposed approach.  The paper is wellwritten logically structured and provides a substantial amount of detail regarding the problem formulation algorithmic design and experimental setup allowing for reproducibility. Weak Points:  While the paper excels in empirical validation it lacks a deep theoretical analysis or formal proof to back the proposed methods effectiveness which could strengthen the papers claims.  The evaluation mainly focuses on visual perception changes additional experimentation in more diverse settings including changes in dynamics might reveal the methods full spectrum of applicability.  The comparison with related work could be expanded to include discussion on how the proposed method might integrate with or improve upon existing approaches outside of the specific baseline methods tested.  Little attention is given to the computational cost of deploymenttime adaptation which is crucial for realtime applications. Questions and Feedback to the Author: 1. Could the authors provide further insights into the theoretical underpinnings of why matching latent distributions leads to successful adaptation in unseen environments 2. How does the computational overhead of the \"Invariance through Inference\" approach during deployment compare to the baseline methods and what implications does this have for realtime applications 3. Have the authors considered extending their experiments to include scenarios with dynamic changes in the environment and if so how would the method need to be adjusted  3. Review Summary Overall the paper introduces an innovative and efficacious approach to a significant problem in RL backed by strong empirical evidence. The methods ability to adapt in unseen environments without relying on paired data or deploymenttime rewards addresses a critical gap in current generalization strategies. However the paper would benefit from deeper theoretical analysis broader experimentation across different types of environmental changes and a discussion on computational costs. Thus while the paper is strong in its current form addressing these points could greatly enhance its impact and applicability. Given the novelty and demonstrated performance improvements I recommend this paper for publication contingent upon consideration of the highlighted weaknesses and questions.", "s-b95PMK4E6": " Review of \"Hierarchical Approach for Compositional Reasoning (HACR) in Interactive Instructions Following\"  1) Summary: The paper presents HACR a hierarchical modular robotic agent designed to perform domestic chores as directed through natural language. This approach divides the task of understanding and executing instructions into highlevel subgoals processed by specific subgoal policies. The method involves three main components: a Policy Composition Controller (PCC) that generates a sequence of subgoals from linguistic directives a Master Policy (MP) for navigation aided by an Object Encoding Module (OEM) and a Loop Escape Module (LEM) and several Interaction Policies (IP) for object manipulation. Tested on the ALFRED benchmark the HACR system outperforms existing approaches showcasing superior performance in both seen and unseen environments.  2) Strengths and Weaknesses:  Strong Points: a) Hierarchical Structure: The layered approach allows for the separation of task types (navigation vs interaction) likely contributing to the improved performance and efficiency. b) Modularity: By designing independent modules for different subgoals HACR allows for specialized handling of diverse tasks facilitating easier debugging and potential reusability. c) Empirical Success: Achieving stateoftheart results on the rigorous ALFRED benchmark testifies to the effectiveness of the proposed method.  Weak Points: a) Complexity and Scalability: The addition of multiple modules policies and controllers raises questions about system complexity and ease of scaling to other domains or more complex scenarios. b) Dependence on Structured Language Instructions: The system\u2019s performance might significantly drop if the natural language instructions deviate substantially from the \"stepbystep\" structure assumed in this study. c) Overhead on Component Coordination: Managing interactions between hierarchical levels and modules could introduce significant computational overhead or latency particularly in realtime applications.  Questions and Feedback: a) Can the authors provide insights into how the system would handle ambiguous or incomplete natural language instructions b) It would be beneficial if the authors discussed potential scalability challenges especially regarding incorporating a wider range of tasks or dealing with dynamically changing environments. c) The paper could gain by including a discussion on the tradeoffs between the improved task performance and the increased system complexity especially in terms of computational resources and realtime applicability.  3) Review Summary: The paper introduces an innovative approach to robotic task execution through hierarchical modular learning demonstrating significant performance advancements on a challenging benchmark. The hierarchical organization and modular design represent meaningful departures from traditional flat policy systems enabling specialized processing of subtasks and potentially offering insights into human cognitive task decomposition strategies. However the increased system complexity dependence on structured linguistic inputs and potential challenges in scalability and realtime application present areas for further exploration and clarification. These issues notwithstanding the paper contributes substantially to the field of embodied AI and interactive instruction following opening new avenues for research in task decomposition and modular learning approaches. The empirical success coupled with a wellstructured approach to task handling supports a strong recommendation for publication encouraging future work to build upon these foundations while addressing the outlined limitations.", "ydopy-e6Dg": " Summarize the paper The paper introduces iBOT a novel selfsupervised framework for Vision Transformers (ViT) that focuses on Masked Image Modeling (MIM) by employing an online tokenizer for improved semantic understanding in images. Unlike previous methods that rely on pretrained tokenizers or simplistic tokenization methods iBOT integrates the tokenizer within the model allowing it to learn semantically meaningful visual tokens concurrently with the primary MIM task. This approach facilitates the direct training of the Vision Transformer on masked inputs utilizing selfdistillation techniques borrowed from knowledge distillation literature. The authors demonstrate that iBOT achieves stateoftheart results on image classification object detection instance segmentation and semantic segmentation tasks emphasizing the emergence of partlevel semantics that significantly enhance model robustness and transfer capabilities.  Strong and Weak Points Questions and Feedback to the Author  Strong Points: 1. Novelty and Technical Merit: iBOT introduces an innovative approach to masked image modeling by incorporating an online tokenizer. This method addresses the problem of pretraining a semantically meaningful tokenizer and seamlessly integrates it into the learning process representing a significant technical advancement. 2. Comprehensive Evaluation: The paper presents a thorough evaluation of iBOT across various tasks (classification detection segmentation) and conditions (semisupervised unsupervised learning) demonstrating its superior performance and versatility compared to existing methods. 3. Robustness and Transferability: The identified partlevel semantics not only enhance recognition accuracy but also significantly improve the models robustness against common image corruptions and its ability to transfer learned knowledge to different datasets.  Weak Points: 1. Scalability Concerns: Although iBOT shows impressive results there is limited discussion on the computational resources required which could be a barrier for adoption particularly in larger datasets or models. Further elaboration on computational efficiency and potential optimization strategies would be beneficial. 2. Generalization Beyond PreDefined Tasks: The paper thoroughly evaluates iBOT on a set of standard vision tasks but there is little discussion on its generalization capabilities to new or unseen tasks which is critical for assessing the models flexibility and utility in realworld applications. 3. Explanation of Emergent Semantics: While the emergence of partlevel semantics is highlighted as a key strength the paper could benefit from a deeper exploration or theoretical analysis of why and how these semantics emerge within iBOT offering insights into the workings of selfsupervised learning mechanisms in vision models.  Questions: 1. How does the performance of iBOT scale with the size of the dataset and model complexity Are there diminishing returns or particular challenges encountered with larger scales 2. Can iBOT be adapted or extended to work in multimodal contexts (e.g. vision and language) given its strong semantic understanding capabilities 3. How does the selfdistillation mechanism in iBOT compare or interact with other selfsupervised contrastive or predictive learning approaches  Feedback: The paper presents a significant advancement in selfsupervised learning for vision tasks through the innovative use of an online tokenizer within the masked image modeling framework. Future work could further elaborate on the scalability and computational efficiency of iBOT explore its generalization capabilities across a wider range of tasks and provide a deeper theoretical understanding of the emergent semantics. Addressing these points would strengthen the papers contributions and offer insights into the broader implications of iBOTs approach for the field.  Review Summary The paper introduces iBOT a compelling new framework for Masked Image Modeling with an online tokenizer significantly advancing the stateoftheart in selfsupervised learning for vision transformers across multiple tasks. The key innovation lies in the integration of the tokenizer into the learning process allowing for realtime semantic tokenization which enriches model understanding and robustness. While the results showcase the methods effectiveness addressing its scalability generalization to unseen tasks and providing a more detailed theoretical rationale for the emergent semantics would enhance its contribution. Overall the work represents a notable step forward in selfsupervised vision learning opening new avenues for research and application.", "psNSQsmd4JI": " Review of \"Containerized Distributed ValueBased MultiAgent Reinforcement Learning Framework\"  Summary This paper presents a novel distributed learning framework specifically tailored for valuebased multiagent reinforcement learning (MARL) aimed at overcoming the inherent challenges of such environments such as demanding data transfer communications management and a high requirement for exploration. The proposed Containerized MARL (CMARL) framework leverages a containerized architecture where each container encompasses several actors interacting with environment instances a local buffer and a local learner all managed by a multiqueue system to avoid data transfer bottlenecks and enhance data priority handling. The novel aspect of prioritizing data transfer based on trajectory importance and encouraging diversity in local policy learning across containers is particularly highlighted. The framework has demonstrated significant improvements over stateoftheart nondistributed MARL algorithms in challenging environments such as Google Research Football (GRF) and the StarCraft II micromanagement benchmark (SMAC) showing both scalability and efficiency.  Strengths and Weaknesses Strong Points: 1. The architecture uniquely addresses the inherent challenges of distributed MARL with innovative solutions like containerization multiqueue management for nonblocking operations and localglobal learning coordination. 2. The paper demonstrates a comprehensive experimental setup with challenging benchmarks (GRF and SMAC) showing the methods effectiveness over existing stateoftheart techniques with significant performance improvements. 3. Encouraging diversity in local policies and selective data prioritization for global learning are effectively employed to solve complex tasks and enhance exploration efficiency. 4. The paper is wellstructured providing a clear introduction to the problem a detailed description of the CMARL framework thorough experimental validations and a candid discussion on the novelty and implications of the findings. Weak Points: 1. The paper lacks a detailed discussion on the limitations and potential drawbacks of the proposed framework particularly in terms of computational overheads scalability beyond the tested environments and adaptability to different MARL setups. 2. While the paper claims significant improvements over nondistributed MARL algorithms it does not provide a direct comparison with existing distributed MARL approaches which would strengthen the evaluation. 3. The reproducibility of results could be improved by providing more details on hyperparameter settings environment configurations and possibly challenges faced during implementation. Questions and Feedback: 1. How does the containerized framework perform in terms of computational efficiency especially considering the overheads introduced by container management and multiqueue operations 2. Could the authors elaborate on how the framework can be adapted or extended to different multiagent learning paradigms beyond valuebased methods 3. It would be beneficial for the authors to discuss the potential limitations of the CMARL framework and any perceived challenges in applying it to realworld multiagent scenarios beyond the tested benchmarks.  Review Summary The proposed CMARL framework introduces an innovative approach to distributed valuebased multiagent reinforcement learning addressing critical challenges such as efficient data handling exploration and scalability. The experimental results are impressive providing clear evidence of the frameworks superiority over existing techniques in complex benchmark environments. However the paper would benefit from a more thorough examination of its limitations direct comparisons with distributed MARL approaches and detailed discussion on computational efficiency and adaptability. Overall the paper makes a significant contribution to the field of MARL offering a promising direction for future research and application in solving complex multiagent tasks.", "xVlPHwnNKv": " Peer Review  1) Summary of the Paper The paper presents an innovative approach to optimizing the actorcritic methodology within reinforcement learning particularly focusing on overcoming the hurdles of high computational complexity and slow convergence rates experienced in Stackelberg actorcritic schemes. The authors introduce a novel approximated Stackelberg Deterministic Policy Gradient (ASDPG) mechanism. This mechanism enhances the traditional Stackelberg learning framework by eliminating terms containing Hessian matrices and employing a block diagonal approximation technique for the remaining inverse matrices. According to their research findings this strategy results in a significant reduction in computational complexity and an acceleration in convergence rates. The authors support their claims by comparing the performance of their approach against stateoftheart methodologies across several benchmarks notable for its superior average returns and acceptable training times.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper addresses critical challenges in advanced actorcritic methods by proposing a simplified Stackelberg learning framework. This innovation is likely to contribute significantly to the field of reinforcement learning.  Detailed Analysis: The incorporation of theoretical analysis alongside empirical validation provides a comprehensive understanding of the proposed methods efficacy and efficiency.  Practical Relevance: The approach focuses on reducing computational complexity a significant barrier in deploying reinforcement learning solutions making it highly relevant for practical applications. Weak Points:  Lack of Comparative Analysis Details: While the paper claims superiority over stateoftheart methods it could benefit from a more detailed comparative analysis specifically highlighting the scenarios where the proposed method underperforms if any.  Generalizability of the Results: The experiments are conducted in controlled environments. Its unclear how the proposed method performs in more complex realworld scenarios where model assumptions might not hold.  Assumptions on Hessian Matrices and Convergence Rates: The paper assumes that removing Hessian matrices and employing block diagonal approximation simplifies the computation without significantly affecting the models predictive performance. However a deeper investigation into the potential tradeoffs involved would be beneficial. Questions and Feedback:  Could the authors elaborate on any instances where the proposed method may not be applicable or underperform compared to the traditional Stackelberg actorcritic methods  It would be helpful to include a discussion on the scalability of the proposed method in more dynamically complex environments.  Feedback would be to possibly explore the impacts of different degrees of the approximation on the convergence rates more thoroughly providing insights into how to balance between computational efficiency and accuracy effectively.  3) Review Summary The paper presents a meaningful advancement in the actorcritic methods realm by addressing significant challenges like computational complexity and slow convergence rates through an approximated Stackelberg Deterministic Policy Gradient technique. The innovative approach backed by theoretical and empirical analysis underlines the methods potential in enhancing efficiency and performance in reinforcement learning tasks. While the paper stands out for its innovative contributions and practical relevance it would benefit from addressing the scalability concerns in more complex environments and providing a more detailed comparative analysis to understand the limitations and potential tradeoffs of the proposed method better. I recommend this paper for publication considering that the authors can address or clarify the outlined weak points and questions which would undoubtedly improve the papers contribution to the reinforcement learning field.", "pfNyExj7z2": " Draft Peer Review  1) Summary of the Paper This paper introduces a novel Vectorquantized Image Modeling (VIM) technique that leverages pretraining a Transformer model for autoregressive prediction of rasterized image tokens. These tokens are encoded via an enhanced VisionTransformerbased VQGAN (ViTVQGAN) which outperforms traditional VQGAN through several architectural and codebook learning improvements. This approach significantly advances the fields of unconditional classconditioned image generation and unsupervised representation learning. Key results include achieving groundbreaking performance scores on ImageNet with an Inception Score (IS) of 175.1 and Fr\u00e9chet Inception Distance (FID) of 4.17. Furthermore the unsupervised learning evaluation through linear probe accuracy demonstrates notable improvements over existing models such as iGPT.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Methodology: The paper introduces significant improvements over traditional VQGAN by employing a Vision Transformer (ViT) as both the encoder and decoder which is novel in the context of image modeling and shows an innovative approach to leveraging Transformer architectures outside of natural language processing tasks.  Impressive Performance Gains: The proposed method outperforms stateoftheart models on several fronts including reconstruction quality image synthesis abilities and unsupervised learning evaluations offering clear and substantiated advancements over previous methods.  Comprehensive Evaluation: The evaluation section is thorough with comparisons to existing methods across different tasks and metrics providing a clear picture of the proposed methods strengths. Weak Points:  Complexity and Accessibility: The architectures complexity and the computational resources required for training on large datasets like ImageNet might limit its accessibility and applicability in some realworld scenarios or for smaller research groups.  Generalization Across Datasets: While the method shows remarkable results on ImageNet CelebAHQ and FFHQ it remains partly unclear how well this approach generalizes to other datasets or more specialized image modeling tasks.  Limited Discussion on Failure Cases: The paper could benefit from a discussion on the limitations and possible failure modes of the proposed method which would offer a more balanced view and insights into areas of potential improvement. Questions and Feedback:  Could the authors detail any attempts or considerations towards reducing the models computational requirements making it more accessible for wider use  How well does the proposed method generalize to other datasets or image modalities not covered in the paper Are there specific types of images or tasks where performance might degrade  In terms of future work how do the authors envision tackling the limitations of their current approach especially concerning the potential biases identified in the datasets used for training  3) Review Summary The paper presents a compelling and wellformulated advancement in image modeling leveraging the strengths of Transformers and an improved VQGAN architecture. The methodology is both novel and thoughtfully developed yielding significant performance improvements that substantiate the proposed methods value and potential impact on the field. However the paper would benefit from addressing a few areas for enhancement particularly concerning the discussion of its limitations potential failure cases and the models generalizability across a broader spectrum of datasets and tasks. Addressing these points could provide a more rounded perspective on the methods applicability and future directions for research. Recommendation: Accept with Minor Revisions Given the substantial contributions and the relatively minor areas for improvement I recommend accepting the paper after minor revisions particularly those aimed at providing additional clarity on the methods limitations and potential for broader applicability.", "o_HsiMPYh_x": " Summarize the paper The article introduces Average Thresholded Confidence (ATC) a novel method designed to predict the accuracy of machine learning models on target domains using labeled source data and unlabeled target data. ATC works by finding a threshold in the models confidence level beyond which it predicts the accuracy based on the fraction of target examples that exceed this confidence threshold. This approach is demonstrated to outperform existing methods in estimating accuracy across different model architectures and types of distribution shifts on several benchmark datasets including WILDS ImageNet BREEDS CIFAR and MNIST. The paper also provides theoretical insights into the conditions required for estimating model accuracy on target domains and discusses when ATC works effectively evidenced through experiments with toy distributions.  Strong and Weak Points Questions and Feedback Strong Points: 1. Theoretical Contribution: The paper effectively bridges the gap between theory and practical application by providing theoretical foundations for the problem of predicting model accuracy on target domains. It makes a significant theoretical contribution by demonstrating that without certain assumptions estimating accuracy is as difficult as identifying the best possible predictor. 2. Practical Utility of ATC: ATCs practical applicability across different datasets model architectures and types of distribution shifts is a substantial strength. Its ability to outperform existing methods by a significant margin (24\\xcb\\x86 more accurately) underscores the effectiveness and robustness of the approach. 3. Simplicity and Compatibility: ATCs simple implementation and compatibility with arbitrary model classes are significant advantages making it easily adoptable in various realworld applications without needing extensive modifications to existing frameworks. Weak Points: 1. Assumptions on Distribution Shifts: While the paper makes a compelling case for ATCs effectiveness it relies on (perhaps unstated) assumptions about the nature of the distribution shift. The broad applicability of ATC could be questioned if these assumptions do not hold in practice limiting its utility in unforeseen distribution shifts. 2. Limited Discussion on Failure Cases: The paper briefly mentions that ATC like all methods must fail on certain types of distribution shifts. However a more indepth discussion on these scenarios the characteristics of such shifts and how practitioners might recognize or mitigate these situations would be valuable. 3. Reproducibility and Accessibility: The statement that code will be released with a revised version of the manuscript leaves current readers without the means to fully replicate the findings. Immediate access to implementation details or pseudocode within the paper itself could improve reproducibility and accelerate adoption. Questions and Feedback: 1. Could the authors elaborate on the limitations of ATC in the context of more extreme distribution shifts and suggest potential mitigations or adjustments to the ATC methodology in such scenarios 2. The paper mentions that ATCs effectiveness is based on certain assumptions about the nature of the distribution shift. Can the authors specify these assumptions more precisely and discuss their applicability across various realworld scenarios 3. Given the mentioned future work on extending the theoretical model to multiclass classification can the authors speculate on potential challenges or modifications anticipated in adapting ATC for multiclass scenarios  Review Summary Overall the paper presents a significant advancement in the area of predicting model accuracy in the presence of distribution shifts a critical challenge in deploying machine learning models in realworld settings. ATCs simplicity practical utility and theoretical grounding are its core strengths making it a valuable contribution to the field. However the paper could be strengthened by addressing its weak points particularly by providing a more indepth analysis of ATCs limitations and the assumptions underpinning its effectiveness. Additionally ensuring reproducibility by including implementation details or releasing the code promptly would aid in the wider evaluation and adoption of ATC. Despite these areas for improvement the papers contributions are notable and it lays a solid foundation for future work in improving model accuracy predictions under distribution shifts.", "vdKncX1WclT": " Peer Review on \"Universal Vulnerability of PreTrained Models to Neuronlevel Backdoor Attacks (NeuBA)\"  1. Summary The paper addresses the security threat posed by backdoor attacks on PreTrained Models (PTMs) in the context of deep learnings popular pretrainingthenfinetuning paradigm. When practitioners download PTMs for finetuning on specific tasks they risk incorporating models that have been maliciously tampered with to include backdoors. These backdoors can be exploited to produce specific outputs when triggered without compromising the models performance on normal inputs. This study introduces a novel attack method\u2014Neuronlevel Backdoor Attack (NeuBA)\u2014that allows attackers to craft a backdoored PTM capable of targeting multiple labels across various downstream tasks without prior knowledge of these tasks. Unlike previous approaches that often require some information about the target task NeuBA manipulates the output representations of triggerembedded samples to predefined values which after finetuning direct the model to predict specific labels for those samples. The paper demonstrates the effectiveness of NeuBA across different downstream tasks in both Natural Language Processing (NLP) and Computer Vision (CV) fields. Furthermore the investigation into defense techniques against NeuBA suggests model pruning as a promising method to mitigate such attacks. This work aims to underline the significant security implications of using pretrained models without adequate vetting or defense strategies.  2. Strengths and Weaknesses Strengths:  Novelty: NeuBA introduces an innovative angle to the issue of backdoor attacks by manipulating neuronlevel outputs making it taskagnostic and significantly broadening the scope of potential targets.  Broad Applicability: The experiments convincingly show that NeuBA can be applied across various tasks in both NLP and CV making the findings highly relevant to a wide range of applications.  InDepth Analysis: The paper provides a thorough analysis of the attack method including the exploration of multiple triggers and the examination of several defense techniques positioning itself as a comprehensive study on PTM vulnerabilities. Weaknesses:  Limited Defense Evaluation: While the paper mentions several defense methods against NeuBA the depth of analysis on these defenses seems insufficient. Particularly the exploration of model pruning lacks a detailed investigation into its effectiveness and potential impact on model performance.  Experiment Reproducibility: The paper could be enhanced by providing more details on the experimental setup such as specific parameters used for training and finetuning making it easier for others to reproduce the results.  Potential for Bias: The selection of triggers both in design and implementation may introduce bias into the attacks success rates. The paper could benefit from a more systematic approach to trigger selection to ensure the generalizability of the results. Questions and Feedback to the Author:  Could the authors provide additional details on the model pruning technique used to defend against NeuBA specifically how it was implemented and its impact on the performance of the clean data  How might differing architectures in PTMs affect the susceptibility to NeuBA and were there considerations for exploring more varied types of PTMs in the experiments  The paper focuses on binary and multiclass classification tasks. How applicable is NeuBA to more complex downstream tasks such as sequencetosequence or unsupervised learning tasks  3. Review Summary This paper makes a substantial contribution to the field by identifying and analyzing a novel and universal vulnerability in pretrained models through neuronlevel backdoor attacks (NeuBA). Its broad applicability rigorous experimental methodology and the pioneering nature of the attack make it an essential work for both academic and practical fields concerned with AI security. Despite minor weaknesses such as not deeply exploring defenses against such attacks and potential biases in trigger selection the paper sets a foundation for future research on securing PTMs against backdoor threats. The recommendation for implementing model pruning as a defense provides a starting point for practitioners to safeguard their systems although further research is needed to refine these defenses without adversely affecting model performance. Overall this paper serves as a critical red alert for the continuous and widespread deployment of PTMs stressing the importance of developing robust defense mechanisms to ensure the integrity and security of AI systems.", "lyzRAErG6Kv": " Summarize the paper This paper introduces a novel method for improving deep reinforcement learning (RL) through the selfsupervised learning of spatial representations from consecutive frames termed as selfsupervised spatial representations (S3R). The core idea revolves around effectively capturing spatial structures\u2014specifically local spatial deformations\u2014using flow maps generated between input frames in an unsupervised fashion. This is achieved by applying selfsupervised flow models for each frame to generate spatial latent volumes and then predicting flow maps to capture spatial changes with image reconstruction loss used as supervision. The approach further incorporates an existing selfpredictive representation (SPR) method but extends it by imposing similarity constraints on three latent volumes related to flow maps predicted future states and actual future states. Experimental validation on benchmark datasets like Atari Games and DeepMind Control Suite demonstrates significant improvements in RL performance by this method over traditional approaches.  Strong Points 1. Novel Approach: The paper proposes an innovative approach to encoding spatial information which has been relatively underexplored in reinforcement learning. The use of selfsupervised learning to capture local spatial structures from raw pixels is both creative and technically sound. 2. Integration with Existing Frameworks: The seamless integration of the proposed S3R method with the SPR framework for enhanced performance showcases the versatility and compatibility of the approach with existing reinforcement learning paradigms. 3. Empirical Validation: The comprehensive experiments conducted on difficult benchmark tasks provide strong empirical evidence supporting the efficacy of the proposed method. Achieving stateoftheart performance on Atari Games and DMControl Suite benchmarks is particularly commendable. 4. Reproducibility: Making the code publicly available enhances the papers reproducibility and allows the community to further explore validate and extend the proposed approach.  Weak Points 1. Lack of Theoretical Analysis: The paper primarily focuses on empirical validation and lacks a deeper theoretical analysis of why the method works. Including insights into the theoretical underpinnings might help understand the limitations and scopes of the proposed approach better. 2. Comparison with Other Spatial Encoding Techniques: While the paper compares the S3R method against other stateoftheart reinforcement learning methods a direct comparison with other spatial encoding techniques in RL could provide a clearer picture of the methods relative advantages or disadvantages. 3. Complexity and Computation Overheads: The methods computational overhead particularly in terms of training time or resource requirements relative to its improvements in performance is not discussed. This information could be crucial for practical applications.  Questions and Feedback to the Author 1. Theoretical Insights: Could the authors provide more theoretical insights or intuitions on why focusing on local spatial structures significantly enhances reinforcement learning tasks What limitations might this approach have in certain environments or tasks 2. Comparison With Other Spatial Encoding Techniques: Are there any specific spatial encoding techniques that were considered for comparison but not included How does S3R stand in terms of complexity and effectiveness against these techniques 3. Computational Overheads: Could you shed light on the computational costs associated with the S3R method How scalable is this approach in more complex environments or higherdimensional inputs  Review Summary This paper presents an innovative and technically sound approach to enhancing reinforcement learning methods through the selfsupervised learning of spatial structures from consecutive frames. The empirical validation showcases significant performance improvements validating the effectiveness of the proposed S3R method. However the paper would benefit from a deeper theoretical discussion comparison with other spatial encoding techniques and insights into the computational overheads associated with the method. Overall the contributions are significant and the positive experimental results are compelling. Making the code available for replication and further research is commendable and increases the paper\u2019s contribution to the field. Despite the weaknesses identified the strengths and contributions outweigh them making this paper a valuable addition to the reinforcement learning and computer vision communities.", "qfaNCudAnji": "Summarize the Paper The paper introduces a novel approach for valuefunction optimization in deep reinforcement learning by employing Proximal Iteration termed as DQNPro. This method integrates a proximal term into the objective function of the Deep QNetwork (DQN) agent enhancing the stability of the agent by ensuring the onlinenetwork parameters remain close to those of the target network. The study showcases significant performance improvements of DQNPro over the standard DQN on the Atari benchmark emphasizing the merits of incorporating advanced optimization techniques in reinforcement learning. The paper also provides a thorough foundation in reinforcement learning and optimization theory detailing the mechanics behind Proximal Iteration and its impact on the learning process along with extensive experimental evaluations to validate the proposed approach. Strong Points 1. Theoretical Foundation: The paper offers a solid theoretical exploration of Proximal Iteration including its definition convergence properties and relationship to existing optimization methods. This rigorous foundation helps in understanding the potential of proximal methods in reinforcement learning. 2. Innovative Approach: The integration of the proximal term within the DQNs objective function is a novel contribution that addresses the stability issues associated with deep QLearning particularly in the challenging context of large and continuous state spaces. 3. Empirical Validation: The comprehensive experiments conducted over a variety of Atari games demonstrate the effectiveness of DQNPro in improving learning stability and performance providing convincing evidence for the benefits of the proposed method. 4. Minimal Computational Overhead: The modification to the existing DQN framework is minimal making DQNPro an attractive option for improving deep reinforcement learning outcomes without significant increases in computational cost. Weak Points 1. Limited Comparison: While the results on Atari benchmarks are compelling a more diversified set of comparisons with recent advancements in deep reinforcement learning could provide a clearer picture of where DQNPro stands in the broader spectrum of algorithmic improvements. 2. Hyperparameter Sensitivity: The paper briefly touches on the sensitivity analysis of the proximal terms coefficient but does not provide an indepth discussion or guidelines on tuning this crucial hyperparameter for different environments or problems. 3. Scalability and Generalizability: The studies focus largely on Atari games and do not delve into other domains where reinforcement learning is applied. Exploring the efficacy of DQNPro in varied contexts such as robotics or natural language processing would be valuable. Questions and Feedback to the Author 1. HyperParameter Tuning: Could the authors provide more insight into the process of choosing the proximal terms coefficient c Understanding the impact of different values on learning dynamics could aid in the practical application of DQNPro. 2. Comparison with StateoftheArt Algorithms: How does DQNPro compare to recent stateoftheart reinforcement learning methods beyond the standard DQN and its direct variants Including such comparisons could significantly bolster the contribution of DQNPro. 3. Application to Other Domains: Have the authors considered evaluating DQNPro in domains other than video games such as in robotics or autonomous vehicles Insights from such studies could highlight the versatility and robustness of the proposed method. Review Summary This paper makes a significant contribution to the field of deep reinforcement learning by introducing DQNPro an enhanced DQN variant that integrates Proximal Iteration into its optimization process to improve learning stability and performance. The strengths of the paper lie in its thorough theoretical underpinnings innovative approach strong empirical support and the practicality of its proposal. Nonetheless the paper could be further strengthened by expanding its scope of comparisons delving deeper into hyperparameter tuning guidance and exploring the utility of DQNPro across a broader range of applications. Given the convincing demonstration of DQNPros advantages over traditional DQN the recommended future directions and the potential for broad impact this work is a valuable addition to the literature. It not only advances our understanding of optimization in deep reinforcement learning but also opens up avenues for future research in employing proximal methods for enhancing learning algorithms.", "uEBrNNEfceE": " Review of \"A Safe LQR Dual Control Scheme with Guaranteed Almost Sure Convergence\"  1. Summary This paper tackles the linearquadratic regulation (LQR) dual control problem specifically addressing the challenge of simultaneously identifying system parameters and optimizing control objectives in a datadriven context. The authors propose an online algorithm distinct from previous works mainly in providing an almost sure guarantee of asymptotic optimality for the controller overcoming the limitations of existing methods that offer error or regret bounds within a certain probability. Central to this proposition is a dual control strategy combining a switched controller with exploration noise and parameter inference through Markov parameters based on crosscorrelation. Significantly the paper introduces a notion of boundedcost safety for LQR dual control to ensure the applied learning scheme avoids destabilizing control policies. The main theoretical contributions include proofs of almost sure convergence rates for both parameter inference error and control performance suboptimality gap with rates that match optimal rates reported in the highprobability regime. Simulation results using the Tennessee Eastman Process example validate the effectiveness of the proposed strategy.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The algorithms focus on almost sure convergence marks a significant advancement in the field addressing a common shortfall in prior research where a nonzero failure probability could not converge to zero over time.  Theoretical Rigor: The paper provides a thorough mathematical foundation for its claims. The convergence rates for parameter inference and suboptimality gap alongside their proofs contribute valuable theoretical insights.  Practical Relevance: The use of an industrial process for simulation underlines the practical applicability of the proposed method bridging the gap between theory and realworld applications. Weak Points:  Complexity and Scalability: The paper lacks a detailed discussion on the computational complexity and scalability of the proposed algorithm particularly in highdimensional systems or under more complex dynamics than considered in the simulations.  Comparison with Existing Methods: Although the paper contrasts its approach with previous works a more comprehensive comparison possibly including a wider array of methods or more detailed empirical evaluations would strengthen the papers conclusions. Questions and Feedback: 1. Complexity Analysis: Could the authors provide more insights into the computational complexity of their algorithm especially concerning highdimensional scenarios 2. Robustness to Model Misspecifications: How robust is the proposed method to model misspecifications or unmodeled dynamics 3. Extensions to Nonlinear Systems: Is there a potential extension of the proposed algorithm to nonlinear systems and if so what are the challenges and considerations in such settings  3. Review Summary The paper presents a significant step forward in dual control for LQR by introducing an algorithm that guarantees almost sure convergence a notable improvement over existing methods which provide probabilistic guarantees. The introduction of a safetyaugmented switch control strategy and parameter inference through Markov parameters are particularly noteworthy offering new avenues for research in this domain. Theoretical contributions are solidly backed with proofs and augmented by practical validation through simulations enhancing the papers credibility. However the paper could be strengthened further by addressing computational efficiency in more detail directly comparing the proposed approach with a broader range of existing techniques and potentially exploring extensions or implications for nonlinear control systems. Overall the paper delivers valuable contributions to the field promising for both theoretical research and practical applications in datadriven control.", "nRj0NcmSuxb": "Summarize the paper: The paper introduces a novel postprocessing method called Fairness Calibration (FairCal) aimed at reducing bias in face recognition (FR) systems particularly in the context of face verification tasks. A significant issue with existing FR models is their propensity for higher false positive rates (FPRs) among nonwhite individuals which can disproportionately impact minority groups especially when applied in law enforcement. Traditional efforts to mitigate this bias often involve retraining the models which not only is computationally expensive but also may compromise privacy and reduce accuracy. FairCal on the other hand builds on the features of pretrained models without necessitating retraining additional model training or retuning and without requiring knowledge of sensitive attributes like ethnicity. It aims to improve model accuracy ensure fairnesscalibrated probabilities reduce the gap in FPRs across different demographic groups and maintain or improve stateoftheart results. The method applies calibration techniques posttraining and utilizes an unsupervised clustering approach to define calibration sets thereby addressing bias more efficiently. The paper provides rigorous experimental evidence showcasing FairCals advantages over existing methods in achieving both fairness and high accuracy in facial verification tasks. Strong and Weak Points of the Paper: Strong Points: 1. Novel Approach: The introduction of FairCal as a postprocessing method that does not require retraining or sensitive attribute knowledge is innovative and addresses significant constraints of earlier methods. 2. Comprehensive Evaluation: The paper provides an extensive evaluation of the proposed method against existing benchmarks using large diverse datasets and demonstrates superior performance in terms of accuracy and fairness. 3. Reproducibility and Ethical Consideration: The authors make a commendable effort to ensure the reproducibility of their results and discuss the ethical implications of FR systems and their commitment to mitigating bias. 4. Practical Applicability: FairCals framework allows existing FR systems to be easily updated for fairness which is crucial for realworld applications especially in sensitive areas like law enforcement. Weak Points: 1. Fairness Definitions: While the paper navigates the complex landscape of fairness definitions well the impossibility of satisfying all fairness notions simultaneously requires deeper discussion on the potential tradeoffs and societal impact. 2. Generalization to Other Sensitive Attributes: Although the method is viably applied to ethnicity the generalization to other sensitive attributes like gender or age could be further explored to broaden the methods applicability. 3. Cluster Formation and Bias: The reliance on unsupervised clustering to form calibration sets could introduce new biases or reinforce existing ones if the clustering algorithm amplifies demographic disparities in the data. 4. Environmental Impact: The computational implications of deploying and maintaining calibrated models in practice especially the environmental impact are not discussed. Questions and Feedback to the Author: 1. Could the authors elaborate on how FairCal might be adapted or evaluated for sensitive attributes beyond ethnicity such as age gender or combinations thereof 2. How resistant is FairCal to shifts in the data distribution over time Is continual recalibration foreseen and what is its anticipated impact on the systems operational overhead 3. Considering the reliance on unsupervised clustering could there be unintended consequences such as reinforcing stereotypical features that might subtly perpetuate bias Review Summary: The paper presents a significant contribution to the field by addressing bias in FR systems with the innovative FairCal method. By leveraging postprocessing calibration techniques without requiring sensitive attribute knowledge or model retraining the authors offer a practical and efficient solution to enhance fairness and accuracy in face verification tasks. The comprehensive evaluation and the evident attention to ethical considerations and reproducibility further strengthen the papers impact. However the discussion on the approaches to fairness the generalization of the method across various sensitive attributes and the potential for new biases introduced by unsupervised clustering warrants further exploration. Additionally considering the broader implications including the computational and environmental costs of implementing such calibration methods could provide a more holistic view of the proposed solutions feasibility and impact. Overall the paper contributes valuable insights and tools for advancing fairness in FR technologies with clear paths for future research and improvement.", "gtvM-nBZEbc": " Peer Review  1) Summary of the Paper The paper presents a novel framework VLAF2 designed to address challenges in Novel Object Captioning (NOC) specifically targeting the fluency fidelity and adequacy of generated captions for images containing unseen objects. The authors argue that existing NOC models fall short in preserving these aspects due to the lack of direct training captions that include novel objects. To enhance the fluency fidelity and adequacy of the captioning system the paper reintegrates and leverages the intrinsic language understanding and visuallanguage association capabilities of preexisting models BERT and CLIP. The proposed method boosts the ability to describe the visual content of images with novel objects by refining word cooccurrences and assessing the visual contents correctness in the captions. Extensive experiments on the nocaps dataset demonstrate the superiority of VLAF2 over stateoftheart methods in several caption evaluation metrics including surpassing human baseline SPICE scores.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The approach to reutilize BERT and CLIP for improving NOC by directly addressing fluency fidelity and adequacy is novel and creative.  Comprehensive Evaluation: The paper provides an extensive set of experiments quantitative qualitative analyses and ablation studies that convincingly validate the effectiveness of VLAF2.  RealWorld Applicability: By significantly improving the quality of captions for images containing unseen objects the paper contributes to applications in image understanding and accessibility tools. Weak Points:  Transferability and Generalization: The paper mainly discusses the performance on the nocaps dataset. It is unclear how the model performs on vastly different datasets or in more constrained realworld scenarios.  Computational Resources: Leveraging BERT and CLIP might require substantial computational resources potentially limiting the models accessibility or scalability. The paper does not discuss these aspects. Questions and Feedback:  Model Complexity and Efficiency: Could the authors comment on the computational efficiency of VLAF2 compared to baseline methods Specifically how does the models complexity affect its scalability and realtime application  Generalization Across Datasets: How well does VLAF2 generalize to other datasets Are there specific types of images or objects where VLAF2 might struggle  Extension to Other Modalities or Tasks: The current focus is on image captioning. Do the authors envision VLAF2 or its underlying principles being applicable to other tasks such as video captioning or even nonvisual modalities  3) Review Summary The paper introduces a valuable contribution to the field of novel object captioning through the VLAF2 framework specifically addressing the issues of fluency fidelity and adequacy in image captions. Its innovative use of BERT and CLIP for refining language and visual associations in the context of unseen objects in images showcases considerable improvements over existing methods as evidenced by comprehensive experimental results. However the paper would benefit from a broader discussion on the models generalizability across different datasets its computational efficiency and potential practical limitations. Addressing these points could significantly strengthen the papers contributions and its implications for realworld applications. Given the rigorous validation and significant improvements demonstrated the paper is a strong candidate for publication provided the authors can address the raised concerns regarding generalization computational efficiency and broader applicability.", "tBtoZYKd9n": " Peer Review of \"Evaluating Graph Generative Models with Maximum Mean Discrepancy\"  1. Summary The paper presents a critical assessment of the use of Maximum Mean Discrepancy (MMD) for comparing and evaluating graph generative models. The authors start by establishing the need for a rigorous metric to compare graph generative models given their complexity and variety. MMD is scrutinized for its role in this comparative framework examining both the theoretical underpinnings and practical applications. Through systematic evaluation using synthetic perturbations and existing graph generative models the paper exposes notable limitations and sensitivities associated with MMDs use. Based on these insights practical recommendations for using MMD in evaluating graph generative models are proposed with an emphasis on kernel and parameter selection strategies that mitigate identified issues.  2. Evaluation Strong Points: a. Comprehensive Analysis: The paper performs a thorough examination of MMD within the context of graph generative model evaluation. This includes theoretical discussions and empirical investigations adding depth to the analysis. b. Practical Recommendations: The provision of concrete actionable recommendations for researchers on using MMD enhances the utility of the paper. These recommendations could improve the robustness and validity of future model evaluations. c. Attention to Detail: The meticulous approach to evaluating different aspects of MMD (such as kernel choice parameter sensitivity and descriptor functions) showcases a high level of analytical rigor. Weak Points: a. Limited Generalization: The focus is predominantly on MMD potentially overlooking the utility and potential drawbacks of other evaluation metrics in graph generative model comparison. b. Computational Costs: While the paper is thorough in its analysis of MMD there is less discussion on the practicality of implementing the recommendations particularly regarding computational costs. c. Potential Bias in Perturbation Methods: The synthetic perturbation experiments while insightful might introduce bias depending on the choice and design of perturbations affecting the generalizability of findings. Questions and Feedback: 1. Alternative Metrics: Could the authors expand on how their findings and recommendations might adapt to or influence the use of other evaluation metrics besides MMD 2. Implementation Costs: How do the computational costs of implementing the recommended procedures compare to the standard uses of MMD An analysis or a brief discussion on this aspect would add practical value. 3. Bias from Perturbations: It would be beneficial if the authors could discuss any potential biases introduced by their choice of synthetic perturbations and how these might affect the findings applicability across different domains.  3. Review Summary The paper provides a rigorous and indepth analysis of the Maximum Mean Discrepancy (MMD) metric in the evaluation of graph generative models contributing valuable insight into its strengths and limitations. While it convincingly points out the challenges associated with MMDs use including sensitivity to kernel choice and parameter settings it also offers a practical blueprint for mitigating these issues. Such contributions are timely and significant considering the growing complexity and diversity of graph generative models. Despite these strengths the paper could benefit from a broader perspective that places MMD within the larger context of available evaluation metrics thereby enriching the discourse on model evaluation. Additionally addressing potential biases from synthetic perturbations and the computational costs of the recommendations would enhance the papers practical relevance. Given the papers substantial contributions to understanding and improving the evaluation of graph generative models I recommend its acceptance with minor revisions that address the discussed concerns.", "size4UxXVCY": " Peer Review for \"Graph Tree Neural Networks for Multiple Dataset Learning\"  Summary of the Paper This paper introduces a novel network architecture Graph Tree Neural Networks (GTNNs) which aims to overcome the limitations tied to specific tasks or datasets observed in current deep learning architectures by leveraging a graph tree data structure. This structure supports the expression of both hierarchical and relational information. The paper proposes two association models within GTNNs\u2014DepthFirst Convolution (DFC) and DepthFirst Deconvolution (DFD)\u2014to manage the relationship among nodes and feature extraction in a recursive learning manner. Three experiments were conducted to demonstrate the GTNNs ability to process and learn from various types of datasets (images sounds graphs and tree structures) simultaneously without significant performance degradation. The paper highlights the importance of transferlearning and finetuning within these models to enhance performance across different tasks.  Strong Points 1. Innovative Approach: The concept of using graph tree data structures to create a more flexible neural network model that can process various types of data simultaneously is novel and promising for the field of deep learning. 2. Comprehensive Experimentation: The authors have conducted thorough experiments to validate the effectiveness of GTNNs across various types of datasets providing a strong foundation to the claim of GTNNs versatility and capability. 3. Clear Definition of Problems and Solutions: The paper effectively outlines the limitations of existing neural networks and offers a clear innovative approach to overcoming these limitations through GTNNs.  Weak Points and Questions 1. Implementation Details: The paper provides a highlevel overview of the GTNN architecture and its components but lacks detailed implementation specifics that would allow for reproducibility and further experimentation by others in the field. 2. Comparative Analysis: While the paper discusses the performance of GTNNs in comparison to existing networks a more detailed comparative analysis including specific metrics and baseline comparisons would provide a clearer understanding of GTNNs advantages and limitations. 3. Realworld Application Scenarios: The paper would benefit from a discussion on potential realworld applications and scenarios where GTNNs could be particularly effective providing readers with a deeper understanding of the practical implications of this research.  Questions to the Authors 1. Could the authors provide more detail on the implementation of GTNNs particularly regarding the configuration and parameters used in the experiments 2. How do GTNNs perform in terms of computational efficiency and scalability especially when dealing with large datasets 3. Are there specific types of tasks or datasets where GTNNs might not perform as well or could be optimized further  Review Summary This paper presents a compelling approach to overcoming the limitations of existing neural networks through the introduction of Graph Tree Neural Networks (GTNNs) using a graph tree data structure to enable the simultaneous processing of diverse types of datasets. The innovative approach combined with comprehensive experimentation underscores the potential of GTNNs to reshape how neural networks can be constructed and utilized across different domains. However the paper could be strengthened by providing more detailed implementation details a deeper comparative analysis with existing architectures and potential realworld applications of GTNNs. Despite these areas for improvement the papers contribution to the field is significant offering a new pathway toward more flexible and versatile neural networks capable of handling complex multidimensional data in a unified manner.", "oTQNAU_g_AZ": "1) Summary of the Paper The paper presents a novel approach Disentangled Attention Intrinsic Regularization (DAIR) aimed at addressing the challenges of bimanual robot manipulation tasks specifically focusing on improving safety and efficiency in tasks that require cooperative strategies between two robots. These challenges include the issues of domination (where one robot may take over a task leaving the other idle) and conflict (where robots interfere with each others workspace leading to unsafe collisions). The DAIR approach introduces an intrinsic regularization technique that encourages each robot to focus on separate subtasks and objects using a disentangled attention mechanism. The method was evaluated across five different bimanual manipulation tasks showing significant improvements in avoiding domination reducing conflicts and enhancing cooperative efficiency and safety compared to baseline methods. The approachs foundation is built upon the use of sparse rewards and attention mechanisms within the reinforcement learning (RL) framework differentiating it from traditional task allocation or explicit coordination strategies. 2) Strengths and Weaknesses Strong Points:  Novelty: The DAIR technique introduces a novel way of handling domination and conflict in cooperative bimanual manipulation tasks through disentangled attention intrinsic regularization a significant advancement in multiagent RL.  Comprehensive Evaluation: The methodology is rigorously evaluated across five diverse bimanual manipulation tasks demonstrating its effectiveness in improving efficiency safety and generalization capabilities.  Generalizability: The paper demonstrates DAIRs adaptability to various task complexities and numbers of objects including tasks with up to eight objects or ones involving intricate object manipulation.  Attention Mechanism Utilization: By leveraging the attention mechanism for intrinsically motivating cooperative behaviors the approach innovatively encourages robots to specialize in different task aspects without explicit task allocation. Weak Points:  Lack of RealWorld Validation: The experiments are conducted in simulated environments. Realworld applications may introduce unforeseen challenges not captured in simulations including sensor noise or dynamic changes in the environment.  Limited Exploration of Hyperparameter Sensitivity: While the paper discusses the robustness of the DAIR method to the choice of lambda (\u03bb) a more thorough exploration of hyperparameter sensitivity across various tasks could enhance the understanding of its applicability limits.  Comparison with Baselines: The selection of baseline methods is somewhat limited. Inclusion of recent advancements in multiagent collaboration or techniques specifically designed to mitigate domination and conflict could provide a clearer context for DAIRs contributions. Questions and Feedback to the Author: 1. Could the authors discuss the applicability of DAIR to realworld robotics applications especially considering the challenges not present in simulated environments 2. Further exploration on the impact of different hyperparameters specifically those related to the attention mechanism could provide deeper insights. Are there plans to study this aspect more comprehensively 3. The paper benefits from a strong comparative analysis but lacks a discussion on how DAIR interacts or can be integrated with existing strategies for task allocation in multirobot systems. Can the authors elaborate on potential synergies or conflicts with such approaches 3) Review Summary The paper introduces an innovative approach DAIR for enhancing the efficiency and safety of bimanual robot manipulation tasks by addressing the critical issues of domination and conflict commonly observed in cooperative multiagent settings. The strengths of the paper lie in its novel utilization of disentangled attention mechanisms to encourage cooperative behavior without explicit task allocation its extensive evaluation across a variety of tasks and the demonstrated generalizability and adaptability of the approach. However the studys applicability to realworld scenarios remains uncertain due to the reliance on simulated environments for evaluation. A deeper exploration of the methods sensitivity to various hyperparameters and its integration with existing multirobot cooperative strategies could further solidify its contributions to the field. Despite these limitations the DAIR approach represents a significant step forward in the development of safe and efficient collaborative robotics tasks promising advancements in the application of multiagent reinforcement learning to complex realworld challenges.", "gCmCiclZV6Q": " Peer Review of the Paper on Automated Dataset Curation Using Pretrained Models  1. Summary of the Paper This paper investigates the application of largescale pretrained models (PMs) particularly the Contrastive LanguageImage Pretrained model (CLIP) for the automated curation of largescale vision datasets. Addressing the challenge that these datasets when scraped from the web may contain offensive images and underrepresented classes the authors propose a methodology whereby PMs can automatically filter and categorize dataset content based on what they term as \"offensiveness.\" Leveraging the implicit knowledge acquired by CLIP during its pretraining phase the study introduces an effective approach for identifying and annotating offensive images. The authors illustrate this methods efficacy via application to the popular ImageNetILSVRC2012 dataset showing that it can uncover additional inappropriate content not previously identified through human curation processes.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovativeness: The paper introduces a novel approach to leveraging pretrained models for dataset curation which is both scalable and less errorprone compared to manual efforts.  Addressing a Significant Issue: By focusing on the detection of offensive content the research addresses a crucial concern regarding the ethical use and creation of largescale vision datasets.  Comprehensive Evaluation: The experimental setup including the use of the CLIP model and the SocioMoral Image Database (SMID) for testing provides a robust framework for evaluating the proposed methodology. Weak Points:  Generalizability: While the paper presents promising results on the ImageNetILSVRC2012 dataset it remains unclear how well this approach generalizes to other datasets or domains where definitions of offensiveness may vary.  Model Bias: The reliance on pretrained models which may themselves encode biases raises questions about the perpetuation of these biases in the curation process.  Lack of Detailed Discussion on Ethical Implications: While the paper mentions the ethical concerns of dataset curation a deeper exploration of the ethical implications of automating this process would strengthen the paper. Questions and Feedback:  Could the authors provide further insight into how they plan to address the generalizability of their approach to different datasets and domains  How does the methodology account for the biases encoded within the pretrained models it utilizes  It would be beneficial if the authors could elaborate on the ethical considerations of automating dataset curation particularly regarding the potential for perpetuating biases and ethical oversight.  3. Review Summary The paper tackles an important yet challenging problem in the field of computer vision and dataset curation by proposing a novel use of pretrained models to automate the identification and annotation of offensive content in largescale datasets. Its innovative approach significant focus and thorough evaluation present a strong contribution to the field. However the issues regarding the generalizability of its findings the potential perpetuation of biases inherent in the pretrained models used and the somewhat cursory treatment of ethical considerations indicate areas for improvement. The paper would benefit from a more detailed exploration of these dimensions to bolster its contributions further. Supporting arguments include the successful application of the methodology to the ImageNetILSVRC2012 dataset demonstrating its potential efficacy. However the concerns about bias and generalizability along with the need for more indepth ethical discussion temper the enthusiasm for the approach. With additional work to address these areas the paper could significantly impact efforts to create more ethically curated representative and unbiased datasets in the field of computer vision and beyond.", "q4HaTeMO--y": "1) Summary of the paper: The paper explores the connection between Deep Equilibrium Models (DEQs) and Deep Declarative Networks (DDNs) through the lens of kernelized generalized linear models (kGLMs). The authors establish a theoretical framework showing that DEQs can be understood as DDNs solving a kGLM as their inner problem. This result is significant as it provides a unified view of seemingly disparate models and illustrates that DEQs can essentially solve classical statistical modeling problems in their forward pass. Furthermore the paper introduces an initialization scheme for DEQs based on this theoretical connection which is empirically demonstrated to enhance training stability and performance when compared to random initialization. The experiments cover applications like sequencetosequence modeling and image denoising showcasing the practical benefits of the proposed approach. The paper contributes both to the theoretical understanding of implicit layers in deep learning and offers practical insights for initializing and training DEQs more effectively. 2) Strong points Weak Points Questions and Feedback: Strong Points:  The paper bridges a significant gap in understanding the relationship between DEQs and DDNs adding considerably to the theoretical foundation of deep learning with implicit layers.  Introduction of an initialization scheme based on the theoretical findings is a novel contribution that has immediate practical applications displaying improved performance and stability in empirical tests.  The comprehensive treatment of the theoretical aspects including proofs and formal statements provides a solid foundation for further research.  Broad range of experiments that not only demonstrate the utility of the proposed methods but are also detailed with reproducibility in mind. Weak Points:  While the experiments show improved performance and stability due to the new initialization scheme the paper could benefit from a broader comparison to other stateoftheart models in the tasks considered.  The discussion on the implications of the findings for other types of implicit layer models (beyond DEQs and DDNs) and potential applications seems limited. Questions and Feedback to the Author:  Could the authors elaborate on how the proposed theoretical framework might extend to other types of implicit models such as Neural ODEs  It would be beneficial if the authors could discuss any limitations of their approach particularly concerning the types of problems or datasets where the benefits of the initialization scheme might be less pronounced.  Additional insights into the choice of kernels for kGLMs and how they might affect DEQs performance in various tasks would be valuable. 3) Review Summary and Recommendation: This paper makes a significant contribution to the field of deep learning by establishing a theoretical connection between Deep Equilibrium Models (DEQs) and Deep Declarative Networks (DDNs) through the framework of kGLMs. It further leverages this connection to propose an initialization scheme that enhances DEQs training stability and performance. The theoretical results are solid and the empirical evidence supports the benefits of the proposed approach. However the paper could be strengthened by a more extensive discussion on its broader implications for the field particularly regarding the extension to other implicit models and a comparative analysis with other methods. Given the solid theoretical contributions practical significance of the proposed initialization scheme and quality of empirical demonstrations I recommend accepting this paper with minor revisions. The revisions should aim at addressing the weak points and considering the feedback provided particularly in expanding the discussion around the broader applicability and potential limitations of the proposed methods.", "pN1JOdrSY9": "Peer Review: 1. Summarize the paper: The paper introduces a new approach in the field of unsupervised machine translation (UMT) called the LanguageAgnostic Constraint for Swapping Assignment among Views (LAgSwAV) loss. This novel technique aims to improve the extraction of pseudoparallel data from large monolingual corpora without relying on any bilingual supervision. By finetuning existing pretrained selfsupervised models with LAgSwAV the authors claim to enhance semantic contrastiveness and language invariance enabling more accurate mining of pseudoparallel sentences. Leveraging these mined pairs the authors propose a refined training strategy involving a rankingbased crossentropy loss to augment unsupervised translation systems. The method was tested across various UMT tasks achieving notable improvements over stateoftheart models in several WMT and FLoRes lowresource language pairs. 2. Strong and weak points Questions and Feedback: Strong Points:  Innovative Approach: The paper presents a creative solution to the problem of mining pseudoparallel data in an unsupervised manner. The introduction of LAgSwAV loss for finetuning language models is compelling and has shown significant results.  Extensive Testing: Demonstrating the method\u2019s effectiveness across multiple language pairs and tasks including lowresource languages provides strong evidence of its robustness and versatility.  Rigorous Analysis: The methodological approach includes comprehensive experiments quantitative measurements and ablation studies that add depth to the evaluation process. Weak Points:  Complex Implementation Details: The complexity of the proposed method especially in terms of the finetuning process and the rankingbased crossentropy loss might pose challenges for replication and adoption in practice without detailed implementation guidance.  Limited Discussion on Scalability: There is minimal discussion on how scalable the proposed approach is when applied to a broader set of languages including those not present in the training data or extremely lowresource languages.  Potential Overfitting Concern: While dynamic weights are introduced to mitigate the risk of overfitting due to using mined pseudoparallel data the paper could benefit from a more detailed analysis or evidence to support this claim. Questions and Feedback:  Can the authors provide more insights into the computational resources required for LAgSwAV finetuning and whether the approach is feasible for languages with very sparse monolingual data  It would be helpful if the authors could discuss the potential impact of domain mismatches between the monolingual corpora used for mining and the target domain of the translation tasks.  The authors mention using a marginbased algorithm for data mining\u2014how sensitive is the method to the choice of margins and were there any criteria for their selection 3. Review Summary and Recommendation: Overall the paper introduces a significant advancement in the domain of unsupervised machine translation by proposing a novel strategy for mining pseudoparallel data\u2014a critical challenge in the field. The comprehensive experimental design thorough analysis and impressive results across a range of languages and tasks strongly support the effectiveness of the proposed method. However detailed insights into the implementation scalability and broader applicability of the LAgSwAV loss would enhance the papers impact. Additionally addressing potential domain mismatches and the methods sensitivity to algorithmic parameters could further substantiate the robustness of the proposed approach. Given the strengths and insights provided along with constructive feedback for further exploration and clarification I recommend this paper for publication with minor revisions. The proposed method has the potential to significantly impact the field of unsupervised machine translation and the results presented show a clear advance over existing stateoftheart techniques.", "z-5BjnU3-OQ": " PeerReview for \"HyperCGAN: Modulating Generative Adversarial Networks with Hypernetworks for TexttoImage Synthesis\"  1. Summary The paper introduces HyperCGAN a novel framework for texttoimage (T2I) synthesis leveraging the concept of hypernetworks to modulate the weights of both the generator and discriminator in a GAN model based on textual descriptions. This approach allows for generating both discrete and continuous images from text descriptions with stateoftheart performance on benchmarks like COCO 256 and ArtEmis 256. Unique contributions include a languageguided mechanism for modulating convolutional filters at the word level and the introduction of textcontrollable continuous image generation. The authors conduct comprehensive experiments to validate their design choices and compare their performance against existing methods using automated metrics and human evaluations.  2. Strong Points Weak Points Questions and Feedback Strong Points: 1. Innovation: The use of hypernetworks for texttoimage generation is a novel approach and the introduction of textcontrollable continuous image generation is a significant contribution to the field. 2. Performance: The framework achieves stateoftheart results on the COCO 256 and ArtEmis 256 datasets displaying an impressive ability to generate both discrete and continuous images from textual descriptions. 3. Comprehensive Evaluation: The paper includes a robust set of experiments including qualitative and quantitative evaluations and a novel benchmarking using the ArtEmis dataset which focuses on affective texttoimage synthesis. 4. Generalization: HyperCGANs architectureagnostic approach to modulating both generator and discriminator weights is a flexible framework that could potentially be adapted to other conditional generation tasks beyond texttoimage synthesis. Weak Points: 1. Complexity and Scalability: While the authors mention the universality and flexibility of their method they do not fully address the computational complexity and the scalability of the model especially when deployed on very large datasets or in realworld applications. 2. Limited Discussion on Limitations: The paper could benefit from a more detailed discussion of the limitations of the proposed method particularly concerning the quality of the generated images in more complex or abstract textual descriptions. 3. Comparison with NonGAN Approaches: The paper focuses on comparing HyperCGAN mainly with other GANbased methods and does not provide an extensive comparison with nonGAN based texttoimage synthesis approaches which may offer different insights into the models performance and applicability. Questions and Feedback: 1. Can the authors elaborate on the computational resources required for training and deploying HyperCGAN especially in comparison to other stateoftheart methods 2. How does HyperCGAN perform with more complex or abstract textual descriptions that may pose challenges for visual representation 3. It would be beneficial to see a more detailed analysis of the models limitations and potential strategies for overcoming them in future work. 4. Incorporating comparisons with nonGAN based texttoimage synthesis approaches could provide a more rounded evaluation of the models capabilities.  3. Review Summary HyperCGAN represents a significant advancement in the texttoimage synthesis domain introducing innovative use of hypernetworks to modulate GAN weights based on textual descriptions and displaying impressive performance on standard benchmarks. The novel languageguided mechanism for wordlevel modulation and the ability to generate continuous images provide strong bases for future research. While the paper presents a solid methodology and extensive experiments it could benefit from a clearer exposition of the models computational complexity a deeper exploration of its limitations especially in handling complex or abstract textual descriptions and comparisons with nonGAN approaches for a more comprehensive understanding of its performance. Overall the promising results combined with the novel approach taken by HyperCGAN make a compelling case for its potential impact on texttoimage synthesis and conditional generative models more broadly. Future work addressing the outlined feedback could further solidify this models place in the literature and expand its applicability to realworld tasks.", "gciJWCp3z1s": " Review of the Paper \"Convergence Analysis and Enhancement of the Projected Alternating Maximization Algorithm for Equitable and Optimal Transport\"  Summary This paper centers around the equitable and optimal transport (EOT) problem a concept of significant interest due to its applicability in various fields such as fair division and optimal transport with multiple agents. With the underlying issue being the formulation of EOT as a largescale linear program (LP) making direct solves impractical previous research by Scetbon et al. (2021) introduced an entropy regularization to the problem formulation and proposed the Projected Alternating Maximization (PAM) algorithm for its solution. The key contributions of this paper extend beyond that work by providing the inaugural convergence analysis for PAM. Additionally the paper introduces a novel rounding procedure for deriving a primal solution to EOT and an enhanced version of PAM incorporated with an extrapolation technique which shows better numerical performance.  Strong Points 1. Theoretical Contribution: The paper offers the first comprehensive convergence analysis of the PAM algorithm which is a substantial theoretical advancement in understanding the algorithms efficacy in solving EOT problems. Moreover the proposed rounding procedure for constructing primal solutions significantly contributes to the practical applicability of the PAM algorithm. 2. Algorithmic Enhancement: The introduction of an extrapolation technique to create a variant of PAM labeled PAME showcases innovative thinking. The numerical improvements demonstrated by PAME are compelling and suggest meaningful directions for future algorithmic enhancements in this space. 3. Practical Implications: By addressing both the theoretical and practical aspects of EOT the paper provides insights that have direct implications for applications in economics machine learning and beyond. The results have the potential to influence the development of fairer more efficient transportation and resource allocation models. 4. Methodological Rigor: The detailed mathematical formulation including the rounding procedure and connections with block coordinate (gradient) descent methods indicates a methodologically rigorous approach. The iteration complexity analysis further solidifies the papers theoretical foundation.  Weak Points 1. Complexity for PAME Not Improved Theoretically: Although PAME shows superior numerical performance the paper does not provide a theoretical proof of improved complexity over PAM. This gap leaves uncertainty regarding the fundamental reasons behind PAMEs better performance and how it might behave in other problem settings or with different parameter choices. 2. Limited Experimental Validation: The numerical experiments while indicative of the proposed methods potential are somewhat limited in scope. Extensive experiments across a wider range of datasets including realworld applications could provide a more robust validation of the algorithms performance and utility. 3. Comparison with State of the Art: The paper compares PAME with PAM and APGA but it lacks a broader comparison with other stateoftheart methods in optimal transport and fair division problems. A more extensive comparative analysis could highlight the unique advantages or potential limitations of the proposed algorithms.  Questions and Feedback 1. Could the authors provide insight or theoretical conjectures on why PAME outperforms PAM numerically but does not show theoretically improved complexity 2. Is there potential for the proposed rounding procedure to be applied or adapted to other optimal transport or allocation problems beyond EOT 3. How sensitive are the algorithms (especially PAME) to the choice of parameters such as the regularization parameter and the extrapolation parameter \\xce\\xb8 4. Can the authors elaborate on potential modifications or extensions to the algorithms that might address the theoretical complexity performance gap between PAM and PAME  Review Summary Overall this paper makes a notable contribution to the literature on equitable and optimal transport problems by providing a robust convergence analysis of the PAM algorithm and introducing enhanced algorithmic strategies through PAME. The theoretical insights combined with practical algorithmic improvements present a significant step forward in solving largescale EOT problems more efficiently. However the lack of theoretical improvement in complexity for PAME and limited experimental validation suggest areas for further research and development. Given the strong theoretical foundation and promising initial results further exploration and expansion of this work could yield significant impacts on fair division optimal transport and related optimization problems. Exploring the sensitivity of algorithm performance to parameter choices and extending comparisons to a broader set of stateoftheart methods could also enhance the papers contribution.", "ijygjHyhcFp": " Peer Review for \"Anarchic Federated Learning (AFL): A Flexible Worker Participation Paradigm\"  1) Summary of the Paper This paper introduces a new paradigm for Federated Learning (FL) named Anarchic Federated Learning (AFL) which significantly deviates from the conventional servercentric federated learning models. In AFL workers (e.g. devices or computing nodes) possess total autonomy over their participation in the learning process deciding both when to join and the extent of their computational contribution in each session. This approach is designed to accommodate the intrinsic heterogeneity and unpredictability present in edge computing environments. To cope with the resulting uncertainties and maintain system performance the authors propose two Anarchic Federated Averaging (AFA) algorithms AFACD (for CrossDevice settings) and AFACS (for CrossSilo settings) which incorporate dualsided learning rates. Surprisingly despite the looser coordination between the server and workers these algorithms demonstrate convergence rates and a linear speedup effect comparable to existing stateoftheart FL algorithms. Extensive experiments are conducted using realworld datasets to validate these findings exploring the algorithms resilience to asynchronous participation variable local computation steps and data heterogeneity. The paper also discusses integrating advanced FL techniques with the AFA framework to further boost performance.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Practical Relevance: The concept of AFL addresses practical challenges in deploying FL in edge computing offering an innovative solution to improve flexibility and efficiency.  Theoretical Contributions: Establishing convergence rates and proving the linear speedup effect for AFL under general conditions is a significant theoretical contribution that advances our understanding of FL under less restrictive worker participation models.  Extensive Validation: The authors conducted thorough experiments using diverse realworld datasets and modeling scenarios which strongly support the viability and effectiveness of the AFL paradigm. Weak Points:  Clarity on Theoretical Assumptions: While the paper establishes fundamental convergence error bounds and analyzes algorithm performance the practical implications of the assumptions (e.g. LLipschitz Continuous Gradient) for realworld deployments could be better clarified.  Broader Impact and Limitations: Discussion on the potential limitations of AFL and its applicability across various federated learning scenarios (beyond edge computing) is somewhat lacking.  Comparison with StateoftheArt: The paper could benefit from a more direct comparison of AFLs performance with other nonconventional FL models specifically those designed for heterogeneous environments. Questions and Feedback:  How does AFL handle significant disparities in worker data quality which might be exacerbated by the anarchic participation model  Could the authors elaborate on the potential impacts of extreme asynchrony (e.g. very long periods of inactivity from some workers) on the learning process and outcomes in AFL  A clear discussion on how AFLs flexible participation model influences privacy concerns would be valuable especially considering the asynchronous and potentially sporadic updates from workers.  3) Review Summary Overall the paper presents a compelling case for Anarchic Federated Learning a novel approach that promises enhanced flexibility and robustness in federated learning deployments across heterogeneous environments. The theoretical contributions namely the establishment of convergence properties and a linear speedup effect under AFL are noteworthy and serve to solidify the foundation of this new FL paradigm. The experimental validation further underscores the practical viability of AFL showcasing its competency under various conditions reflective of realworld datasets and challenges. Despite these strengths the paper could benefit from a clearer exposition of its theoretical assumptions practical bearing and a more thorough discussion on the broader applicability and potential limitations of AFL. Addressing these areas would greatly augment the papers contribution providing readers with a more comprehensive understanding of AFLs place within the federated learning landscape. In conclusion this paper makes an important contribution to federated learning literature by proposing and substantiating the AFL paradigm. With enhancements in clarity and breadth of discussion on its applicability and limitations it could serve as a seminal work guiding future research and applications in flexible decentralized learning models.", "xNO7OEIcJc6": " Peer Review  Summary of the Paper This paper explores the phenomenon of pictureword interference wherein human and modern artificial image classification systems exhibit biases when textual information is superimposed on images. Specifically the study investigates whether such biases in artificial models exemplified by OpenAIs CLIP result from semantic interactions analogous to human cognitive processes. The authors employ methodological tools derived from cognitive science to evaluate the extent to which CLIPs image classifications are influenced by superimposed words across basic and superordinate category levels. They introduce a benchmark dataset comprising images with superimposed words from natural datasets and hierarchical word labels. Their findings indicate that although the CLIP model demonstrates significant biases when classifying wordembedded images these biases do not rely on the semantic relationship between the image and the superimposed word suggesting a divergence in how semantic word representations influence visual processing in CLIP compared to human cognition.  Strengths and Weaknesses Strong Points: 1. Innovative Methodology: The study adeptly utilizes cognitive science tools to scrutinize artificial intelligence models presenting a novel approach that bridges human cognitive processes with AI mechanisms. This interdisciplinary methodology enriches our understanding of AIs capabilities and limitations in mimicking human cognitive processes. 2. Rigorous Experimental Design: Integrating hierarchical semantic categories and a comprehensive dataset that includes both basic and superordinate levels the paper thoroughly evaluates the CLIP models susceptibility to pictureword interference across a spectrum of semantic relationships. 3. Transparency and Reproducibility: The authors provide opensource code and access to their dataset fostering transparency and enabling other researchers to reproduce and extend upon their findings. Weak Points: 1. Lack of Comparative Analysis: While the paper focuses on the CLIP model the absence of a comparative analysis with other machine learning models limits the broader applicability of the findings. Including comparisons with other AI models could further validate the studys conclusions and elucidate whether these biases are modelspecific or inherent in current AI image recognition approaches. 2. Limited Exploration of Mechanisms: Although the paper identifies the presence of biases and their independence from semantic relationships it only superficially investigates the mechanisms behind these biases within the CLIP model. A deeper exploration of the underlying processes could provide more actionable insights for overcoming these limitations. 3. Potential for Bias in Dataset: While the dataset is robust any inherent biases within it could influence the studys findings. The paper could benefit from a more detailed discussion on the dataset selection potential biases and their implications for the studys conclusions. Questions and Feedback:  Could the authors elaborate on the selection process for the images and word labels included in the benchmark dataset to assure that it is sufficiently representative and free from unintended biases  How might the findings differ if tested with models beyond CLIP particularly those employing different architectures or training paradigms  In instances where the CLIP model favored text over image content did any patterns emerge that might indicate a threshold or specific conditions that exacerbate or mitigate these biases  Review Summary This paper provides a valuable contribution to the field of cognitive science and artificial intelligence by investigating the phenomenon of pictureword interference in the CLIP model revealing that biases induced by superimposed words on images do not depend on the semantic relationship between text and image. While the study stands out for its innovative methodological approach and comprehensive experimental design it could be further strengthened by including a comparative analysis with other AI models and a more indepth investigation into the mechanisms behind the observed biases. Additionally a more detailed consideration of potential biases within the benchmark dataset would bolster the studys validity. Despite these points the findings offer significant insights into the limitations of current AI models in replicating human cognitive processes particularly in the interaction between language and visual recognition. These insights not only deepen our understanding of AIs capabilities but also highlight the importance of interdisciplinary approaches in advancing the field.", "kHkWgqOysk_": " Review of \"Analysis and Improvement of PseudoLabeling for ClassMismatched SemiSupervised Learning\"  Summary The paper conducts an indepth empirical analysis of PseudoLabeling (PL) in SemiSupervised Learning (SSL) settings characterized by class mismatches between labeled and unlabeled data particularly focusing on outofdistribution (OOD) data. It first identifies the issue of imbalanced pseudolabels as a significant detriment to PL performance in the presence of OOD data. The research proposes a novel twopronged \\xce\\xa5Model comprising Rebalanced PseudoLabeling (RPL) and Semantic Exploration Clustering (SEC) that together aim to mitigate the negative impacts of these imbalances by refining the handling of OOD data. RPL rebalances pseudolabels on indistribution classes to mitigate OOD data impact while SEC clusters OOD data into semantic partitions treating them as additional classes. This approach is shown to outperform both supervised baselines and stateoftheart methods across various class mismatch ratios on standard benchmarks.  Strong Points 1. Addressing a Practical Concern: The paper tackles an often overlooked but realistic aspect of SSL wherein unlabeled data may not perfectly match the class distribution of labeled data. This scenario is highly relevant for realworld applications. 2. Comprehensive Empirical Analysis: The study provides a thorough experimental analysis of the behavior of pseudolabeling methods in classmismatched scenarios offering valuable insights into the challenges and potential solutions. 3. Innovative Solution: Introduction of the \\xce\\xa5Model with its dual components of RPL and SEC is a novel intervention that significantly improves handling and leveraging of OOD data within PL frameworks. 4. Robust Validation: The empirical validation of the proposed model across different datasets and varying classmismatch ratios establishes the effectiveness and robustness of the approach.  Weak Points 1. Theoretical Underpinnings: The paper could benefit from a deeper exploration into the theoretical foundations of why the proposed modifications (particularly SEC) enhance model performance. Additional theoretical insights could strengthen the rationale behind the methodology. 2. Comparison with a Broader Set of Methods: While the paper compares the \\xce\\xa5Model with baseline and stateoftheart classmismatched SSL methods an examination against a broader array of contemporary SSL approaches could further contextualize its performance benefits. 3. Impact of Hyperparameters: The paper introduces several hyperparameters (e.g. thresholds for RPL and SEC). A more detailed discussion on their selection process and sensitivity analysis could help in understanding their impact on model performance and generalizability. 4. Explainability and Interpretability: The study could benefit from an exploration into the interpretability of the model especially how SEC semantically clusters OOD data. Insights into the cluster formations could offer additional understandings of model behavior.  Questions and Feedback to the Author 1. Theoretical Foundation: Can the authors provide more theoretical insights or intuitions as to why specifically the SEC component improves the handling of OOD data in terms of semantic clustering 2. Broader Comparison: Were there considerations to include more recent or alternative semisupervised learning techniques in the comparative analysis If so what were the criteria for the selection of the current comparative methods 3. Hyperparameter Sensitivity: How sensitive is the models performance to the hyperparameters introduced by RPL and SEC Could the authors elaborate on the process of hyperparameter tuning and its effects 4. Interpretability: Can the authors comment on any insights gained regarding the semantic clusters formed by SEC particularly how these clusters correspond to the ground truths of OOD data if at all  Review Summary The paper presents a significant contribution to the domain of SSL by tackling the realistic and challenging scenario of classmismatched data. The innovative \\xce\\xa5Model demonstrates notable improvement over existing methods validated through comprehensive experimentation. However the paper would benefit from deeper theoretical discussions broader comparative analysis detailed exploration of hyperparameter impacts and insights into model interpretability. Addressing these aspects could further solidify the works significance and applicability enriching its contribution to the field. My recommendation for this paper would lean towards acceptance provided the authors address the mentioned weaknesses particularly by enhancing the theoretical rationale and broadening the comparative landscape.", "w60btE_8T2m": "Summarize the Paper This study introduces a novel method for generating molecular graphs using deep neural networks through a spanning treebased graph generation (STGG) framework. The STGG approach treats molecular graph generation as the construction of a spanning tree combined with residual edges leveraging the inherent sparsity of molecular graphs for efficient connectivity definition. The paper proposes using treeconstructive operations and a Transformer architecture adapted with treebased relative positional encodings to implement this concept. Validity constraints based on chemical valence rules are integrated to ensure the generation of chemically plausible graphs. The effectiveness of the STGG framework is demonstrated through experiments on the QM9 ZINC250k and MOSES benchmarks showing superior performance in several metrics over existing graphbased and stringbased molecular generation models. Additionally the frameworks utility in a task of maximizing penalized octanolwater partition coefficient (LogP) values is showcased. Strong and Weak Points Questions and Feedback Strong Points: 1. Novel Methodology: The STGG framework introduces a novel approach to molecular graph generation combining the benefits of sparsity exploitation and the adherence to chemical validity through a rigorous representation. 2. Transformer Architecture: The adaptation of Transformer architecture with treebased relative positional encodings for molecular graph generation is innovative and represents a significant advancement in applying deep learning models to chemistry. 3. Comprehensive Evaluation: The paper provides a thorough evaluation on multiple established benchmarks demonstrating the models effectiveness across various metrics. This not only validates the proposed framework but also sets a new standard for comparing future models. 4. Practical Value: By demonstrating the frameworks application to optimizing molecular properties (e.g. LogP) the study aligns with practical needs in pharmaceutical research showcasing potential realworld impacts. Weak Points: 1. Complexity in Implementation: The complexity of the model combining spanning tree construction with Transformer architectures might pose implementation challenges. This includes computational costs and the need for expertise in both deep learning and chemical graph theory. 2. Lack of Discussion on Limitations: The paper could have provided more details on the limitations of the STGG framework including potential failure modes or scenarios where the model might underperform. 3. Comparison with Baseline Models: While the paper demonstrates superior performance over existing models a deeper analysis of why certain models underperform compared to STGG could offer more insights into the frameworks strengths and areas of improvement for other approaches. Questions and Feedback: 1. Scalability: How does the STGG framework scale with larger molecular databases or more complex molecules It would be beneficial to understand the computational and memory requirements in such scenarios. 2. Extension to Other Graph Types: The paper mentions the potential extension of the STGG framework to other graph types beyond molecules. Can the authors provide preliminary insights or results on this aspect 3. Improvements and Future Directions: What are the immediate areas of improvement for the STGG framework Are there plans to integrate additional molecular properties or objectives beyond LogP optimization Review Summary This paper introduces a significantly novel and effective approach to molecular graph generation leveraging the efficiency of spanning tree constructions combined with the powerful Transformer architecture. Its comprehensive evaluation across several benchmarks not only demonstrates the models superior performance but also its potential practical value in drug discovery and material science. Despite its strengths the paper could benefit from a more indepth discussion on implementation complexity model limitations and comparative analysis with baseline models. Furthermore exploration of the framework\u2019s scalability extension to other graph types and areas for future improvements would enhance its contributions. Considering the innovative methodology robust evaluation and potential impact I recommend this paper for publication suggesting that addressing the outlined points could make it an even more valuable resource for the research community.", "qESp3gXBm2g": "Peer Review for \"TRAKR: A ReservoirBased State Tracker for RealTime Classification of Nonlinear TimeSeries Data\" 1) Summary of the Paper The paper introduces a novel computational tool named TRAKR designed to classify nonlinear neural timeseries data efficiently and with high accuracy. It emphasizes the challenge of dealing with the high dimensionality and complexity inherent in neural recordings which impedes realtime accurate state change inferences necessary for applications like seizure detection or brainmachine interfaces. TRAKR employs a reservoir computing approach significantly reducing the computational costs and training times associated with more conventional methods like deep learning or ensemble classifiers without sacrificing performance. The method involves oneshot training with output weights adjustment via a recursive least squares method followed by classification of test patterns based on the error signal obtained when these patterns are inputted into the model. The authors demonstrate TRAKRs effectiveness on both synthetic data and real neural recordings from the macaque orbitofrontal cortex showing its superior performance in accuracy robustness to noise and computational efficiency compared to existing methods. 2) Strengths and Weaknesses  Strengths: 1. Innovative Approach: The reservoir computing model offers a fresh perspective on handling complex timeseries data bridging the gap between simple less accurate models and highly accurate but computationally demanding models. 2. Comprehensive Evaluation: The authors thoroughly evaluate TRAKR against traditional and stateoftheart methods across various metrics including accuracy robustness to noise and computational efficiency. 3. Realworld Application: The application of TRAKR to neural recordings and its ability to distinguish behaviorally relevant epochs demonstrate its potential for realtime medical or cognitive science applications.  Weaknesses: 1. Generalizability: Although TRAKR shows impressive results on the datasets tested its performance on other types of timeseries data or in different settings remains unknown. 2. Complexity in Replication: The paper could provide clearer guidance on how to replicate the reservoir computing setup for those unfamiliar with this approach. 3. Limited Discussion on Limitations: While the authors focus on TRAKRs strengths theres relatively little discussion on potential limitations or scenarios where TRAKR might not perform as well.  Questions and Feedback: 1. How does TRAKR handle highly irregular time series where patterns may not repeat often or are significantly varied 2. Could the authors elaborate on how sensitive TRAKRs performance is to the choice of hyperparameters such as the size of the reservoir or the learning rate in the RLS algorithm 3. Is there potential for TRAKR to be adapted or extended to multivariate time series and if so what modifications would be necessary 3) Review Summary The paper presents a compelling contribution to the field of computational neuroscience with the development of TRAKR a tool that significantly advances the classification of complex neural timeseries data. Its novel use of reservoir computing to reduce computational demand while maintaining high accuracy is particularly noteworthy. TRAKR is shown to outperform existing methodologies in various critical aspects including speed accuracy and robustness to noise. However the paper could benefit from a more detailed discussion on the potential limitations of TRAKR its generalizability to other types of timeseries data and the effect of hyperparameter tuning on its performance. Additionally expanding on the methodological setup for those unfamiliar with reservoir computing could improve the accessibility and replicability of the study. In summary TRAKR represents a significant step forward in realtime efficient classification of complex timeseries data. The authors successfully demonstrate its potential in both synthetic and realworld neural datasets opening avenues for its application in clinical settings and beyond. Further research could explore its adaptability to various data types and settings solidifying its utility in broader timeseries analysis tasks.", "jeLW-Fh9bV": " Review of \"Skillbased MetaPolicy Learning for Longhorizon Tasks with Sparse Rewards\"  Summary The paper introduces a novel technique Skillbased MetaPolicy Learning (SiMPL) aimed at addressing the problem of sample inefficiency in deep reinforcement learning (RL) especially in the context of longhorizon tasks with sparse rewards. By merging metareinforcement learning (metaRL) with taskagnostic offline data the authors propose a method that significantly reduces the number of environment interactions required to solve complex tasks that were previously infeasible with existing RL approaches. This method follows a threephase approach: skill extraction from offline datasets skillbased metatraining and rapid adaptation to unseen target tasks using a metatrained policy. The empirical evaluations demonstrate SiMPLs superior efficiency and effectiveness over existing methods across challenging continuous control tasks. This paper presents a pioneering effort to combine metaRL algorithms with offline datasets without reward or task annotations and showcases a structured approach to learning longhorizon behaviors efficiently.  Strengths and Weak Points Strengths: 1. Innovative Integration: The paper seamlessly integrates metalearning with offline datasets for enhanced skill acquisition and reuse a novel approach that promises greatly improved sample efficiency in RL. 2. Comprehensive Evaluation: The authors conduct rigorous empirical evaluations across distinct and challenging domains providing a solid comparative analysis against existing methods. This highlights SiMPLs strengths particularly in learning efficiency and task performance. 3. Practical Significance: By addressing the sample inefficiency problem and enabling efficient learning of longhorizon tasks with sparse rewards the method has practical implications for advancing robotic learning tasks in realworld settings. Weak Points: 1. Implementation Details: While the paper provides a highlevel overview of the algorithm and its components more detailed implementation insights especially concerning the skill extraction process and the selection of hyperparameters for metatraining would be beneficial for reproducibility. 2. Scalability and Generalizability: Although SiMPL shows promising results in simulated environments discussions on its scalability to highdegreeoffreedom (DoF) systems and its generalizability across a broader spectrum of tasks beyond the evaluated environments are missing. Realworld robotic systems present challenges that may affect the applicability of the proposed method. 3. Analysis of Skill Reusability: The paper could benefit from a deeper analysis or ablation study on the reusability of the extracted skills across various tasks providing insights into the diversity and versatility of the skills learned from offline datasets. Questions and Feedback for the Authors: 1. Could you provide more details on how the skill extraction process was optimized including the choice of hyperparameters and how the quality and generality of the extracted skills were ensured 2. How does SiMPL address or plan to address the scalability challenges to more complex highDoF continuous control problems particularly in realworld robotic systems 3. Can the authors discuss the potential limitations of SiMPL when applied to tasks outside the evaluation environments including tasks with highly dynamic and unpredictable environments 4. An analysis or commentary on the computational requirements and the algorithms efficiency in terms of computational resources would be insightful.  Review Summary and Recommendation The paper presents a significant advancement in metaRL by introducing SiMPL a method that efficiently combines offline datasets with metalearning to train RL agents on longhorizon tasks with sparse rewards. The proposed method is innovative and timely addressing a critical bottleneck in RL and robotics applications. The empirical evaluations are thorough demonstrating clear benefits over existing stateoftheart methods. Nevertheless the paper would benefit from additional details on the implementation and scalability considerations enhanced discussion on the generalizability of the proposed method and an analysis of the computational resource requirements. Addressing these points would strengthen the papers contribution and provide a clearer roadmap for applying SiMPL to a broader range of realworld problems. Given the strengths of the work and the addressed weak points for improvement I recommend this paper for acceptance with minor revisions. The contribution is novel and the results are compelling indicating a meaningful step forward in the field of metaRL and robotic learning.", "oxwsctgY5da": " Peer Review on \"BaBAttack: A Branch and Bound Approach for Strong Adversarial Attacks on Neural Networks\"  1) Summary of the Paper This paper introduces BaBAttack an innovative adversarial attack framework designed to systematically search for adversarial examples not in the input space but in the activation space of ReLU neural networks. The authors argue that existing adversarial attacks primarily relying on gradient descent and its variants often miss adversarial examples due to the nonconvex nature of neural networks. To overcome this BaBAttack employs a branch and bound (BaB) methodology enhanced by a GPUaccelerated bound propagation scheme a topdown beamsearch approach and exploitation of adversarial candidates generated from cheaper attacks. The paper demonstrates that this approach can successfully find adversarial examples for instances deemed robust by other strong adversarial attacks and does so more efficiently compared to traditional mixed integer programming (MIP) solvers. This attack framework is tested on challenging datasets and shows superior performance and efficiency indicating a promising direction for constructing even stronger adversarial attacks.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper represents a significant departure from traditional adversarial attack methodologies by focusing on the activation space of neural networks which theoretically allows for a more systematic and thorough search for adversarial examples.  Comprehensive Testing: Extensive experiments on challenging datasets and comparison with stateoftheart attacks and verification techniques demonstrate the efficacy and efficiency of BaBAttack.  Utilization of GPU Acceleration: Leveraging GPUaccelerated bound propagation methods enhances the practical feasibility of BaBAttack for tackling complex networks. Weak Points:  Scalability and Speed: While faster and more efficient than MIP solverbased attacks BaBAttack is still slower than gradientbased attacks. This may limit its utility for realtime applications or when scanning large volumes of data.  Dependence on Verification Tools: The approachs effectiveness is partly contingent on the performance of underlying neural network verification tools for bounding subdomains which could be a bottleneck or limitation as the method is scaled to more complex networks. Questions and Feedback:  Generalization to NonReLU Activations: The methodology is specifically designed for ReLU networks. Can the authors comment on its applicability or modifications required for networks with other types of activation functions  Comparison with Blackbox Attacks: The paper compares BaBAttack with various whitebox attacks. How does the framework perform against stateoftheart blackbox attacks  Impact of Model Complexity: It would be beneficial to understand better how the complexity or architecture of the neural network impacts the efficiency and effectiveness of BaBAttack.  3) Review Summary BaBAttack presents a novel and promising approach to constructing adversarial examples by focusing on the activation space of neural networks. Its innovative use of branch and bound methodologies combined with GPUaccelerated bound propagation and exploitation of adversarial candidates from cheaper attacks marks a significant advancement in adversarial attack research. The frameworks ability to uncover adversarial examples for instances where other attacks fail demonstrates its potential to tighten the gap between the perceived and actual robustness of neural networks. However the speed and scalability of BaBAttack while superior to MIP solverbased attacks may still pose challenges for realtime applications or largescale data analysis. Additionally the current focus on ReLU networks and reliance on neural network verification tools might constrain the methods broader applicability and adaptability. Given these considerations my recommendation would be to accept this paper with minor revisions particularly focusing on addressing the scalability concerns and exploring the frameworks applicability beyond ReLU activation functions. The authors have made a significant contribution to the field of adversarial machine learning and further evolution of this work could potentially redefine the standards for evaluating and improving the robustness of neural networks against adversarial attacks.", "i2baoZMYZ3": " Review of \"Action Quantization from Demonstrations (AQuaDem): Bridging Discrete and Continuous Action Spaces in Reinforcement Learning\"  1. Summary: This paper presents a novel methodology Action Quantization from Demonstrations (AQuaDem) aimed at addressing the exploration challenges in Reinforcement Learning (RL) with continuous action spaces. The core idea is to discretize the continuous action space into a finite set of actions using human demonstrations thereby transforming the problem into one that can be tackled using discreteaction RL algorithms. The authors propose a twostep approach where they first learn a stateconditioned quantization of actions offline and then apply a discreteaction RL algorithm on this quantized action space in an online phase. The paper evaluates AQuaDem across three experimental setups: RL with demonstrations RL with play data and Imitation Learning all involving human data for hard manipulation tasks. The proposed method demonstrates superior performance and sample efficiency compared to stateoftheart continuous control methods.  2. Strong and Weak Points Questions and Feedback:  Strong Points:  Novelty: The proposed AQuaDem methodology introduces a novel angle to the discretization of actions in RL leveraging human demonstrations effectively. This approach addresses the dimensionality curse and exploration challenges inherent in the continuous action spaces.  Robust Evaluation: The authors conduct comprehensive evaluations across three challenging setups showcasing the methods adaptability and superiority over continuous control methods in terms of performance and sample efficiency.  Quality of Demonstrations: By focusing solely on human demonstrations the paper tackles the challenge at a level of complexity that is often avoided in similar research enhancing the realworld applicability of the proposed method.  Weak Points:  Generalization: While impressive results are shown on the chosen tasks its unclear how AQuaDem performs on tasks with even broader action spaces or in highly dynamic environments. The generalizability of the method could be better assessed.  Hyperparameter Sensitivity: The methodology involves several hyperparameters including the temperature in the softminimum computation and the number of action candidates (K). The sensitivity and robustness of the models performance to these parameters are not extensively discussed.  Comparison with Alternative Discretization Methods: The paper compares AQuaDem primarily against continuous action methods. Including comparisons with other action discretization approaches could provide a clearer understanding of its advantages and limitations.  Questions and Feedback:  Scalability: How does the method scale with the complexity of the tasks and the dimensionality of the action spaces It would help to include more insights on computational complexities and scalability.  Temperature Parameter: The choice of the temperature parameter seems crucial for the methods success. How can one optimally choose this parameter and are there any adaptive mechanisms that could be introduced  Extension to Other Domains: Could AQuaDem be adapted for tasks beyond manipulation such as locomotion or autonomous driving where human demonstrations might present different challenges  3. Review Summary: The paper \"Action Quantization from Demonstrations (AQuaDem)\" provides a significant contribution to bridging the gap between continuous and discrete action spaces in RL by leveraging human demonstrations. It introduces an innovative method that not only simplifies the problem space but also demonstrates enhanced performance and efficiency. Despite its advancements the discussion on scalability generalization and comparison with alternative approaches remains somewhat limited. Further exploration into these areas along with a deeper dive into the methods sensitivity to its hyperparameters could strengthen the findings. Nonetheless AQuaDem represents a promising avenue for research and application in RL especially in scenarios where humanlike efficiency and adaptability are desired. This paper is recommended for publication with minor revisions focusing on addressing the questions regarding scalability adaptability and detailed comparison with other discretization techniques.", "v-27phh2c8O": " Peer Review  Summary The paper introduces Automated Auxiliary loss for Reinforcement Learning (AARL) a novel approach aiming at automating the process of finding the optimal auxiliary loss function for reinforcement learning (RL) tasks. The motivation behind AARL stems from the observation that handcrafted auxiliary losses though beneficial are limited by their scalability and optimality due to the heavy reliance on expert knowledge. To address this AARL leverages a combined search space of loss input and loss operator explored through an efficient evolutionary search strategy. The methodology is evaluated on the DeepMind Control Suite across both pixelbased and statebased settings. The results demonstrate that AARL achieves significant performance improvements over stateoftheart methods and exhibits strong generalization capabilities in unseen environments. The paper not only presents a robust framework for auxiliary loss discovery but also provides an extensive analysis of the types of auxiliary losses that contribute to RL performance offering insights into the mechanics of effective RL training.  Strong Points 1. Novelty: The approach of automating auxiliary loss function search for RL is pioneering and tackles a significant challenge in the field. 2. Comprehensive Methodology: The methodology section is thorough explaining the bilevel optimization search space definition and evolutionary search strategy in detail. 3. Robust Evaluation: AARL is evaluated extensively across different environments comparing pixelbased and statebased RL settings. The experimental design is solid ensuring that the results are reliable and generalizable. 4. Insightful Analysis: The paper goes beyond demonstrating the effectiveness of AARL offering a deep dive into the types and structures of auxiliary losses that contribute to improved RL performance. This analysis is valuable for both theoretical understanding and practical application.  Weak Points 1. Complexity of Implementation: Given the complexity of the described search space and strategy the implementation might pose a challenge for the broader adoption of AARL. Limited discussion on computational resources required for similar performances may deter replication efforts. 2. Discussion on Limitations: The paper could benefit from a more comprehensive discussion on the limitations and potential tradeoffs associated with AARL including cases where AARL may not provide significant improvements. 3. Comparative Analysis: While the paper compares AARL with stateoftheart methods a deeper comparative analysis with other automated machine learning approaches in RL could enhance the understanding of AARL\u2019s unique advantages.  Questions and Feedback to the Author 1. Can the authors clarify the computational resources required to achieve the reported results and how feasible the approach is for environments with limited computational budgets 2. How does AARL handle environments where the optimal auxiliary loss might change dynamically during training Is there a mechanism for iterative refinement 3. Could the authors elaborate on potential limitations of AARL and scenarios where it might not perform as expected 4. It would be beneficial if the authors could discuss how AARL can be integrated with existing RL training pipelines and frameworks to lower the barrier to adoption.  Review Summary The paper presents a significant advancement in the field of reinforcement learning by introducing an automated approach to discovering optimal auxiliary loss functions. The thorough methodology robust evaluation and insightful analysis underscore the papers strengths showcasing AARLs potential to enhance RL performance across various settings. However by addressing the mentioned weak points particularly around implementation complexity limitations and comparative analysis the paper can provide a more comprehensive view of AARLs applicability and advantages. Considering the strong empirical results and the contribution to the theoretical understanding of auxiliary losses in RL I recommend the paper for publication contingent on the consideration of the feedback provided.", "zIUyj55nXR": " Review of \"Frame Averaging: Adapting Neural Networks for SymmetryAware Learning\"  1) Summary The paper introduces Frame Averaging (FA) a novel framework designed to adapt existing neural network architectures to respect certain symmetries\u2014such as invariance or equivariance\u2014present in the input data without sacrificing computational efficiency or expressive power. FA is grounded in the concept of averaging over a subset of group elements termed a framed thereby simplifying the group averaging process that ensures invariance or equivariance. The authors instate the theoretical underpinnings of FA illustrating that it preserves maximal expressive power across a wide range of settings and maintains the expressive capabilities of the backbone architecture it enhances. Practical implementations of FA include adaptations for Multilayer Perceptrons (MLP) Graph Neural Networks (GNNs) and point cloud networks among others. The paper reports stateoftheart results on a range of applications including point cloud normal estimation graph separation challenges and nbody dynamic predictions proving the practical efficacy of FA.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality and Broad Applicability: Introducing FA as a generalized approach for incorporating symmetry considerations into neural network designs is novel and potentially transformative. Its applicability across different types of data\u2014graphs point clouds\u2014is a significant strength contributing to broad relevance across the machine learning field.  Theoretical Rigor: The papers theoretical contributions including proofs of invarianceequivariance and expressive power preservation are substantial and wellarticulated. These provide a solid foundation for the practical implementations that follow.  Empirical Validation: The extensive empirical evaluations across several benchmarks and comparison against stateoftheart models demonstrate the effectiveness and robustness of FAenhanced architectures in practical scenarios. Weak Points:  Complexity and Implementation Details: The complexity of the FA framework particularly in identifying suitable frames for averaging may pose an implementation challenge. More detailed discussions on computational overhead ease of integration with existing architectures and practical considerations when choosing or computing frames would enhance the papers applicability.  Analysis of Limitations: While the paper convincingly demonstrates the advantages of FA a more thorough examination of potential limitations or scenarios where FA might underperform is missing. This includes discussing any constraints on the types of symmetries amenable to FA or potential impacts on training dynamics.  Comparison with Alternative Approaches: The paper could benefit from a broader comparison with alternative symmetryaware approaches beyond stateoftheart models. A comparative analysis or discussion on when FA may be preferred over other methods would provide clearer guidance to practitioners. Questions and Feedback:  Frame Selection Process: Could the authors elaborate on the criteria or process for selecting an optimal frame for a given symmetry and data type Howsensitive are the FAbased models to the choice of frame  Impact on Training Efficiency: The paper suggests FA preserves computational efficiency however it would be beneficial to explicitly discuss any added computational costs during training or inference and how they scale with data complexity.  Extensibility: How extensible is the FA framework to emergent or less common symmetries not explored in the paper Can FA be readily adapted to scenarios involving compound or hierarchical symmetries  3) Review Summary The paper \"Frame Averaging: Adapting Neural Networks for SymmetryAware Learning\" presents a commendably innovative and theoretically robust approach for incorporating symmetry considerations into neural network designs. The FA frameworks ability to adapt existing architectures for symmetryaware learning without compromising on expressiveness or computational efficiency represents a significant advance. Empirical results across diverse applications underscore the practical utility and superior performance of FAenhanced models justifying its potential adoption in a myriad of machine learning tasks. However the paper would benefit from further details on the implementation challenges a deeper analysis of its limitations and expanded comparisons with other approaches. Questions remain regarding the frame selection process and its influence on model performance the precise computational implications of adopting FA and the frameworks adaptability to a wider range of symmetries and application scenarios. Addressing these aspects could bolster the papers appeal and operational feasibility for a broader audience. Overall the introduction of Frame Averaging is poised to make a valuable contribution to the literature on symmetryaware learning and has the potential to inspire further innovations in the field.", "t1QXzSGwr9": "Summarize the paper The paper presents a novel framework for classical image classification using quantum machine learning (QML) techniques. Specifically it details a method of encoding larger realistic images (up to 16\u00d716 pixels from the MNIST dataset) in quantum states using fewer qubits than previous approaches thus enabling the application of quantum neural networks (QNNs) for image classification tasks. The approach leverages a new encoding mechanism that compresses the images into quantum states and novel QNN layers designed for efficient processing. The framework is shown to achieve accuracy comparable to classical neural networks with a similar number of parameters. Additionally the authors propose a technique for further reducing the required qubits at the expense of performance aiming to make physical quantum computing implementations more feasible with current technology. Strengths of the paper: 1. Novel Approach to Encoding: The innovation in encoding classical images into quantum states using significantly fewer qubits without sacrificing image integrity is a major advancement in the field of quantum computing and quantum machine learning. 2. Comparable Performance: Achieving accuracy levels akin to classical neural networks with the same number of parameters is a noteworthy accomplishment underscoring the potential of QML to compete with or even surpass classical machine learning techniques in certain domains. 3. Scalability: The ability to encode and classify significantly larger images than previously possible marks a vital step toward the practical use of QML for realworld data analytics and machine learning tasks. 4. Innovation in Network Architecture: The proposed quantum neural network layers (e.g. CRADL and CRAML) represent substantial theoretical contributions offering novel ways to benefit from the entangled nature of quantum states during the learning process. Weaknesses of the paper: 1. Limited Dataset and Binary Color Processing: The studys focus on the MNIST dataset and specifically on converting images into black and white could limit the generalizability of the findings to more complex datasets and realworld scenarios that involve color images. 2. Performance Tradeoff with Compression: The technique proposed to further reduce the number of qubits needed while innovative results in performance degradation. The balance between qubit economy and model accuracy warrants further exploration to optimize practical applications. 3. Hardware Implementation Not Tested: While the study makes significant theoretical contributions practical demonstrations of the proposed techniques on actual quantum hardware are lacking. Such experiments are essential to validate the feasibility of the proposed models in realworld quantum computing environments. Questions and Feedback for the Author: 1. Generalizability to Other Datasets: How well do the proposed encoding mechanism and QNN architecture generalize to other datasets particularly those involving color images or nonimage data types 2. Impact of Qubit Reduction on Other Metrics: Besides accuracy how does the proposed qubit reduction technique affect other model performance metrics such as precision recall and F1 score Understanding these impacts can provide a more nuanced view of the tradeoffs involved. 3. Exploration of Hybrid Models: Have the authors considered hybrid models that combine classical and quantum layers to possibly enhance performance or reduce resource requirements further 4. Future Work on Hardware Implementation: What are the authors plans for testing and refining these models on physical quantum computing systems Insights from such experiments could significantly advance the field by highlighting practical challenges and opportunities. Review Summary: The paper accomplishes a significant step forward in the application of quantum machine learning for image classification tasks by addressing critical limitations in qubit availability and image size. The proposed encoding mechanism and QNN architecture substantially advance the stateoftheart with potential implications for a broader range of applications beyond image classification. Despite these strengths the study does face limitations notably in its focus on the MNIST dataset lack of color image processing and the observed performance tradeoff with further qubit compression. Consequently while the paper presents substantial theoretical and methodological contributions further research is essential to validate these findings in more diverse contexts and on actual quantum hardware. Addressing these challenges will significantly enhance the relevance and applicability of quantum machine learning in practical settings.", "qhkFX-HLuHV": " Peer Review of \"Casting Video Recognition as an Image Classification Task\"  1. Summary of the Paper The authors propose a novel approach to video action recognition by treating it as an image classification problem. Their method titled SIFAR (Super Image for Action Recognition) converts a sequence of video frames into a \"super image\" by rearranging the frames into a predefined spatial layout. This transformation allows the use of traditional image classifiers like the transformerbased Swin Transformer and the CNNbased ResNet to directly perform action recognition without the need for dedicated temporal modeling modules. The authors validate their method on several public datasets including Kinetics400 Moments In Time SSV2 Jester and Diving48 and compare its performance with stateoftheart (SOTA) methods. Their findings suggest that SIFAR despite its simplicity competes favorably against existing approaches in terms of accuracy computational efficiency and model size.  2. Strong and Weak Points Question and Feedback to the Author  Strong Points:  Novel Approach: The paper introduces an innovative perspective by simplifying video action recognition to an image classification task. This approach not only challenges the conventional temporalspatial modeling paradigm but also showcases the versatility of image classifiers in handling video data.  Comprehensive Experimental Validation: The authors conduct extensive experiments across multiple datasets and compare their method against both CNN and transformerbased SOTA methods providing a thorough validation of their approach.  Efficiency: SIFAR demonstrates computational efficiency and reduced model size while maintaining competitive accuracy. This trait is particularly beneficial for deploying models in resourceconstrained environments.  Weak Points:  Generalizability Concerns: While the results are promising the paper lacks a discussion on the methods performance on videos with complex temporal dynamics or on datasets not included in the experiment. This raises questions about SIFARs generalizability to a broader range of video content.  Interpretability and Insight: The paper could benefit from a deeper analysis of why and how the super image approach works potentially through visualizations of the internal model representations or further ablation studies on the layout and size of super images.  Scope for Model Optimization: The paper explores adjustments to window size in transformers and kernel sizes in CNNs but doesnt delve into optimizing the architecture of the image classifiers themselves for the specific task of action recognition from super images.  Question and Feedback:  Question: Have the authors considered the impact of different video frame rates on the effectiveness of their approach given that temporal information is now encoded spatially  Feedback: It would be enriching to see a more detailed discussion on the limitations of SIFAR and potential directions for future research especially concerning the generalizability issues and the optimization of classifiers for this specific task.  3. Review Summary The paper presents a compelling new approach to video action recognition by simplifying it to an image classification task using a novel concept of super images. This method stands out for its simplicity efficiency and the promising results it achieves across several benchmarks. However to strengthen the paper further the authors should address questions about the generalizability of their method to a wider variety of video data offer deeper insights into the mechanism of their approach and explore optimizations specific to the repurposed image classifiers for action recognition. Nonetheless SIFAR represents a significant step forward in video understanding opening up new possibilities for future research in the field. The paper is wellstructured and presents a clear and methodical exploration of an innovative idea making a valuable contribution to the literature on video action recognition.", "xaTensJtCP5": " Review of \"Constructing Ab Initio Objective Functions for Neural MCMC Proposal Optimization\"  1. Summary This paper tackles the challenge of optimizing Markov Chain Monte Carlo (MCMC) proposal distributions particularly when parameterized by deep generative models to enhance numerical estimation and statistical inference within complex problem domains. Traditionally the development of efficient MCMC proposal distributions has been constrained by computational limitations and the architectural restrictiveness of models leading to a reliance on certain schemes like Random Walk Metropolis and Hamiltonian Monte Carlo among others. However the advent of deep generative models offers a potent avenue for parameterizing highly expressive proposal distributions. The core contribution of this work is the introduction of \"Ab Initio\" objective functions designed to optimize neural MCMC proposal distributions irrespective of their specificity or complexity. These objective functions are crafted through a semiempirical process that seeks to approximate a \"ground truth\" definition of MCMC efficiency by adhering to established theoretical insights and empirical observations. Through experimentation the paper validates the robustness and efficacy of Ab Initio objective functions over traditional ones across various optimization tasks showing favorable performance and optimization behavior.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The proposal of Ab Initio objective functions marks a significant step forward in the domain of MCMC proposal optimization especially in how they free the optimization process from the constraints of model architecture.  Theoretical Rigor: The theoretical foundation for Ab Initio objective functions is wellarticulated linking the proposed method solidly with existing literature and theoretical insights.  Empirical Validation: The experiments conducted provide a comprehensive validation of the proposed objective functions showing their superior performance and robustness across a wide range of tasks. Weak Points:  Accessibility to a Broader Audience: The paper is highly technical and might be challenging for readers who are not deeply familiar with MCMC methods or the mathematical underpinnings of neural network optimization.  Need for More Diverse Experimental Settings: While the experiments are compelling additional experiments in more varied contexts (e.g. different types of deep generative models realworld largescale data) could further strengthen the papers conclusions. Questions and Feedback:  Scalability: How do Ab Initio objective functions scale with the complexity and size of the problem domain especially in highdimensional spaces or with extremely large datasets  Comparison with Other Methods: While the paper compares Ab Initio objective functions with traditional objectives how do they fare against other recent advances in neural MCMC proposal optimization  Practical Impact: Can the authors provide insights or speculate on the impact of adopting Ab Initio objective functions in practical applications particularly in fields requiring complex statistical inference  3. Review Summary The paper presents a significant advancement in the optimization of MCMC proposal distributions using deep generative models. By introducing Ab Initio objective functions the authors offer a novel solution that overcomes the limitations imposed by specific model architectures promising broader applicability and superior performance in optimizing neural MCMC proposals. The theoretical motivations are sound and the empirical validation is robust demonstrating the potential of Ab Initio objective functions to enhance MCMC efficiency across various tasks. However the paper could improve its appeal by addressing its accessibility and providing additional empirical evidence in more diverse scenarios or realworld applications. Despite these minor shortcomings the contribution is of high value paving the way for further research and practical implementations in complex statistical inference tasks. Therefore I recommend this paper for publication but encourage the authors to consider the questions and feedback provided to strengthen their work further.", "xS8AMYiEav3": " Peer Review  Summary of the Paper: This paper introduces REASSURE an innovative methodology for rectifying bugs in ReLU activation functionbased neural networks without altering the networks global behavior. The method distinctively synthesizes a patch network tailored to a specific linear region harboring buggy inputs effectively correcting the behavior on those inputs. This approach is grounded in leveraging the continuous piecewise linear (CPWL) nature of ReLU networks ensuring soundness (guaranteed correction of buggy behavior) and completeness (capability to find a patch for any buggy input). Additionally the method inherently generalizes the repair to encompass all inputs within the repair region including previously undetected bugs while ensuring minimal changes in the function space and no impact on unaffected regions. Through a comparative analysis with stateoftheart methods REASSURE is demonstrated to exhibit superior performance in terms of preserving locality and minimizing negative side effects.  Strong Points: 1. Theoretical Robustness: REASSURE is the first to offer a sound and complete repair methodology for ReLU networks with strong theoretical guarantees. This is a significant advancement over existing methods that lack formal guarantees. 2. Localized and Minimal Impact: The methods ability to correct buggy behavior with minimal changes and localized impact is commendable. This approach helps in preserving the networks overall functionality while ensuring that changes do not affect inputs outside the repair region. 3. Generalizability: REASSUREs automatic generalization of repairs to include potentially undetected buggy inputs within the repair region significantly enhances its practical utility. 4. Efficiency: The proposed method offers a polynomialtime solution to the repair problem which is a noteworthy achievement potentially making it suitable for realworld applications.  Weak Points and Questions: 1. Scalability: While the paper claims polynomialtime efficiency it remains unclear how the method scales with very large networks and highdimensional input spaces especially given the computational challenges mentioned in section 5.3 dealing with featurespace repairs. 2. Comparative Analysis: The paper extensively compares REASSURE with existing methodologies but it lacks a direct comparison in terms of computational resources required (time and memory) for repairs. 3. Application Breadth: The paper focuses on ReLU networks. It would be beneficial to understand the potential extensions of REASSURE to other types of networks especially those using different activation functions. 4. Impact of Parameter Choices: The role of the \u03b3 parameter and its selection process is crucial for the methods success. Could the authors elaborate on how one might dynamically choose or adapt \u03b3 for different scenarios  Feedback to the Author: The paper presents a novel and theoretically robust methodology for neural network repair which is a significant contribution to the field. However it would be beneficial for readers to understand the scalability of the proposed method better considering the computational resource implications. Additionally more insights into the practical aspects of applying REASSURE such as the impact of parameter choices and its applicability to broader network types would enhance its utility for readers and practitioners. Addressing these points could increase the methods attractiveness and applicability to a wider range of realworld problems.  Review Summary: Overall the paper introduces a groundbreaking methodology for repairing ReLUbased neural networks with significant merits including theoretical robustness localized and minimal impacts on the networks function and efficiency. Despite these strengths there are areas that require further clarification such as the methods scalability comparative analysis on computational resources broader applicability and practical considerations like parameter selection. Addressing these concerns and providing additional insights would make the contributions of this work even more compelling. Given the novel approach and the potential impact of REASSURE I recommend this paper for publication once the mentioned points are addressed or clarified.", "tx4qfdJSFvG": " Review of \"Feature Propagation: Handling Missing Node Features in Graph Neural Networks via Dirichlet Energy Minimization\"  1) Summary This paper addresses a significant challenge in the application of Graph Neural Networks (GNNs): the presence of missing node or edge features in realworld graph data. The authors propose a novel method Feature Propagation (FP) to reconstruct missing features by minimizing Dirichlet energy through a diffusiontype differential equation. The method stands out for its simplicity speed scalability and ability to perform well even with extremely high rates (up to 99) of missing features. The core idea is to use the known features as boundary conditions in a diffusion process across the graph thereby interpolating the missing features. This interpolation process precedes the application of a standard GNN for downstream tasks like node classification. The authors experimentally demonstrate that FP significantly outperforms previous approaches on several benchmarks with a minor relative accuracy drop in the presence of high rates of missing features and remarkable scalability to large graphs.  2) Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Foundation: The approach is wellgrounded in theory emerging from the minimization of Dirichlet energy. This solid theoretical basis provides clarity and robustness to the method.  Performance with Extreme Missing Feature Rates: The ability to maintain relatively high accuracy even when 99 of the features are missing is particularly impressive. This highlights the methods robustness and potential applicability to scenarios where data collection is challenging.  Scalability: Demonstrating the methods performance on a graph with \\xe2\\x88\\xbc2.5M nodes and \\xe2\\x88\\xbc123M edges is a significant achievement. This level of scalability ensures the methods relevance for realworld applications. Weak Points:  Homophily Assumption: While the method thrives in homophilic graph data contexts (where neighbors share similar attributes) its efficacy in heterophilic or low homophily scenarios remains uncertain. This limitation could restrict the methods applicability across diverse graph types.  Independence of Feature Channels: Treating each feature channel independently in the diffusion process might overlook the interdependencies between features potentially leading to suboptimal reconstruction of missing features. Questions and Feedback:  Extending to Heterogeneous Graphs: Have the authors considered adapting FP to heterogeneous graphs If so what modifications would be necessary and how do they anticipate this would affect performance  Dealing with Heterophily: Could the authors elaborate on potential strategies to adapt FP for lowhomophily or heterophilous graphs Might incorporating feature dependencies into the diffusion process be a feasible approach  3) Review Summary The paper presents a compelling approach Feature Propagation (FP) that effectively addresses the challenge of missing features in graph neural networks. Its theoretical motivation combined with impressive experimental results demonstrating robustness to high rates of missing features and scalability to large graphs underscores the methods potential for broad applicability. Nonetheless the methods assumptions of homophily and independent feature channels pose limitations suggesting areas for future work. Adapting FP to heterogeneous graphs and addressing the challenges presented by lowhomophily scenarios could further enhance its utility. Considering these strengths and areas for improvement I recommend accepting this paper. The novel approach it introduces to handling missing features in GNNs represents a significant contribution to the field despite some limitations that warrant further research.", "zAyZFRptzvh": " Review of \"AuditAI: A Framework for Auditing Deep Learning Models through SemanticallyAligned Unit Tests\"  1. Summary of the Paper The paper introduces AuditAI a novel framework designed for the purpose of auditing Deep Learning (DL) models before deployment particularly focusing on applications where safety and reliability are paramount. The core innovation of AuditAI lies in its utilization of semanticallyaligned unit tests that evaluate a DL models compliance with predefined specifications (e.g. accuracy thresholds) under controlled semantic variations in the input space (e.g. changes in the angle of a face relative to a camera in a facial recognition system). This is achieved by linking the DL models with generative models to share a common semantically interpretable latent space allowing for the generation of varied inputs that closely mimic realworld variations. The study demonstrates the effectiveness of AuditAI across multiple datasets (ImageNet Chest XRays LSUN FFHQ) showcasing its capability in producing models that are robust to a broader range of input variations compared to traditional pixelbased adversarial training methods. Additionally the auditing framework includes a certification training phase that enhances the models verifiability and interpretability through the shared latent space. Ultimately the paper claims that AuditAI bridges the gap between the need for formal interpretable verification of DL models and the scalability requirements to handle contemporary largescale models.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The idea of leveraging a shared latent space between generative and DL models for auditing purposes is novel and addresses a significant gap in scalable and interpretable DL model verification.  Practicality in Deployment: AuditAIs emphasis on realworld applicability demonstrated through semanticallyaligned unit tests and certification training offers practical benefits for deploying DL models in safetycritical domains.  Comprehensive Evaluation: The evaluation across four diverse datasets along with a comparison to pixelbased adversarial training methods provides strong empirical evidence of AuditAIs advantages. Weak Points:  Dependency on Generative Models: The efficacy of AuditAI heavily relies on the quality of the generative models used. There is a risk that biases or quality issues within these models could undermine the auditing process.  Limited Scope in Demonstrations: While the evaluation is thorough it primarily focuses on imagebased datasets. Expanding the scope to include other types of data (e.g. text audio) could further validate the frameworks versatility.  Interpretability and Domain Expertise: The paper briefly mentions the integration of domain expertise in the discussion but does not detail how AuditAI facilitates such integration beyond the curation of datasets for the generative model. More explicit mechanisms for incorporating expert insights could enhance the framework. Questions and Feedback:  Could the authors elaborate on how the framework might be adapted or extended to handle data beyond images such as text or audio which also present critical auditing needs  Feedback: A more detailed discussion on the limitations regarding the dependency on generative models and potential mitigation strategies would be beneficial. This could include approaches to assess the quality and biases of generative models used within AuditAI.  3. Review Summary and Recommendation The paper presents a compelling approach to auditing DL models through semanticallyaligned unit tests enabled by a shared latent space with generative models. The strengths of the paper lie in its novel auditing framework empirical evaluation and relevance to deploying DL models in safetycritical scenarios. However concerns about the dependency on generative models the need for broader validation across different data types and the integration of domain expertise warrant further exploration. Based on the strengths and the innovative nature of the proposed framework I recommend this paper for acceptance provided the authors can address the mentioned weaknesses particularly around the frameworks dependency on generative models and expanding its applicability and interpretability. With these considerations the paper makes a significant contribution to the field of DL model auditing offering a promising direction for future research and development.", "gmxgG6_BL_N": " Peer Review on: \"Supervised Variational Component Decoder for Nonlinear Signal Extraction\"  1. Summary of the Paper The paper introduces a novel deep learning framework called supervised Variational Component Decoder (sVCD) aimed at extracting a single source signal from a nonlinear mixture of signals. Leveraging the abilities of sequencetosequence translation and variational inference the sVCD framework proposes a modified approach for variational lower bound optimization specifically tailored for source extraction tasks. The method shows significant improvements in extracting the desired signal with robust performance across different datasets including artificially generated signals RFPPG data for heartbeat monitoring and EEGEOG signals for brain activity and eye movement separation.  2. Strengths and Weaknesses of the Paper Strong Points:  Innovative Methodology: The introduction of sVCD to handle nonlinear signal extraction by making use of sequencetosequence translation coupled with variational inference marks a notable enhancement over prevailing methods. This targeted approach supported by mathematical formulation and proofs addresses the inefficiency of blind source separation (BSS) in such scenarios directly.  Empirical Validation: The exhaustive evaluation conducted across diverse and challenging datasets with comparisons to stateoftheart methods like ICA and ConvTasNet provides strong empirical evidence of sVCDs efficacy and versatility.  RealWorld Applicability: The paper tackles a significant problem in signal processing with broad implications across various domains including healthcare and biosignal analysis. The results on heart rate monitoring and EEGEOG signal separation highlight the framework\u2019s practical utility. Weak Points:  Dataset Accessibility and Reproducibility Concerns: While the paper contributes towards reproducibility by sharing the source code the unavailability of the RFPPG dataset due to privacy concerns may limit the ability of others to fully replicate and validate the findings.  Limited Scope on Comparative Analysis: The paper primarily compares sVCD with methods designed for linear signal separation. Although the performance superiority is clear a comparison with recent advancements specifically aimed at nonlinear mixture scenarios could have provided a more comprehensive validation.  Potential Overfitting on Specified Scenarios: Theres a minor concern on how well the model generalizes across broader signal extraction tasks beyond the assessed datasets considering specialized design elements like the attention mechanism. Questions and Feedback: 1. Generalization Capability: How does sVCD perform when exposed to signals with significantly different characteristics from the datasets used in this study Is there any observed performance degradation 2. Comparison Basis: Could the authors elaborate on the decision behind the chosen baseline comparisons and whether efforts were made to benchmark against recent nonlinear signal extraction methods 3. Effect of Hyperparameters Tuning: How sensitive is sVCDs performance to the tuning of its hyperparameters especially the variational lower bound weight (\u03bb) Some insights into this aspect could enhance the understanding of its robustness.  3. Review Summary (Supporting Recommendation) The proposed sVCD framework stands out as a significant contribution to the signal processing field specifically targeting the extraction of signals from nonlinear mixtures. Its design embedding sequencetosequence capabilities with variational inference addresses a notable gap by focusing on the efficiency and effectiveness of extracting a single source signal as opposed to recovering all sources which are often unnecessary and computationally expensive. The comprehensive experimental validation showcases sVCD\u2019s superior performance against established methods and across varied datasets emphasizing its potential for realworld applications such as healthcare monitoring and brain signal analysis. Despite minor concerns regarding the generalization across drastically different signals and full reproducibility due to dataset access restrictions the paper establishes a solid foundation for future exploration and advancements in targeted signal extraction. Given the strengths outweigh the limitations and with the anticipation that future work could explore extended comparisons and address the generalization question the paper is recommended for publication. The innovative approach combined with substantial empirical validation contributes essential insights and tools to the signal processing community opening avenues for enhanced source extraction methodologies.", "fRb9LBWUo56": " Review of the Paper \"ReExamining Deep Reinforcement Learning Approaches for Accelerating MRI Sampling\" 1) Summary This paper critically evaluates the use of Deep Reinforcement Learning (RL) methods for accelerating Magnetic Resonance Imaging (MRI) by optimizing sampling masks. It replicates and extends experiments from stateoftheart (SotA) RL sampling approaches comparing them against fixed greedy policy optimizations such as Learningbased Compressed Sensing (LBCS). The authors find that fixed policies either match or outperform RL methods in optimizing MRI sampling masks. They identify discrepancies in previous works regarding preprocessing and metric reporting showing how these can significantly affect performance comparisons. Through extensive ablations the paper questions the added value of adaptive RL methods over simpler fixed policies for MRI sampling suggesting that improvements in results may rather stem from changes in preprocessing methods mask designs or reconstruction architectures than from the RL algorithms themselves. 2) Strengths and Weaknesses Strong Points:  Rigorous Evaluation: The paper presents a comprehensive and meticulous comparison of RLbased sampling methods against simpler fixed policies addressing an important gap in the literature by questioning the real benefits of RL for this application.  Clarification of Previous Results: The identification and resolution of variations in preprocessing and reporting in prior work provide valuable insights into how these factors can lead to misinterpretations of methodological advancements.  Extensive Ablations and Recommendations: The ablations thoroughly examine the effects of various factors on performance leading to grounded recommendations for future work in MRI sampling optimization. This is particularly useful for guiding subsequent research efforts. Weak Points:  Limited Discussion on RL Potential: While challenging the current achievements of RL in MRI sampling the paper might not sufficiently acknowledge the potential for future RL advancements. The discussion could benefit from exploring possible directions for enhancing RL methods in this context.  Potential Bias towards Fixed Policies: Given the focus on comparing RL methods with fixed policies there might be an implicit bias favoring the latter. A more balanced discussion on the inherent strengths of adaptivity in RL and how they might be leveraged in future works would add value. Questions and Feedback to the Author:  Future of RL in MRI Sampling: Could the authors elaborate on potential improvements or novel approaches within RL that might overcome the current limitations observed for MRI sampling optimization  Impact of Recent Advances in RL: Have the authors considered evaluating their methodology against very recent advances in RL which might offer improved adaptivity or efficiency for MRI sampling  Generalizability to Other Imaging Modalities: Do the authors believe their findings could generalize to other imaging modalities or optimization tasks outside the MRI domain 3) Review Summary This paper provides a valuable critique of the current use of Deep RL for optimizing MRI sampling masks challenging the prevailing notion that RL methods offer significant advantages over fixed greedy policies. By conducting a thorough reproduction and extension of existing approaches coupled with detailed ablations the authors effectively illustrate how variations in preprocessing reporting and methodological nuances can obscure the true source of improvements in accelerated MRI. However the paper could potentially understate the prospective contributions of RL to this field. Future improvements in RL techniques particularly those enhancing adaptability or computational efficiency might address the current shortcomings highlighted in this work. Moreover a more nuanced discussion on the inherent capabilities of RL for dealing with dynamic and complex decisionmaking tasks could provide a balanced view on its applicability to MRI sampling. Overall the research is comprehensive shedding light on critical aspects that have been overlooked in previous studies. Its findings encourage a reevaluation of current practices and underline the importance of establishing solid baselines and consistent reporting standards for advancing in the optimization of MRI sampling. The recommendations offered not only serve as practical guidance for future investigations but also as a call for more critical scrutiny of methodological advancements in the field.", "mRF387I4Wl": " Peer Review of \"FlowX: A Novel Method to Explain Graph Neural Networks through Message Flows\"  Summary of the Paper The paper presents FlowX a novel approach designed for improving the explainability of Graph Neural Networks (GNNs) by focusing on message flows. The authors argue that traditional explainability methods which usually target graph nodes edges or subgraphs do not adequately capture the workings of GNNs. Instead they propose that the examination of message flows \u2013 the paths through which node information is passed during message passing operations within GNNs \u2013 offers a more natural and intuitive framework for explainability. FlowX quantifies the importance of these message flows by adapting Shapley values from cooperative game theory and proposes an approximation scheme to manage the computational complexity involved. A subsequent learning algorithm refines these initial assessments to produce final flow scores thereby enhancing explainability. The paper reports that extensive experiments on a variety of datasets demonstrate improved explainability of GNNs by FlowX over existing methods both in quantitative and qualitative terms.  Strong Points 1. Novelty and Relevance: The paper addresses the critical yet underexplored area of GNN explainability by introducing a unique perspective on importance attribution to message flows. This approach directly tackles the complexity and opacity inherent in deep GNN models. Given the proliferation of GNN applications enhancing their explainability is highly relevant to the field. 2. Theoretical Soundness: FlowX\u2019s reliance on the Shapley value concept for quantifying the importance of message flows provides a theoretically robust foundation. This choice enables a fair distribution of \"credit\" to various flows for their contribution to predictions which is logically sound and wellmotivated. 3. Experimental Coverage: The authors present a comprehensive set of experiments on synthetic and realworld datasets covering a variety of GNN tasks. These experiments not only validate the effectiveness of FlowX but also provide insights into its general applicability across different domains and GNN architectures.  Weak Points 1. Computational Complexity Concerns: Despite the proposed approximation scheme the algorithm\u2019s reliance on Shapley values \u2014 known for their computational intensity \u2014 could limit its scalability. The paper mentions approximation and a learning algorithm to mitigate this but a deeper discussion on computational costs and scalability especially for very large graphs and deep GNN models would be beneficial. 2. Comparison with Baseline Methods: While the paper compares FlowX with several existing methods the criteria for these comparisons (e.g. Fidelity and Sparsity scores) may not fully capture the nuanced differences in explainability approaches particularly in qualitative terms. Furthermore more detailed insights into why certain methods underperform or how FlowX specifically outperforms them would enrich the discussion. 3. Impact on Model Training and Deployment: The paper primarily focuses on the explainability aspect post hoc with less emphasis on how insights derived from FlowX might influence model training refinement or deployment strategies. A discussion on incorporating FlowX insights into practical GNN development workflows could significantly enhance the paper\u2019s impact.  Questions and Feedback to the Author 1. Scalability: Can the authors elaborate on the scalability of FlowX specifically how it performs in terms of computational and memory requirements on very large graph datasets and deep GNN models 2. Insights for Model Improvement: How can the explanations generated by FlowX be used to improve the accuracy or efficiency of GNN models Are there examples or case studies where FlowX insights led to model adjustments with positive outcomes 3. Comparison Criteria: On what basis were the baselines chosen for comparison and are there qualitative dimensions of explainability that were not captured by the quantitative metrics used How does FlowX fare on those unquantified aspects  Review Summary and Recommendation FlowX represents a significant and novel contribution to the domain of GNN explainability addressing a crucial gap by focusing on message flows. Its theoretical grounding in Shapley values adds to its strength and the experimental results convincingly demonstrate its potential to enhance the understanding of GNN decisions significantly. Despite this there are areas where more depth could improve the paper particularly concerning computational aspects a deeper analysis of the comparisons made and the practical impact of the proposed explanations. Given its strong points the paper is a valuable addition to the literature on GNNs. However addressing the mentioned weaknesses would greatly strengthen the paper. It is recommended for publication contingent upon careful consideration and incorporation of the feedback provided especially concerning the scalability and practical utility of FlowX.", "hniLRD_XCA": " Peer Review of \"A Robust Learning Control Framework Using Deep Stochastic Koopman Operator\"  1. Summary This paper introduces the deep stochastic Koopman operator (DeSKO) model within a robust learning control framework aimed at stabilizing nonlinear stochastic systems. DeSKO advances the Koopman operator theory by incorporating uncertainty into its functional framework addressing the realworld application challenge where traditional Koopman operators underperform due to their deterministic nature. The primary innovation lies in the models ability to encode observable distributions probabilistically thereby capturing the inherent uncertainty in dynamical systems. This allows for the design of a robust stabilizing closedloop controller that improves the systems resilience to internal and external disturbances. The paper validates DeSKOs effectiveness through modeling and control experiments on various advanced control benchmarks including soft robotic arms legged robots and biological gene regulatory networks showcasing superior scalability and robustness compared to current deep Koopman operators and reinforcement learning methods.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Innovation: The introduction of a stochastic approach to the Koopman operator theory to include uncertainties in dynamical systems is a significant leap. It addresses a critical gap in the application of Koopman operators for realworld systems. b. Robust and Scalable Framework: Demonstrating the frameworks robustness through control benchmarks and its scalability to highdimensional systems is highly commendable. c. Comprehensive Evaluation: The extensive experiments including comparisons with stateoftheart methods and the inclusion of stress tests against unseen disturbances provide strong evidence supporting the proposed models effectiveness. Weak Points: a. Complexity and Accessibility: The papers high technical density could hinder accessibility for readers not already wellversed in Koopman theory or advanced control systems. b. Generalization: While impressive the demonstration of the methods effectiveness across selected benchmarks leaves open questions regarding its generalization capabilities particularly in environments with significantly different dynamics or noise characteristics. c. Computational Efficiency: There is a lack of discussion on the computational efficiency of DeSKO especially important for its adoption in realtime applications. Questions: 1. Can the authors provide insights or preliminary results on how DeSKO might perform in realworld physical systems beyond simulations 2. Regarding scalability are there specific limitations or performance degradation points as system complexity increases 3. How might the proposed method integrate or complement existing reinforcement learning frameworks especially those already deployed in practical scenarios Feedback:  A deeper analysis or discussion of DeSKOs computational requirements would be valuable especially comparing it to both traditional Koopman operators and contemporary RL methods in terms of computational and data efficiency.  Addressing the potential for model overfitting or lack of generalization to drastically different environments would strengthen the papers claims.  The paper could benefit from a clearer exposition or a dedicated section summarizing the theoretical contributions and practical implications making the significant advancements more accessible.  3. Review Summary The paper presents a novel and theoretically grounded advancement in the application of Koopman operator theory to nonlinear dynamical systems through the introduction of the deep stochastic Koopman operator (DeSKO) model. This work impressively addresses a critical limitation of traditional Koopman operator models by incorporating system uncertainties directly into the modeling and control processes. The comprehensive evaluation through modeling and control experiments rigorous benchmark comparisons and robustness tests underlines DeSKOs superior performance and practical value. Despite its strengths the paper could be improved by addressing its computational efficiency exploring broader generalization capabilities and enhancing its accessibility to a wider audience. Overall this work is a substantial contribution to the field pushing forward the practical application of Koopman operator theory in realworld highdimensional nonlinear systems. The recommendations provided aim to refine and clarify the significant advancements ensuring the papers impact is fully realized within the scientific community and beyond.", "vLz0e9S-iF3": "Review of \"A Novel QuasiPotential Theory for the Escape Problem of Stochastic Gradient Descent\" 1) Summary: The paper develops a quantitative theory based on the concept of quasipotential from large deviation theory to analyze how stochastic gradient descent (SGD) algorithms escape from local minima on loss surfaces. The authors examine the influence of the sharpness of these loss surfaces on the escape dynamics. They introduce steepness of a trajectory as a tool to understand SGDs escape dynamics finding that while sharper minima slow down the escape the inherent noise in SGD leads to a faster escape confirming and generalizing previous findings. Experimental validation using real datasets and neural networks is provided to support the theoretical results. The paper claims to be the first to apply quasipotential to the SGD escape problem offering a rigorous description without auxiliary variables and flexibility in application beyond prior works. 2) Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Approach: The introduction of quasipotential theory to the escape problem of SGD is novel and provides a fresh perspective on analyzing SGD dynamics. It fills a gap in the literature by offering a rigorous and generalizable framework.  Comprehensive Analysis: The paper thoroughly investigates the effects of different parameters like batch size learning rate and loss surface sharpness on SGDs escape dynamics. This comprehensive analysis enhances the understanding of SGD behavior in practical settings.  Empirical Validation: The experimental results strengthen the papers claims demonstrating the theorys applicability to realworld scenarios and datasets. It is commendable that the authors took care to ensure their assumptions were met in the experimental setup. Weak Points:  Assumptions Validity: While the paper provides assumptions for their analytical framework there is a lack of discussion on the practical validity and applicability of these assumptions across different deep learning settings and architectures.  Comparison with Existing Works: Although the paper mentions comparisons with existing studies a more detailed discussion or a dedicated section comparing the proposed theorys implications against existing results (both theoretical and empirical) would add value.  Accessibility: Given the theoretical depth the paper may be challenging for readers who are not wellversed in stochastic processes or dynamical systems theory. A section simplifying the main concepts for a broader audience could enhance accessibility. Questions and Feedback:  Regarding Assumptions: Can the authors elaborate on the practical scenarios where the assumptions might fail Understanding the limitations would greatly benefit readers in applying this theory correctly.  Experimental Setup: Could the authors provide more details on the choice of datasets and models Specifically how robust are the findings across different types of datasets and neural network architectures  Impact on Training Strategies: The theory suggests that SGDs noise helps escape sharp minima faster. How can this insight be translated into practical training strategies or modifications to the SGD algorithm 3) Review Summary: The paper introduces a sophisticated and novel theoretical framework to analyze SGDs escape from local minima making a significant contribution to understanding the dynamics of SGD in the context of deep learning optimization. The use of quasipotential theory is particularly innovative providing a solid analytical foundation that is validated through experiments with realworld data. However the paper could benefit from a more detailed discussion on the practical applicability of its assumptions as well as comparative analysis with existing literature. Despite these minor shortcomings the findings are robust and offer valuable insights into the behavior of SGD including the counterintuitive role of noise in facilitating escape from sharp minima. The theoretical advancements complemented by empirical validation make a strong case for the significance of this work in the field of machine learning and optimization. This paper is poised to stimulate further research in understanding and optimizing SGD and its variants given its detailed analysis and innovative approach.", "uxgg9o7bI_3": " Review of \"Designing More Expressive Graph Neural Networks\"  1. Summary This paper introduces a novel perspective on enhancing Graph Neural Networks (GNNs) by injecting structural properties of graphs into their messagepassing aggregation schemes. The authors establish a theoretical framework proposing a hierarchy of local isomorphism on neighborhood subgraphs to conceptually fuel this enhancement. They argue that their approach allows GNNs to surpass the expressiveness of the WeisfeilerLehman (WL) test a standard for graph isomorphism problems. A new GNN model termed GraphSNN is presented and theoretically proven to be more expressive than the WL test. Empirical evaluations across different graph learning tasks demonstrate that GraphSNN significantly improves upon stateoftheart methods in terms of accuracy without compromising computational efficiency.  2. Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Contribution: The paper successfully introduces a new hierarchy of local isomorphism providing a solid theoretical foundation for the proposed method. By establishing a clear connection between this hierarchy and the WL test the authors make a significant contribution to the understanding of GNN expressiveness.  Novel GNN Model (GraphSNN): GraphSNN represents a significant advancement in GNN design. Its ability to be more expressive than the WL test paired with computational efficiency addresses a critical gap in the current landscape of GNNs.  Empirical Validation: The authors provide extensive empirical evidence demonstrating the superiority of GraphSNN over stateoftheart GNN models across various benchmark datasets. This thorough validation is commendable.  Oversmoothing Analysis: The inclusion of an analysis on how GraphSNN mitigates the oversmoothing issue a common problem in deep GNNs adds depth to the paper. Weak Points:  Lack of Detailed Comparison: While the paper compares GraphSNN to several baselines it somewhat lacks a detailed discussion on how it relates to or improves upon preexisting methods that aim to extend GNNs beyond the WL test.  Interpretability of Structural Coefficients: The paper could benefit from a deeper exploration of the interpretability of the structural coefficients \\xcf\\x89. Understanding how these coefficients capture structural properties and affect the models decisions could provide valuable insights into the models workings and applicability to different graph structures. Questions and Feedback:  Scalability: Given the computational efficiency of GraphSNN how does its performance scale with the size of the graph data particularly in comparison to the baseline models  Generalization Capability: Could the authors provide more insights into the generalization capability of GraphSNN especially in unseen graph structures or domains  Impact of \u03bb Values: The paper mentions that different \u03bb values are optimal for node classification and graph classification. Could the authors explore the potential reasons behind this and whether theres a method to dynamically determine optimal \u03bb values based on the task  3. Review Summary The paper presents a wellstructured and insightful exploration into enhancing the expressiveness of GNNs by incorporating structural graph properties into their aggregation schemes. The theoretical contributions are strong providing a novel hierarchy of local isomorphism that underpins the design of the new GNN model GraphSNN. GraphSNNs empirical performance across several benchmarks not only validates the proposed theoretical claims but also demonstrates its potential to advance the state of the art in graph learning tasks. However the paper could be improved by including a more detailed comparison with existing methods that extend GNNs beyond the WL test offering a richer context for GraphSNNs contributions. Additionally exploring the interpretability of the structural coefficients and the implication of different \u03bb values in more depth could provide readers with clearer guidance on the models application to various problems. Despite these areas for potential enhancement the paper makes a significant contribution to the field of GNNs and I recommend it for publication. The novel theoretical insights paired with strong empirical results make it a valuable addition to the literature.", "onwTC5W0XJ": " Peer Review for \"Causally Focused Convolutional Networks for Guided Feature Extraction\"  1. Summary of the Paper The paper presents an innovative approach to image classification using Convolutional Neural Networks (CNNs) by guiding the feature extraction process towards causal features rather than merely correlated features. This is achieved using minimal human guidance through activation masks which help the network focus on areas of the image deemed causally relevant for classification. The authors argue that this method not only enhances the accuracy of feature extraction from a limited dataset but also ensures that the learned features are causally related to the classification labels addressing a common critique of CNNs tendency to learn spurious correlations. Through extensive experiments across several datasets including medical images the paper demonstrates that Causally Focused Convolutional Networks (CFCNs) outperform traditional CNNs and related models in accuracy and robustness against adversarial perturbations. The methodology includes tweaking the CNNs learning process with an additional loss function (causallyfocused loss) that works alongside the traditional categorical crossentropy loss prioritizing the extraction of causally relevant features.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The method of integrating humanguided activation masks to direct CNNs towards learning causally relevant features is novel and addresses a significant limitation of current CNN models.  Robust Experimentation: The paper provides a comprehensive experimental setup across diverse datasets that convincingly demonstrates the effectiveness of CFCNs in enhancing model performance especially in situations with limited data.  Practical Implications: The focus on medical imaging and the demonstrated ability of CFCNs to work efficiently with smaller datasets have substantial practical implications potentially improving diagnostic tools where data scarcity is a common issue. Weak Points:  Generalization of Humanguided Masks: While the concept of using activation masks is innovative the paper doesnt fully address how scalable or practical it is to generate these masks across various domains beyond the ones tested.  Comparison with StateoftheArt: The paper could have provided a more detailed comparison with the current stateoftheart models in causal feature learning offering a clearer understanding of the methods relative advancements.  Explanation of Causal Mechanism: The manuscript could benefit from a more detailed theoretical discussion on how and why focusing on causal features improves learning and robustness providing deeper insights into the underlying mechanisms. Questions and Feedback:  Scalability Concerns: How does the proposed method scale with larger more complex datasets and what is the overhead of generating and applying activation masks in such cases  Transferability to Other Domains: Can the authors elaborate on the potential applicability of their method to other domains beyond image classification such as natural language processing or video analysis  Impact on Interpretability: The paper touches on model justifiability but can the authors expand on how their method impacts the interpretability of the CNN models and their decisions  3. Review Summary The paper introduces a significant improvement to CNNs by guiding them to focus on causally relevant features through humandirected activation masks showing promising results across various datasets. This represents a notable step towards addressing CNNs tendency to learn spurious correlations particularly vital in domains like medicine where justification for decisions is crucial. While the method showcases improved accuracy and robustness primarily in settings with limited data the practicality of generating and applying activation masks at scale remains a concern. Additionally a deeper comparison with existing models and a more comprehensive theoretical underpinning of the causal mechanisms impact on learning and interpretability would strengthen the paper. Overall the presented work is compelling and offers substantial contributions to the field of image classification especially in improving CNNs efficiency and reliability through causally focused feature extraction. The authors are encouraged to address the feedback and questions raised to enhance the manuscripts clarity and comprehensiveness.", "tUMr0Iox8XW": " Peer Review of \"Closing the Performance Gap Between Infinite and Finite Neural Networks through Feature Learning Limits\"  1. Summarize the paper This paper introduces a novel mathematical construct the \u03c0limit aiming to address the feature learning inadequacies of infinitewidth neural networks specifically the Neural Tangent Kernel (NTK) models. Building upon the concept of the \u03bclimit which has shown promise in bridging the theoretical and practical aspects of wide but finite neural networks the authors propose the \u03c0limit to eliminate computational challenges associated with deep nonlinear multilayer perceptrons (MLPs). The key innovation lies in using projected gradient descent in training simplifying both computation and theoretical analysis. The efficacy of the \u03c0limit is demonstrated through superior performance on CIFAR10 and Omniglot datasets compared to both finitewidth networks and NTK showcasing an advancement in aligning infinitewidth neural network theory with practical application capabilities.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The introduction of the \u03c0limit as a solution to the computational and theoretical challenges of the \u03bclimit in deep nonlinear networks is both innovative and significant. It represents a significant step forward in the theory of infinitewidth neural networks.  Empirical Validation: The comprehensive evaluation of the \u03c0limit on CIFAR10 and Omniglot datasets demonstrating its superiority over NTK and finitewidth networks provides strong empirical support for the proposed approach.  Practical Relevance: By bridging the performance gap between theoretical (infinitewidth) and practical (finitewidth) neural networks this work has the potential to influence both the development of new neural network architectures and the theoretical understanding of deep learning. Weak Points:  Scalability Concerns: While the \u03c0limit addresses computational difficulties of the \u03bclimit the paper does not thoroughly assess the scalability of this method to even larger and more complex datasets or deep learning architectures beyond MLPs.  Limited Comparison: The paper focuses on comparing the \u03c0limit primarily with NTK and finitewidth networks. Including comparisons with other stateoftheart feature learning or kernel approximation methods could provide a broader context for evaluating its performance. Questions and Feedback:  Generalization to Other Architectures: How generalizable is the \u03c0limit approach to other types of neural network architectures beyond MLPs such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)  Impact of Hyperparameters: The paper could benefit from a deeper analysis of how different hyperparameters (e.g. learning rates batch sizes) impact the performance of the \u03c0limit especially in comparison to finitewidth and NTKbased approaches.  Theory Behind Performance Improvement: It would be insightful to include a more detailed theoretical explanation or analysis of why the \u03c0limit outperforms NTK and finitewidth networks focusing on feature learning dynamics.  3. Review Summary The paper presents a compelling advancement in the theory of infinitewidth neural networks through the introduction of the \u03c0limit. By addressing the computational and theoretical challenges of feature learning in deep nonlinear networks the authors bridge a crucial gap between the theoretical models and practical applications of neural networks. The empirical evidence supporting the \u03c0limits superiority over both finitewidth networks and NTK models on standard datasets is robust and persuasive. However the exploration of the \u03c0limits scalability its applicability to a broader spectrum of neural network architectures and a detailed comparative analysis with other stateoftheart methods are areas that could enhance the papers impact. Additionally a deeper theoretical understanding of the \u03c0limits feature learning capabilities would add value to this already significant contribution to the field of deep learning. Overall this work represents a substantial step forward in resolving the discrepancies between theoretical and empirical performances of neural networks possessing both theoretical novelty and practical relevance. It opens up exciting avenues for future research on infinitewidth neural networks and their application in solving complex problems.", "mKDtUtxIGJ": " Peer Review of \"A TwoStage Deep Point Cloud Reconstruction Network\"  Summary of the Paper This paper presents a novel deep learning framework for point cloud reconstruction aiming to address the challenges associated with the sparsity noise and irregularity of point clouds obtained from 3D scanning devices. The authors propose a twostage deep point cloud reconstruction network which first densifies and denoises the raw point cloud through a 3D sparse stackedhourglass network and then refines the reconstruction with a voxel relocalization network that utilizes transformers for converting the voxels back into 3D points. A key innovation of this work is the introduction of an amplified positional encoding method to improve the performance of the transformer by modifying the magnitude of the positional encoding vectors based on the points\u2019 distances. The proposed approach is evaluated on several datasets (ScanNet ICLNUIM and ShapeNetPart) demonstrating stateoftheart performance in point cloud reconstruction tasks and showing strong generalization capabilities to unmet scenes.  Strengths and Weaknesses Strong Points: 1. Novelty in Methodology: The paper introduces a unique twostage approach combining voxel generation with a subsequent voxeltopoint conversion process enhanced by an innovative amplified positional encoding. This methodology tactfully addresses the inherent shortcomings of point clouds. 2. Comprehensive Evaluation: Extensive experiments across multiple datasets along with comparisons to stateoftheart methods provide strong evidence of the effectiveness and generalization capability of the proposed network. 3. Practical Relevance: The paper tackles a significant problem in the field of 3D point cloud processing offering a solution that has a wide range of potential applications in autonomous driving robotics and 3D mapping. Weak Points: 1. Lack of Theoretical Analysis: The paper could benefit from a deeper theoretical discussion on how and why the amplified positional encoding specifically benefits the voxeltopoint conversion process providing insights into the models inner workings. 2. Overlooked Computational Efficiency: There is a limited discussion on the computational demands of the proposed approach an aspect critical for realtime applications. Insight into runtime memory requirements and scalability would be valuable. 3. Insufficient Ablation Studies: While baseline comparisons and ablation studies are presented further experiments such as investigating the impact of different voxel sizes or the effect of each stage independently on various types of point cloud anomalies would provide a more nuanced understanding of the models capabilities and limitations. Question and Feedback to the Author: 1. Could the authors provide further insights or theoretical grounding on the design choice and effectiveness of the amplified positional encoding compared to conventional methods 2. How does the models computational efficiency compare to existing techniques particularly in scenarios demanding realtime processing 3. An expanded ablation study exploring the individual contributions of each network component and process could enrich the paper. Are there plans to include such analyses  Review Summary The presented paper makes significant contributions to the field of 3D point cloud processing by addressing sparsity noise and irregularities through a novel twostage deep learning framework. Its introduction of an amplified positional encoding mechanism represents a notable advance in the voxelstopoints conversion process empirically demonstrating superior performance and generalization across several challenging datasets. However the paper could be further strengthened by a more thorough theoretical exploration of its novel encoding mechanism an evaluation of computational demands and expanded ablation studies to dissect the contributions of its various components more comprehensively. Despite these areas for potential improvement the work stands out for its innovative approach and substantial practical relevance marking a significant step forward in the pursuit of highquality point cloud reconstruction.", "k7-s5HSSPE5": " Peer Review for \"ImportanceWeighted Domain Alignment for Unsupervised CrossLingual Transfer\"  1. Summarize the paper. This paper investigates the factors contributing to the successful crosslingual transfer capabilities of deep multilingual language models (LMs) like multilingual BERT (mBERT) and XLMR. It identifies two major factors: the invariance of feature representations across languages and the distributional shifts in class priors between source and target language data. The authors propose a novel method ImportanceWeighted Domain Alignment (IWDA) which adjusts for these factors by aligning feature representations and correcting for class prior shifts using unlabeled target language data. The effectiveness of IWDA is demonstrated through empirical evaluation on three NLP tasks: sentiment analysis namedentity recognition (NER) and textual entailment across multiple languages. The results indicate that IWDA outperforms conventional semisupervised learning (SSL) methods especially under conditions of large class prior shifts and can be further improved by combining with SSL techniques.  2. List strong and weak points of the paper Question and Feedback to the author. Strong Points:  Innovative Approach: The paper presents a novel method (IWDA) that directly addresses both representation invariance and class prior shift which are vital for improving unsupervised crosslingual transfer performance.  Comprehensive Evaluation: The empirical evaluation across different tasks and languages provides a solid foundation for the methods effectiveness particularly under the usually challenging condition of class prior shifts.  Open Source Code: Providing the implementation code at https:github.comrxiandomainalignment ensures transparency and allows for reproducibility and further research based on this work. Weak Points:  Comparison Baselines: While the paper compares IWDA against some baseline SSL methods it lacks a direct comparison with other stateoftheart unsupervised domain adaptation techniques that are not SSLbased. This comparison could provide a clearer picture of IWDAs standing in the broader unsupervised learning landscape.  Detailed Analysis Missing: The paper could benefit from a more detailed analysis of why IWDA works well offering more insights into the conditions and mechanisms that make IWDA effective or less effective under certain circumstances.  Limited Exploration of Assumptions: The success of the IW estimation part of IWDA hinges on assumptions related to feature alignment and label shifts. It would be enriching to explore the robustness of IWDA under violations of these assumptions. Questions and Feedback:  Could the authors compare IWDA with other nonSSL unsupervised domain adaptation techniques to highlight its unique advantages or disadvantages  It would be beneficial if the authors could provide more insights into the limitations of IWDA particularly in scenarios where it did not perform as expected.  Understanding the impact of linguistic diversity on the effectiveness of IWDA would be interesting. How does the linguistic similarity between the source and target languages affect the performance of IWDA  3. Review Summary (Supporting arguments for the recommendation). The paper makes a significant contribution by addressing two crucial factors affecting unsupervised crosslingual transfer: feature representation invariance and class prior distribution shifts. The proposed IWDA method is both innovative and effective as demonstrated by comprehensive empirical evaluations across various tasks and languages. Also by making the code available the authors ensure that their work can be replicated and built upon which is commendable. However to strengthen their findings and the overall impact of their work the authors could consider extending their comparisons to include a broader range of unsupervised domain adaptation techniques. Furthermore a deeper investigation into the conditions that affect IWDA\u2019s effectiveness would provide valuable insights for future research in this area. Overall given the novelty of the approach its demonstrated effectiveness and the thoroughness of the evaluation I recommend this paper for publication. It contributes meaningfully to the field of unsupervised crosslingual transfer and opens up several avenues for future work including exploring the limits of IWDA and combining it with other approaches for enhanced performance.", "iedYJm92o0a": " Peer Review  1) Summarize the paper This paper investigates the problemsolving capabilities of large pretrained language models (LMs) particularly focusing on tasks that require multistep computations such as arithmetic operations and program execution. While these models excel in tasks that can be completed in a single computational pass they struggle with more complex iterative tasks. The paper proposes a novel approach to address this limitation by introducing a \"scratchpad\" where models can generate intermediate steps before arriving at the final solution. By training Transformers to use this scratchpad for multistep computations the authors demonstrate significant improvements in performing tasks like long addition polynomial evaluation and executing Python programs. The experiments show that this method not just enhances performance but also aids in outofdistribution generalization and provides insights into common errors.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Approach: The introduction of the scratchpad concept as a means to improve the multistep computational abilities of Transformers without altering the underlying architecture is innovative.  Comprehensive Evaluation: The paper presents a thorough evaluation across a variety of tasks demonstrating the versatility and effectiveness of the scratchpad method.  Interpretability and Error Analysis: The scratchpad outputs offer a window into the models thought process allowing for easier error analysis and interpretability which is often lacking in deep learning models. Weak Points:  Scalability Concerns: While the scratchpad approach improves performance on algorithmic tasks the paper does not extensively discuss its scalability especially for more complex or longer tasks that might exceed the models generation window.  Dependency on Supervised Training: The method relies heavily on supervised training with intermediate steps explicitly provided. Its unclear how the model would perform in more unsupervised scenarios or with less granular training data.  Limited Discussion on Limitations: The paper mentions limitations like context window size but doesnt delve into potential mitigation strategies or the tradeoffs involved in implementing the scratchpad method in practical applications. Questions and Feedback:  Could the authors expand on the potential scalability of the scratchpad approach for more complex tasks  It would be beneficial if the paper could discuss potential strategies for reducing the models reliance on supervised training for intermediate steps possibly incorporating unsupervised learning techniques.  Further elaboration on the limitations and potential drawbacks of this approach including computational overhead and practical implementation challenges would provide a more balanced view.  How well does the scratchpad method generalize to other types of computational tasks not covered in the paper such as tasks with higher complexity or those requiring nuanced logical reasoning  3) Review Summary (Recommendation) The paper presents a compelling approach to enhancing the multistep computational capabilities of large language models through the introduction of a scratchpad for intermediate computations. This innovative method not only improves performance across a range of tasks but also enhances model interpretability. Despite its strengths there are areas that require further exploration particularly regarding scalability reliance on supervised training and a more indepth discussion of limitations. Given the methods demonstrated effectiveness and the novelty of the approach I recommend acceptance. However I strongly encourage the authors to consider the feedback provided especially in addressing the scalability concerns exploring unsupervised or semisupervised adaptations of the method and providing a more comprehensive discussion of potential limitations and practical implementation challenges.", "mMiKHj7Pobj": " Peer Review for \"Autoinduced Distributional Shift (ADS) in Machine Learning Systems\"  1. Summary of the Paper The paper examines a phenomenon termed autoinduced distributional shift (ADS) which occurs when the behavior of a machine learning algorithm modifies the distribution of its inputs in a way that affects its performance. This issue is analyzed primarily in the context of content recommendation systems. The authors argue that while some shifts may be desirable unintended shifts can lead to adverse outcomes such as reinforcing filter bubbles or spreading misinformation. To address this the paper introduces \"unit tests for incentives\" which are experimental setups designed to reveal whether an algorithm possesses hidden incentives leading to ADS. Through the application of metalearning techniques such as PopulationBased Training (PBT) on these unit tests the study demonstrates how certain learning algorithms may unveil and follow incentives for ADS thereby altering their behavior without changing the performance metrics. Additionally it explores a mitigation strategy named \"context swapping\" to prevent such shifts. The experiments confirm that this approach can effectively hide incentives for ADS though its effectiveness may vary based on several factors including the choice of learning algorithms.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Originality: The concept of ADS and its implications on machine learning systems particularly on content recommendation is a valuable addition to ML literature. This presents a new lens through which algorithm behavior can be evaluated.  Relevance: With the growing impact of ML algorithms on information dissemination and social dynamics understanding and mitigating unintended influences such as ADS is critically important.  Methodological Rigor: The introduction of unit tests for diagnosing incentives for ADS is an innovative approach. The experiments are thorough and welldesigned demonstrating the practical applicability of the proposed methods.  Weak Points:  Generalizability: Its unclear how the findings particularly the effectiveness of the context swapping technique generalize across different types of machine learning applications beyond content recommendation.  Mitigation Strategy Limitations: While context swapping shows promise the paper does not discuss potential downsides or limitations in detail nor does it propose alternative strategies if context swapping fails.  Questions and Feedback:  Could the authors provide more insight into how ADS impacts could be measured or quantified in realworld settings beyond the toy environments used for the experiments  It would be beneficial to expand on the discussion of why certain algorithms reveal and pursue ADS incentives under metalearning and not others. What characteristics of these algorithms contribute to this behavior  To enhance the robustness of the findings could additional mitigation strategies be explored and compared against context swapping It would offer a broader toolkit for dealing with ADS in various scenarios.  3. Review Summary The paper provides a significant contribution to the understanding of an underexplored phenomenon in machine learning: autoinduced distributional shift (ADS). Through carefully constructed experiments it demonstrates how certain machine learning algorithms especially when combined with metalearning techniques can reveal and act on hidden incentives leading to ADS. This insight is crucial for the development of more transparent predictable and safe ML systems particularly given the increasing reliance on these systems for content recommendation and other applications with broad societal impacts. However the scope of the findings and their generalizability to other ML domains remain uncertain. The paper could be strengthened by discussing the broader implications of ADS beyond content recommendation exploring alternative mitigation strategies and detailing the potential limitations of the context swapping technique. Addressing these aspects would not only provide a more comprehensive understanding of ADS but also equip practitioners and researchers with a wider array of tools for mitigating undesired algorithmic behaviors. In conclusion the paper offers valuable insights and methodologies for identifying and mitigating incentives for ADS in machine learning algorithms. With further exploration and discussion of the weak points noted above this work has the potential to significantly influence future research and practice in the field of machine learning particularly in ensuring that algorithms behave in ways that are aligned with societal values and expectations.", "rHMaBYbkkRJ": " Review of \"Continual Learning EValuation Assessment Compass: A Tool for Transparent and Comparative Continual Learning Research\"  1. Summary of the Paper This paper presents a novel tool the Continual Learning EValuation Assessment (CLEVA) Compass designed to address and mitigate the reproducibility crisis in machine learning specifically focusing on the domain of continual learning. The authors argue that the field of continual learning characterized by the aim of developing systems that can accumulate knowledge over time without revisiting previous experiences faces unique challenges in terms of reproducibility and fair comparison due to its inherent dynamics. Traditional static benchmarks and evaluation protocols fall short in capturing the complexity and variability across different continual learning scenarios. The CLEVACompass aims to fill this gap by providing a compact visual representation that serves two main purposes: it aids in identifying a work\u2019s objectives and positioning within the literature and it facilitates direct comparison of methods across reported metrics and setup details. To substantiate the compasss utility the paper outlines the conceptual challenges of continual learning evaluation introduces the CLEVACompass framework and demonstrates its application across five example continual learning methods.  2. Strengths and Weaknesses  Strong Points: 1. Novelty and Relevance: The introduction of the CLEVACompass addresses a critical and timely issue within the continual learning and broader machine learning research communities. Its focus on promoting transparency reproducibility and fair comparisons is highly relevant given the current reproducibility crisis. 2. Comprehensiveness: The paper thoroughly discusses the underlying challenges in evaluating continual learning methods justifying the necessity for a tool like CLEVACompass. The examples provided span a range of methods and scenarios offering a detailed illustration of the tools application and utility. 3. Accessibility and Utility: By providing templates for LaTeX a Python script and a GUI for the CLEVACompass along with a repository for aggregating methods the authors ensure that the tool is accessible and can be readily utilized by the research community.  Weak Points: 1. Potential for Oversimplification: While the CLEVACompass is intended to facilitate comparison and transparency there is a risk of oversimplification where the complexity and nuances of different continual learning approaches could be underrepresented in the compass visualization. 2. Adoption Challenges: Convincing the broader research community to adopt a new tool or framework can be challenging. The paper could benefit from a more detailed discussion on strategies for encouraging widespread adoption of the CLEVACompass. 3. Evaluation and Validation: The paper lacks empirical validation of the tools effectiveness in improving research transparency and comparison. Future work could include studies demonstrating how CLEVACompass influences the assessment and discourse around continual learning research.  Questions and Feedback to the Author: 1. How does the CLEVACompass account for the potential complexity and subtlety of different continual learning approaches beyond what is visually representable in the compass framework 2. Can the authors provide insights or preliminary data on the research communitys receptiveness to adopting the CLEVACompass 3. What measures will be implemented to ensure that the CLEVACompass remains uptodate and evolves in line with emerging paradigms and advances in continual learning research  3. Review Summary The paper introducing the CLEVACompass makes a significant contribution toward addressing key challenges in the evaluation and comparison of continual learning methods. The tools focus on transparency reproducibility and fair comparison is both timely and critical given the ongoing concerns about the reproducibility crisis in machine learning research. While the CLEVACompass represents a valuable step forward it is important to consider the potential risks of oversimplification and the challenges associated with encouraging widespread adoption within the research community. Further empirical validation of the tools effectiveness in achieving its goals would strengthen the case for its utility. Ultimately the CLEVACompass presents an innovative approach to improving the clarity and rigour of continual learning research and with further development and community support it has the potential to become an indispensable resource for researchers in the field.", "vnENCLwVBET": " Peer Review of \"An Objective and Universal Metric for Text Generation Evaluation Without Reference Standards (OUMG)\"  1. Summary of the Paper The paper introduces OUMG an innovative evaluation metric designed for text generation tasks that do not have fixed optimal answers such as story and poetry generation. Traditional evaluation methods depend heavily on comparing the generated text with reference texts which is not feasible for tasks where creativity and diversity are integral. OUMG circumvents this issue by employing a discriminator model trained to differentiate between humangenerated and machinegenerated texts. The discriminator scores the generated texts with higher scores indicating greater similarity to humangenerated texts. This score serves as a proxy for the quality of the generated text. OUMG also supports guiding the text generation process potentially improving generated text quality. Experiments focusing on poetry generation demonstrate the metrics effectiveness and its ability to enhance the original models performance.  2. Strengths and Weaknesses Strengths:  Innovativeness: The paper presents a novel approach to evaluating text generation tasks where traditional referencebased metrics fall short. Its independence from reference standards addresses a critical gap in the current evaluation landscape.  Utility: The ability of OUMG to also guide text generation is a notable strength offering a practical tool for improving text generation models.  Comprehensive Experimentation: The paper provides a detailed experimental setup especially in the context of Chinese poetry generation showcasing OUMGs effectiveness empirically. Weaknesses:  Generalizability Concerns: The paper predominantly focuses on poetry generation. More evidence is needed to support the claim that OUMG is universally applicable across different text generation tasks without reference standards.  Comparison with Existing Methods: While the paper positions OUMG as superior to referencebased methods a more direct comparison with existing nonreference based methods if any would strengthen the case for OUMG.  Details on Training Data for the Discriminator: The paper lacks explicit details on how diverse and extensive the training data is for the discriminator. Since the models performance heavily relies on this more information would be beneficial. Questions and Feedback to the Author: 1. How does OUMG perform across diverse text generation tasks such as story generation where narrative and structural coherence are crucial 2. Can the authors provide more insight into the discriminators training process especially regarding the diversity of the dataset used 3. How does OUMG compare to other nonreferencebased evaluation metrics if applicable 4. The paper could benefit from a section discussing any limitations and potential biases of OUMG especially given its reliance on a discriminator model.  3. Review Summary The paper introduces OUMG a promising metric for evaluating text generation tasks where no fixed optimal answers exist. This approach is innovative and addresses a significant gap in text generation evaluation metrics. It is particularly relevant for creative text generation tasks such as poetry and story generation where traditional metrics based on reference matching are inadequate. The experimental results especially in the context of Chinese poetry generation are compelling and suggest that OUMG not only serves as an effective evaluation metric but also can guide the generation process to improve text quality. However the paper would benefit from a broader application of OUMG across various text generation tasks to fully substantiate its universality claim. Additionally a deeper dive into the training process of the discriminator including the diversity and size of the training dataset would enhance the papers credibility. Overall OUMG represents a significant step forward in evaluating text generation models offering an innovative solution to the longstanding issue of evaluating creative text generation tasks. With further refinement and broader testing OUMG has the potential to become a standard tool in the field of natural language generation.", "rqolQhuq6Hs": " Summarize the paper The paper explores an intriguing characteristic of Stochastic Gradient Descent (SGD) when applied to meansquare loss optimization within deep learning contexts. It is revealed that SGD experiences multiplicative noise complicating its dynamics. The authors propose a novel approach by deriving a stochastic differential equation (SDE) that simplifies this noise into an additive form through a process of random time change. This transformation effectively replaces the gradient of the loss function with the gradient of the logarithmized loss function leading to a new form of escape rate from local minima. Key to this discovery is the finding that the escape rate depends not on the direct loss barrier height but on the logarithmized version of this barrier further influenced by the number and magnitude of outlier eigenvalues of the Hessian at the local minimum. This result provides a theoretical underpinning to the observed preference of SGD for flat minima with low effective dimensions hinting at an inherent bias within the algorithm that favors certain types of solution spaces.  Strong and Weak Points Question and Feedback to the Author Strong Points: 1. Novel Contribution: The paper introduces a new method to approximate the SGD noise and derive an additivenoise SDE. This represents a significant theoretical advancement in understanding SGD dynamics. 2. Empirical Validation: Experiments conducted to validate the derivations specifically the proportional relationship between SGD noise strength and loss function strengthen the papers findings. 3. Practical Insights: The elucidation of why SGD prefers flat minima with fewer dimensions has practical implications for optimizing neural network training processes benefiting practitioners in the field. Weak Points: 1. Assumptions Limit Generalizability: The heavy reliance on approximations and assumptions particularly the decoupling approximation may limit the general applicability of the findings. It is unclear how well these results generalize to various architectures and loss functions beyond meansquare loss. 2. Complexity of Theoretical Derivations: The mathematical complexity of the derivations may make the paper less accessible to practitioners or researchers from adjacent fields potentially limiting its impact. 3. Experimental Setup Detail: While the paper includes experimental validation it could benefit from a more detailed description of the setup including hyperparameters and choice of datasets to ensure reproducibility. Question and Feedback to the Author: Could the authors provide further insight or evidence on how their findings might translate to more practical complex scenarios involving different network architectures or nonmeansquare loss functions It would enrich the paper to discuss or even speculate on the implications of the proposed SGD dynamics model in broader contexts. Additionally considering the assumptions necessary for the theoretical model it would be beneficial to discuss potential limitations these assumptions impose and how they might affect the interpretation of results in practical applications.  Review Summary The paper offers a significant theoretical contribution by proposing a novel way to model SGD dynamics revealing a preference of SGD for flat minima with low effective dimensions. This not only sheds light on an empirical fact seen in deep learning training regimes but also contributes to a deeper understanding of the implicit biases present in SGD. The experiments conducted to validate theoretical derivations strengthen the papers claims providing credibility to the proposed model. However the paper would benefit from addressing the generalizability of its findings beyond the specific conditions tested including different loss functions and network architectures. Additionally a thorough discussion on the limitations imposed by the necessary assumptions and potential avenues for future work to explore these areas could further enrich the papers contribution. Overall the paper makes an important step forward in understanding the intricate dynamics of SGD offering both theoretical insights and practical implications for the deep learning community. Its findings could have farreaching implications informing future research directions and potentially impacting how neural network training procedures are designed and implemented.", "yV4_fWe4nM": " Peer Review  1. Summary of the Paper The paper presents a novel framework for integrating fairness into deep clustering algorithms. It addresses the problem that deep clusterings representation learning capabilities might inadvertently result in unfair clustering for example by clustering portraits based on racial features. The authors define grouplevel fairness for clustering in the presence of protected status variables (PSVs) which can be binary or multistate. Their method involves formulating the problem as an integer linear programming (ILP) problem which is efficiently solvable due to its totally unimodular constraint matrix. They embed this solver into a deep clustering framework and propose a refinement learning algorithm that balances the clustering objectives with fairness constraints. Their experimental results on various datasets demonstrate that the proposed framework not only achieves competitive clustering performance but also ensures fairness according to the defined metrics. The paper provides extensive analysis including comparisons with stateoftheart fair clustering algorithms feature space visualization hyperparameter tuning and empirical convergence studies.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The paper tackles the underexplored area of incorporating grouplevel fairness into deep clustering making it an essential contribution to fair AI research.  Theoretical Soundness: The approach of formulating the fairness constraints as an ILP problem which is efficiently solvable is theoretically sound and novel in the context of deep clustering.  Extensive Experimental Evaluation: The authors provide a comprehensive experimental validation of their approach across different datasets and compared to existing methods showing the effectiveness of their method in ensuring fairness without significantly compromising clustering quality. Weak Points:  Model Complexity: The paper introduces considerable complexity in terms of the optimization problem formulation and the endtoend training process which might be challenging to replicate or extend.  Generalization and Scalability: While the paper presents results on specific datasets its less clear how the proposed method scales to larger datasets or how it performs across a broader range of domains.  Lack of Discussion on Ethical Implications: Although there is an ethics statement the discussion on potential misuse and broader societal implications of the developed techniques seems somewhat limited. Questions and Feedback:  Scalability: Can the authors provide more insights into the scalability of their approach especially concerning larger datasets and potentially higherdimensional PSVs  Model Complexity: How does the added complexity of the proposed framework impact the training time and computational resources required compared to traditional deep clustering approaches  Broader Implications: Could the authors expand on the potential for misuse of their algorithm specifically how unjust fairness constraints might be identified and mitigated  3. Review Summary and Recommendation The paper presents a significant advancement in the field of fair AI by developing a novel framework that integrates fairness into deep clustering algorithms. The authors approach is both innovative and theoretically sound and they provide a comprehensive set of experiments to validate their claims. However the complexity of the proposed method and questions around scalability and broader ethical implications suggest areas for improvement. Despite these concerns the strengths of the paper particularly its novelty rigorous experimental validation and the practical importance of the problem it addresses make it a valuable contribution. Hence I recommend this paper for publication provided the authors address the scalability concerns and elaborate on the ethical implications and potential for misuse of their technique. The papers contribution to enhancing fairness in AI tools makes it a noteworthy addition to the literature offering a strong foundation for future research in fair deep clustering.", "wfRZkDvxOqj": " Peer Review for \"MultiTask Neural Processes for MultiTask Learning\" 1) Summary of the Paper: This paper introduces MultiTask Neural Processes (MTNPs) a novel variant of neural processes tailored for multitask learning. It extends the methodology by incorporating a hierarchical Bayesian inference framework to explore task relatedness in the function space. This approach allows each task to benefit from shared knowledge derived from related tasks thus providing an inductive bias that improves prediction accuracy. The authors assert that MTNPs can learn multiple tasks simultaneously even with limited labeled data and across domains. Extensive experiments on benchmarks for multitask regression and classification demonstrate the superior performance of MTNPs in transferring knowledge among tasks and achieving higher accuracy in various applications including brain image segmentation. 2) Strong and Weak Points Questions and Feedback: Strong Points: a. Innovative Approach: The integration of a hierarchical Bayesian inference framework to define function priors based on shared knowledge among tasks is novel and wellthought. This methodological innovation expands the application of neural processes to multitask learning effectively. b. Extensive Evaluation: The paper provides a comprehensive experimental validation across multiple benchmarks and tasks. The diverse range of applications from regression to classification and image segmentation showcases the versatility and robustness of MTNPs. c. Theoretical and Practical Value: Offering both theoretical insights and practical implementations the paper stands out by not only proposing a new methodology but also providing empirical evidence to support its effectiveness. The inclusion of a reproducibility statement and code availability further enhances its value. Weak Points: a. Assumption on Task Relatedness: The performance of MTNPs heavily relies on the degree of relatedness among tasks. The paper could benefit from a deeper discussion or analysis on how the method performs when tasks are less related or completely unrelated. b. Complexity and Scalability Concerns: While the hierarchical Bayesian inference provides customization for taskspecific contexts it might introduce additional complexity to the model. The paper lacks a detailed discussion on the computational implications of adopting this framework and how it scales with the number of tasks or the size of data. c. Comparison with StateoftheArt Methods: Although the paper compares MTNPs with several baselines it would be beneficial to see a comparison against stateoftheart approaches in multitask learning especially those employing different methods of knowledge transfer and task relation exploration. Questions and Feedback: a. Impact of Task Unrelatedness: How does the introduction of unrelated tasks affect the performance of MTNPs Is there a mechanism to identify and mitigate the potential negative transfer b. Model Complexity and Optimization: Can the authors elaborate on the computational resources required for training MTNPs especially compared to traditional neural processes or other multitask learning frameworks c. Extension to Other Domains: Given the positive results in the tested benchmarks does the proposed MTNP framework have limitations in its application to other domains such as natural language processing or recommendation systems 3) Review Summary and Recommendation: The paper presents a significant advancement in multitask learning by introducing MultiTask Neural Processes which leverages a hierarchical Bayesian framework for effective knowledge transfer among related tasks. The innovative approach coupled with extensive evaluation demonstrates the potential of MTNPs in improving predictions across various tasks and domains with limited data. However the paper would benefit from a more detailed discussion on the implications of task unrelatedness the computational complexity of the proposed model and comparative analysis with stateoftheart approaches. Addressing these points would strengthen the papers contributions and offer clearer insights into its applicability and limitations. Considering the novel methodology promising results and the potential for wide application I recommend this paper for publication. Nonetheless addressing the feedback and questions raised could significantly enhance its impact and clarity for the research community.", "kcwyXtt7yDJ": "Summary of the Paper This paper introduces a novel method for domain adaptation (DA) named GraphRelational Domain Adaptation (GRDA) which considers the structural adjacency among domains encoded in a domain graph. Unlike traditional DA methods that enforce uniform alignment across all domains GRDA adapts based on the proximity of domains represented in the graph allowing for more flexible alignment. This method employs a unique adversarial learning framework featuring a graph discriminator which distinguishes it from classic DA approaches by aiming to reconstruct the domain graph from data encodings. The paper presents theoretical analysis demonstrating that GRDA achieves classic DA under a fully connected domain graph (clique) and facilitates intuitive domain alignment for nonclique graphs. Empirical evaluation on synthetic and realworld datasets showcases GRDA\u2019s superiority over existing DA techniques. Strengths: 1. Innovation in DA Approach: The papers introduction of using a domain graph for adapting domain relationships and the employment of a graph discriminator represent a significant innovation in the field of domain adaptation potentially opening new avenues for research. 2. Theoretical underpinning: The theoretical framework provided offers a solid foundation for understanding why GRDA works particularly its ability to recover classic DA under certain conditions and its behavior with various types of domain graphs. 3. Empirical Validation: The comprehensive empirical evaluation including experiments on both synthetic and realworld datasets provides strong evidence for the efficacy of GRDA compared to stateoftheart DA methods. Weaknesses: 1. Assumption of Domain Graph Availability: The method assumes the availability of a domain graph which may not be readily available in all practical applications. The paper does not address how one might construct or infer such a graph when its not provided. 2. Complexity and Scalability: The introduction of a graph discriminator may add complexity to the model potentially impacting its scalability and efficiency in handling very large or complex domain graphs. 3. Domain Graph Type Limitation: While the paper demonstrates GRDAs effectiveness with certain types of domain graphs (e.g. cliques stars chains) it is unclear how well the approach generalizes to more complex or irregular graph structures. Questions and Feedback for the Author: 1. Can the authors elaborate on strategies for constructing or inferring the domain graph in scenarios where such information is not readily available 2. How does the complexity of the graph discriminator impact the scalability of GRDA particularly in cases with a large number of domains or complex domain relationships 3. Have the authors considered the impact of inaccuracies in the domain graph on the performance of GRDA It would be informative to understand how sensitive GRDA is to errors in the domain graph structure. 4. The paper primarily discusses domain graphs with binary edges. Could the authors discuss potential extensions of GRDA to handle weighted edges in the domain graph representing varying degrees of domain similarity Review Summary: The paper presents an innovative approach to domain adaptation by leveraging graph relations among domains through a novel adversarial learning framework that includes a graph discriminator. This approach addresses a significant limitation in existing DA methods by allowing for nonuniform domain alignment based on the structural proximity of domains as encoded in a domain graph. The theoretical groundwork and empirical evaluations provided offer convincing evidence of the methods effectiveness and potential advantages over traditional DA techniques. However the reliance on the availability of a domain graph potential scalability issues due to increased model complexity and the methods generalization across different or inaccurately represented domain graphs are concerns that merit further exploration. Addressing these points could significantly enhance the applicability and robustness of GRDA. In conclusion GRDA represents a notable advancement in the domain adaptation field promising more flexible and potentially more accurate adaptation across diverse application scenarios. Further investigation into the methods limitations and potential extensions is warranted to fully realize its potential.", "xtZXWpXVbiK": " Review of \"FlOwbased Recurrent BElief State model (FORBES) for Continuous Partially Observable Markov Decision Processes\"  1) Summary The paper presents FORBES a novel model designed to address the challenges in solving Partially Observable Markov Decision Processes (POMDPs) particularly in highdimensional continuous spaces with unknown models. Traditional methods struggle with accurately determining the belief state\u2014a probability distribution over unobservable environment states based on historical data\u2014which is crucial for optimal decisionmaking. The authors propose incorporating normalizing flows into variational inference to construct flexible continuous belief states showing how these states can be effectively used with downstream Reinforcement Learning (RL) algorithms to improve performance and sample efficiency in visualmotor control tasks. The introduction of FORBES enables the learning of general continuous belief states facilitating multimodal predictions higher quality reconstructions and better performance in challenging tasks compared to existing Gaussianbased belief state approximations.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality and Novelty: The incorporation of normalizing flows into variational inference for learning belief states in POMDPs is a significant and novel contribution addressing wellknown limitations of current Gaussian approximation methods.  Theoretical Justification: The paper provides solid theoretical underpinnings for the approach including demonstrations of improved approximation of the true belief states theoretically.  Empirical Validation: Extensive experiments are conducted on visualmotor control tasks including a digit writing task. These demonstrate not only the feasibility of FORBES for complex belief state learning but also its superior performance over contemporary stateoftheart approaches. Weak Points:  Complexity and Scalability Issues: The paper does not thoroughly discuss the computational complexity and scalability of FORBES especially concerning the use of normalizing flows which can be computationally intensive.  Generalization across Different Environments: Although the method shows promise on tested environments the paper lacks a discussion on its generalizability across a wider range of POMDP problems.  Ablation Studies: While some ablation studies are presented more detailed experiments could help understand the contribution of each component of FORBES particularly the impact of the normalizing flows depth and structure on performance. Questions and Feedback: 1. Scalability and Efficiency: Can the authors comment on the computational scalability of FORBES especially when applied to very highdimensional state spaces or environments with complex dynamics 2. Model Extension: How adaptable is FORBES to environments with significantly different characteristics from the tested visualmotor control tasks 3. Ablation Studies: Could additional ablation studies provide insight into the contribution of normalizing flows to the models success Specifically comparing different configurations of normalizing flows in terms of their structure and depth might clarify their impact.  3) Review Summary The paper introduces FORBES a pioneering approach to learning general continuous belief states in POMDPs by utilizing normalizing flows substantially extending the capabilities of current methods predominantly based on Gaussian approximations. This approach not only addresses the inherent limitations of existing models but also pushes forward the frontier in solving complex sequential decisionmaking problems in continuous unknown environments. The theoretical foundation laid out for FORBES is robust and the experimental validation showcases its superior performance and sample efficiency in visualmotor control tasks. However areas such as the computational complexity scalability and broader applicability of FORBES remain somewhat underexplored. Given the originality of the approach its demonstrated empirical success and the potential for significant impact on solving POMDPs in a broad range of applications I recommend this paper for acceptance. Nonetheless addressing the raised concerns regarding scalability generalization and the detailed contributions of its components could strengthen the paper further.", "yCS5dckx_vj": " Review of the Paper: Generalizing DirectPred for SelfSupervised Learning in Neural Networks  Summary The paper extends the study of noncontrastive selfsupervised learning (ncSSL) specifically analyzing a generalized algorithm named DirectSet(\u03b1) which builds upon the DirectPred algorithm proposed by Tian et al. (2021). DirectPred was notable for setting the predictor directly using eigendecomposition of the input correlation matrix to avoid trivial solutions in representation learning without the need for negative pairs. The authors argue that DirectSet(\u03b1) can learn a desirable projection matrix in linear networks and reduce sample complexity for downstream tasks. Through theoretical analysis and empirical evaluation on benchmark datasets (CIFAR10 CIFAR100 STL10 and ImageNet) the paper introduces DirectCopy a simplified version of DirectPred that omits the eigendecomposition step offering computational efficiency while maintaining or improving performance.  Strong Points 1. Theoretical Contributions: The paper makes significant theoretical contributions by explaining the mechanism through which ncSSL algorithms can successfully learn nontrivial representations. It introduces the concept of weight decay as an implicit threshold to discard features with high variance under augmentation retaining only invariant features and presents the first sample complexity result in ncSSL to our knowledge. 2. Practical Impact: By simplifying DirectPred into DirectCopy the authors directly address the computational inefficiencies of eigendecomposition in the former method. The practical implications are substantial offering a more efficient algorithm without sacrificing performance which is evidenced by the empirical results across multiple datasets. 3. Empirical Validation: The comprehensive experiments not only validate the theoretical analysis but also demonstrate the robustness and effectiveness of DirectCopy. The authors have thoroughly compared DirectCopy against DirectPred and other baselines under various settings using popular benchmark datasets.  Weak Points 1. Limited Discussion on Generalizability: While the paper presents results from extensive experiments there is a somewhat limited discussion on the generalizability of DirectCopy beyond the datasets and tasks considered. Theoretical analysis is primarily framed within linear network contexts and its applicability to more complex network architectures or differing domains (such as text or audio) remains somewhat unclear. 2. Analysis of Feature Selection Process: The role of weight decay in feature selection while novel is treated mostly from a theoretical standpoint. There could be a deeper empirical investigation into how exactly weight decay influences feature selection across different augmentation strategies or datasets particularly in nonlinear or more complex models. 3. Comparison with Contrastive Learning Approaches: The paper focuses on comparing DirectCopy with its predecessor (DirectPred) and does not delve deeply into a comparison with contemporary contrastive learning methods. Given that the authors mention these methods in the beginning a direct comparison might have provided clearer insights into where ncSSL stands in the broader landscape of selfsupervised learning.  Questions and Feedback to the Author 1. Generalizability to NonLinear Networks: Can the authors elaborate on how the theories proposed (particularly regarding weight decay and feature variance) extend to nonlinear or more complex network architectures 2. Impact of Different Augmentation Strategies: How sensitive is DirectCopy to the choice of data augmentation strategies Does the performance heavily depend on the variance introduced by these augmentations and how does it compare across different types of augmentations 3. Comparison with Contrastive Learning: While the focus on improving ncSSL is clear how does DirectCopys performance compare to stateoftheart contrastive learning methods especially in scenarios with highly limited labeled data  Review Summary This paper provides a valuable contribution to the field of selfsupervised learning by extending the understanding of noncontrastive methods specifically through the theoretical and empirical development of DirectCopy. The authors successfully address a significant limitation of DirectPred offering a more practical algorithm that achieves competitive performance across several benchmarks. While the paper advances our understanding of how representations are learned in ncSSL and proposes a more efficient learning method it leaves some areas underexplored such as the generalizability of its findings to more complex models and a broader comparative analysis with contrastive learning methods. Expanding the discussion in these areas could further strengthen the papers contributions. Despite these minor weaknesses the strengths and contributions of the paper are clear and substantial. The theoretical insights combined with strong empirical performances position this work as an important step forward in the understanding and practical application of ncSSL methods. The simplification of DirectPred to DirectCopy is particularly noteworthy for its potential impact on reducing computational costs while preserving or enhancing model performance.", "sX3XaHwotOg": " Review of \"AMOS: Adversarial Mixture Of Signals for Efficient Pretraining of Text Encoders\"  1. Summary The paper introduces AMOS an innovative framework designed to improve text encoder pretraining efficiency and effectiveness through an adversarial learning curriculum that utilizes a mixture of signals from multiple auxiliary generators. Unlike its predecessor ELECTRA which employs a single masked language model (MLM) as the generator AMOS simultaneously trains multiple MLMs of varying sizes. This diversity provides training signals at different levels of difficulty significantly enhancing the learning process. AMOSs novelty lies in its ability to seamlessly blend these signals into a unified model adjusting the mixture weights to challenge the discriminator further and enhance learning outcomes. The frameworks effectiveness is demonstrated through improved performance on the GLUE and SQuAD benchmarks where AMOS outperforms both ELECTRA and other stateoftheart models.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: AMOSs method of utilizing multiple generators to produce a diverse set of pretraining signals is novel and intelligently designed. Its ability to adjust these signals dynamically based on the discriminators feedback allows for a more robust and effective learning process.  Empirical Validation: The paper provides comprehensive empirical evidence to back its claims. The improvements over ELECTRA and other stateoftheart models across various benchmarks are significant and compelling.  Efficiency Improvement: By assembling multiple MLMs into one model and employing an adversarial learning curriculum AMOS not only improves the quality of pretraining but also its efficiency addressing a critical challenge in the pretraining of text encoders. Weak Points:  Complexity and Comprehensibility: The complexity of the AMOS framework might pose challenges in comprehension and implementation. The paper could benefit from a more detailed explanation or visualization of the model architecture and training dynamics especially for readers not deeply familiar with adversarial training methods.  Scalability Concerns: While the paper mentions AMOSs efficiency theres limited discussion on the scalability of the framework particularly concerning training time and computational resources as the model scales to larger datasets or more complex scenarios.  Generalization Beyond Benchmarks: The paper effectively demonstrates AMOSs performance on GLUE and SQuAD benchmarks. However it does not explicitly address how well AMOS generalizes to realworld datasets or tasks that might present challenges that are not covered by these benchmarks. Questions and Feedback:  Could the authors shed more light on how the framework might be adapted or extended beyond the text encoding tasks evaluated For instance its application in tasks requiring a deeper understanding of context or sequential data.  The use of multiple generators introduces additional hyperparameters (e.g. the sizes and mixture weights of generators). How sensitive is AMOSs performance to these hyperparameters and what strategies are recommended for their selection  Regarding the adversarial training aspect were there any particularly challenging scenarios encountered during development Examples might include cases where the discriminator quickly overpowered the generators or vice versa and how these were mitigated.  3. Review Summary The paper presents a compelling advancement in the pretraining of text encoders coupling the innovative use of multiple auxiliary generators with an adversarial learning curriculum. This approach not only addresses efficiency and effectiveness concerns in current methodologies but also demonstrates significant empirical improvements on established benchmarks. Despite its strengths the paper could provide more clarity on the frameworks scalability generalization capabilities and the challenges of hyperparameter selection. Given the outlined contributions and the empirical evidence supporting them the recommendation for this paper is strong acceptance with a suggestion to address the raised questions and feedback for an even more robust contribution.", "lD8qAOTu5FJ": " Summary This paper introduces the KnowledgeAware contiNual learner (KAN) a novel approach designed to address the stabilityplasticity dilemma in continual learning (CL) focused on classincremental learning (classIL). The core innovation of KAN is leveraging the semantic similarity between old and new classes to enhance model adaptation when facing new tasks without forgetting previously learned ones. The authors argue that by identifying and exploiting existing relevant knowledge its possible to achieve a balance between forward transfer (learning new tasks effectively) and mitigating forgetting (preserving performance on previous tasks).  Strong and Weak Points Strong Points: 1. Innovative Approach: The paper tackles the common problem of catastrophic forgetting in a unique way by focusing on the semantic similarities between tasks to efficiently allocate and reuse model components. 2. Comprehensive Experiments: The experiments are thorough comparing KAN against various baselines and within different scenarios of task similarities. This provides a clear understanding of the models performance across a broad spectrum of conditions. 3. Resource Efficiency: The KAN model\u2019s ability to reuse components and apply selective updates shows a promising path towards building more resourceefficient CL systems an essential criterion for practical applications. Weak Points: 1. Complexity in Implementation: The paper while thorough in its experimentation hints at a potentially complex implementation process which might hinder widespread adoption without welldocumented code and guidelines (which the authors promise to make available). 2. Ambiguity in Class Similarity Detection: Although the paper briefly mentions approaches for determining class similarities the details are somewhat vague. This critical component could dramatically affect the model\u2019s ability to properly leverage semantic similarities. 3. Limited Generalization Discussion: The paper predominantly focuses on CIFAR10 and CIFAR100 datasets. More insights into how KAN would perform or adapt to datasets with higher complexity or less clearly defined semantic relations would be beneficial. Questions and Feedback to the Author: 1. Code Availability and Usability: Could the authors provide more insight into the support and documentation we can expect alongside the code This would greatly aid in the adoption and further development of KAN. 2. Further Clarification on Class Similarity Detection: How robust is KAN to variations in class similarity detection effectiveness Are there fallback mechanisms in case of inaccurate similarity assessments 3. Generalization Capabilities: How might KAN be adapted or perform when applied to datasets beyond CIFAR10 and CIFAR100 especially those with more complex or less distinct classes  Review Summary The paper presents a significant advancement in addressing the stabilityplasticity dilemma within continual learning through the introduction of KAN. This models focus on semantic similarities provides a novel perspective on mitigating catastrophic forgetting while enhancing forward transfer. The experiments showcase KANs effectiveness across different task similarities highlighting its potential to increase resource efficiency a notable advantage over existing methods. However the practical complexity of implementing KAN the potential challenges around accurately detecting class similarities and the papers limited discussion on generalizing to more diverse or complex datasets are areas that warrant further exploration. Despite these considerations the strength of the experimental results and the innovative approach to continual learning suggests that KAN represents a meaningful step forward in the field. I recommend the acceptance of this paper but I also encourage the authors to address the identified weaknesses particularly around the practical application and generalization capabilities of KAN to strengthen the paper further.", "vaRCHVj0uGI": "Summary of the Paper: This paper introduces an innovative approach to reconstruct medical images from partial measurements in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) by leveraging unsupervised learning. Traditional methods rely on supervised learning techniques that map directly from measurements to images requiring a paired dataset that is synthesized with a fixed physical model. This approach faces challenges in generalizing to new measurement processes. The authors propose a solution using scorebased generative models without the need for paired data making the method adaptable to various measurement processes. By training a scorebased generative model on a collection of medical images to learn their prior distribution the method can then sample from this distribution conditioned on observed measurements. The authors demonstrate that this approach not only competes with but in some cases outperforms supervised methods in terms of generalization to unknown measurement processes and quality of reconstructed images in CT and MRI contexts. Strong Points: 1. Innovativeness: The paper introduces a novel unsupervised approach using scorebased generative models for medical image reconstruction. This methods flexibility to adapt to different measurement processes without retraining is particularly innovative. 2. Generalization Capability: The authors successfully show the methods superior generalization to new measurement processes addressing a significant limitation of existing supervised learning methods. This feature is highly valuable in clinical settings where different machines and protocols can lead to variations in measurement processes. 3. Empirical Evidence: The paper provides strong empirical evidence supporting their claims with the method outperforming or being competitive with supervised counterparts in various tasks and datasets. Including performance on clinical CT data further emphasizes the methods practical relevance. Weak Points: 1. Complexity and Accessibility: Scorebased generative models are comparatively more complex than some traditional approaches potentially limiting the methods accessibility and adaptability in practical clinical settings. The paper could benefit from a discussion on computational requirements and ease of implementation. 2. Comparison with Latest Techniques: While the method surpasses several supervised and unsupervised baselines its unclear how it compares against the very latest in medical imaging reconstruction techniques especially other unsupervised or semisupervised methods that have emerged. 3. Detailed Evaluation Metrics: The evaluation primarily focuses on PSNR and SSIM which while standard do not fully capture clinical efficacy. Additional metrics or evaluations such as clinical readability tests by radiologists could provide more comprehensive insights into the practical utility of the reconstructed images. Questions and Feedback to the Author: 1. Could the authors elaborate on the computational cost relative to traditional supervised methods Considering the iterative sampling required by scorebased generative models understanding the tradeoff between performance and computational efficiency would be beneficial. 2. Have the authors considered integrating more recent or alternative generative models to compare performance It would be interesting to see if advancements in generative modeling further improve the methodologys efficacy. 3. Regarding the evaluation metrics could the authors include more domainspecific criteria such as the ability of the reconstructed images to reveal medically relevant details that align with clinical diagnoses Review Summary: The paper presents a significant advancement in the field of medical image reconstruction by introducing a fully unsupervised technique using scorebased generative models. This methods ability to generalize to new measurement processes represents a considerable step forward in addressing a critical limitation of existing supervised techniques. While the theoretical and empirical grounding of the method is strong there are areas where additional details and broader comparisons could further solidify the papers contributions. Questions about computational costs comparisons with the latest generative models and more comprehensive evaluation metrics are areas where the paper could provide further insights. These points notwithstanding the overall significance and quality of the work are high making it a valuable contribution to the field.", "hcoswsDHNAW": " Summarize the Paper The paper introduced Fast AdvProp an efficient approach to optimizing adversarial propagation (AdvProp) for training recognition models without increasing computational costs. The authors explore the idea that improvements in model performance through AdvProp do not necessarily require extensive use of adversarial examples or multistep adversarial attacks. By repositioning adversarial examples as a supplementary part of the training set and integrating techniques for accelerating adversarial training Fast AdvProp is able to enhance model accuracy and robustness on various benchmarks while maintaining a computational cost comparable to vanilla training methods. The paper provides empirical evidence showing that Fast AdvProp can improve performance on ImageNet and other datasets especially when used with larger models and in combination with other data augmentation techniques. The proposed method addresses the issue of high training costs associated with AdvProp making it more accessible for wider use.  Strong Points 1. Efficiency Improvement: The paper successfully addresses a significant drawback of AdvProp  its high computational cost. By simplifying adversarial training components and reducing the reliance on adversarial data Fast AdvProp achieves costeffective improvements in model performance. 2. Empirical Validation: The authors provide comprehensive empirical results across multiple datasets and models showcasing Fast AdvProp\u2019s ability to improve both accuracy and robustness without incurring extra computational costs. 3. Compatibility and Scalability: The methods compatibility with popular data augmentation techniques and its scalability with larger models are welldemonstrated highlighting its practical applicability in a broad range of scenarios. 4. Opensource Contribution: Making the code available is commendable as it encourages reproducibility and enables the community to further explore validate or build upon the work.  Weak Points and Questions 1. Broader Impact Discussion: The paper focuses heavily on the technical advancement and empirical results but lacks a discussion on the broader impact of the method especially in realworld applications or potential negative implications. 2. Comparative Analysis Depth: While the paper provides a comparison with vanilla training baselines it would be beneficial to see a more detailed comparison with other stateoftheart adversarial training methods discussing the tradeoffs in more depth. 3. Limitations and Failure Cases: The paper could improve by discussing any limitations or specific scenarios where Fast AdvProp might not perform as expected. Understanding these aspects can guide future work and applications. 4. Robustness Testing Depth: The paper tests robustness on multiple benchmarks but does not explore adversarial robustness extensively which could be a critical aspect considering the adversarial nature of the training method.  Feedback to the Author The paper makes a significant contribution to efficient adversarial training proposing a method that balances performance improvements with computational costs. However expanding the comparative analysis addressing the broader impacts discussing limitations and exploring adversarial robustness in more depth would strengthen the paper. Additionally incorporating feedback on these aspects would not only provide a clearer understanding of Fast AdvProp\u2019s position within the field but also highlight potential directions for future research.  Review Summary The paper introduces an innovative approach Fast AdvProp to reduce the computational costs associated with adversarial propagation techniques while retaining and in some aspects improving model performance on various benchmarks. This work addresses a critical barrier to the widespread adoption of AdvProp and makes a valuable contribution to the field of adversarial training. However by expanding the discussion on broader impacts comparative analyses and limitations the paper could provide a more comprehensive insight into the methods applicability and potential areas for further research. Despite these suggestions for improvement the methods compatibility with existing data augmentation methods scalability with larger models and opensource code contribution are significant strengths. Overall this paper is a solid work with meaningful contributions to efficient adversarial training practices deserving of positive consideration.", "uYLFoz1vlAC": "Review of \"Structured State Space Sequence Model for LongRange Dependencies\" 1) Summary The paper introduces a novel sequence modeling technique named Structured State Space (S4) sequence model targeting the efficient modeling of longrange dependencies (LRDs) across various data modalities such as images audio and text. The S4 model innovatively reparameterizes the state space model (SSM) with a combination of lowrank correction and skewsymmetry enabling scalable and efficient computations through reduction to a Cauchy kernel problem. This approach significantly improves computational efficiency both theoretically and empirically achieving stateoftheart (SoTA) results on numerous benchmarks including the challenging Long Range Arena (LRA) benchmark and sequential CIFAR10. The authors argue that the S4 model can serve as a generalpurpose sequence model that competes or even surpasses specialized models in various tasks without requiring substantial domainspecific modifications. 2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Methodology: The introduction of a new SSM reparameterization that allows efficient computation is notably innovative. Treating the SSM as a diagonal plus lowrank problem facilitates the use of theoretical results like the Woodbury identity and Cauchys kernel computation rendering an efficient algorithmic solution for sequence modeling.  Broad Applicability: The paper demonstrates S4\u2019s capabilities across a diverse set of tasks and data modalities indicating its potential as a versatile tool in the machine learning toolkit. Particularly impressive are its performances on the LRA benchmark and in generative modeling tasks.  Empirical Results: The empirical validations showcasing SoTA performances and significant improvements over both LSSL and various Transformer models strongly support the papers claims. The comparison is comprehensive covering training efficiency memory usage and accuracy across benchmarks. Weak Points:  Robustness and Sensitivity Analysis: While the paper provides substantial empirical evidence of S4s effectiveness there is a lack of discussion regarding the models robustness to hyperparameter settings and sensitivity analysis which could be critical for practical adoption and understanding the models limitations.  Comparison with Recent Innovations: The field of sequence modeling is rapidly evolving with several new models proposed since the benchmarks cited were published. An analysis or discussion comparing S4 with these very recent advances would have been beneficial to understand its competitive edge more comprehensively.  Insights on Learned Representations: The paper briefly mentions the interpretability of learned representations particularly in the context of the PathX task. However it stops short of providing deeper insights or broad analysis on what makes S4 effective at capturing LRDs beyond computational efficiency. Questions and Feedback: 1. Could the authors provide insights or empirical evidence on the sensitivity of S4\u2019s performance to its hyperparameter settings particularly the lowrank approximations rank 2. How does S4 perform in settings with irregularly sampled data or where the sequence length dynamically varies significantly which is common in many realworld applications 3. The authors discuss the potential of S4 as a generalpurpose model but mainly focus on tasks traditionally considered challenging due to LRDs. How does S4 perform on shorter sequence tasks compared to specialized models 4. Can the authors comment on any limitations or potential drawbacks of the S4 model when deployed in practical largescale systems especially in data streaming or realtime prediction scenarios 3) Review Summary The paper \"Structured State Space Sequence Model for LongRange Dependencies\" introduces a compelling new approach to sequence modeling that theoretically and empirically addresses the challenges of modeling longrange dependencies efficiently. The S4 models innovative parameterization strategy demonstrates remarkable improvements in computational efficiency and stateoftheart performance across a variety of benchmarks. The broad applicability and impressive empirical results signify a significant advancement in the field of sequence modeling. However a deeper exploration into model robustness the comparison with the latest sequence modeling advancements and a more detailed analysis of learned representations could further strengthen the paper. Answers to questions regarding hyperparameter sensitivity performance in more versatile settings and potential limitations in practical applications would be valuable additions. Overall the paper presents a substantial contribution to sequence modeling research showcasing a promising direction towards developing universally effective sequence models. Given the strong empirical results comprehensive experimentation and the potential for wide applicability I recommend this paper for publication subject to addressing the discussed points for further strengthening the work.", "jJOjjiZHy3h": " Peer Review  1. Summary The paper proposes AdversarialAugment (AdA) a novel adversarial training procedure that aims to enhance the robustness of neural network models to common and adversariallychosen image corruptions while also improving performance against `pnorm bounded perturbations. Unlike existing methods that employ handcrafted heuristics AdA utilizes arbitrary pretrained imagetoimage models optimizing their weights to generate adversarially corrupted images. Through a combination of theoretical analysis and empirical testing the authors demonstrate that models trained with AdA outperform those trained with current stateoftheart methods (such as AugMix and DeepAugment) on popular benchmarks like CIFAR10C and IMAGENET. Additionally the paper contributes to the literature by providing sufficient conditions for the consistency of AdA and comparing its theoretical framework to that of existing methods.  2. Strengths and Weaknesses Strong Points:  The idea of leveraging pretrained imagetoimage models for generating adversarial corruptions is novel and potentially impactful as it moves beyond the limitations of handcrafted heuristics.  The paper provides rigorous theoretical analysis including sufficient conditions for consistency and PACBayesian performance guarantees which is commendable and enhances the readers understanding of the methods robustness.  Empirical results show significant improvements over existing methods on CIFAR10C and IMAGENET benchmarks which reinforce the practical utility of AdA in enhancing model robustness to image corruptions.  The combination of AdA with existing data augmentation strategies (AugMix and DeepAugment) to achieve stateoftheart results is a good demonstration of the methods compatibility and effectiveness in conjunction with other approaches. Weak Points:  The computational complexity of the proposed AdA method could limit its applicability in practice. The paper mentions this as a limitation but more detailed comparisons of computational costs and potential optimizations would be beneficial.  While the paper discusses the theoretical underpinnings of AdA it could provide more insights into the failure modes of the method and the types of corruptions or adversarial attacks where it might not perform well.  The analysis mainly focuses on `pnorm bounded perturbations it would be interesting to see how AdA performs against a broader range of adversarial attack types. Questions and Feedback: 1. Could the authors elaborate on potential approaches to reduce the computational demands of AdA while maintaining its effectiveness 2. How does AdA perform against more sophisticated or novel adversarial attacks beyond `pnorm bounded perturbations 3. Can the authors discuss any failure modes identified during the development or testing of AdA Understanding where the method might struggle could be crucial for future improvements.  3. Review Summary This paper introduces AdversarialAugment (AdA) a compelling approach to improving neural networks robustness against common image corruptions and `pnorm bounded perturbations by optimizing the weights of pretrained imagetoimage models. The extensive theoretical analysis and promising empirical results position AdA as an attractive alternative to existing data augmentation and adversarial training methods. However the significant computational requirements of AdA and the need for further exploration of its limitations and performance against a wider array of adversarial attacks suggest areas for future research. Overall the novel methodology combined with both theoretical and empirical substantiation make a strong case for the potential of AdA in advancing the field of robust image classification. Supporting Arguments:  The methods innovative use of pretrained imagetoimage models for generating adversarial corruptions could set a new direction for future work in adversarial robustness and data augmentation.  Comprehensive theoretical backing and superior empirical results on established benchmarks provide strong evidence of AdAs effectiveness and potential impact.  While acknowledging its limitations the paper opens up several avenues for ongoing and future work to address the challenges of computational efficiency and broader attack resistance. Based on these evaluations the paper makes a valuable contribution to the literature on adversarial robustness and data augmentation warranting its publication with minor revisions to address the outlined weak points and questions.", "qCBmozgVr9r": " Peer Review of \"Rapid Learning of Novel Attributes in FewShot Attribute Learning\"  1) Summary The paper introduces a novel paradigm in machine learning specifically in the context of fewshot learning (FSL) termed fewshot attribute learning (FSAL). Unlike traditional FSL that focuses on learning new classes with limited examples FSAL aims at learning new attributes that were not previously labeled enhancing a models ability to adapt and learn continually. The authors argue that learning new attributes presents a more challenging problem than classbased FSL as it requires the model to generalize based on attributes rather than fixed class labels. To address this challenge they propose leveraging unsupervised contrastive pretraining to learn generalpurpose features followed by supervised finetuning on limited labeled data. The efficacy of their approach is validated through experiments on new benchmark datasets: CelebA Zappos50K and ImageNetwithAttributes demonstrating superior performance in learning novel attributes compared to standard supervised methods and other fewshot learners.  2) Evaluation  Strong Points: 1. Novelty: The introduction of FSAL as a learning paradigm extends the boundaries of fewshot learning and highlights the importance of attribute knowledge in machine learning models reflecting a more humanlike learning process. 2. Methodological Rigor: The approach combines unsupervised pretraining and supervised finetuning in a novel way to address the challenge of attribute generalization which is well justified and empirically successful. 3. Comprehensive Experiments: The experiments are wellstructured spanning several datasets and comparing a wide array of methods providing a clear picture of the strengths of the proposed approach.  Weak Points: 1. Lack of Theoretical Insights: While the method shows practical effectiveness the paper lacks a theoretical analysis of why contrastive pretraining followed by finetuning works particularly well for attribute learning. 2. Model Complexity: The paper does not thoroughly discuss the computational implications of adopting unsupervised pretraining which can be significant especially considering the largescale datasets involved. 3. Generalization to NonVisual Attributes: The focus is predominantly on visual attributes. It remains unclear how the proposed FSAL framework might adapt to nonvisual data or more abstract attributes.  Questions and Feedback: 1. Could the authors elaborate on the theoretical underpinnings that make contrastive learning particularly suited for FSAL 2. It would be beneficial to discuss the computational costs of the unsupervised pretraining stage and potential methods to mitigate these costs. 3. Can the authors speculate or provide initial insights into how their FSAL approach might extend to learning nonvisual attributes or attributes in different modalities (e.g. text)  3) Review Summary The paper addresses an underexplored aspect of fewshot learning\u2014rapidly learning new attributes with limited examples. The introduction of FSAL represents a significant stride toward more adaptive machine learning models. The methodology combining unsupervised pretraining with supervised finetuning showcases a compelling strategy for enabling models to learn novel attributes effectively. The experimental validation is robust spanning multiple datasets and baselines illustrating the approachs superiority in learning novel attributes. However the paper could be strengthened by providing more theoretical insight into why the proposed method works well for FSAL discussing the computational aspects of the approach in greater depth and considering its application beyond visual attributes. Addressing these points could elevate the contribution further offering a more comprehensive foundation for future research in attributebased learning and continual learning scenarios. Overall the paper constitutes a valuable contribution to the field opening new avenues for research in fewshot learning and attribute representation in machine learning. The proposed FSAL paradigm has the potential to inform the development of more flexible and adaptive learning systems.", "hOaYDFpQk3g": " 1. Summarize the paper The paper introduces LightWaveS a novel framework designed to address the challenges of multivariate time series classification (MTSC). It arises from the need for models that not only achieve high accuracy but also maintain practical deployment concerns like prediction speed especially in edge computing environments. The paper critiques existing approaches like ROCKET MINIROCKET and MultiRocket for their extensive feature use which while enhancing accuracy detrimentally affects inference time. LightWaveS leverages wavelet scattering transformations and distributed feature selection to drastically reduce feature count (to 2.5 of ROCKETs) without sacrificing accuracy. Its distributed architecture allows for scaling with additional nodes and channels offering significant inference speed improvements (9x65x faster than ROCKET) and insight into useful data channels by selective channel retention. The methodologys effectiveness is validated across various datasets showing comparable accuracy to stateoftheart solutions with much faster training and inference times particularly on edge devices.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Innovative Approach: The paper successfully combines wavelet scattering distributed computing and feature selection to address both the accuracy and speed challenges in MTSC.  Efficiency and Scalability: LightWaveS shows impressive improvements in training and inference speeds without compromising on accuracy. Its scalability with additional nodes and channels is welldemonstrated and valuable for practical applications especially in edge computing scenarios.  Insightful Channel Reduction: The ability to reduce input channels significantly while maintaining model performance provides not just computational benefits but also insights into the most informative channels for a given problem.  Weak Points:  Comparative Analysis Depth: While comparisons are made with (MINI)ROCKET and other models a deeper analysis into why certain datasets do not perform as well could provide more insight into LightWaveSs limitations.  Generalizability Concerns: The paper shows impressive results across selected datasets but lacks a discussion on the models performance in datasets with extremely highdimensional timeseries data or in highly noisy environments.  Parameter Selection and Tuning: There is brief mention of empirical experimentation for parameter selection but detailed insights into how these choices affect model performance could enhance the papers value.  Questions and Feedback:  Question: Can you elaborate on the impact of the feature selection process on the models interpretability particularly in contexts where understanding model decisions is critical  Feedback: It would be beneficial to incorporate a more detailed discussion on the robustness of LightWaveS across varied data properties including datasets with high noise levels and those requiring significant feature engineering.  Suggestions for Improvement:  Experimental Robustness: Extend comparisons to include more varied datasets especially those posing challenges not addressed by the current evaluation.  Parameter Exploration: Provide a more comprehensive exploration of the parameter space including the effects of different kernel choices dilations and the rationale behind specific selection criteria.  Interpretability Focus: Given the channel reduction capability delve deeper into how LightWaveS can aid interpretability in MTSC problems possibly with case studies highlighting this aspect.  3. Review Summary The paper presents a significant contribution to MTSC through LightWaveS striking a commendable balance between accuracy and computational efficiency. Its innovative use of wavelet scattering transformation combined with distributed feature selection positions it as a highly valuable approach for realworld applications particularly in edge computing scenarios where resources are constrained. The demonstrated scalability and efficiency improvements are compelling however the paper could benefit from a more granular analysis of model performance variation across datasets and an expanded discussion on robustness in different operational environments. Addressing these areas could elevate the papers impact further. My recommendation is for acceptance contingent upon addressing the highlighted concerns particularly around model generalizability and parameter selection justification. LightWaveSs approach to MTSC is not only innovative but highly relevant for current and future deployment scenarios demanding speed and accuracy.", "xDIvIqQ3DXD": "Summarize the Paper: The paper investigates the approximation properties of recurrent encoderdecoder architectures especially within a linear framework. It specifically demonstrates how these architectures generalize RNNs by efficiently learning timeinhomogeneous temporal relationships and how they are structured around a unique \"temporal product structure\". Through a mathematical formulation the study establishes a universal approximation result for these architectures explains the significance of the temporal product structure in determining the approximation efficiency and outlines how encoderdecoder models differ from RNNs by their ability to adapt to different temporal structures of inputoutput relationships. The paper concludes with numerical illustrations supporting the theoretical findings. Strong Points: 1. Theoretical Depth: The paper presents a comprehensive theoretical analysis of the approximation properties of encoderdecoder architectures filling a gap in the current understanding of these models. The introduction of the temporal product structure as a key differentiating factor is particularly noteworthy. 2. Generalization of RNNs: Demonstrating how encoderdecoder structures can approximate timeinhomogeneous relationships beyond the capabilities of RNNs provides valuable insights into the versatility and potential applications of these models. 3. Mathematical Rigor: The mathematical formulation of the problem along with the definitions and theorems provided are both detailed and rigorous. The inclusion of proofs in the appendices enhances the papers credibility. 4. Numerical Illustrations: The inclusion of numerical examples to illustrate theoretical findings enhances the readers\u2019 understanding and supports the theoretical results with empirical evidence. Weak Points: 1. Complexity for Practitioners: While theoretically rich the paper could be challenging for practitioners to digest and apply. Simplifying the presentation or providing more intuitive explanations could improve its accessibility. 2. Linear Setting Assumption: The paper focuses on linear settings which may limit its applicability to nonlinear realworld sequence modeling tasks. Exploring nonlinear approximations would be beneficial. 3. Limited Consideration of Computational Efficiency: There is little discussion on the computational implications of using encoderdecoder models particularly concerning the size of the coding vector and its effect on training and inference times. Questions and Feedback to the Author: 1. Could the authors clarify how the results might extend to nonlinear settings if at all This would greatly enhance the papers relevance to a broader range of sequence modeling tasks. 2. Is there an intuition or simplified explanation that could make the concept of the temporal product structure more accessible to practitioners interested in applying these findings 3. Have the authors considered the computational efficiency of the models studied particularly in relation to the dimension of the coding vector and its practical implications 4. It would be interesting to see some discussion on potential limitations of the proposed framework in realworld applications or scenarios where it might not perform as expected. Review Summary: The paper provides a significant theoretical contribution to understanding the approximation properties of recurrent encoderdecoder architectures emphasizing how these models generalize RNNs and introduce a unique temporal product structure. The mathematical rigor and the inclusion of numerical illustrations are strong aspects of this work. However the focus on linear settings and the limited discussion on computational efficiency could be considered drawbacks. Feedback for the authors includes clarifying the extension to nonlinear settings simplifying the explanation of key concepts for broader accessibility and addressing computational efficiency. Despite these points the paper forms an important step towards comprehensively understanding modern deep learning architectures for sequence modeling. The theoretical depth and analytical approach are commendable and with some refinement the paper could significantly impact both the theoretical and practical aspects of machine learning with encoderdecoder models.", "gKWxifgJVP": " Review of \"FOCAL REASONER: FactDriven Logical Reasoning Model for Machine Reading Comprehension\"  1) Summary of the paper The paper presents FOCAL REASONER a new model designed to tackle the challenges of logical reasoning within machine reading comprehension (MRC). Recognizing the limitations of existing models that primarily focus on global entityaware knowledge the authors propose a more nuanced approach that considers both global knowledge and local detailed facts or events. These local insights are often overlooked but are critical in logical reasoning tasks. To this end FOCAL REASONER leverages fact units derived from syntactic processing of text (e.g. subjectverbobject trios) to capture both global connections and local details within a supergraph framework. This dual focus is shown to dramatically improve performance on logic reasoning benchmarks compared to baseline models. The methodology involves extracting fact units constructing a supergraph from these units and then performing reasoning over this graph. The approach is evaluated across several benchmarks including ReClor and LogiQA for logic reasoning and MuTual for dialogue reasoning showcasing significant improvements in handling logic reasoning tasks.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The innovation in leveraging both global knowledge and local specific factsevents through fact units within a supergraph structure is commendable. This broader consideration of facts beyond traditional named entities represents a significant leap forward in modeling for NLP.  Performance: FOCAL REASONERs stateoftheart performance on challenging benchmarks is a strong testament to the models effectiveness. This signifies a meaningful advancement in the NLP field specifically for logic reasoning tasks.  Interpretability Case Study: The inclusion of a case study to demonstrate the models reasoning process offers insight into how FOCAL REASONER bridges context question and option enhancing transparency and interpretability of model predictions. Weak Points:  Model Complexity: While the paper mentions the models parameter size a deeper discussion recounting the computational complexity especially in comparison with baseline models would be beneficial for understanding practicality and scalability.  Generalization: The paper could benefit from more extensive discussions or experiments on the models generalizability across various NLP tasks beyond the tested benchmarks to underline its adaptability. Questions and Feedback:  Transferability and Adaptation: How easily can FOCAL REASONER be adapted or transferred to other NLP tasks outside the domains it was tested on given its reliance on fact units and supergraph construction  Detailed Model Complexity Analysis: Could the authors provide a more detailed discussion on the computational complexity of FOCAL REASONER Understanding its performance in terms of computational resources and time efficiency is crucial for realworld applications.  Impact of Fact Unit Granularity: The paper mentions replacing fact units with named entities and semantic roles to test performance implications. However it would be interesting to explore how varying levels of granularity in fact units impact the models reasoning capabilities and performance.  3) Review Summary and Recommendation The paper introduces a novel and effectively designed model FOCAL REASONER addressing the gap in logical reasoning within NLP tasks by incorporating a broader spectrum of facts beyond merely global entityaware knowledge. Its stateoftheart performance across multiple logic reasoning benchmarks underscores its potential to significantly advance the field of NLP. However the paper could benefit from a more detailed exploration of model complexity and computational efficiency. Similarly expanding on the models generalizability and adaptability to a wider range of NLP tasks beyond the current benchmarks would enhance its contribution to the field. Based on the significant advancements demonstrated the innovative approach to incorporating both global and local knowledge through fact units and the addressing albeit partially of interpretability in model predictions I recommend the paper for publication. The suggestions for further detailing model complexity and broader applicability should be considered to strengthen the papers impact and utility in realworld applications.", "ljCoTzUsdS": " Summarize the Paper This paper explores how machine learning models generalize to unseen data focusing on the role of inductive biases  the preconceptions that guide models when making predictions outside their training data. It introduces a novel experimental protocol drawing from cognitive psychology to dissect two specific types of inductive biases: featurelevel bias and exemplarvsrule bias. The paper systematically analyzes these biases across various models and domains using both synthetic and realworld datasets in image and language processing. It demonstrates that most neural networks lean towards exemplarbased generalization and discusses the potential implications for data augmentation fairness and systematic generalization.  Strong and Weak Points Strong Points: 1. Interdisciplinary Approach: The papers methodology innovatively combines insights from cognitive psychology with machine learning offering a fresh perspective on understanding model behavior. 2. Comprehensive Empirical Analysis: The extensive experiments across multiple models and domains (including realworld datasets) provide robust evidence for the authors claims. 3. Novel Protocol for Bias Detection: The introduction of a new protocol to examine inductive biases in machine learning systems is a significant contribution enhancing our ability to scrutinize and understand model generalization. Weak Points: 1. Lack of Theoretical Underpinning: While the empirical approach is strong the paper lacks a theoretical framework to explain why certain models exhibit exemplar vs. rulebased biases. 2. Potential for Overgeneralization: The conclusions drawn from the experiments particularly concerning the nature of neural networks\u2019 biases might be too broad and not applicable to all architectures or settings. 3. Limited Exploration of Solutions: The paper effectively identifies problems related to inductive biases but falls short in proposing solutions or mitigation strategies for the described issues.  Questions and Feedback to the Author 1. Regarding Theoretical Foundations: Could the authors provide more theoretical insights or hypotheses to explain the root causes of the observed exemplar vs. rule biases in different models 2. Model Diversity: How might these findings vary with the inclusion of more recent or less common model architectures that potentially employ different generalization strategies 3. Mitigation Strategies: What practical steps do the authors recommend for mitigating the negative effects of exemplarbased biases especially in critical applications where fairness and reliability are paramount 4. Impact of Data Representation: To what extent do the choice of data representation and preprocessing steps affect the observed inductive biases and how can these factors be controlled or accounted for in experiments  Review Summary The paper presents compelling empirical research on inductive biases in machine learning models particularly focusing on the distinction between featurelevel and exemplarvsrule biases. The use of methodologies inspired by cognitive psychology is innovative and provides a unique lens through which model behavior can be examined. The results across various models and domains underscore the prevalence of exemplarbased biases in neural networks raising important questions about systematic generalization fairness and data augmentation strategies. However the paper would benefit from deeper theoretical exploration to better understand why certain biases emerge and vary across different models. Furthermore broad generalizations based on the analyzed models might not hold universally suggesting a need for caution when extending these findings to all neural networks or machine learning systems. Despite these areas for improvement the paper makes a valuable contribution by highlighting the critical role of inductive biases in machine learning generalization. It opens up new avenues for future research particularly in developing more robust models capable of systematic generalization and in designing empirical protocols to better diagnose and mitigate undesirable biases. Overall the strengths of the paper outweigh its weaknesses showcasing a promising direction for ongoing research in machine learning and cognitive science.", "wwDg3bbYBIq": " 1. Summary of the Paper The paper introduces a novel perspective for traffic forecasting by transforming the problem into a patternmatching task utilizing historical traffic data to generate representative patterns. The proposed model PatternMatching Memory Networks (PMMemNet) employs a keyvalue memory structure to match input data with these patterns leveraging a new memory architecture GCMem that integrates attention mechanisms and graph convolution to model spatiotemporal correlations of traffic. The model outperforms existing stateoftheart models in accuracy and responsiveness especially in conditions with rapid speed changes. The paper thoroughly evaluates PMMemNet using two datasets and demonstrates its superiority through various experimental settings and ablation studies. Finally it discusses the limitations and future research directions emphasizing the potential application of neural memory networks in the traffic forecasting domain.  2. Strong Points Weak Points Questions and Feedback  Strong Points: 1. Innovative Approach: The paper conceptualizes traffic forecasting as a patternmatching problem which is a fresh and promising approach. This perspective could potentially simplify complex spatiotemporal forecasting tasks by generalizing traffic data into representative patterns. 2. Integration of GCMem: The introduction of GCMem which combines graph convolution and memory networks is particularly noteworthy. This novel architecture effectively captures the spatial and temporal dependencies of traffic data which is crucial for enhancing forecasting accuracy. 3. Comprehensive Evaluation: The rigorous experimental setup including comparisons with stateoftheart models ablation studies and qualitative analyses thoroughly validates the effectiveness of PMMemNet. The use of two distinct datasets further strengthens the papers claims.  Weak Points: 1. Complexity and Scalability Concerns: The paper provides limited discussion on the computational complexity and scalability of PMMemNet. Considering the extensive use of memory and graph convolution the model might be resourceintensive which could limit its applicability in realworld scenarios with constraints on computational resources. 2. Generalization across Different Domains: While PMMemNet shows promising results for traffic forecasting its unclear how well this approach would generalize to other spatiotemporal forecasting domains or datasets with significantly different characteristics. 3. Representation of Rare Events: The paper mentions that patterns not referred to are not trained which might lead to the models decreased performance in predicting rare or unprecedented traffic events.  Questions and Feedback: 1. Model Complexity and Scalability: Could the authors comment on the computational complexity of PMMemNet and its scalability to larger networks or datasets Are there any proposed strategies to manage computational resources more efficiently 2. Generalization Capabilities: Have the authors considered evaluating PMMemNets performance on other types of spatiotemporal data or forecasting tasks This could help assess the models generalizability and applicability beyond traffic forecasting. 3. Handling of Unprecedented Events: How does PMMemNet handle traffic conditions or patterns that are significantly different from those in the historical training data Is there a mechanism to update or expand the memory with new patterns over time  3. Review Summary and Recommendation The paper presents a compelling new approach to traffic forecasting leveraging pattern matching and a novel memory network architecture to improve on existing methodologies. The comprehensive evaluation demonstrates the models effectiveness and the innovative use of GCMem architecture is a significant contribution to the field. However there are concerns regarding the models complexity scalability and its ability to generalize across different domains or handle rare events effectively. Considering the strengths of the proposed model and its potential to shift how traffic forecasting is approached I recommend acceptance. Nevertheless for the work to have a broader impact it is vital for future research to address the mentioned weaknesses particularly focusing on scalability generalization and updating mechanisms for the memory component to include new or rare traffic patterns.", "ht61oVsaya": " Review of \"Distributive Exploration Safety Training Algorithm (DESTA) for Reinforcement Learning\"  Summary The paper introduces a novel framework for addressing the challenge of safe exploration in reinforcement learning (RL) termed Distributive Exploration Safety Training Algorithm (DESTA). This approach involves a twoplayer framework with one agent focusing on minimizing safety violations (Safety Agent) and the other on maximizing task performance (Task Agent). Unlike traditional methods that either compromise task performance for safety or viceversa DESTA aims to decouple these objectives allowing for simultaneous optimization. The framework utilizes Markov games to facilitate strategic interactions between the two agents enabling the Safety Agent to intervene in critical states to ensure safety while allowing the Task Agent to focus on task performance elsewhere. The efficacy of DESTA is demonstrated through comparisons with stateoftheart RL methods in Safety Gym Benchmarks and the Lunar Lander environment from OpenAI gym.  Strong Points 1. Innovative Approach: The concept of using a twoagent framework to separately manage task performance and safety is novel and addresses a critical gap in safe RL research. 2. Decoupling Objectives: By decoupling safety and task performance objectives DESTA allows for more focused optimization of each potentially leading to improved results in both areas. 3. Flexibility: The frameworks ability to integrate with any RL policy enhances its applicability across various domains and settings making it a versatile tool for safe RL exploration. 4. Empirical Validation: The comprehensive experimental setup and comparison with several baseline methods provide strong evidence of DESTAs effectiveness in both task performance and safety management.  Weak Points 1. Complexity of Implementation: The twoagent system while beneficial introduces additional complexity in terms of policy coordination and optimization which may pose challenges in practical implementations. 2. Scalability Concerns: The paper does not address potential scalability issues especially in environments with highdimensional state or action spaces where coordinating the actions of two agents could become computationally demanding. 3. Limited Exploration of Intervention Policy: Although the intervention policy is a crucial component the paper could benefit from a deeper exploration of its design and optimization particularly in dynamic environments with changing safety conditions.  Questions and Feedback 1. Generalization across Environments: How well does DESTA generalize across different environments especially those with more complex or dynamic safety requirements 2. Scalability: What measures can be taken to ensure the scalability of DESTA in highdimensional environments and how does the performance of DESTA compare in such settings 3. Intervention Policy Optimization: Could the authors elaborate on the process of optimizing the intervention policy especially in terms of balancing intervention frequency and impact on task performance  Review Summary The paper presents an innovative approach to safe exploration in RL addressing a key challenge in the field by decoupling the objectives of safety and task performance. This approach not only allows for targeted optimizations but also introduces a novel mechanism for preventing safety violations without significantly compromising task achievements. The empirical evaluations provide strong support for the method\u2019s effectiveness compared to current stateoftheart approaches. However the added complexity and potential scalability issues necessitate further investigation particularly in how the intervention policy is optimized and applied in more dynamically challenging environments. Additionally exploring the generalizability of the framework across a broader range of environments would add valuable insights into its applicability and limitations. Nonetheless DESTA represents a significant step forward in the domain of safe RL offering a promising framework for further research and development.", "naoQDOYsHnS": " Summarize the paper The paper presents a novel framework Behavioral Metrics of Actions (BMA) specifically designed to enhance the performance of offline reinforcement learning (RL) in environments with large discrete action spaces which is a challenging scenario for existing methods. BMA introduces an action representation learning approach based on a pseudometric that quantifies both behavioral and datadistributional relations between actions. This framework aims to address the performance limitations faced by offline RL methods due to inaccurate value estimation for outofdistribution (O.O.D.) actions. The authors provide a theoretical analysis showing that BMA leads to smoother Qfunctions and more accurate value estimations thereby enabling better policy learning in offline settings. Experimental results demonstrate significant improvements in policy performance in both simulated tasks and realworld applications when applying BMA to standard offline RL algorithms.  Strong and weak points of the paper Strong Points: 1. Novelty in Methodology: The introduction of a pseudometricbased action representation framework to address the specific challenges of offline RL in environments with large discrete action spaces represents a novel and creative approach. 2. Theoretical Analysis: The paper provides a rigorous theoretical foundation for BMA including proofs on the continuity and bounds of the expected Qvalues. This contributes to a deeper understanding of why the proposed method works. 3. Empirical Validation: Comprehensive experiments demonstrate the effectiveness of BMA across different environments offering substantial empirical backing for the proposed approach. 4. Practical Significance: By addressing a significant challenge in offline RL this work has notable implications for realworld applications in areas such as recommender systems and dialogue systems. Weak Points: 1. Complexity and Implementation Details: The paper introduces a complex methodology with several components. More detailed discussions on the implementation challenges in tuning the proposed pseudometric and practical considerations for deploying BMA would be beneficial. 2. Comparison with StateoftheArt: While the paper compares BMA with baseline methods a more extensive comparison with the latest stateoftheart methods in offline RL and action representation learning could provide a clearer picture of the method\u2019s standing. 3. Generalization to Continuous Action Spaces: The focus is primarily on large discrete action spaces. Exploring how BMA could be adapted or extended to continuous action spaces or hybrid action spaces might increase its applicability. Questions and Feedback to the Author: 1. Could the authors elaborate on the challenges faced in tuning the pseudometric and any practical guidelines for its implementation and optimization 2. How do the authors envision extending BMA to environments with continuous action spaces or is the framework strictly limited to discrete action scenarios 3. Could further insights be provided on how BMA compares with the very latest approaches in offline RL especially those that have emerged since the conception of this work  Review Summary This paper introduces an innovative framework for improving offline RL in environments characterized by large and discrete action spaces. The proposed BMA framework leverages a pseudometric to learn action representations that effectively encode both behavioral and distributive relationships between actions. The theoretical grounding and experimental validation provided by the authors highlight the frameworks potential to significantly enhance policy learning by addressing the critical challenge of inaccurate value estimation for O.O.D. actions. While the methodologys novelty and the empirical results stand out as major strengths the paper could benefit from more detailed discussions on implementation challenges broader comparisons with stateoftheart methods and potential extensions to continuous action spaces. Addressing these points could enhance the comprehensiveness and applicability of the work. Overall the paper presents a significant contribution to the field of offline RL offering new pathways for advancement in environments with large discrete action spaces. Future work that broadens the scope of BMA and further explores its practical implications has the potential to have a substantial impact on various realworld applications.", "xxU6qGx-2ew": " Review of \"Gaussian Differential Privacy Transformation: Characterization Identification and Measurement\"  Summary This paper makes significant strides in refining the Gaussian Differential Privacy (GDP) framework a pivotal area given the rising concerns over individual privacy in machine learning model applications. The authors start by examining the privacy profiles under the traditional (\u03b5 \u03b4)Differential Privacy (DP) paradigm identifying a critical partial order in (\u03b5 \u03b4)DP conditions that previous works have overlooked. Building on this foundational analysis they introduce a novel tool termed the Gaussian Differential Privacy Transformation (GDPT) which facilitates the identification and characterization of GDP algorithms through an innovative easytoverify criterion based on an algorithms privacy profile. By employing the GDPT the authors provide a methodology to precisely estimate the optimal privacy measurement \u00b5 with minimal error margins. They further utilize GDPT to enhance the utility of established (\u03b5 \u03b4)DP algorithms compare the DP and \u00b5GDP frameworks and scrutinize the impact of subsampling within the GDP context. These applications underscore the GDPTs versatility and practical value in improving privacy guarantees and understanding in computational privacy.  Strong Points 1. Theoretical Contribution: The paper successfully bridges a crucial gap in the differential privacy literature by providing a deeper theoretical understanding of the GDP framework. The introduction of GDPT as a tool to characterize and identify GDP algorithms is particularly novel and represents a significant theoretical advancement. 2. Practical Utility: Beyond theoretical implications the authors present practical applications of their proposed tools demonstrating their relevance and potential impact on enhancing privacy in machine learning models. This balance between theory and practice enhances the papers appeal to a broad audience. 3. Clarity and Depth of Analysis: The comprehensive examination of (\u03b5 \u03b4)DP conditions coupled with the rigorous development of GDPT and its subsequent applications reflects a clear and thorough analytical approach. The paper is wellstructured progressively building upon each section to culminate in a compelling argument for the proposed contributions.  Weak Points 1. Complexity of Concepts: While the paper is wellwritten the complexity of the concepts and the dense mathematical notation could potentially limit its accessibility to readers less familiar with the differential privacy landscape. Simplification where possible or a more detailed exposition might aid comprehension. 2. Empirical Validation: The paper primarily focuses on theoretical advancements and the conceptual introduction of GDPT. Additional empirical validation demonstrating the practical effectiveness of GDP algorithms identified and measured using GDPT could further strengthen the papers claims. 3. Comparison with Related Work: While the paper does mention the drawbacks of existing DP frameworks and positions GDPT as an advancement a more detailed comparison with related approaches possibly including empirical benchmarks would provide clearer insights into the proposed methods relative advantages.  Question and Feedback to the Author 1. Accessibility and Comprehension: Could the authors explore ways to simplify the exposition of core concepts or provide additional explanatory material (e.g. a supplementary tutorial) to make the paper more accessible to nonspecialists 2. Empirical Evidence: Are there plans to include empirical studies or realworld applications that demonstrate the practical benefits of using GDPT for identifying and measuring GDP algorithms 3. Comparison with Other Approaches: Could the authors expand on the comparison between GDPT and other differential privacy characterization tools possibly highlighting specific scenarios where GDPT offers tangible benefits  Review Summary This paper presents a significant theoretical and practical advancement in the area of Gaussian Differential Privacy (GDP) by introducing the Gaussian Differential Privacy Transformation (GDPT). The proposed GDPT tool not only facilitates a deeper theoretical understanding of GDP algorithms but also enables practical improvements in privacy guarantees for machine learning models. While the paper adeptly bridges crucial gaps in the existing literature and demonstrates the utility of GDPT through various applications it could benefit from simplified explanations to enhance accessibility further empirical validation and a more detailed comparison with related work. Nonetheless the contributions are robust offering valuable insights and tools for researchers and practitioners in the field of computational privacy. The theoretical depth coupled with practical implications makes this work a noteworthy addition to the differential privacy corpus.", "hbGV3vzMPzG": " 1) Summary of the Paper The presented work scrutinizes the overfitting phenomena in adversarial training by focusing on training instances difficulty. The authors propose a quantitative metric to measure the difficulty of training instances and observe that adversarial overfitting primarily arises as models attempt to fit harder adversarial instances. Theoretical analysis for both linear (logistic regression) and general nonlinear models supports these empirical findings by showing that training on harder instances leads to greater generalization gaps and that this discrepancy is exacerbated with larger adversarial budgets. Moreover the paper theorizes and empirically validates that the lower bound of a models Lipschitz constant\u2014and thus its susceptibility to adversarial attacks\u2014increases with both the adversarial budget size and the difficulty of training instances. To combat adversarial overfitting the authors propose adaptive training strategies including instance reweighting and adaptive targets which demonstrate improved robustness in both fast adversarial training and adversarial finetuning scenarios.  2) Strong and Weak Points Questions and Feedback Strong Points:  The paper offers a quantitatively backed insight into the mechanism behind adversarial overfitting filling a research gap with significant theoretical and practical implications.  It provides a comprehensive theoretical analysis supporting empirical findings a rarity that bridges the gap between experimental results and foundational principles in machine learning robustness.  Adaptive training strategies proposed to mitigate such overfitting are both innovative and effective as evidenced by the notable improvements in model robustness across multiple scenarios. Weak Points:  While the adaptive methods introduced mark an advancement the exploration of their integration with existing adversarial training frameworks seems limited. More extensive results across different adversarial training variants could enrich the paper.  The research predominantly focuses on the adversarial budget defined by the l\\xe2\\x88\\x9e norm limiting insights into the effects of diverse adversarial perturbation strategies.  The methodology might benefit from further discussion on the scalability of the introduced quantitative metric for instance difficulty especially in more complex or larger datasets beyond CIFAR10 and SVHN. Questions:  How does the proposed instance difficulty metric behave in unsupervised or semisupervised settings where label noise might influence instance difficulty estimation  Could the authors elaborate on any observed tradeoffs between model complexity (size) and the effectiveness of their proposed adaptive training strategies  Is there potential applicability of the findings in this paper to other domains of AI outside image classification such as natural language processing or reinforcement learning Feedback:  Extending the analysis to include a broader range of adversarial perturbation strategies could offer richer insights into the generalizability of the proposed solutions across different adversarial landscapes.  Considering the scalability of the difficulty metric and adaptive training methods to larger datasets or more complex network architectures could significantly enhance the impact and applicability of the findings.  A deeper dive into the integration or comparison with alternative adversarial training strategies could provide a clearer positioning of the proposed methods within the broader landscape of adversarial robustness research.  3) Review Summary The paper critically addresses an underexplored aspect of adversarial training: how the difficulty of training instances impacts overfitting and generalization performance. With a novel quantitative metric for instance difficulty and theoretical backing it unveils that adversarial overfitting is tied to the fitting of harder instances. The proposed adaptive training methods not only provide practical solutions to mitigate adversarial overfitting but also open new avenues for research on training instance management for model robustness. While the investigations depth into varying adversarial budgets and perturbation strategies might be improved the paper stands out for its blend of theoretical rigor and practical relevance. The concepts and methodologies introduced are likely to inspire future research in adversarial machine learning extending beyond image classification to other domains. Given these strengths and the clear demonstration of significantly improved robustness in adversarial settings the paper is a valuable contribution to the field. Nevertheless expanding the scope to cover a wider range of datasets adversarial perturbation types and training frameworks could further enhance its impact and broader applicability.", "iC4UHbQ01Mp": " Review of \"Vulnerabilities in Multimodal Contrastive Learning: Poisoning and Backdooring Attacks\"  1. Summary This paper investigates the potential risks associated with training multimodal contrastive learning models on noisy and uncurated data sets such as those scraped from the Internet. The authors focus on the susceptibility of these models to backdoor and poisoning attacks revealing that even a minuscule fraction of the data being compromised could significantly jeopardize the models integrity. Through extensive empirical analysis the paper demonstrates that poisoning just 0.01 or even 0.0001 of a dataset can lead to substantial misclassification rates at test time. The principal contribution lies in adapting existing attack techniques to this novel domain and empirically validating their effectiveness thereby challenging the current trend of training on uncurated data due to cost and robustness benefits.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Relevance: Addressing the vulnerability of multimodal contrastive models to poisoning and backdoor attacks is both novel and highly relevant considering the increasing prevalence of training on noisy Internetscraped data.  Comprehensive Empirical Evaluation: The paper presents a robust empirical evaluation of the attacks showcasing their practicality and providing a critical perspective on the robustness of contrastive learning models.  Clear Exposition: The explanation of contrastive learning attack algorithms and the empirical findings are articulated clearly and logically making the paper accessible to readers unfamiliar with the domain. Weak Points:  Lack of Theoretical Analysis: While the empirical results are compelling the paper lacks a theoretical framework or analysis explaining why these models are particularly vulnerable to such low percentages of poisoned data.  Generalization of Findings: Although the attacks were demonstrated on specific datasets and models it remains unclear how broadly these findings generalize across other datasets or multimodal contrastive learning frameworks.  Defensive Measures: The conclusion points to the necessity of developing defenses against such attacks but the paper does not explore potential countermeasures or how existing defenses might be adapted to this context. Questions and Feedback: 1. Generalization Across Frameworks: Can the authors comment on how these attack methodologies might perform against other contrastive learning frameworks or datasets 2. Defensive Strategies: Are there preliminary ideas or ongoing work on how to detect or mitigate the effects of these poisoning and backdooring attacks without resorting to extensive manual data curation 3. Theoretical Grounding: Could the authors elaborate on any theoretical insights or hypothesized mechanisms that make multimodal contrastive learning models particularly susceptible to these types of attacks 4. Future Directions: Beyond defense mechanisms what further research directions do the authors foresee in understanding and securing multimodal contrastive models against adversarial threats  3. Review Summary This paper addresses a critical and timely issue in the field of machine learning: the vulnerability of multimodal contrastive learning models to poisoning and backdoor attacks when trained on noisy uncurated data. The rigorous empirical evaluation stands out as a significant contribution highlighting a considerable risk of relying on large Internetscraped datasets for training. However the absence of a theoretical analysis to explain the empirical findings and discuss potential countermeasures or defenses against such attacks is a notable gap. Drawing together the argument while this paper opens an essential discourse on the vulnerabilities of current training practices for multimodal contrastive learning models it also leaves room for further exploration into mitigating strategies and theoretical underpinnings. Given its novelty empirical rigor and relevance to current ML practices the paper makes a compelling case for reevaluating the security and robustness of machine learning models trained on uncurated data. Nonetheless future work should aim to extend these findings through theoretical analysis explore broader generalizability and develop robust defensive mechanisms.", "tDirSp3pczB": " Peer Review on: \"Understanding the Impact of Negative Sample Size in Contrastive Unsupervised Representation Learning\"  1. Summary This paper investigates the theoretical and empirical aspects of negative sample size (denoted as K) in contrastive unsupervised representation learning (CURL). The authors aim to explain the observed empirical fact that larger negative samples lead to improved downstream classification performance which previous theoretical studies have failed to sufficiently clarify. Through deriving new upper and lower bounds on the downstream classification loss the study asserts that these bounds are tight in K and that the larger K reduces the estimation gap of the downstream classification loss. This conclusion which supports the empirical success of larger negative samples is substantiated with experiments on synthetic vision and language datasets.  2. Strengths and Weaknesses  Strong Points  Theoretical Contribution: The paper presents a significant theoretical contribution by introducing sharp upper and lower bounds on the downstream classification loss in CURL improving upon existing bounds. This helps bridge the gap between empirical observations and theoretical explanations.  Empirical Evidence: The extensive experiments conducted validate the theoretical findings. The experiments not only confirm the theory but also showcase the practical utility of the proposed bounds across multiple types of data.  Comprehensiveness: The paper thoroughly discusses the implications of its findings compares them with existing studies and provides a comprehensive understanding of the impact of negative sample size in CURL.  Weak Points  Assumption Clarity: The paper assumes representations are normalized and employ cosine similarity which is common but not universal in CURL. Its unclear how deviations from this assumption might affect the applicability of the bounds.  Generalization to Different Datasets: While experiments on synthetic vision and language data show the theorys consistency it might be valuable to explore its applicability to more diverse datasets or different tasks beyond classification.  Questions and Feedback to the Author  How do the proposed bounds change if the assumption of normalized representations or the use of cosine similarity is relaxed Could other similarity metrics affect the tightness or applicability of these bounds  Could the authors discuss potential limitations or challenges in extending this framework to other forms of unsupervised learning tasks such as clustering or dimensionality reduction  3. Review Summary This paper offers a valuable theoretical framework that explains the empirically observed benefits of larger negative sample sizes in CURL an area that previously lacked sufficient theoretical underpinning. By establishing tighter upper and lower bounds on the downstream classification loss the paper advances our understanding of CURLs effectiveness and sheds light on previously unexplained phenomena. The experimental validation across different datasets supports the theoretical claims and underscores the practical relevance of the findings. However the study relies on certain assumptions about representation normalization and the similarity metric used which might limit the generalizability of the results to varying contexts within unsupervised learning. Additionally further investigation could enhance our understanding of how these bounds operate in the face of different dataset characteristics and in tasks beyond classification. In conclusion the paper represents a significant step forward in aligning theoretical insights with empirical observations in CURL offering both a deeper understanding and a guide for future research in the representation learning domain. It is recommended that the authors address the mentioned weaknesses to strengthen the papers impact further.", "wQStfB93RZZ": " PeerReview on \"Asynchronous MultiAgent ActorCritic Methods for MacroActions\"  1. Summary The paper introduces novel macroactionbased multiagent actorcritic methods designed to address the challenges of asynchronous decisionmaking in multiagent problems modeled as MacroAction Decentralized Partially Observable Markov Decision Processes (MacDecPOMDPs). The presented methods extend typical actorcritic frameworks to support asynchronous learning and execution by incorporating macroactions  temporally extended actions with potentially different durations for each agent. The authors propose three training paradigms: decentralized learning centralized learning and centralized training for decentralized execution (CTDE) and introduce two CTDEbased methods  Naive Independent Actor with Centralized Critic (Naive IACC) and Independent Actor with Individual Centralized Critic (MacIAICC) in addition to macroactionbased independent actorcritic (MacIAC) and centralized actorcritic (MacCAC) methods. Empirical results across multiple domains demonstrate the effectiveness of these methods particularly highlighting the superiority of MacIAICC in scenarios involving complex asynchronous executions and partial observability.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The study addresses a significant gap in multiagent reinforcement learning (MARL) by proposing a framework for asynchronous macroactionbased learning. This is a notable contribution to literature concerning MacDecPOMDPs and asynchronous agent interactions.  Comprehensiveness: The paper extensively covers background theory methodological advancements and empirical evaluations across different domains. The inclusion of different training paradigms and comparison between them provides a thorough understanding of the subject matter.  Practical Relevance: By validating the proposed methods across diverse scenarios including search and rescue package delivery and more complex simulations like Overcooked and Warehouse Tool Delivery the paper demonstrates significant practical utility in realworld applications. Weak Points:  Complexity for Practitioners: While theoretically sound the complexity of the methods described especially around the training of decentralized and centralized actorscritics may pose implementation challenges for practitioners. Simplified explanations or stepbystep guidelines could enhance accessibility.  Scalability and Computational Costs: The investigations into scalability focus primarily on performance metrics with less emphasis on computational requirements and scalability concerning the number of agents and the complexity of tasks. An analysis of computational overheads across different methods would provide a more rounded evaluation. Questions and Feedback:  Adaptation to Highly Dynamic Environments: How well do these methods adapt to environments where the task constraints or objectives change over time Dynamic adaptability is crucial for practical applications.  Comparison with NonMacroAction Methods: Could the authors further elaborate on the advantages of macroactions over traditional actions beyond performance metrics perhaps in terms of sample efficiency or learning stability  Impact of Partial Observability: How do the levels of partial observability in different domains impact the relative effectiveness of each proposed method An analysis or discussion on this aspect would add depth to the findings.  3. Review Summary The paper makes a significant contribution to the field of MARL by addressing the complex issue of asynchronous decisionmaking through the introduction of novel macroactionbased actorcritic methods. These methods especially the MacIAICC demonstrate substantial potential in enabling highquality asynchronous collaborative behaviors among agents in partially observable environments. Despite the methodological complexity and relatively cursory treatment of computational efficiency the empirical validations across multiple domains convincingly illustrate the applicability and superiority of the proposed approaches compared to existing methods. Future work could benefit from addressing the dynamics of changing environments providing more accessible implementation guidelines and delving deeper into the computational tradeoffs involved in deploying these methods in largescale realworld applications. Overall this paper is a valuable addition to the literature paving the way for further research into macroactionbased MARL and its practical applications in complex asynchronous multiagent settings.", "mL07kYPn3E": "Peer Review for \"Big Prototypes: Enhancing Expressivity with Hyperspheres in FewShot Learning\" 1. Summary of the Paper: The paper introduces a novel methodology for fewshot learning termed \"Big Prototypes\" which aims to enhance the representation of classlevel information by utilizing hyperspheres in place of the traditional pointbased prototypes. In the context of fewshot learning where a model learns from a limited number of examples previous methods have commonly used dense vectors or prototypes calculated as the mean of instance embeddings to represent classes. However this approach may not fully capture the entirety of classlevel information especially under constraints of biased sampling. To address this the authors propose representing classes with hyperspheres in the feature space defined by two learnable parameters: a center (a dense vector) and a radius (a scalar). This representation is argued to simplify modeling and facilitate metricbased classification by calculating distances to hypersphere surfaces. Extensive experiments across natural language processing (NLP) and computer vision (CV) tasks demonstrate the effectiveness of Big Prototypes showing significant improvements over baseline models especially in tasks requiring crossdomain adaptability. 2. Strong and Weak Points Questions and Feedback: Strong Points: a. Innovative Approach: The conceptual move from pointbased to areabased (hypersphere) class representation is innovative and addresses key limitations in the expressivity of traditional prototypes. This promising direction could inspire future research in fewshot learning and beyond. b. Empirical Validation: Extensive experimentation across multiple domains (NLP and CV) and tasks provides strong empirical support for the methods effectiveness. The detailed comparison with baseline models and across different settings (e.g. different shot numbers) is particularly commendable. c. CrossDomain Adaptability: The demonstrated ability of Big Prototypes to perform exceedingly well in crossdomain tasks especially with the use of pretrained models like BERT in NLP is a notable strength. This suggests potential for the method in broader transfer learning applications. Weak Points: a. Complexity in Understanding: While the paper explains the underlying concept of using hyperspheres it could potentially improve readers understanding by providing more intuitive examples or visualizations early in the paper. b. Limited Discussion on Limitations: The paper discusses the strengths and experimental validation of Big Prototypes extensively but could benefit from a more detailed discussion on potential limitations or scenarios where this approach might not be as effective. c. Comparison with NonPrototypical Methods: While the comparison with prototypebased methods is thorough expanding the comparison to include nonprototypical fewshot learning approaches could provide a more comprehensive analysis of the methods positioning in the broader research landscape. Questions and Feedback: 1. Can the authors elaborate on how the choice of distance metric (Euclidean) affects the models performance and if other metrics were considered 2. The paper mentions that Big Prototypes demonstrate higher performance in scenarios with more examples (e.g. 510 shots). Could the authors speculate on how the method might perform in even largershot settings or standard supervised learning scenarios 3. Given the significant improvements observed in crossdomain tasks what insights do the authors have on why Big Prototypes particularly excel in these scenarios 4. Feedback: It would be beneficial to offer more insights or analysis on the dynamics of hypersphere sizes during training and their correlation with the characteristics of different classes. 3. Review Summary: The paper presents a novel and empirically validated method for enhancing fewshot learning through the use of hyperspheres to represent class information termed Big Prototypes. This approach addresses significant limitations in the expressivity of traditional prototype methods by modeling class representations as areas rather than points. The broad experimentation across multiple domains and tasks effectively demonstrates the methods strengths especially its adaptability in crossdomain scenarios and compatibility with pretrained models. While the paper is strong in its innovative approach and empirical validation it could benefit from a clearer explanation of the methods limitations more intuitive examples or visualizations to aid understanding and expanded comparison to nonprototypical methods. Nonetheless this work represents a significant contribution to the field of fewshot learning and opens up promising avenues for future research in representation learning.", "zbZL1s-pBF": "Summarize the paper: This paper introduces a novel trainingfree robust latefusion method for multimodal learning specifically designed to improve model performance under adverse conditions like adversarial attacks and random corruptions. By leveraging the conditional independence assumption and employing Jacobian regularization the authors focus on minimizing the Frobenius norm of a Jacobian matrix. This optimization problem is then simplified to a Sylvester equation making it computationally efficient. The method is validated through theoretical error bounds and a series of numerical experiments using datasets such as AVMNIST RAVDESS and VGGsound demonstrating superior performance over existing latefusion approaches under various perturbations. Strong and weak points of the paper: Strong Points: 1. Innovation: The introduction of a trainingfree robust latefusion approach based on Jacobian regularization is original and addresses the underexplored area of robustness in multimodal fusion methods. 2. Theoretical Foundation: The provision of a theoretical error bound offers analytical insights into the methods robustness reinforcing its credibility. 3. Practical Relevance: Focusing on latefusions deployment ease and privacy advantages is of high practical relevance especially in sensitive applications like healthcare. 4. Comprehensive Validation: The extensive experiments across multiple datasets and under varied conditions (adversarial attacks random corruptions) thoroughly validate the methods effectiveness. Weak Points: 1. Model Complexity and Scalability: While the formulation is efficient the paper does not discuss the scalability of the proposed method to very large datasets or realworld scenarios involving numerous modalities. 2. Comparison Baseline: The comparison mostly focuses on latefusion methods. Including early and middlefusion approaches in the analysis might provide a more comprehensive performance context. 3. DomainSpecific Fusion Effects: The paper could have expanded on how different domains (e.g. healthcare vs. automotive) might affect the fusions effectiveness and robustness. Questions and Feedback to the Author: 1. How does the proposed method scale with significant increases in data size or modality count 2. Could the authors elaborate more on the potential effects of applying their method in distinct application domains 3. Are there any specific types of adversarial attacks or corruptions where the methods performance might degrade Review Summary: The paper presents a significant advancement in the field of multimodal learning by introducing a robust latefusion approach that is both innovative and theoretically sound. Its practical application especially under adversarial conditions is highly relevant in todays increasingly interconnected and datadriven world. The methods validation across different datasets and perturbation types is thorough and convincing. However the paper could benefit from a more detailed discussion on scalability a broader comparison including nonlatefusion methods and a deeper dive into domainspecific considerations. Addressing these points could enhance the papers impact and applicability. Overall the strengths of the paper particularly its novel approach and comprehensive validation strongly support its contribution to advancing robust multimodal learning. With some elaboration on the mentioned weaknesses this work has the potential to be a significant resource for researchers and practitioners in the field.", "g4nVdxU9RK": " Summary of the Paper This paper introduces Rewardless OpenEnded Learning (ROEL) a novel unsupervised deep Reinforcement Learning (RL) algorithm that combines the principles of openended learning with unsupervised reinforcement learning. By leveraging the Paired OpenEnded Trailblazer (POET) framework and skilldiscovery via mutual information ROEL aims to generate agents capable of demonstrating a diverse set of complex skills across various environments without relying on predefined reward functions. The key idea is to use environment mutation and skill transfer learning to foster the development of skills that are both diverse and applicable to solving a myriad of novel and complex tasks. The algorithms performance has been tested on the bipedalwalker environment showcasing its ability to learn more general and diverse policies compared to agents trained in static environments. The paper elucidates on how the integration of openendedness in unsupervised RL could lead to skills that continuously increase in complexity positing ROEL as a significant step forward in the development of autonomous and adaptive agents.  Strong Points 1. Innovative Combination of Techniques: The papers proposal to integrate openended learning with unsupervised RL is novel and addresses a significant challenge in the RL field  the development of agents capable of learning a diverse set of skills without predefined rewards. 2. Empirical Evidence: The experimental setup and results provide concrete evidence of ROELs superiority over conventional methods especially in terms of skill diversity and generalizability to new environments. 3. Potential for Generalization: The demonstrated ability of ROEL to generalize skills across different environments without prior exposure is particularly compelling and hints at the algorithm\u2019s potential in solving realworld tasks with dynamic conditions.  Weak Points 1. Complexity and Computation Demands: The algorithms reliance on constant environment mutation and the POET framework might result in increased computational demands limiting its applicability to more extensive or resourceconstrained scenarios. 2. Lack of Comparative Analysis: While the paper demonstrates ROELs effectiveness in comparison to conventional methods it lacks a broader comparison with other stateoftheart unsupervised RL and openended learning methods. 3. Scalability Concerns: There are potential scalability issues especially regarding the training duration and resource requirements which are not thoroughly addressed. As the environments and tasks become more complex these issues might become more pronounced.  Questions and Feedback 1. Have the authors considered alternative frameworks to POET for implementing ROEL and if so why was POET chosen over others 2. Can the authors provide more insight into the computational costs associated with ROEL perhaps compared with traditional unsupervised RL methods 3. It would be beneficial to include a broader range of environments and tasks in future work to assess ROELs adaptability and scalability more comprehensively. 4. The paper could benefit from a deeper analysis of how ROEL performs in environments with significantly differing dynamics from the training set to further validate its generalization capabilities.  Review Summary ROEL presents a novel and promising approach to unsupervised reinforcement learning by combining openended learning with unsupervised RL specifically tailored to generate diverse and adaptable skills in agents without predefined reward functions. The experimental results convincingly show the algorithms ability to foster skill diversity and generalizability marking a significant stride towards developing versatile and autonomous learning agents. However the paper could be strengthened by addressing concerns regarding computational efficiency scalability and providing a broader comparative analysis with other stateoftheart methodologies. Given the compelling evidence of ROELs potential and despite the noted limitations this work is a valuable contribution to the field of reinforcement learning meriting further investigation into its capabilities and applicability in more range and dynamic environments.", "xxyTjJFzy3C": " Review of \"Learning 3D Shape Descriptors via Contrastive Adversarial Views Generation\"  1. Summary The paper presents a novel approach for learning 3D shape descriptors via a selfsupervised contrastive learning model named CoLAV. CoLAV addresses critical challenges in 3D computer vision by reducing dependency on labeled data and dynamically generating adversarial views of 3D shapes to learn robust and informative descriptors. This process leverages the notion of contrastive learning and adversarial views to enhance the learning frameworks capacity to handle diverse and challenging views thus improving 3D shape recognition and retrieval tasks without the explicit need for groundtruth annotations.  2. Strong and Weak Points Questions and Feedback Strong Points: a. SelfSupervised Learning Framework: The paper adeptly tackles the challenge of learning from unlabeled data by presenting a novel selfsupervised learning strategy. This significantly reduces the dependency on large labeled datasets which are hard to come by in 3D vision tasks. b. Dynamic Adversarial View Generation: The dynamic generation of adversarial views based on shapeinstancedependent parameters is a unique approach. This strategic generation of challenging views for training impressively improves the robustness and informativeness of the learned 3D shape descriptors. c. Superior Experimental Results: The proposed method demonstrates superior performance over existing techniques in 3D shape recognition and retrieval tasks on the ModelNet40 dataset even outperforming supervised methods which showcases the effectiveness of CoLAV. Weak Points: a. Complexity and Computational Resources: The paper does not thoroughly discuss the computational complexity or the resources required for training the described model which might be crucial for potential adopters of the technology. b. Lack of Diversity in Datasets: While ModelNet40 is a benchmark dataset the validation of the proposed method on more diverse or complex datasets (e.g. ScanObjectNN as mentioned but not detailed) would strengthen the papers claims on generalizability. c. Discussion on Failure Cases: The paper lacks a discussion on the limitations or failure cases of CoLAV. Understanding scenarios where CoLAV might not perform as expected could provide valuable insights for further improvements. Questions and Feedback: 1. Can the authors provide more insight into the computational demands of the proposed method compared to existing techniques 2. How does CoLAV perform on datasets with more complex or noisy 3D shapes such as those derived from realworld scans 3. It would be beneficial if the authors could elaborate on the limitations of their approach and possible failure cases to give a more balanced view of their methods applicability. 4. A discussion on how CoLAV might integrate with or complement other 3D vision tasks (e.g. segmentation or registration) would be valuable for readers interested in a broader application scope.  3. Review Summary The paper introduces an innovative and effective approach for learning 3D shape descriptors without the need for labeled data. By leveraging selfsupervised contrastive learning and dynamically generating adversarial views CoLAV significantly advances the stateoftheart in 3D shape recognition and retrieval. The experimental results presented solidify the method\u2019s superiority over existing techniques demonstrating its potential impact on the field of 3D computer vision. However the paper could be improved by addressing the computational aspects of the proposed method validating its performance on a wider variety of datasets and discussing potential limitations and failure cases. Overall the strength of the results the novelty of the approach and its contribution to the field strongly support the publication of this work albeit with minor revisions to address the highlighted concerns.", "yzDTTtlIlMr": "Summary of the Paper: This paper delves into the impacts of momentum acceleration in optimization algorithms particularly focusing on its influence on generalization performance. The authors rigorously analyze the implicit bias of momentumbased optimizers specifically Stochastic Gradient Descent with Momentum (SGDM) and Adam (without stochasticity) in the context of linear classification problems with separable data. Through their analysis they demonstrate that both SGDM and Adam converge to the L2 maxmargin solution for exponentialtailed loss paralleling the behavior of vanilla gradient descent. This convergence suggests that these momentumenhanced optimizers still favor lowcomplexity models thereby providing generalization guarantees. A novel methodological contribution is the construction of new Lyapunov functions to address the challenges in analyzing the momentums impact facilitating the analysis of the parameter convergence towards the maxmargin solution. Strong Points: 1. Originality: The paper addresses the largely unexplored terrain of the implicit bias of momentumbased optimizers. It fills a significant gap in theoretical understanding by establishing a connection between the momentum in optimizers and their convergence to lowcomplexity models. 2. Technical Rigor: The analytical approach is robust with rigorous proofs provided for Theorems related to the convergence behavior of SGDM and Adam optimizers. The utilization of new Lyapunov functions for this analysis is particularly noteworthy as a technical innovation. 3. Relevance: The momentum mechanism is widely used in practice for accelerating optimization. This works insights into how momentum affects generalization are highly relevant for both theoretical research and practical applications in machine learning and optimization. Weak Points: 1. Empirical Validation: Although the paper offers extensive theoretical contributions it lacks empirical studies or simulations that validate the theoretical findings with practical datasets or scenarios. Including such validations could strengthen the papers impact. 2. Discussion on Assumptions: While the paper bases its analysis on certain assumptions (e.g. exponentialtailed loss linear separability) the discussion on the limitations or implications of these assumptions is somewhat insufficient. Expanding on how these assumptions might affect the generalizability of the results would be beneficial. 3. Complexity of Analysis: The papers technical sections are densely packed with mathematical analysis which could be difficult for readers not deeply familiar with optimization theory. Providing more intuitive explanations or graphical illustrations could make the findings more accessible. Questions and Feedback to the Author: 1. Could the authors provide empirical evidence or simulations that support the theoretical outcomes presented particularly regarding the convergence to the L2 maxmargin solution in practical scenarios 2. How sensitive are the papers conclusions to the assumptions made especially the use of exponentialtailed loss and linear separability Would relaxing these assumptions drastically alter the findings 3. Could the authors discuss potential implications or extensions of this work towards understanding the implicit bias of momentum in nonlinear models or models with nonseparable data Review Summary: This paper makes a significant theoretical contribution to understanding the role of momentum in optimization algorithms specifically regarding its impact on the generalization performance through the lens of implicit bias analysis. The introduction of novel Lyapunov functions for addressing the complexities introduced by momentum represents a notable methodological advance. However the paper would benefit greatly from empirical validations of its theoretical findings and a more thorough discussion on the ramifications of its underlying assumptions. Furthermore simplifying the technical exposition could enhance its accessibility to a broader audience. Overall this work has the potential to significantly influence both the theoretical landscape of optimization analysis and the practical deployment of momentumbased optimization strategies.", "u6TRGdzhfip": " Summarize the Paper The paper presents an approach for distilling adversarial robustness from teacher models to student models known as Introspective Adversarial Distillation (IAD). The key contribution of this work is addressing the reliability of teacher models in the context of adversarial robustness. The authors observe that unlike conventional distillation teacher models may not provide reliable guidance on adversarial inputs due to the evolving nature of adversarial challenges. IAD introduces a method where the student model dynamically adjusts its reliance on the teacher based on the reliability of the teachers guidance. This adjustment is determined through the evaluation of the teacher models performance on both natural and adversarial data. The framework encourages student models to trust the teacher fully partially or not at all depending on the teachers performance. Extensive experiments on CIFAR10CIFAR100 and TinyImageNet datasets show that IAD improves upon the adversarial robustness of models distilled using conventional methods demonstrating its effectiveness.  Strong and Weak Points Question and Feedback  Strong Points: 1. Novel Approach: The concept of not fully trusting the teacher model and instead using a dynamic trust adjustment based on performance introduces a significant shift from traditional distillation methods tailored for enhancing adversarial robustness. 2. Extensive Experiments: The extensive experimental setup across multiple datasets (CIFAR10CIFAR100 TinyImageNet) and the comparison with benchmark methods offer a thorough validation of IAD. 3. Detailed Theoretical Analysis: The paper provides a comprehensive theoretical background and rationale for its approach including an insightful analysis into the behavior of teacher models during adversarial distillation.  Weak Points: 1. Complexity and Computational Overhead: The dynamic adjustment mechanism while effective introduces additional complexity to the distillation process. It may lead to increased computational overhead which is not thoroughly discussed. 2. Generalization to Diverse Architectures: Although the method is demonstrated on specific datasets and architectures there is limited discussion on its applicability to more diverse or complex architectures beyond those evaluated. 3. Exploration of Hyperparameters: The paper could benefit from a more detailed exploration of the sensitivity of the IAD method to its hyperparameters specifically the impact of \u03b2 and \u03b3 on robustness and natural accuracy.  Questions and Feedback: 1. How does IADs computational overhead compare to conventional adversarial training and distillation methods in practice 2. Could the authors elaborate on the potential effectiveness of IAD across a wider range of model architectures particularly in domains beyond vision such as NLP or speech 3. It would be beneficial to include an analysis of how sensitive IAD is to the choice of hyperparameters (\u03b2 \u03b3) and what strategies might be employed for their optimal selection.  Review Summary This paper introduces a novel method for adversarial distillation namely Introspective Adversarial Distillation (IAD) addressing the issue of unreliable teacher models in conventional distillation processes. Through a mechanism that adjusts the level of trust in teacher models based on their performance IAD demonstrates improved adversarial robustness across several benchmarks. While the approach innovatively tackles a critical limitation of adversarial distillation it introduces additional complexity and potential computational overhead that warrant further exploration. Additionally the applicability of IAD to a broader range of architectures and its sensitivity to hyperparameters are areas that could benefit from deeper investigation. Nevertheless the comprehensive experimental validation and the theoretical framework provided contribute significantly to the body of knowledge on enhancing model robustness via distillation. Given the strengths and potential areas for further development this work represents a valuable step forward in the pursuit of more robust deep learning models.", "nuWpS9FNSKn": "Peer Review of \"Exploring the Robustness of SelfSupervised Learning in Topic Models\" 1. Summarize the Paper The paper presents a comprehensive examination of selfsupervised learning (SSL) and its application to topic models in Natural Language Processing (NLP). The authors argue that SSL techniques specifically through the use of reconstructionbased and contrastive objectives can effectively infer document representations from data generated by various topic models without being sensitive to the exact underlying model specifications. This characteristic is highly beneficial as it proves SSLs robustness against model misspecification a common issue in probabilistic modeling. The paper first lays a theoretical foundation proving that SSL objectives can indeed extract meaningful posterior topic information. Then it proceeds with empirical validation on both synthetic and realworld datasets demonstrating that SSL objectives manage to perform competitively against posterior inference with the correct model and outperforming it where the model is misspecified. Additionally the authors investigate the influence of neural network architecture complexity on the efficacy of SSL further emphasizing its robustness across different topic model complexities. 2. Strong and Weak Points Questions and Feedback Strong Points: a. Theoretical Foundation: The theoretical grounding provided for the claims regarding SSL\u2019s capabilities in the context of topic models is very strong. The extension of existing theories to both reconstruction and contrastive learning objectives adds significant value to the discourse around SSL. b. Robustness to Model Misspecification: The paper effectively highlights a crucial advantage of SSL in handling model misspecification an issue that traditional inference methods often struggle with. This makes a strong case for the practical application of SSL in NLP. c. Empirical Validation: The comprehensive empirical studies including synthetic experiments and real data analysis robustly support the theoretical claims. The inclusion of various topic models and different complexities in the experiments presents a thorough evaluation of SSLs capabilities. Weak Points: a. Assumption Clarity: While the paper outlines the theoretical assumptions for SSL objectives it could benefit from a clearer discussion on the implications or limitations these assumptions might impose on the practical applicability of these findings. b. Comparative Analysis Depth: The empirical comparison with traditional inference methods is valuable but a deeper dive into why SSL outperforms these methods in certain scenarios could enrich the reader\u2019s understanding. c. Discussion on Network Architecture Adaptation: The paper suggests adapting the architecture to the complexity of the topic model but does not delve deeply into how this adaptation process could be optimized or automated which leaves an area of exploration somewhat untouched. Questions and Feedback: 1. Can the authors elaborate on possible limitations imposed by the assumptions for SSL objectives within the topic model context 2. It would be beneficial to include a more detailed discussion on the failure cases of SSL in comparison to traditional methods. Where does SSL fall short if at all 3. Regarding the adaptation of neural network architectures to topic model complexity can the authors propose potential strategies or thoughts on automating this adaptation process 3. Review Summary This paper provides a valuable contribution to the understanding of selfsupervised learning (SSL) in natural language processing specifically in the application to topic models. It successfully bridges a theoretical understanding with empirical validation demonstrating SSL\u2019s robust performance across a variety of topic models and its advantageous resilience to model misspecification. Such findings could motivate further research and application of SSL in areas where model precision is critical but challenging to achieve. However to strengthen the paper further it is recommended that the authors clarify the practical implications of the theoretical assumptions more thoroughly and consider expanding on the comparative analysis to elucidate why SSL exhibits superior performance in certain conditions. Additionally offering insights or preliminary strategies on adapting neural network architectures to the complexity of different topic models could enhance the papers utility for researchers aiming to apply these findings in practice. Overall the paper represents a significant step forward in the literature on SSL with topic models. With the suggested enrichments it promises to serve as a foundational text for future research exploring the vast potential of SSL in handling complex NLP tasks under uncertainty and variability.", "nbC8iTTXIrk": "Summarize the Paper: This paper introduces and elaborates on Multibranch Optimization induced Equilibrium networks (MOptEqs) an innovative implicit model architecture that leverages multiscale information for recognition tasks while also maintaining a connection to an underlying optimization problem. The model is distinct in its ability to utilize hierarchical patterns for recognition tasks with improved interpretability and performance compared to stateoftheart models like MDEQ with the added benefit of reduced parameter consumption. Central to MOptEqs is the design of a novel objective function that incorporates hierarchical and diversity considerations across different network branches resulting in an equilibrium network that is both efficient and interpretable. Additionally a Perturbation Enhanced (PE) strategy is proposed enhancing model performance and robustness by capitalizing on the insights derived from the hidden objective function. Experimental results including comparisons with existing implicit models and ablation studies underline the superiority of MOptEqs across various datasets and tasks. Strong Points: 1. Innovative Model Design: The paper presents a novel approach to integrating multiresolution inputs into equilibrium models addressing a significant gap in the literature concerning implicit models interpretability and performance in multiscale scenarios. 2. Hierarchical and Diversity Modeling: By incorporating hierarchical heritage and diversity modeling into the objective function MOptEqs uniquely balances the interbranch dependencies and feature diversity which is a commendable analytical depth. 3. Perturbation Enhanced Strategy: The introduction of a perturbation strategy based on the model\u2019s hidden objective function is both novel and practical showing improvements in model robustness and performance without significant computational overhead. 4. Comprehensive Experimental Validation: The paper presents a thorough experimental analysis including comparisons with stateoftheart models ablation studies and implications on interpretability. The results convincingly demonstrate the effectiveness of MOptEqs. Weak Points: 1. Lack of Theoretical Justification: While the empirical results are strong the paper could benefit from a deeper theoretical analysis particularly regarding the convergence properties and the theoretical underpinnings of the optimality of MOptEqs architecture. 2. Computational Efficiency Concerns: Even though the paper mentions reduced parameter consumption and improved efficiency there is insufficient discussion on computational cost especially compared to explicit models and other implicit models. Detailed complexity analysis would be beneficial. 3. Generalization Beyond Recognition Tasks: The paper focuses predominantly on recognition tasks. It is unclear how well MOptEqs would generalize to other domains or tasks within deep learning such as regression tasks or generative models. Questions and Feedback to the Author: 1. Could the authors expand on the theoretical basis for the proposed equilibrium equations and the Perturbation Enhanced strategy Insights into the stability and convergence guarantees would be invaluable. 2. It would be insightful to include a computational cost analysis comparing MOptEqs not only in terms of parameter efficiency but also in actual compute time and resource utilization against both explicit models and other implicit models. 3. Are there insights or preliminary results on how MOptEqs might perform or be adapted to tasks outside of recognition such as generative tasks or sequence modeling This could significantly broaden the impact and applicability of the work. Review Summary: The paper introduces a significant advancement in the field of implicit models with the proposition of Multibranch Optimization induced Equilibrium networks (MOptEqs). By ingeniously integrating multiscale information and proposing a novel hierarchical and diversity objective function alongside a Perturbation Enhanced strategy the authors address critical gaps in model interpretability and performance. The comprehensive experimental evaluation demonstrates the models superiority in recognition tasks showcasing both novel methodological contributions and practical implications. However the paper could be strengthened by deeper theoretical insights into the models convergence properties and operational efficiency. Moreover discussing the models applicability beyond the scope of recognition tasks could significantly enhance its contribution. Overall this work presents a compelling case for the potential of MOptEqs suggesting fruitful avenues for future research in implicit model design and optimization.", "lpkGn3k2YdD": " PeerReview  1. Summary of the Paper This paper introduces a novel algorithm Randomized Return Decomposition (RRD) aimed at solving the problem of episodic reinforcement learning where rewards are sparse and only obtained at the end of trajectory sequences. Recognizing the challenge in attributing actions to delayed rewards the authors propose a surrogate optimization framework that utilizes a MonteCarlo estimator for scalable leastsquaresbased reward redistribution across extended horizons. The core idea is to predict episodic returns from random subsequences of the agents trajectory allowing for an efficient computation via minibatch training. The algorithms theoretical foundations are discussed with connections established to previous approaches in reward redistribution. Extensive experimental validation on MuJoCo benchmark tasks demonstrates RRD\u2019s superior performance over traditional reinforcement learning algorithms.  2. Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses the critical issue of sparse and delayed feedback in reinforcement learning which is pertinent to many realworld applications.  Introduction of a novel algorithm (RRD) that effectively scales up leastsquaresbased reward redistribution to longhorizon problems.  Comprehensive theoretical analysis provides insights into the algorithms properties and establishes connections with existing methods.  Extensive empirical evaluation with benchmark tasks demonstrates the effectiveness and superior performance of RRD over baseline algorithms. Weak Points:  While the MonteCarlo estimator aids scalability there\u2019s limited discussion on its variance impacts especially in highly stochastic environments.  The experiments focus mainly on MuJoCo locomotion tasks. It\u2019s unclear how the algorithm performs in other domains or tasks with different characteristics.  The paper could benefit from a deeper analysis of hyperparameter sensitivity particularly the choice of K in the subsequence sampling distribution. Questions:  How does the performance of RRD compare in environments with higher stochasticity Would the variance of the MonteCarlo estimator significantly affect its effectiveness  Can the authors discuss the algorithms computational complexity especially in comparison to the baseline algorithms  Is there a theoretical guarantee for the convergence of RRD and how sensitive is the algorithm to the choice of the hyperparameter K Feedback:  It would be valuable to test RRD in a broader range of environments including nonlocomotion tasks or tasks with higherdimensional state spaces to assess its generalizability.  A more detailed analysis on the impact of hyperparameter K on the algorithm\u2019s performance could provide practical insights for applications.  Providing insights or strategies to mitigate potential issues arising from the variance of the MonteCarlo estimator in different environments could strengthen the paper.  3. Review Summary The paper presents a significant contribution to the field of reinforcement learning by addressing the challenge of learning from sparse and delayed rewards. The proposed RRD algorithm demonstrates a novel and effective approach for episodic reinforcement learning with trajectory feedback highlighting substantial improvements over existing techniques. The theoretical analysis and extensive empirical validation underline the paper\u2019s strengths. However exploring the algorithms performance across a wider range of tasks and discussing the impact of the MonteCarlo estimators variance in detail could further solidify the findings. Furthermore insights into the algorithms sensitivity to the choice of hyperparameters and its computational complexity would enhance the papers practical implications. Overall given the strong theoretical foundations and demonstrated empirical results the paper stands as a valuable contribution to advancing reinforcement learning methodologies.", "gi4956J8g5": " Review of \"Twostage SecondOrder Unsupervised Feature Selection via knowledge contrastive disTillation (SOFT)\"  Summary The paper introduces SOFT a novel twostage model for unsupervised feature selection which leverages secondorder information (covariance matrix) in addition to the firstorder data (original features) to address redundancy and improve selection quality. In the initial stage SOFT develops a sparse attention matrix to capture secondorder relationships among features. Subsequently by constructing a relational graph from the attention matrix the model segments the graph to select features ensuring minimal redundancy. The authors test their method across 12 datasets where it consistently outperforms existing classical and modern unsupervised feature selection techniques underlining the significance of secondorder feature relationships for efficient feature selection.  Strong and Weak Points Question and Feedback Strong Points: 1. Innovativeness: The incorporation of secondorder information along with graphbased feature selection presents an innovative approach contrasting significantly with prevailing methods that rely chiefly on firstorder data. 2. Experimental Rigor: The comprehensive evaluation across multiple datasets and the comparison with a wide range of stateoftheart methods strengthen the credibility of the results and the consequential findings. 3. Highlight of Realworld Applicability: Beyond theoretical contributions the method demonstrates practical viability by achieving superior performance metrics across datasets of varying sizes and types. Weak Points: 1. Method Complexity and Scalability: The elaborateness of the SOFT model with its multistage process and the utilization of graph convolution networks might pose challenges in terms of computational efficiency and scalability especially with exceedingly large datasets. 2. Lack of Theoretical Analysis: Theres an apparent gap in theoretical foundation regarding the convergence properties of the model the choice of hyperparameters (\u03b1 and \u03b2) and their impact on the models performance. 3. Limited Discussion on Failure Cases: While the paper highlights the successes on 11 out of 12 datasets it does not delve deeply into the reasons behind the underperformance or limitations observed on the remaining dataset. Questions and Feedback: 1. Scalability Concerns: How does the model\u2019s computational complexity scale with the number of features and instances in the dataset Can the authors provide more details or empirical evidence on its scalability and efficiency 2. Theoretical Underpinnings: Could the authors elaborate on the theoretical basis for their approach specifically how the model ensures convergence and the principled approach behind the selection of hyperparameters \u03b1 and \u03b2 3. Broader Applicability: The authors should consider discussing the models applicability or adaptability to semisupervised or supervised contexts which could significantly broaden its utility and appeal. 4. Insights on Underperformance: An indepth analysis of the dataset where SOFT did not outperform could offer valuable insights into its limitations or potential areas for enhancement.  Review Summary The paper puts forth SOFT a commendable novel approach to unsupervised feature selection that adeptly integrates secondorder feature relationships and graph segmentation for enhanced performance and minimal redundancy. The models design is both robust and innovative with strong empirical support through extensive evaluations. However notwithstanding its strengths there are notable areas for improvement particularly concerning the scalability of the approach the lack of thorough theoretical analysis and a detailed examination of limitations or underperformance instances. It is recommended that the authors address these points to strengthen the papers contribution further. Given the methods significant performance improvements over existing techniques the paper is on the whole a valuable addition to the literature in feature selection meriting consideration for publication after addressing highlighted concerns.", "oWZsQ8o5EA": " Peer Review on \"New InformationTheoretic Generalization Bounds for SGDTrained Neural Networks and Implications\"  1. Summary of the Paper This paper expands on previous work by Neu et al. (2021) to present novel informationtheoretic upper bounds on the generalization error for machine learning models particularly focusing on neural networks trained using stochastic gradient descent (SGD). The authors derive and apply these bounds to both linear and twolayer ReLU networks providing an analytical lens through which the generalization behavior of these models can be examined. Notably the paper introduces a new regularization scheme Gaussian Model Perturbation (GMP) which the authors claim performs comparably with the current stateoftheart methods. Through theoretical analysis and empirical experiments this work aims to contribute to the understanding of why deep neural networks generalize well despite being highly overparameterized.  2. Strong and Weak Points Questions and Feedback Strong points:  Innovative Approach: The paper successfully leverages informationtheoretic techniques to derive new upper bounds on generalization error which is a commendable theoretical advancement.  Empirical Validation: The authors provide comprehensive empirical studies that validate the theoretical findings and demonstrate the practical implications of their work especially through the introduction and evaluation of the GMP regularization scheme.  Relevance: Given the widespread use of SGD in training neural networks these findings are highly relevant and contribute valuable insights into the ongoing discussion on generalization in deep learning. Weak points:  Complexity of Theoretical Concepts: The paper is heavily theoretical and the complexity of the presented concepts and bounds might be challenging for readers not wellversed in information theory or advanced statistical learning theory.  Comparative Analysis: While the paper suggests that the GMP scheme is comparable to stateoftheart there seems to be a lack of detailed comparison in diverse settings or against a wider range of stateoftheart methods.  Generalizability of Results: The empirical evaluations focus on linear and twolayer ReLU networks. It would be beneficial to understand how these bounds and the GMP scheme perform across a broader spectrum of architectures and tasks. Questions and Feedback: 1. Clarification on GMPs Practicality: Given the additional computational overhead associated with the GMP scheme (requiring multiple forward passes) how does its computational cost compare to other regularization techniques in practice 2. Bounds Sensitivity to Model Architecture: The paper focuses on linear and twolayer ReLU networks. How sensitive are the derived bounds to changes in network architecture particularly deeper and more complex models 3. Extension to Other Optimization Techniques: The current study is confined to models trained with SGD. Do the authors anticipate their findings could be extended or adapted to examine generalization in models trained with other optimization methods such as Adam or RMSProp  3. Review Summary The paper makes a significant theoretical advance in deriving new upper bounds on the generalization error of neural networks trained with SGD supported by empirical evidence through experiments on linear and twolayer ReLU networks. The introduction of the Gaussian Model Perturbation (GMP) scheme as a practical implication of the theoretical findings adds to its merit bringing forth a novel regularization technique that reportedly matches or exceeds current stateoftheart performance. Despite its theoretical strength and potential practical implications the papers accessibility might be limited by its complexity and the specificity of its empirical evaluation to simpler network architectures. Future work could aim to extend these findings across a wider array of deep learning models and tasks as well as compare the proposed GMP scheme against a broader set of regularization techniques under varying computational constraints. Overall the paper represents a valuable contribution to the field advancing our understanding of the principles underlying generalization in neural networks trained with SGD. It opens several avenues for future research including the exploration of these bounds in the context of other optimization techniques and deeper network architectures.", "t3BFUDHwEJU": " Peer Review  Summary of the Paper The paper addresses a significant limitation in reinforcement learning (RL) related to the geometric discounting method which undervalues future rewards. This approach is less effective in scenarios where approaching tasks with a longterm perspective could be more beneficial such as in environments with sparse deceptive or adversarial rewards. To address this the paper introduces a novel family of delayed discounted criteria that generalizes the classical geometric discount allowing for more emphasis on future outcomes by adjusting the time weighting distributions of rewards. It presents both theoretical underpinnings and practical algorithms for deriving optimal stationary and nonstationary policies under this framework. The proposed methods were evaluated on hard exploration problems in tabular environments and continuous longepisodic robotics benchmarks demonstrating improved sample efficiency and problemsolving capabilities over traditional SoftActorCritic (SAC) algorithms.  Strong Points 1. Novelty: The paper introduces an innovative approach to discounting in RL that allows for a more nuanced and flexible handling of future rewards. This is particularly novel in addressing environments where future rewards are not necessarily exponentially less valuable. 2. Theoretical and Practical Contributions: The paper successfully bridges theoretical constructs with practical application. It not only proposes a new generalized discounting criterion but also presents algorithms that operationalize these concepts for both stationary and nonstationary policies. 3. Empirical Validation: The authors provide a comprehensive evaluation of their methods across different problem settings and benchmarks. The inclusion of both tabular environments for hard exploration problems and continuous control benchmarks helps validate the proposed approach across varied contexts. 4. Code Accessibility: The commitment to releasing the code is commendable. It ensures reproducibility and facilitates further research and experimentation by the broader community.  Weak Points 1. Complexity and Accessibility: The paper introduces a significant amount of novel theoretical content which might be challenging for readers unfamiliar with the advanced aspects of RL. A more detailed explanation or a simplified overview of the key concepts might enhance readability and accessibility. 2. Comparative Analysis: While the paper includes comparisons with the SAC algorithm it would be beneficial to see how the proposed methods stack up against a wider array of contemporary RL algorithms especially those designed to tackle similar issues of sparse rewards and longterm decision making. 3. Discussion on Limitations and Scalability: The paper falls short of discussing potential limitations and scalability concerns. Given the computational complexities associated with handling nonstationary policies and higher dimensions in continuous environments insights into the practical limitations and computational overhead would be valuable. 4. Broader Impact Considerations: While the broader impact statement emphasizes the potential of the proposed criteria for realworld applications a deeper exploration of specific domains or scenarios where these advantages could be most pronounced would provide tangible insights into its practical utility and societal impact.  Question and Feedback to the Author 1. Could the authors elaborate on how the proposed delayed discounted criteria could be integrated with other stateoftheart exploration strategies in RL especially in extremely sparse or deceptive environments 2. Feedback: It would be enriching if future revisions of the paper could include a more diverse set of comparative benchmarks and delve deeper into discussions on practical limitations computational efficiencies and potential negative societal impacts.  Review Summary Overall the paper presents a significant advancement in the field of reinforcement learning by proposing a novel family of delayed discounted criteria that addresses the limitations associated with geometric discounting. Its comprehensive theoretical foundation coupled with robust empirical validations demonstrates the potential of the proposed methods in improving longterm decisionmaking and sample efficiency in RL tasks. Despite some minor shortcomings in terms of complexity and comparative analysis the paper makes a strong case for its contributions to the field. Given its novelty theoretical and practical significance and the positive outcomes evidenced in the experiments this paper is a valuable addition to the RL literature. However it would benefit from addressing the feedback provided to reinforce its contributions further and expand on its practical applicability and limitations.", "iUuzzTMUw9K": " Review of \"StyleNeRF: A 3DAware Generative Model for HighResolution PhotoRealistic Image Synthesis\"  Summary The paper introduces StyleNeRF a novel 3Daware generative model designed for synthesizing photorealistic highresolution images with consistent multiview representations capable of being trained on unstructured 2D images. By integrating neural radiance fields (NeRF) with a stylebased generator StyleNeRF addresses the limitations of existing approaches related to highresolution synthesis 3D inconsistency and lack of control over style and explicit camera poses. The model accomplishes this through innovative methods such as rendering lowresolution feature maps followed by progressive 2D upsampling implementing improved upsampling techniques and introducing a new regularization loss to maintain 3D consistency. Additionally StyleNeRF allows for direct control over camera poses and style attributes supporting advanced tasks like style mixing and semantic editing. Evaluations on various challenging datasets demonstrate its superiority over current methods in generating highquality consistent images at interactive rates.  Strong Points 1. Innovation: The paper presents a significant advancement by merging the strengths of NeRF and stylebased generative models addressing critical challenges in the field like 3D inconsistency and resolution limitations in previous models. 2. Quality and Efficiency: It successfully generates highresolution images while maintaining photorealism and multiview consistency an achievement not matched by existing methods. The integration of efficient upsampling and novel regularization strategies is particularly noteworthy. 3. Functionality: StyleNeRFs capabilities extend beyond image synthesis to include explicit control over camera poses and style levels which showcases the models flexibility and broad application potential. 4. Reproducibility and Accessibility: The commitment to releasing code and pretrained models is commendable fostering further development and research in the community.  Weak Points and Questions 1. Complexity and Overhead: While the model significantly improves efficiency and quality the architectures complexity may introduce computational overhead particularly in training. How does the model\u2019s efficiency compare in terms of training times and resource requirements against existing benchmarks 2. Generalization: The paper highlights promising results on diverse datasets but does not extensively discuss the models generalization capabilities to domains significantly different from the training datasets. Can StyleNeRF maintain its performance in highly unfamiliar environments or with objects vastly different from those seen during training 3. Latent Space Interpretability: While the paper discusses style control there is limited discussion on the interpretability and manipulability of the latent space for specific attribute alterations. How interpretable is the model\u2019s latent space particularly in terms of disentangling and individually controlling complex features or attributes  Feedback and Suggestions  Benchmarking Against RealTime Applications: Considering the models touted efficiency and interactive rates further benchmarking against realtime applications could provide valuable insights into its practical deployment.  Exploring Transfer Learning: Investigating the models responsiveness to transfer learning approaches for rapid adaptation to new domains with minimal training data could widen its applicability and utility.  Detailed Ablation Studies: Additional ablation studies focusing on individual components contributions to the overall performance (e.g. the blending of NeRF with stylebased generative components progressive training strategies) would enhance the understanding of the model\u2019s operational dynamics.  Investigating Failure Modes: Discussing potential failure modes or limitations in more detail including conditions under which the model may generate artifacts or degrade in performance would be beneficial for future improvements and applications.  Review Summary StyleNeRF represents a significant step forward in 3D image synthesis effectively addressing longstanding issues in resolution consistency and controllability. Its novel integration of NeRF and stylebased generation alongside contributions like innovative upsampling and regularization strategies distinguish it within the field. The paper is wellstructured offering both comprehensive methodological insights and extensive evaluations showcasing the models superiority over existing methods. However exploring aspects like model complexity generalization beyond trained datasets and the interpretability of the latent space could provide deeper insights. Overall StyleNeRFs advancements and its potential for wideranging applications from content creation to scientific visualization make it a valuable contribution to the generative modeling arena.", "ugxdsne_TlO": "Summarize the paper: The paper introduces a novel machine learning approach named generalized causal forest (GCF) aimed at estimating heterogeneous treatment effects (HTE) with continuous treatments. This expands upon the capabilities of the traditional causal forest (CF) model by incorporating nonparametric doseresponse functions (DRFs) estimated through kernelbased DoubleDebiased Machine Learning (DML) estimators. It addresses model misspecification issues present in existing methodologies by proposing a distancebased splitting criterion in the functional space of partial DRFs. The efficacy of GCF is showcased through synthetic data and proprietary realworld datasets with performance benchmarks highlighting its superiority to stateoftheart (SOTA) methods especially in settings that feature continuous treatments. The GCF algorithm accommodates highdimensional feature spaces and is deployed on Apache Spark for scalability and efficiency in largescale data processing. Strong Points: 1. Innovative Approach: The introduction of GCF particularly its use of nonparametric DRFs and a novel splitting criterion is a significant advancement over conventional CF models. This approach not only broadens the applicability of CF to continuous treatments but also offers a solution to the drawbacks of model misspecification. 2. Comprehensive Methodology: The paper provides a thorough explanation of the development and workings of the GCF algorithm including theoretical underpinnings algorithmic implementation and practical considerations. 3. Empirical Validation: By conducting extensive experiments on both synthetic and proprietary realworld datasets the paper successfully validates the effectiveness of GCF in terms of computational efficiency and treatment effect estimation accuracy demonstrating clear improvements over existing methods. Weak Points: 1. Complexity and Accessibility: The high level of technical detail while thorough may present a barrier to understanding for practitioners not deeply versed in machine learning or statistical modeling. Simplifying the exposition or providing more intuitive explanations could improve accessibility. 2. Potential for Overfitting: While the paper discusses model performance theres limited discussion on measures taken to prevent overfitting particularly given the complexity of the model and the highdimensional data involved. 3. Comparative Analysis: The comparison to stateoftheart methods is comprehensive but lacks a discussion on the tradeoffs or situations where GCF might not perform as well as others. This would be critical for realworld application considerations. Questions and Feedback to the Author: 1. Overfitting Concerns: Could the authors expand on any measures or techniques employed within GCF to mitigate the risk of overfitting especially when dealing with highdimensional feature spaces 2. Model Interpretability: Given the complexity of the model how do the authors suggest practitioners interpret the results of GCF especially in domains requiring explanation of the model decisions (e.g. healthcare) 3. Scalability and Performance: The implementation on Apache Spark signifies the models scalability. However can the authors provide more insights into the computational requirements and potential bottlenecks when deploying GCF at scale 4. Further Comparisons: Are there conditions under which conventional CF or other methods might outperform GCF Discussing such scenarios would help underscore the models applicability and boundaries. Review Summary: The paper represents a significant advancement in the estimation of heterogeneous treatment effects (HTE) with continuous treatments through the development of the generalized causal forest (GCF) algorithm. The novel methodology which combines nonparametric DRFs with a unique splitting criterion addresses several limitations of existing models. Empirical evidence from both synthetic and realworld datasets substantiates the methods computational efficiency and superior performance in estimating HTE compared to stateoftheart methods. Despite its strengths the paper could benefit from addressing potential overfitting concerns elucidating the models interpretability and discussing computational scalability in more detail. Additionally further clarification on scenarios where GCF might not outperform existing methods would enhance its practical applicability insights. Overall the paper makes a robust contribution to the field promising significant improvements in precision policy making and personalized treatment strategies across multiple disciplines.", "jbrgwbv8nD": " Summary of the Paper The paper presents a novel extension of Conditional Random Fields (CRFs) named regularconstrained CRFs (RegCCRFs) for enhancing structured prediction in machine learning. The primary advancement of RegCCRFs over traditional CRFs is their ability to respect a broad class of constraints including those concerning nonlocal dependencies within output label sequences by modeling the space of viable output structures as a regular language L. This not only enables RegCCRFs to assign zero probability to all label sequences outside L but also allows the integration of constraints during the model training process a significant departure from related models that enforce constraints only during the decoding phase. The authors demonstrate that constrained training is theoretically and empirically superior to constrained decoding and showcase the practical effectiveness of RegCCRFs through improved performance in a semantic role labeling task setting new stateoftheart results on a standard dataset.  Strong Points 1. Novelty and Significance: The introduction of RegCCRFs and the method for enforcing constraints derived from regular languages during both training and decoding processes represent notable advancements in the field of structured prediction. This approach not only fills a significant gap in the existing models but also paves the way for more precisely modeled predictions in complex sequence labeling tasks. 2. Theoretical Rigor: The paper provides a robust theoretical foundation for RegCCRFs including proofs demonstrating that constrained training outperforms constrained decoding. This solid theoretical base enhances the credibility of the claims made about the models performance. 3. Empirical Validation: The empirical evaluation particularly the application to semantic role labeling where the RegCCRF achieves stateoftheart performance effectively demonstrates the practical benefits of the proposed model. These results are critical for showing the models realworld applicability and effectiveness. 4. Contribution to Knowledge: By proving the feasibility and benefits of incorporating constraints during training and not just at decoding the paper contributes valuable insights to the structured prediction domain. It expands the toolkit available to researchers and practitioners while offering a new perspective on modeling strategies.  Weak Points 1. Complexity Concerns: The paper discusses the methods theoretical and empirical effectiveness but provides limited discussion on the computational complexity and scalability of RegCCRFs especially in comparison to traditional CRFs and other related models. 2. Limitation in Constraint Types: While the paper highlights the ability to enforce a broad class of constraints it focuses predominantly on constraints expressible as regular languages. The extension of this approach to more complex constraints such as those represented by contextfree languages remains unexplored. 3. Detailed Comparison Lacking: The comparison with related methods especially those employing different strategies for constraint enforcement could have been more comprehensive. A detailed analysis of the performance tradeoffs between these methods and RegCCRFs would provide a clearer understanding of the strengths and limitations of the proposed model.  Question and Feedback to the Author 1. Complexity and Scalability: Could the authors elaborate on the computational complexity and scalability of the RegCCRFs especially in scenarios with highly complex constraint sets or very large datasets 2. Extension to Other Constraint Types: Are the authors considering extending the model to support constraints represented by more complex languages such as contextfree languages If so what challenges do they anticipate in this extension 3. Detailed Comparison: Could future work include a more detailed comparison with other constraint enforcement methodologies investigating not just performance metrics but also computational efficiency ease of constraint specification and model interpretability  Review Summary The paper introduces an innovative and theoretically sound extension to Conditional Random Fields\u2014the regularconstrained CRFs (RegCCRFs)\u2014which significantly enhances the models ability to respect a wide variety of constraints during both the training and decoding phases. This advancement not only addresses a critical limitation in structured prediction tasks but also demonstrates substantial empirical benefits in tasks like semantic role labeling where the approach sets new benchmarks. While the model represents a significant leap forward discussions on computational complexity scalability and extension to more complex constraint types would enrich the paper further. Overall the proposed RegCCRFs method is a notable contribution to the field with potential widespread applications in structured prediction tasks across various domains.", "kF9DZQQrU0w": " Peer Review for \"Exact Mutual Information in Quantized Neural Networks for Verifying the Information Bottleneck Principle\"  1) Summary: The paper investigates the Information Bottleneck (IB) principle as applied to neural networks aimed at understanding their learning dynamics particularly focusing on the mutual information (MI) between hidden layers and the inputs and outputs. Specifically it addresses the controversy around the IB principles application where previous studies reported a fitting and a compression phase in the learning process with subsequent works challenging these findings based on the argument that the observations heavily depend on the method of MI estimation. To transcend the discrepancies rooted in MI estimation methods this study employs a novel approach by utilizing quantized neural networks enabling exact MI calculation without the need for approximations or binning methods. The findings confirm the occurrence of both fitting and compression phases in the output layer while the presence of these phases in hidden layers varies based on the activation function used. The comprehensive experiments across different settings underline that the initial IB findings are not mere artifacts of MI computation methods though they also corroborate that compression phase observation is conditional on the networks specifics.  2) Strong and Weak Points Questions and Feedback to the Author: Strong Points:  Novel Approach: Employing quantized neural networks for exact MI computation is innovative and addresses a crucial methodological challenge in verifying the IB principle.  Comprehensive Examination: The paper thoroughly evaluates the IB principle across different neural network settings offering a robust analysis that accounts for previous criticisms.  Clarification of Controversy: The study significantly contributes to the ongoing debate around the IB principle by presenting empirical evidence that reconciles previous conflicting observations. Weak Points:  Generalizability: While the study provides insight into the behavior of quantized neural networks it remains unclear how these findings generalize to more common nonquantized networks in practical applications.  Network Complexity: The paper primarily focuses on relatively simple network architectures. The implications of the IB principle in the context of more complex realworld architectures remain underexplored.  Lack of Theoretical Analysis: While empirical findings are robust the paper could benefit from a deeper theoretical discussion on why certain networks exhibit the compression phase and others do not. Questions and Feedback:  Could the authors elaborate on the potential limitations of their quantization approach in capturing the nuanced dynamics of neural networks especially those employing emerging or complex architectures  How do the authors envision the practical applications of their findings considering the predominance of nonquantized networks in current deep learning tasks  It would be beneficial if the authors could discuss the broader implications of their work in the context of improving neural network generalizability and efficiency.  3) Review Summary: The paper presents a compelling and methodologically innovative examination of the Information Bottleneck principle in deep learning addressing a notable gap in the literature concerning the reproducibility of certain key findings related to learning dynamics. By employing quantized neural networks to enable exact MI computation the study not only critically engages with previous controversies but also charts a path forward for future research in this area. However the focused scope on quantized networks and relatively straightforward network architectures limits the general applicability of the findings. Despite these limitations this work is an important step towards resolving longstanding disputes surrounding the IB principle and lays valuable groundwork for further exploration. Future research would benefit from extending this approach to more diverse and complex network models as well as from integrating these empirical insights with theoretical advancements in understanding deep learning dynamics.", "yK_jcv_aLX": " Summarize the Paper This paper introduces a method for learning minimal sufficient state representations termed ActionSufficient state Representations (ASRs) for policy learning in partially observable environments. The authors propose a structured sequential Variational AutoEncoder (SSVAE) to estimate the generative environment model and extract ASRs focusing on capturing the essential information required for decisionmaking while discarding irrelevant details. This approach improves computational efficiency generalization ability and sample efficiency by enabling learning from imagined outcomes in a compact latent space. The effectiveness of ASRs is demonstrated through empirical results on two complex environments: CarRacing and VizDoom.  Strong and Weak Points Questions and Feedback Strong Points: 1. Novelty in Objective: The paper addresses the significant challenge of extracting minimal and sufficient state representations for policy learning which is crucial for efficiently solving reinforcement learning problems in highdimensional partially observable environments. 2. Structured Methodology: The introduction of SSVAE as a tool for identifying and extracting ASRs based on structured constraints and reward maximization goals is innovative. It offers a principled way to navigate the complexity of environmental dynamics beyond traditional methods. 3. Empirical Validation: The decision to validate the effectiveness of ASRs with controlled experiments in CarRacing and VizDoom environments provides a solid foundation for assessing the models performance and practical applicability. Weak Points: 1. Lack of Comparative Analysis: While the paper presents improvements over traditional VRL methods it somewhat lacks an extensive comparative analysis with recent advancements directly tackling the issue of state representation in similar POMDP settings. 2. Generalizability Concerns: The paper could expand on the discussion regarding the generalizability of the proposed approach across a broader spectrum of environments particularly in scenarios with more complex or dynamic reward structures. 3. Technical Details and Assumptions: Some of the assumptions and specific implementation details of SSVAE and its components could be elaborated on further to aid replication and understanding of boundary conditions for the proposed methods applicability. Questions and Feedback: 1. Comparative Analysis Detail: Can the authors provide more detailed comparisons including quantitative and qualitative analyses with stateoftheart methods that focus on similar challenges in partially observable environments 2. Generalizability and Adaptability: How does the proposed method adapt to changes in environment dynamics over time or to environments with significantly different characteristics than CarRacing and VizDoom Is there an inherent mechanism within the SSVAE that allows for adaptability or would this require retraining or adjustments to the method 3. Assumptions and Limitations: Could the authors discuss in greater detail the assumptions made within the model especially concerning the structural constraints and environmental models and how these assumptions might limit the applicability of the proposed method in diverse scenarios  Review Summary (Recommendation) The paper presents a novel and significant contribution to the field of reinforcement learning particularly in learning minimal sufficient state representations for efficient policy learning in partially observable environments. The proposed SSVAE methodology demonstrates a structured and principled approach that showcases clear advantages in terms of model and sample efficiency through empirical experiments. However the paper could benefit from a more extensive comparative analysis with contemporary methods and a deeper discussion on the generalizability and adaptability of the proposed approach across diverse environments and dynamic settings. It is also essential to elucidate further the technical assumptions and limitations inherent in the model to foster wider applicability and understanding. Despite these suggestions for enhancement the papers efforts in tackling a pertinent challenge and its promising results warrant a positive recommendation with the anticipation that the authors would address the outlined feedback for an even stronger contribution to the field.", "lY0-7bj0Vfz": " Peer Review for the Manuscript: Enhancing FewShot Image Generation with Memory Concept Attention (MoCA)  1) Summary of the Paper The manuscript presents a novel approach to improve fewshot image generation through the introduction of Memory Concept Attention (MoCA) a mechanism inspired by neurophysiological findings in macaque monkeys\u2019 primary visual cortex (V1). The authors draw parallels between the functionality of \"grandmother cells\"  neurons that respond selectively to complex patterns  and prototype memory priors that enhance image synthesis quality by biasing and shaping the distributed feature processing. MoCA achieves this by clustering visual concepts into prototype memories that are dynamically updated. These prototypes then guide the generation process through attention operations. The paper demonstrates that MoCA when integrated into generative adversarial networks (GANs) significantly improves image generation quality across several datasets and conditions including resistance to noise corruption. The experiments are conducted using two stateoftheart GAN architectures FastGAN and StyleGAN2 across multiple fewshot image datasets.  2) Strong and Weak Points Questions and Feedback Strong Points:  The manuscript addresses a challenging problem in computer vision fewshot image generation by proposing an innovative solution that mimics biological processes in the visual cortex.  The integration of neurophysiological insights into deep learning architectures is both novel and intriguing potentially opening up new avenues for research at the intersection of neuroscience and AI.  The extensive experiments across various datasets and conditions as well as the comparison with stateoftheart GAN architectures provide strong evidence for the effectiveness of the proposed MoCA module.  The paper is wellstructured with clear explanations of the proposed method and its theoretical underpinnings making it accessible to readers with backgrounds in either computer vision or computational neuroscience. Weak Points:  While the neurophysiological inspiration is fascinating the paper lacks a deeper discussion on how closely the MoCA mechanism mirrors actual neural processes. Are there specific aspects of \"grandmother cells\" that are not captured by MoCA  The manuscript could benefit from a more detailed discussion on the limitations of the proposed approach. For example does the performance of MoCA significantly depend on the chosen clustering method or the initial set of prototypes  It would be interesting to see a more thorough analysis of the influence of each component of MoCA (e.g. momentum online clustering and memorybased attention) on the overall performance. Questions and Feedback:  Could the authors elaborate on how MoCA could be adapted or extended to other vision tasks beyond image generation such as object detection or segmentation  How sensitive is MoCA to the size and diversity of the prototype memory Would there be a point of diminishing returns in terms of memory size or diversity  It\u2019d be beneficial for the paper to include a comparative analysis against a baseline model that incorporates standard selfattention without the prototype memory mechanism for a direct comparison of improvements attributed specifically to MoCA.  3) Review Summary The manuscript makes a significant contribution to the field of fewshot image generation by introducing a novel mechanism Memory Concept Attention (MoCA) that leverages neurophysiological insights to improve image synthesis quality and robustness. The broad range of experiments and comparisons with existing stateoftheart models provide strong support for the efficacy of MoCA. However the paper could be strengthened by addressing its weaknesses such as the need for a deeper discussion on the correspondence between MoCA and actual neural mechanisms the potential limitations of the approach and a more detailed component analysis. Addressing these points would not only solidify the current claims but also provide clearer directions for future research. Overall given the innovative approach and strong experimental results the paper stands as a valuable addition to the literature on image generation and neuroinspired computing.", "r4PibJdCyn": " Peer Review: Summarize the Paper: This paper introduces a novel bidirectional candidates generation framework TotalRecall (TR) aimed at addressing two foundational problems in Ecommerce systems namely Recommender Systems (RS) and AdvertisingMarketing Systems (AS). The TR framework is capable of generating candidate items for users (u2i) and identifying potential users for items (i2u) in an efficient and theoretically sound manner. The paper demonstrates the equivalency of the TRs modeling objective to those of stateoftheart (SOTA) algorithms its ability to incorporate various DNN architectures to achieve SOTA results and its effectiveness in diversifying candidates while ensuring rapid convergence. TRs practicality is validated through applications in largescale potential users mining and in the context of small and medium enterprises (SMEs) leveraging cloud services. Strong Points: 1. Novel Approach: The introduction of a framework that unifies the candidate generation process for both RS and AS is innovative. TR\u2019s ability to generate candidates bidirectionally is a significant advancement over traditional systems that treat RS and AS separately. 2. Theoretical Foundations: The paper provides a solid theoretical grounding by demonstrating the equivalency of TR\u2019s objective with those of existing SOTA algorithms. This not only validates the approach but also enriches the theoretical understanding of the problem space. 3. Flexibility and Performance: The fact that TR can easily incorporate various DNN architectures and still achieve or surpass SOTA results is commendable. This flexibility coupled with its demonstrated fast convergence and generation of diversified candidates speaks to its effectiveness. 4. Practical Applicability: The application of TR in realworld scenarios particularly in assisting SMEs with cloudservicebased marketing strategies and in largescale useritem matching tasks underscores its practical value. Weak Points: 1. Comparative Analysis Depth: While the paper compares TR with MFb and sequential modeling using multinomial distribution a deeper comparative analysis with a broader range of existing solutions could provide a clearer picture of its relative advantages and limitations. 2. Implementation Details and Reproducibility: The paper could benefit from a more detailed discussion on the implementation aspects of TR including the specific DNN architectures tested and hyperparameter settings which would aid in reproducibility and further experimentation. 3. Impact on User Experience: While the paper discusses the increased diversity of candidates and fast convergence from a system performance perspective an analysis of the impact of these improvements on the enduser experience could enrich the findings. Questions and Feedback to the Author: 1. Can the authors elaborate on the specific DNN architectures that were tested with the TR framework and the outcome of each More detailed findings could help understand TR\u2019s flexibility. 2. How does TR address the cold start problem commonly associated with recommender systems and advertisingmarketing systems 3. The paper could benefit from further elucidation on the scalability of TR particularly in handling dynamically changing Ecommerce environments with rapidly evolving user preferences and item catalogs. Review Summary: The paper presents a compelling and theoretically grounded framework TotalRecall (TR) that effectively unifies the candidate generation processes for recommender systems and advertisingmarketing systems in Ecommerce. The novel bidirectional approach theoretical validations flexibility in incorporating DNN architectures and practical applicability are significant strengths. However the paper could be enhanced by including a broader comparative analysis detailed implementation and reproducibility information and an exploration of the user experience impact. Overall this work represents a significant contribution to the field offering new pathways for future research and practical applications in Ecommerce. The recommendation is for acceptance with minor revisions to address the points outlined.", "rF5UoZFrsF4": " Peer Review for \"VUT: A Versatile UI Transformer for Multitask Learning in the UI Domain\"  Summary The paper presents VUT (Versatile UI Transformer) a novel multimodal multitask learning model tailored for the user interface domain. VUT handles three types of data\u2014images structures (view hierarchies) and language\u2014and simultaneously performs five distinct tasks that are crucial in the UI modeling literature: UI object detection natural language command grounding widget captioning screen summarization and UI tappability prediction. The authors argue that learning these tasks with a single model can not only improve scientific understanding of these tasks interrelations but also significantly reduce computational and storage overhead. The proposed model leverages a twotower Transformer architecture bifurcated for handling imagestructure and language data facilitating both encoding and decoding across modalities with crosstower attention. The model has been evaluated on five datasets demonstrating its ability to perform multitask learning effectively often outperforming or matching the performance of singular models dedicated to individual tasks.  Strengths 1. Innovative Approach: The paper introduces a groundbreaking approach towards a unified architecture that can handle a diverse set of tasks in the user interface domain which is traditionally approached with multiple taskspecific models. 2. Multitask Efficiency: VUT\u2019s architecture showcases the practicality and efficiency of using a single model to learn multiple tasks simultaneously potentially reducing computational resource requirements and model deployment complexity. 3. Extensive Evaluation: The authors provided a comprehensive evaluation of the model across five different datasets presenting a thorough comparison between multitask learning and singletask learning scenarios. 4. Clear Explication of Model Architecture: The paper provides detailed descriptions of the model components including the problem formulation architecture design and taskspecific adaptations making it easy to understand and replicate.  Weak Points and Questions 1. Scalability and Generalization: While the proposed model shows promise in the scope of the evaluated tasks the paper lacks discussion on the models scalability and its adaptability to other UIrelated tasks or domains. 2. Impact on Model Complexity: Combining multiple tasks in a single model may introduce complexity in understanding which features the model leverages for specific tasks. How does the model ensure taskspecific performance without compromising other tasks 3. Interpretability: With the focus on performance metrics there is little mention of the models interpretability. How does the VUT model ensure decisions can be traced and understood especially in scenarios where it may make incorrect predictions 4. Feedback to Authors: It might be beneficial to include case studies or examples where VUT\u2019s multitask capabilities provide distinct advantages over traditional singletask models in practical applications. Additionally exploring the models performance on unseen tasks or its ability to adapt to new UI domains could enhance the papers contribution further.  Review Summary This paper offers significant advancements in the development of multitask multimodal learning models for the user interface domain. The proposed VUT model addresses a critical need for efficiency and versatility in handling various tasks traditionally addressed in isolation. While the models performance and the comprehensiveness of the presented evaluations are impressive future work should focus on exploring the models scalability generalization capabilities and interpretability to solidify its applicability in wider contexts. Addressing these aspects could make VUT not only a powerful tool for current UIrelated tasks but also a foundation for future research in intelligent user interfaces. The paper is wellwritten and the models potential impact on reducing the complexity and computational requirements of deploying a wide range of UI enhancement tasks is clear and compelling.", "rN9tjzY9UD": " Review of the Paper: \"Adaptive Learning of Tensor Network Structures and Parameters for Machine Learning Applications\"  Summary The paper introduces a novel adaptive tensor learning algorithm capable of determining both the structure and parameters of Tensor Networks (TN) efficiently from data for a given task. This algorithm leverages the tensor network formalism as a unified framework for tensor decomposition aiming to overcome the limitations in selecting appropriate decomposition models for specific tasks. The proposed greedy algorithm starts with a rankone tensor and incrementally identifies optimal tensor network edges for small rank increases based on a given differentiable objective function. This adaptive mechanism enables the identification of TN structures with a small number of parameters that effectively minimize the objective function demonstrating superior performance in tensor decomposition completion and model compression tasks over existing methods.  Strong Points 1. Novel Approach: The paper addresses a significant gap in current tensor learning approaches by offering a method agnostic to decomposition models and emphasizing the adaptive identification of TN structures from data. This is a substantial advancement over the prevalent practice of preselecting a tensor decomposition model. 2. Comprehensive Evaluation: The method is rigorously evaluated across various tasks: tensor decomposition tensor completion and model compression. These evaluations not only demonstrate the methods effectiveness but also its versatility across different applications. 3. Improved Performance: The proposed algorithm shows significant improvements over stateoftheart methods particularly highlighting its efficiency in discovering tensor network structures that require fewer parameters while achieving lower reconstruction errors in decomposing images and compressing neural networks.  Weak Points 1. Computational Complexity: Despite being more efficient than the compared evolutionary algorithm the computational complexity of the proposed method can still be hefty especially in tasks involving large tensor structures. The paper acknowledges this issue but does not provide a detailed analysis or solutions to mitigate it. 2. Lack of Comparison with NonTN Methods: The paper extensively compares the proposed algorithm with other tensor networkbased methods but lacks a comparison with nonTN machine learning models that could perform similar tasks. Such a comparison could better position the proposed method within the broader machine learning field. 3. Potential for Local Optima: The paper briefly mentions that the greedy approach may converge to locally optimal TN structures but does not explore this issue in depth. More analysis or potential strategies to avoid local optima could strengthen the work.  Questions and Feedback to the Author 1. Scalability Considerations: Could the authors elaborate on potential strategies or ongoing work aimed at reducing the computational complexity especially for very highdimensional data or larger networks 2. Comparison with NonTN Models: Have the authors considered comparing the proposed method with nonTN machine learning models Such comparisons could offer insights into the unique advantages or challenges of using TNs for certain types of data or tasks. 3. Strategies for Avoiding Local Optima: Could the authors provide more details on how significant the issue of converging to locally optimal TN structures is and whether specific strategies have been considered or tested to mitigate this  Review Summary The paper presents a significant advancement in tensor learning introducing an adaptive algorithm that efficiently identifies the structure and parameters of tensor networks from data. This approach addresses the practical challenge of selecting an optimal tensor decomposition model for specific tasks showcasing impressive performance improvements in decomposition completion and model compression tasks. Despite these strengths the methods computational complexity and potential convergence to locally optimal structures pose challenges for its applicability to largerscale problems. Further exploration into computational efficiency and strategies to avoid local optima as well as comparisons with nonTN models could enrich the work. Overall the paper offers valuable insights and advancements in tensor network research presenting a solid foundation for future exploration in adaptive tensor learning algorithms.", "hR_SMu8cxCV": "Summarize the paper: The paper presents a comprehensive empirical analysis of the scaling properties of encoderdecoder Transformer models in neural machine translation (NMT). The authors propose a novel scaling law that accurately predicts the behavior of crossentropy loss as a function of encoder and decoder sizes differing from traditional scaling laws that rely on the total parameter count. They demonstrate that the naturalness of the evaluation data (whether the source or target text originates from natural language or is translated) significantly impacts the models scaling behavior. Their findings suggest an optimal allocation strategy for encoderdecoder capacity and reveal that the composition bias of training and test sets (arising from either humantranslated or machinegenerated text) significantly affects model scaling and performance. They also explore how the scaling behavior translates into generation quality finding divergent trends based on the nature of the test data. Their results are supported by extensive experimentation across multiple language pairs and model sizes. Strong points: 1. Comprehensive Analysis: The paper conducts a thorough empirical study on the scaling behavior of NMT models covering multiple aspects such as encoderdecoder size dataset composition and translation quality. 2. Novel Scaling Law Proposal: The introduction of a new scaling law that accounts for separate scaling of encoder and decoder parameters is a significant contribution offering more nuanced insights into model scaling than previous univariate laws. 3. Practical Recommendations: The paper provides actionable recommendations for optimal allocation of encoderdecoder capacity which can guide practitioners in designing more efficient NMT architectures. 4. Robust Validation: The authors validate their scaling law and findings across different languages model sizes and data composition types enhancing the generalizability of their results. Weak Points: 1. Limited Exploration of Architectural Variants: The focus is predominantly on encoderdecoder transformers and while comprehensive within this realm it would benefit from exploring how these scaling laws apply to other architecture styles or variations. 2. Impact of Dataset Size Unexplored: Although the paper delves into composition bias the effects of varying dataset sizes on the scaling laws and model performance were not thoroughly examined. 3. Dependency on BLEU for Quality Assessment: The reliance on BLEU scores for assessing generation quality is somewhat limiting given its known shortcomings in capturing semantic nuances and translation quality fully. Questions and Feedback to the Author:  Could you elaborate on the potential implications or modifications of the proposed scaling law in the context of evolving NMT architectures like dynamic or adaptive models  How might the scaling laws and optimal parameter allocation strategies shift with significantly larger datasets or is there a point of data saturation where additional data yields diminishing returns on model performance  Considering the limitations of BLEU scores did you consider incorporating more comprehensive evaluation metrics or human evaluations in assessing translation quality especially for finely nuanced languages Review Summary: This paper makes a meaningful contribution to our understanding of neural machine translation models scaling behaviors addressing a gap in existing research by focusing on encoderdecoder Transformers. The introduction of a novel scaling law that finely distinguishes between the roles of encoder and decoder size in influencing model performance is a highlight providing clearer guidance for model design. Its also commendable that the paper doesnt just stop at theoretical propositions but extensively validates its claims across several languages and conditions grounding its findings in practical utility. However the paper could further push the boundaries by exploring beyond the realms of encoderdecoder architectures and considering broader dimensions of dataset characteristics and evaluation metrics for translation quality. Nonetheless the insights offered into optimal model scaling and the influence of data composition present valuable implications for both academic research and applied machine translation development.", "pFyXqxChZc": " Peer Review  1. Summary of the Paper This paper introduces IntSGD an adaptive integer compression mechanism aimed at optimizing distributed Stochastic Gradient Descent (SGD) processes by avoiding the communication of floatingpoint numbers among devices. Instead IntSGD multiplies floatingpoint vectors by a universally known scaling factor and rounds the result to integers achieving significant computational efficiency and convergence guarantees. This method is differentiated from prior works by its adaptive nature in estimating scaling vectors ensuring the compatibility with efficient communication primitives like allreduce and innetwork aggregation (INA) without compromising on performance. The paper is thorough in its theoretical exposition presenting convergence proofs for IntSGD across various types of functions (convex nonconvex smooth nonsmooth) and settings (overparameterization) and also validates these claims through empirical performance on deep learning tasks.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The adaptive scaling factor approach of IntSGD is innovative addressing the drawbacks of previous integer compression methods by providing provable convergence guarantees without the need for expensive operations.  Comprehensiveness: The mathematical analysis of IntSGD\u2019s convergence properties is rigorous and thorough covering a broad spectrum of function types and training conditions.  Empirical Validation: The experimental results offer convincing evidence of IntSGD\u2019s efficiency and effectiveness compared to both traditional and heuristicbased integer compression methods. Weak Points:  Compression Ratio Limitation: The paper mentions a limitation regarding the compression ratio being bounded by 4. This constraint could limit the practical utility of IntSGD in scenarios where higher compression ratios are achievable or required.  Generalizability of Empirical Results: The experiments are conducted on two specific deep learning tasks. While the results are promising additional experiments covering a wider range of models and tasks could provide a more comprehensive understanding of the algorithms performance generalizability.  Scalability Analysis: An important aspect of distributed learning systems is scalability. The paper lacks an explicit scalability analysis of IntSGD specifically how it performs as the number of nodes or the size of the network increases. Questions and Feedback:  Question: Could the authors elaborate on potential strategies to overcome the mentioned limitation regarding the compression ratio  Feedback: It would be beneficial for the readers if the paper included a more detailed discussion on the practical implications of the bounded compression ratio especially in comparison with other methods that might offer higher compression ratios but at the cost of convergence guarantees or computational efficiency.  3. Review Summary (Recommendation) Based on the comprehensive theoretical groundwork empirical validation and the innovative approach to adaptive integer compression for distributed SGD the paper represents a significant contribution to the field. Despite the noted limitations such as the bounded compression ratio and a need for broader empirical validation the strengths of the paper particularly its novelty and the practical significance of making distributed machine learning more efficient outweigh the weaknesses. Recommendation: I recommend accepting this paper. The proposal of IntSGD fills a key gap in distributed machine learning by offering a method that is not only provably convergent and computationally efficient but also applicable to a wide range of function types. The paper is wellstructured presents a strong theoretical foundation and provides convincing empirical evidence supporting the claims made. Future iterations of this work could focus on expanding the scalability analysis and exploring strategies to overcome the compression ratio limitation further solidifying the practical applicability of IntSGD.", "tyTH9kOxcvh": "Summary of the Paper The paper introduces a novel method for multilabel classification termed the Multilabel Box Model (MBM) which aims to address the challenge of predicting multiple output class labels with inherent taxonomic relationships. Unlike existing methods MBM does not require explicit specification of label taxonomy leveraging the power of neural networks combined with probabilistic box embeddings to represent labels as hyperrectangles in a geometric space. This representation allows the model to implicitly capture taxonomic relations among labels and interpret these relationships as calibrated conditional probabilities providing a balance between predictive performance and taxonomic coherence in predictions. Comprehensive experiments across twelve diverse datasets demonstrate MBMs ability to outperform stateoftheart models in coherence and predictive accuracy. Strong Points: 1. Innovative Approach: The papers proposal to use probabilistic box embeddings for capturing label relationships in a multilabel setting is novel and addresses the scalability issues associated with methods requiring explicit taxonomy. 2. Theoretical and Empirical Validation: The authors provide a solid theoretical foundation for the MBM and complement it with extensive empirical evidence across multiple datasets showing the models efficacy in improving taxonomic consistency and predictive performance. 3. Interpretability and Flexibility: The methodology for representing label relationships as probabilistic embeddings not only enhances interpretability but also allows for the incorporation of partial taxonomy information which is highly beneficial in realworld scenarios where complete hierarchical label structures are not always available. Weak Points: 1. Complexity and Scalability Concerns: Despite the claim of scalability the paper does not provide a detailed analysis of the models computational complexity and scalability especially when applied to very large datasets or label spaces. 2. Generalization to NonBox Representation Entities: While effective for hierarchical and DAG relationships it remains unclear how the MBM would perform or adapt to label relationships that do not neatly fit into a taxonomic structure such as overlapping or nonhierarchical relations. 3. Direct Comparisons with StateoftheArt Methods: The paper extensively compares the MBM with other taxonomyaware models but lacks direct comparison with recent advances in multilabel classification that do not use taxonomic information. Understanding how MBM stands against these methods could further justify its contribution. Questions and Feedback to the Author: 1. Can the authors provide more insight into the computational complexity of the MBM especially in comparison to the stateoftheart methods mentioned 2. How does MBM handle scenarios where label relationships might be overlapping or nonhierarchical Can the box embeddings capture such complex relationships effectively 3. The paper could benefit from a discussion on the models limitations and potential areas where its performance might degrade. Are there specific types of datasets or domains where MBM might not be the best fit Review Summary: The paper presents a compelling advance in multilabel classification by introducing the Multilabel Box Model which effectively utilizes box embeddings to capture latent taxonomic relations among labels without the need for explicit taxonomy specification. The integration of probabilistic embeddings not only enhances the interpretability of the model but also offers a novel avenue for incorporating partial taxonomic information. The empirical results across various datasets highlight the models superiority in achieving taxonomic coherence and predictive accuracy positioning MBM as a significant contribution to the field. However the paper could be strengthened by addressing certain limitations including the scalability and computational complexity of the model its generalization capability to nonbox representable label relationships and more extensive comparisons with nontaxonomybased multilabel classification methods. Addressing these points would greatly enhance the papers contribution and provide a clearer understanding of MBMs applicability and boundaries.", "vBn2OXZuQCF": " Review of \"Understanding the Robustness Benefits of Contrastive Pretraining in Unsupervised Domain Adaptation\"  1. Summary The paper presents an indepth analysis of contrastive pretrainings impact on unsupervised domain adaptation (UDA) tasks particularly when there is a notable distribution shift between the source and target datasets. By conducting controlled experiments across four benchmark vision datasets the authors compare the performance of contrastive pretraining with current UDA methods like SENTRY and DIRTT. Their key discovery is that contrary to classical UDA intuition and methods which aim to merge domain features contrastive pretraining maintains clear distinctions in feature space between source and target domains. Yet it manages to achieve comparable or superior performance. A conceptual model based on data augmentation is developed to explain this phenomenon. The authors propose a novel measure of connectivity founded upon the relative connection strengths between same and different classes across domains which correlates well with the empirical success of contrastive pretraining in domain adaptation. The paper further explores the role of augmentations in maintaining robust connections across domains despite the inherent separation in feature space. Lastly the paper presents results from removing data points that contribute most to domain connectivity showing a consistent decline in model performance thereby underscoring the importance of domain connectivity in the success of contrastive pretraining for UDA.  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Novel Insight on Contrastive Pretraining: The paper provides a novel and counterintuitive discovery that contrastive pretraining can maintain separate domain features while still benefiting domain adaptation tasks.  Theoretical Framework: The development of a conceptual framework to explain the empirical observations is commendable. The introduction of connectivity measures is innovative and serves as a bridge to understand the mechanics behind contrastive pretrainings success in UDA.  Comprehensive Experiments: The empirical evaluation is thorough spanning multiple datasets and comparisons with stateoftheart UDA methods. This comprehensive analysis strengthens the papers claims. Weak Points:  Lack of Explanation for Specific Mechanisms: While the conceptual model based on augmentations is insightful the paper could benefit from a deeper theoretical explanation or analysis on why certain augmentations lead to successful domain adaptation.  Limited to Vision Datasets: The focus on vision datasets raises the question of the methods applicability to other types of data such as text or audio. Exploring these areas could greatly enhance the papers impact.  Potential for Bias in Connectivity Measure: The proposed connectivity measure is intriguing but the paper does not fully address potential biases or the robustness of this measure under varying conditions. Questions: 1. Could the authors elucidate further on how the choice of augmentations might play a role in the models performance across different domain shifts Are there guidelines for selecting augmentations 2. How might the presented findings and conceptual model generalize to nonvision datasets and what challenges do you foresee in such cases Feedback:  To address the weak points and strengthen the theoretical basis of the paper consider including a more detailed theoretical discussion on the choice and impact of data augmentations in the success of domain adaptation.  Exploring a wider range of datasets including nonvision ones could provide additional evidence for the generalizability and robustness of the proposed method and conceptual framework.  Provide a more rigorous analysis or sensitivity test of the proposed connectivity measure to assure readers of its reliability and robustness across different settings.  3. Review Summary The paper offers significant contributions to the understanding of unsupervised domain adaptation through contrastive pretraining. Its novel insight that maintaining separate features for source and target domains can still benefit UDA challenges conventional wisdom and enriches the discourse on domain adaptation methods. The introduction of a connectivitybased conceptual model is an innovative approach to explaining the observed empirical success. However the paper could be improved by providing a more indepth theoretical rationale for the mechanisms at play especially regarding the selection and impact of data augmentations. Additionally expanding the scope to include nonvision datasets could greatly enhance the generalizability of the findings. Despite these areas for improvement the paper represents a valuable step forward in the understanding of contrastive pretrainings role in domain adaptation. Given the strong empirical evidence and the novel insights provided I recommend this paper for publication contingent upon addressing the aforestated concerns regarding theoretical depth and generalizability.", "huXTh4GF2YD": " Review of \"Enhancing OpenSet Recognition Using Background Class Regularization with DistanceBased Classifiers\"  Summary of the Paper This paper presents a novel approach for OpenSet Recognition (OSR) by enhancing the robustness of distancebased classifiers through Background Class Regularization (BCR). Traditional methods for OSR largely rely on pretrained Softmax classifiers and postclassification analysis which often do not adequately cater to the distinct issue of unknown class situation. The paper argues that previous BCR approaches are insufficient because they fail to appropriately separate known from unknown classes due to their simplistic grouping of knownclass data. To address this the authors propose a new method that applies a classwise limitation on the feature space of knownclass data and utilizes a novel regularization loss function designed specifically for distancebased classifiers. The proposed method aims to both efficiently distinguish unknownclass samples and maintain high performance for closedset classification. Extensive experiments demonstrate the competitiveness of the proposed method in achieving robust OSR results while simplifying the inference process.  Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The paper introduces a novel BCR strategy that significantly differs from previous methods by addressing the limitations related to classwise feature space limitation and the employment of distancebased classifiers for OSR. 2. Comprehensive Evaluation: The authors conduct extensive experiments covering various settings and comparisons with previous methods. The use of different datasets and the inclusion of an ablation study provide a solid evaluation of the methods effectiveness. 3. Theoretical and Practical Balance: The paper provides a good balance between theoretical foundations and practical implications. The formulation of the novel loss function and the structured approach to tackle OSR problems are grounded in linear discriminant analysis principles which adds to the credibility of the method. Weak Points: 1. Limited Comparison with Latest Methods: While the paper compares its method with several existing approaches there is a lack of comparison with the very latest methods in OSR which might have emerged after the knowledge cutoff or were not included due to other reasons. 2. Generalization across Different Domains: The paper mainly focuses on image classification tasks. Insights into how the proposed method performs across different types of data (e.g. text or speech) could enhance the understanding of its adaptability and limitations. 3. Complexity and Efficiency Discussion: The paper briefly mentions the inference process simplicity but lacks a detailed discussion on the computational complexity training time comparison and scalability of the proposed method compared to others. Questions and Feedback: 1. Can the authors elaborate on the potential of their method in nonvisual domains Given the principles its based on it would be interesting to explore its adaptability beyond image data. 2. How does the proposed methods training and inference time compare with existing methods especially those that rely on complex deep learning models 3. Feedback for improvement: It would be beneficial for the completeness of the study to include more details on the limitations and potential drawbacks of the proposed method. Also discussing the methods scalability in the context of much larger datasets might provide further insights into its practical applicability.  Review Summary The paper presents a commendable effort in advancing the field of OSR through a novel distancebased BCR method. Its approach to addressing the limitations of previous methods with a focus on classwise feature space limitation and the introduction of a tailored regularization loss function is both novel and theoretically sound. The comprehensive experimental setup and the results therein convincingly support the methods effectiveness. However the paper could be strengthened by addressing some of the weak points mentioned such as comparing with the very latest methods exploring generalization across different domains and providing a more indepth discussion of the methods complexity and efficiency. Overall the paper makes a significant contribution to the field of OSR and with further exploration and expansion the proposed method has the potential to see widespread application and development.", "zz9hXVhf40": " Review of the Paper on Uncertainty Penalties in Offline ModelBased Reinforcement Learning  1. Summarize the paper This paper conducts a comprehensive investigation into offline modelbased reinforcement learning (MBRL) focusing on the impact of various uncertainty penalties and hyperparameters on performance. The researchers compare multiple uncertainty heuristics utilized in both online and offline MBRL settings probing their interaction with the number of models in the ensemble and the imaginary rollout horizon. They employ Bayesian Optimization to finetune these hyperparameters leading to superior configurations that substantially outperform handtuned stateoftheart methods. A novel evaluation protocol is introduced to assess the penalties under specific outofdistribution (OOD) data scenarios typical in MBRL. Their findings suggest that larger penalties for longer horizon rollouts can enhance performance and simpler canonical forms of uncertainty estimation like ensemble standard deviation show better performance and calibration. The paper validates its results through rigorous statistical analysis and argues for the realworld applicability of offline MBRL emphasizing improved performance through optimal hyperparameter selection.  2. Strong and Weak Points Questions and Feedback Strong Points:  Comprehensive Evaluation: The paper provides an exhaustive comparison of uncertainty penalties and a novel evaluation protocol that better captures the challenges encountered in realworld applications of offline MBRL.  Use of Bayesian Optimization: The application of Bayesian Optimization for hyperparameter tuning is a strong point as it systematically identifies superior configurations.  Practical Insights: The findings offer insightful implications for the design and implementation of offline MBRL systems especially regarding the balancing of penalties and the utility of longer imaginary rollouts. Weak Points:  Generalization of Results: While the paper shows significant improvements in performance it is unclear how these findings generalize across a broader spectrum of tasks outside the ones tested.  Methodology Transparency: The detailed explanation of the novel evaluation protocol and Bayesian Optimization process could be enhanced to improve replicability.  Limited Comparison: Although the paper compares various uncertainty penalties it briefly discusses the implications of different model architectures or learning algorithms within the MBRL paradigm. Questions and Feedback:  Could the authors provide more insight into how the chosen hyperparameters might vary across different tasks outside the tested environments particularly in scenarios with higher complexity or different dynamics  The paper could benefit from a deeper analysis of the tradeoffs between complexity (in terms of computational cost) and performance gains from the optimal hyperparameter configurations identified.  It would be enriching if the authors explored the relationship between the penalty mechanisms effectiveness and the diversity within the ensemble of models potentially leading to guidelines on model selection within ensembles.  3. Review Summary The paper stands out for its rigorous and comprehensive analysis of uncertainty penalties in the context of offline MBRL. Its novel evaluation protocols and the effective use of Bayesian Optimization to identify optimal hyperparameter configurations are significant contributions to the field. The results convincingly demonstrate the value of systematic hyperparameter tuning and provide practical insights for designing and implementing more effective offline MBRL systems. However the generalizability of the findings beyond the tested environments and the transparency of the methodologies employed could be areas for improvement. Addressing these would bolster the papers impact and applicability. Additionally expanding the comparison to include the implication of different model architectures or algorithms within the MBRL framework could provide a more rounded perspective on the offline MBRL challenge. Overall the paper makes a solid contribution to understanding and improving offline MBRL performance through optimal hyperparameter selection and penalty implementation. It highlights critical areas for future research and presents a compelling case for the practical application of its findings in realworld tasks.", "gPvB4pdu_Z": " 1) Summary of the Paper The paper introduces a novel endtoend compositional training framework for deep AUC maximization (DAM) designed to tackle the challenges in training deep neural networks to maximize the area under the ROC curve (AUC) directly from scratch. Traditional approaches to DAM involve a twostage training process initially optimizing a traditional loss (e.g. crossentropy loss) followed by finetuning using an AUC loss. These methods however have been identified to produce suboptimal feature representations and increase engineering costs. The proposed compositional training framework aims to enhance both the feature learning and classifier robustness by introducing a compositional objective function that integrates gradient descent steps for crossentropy and AUC loss minimization. The authors propose a stochastic optimization algorithm with provable convergence for this novel objective and demonstrate the methods effectiveness over conventional approaches and the twostage DAM process through extensive empirical studies on benchmark and medical image datasets.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The paper addresses a significant gap in the literature by proposing an endtoend training method for DAM avoiding the shortcomings of the twostage process. The compositional training method is a novel approach that improves both feature representations and the final AUC score.  Theoretical Contribution: The authors not only introduce a novel training method but also provide a theoretical foundation for their stochastic optimization algorithm proving convergence rates comparable to standard stochastic gradient descent (SGD).  Empirical Validation: Extensive empirical studies on several datasets demonstrate the superior performance of the proposed method over traditional deep learning approaches and twostage DAM methods significantly enriching the papers contributions. Weak Points:  Complexity and Computational Cost: While the paper mentions improvements in training costs compared to twostage approaches it lacks a detailed analysis of the computational cost and complexity of the proposed method especially in comparison to traditional methods.  Generalizability and Extensibility: The paper strongly focuses on binary classification tasks. Insights into how the proposed method can be extended to multiclass classification or other types of learning tasks (e.g. regression) could greatly enhance its impact.  Discussion on Limitations: The paper could benefit from a more comprehensive discussion on potential limitations or scenarios where the compositional DAM might not perform as expected. Questions and Feedback:  Could the authors provide more insight into the computational complexity of the proposed method compared to both traditional deep learning and twostage approaches  The paper would benefit from a discussion on how the proposed framework can be adapted for multiclass classification tasks or even beyond classification (e.g. regression).  It would be informative to understand potential limitations or conditions under which the compositional DAM might underperform providing guidelines for practitioners.  3) Review Summary and Recommendation The paper presents a compelling and innovative approach to deep AUC maximization addressing a crucial challenge in the field with significant theoretical and empirical contributions. The introduction of a compositional training framework is both novel and welljustified demonstrating clear advantages over existing methods across various datasets. The theoretical grounding of the proposed stochastic optimization algorithm further strengthens the papers contributions. However the paper could be improved by addressing the computational complexity in more detail expanding the discussion on the applicability to broader types of learning tasks and providing a deeper exploration of the methods limitations. Based on the above analysis I recommend accepting this paper with minor revisions. Addressing the highlighted weak points and questions could significantly enhance the papers value making it an outstanding contribution to the field of deep learning and AUC maximization.", "iw-ms2znSS2": " Peer Review of \"Exploring Policy and Value Networks with Randomization for Solving PSPACEhard Planning Problems\"  1. Summary of the Paper The paper investigates the intersection of traditional combinatorial search methods and deep learning techniques to address PSPACEhard planning problems with a focus on the Sokoban domain. By combining bestfirst search and Monte Carlo tree search with Deep Neural Networks (DNNs) the authors propose a novel approach that leverages the policy and value networks within DNNs as heuristics for guiding the search process more effectively. Through extensive experimentation they demonstrate the significant role of policy networks in guiding searches which is further enhanced by value networks. They also reveal that Sokoban planning instances display heavytailed runtime distributions and for the first time identify and characterize left heavy tails in such distributions. The authors introduce an abstract tree model that explains these phenomena and suggests the use of random restart strategies to improve search efficiency which has been confirmed by their extensive experiment data.  2. Strengths and Weaknesses  Strengths:  The paper addresses a challenging and relevant topic in the realm of artificial intelligence and combinatorial problemsolving providing substantial theoretical and practical contributions.  The combination of traditional search methods with DNNs and the exploration of the interplay between policy and value networks in guiding searches in the Sokoban domain is novel and insightful.  The extensive empirical evidence presented including the discovery of left heavy tails in runtime distributions and the development of an abstract tree model demonstrates rigorous experimentation and analysis.  The finding that random restart strategies can effectively balance the search process and avoid heavytailed runtimes could have broad implications for solving similar PSPACEhard problems.  Weaknesses:  While the paper provides comprehensive details on the methodology and findings it could benefit from a more explicit discussion on the limitations of the proposed approach and potential areas where the model might not perform as expected.  The discussion section lacks comparisons with other stateoftheart approaches in the same domain which would offer readers a clearer view of the proposed methods relative performance and significance.  The paper might have missed an opportunity to explore or discuss the potential implications of these findings on other PSPACEhard planning problems beyond the Sokoban domain.  Questions and Feedback to the Author:  Could you elaborate on any potential limitations or scenarios where the proposed approach might not yield optimal results  How does the performance of your approach compare with existing techniques for solving PSPACEhard planning problems particularly those not employing deep learning methods  Would the insights gained from studying the Sokoban domain be applicable to other PSPACEhard problems If so how might the approach be adapted to different domains  3. Review Summary The paper offers a welldeveloped and innovative approach to solving PSPACEhard planning problems by leveraging the capabilities of deep neural networks particularly focusing on how policy and value networks can effectively guide the search process. The identification of left heavy tails in runtime distributions and the proposal of an abstract tree model to explain this phenomenon are both significant contributions that advance our understanding of combinatorial search dynamics. The methodology is robust and the findings are supported by extensive and rigorous experimentation. However the paper could be improved by addressing its limitations more explicitly offering comparisons with other existing methods and discussing the applicability of the findings beyond the Sokoban domain. Overall the study represents a meaningful step forward in the integration of deep learning with traditional search techniques for solving complex planning problems providing valuable insights and suggestions for future research in the area.", "rFbR4Fv-D6-": " Review of \"Automated SelfSupervised Task Search for Graph Data with AUTOSSL\"  1. Summary The paper presents an innovative exploration in the domain of graph selfsupervised learning (SSL) focusing on the challenge of optimally leveraging multiple pretext tasks for enhancing performance on downstream tasks without access to groundtruth labels. Traditional methods have emphasized designing or using single SSL tasks which might not fully capture the richness of graph data. Identifying that different SSL tasks lead to varying performances across datasets the authors propose an automated framework AUTOSSL aimed at efficiently searching and combining multiple SSL tasks. The core idea revolves around using the principle of homophily as a guiding measure termed as pseudohomophily to evaluate and select the combination of tasks that yield the most informative node representations. Two strategies for searching SSL tasks are introduced: an evolutionary strategy (AUTOSSLES) and a differentiable strategy (AUTOSSLDS). The effectiveness of AUTOSSL is demonstrated across eight realworld datasets showing significant improvement over individual SSL tasks and some existing methods in both node clustering and classification tasks.  2. Strong Points  Originality: The paper tackles the novel problem of automated search and combination of multiple SSL tasks for graph data differing from the predominant focus on designing individual tasks.  Theoretical Foundation: Provides a thorough theoretical analysis linking the concept of pseudohomophily to the maximization of the mutual information between pseudolabels and ground truth labels.  Comprehensive Evaluation: Extensive experiments across various realworld datasets demonstrate the efficacy of AUTOSSL in improving the performance of node clustering and classification tasks over individual SSL tasks and other baselines.  Reproducibility and Openness: The authors make the source code available ensuring replicability of the results and easing future advancements in the field.  3. Weak Points  Lack of Discussion on Computational Cost: While the paper mentions that AUTOSSLDS is designed to reduce search costs a detailed analysis comparing the computational requirements and search efficiency of AUTOSSLES and AUTOSSLDS is missing.  Limited Investigation into Task Weight Dynamics: The results highlight the evolution of task weights but lack a deeper exploration into how these weights influence model behavior and performance dynamically over time.  Absence of Broader Applicability Discussion: The paper focuses on specific SSL tasks and discussions on the framework\u2019s applicability to a wider range of SSL tasks or its adaptations to other types of networks (e.g. dynamic graphs) are limited.  Questions and Feedback to the Author 1. Computational Efficiency: Could the authors provide more insights into the computational efficiency of the proposed methods especially comparing AUTOSSLES and AUTOSSLDS in terms of search time and resource consumption 2. Generalization to Other Domains: How adaptable is AUTOSSL to other domains or types of graph data beyond the datasets explored in the paper particularly for graphs exhibiting heterophily 3. Influence of Task Weights: The paper presents the final task weights and mentions that they stabilize however it would be enriching to understand how variations in these weights affect the learning process and final performance throughout training.  Review Summary This paper introduces a significant advancement in the domain of graph selfsupervised learning by presenting AUTOSSL an automated framework for searching and combining multiple SSL tasks using the novel concept of pseudohomophily. The theoretical justification coupled with extensive empirical validations establishes the effectiveness of AUTOSSL in enhancing performance on downstream tasks. Despite its contributions the paper could benefit from a deeper analysis of computational efficiencies and the dynamic influence of task weights on model behavior. Addressing the queries related to computational cost generalization capabilities and the impact of task weight dynamics could further strengthen the paper. Overall the paper stands as a meaningful contribution to the field pushing the boundaries of selfsupervised learning in graph data and opening avenues for future research exploration.", "oOuPVoT1kA5": " Peer Review for \"FEVERLESS: A Fast and Secure Vertical Federated Learning Protocol for Distributed Labels\"  1) Summary of the Paper This paper introduces \"FEVERLESS\" a novel protocol designed for Vertical Federated Learning (VFL) with a specific emphasis on the secure training of XGBoost models with distributed labels. The core innovation lies in its ability to facilitate collaborative global model training across multiple clients possessing vertically partitioned data without disclosing sensitive feature or label information. FEVERLESS employs secure aggregation through an information masking technique along with global differential privacy (GDP) enabled by a randomly selected noise leader greatly minimizing the risk of private information leakage during training. This approach is demonstrated to be effective even under a collusion scenario involving n2 out of n participants ensuring robust privacy protection against honestbutcurious adversaries. The empirical evaluation presented showcases FEVERLESSs capacity to deliver on both efficiency and accuracy fronts purportedly outperforming existing solutions that rely on additive homomorphic encryption in terms of runtime and presenting better accuracy compared to local differential privacy tactics.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Practicality: The paper tackles the less explored territory of securing VFL with distributed labels and proposes a practically viable and novel solution which is a significant contribution to the field.  Comprehensive Security and Efficiency Analysis: The paper includes a detailed security analysis demonstrating the protocols effectiveness against information leakage in a semihonest setting. Efficiency analysis comparing the training time and accuracy with other popular approaches further highlights FEVERLESS\u2019s practical significance.  Realworld Applicability: The experimental evaluation using realworld datasets underlines the protocols applicability and effectiveness in practical scenarios enhancing the contributions relevance. Weak Points:  Lack of Discussion on Scalability: While the paper presents the efficiency and security analyses it does not thoroughly address scalability concerns particularly in situations involving a massive number of clients or datasets of considerably large sizes.  Limited Comparison with StateoftheArt Protocols: Although comparisons are made with additive homomorphic encryption and local differential privacy approaches a broader comparison with other stateoftheart VFL protocols could provide a clearer perspective on FEVERLESSs position within the research landscape.  Potential Bias in Experimental Setup: The paper could benefit from a more diverse experimental setup including a wider range of datasets especially considering the stated limitation regarding smallscale datasets. Questions and Feedback:  Scalability Concerns: Can the authors elaborate on how FEVERLESS might perform in environments with significantly larger datasets or a greater number of participants Are there inherent scalability limitations within the proposed protocol  Comparison With Recent Protocols: Have the authors considered comparing FEVERLESS with any recent VFL protocols that use different approaches for ensuring privacy Such a comparison could potentially highlight FEVERLESSs unique advantages or identify areas for further improvement.  Addressing Smallscale Dataset Limitation: The authors acknowledge that FEVERLESS does not perform as well on smallscale datasets. Have any specific strategies been considered to overcome this limitation or is this an inherent flaw in the approach  3) Review Summary \"FEVERLESS\" represents a significant step forward in the realm of Vertical Federated Learning particularly in scenarios where data labels are distributed across participants. The paper is commendable for its novelty indepth security and efficiency analysis and the practical applicability demonstrated through experiments with realworld datasets. However it falls short by insufficiently addressing scalability considerations offering limited comparative analysis with the broader array of existing VFL protocols and not fully exploring solutions for its suboptimal performance on smallscale datasets. Despite these shortcomings the strengths of the paper especially its innovative approach to enhancing privacy and efficiency in VFL outweigh the weaknesses. The protocols ability to ensure robust privacy protection even in collusion scenarios coupled with its demonstrated practical efficiency and accuracy underscores its potential value to the federated learning community. As such with some improvements particularly in addressing the noted limitations and potentially broadening the scope of its comparative analysis FEVERLESS could pave the way for more secure and efficient federated learning solutions applicable in a wide range of sectors including healthcare and finance where data privacy is paramount.", "inA3szzFE5": " Review of \"Enhancing OutofDistribution Generalization in Deep Neural Networks through Spatial Frequency Sensitivity Analysis and Regularization\"  1. Summary The paper addresses a critical challenge in the domain of computer vision related to deep neural networks (DNNs)  the sensitivity to shifts in Fourier statistics of input data which affects the robustness and generalization capabilities of these models. It introduces a principled framework to analyze and modify the spatial frequency sensitivity (SFS) of DNNs using a novel measure based on the inputJacobian represented in the Fourier basis. This measure identifies the specific spatial frequencies to which a model is most sensitive. To address the identified issues the authors propose a set of spatial frequency regularizers which when included in the training process enable the models to achieve better generalization on outofdistribution data characterized by shifts in Fourier statistics while maintaining high performance on indistribution data. The paper validates these contributions through extensive experiments demonstrating improved performance in scenarios involving Fourier statistic shifts.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The novel measure of SFS based on the inputJacobian in the Fourier basis addresses a gap in the literature by providing a rigorous and principled way to analyze model sensitivity across spatial frequencies.  Practicality of Regularization Method: The proposed spatial frequency regularizers represent a straightforward and effective method for enhancing model robustness against shifts in input datas Fourier statistics which is a practical contribution to improving DNN generalization capabilities.  Extensive Experimentation: The experiments are welldesigned to test the effectiveness of the proposed method across different scenarios involving both artificial and natural shifts in Fourier statistics providing a robust validation of the paper\u2019s claims. Weak Points:  Generalizability to Other Domains: While the paper provides compelling evidence of the effectiveness of the proposed methods on computer vision tasks it is not clear how these methods might generalize to other domains (such as audio processing or timeseries analysis) where Fourier characteristics are also important.  Computational Overhead: Theres minimal discussion on the computational costs associated with the application of the proposed regularizers especially in larger models and datasets. This information would be crucial for assessing the practicality of adopting these regularizers in realworld applications.  Analysis of Failure Modes: Although the paper excels in demonstrating where the method succeeds it lacks a detailed analysis of scenarios or conditions under which the proposed approach might not be as effective. Questions and Feedback:  Could you elaborate on the potential applicability of the proposed SFS analysis and regularization methods beyond image classification tasks particularly in other domains where data can also be represented in the Fourier domain  Is there an observed or anticipated impact on the training time or computational resources when applying the proposed spatial frequency regularizers particularly at scale  A deeper analysis of cases where the proposed methods did not lead to improvement or had lesser impact might provide valuable insights into the limitations and scope of applicability of the method. Could such an analysis be included  It would be beneficial to see a comparative analysis with other methods aimed at enhancing outofdistribution generalization to understand the relative strengths and weaknesses more clearly.  3. Review Summary This paper makes a significant contribution to the ongoing efforts to improve the robustness and generalization of DNNs in computer vision tasks by introducing a novel principled approach to analyzing and modifying the spatial frequency sensitivity of these models. The innovative measure based on the inputJacobian in the Fourier basis and the proposed spatial frequency regularizers represent concrete steps forward in addressing the challenges posed by shifts in Fourier statistics of input data. The extensive experimental validation of the proposed methods underscores their effectiveness and practical utility. However the paper could be strengthened by addressing its generalizability to other domains the computational costs associated with its methods and a more detailed analysis of potential limitations or failure modes. Addressing these points would provide a more comprehensive understanding of the proposed approach\u2019s scope of applicability and its practical implications. Overall the contributions of this paper are valuable and with further exploration and elucidation of the outlined concerns it could have a significant impact on the field.", "qhC8mr2LEKq": " Review of \"CROSSBEAM: Efficiently Learning to Search for Program Synthesis\"  Summary The paper introduces CROSSBEAM a novel approach for program synthesis that leverages a neural model to guide the search process within a bottomup enumeration framework. The key innovation is the utilization of the neural model to dynamically decide which programs or subprograms to combine thereby efficiently navigating the search space. This method is informed by the work in structured prediction and learning to search where the model is trained onpolicy using the data generated from its searches. CROSSBEAM is evaluated against existing stateoftheart methods in two domains: string manipulation and logic programming. The results show a significant improvement over these methods with CROSSBEAM achieving a 62 increase in task resolution in the string manipulation domain and nearly 100 success in logic programming tasks that prior methods could not solve.  Strong Points 1. Originality: The approach of integrating a learnable handson neural model to guide the search process in program synthesis is novel. The unique combination of bottomup search with onpolicy training distinguishes CROSSBEAM from existing methods. 2. Experimental Results: The empirical evaluation shows a clear advancement over stateoftheart methods. In particular the improvement in both the string manipulation and logic programming domains suggests that CROSSBEAM is broadly applicable and effective. 3. Detailed Methodology: The paper provides a comprehensive description of the CROSSBEAM algorithm from the theoretical underpinnings to the specific implementation details. This thoroughness facilitates reproducibility and understanding of the method.  Weak Points 1. Comparison to StateoftheArt: Though the paper claims significant advancement over existing techniques it would benefit from a deeper comparative analysis discussing why CROSSBEAM outperforms these methods in more detail. 2. Broader Applicability and Scalability: While CROSSBEAM shows impressive results in two domains its applicability to other domains and scalability to larger more complex synthesis tasks remains to be demonstrated. 3. Discussion on Limitations: The paper could provide a more comprehensive discussion on the potential limitations of CROSSBEAM particularly regarding the quality of solutions beyond success rates and the computational costs associated with training and search.  Questions and Feedback to the Author 1. Can the authors elaborate on how CROSSBEAM might be adapted or extended to other program synthesis domains not covered in the evaluations 2. It would be beneficial to include a discussion on the potential scalability of CROSSBEAM especially concerning the size of programs it can effectively synthesize and the computational resources required. 3. Could the authors provide insights into the failure cases of CROSSBEAM Understanding its limitations and scenarios where it may not perform optimally would be valuable.  Review Summary CROSSBEAM represents a significant step forward in the field of program synthesis introducing an innovative approach that utilizes a neural model to guide the search process significantly improving efficiency and success rates in two distinct domains. The paper is wellwritten providing clear explanations of the methodology a comprehensive evaluation and demonstrating substantial improvements over stateoftheart methods. However to strengthen the paper further a more detailed comparative analysis with existing techniques a discussion on broader applicability scalability and potential limitations of CROSSBEAM would be beneficial. Addressing these points would provide a more rounded understanding of the methods strengths and areas for further research. Overall CROSSBEAMs novel approach and impressive experimental results make a valuable contribution to the field of program synthesis offering a promising direction for future research.", "neqU3HWDgE": "Summary of the Paper The paper presents a novel approach for unsupervised learning of disentangled representations in autoencoders proposing the use of a tensor product structure that models the latent space as residing on a torus manifold (a product of unit circles). This structure contrasts with traditional representations that typically rely on vectors in a latent space modeled after a Gaussian distribution. The authors argue that their torusbased representation effectively captures the periodicity and compactness of generative factors facilitating more naturally disentangled complete and informative representations. Utilizing extensive experiments across various datasets they demonstrate the superiority of their method (TDVAE) regarding disentanglement completeness informativeness and a new combined metric the DCscore. The experimental results supported by quantitative and qualitative analyses indicate that the torus latent space provides significant advantages over traditional vectorbased latent spaces. Strong and Weak Points of the Paper Strong Points: 1. Novel Approach: The introduction of a torus manifold for the latent space is innovative addressing the limitations of normative vectorbased latent spaces by capturing the periodic nature of generative factors effectively. 2. Comprehensive Evaluation: The paper provides an extensive experimental analysis across multiple datasets showing the methods effectiveness in disentanglement completeness and informativeness including comparisons with leading VAE architectures. 3. New Metric (DCscore): The introduction of the DCscore as a combined metric for disentanglement and completeness is valuable offering a single measure to evaluate the performance of disentangled representations. 4. Opensource Availability: The availability of the code increases transparency and allows for reproducibility and further research by the community. Weak Points: 1. Scalability Concerns: While the torus representation shows advantages the paper briefly touches on scalability issues when dealing with datasets containing a large number of generative factors which might limit its applicability in more complex scenarios. 2. Limited Discussion on Limitations: The discussion on the limitations of the proposed method could be expanded to provide a more balanced view including potential challenges in training stability computational efficiency and practical applicability to realworld problems. 3. Comparative Analysis: While the paper compares the proposed method with other VAE architectures a deeper analysis comparing the proposed torus approach with other nonVAEbased disentanglement methods could provide further insights into its relative advantages and limitations. Questions and Feedback for the Authors:  Could the authors elaborate on potential approaches to address the scalability concerns mentioned particularly when dealing with highdimensional data  Is there any insight into how the methods performance might vary with different kinds of data distributions or in domains beyond image representation  How does the training complexity of TDVAE compare with traditional VAEs and what are the practical implications for its applicability in largerscale problems Review Summary The paper introduces a significant advancement in unsupervised disentanglement learning through the employment of a torus structure in the latent space of autoencoders presenting a novel approach that addresses the limitations of vectorbased representations. The extensive experimental analysis underpins the methods effectiveness against established metrics and introduces a beneficial new metric the DCscore contributing meaningful insights to the field. While highlighting strong points for innovation comprehensiveness and practical contributions the review identifies areas for improvement including addressing scalability providing a broader comparative analysis and expanding the discussion on limitations. Overall the paper represents a valuable contribution to advancing the understanding and application of disentangled representations in unsupervised learning warranting further exploration and adoption in wider contexts.", "kG0AtPi6JI1": " Summary of the Paper This paper addresses a significant limitation in deep neural networks their tendency to specialize for a single task and domain hindering their application in multidomain contexts where domain annotations necessary for current multidomain learning approaches are absent. The authors introduce \"latent domain learning\" a novel learning setting where models are trained across multiple domains without explicit domain annotations. Through comprehensive experiments the paper demonstrates that standard models and existing multidomain strategies perform poorly in this new setting leading to the development of a sparse adaptation strategy. This strategy dynamically adapts internal feature representations to various domains in the data without needing explicit domain labels. The authors assert that their method not only integrates seamlessly with existing models but also supports tasks related conceptually providing examples of empirical fairness problems and longtailed recognition.  Strong and Weak Points of the Paper  Strong Points: 1. Innovative Approach: The introduction of latent domain learning is novel and addresses an underexplored but realistic problem in deep learning research which is the handling of multidomain data without the need for laborintensive data curation. 2. Versatile Methodology: The sparse latent adaptation (SLA) methodology proposed is robust allowing seamless integration with existing models and showing broad applicability to different tasks beyond simple domain adaptation. 3. Comprehensive Experiments: The paper provides a thorough quantitative analysis across various datasets showing how the proposed method outperforms existing models and strategies in the context of latent domain learning. 4. Broad Relevance: The method\u2019s relevance to conceptually related tasks such as empirical fairness issues and longtailed recognition highlights its potential broad impact on various fields of machine learning.  Weak Points: 1. Lack of Comparison with NonSparse Methods: While the paper justifies the choice of sparse gating mechanisms it lacks a direct comparison with potential nonsparse counterparts which could have helped emphasize the advantages of sparsity in adaptation mechanisms. 2. Implementation Details: The paper could have provided more detailed implementation information or access to the codebase for reproducibility and to facilitate understanding of the SLA methods specifics. 3. Discussion on Limitations and Scalability: Although the paper mentions overfitting when \\(K  D\\) in Appendix C a more detailed discussion on the limitations scalability and potential bottlenecks of SLA especially in dealing with larger and more complex datasets would be beneficial.  Questions and Feedback to the Author: 1. Comparison with NonSparse Methods: Can you provide insights or results on how SLA compares with nonsparse latent domain adaptation methods if any were considered 2. Implementation and Reproducibility: Could more detailed implementation details be shared including hyperparameters and the codebase to aid in reproducibility and independent testing of the SLA method 3. Scalability and Limitations: Could the authors elaborate on the potential scalability issues of SLA especially concerning handling larger and more complex datasets including any observed or anticipated bottlenecks  Review Summary This paper presents a groundbreaking approach to a realistic problem in machine learning: learning across multiple domains without explicit domain annotations. The introduction of latent domain learning and the formulation of sparse latent adaptation (SLA) as a solution is both innovative and impactful offering a new direction for future research in deep learning across varied domains. The methods versatility demonstrated effectiveness and relevance to broad machine learning tasks reinforce the significance of this work. However the paper would benefit from providing a clearer comparison to nonsparse methods detailed implementation details for better reproducibility and a thorough discussion on its scalability and limitations. Based on the strong points and the potential for significant impact I recommend this paper for publication contingent on addressing the provided feedback to ensure clarity on the methods applicable scope and limitations.", "h-z_zqT2yJU": " Peer Review of \"Adaptive Temperature Knowledge Distillation for Neural Network Efficiency\"  1. Paper Summary This paper introduces a novel approach named Adaptive Temperature Knowledge Distillation (ATKD) to address the degradation problem encountered in traditional knowledge distillation processes especially when a significantly larger teacher model is used to train a smaller student model. The crux of the problem lies in the sharpness gap generated by the difference in output distribution sharpness between the teacher and the student models with larger models producing sharper outputs. To counter this the authors propose a novel metric for quantifying the sharpness of model outputs and use it to adaptively adjust the temperatures of the teacher and student models during training thus reducing the sharpness gap and making the learning process more effective for the student model. The effectiveness of this approach is demonstrated through extensive experiments on CIFAR100 and ImageNet datasets where ATKD not only alleviates the degradation problem but also achieves noteworthy performance improvements including training the best ResNet18 model on the ImageNet dataset with a reported 73.0 accuracy.  2. Strong and Weak Points Questions and Feedback Strong Points:  The paper addresses a wellknown but underexplored problem in knowledge distillation which has significant implications for the efficiency and practicality of deploying neural network models in resourceconstrained environments.  The introduction of a metric to quantify sharpness is innovative and could have broader applications beyond just adjusting temperature in knowledge distillation.  The experimental validation is thorough covering two major datasets and comparing the proposed method against a comprehensive set of stateoftheart knowledge distillation techniques thus providing a strong empirical basis for the claims made. Weak Points:  While the paper provides a novel mechanism to adjust temperatures based on sharpness metrics it does not fully explore the theoretical underpinnings of why this sharpness gap exists and how it impacts the students learning process in detail.  The methods dependency on the assumption that logits are zeromeaned for each training example is a limitation. It would be beneficial to discuss or test the robustness of ATKD under conditions where this assumption does not hold.  Further insights into the computational overhead introduced by calculating the sharpness metric and adjusting temperatures adaptively would be beneficial especially in the context of resourceconstrained settings. Questions and Feedback:  How does the proposed sharpness metric and adaptive temperature adjustment mechanism perform in scenarios with nonimage data or tasks beyond classification such as detection or segmentation  It would be helpful to include more discussion on the implications of choosing the initial temperatures and how sensitive the ATKD process is to these initial settings.  A comparison between the computational costs of traditional knowledge distillation and ATKD including the overhead of computing the sharpness metric and updating temperatures would provide a clearer picture of the practicality of implementing ATKD in realworld scenarios.  3. Review Summary The paper presents a valuable contribution to the field of neural network efficiency and knowledge distillation by tackling the degradation problem that arises with oversized teachers. The concept of using a sharpness quantification metric to adaptively adjust temperature settings during the distillation process is both novel and pragmatically beneficial as demonstrated by the comprehensive experimental evaluations. However the paper could be strengthened by delving deeper into the theoretical explanations for the observed phenomena exploring the methods applicability beyond classification tasks and providing more details on its computational implications. Nonetheless the approach represents a significant step forward in making knowledge distillation more effective and widely applicable potentially paving the way for further research that could leverage the proposed sharpness metric and adaptive temperature adjustments in a variety of machine learning contexts.", "tFQyjbOz34": " PeerReview of \"Quantifying Modularity in Deep Neural Networks using Spectral Clustering and Interpretability Tools\"  1) Summary The paper investigates the modularity of deep neural networks  a property suggesting that networks structure can perform distinct comprehensible subtasks relevant to the overall task. The authors present a novel framework to quantify modularity through partitioning a networks neurons and assessing these partitions using two proxies  importance and coherence. Importance evaluates how crucial subsets of neurons are to a networks performance while coherence assesses the consistency of associated features among neurons within a subset. The authors utilize statistical methods traditionally applied for analyzing individual neurons to measure these proxies across partitions generated by spectral clustering based on network weights or neurons activation correlations. Their findings from experiments across various network scales and types demonstrate that spectral clustering can reveal partitions significantly more modular (in terms of importance and coherence) compared to random groups hence providing insights into network functionality and aiding interpretability.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Methodology: The paper introduces a unique approach to quantifying neural network modularity leveraging spectral clustering and existing interpretability tools innovatively. This method has the potential to advance understanding of neural network learning dynamics significantly.  Comprehensive Evaluation: The paper conducts thorough evaluations across different network scales and architectures ensuring the robustness and general applicability of the findings.  Contribution to Interpretability: By demonstrating a method to identify modular subsets of neurons this research contributes to the broader goal of making deep learning models more interpretable and understandable. Weak Points:  Lack of Comparison: The paper does not provide a comparative analysis with existing methods targeting similar research questions making it difficult to gauge the relative effectiveness of the proposed approach.  Limited Discussion on Practical Implications: While the methodology is sound the paper could benefit from a deeper discussion on how this approach can be pragmatically applied within the AIML field especially in improving model design or interpretability in realworld applications.  Potential Overfitting Concern: Theres little mention of how the proposed method might be sensitive to overfitting the spectral clustering to the networks analyzed potentially limiting the generalizability of the detected modularity. Questions and Feedback:  How does the proposed method scale with increasingly large and complex networks especially those applied in cuttingedge problems  Can the authors elucidate on any observed tradeoffs between network performance and modularity as quantified by their methodology  Feedback: It would be beneficial to include a comparison with other modularity quantification methods if available and discuss the advantages and challenges of the proposed approach in that context. Additionally exploring the impact of detected modularity on practical concerns such as model compression or adversarial robustness could significantly enhance the papers relevance.  3) Review Summary The manuscript presents a novel and comprehensive framework for quantifying modularity in deep neural networks which stands out for the innovative use of spectral clustering and interpretability tools. The empirical analysis is rigorous covering various network architectures and scales and demonstrates the methods ability to reveal meaningful modular structures within networks. However the paper could be improved by incorporating comparative analyses with existing methods (if applicable) expanding on the practical applications and implications of the methodology and addressing potential sensitivity to overfitting. Despite these suggestions for improvement the paper makes a significant contribution to the ongoing efforts to understand and interpret the complex inner workings of deep neural networks. The methodology proposed has the potential not only to advance academic understanding in the field but also to offer practical benefits in designing more efficient interpretable and robust neural network models.", "upnDJ7itech": " Summarize the paper The paper introduces Knowledge Infused Decoding (KID) a decoding algorithm designed to dynamically incorporate external knowledge into the generation process of pretrained language models (LMs) such as GPT2 and BART. KID is distinguished by its capability to interact with an external knowledge trie and update a local knowledge memory employing reinforcement learning to guide the decoding process with knowledge constraints. The algorithm effectively addresses issues of counterfactual generations and exposure bias particularly in knowledgeintensive natural language generation (NLG) tasks. Evaluation of KID across six diverse NLG tasks demonstrated superior performance to both baseline and stateoftheart models including in fewshot scenarios without necessitating expensive retraining or architectural modifications.  Strong and weak points Questions and Feedback Strong Points: 1. Innovation in Knowledge Integration: The dynamic approach to incorporate external knowledge during each step of the decoding process as opposed to static knowledge embedding represents a significant advancement in tackling the limitations of current LMs in knowledgeintensive tasks. 2. Model and Task Agnosticism: KID\u2019s compatibility across different LMs and NLG tasks without requiring specific architectural changes or extensive retraining underscores its versatility and practical applicability. 3. Performance Improvement: Demonstrated effectiveness in enhancing generation quality across multiple NLG tasks and outperforming stateoftheart methods in specific instances highlight KIDs value. Its ability to mitigate exposure bias and improve longer sequence generation quality is particularly commendable. 4. Empirical Evidence: Comprehensive experiments along with both automated and human evaluations provide a robust evidence base for KID\u2019s efficacy in diverse settings including fewshot learning scenarios. Weak Points: 1. Computational Efficiency Concerns: While KID avoids retraining costs the computational efficiency of the dynamic knowledge retrieval and triebased decoding adjustments especially in realtime applications is not thoroughly discussed. 2. Generalizability and Scalability: The scalability of KID to even larger LMs (e.g. GPT3 175B) and its performance in an extensive range of NLG tasks beyond those tested remain uncertain possibly limiting broader applicability insights. 3. Dependency on External Knowledge Sources: The success of KID heavily relies on the availability and quality of external knowledge sources raising questions about its effectiveness in scenarios where such resources are sparse or of lower quality. Questions and Feedback to the Author: 1. How does KID perform in realtime applications where computational efficiency is crucial Given the dynamic nature of knowledge retrieval and triebased decoding guide insights into latency and computational overhead would be valuable. 2. Could the authors expand on KID\u2019s scalability to larger LMs and a broader range of NLG tasks This would provide a clearer picture of KID\u2019s potential beyond the current evaluation scope. 3. How does KID handle inconsistencies or errors in external knowledge sources An exploration of error propagation from incorrect or outdated external knowledge to generated text would be informative. 4. Feedback: Incorporating analysis on computational efficiency and exploring mechanisms to validate or rectify external knowledge sources could enhance the robustness and applicability of KID. Additionally extending evaluations to cover a wider array of LMs and NLG tasks could further establish its generalizability.", "t98k9ePQQpn": " Review of the Paper on Posthoc Correction via Optimal Transport for Imbalanced Recognition  1) Summary of the Paper: The paper addresses the significant challenge of longtailed recognition in machine learning where the distribution of the training dataset is highly imbalanced affecting model generalization. It distinguishes itself by focusing on posthoc correction strategies to balance predictions after training. The authors introduce a novel approach leveraging optimal transport (OT) theory facilitating the adjustment of predictions to align with the desired distribution. Unlike traditional statistical methods for posthoc correction this paper proposes using a linear mapping technique to adaptively learn the cost matrix required in OT eliminating the need for manual configuration. The paper claims this method achieves superior performance on several benchmarks demonstrating its efficacy and efficiency compared to existing solutions.  2) Strong and Weak Points Questions and Feedback:  Strong Points:  Innovative Approach: The papers focus on using optimal transport theory for posthoc correction is a novel idea that contributes to the fields advancement offering a new way to tackle the longtailed recognition problem.  Automatic Cost Matrix Learning: The ability to learn the cost matrix automatically through linear mapping is a significant advantage potentially reducing the complexity and expert knowledge required in OT applications.  Empirical Results: The experimental section provides strong evidence supporting the methods high efficiency and superior performance compared to other methods underscoring its practical value.  Weak Points:  Lack of Theoretical Analysis: The paper does not provide a detailed theoretical analysis to support the empirical findings especially regarding the convergence properties and the choice of linear mapping for cost matrix learning.  Generalizability Concerns: While the method shows impressive results on specific benchmarks the paper does not discuss its applicability and performance on datasets with different characteristics or in other domains.  Comparison with StateoftheArt: Although the paper claims superior performance over existing methods a more thorough comparison including more recent approaches or other OTbased methods would strengthen the argument.  Questions and Feedback:  How does the method perform in scenarios with extremely high imbalance ratios Is there a point where the methods performance significantly deteriorates  The paper mentions the methods nonapplicability in certain scenarios such as when dealing with small batch sizes. Could the authors elaborate on potential solutions or adaptations to overcome these limitations  Feedback: It would be beneficial to include a theoretical discussion on the rationale behind using linear mapping for learning the cost matrix and its impact on the methods overall performance and robustness.  3) Review Summary and Recommendation: The paper introduces a compelling methodology for posthoc correction in the context of longtailed recognition leveraging optimal transport theory and a novel automatic cost matrix learning technique. The experimental evidence provided supports the methods efficiency and superior performance. However the paper would benefit significantly from a more detailed theoretical analysis and discussion on its applicability across different scenarios and datasets. Additionally a more extensive comparison with stateoftheart methods including those outside the immediate purview of OT would strengthen the papers claims. Given the novelty of the approach and its demonstrated potential I recommend acceptance provided that the authors address the mentioned concerns particularly the need for deeper theoretical insight and broader comparisons. The paper makes a valuable contribution to the machine learning communitys ongoing efforts to tackle longtailed recognition problems and with further refinement it could open new avenues for research in posthoc correction methods.", "roxWnqcguNq": " Peer Review  Summary of the Paper This paper introduces a novel method for identifying Argument Discourse Units (ADUs) in text using a constituency tree representation of sentences. By incorporating grammatical structure into a BERTCRF (Conditional Random Field) model and combining this with Graph Neural Networks (GNNs) the authors aim to improve the accuracy of ADU identification. The paper thoroughly compares the performance of different maximum depths for constituency trees examines the benefits of integrating a GNN and explores the addition of a CRF layer to model global dependencies among labels. An important aspect of this study is its focus on model interpretability seeking to understand how the tree structure contributes to model predictions. The authors claim improvements over existing models for tokenlevel ADU recognition and provide insights into the models decisions through explainability methods.  Strengths and Weaknesses Strengths: 1. Novel Application: The approach of merging constituency trees with BERT and GNNs for argument identification is innovative and addresses the limitations of linear dependencies. 2. Comprehensive Evaluation: The paper presents a detailed comparison of model performances across different configurations offering insights into the impact of tree depth on the models accuracy. 3. Interpretability Focus: By incorporating explainability methods this study not only enhances the performance but also contributes to the understanding of how models make predictions a crucial aspect often overlooked in similar studies. 4. Publicly Available Code and Dataset: Making the code and dataset accessible after the review process is commendable facilitating replication and further research in the field. Weaknesses: 1. Lack of Comparative Baselines: While the paper presents improvements over certain models it lacks a broad comparison with a wider range of existing methods which could provide a clearer picture of the proposed models standing. 2. Dataset Limitations: The paper does not deeply explore the implications of dataset biases or the limitations imposed by focusing on tokenlevel annotations. A discussion on how the model might perform with more diverse or larger datasets would be instructive. 3. Computational Resources: The high memory requirement for training (noted at 30 GB) could limit the accessibility of the proposed approach for researchers with limited computational resources. Questions and Feedback: 1. Comparative Analysis: Could you expand on how your model performs relative to other stateoftheart approaches in ADU recognition beyond the BERTCRF baseline 2. Impact of Dataset Diversity: How do you anticipate the models performance would change with a broader or more complex dataset Is there a plan to test the model on datasets from different domains or languages 3. Handling of Computational Constraints: Given the significant memory requirements are there optimizations or modifications to the model that could reduce these demands without substantially impacting performance  Review Summary The paper presents a significant advancement in argument recognition through the innovative use of constituency trees GNNs and CRFs leveraging grammatical structures for more accurate ADU identification. The enhanced focus on model interpretability is particularly noteworthy providing valuable insights into the decisionmaking processes of complex models. However the study would benefit from a broader comparative analysis with existing methods and a deeper discussion on its potential limitations including dataset diversity and computational resource requirements. The computational demands highlighted present a potential barrier to adoption and further work could explore efficiency improvements. Despite these concerns the novel approach and demonstrated improvements in ADU recognition represent a substantial contribution to the field of NLP. The decision to make resources publicly available postreview is commendable and ensures the studys relevance and utility for further research. Given the strengths of the proposed method and the areas for improvement outlined I recommend this paper for publication with minor revisions particularly those aimed at expanding comparative analyses and addressing dataset diversity considerations. The advancements presented alongside the commitment to transparency and reproducibility underscore the papers value to the NLP and argument mining communities.", "pbduKpYzn9j": " Peer Review of \"Distilling Unconditional Generative Adversarial Networks with a Focus on StyleGAN2 Architecture\"  1) Paper Summary This paper tackles the challenge of deploying Generative Adversarial Networks (GANs) particularly the StyleGAN2 model on resourceconstrained devices by proposing a novel method for GAN distillation. The authors identify output discrepancy  the difference in outputs between the teacher (original model) and the student (compressed model) given the same input  as a significant barrier in unconditional GAN distillation. They demonstrate that the style module within StyleGAN2 is crucial for maintaining semantic consistency between the teacher and student model outputs. To address the output discrepancy they introduce an innovative student model initialization strategy that inherits weights from the teachers style module while keeping convolutional layers randomly initialized. Additionally they propose a latentdirectionbased distillation loss to leverage the relational knowledge in the latent space achieving stateoftheart results in StyleGAN2 distillation. Extensive experiments underscore the effectiveness of their approach significantly outperforming existing GAN distillation methods.  2) Strong and Weak Points Questions and Feedback Strong Points  Innovative Approach: The novel initialization strategy and the latentdirectionbased distillation loss are innovative contributions that address the critical challenge of output discrepancy in GAN distillation.  Comprehensive Analysis: The paper provides a thorough analysis of the challenge of unconditional GAN distillation particularly focusing on the role of the style module in StyleGAN2 which is wellsupported by comparative experiments.  StateoftheArt Results: Demonstrated through extensive experiments the approach significantly outperforms existing methods in distilling StyleGAN2 across various metrics showing a deep understanding of the underlying issues and offering effective solutions. Weak Points  Lack of Details on Implementation and Computational Efficiency: While the paper mentions that the proposed methods achieve better performance there is limited discussion on the computational efficiency and the practical implications of deploying the distilled models on resourceconstrained devices.  Potential Overfitting to StyleGAN2: The focus on StyleGAN2 while understandable given its popularity raises questions about the generalizability of the approach to other GAN architectures. Questions and Feedback  How does the proposed distillation method affect the computational efficiency and latency of the student GAN on resourceconstrained devices More details on this aspect would strengthen the practical applications of the method.  Could the authors elaborate on the potential applicability or modifications needed to apply this distillation approach to other GAN architectures beyond StyleGAN2  It would be beneficial to discuss any limitations or challenges encountered during the development of the initialization strategy and latentdirectionbased loss providing insights into potential areas for future research.  3) Review Summary The paper presents a compelling approach to the distillation of unconditional GANs particularly focusing on the StyleGAN2 architecture. The introduction of a novel initialization strategy and a latentdirectionbased distillation loss addresses the critical challenge of output discrepancy between the teacher and student models a significant contribution to the field. Accompanied by thorough analysis and experimentally demonstrated stateoftheart results the paper makes a strong case for its methods. However there are areas where the paper could be improved for greater impact particularly concerning the specifics of computational efficiency practical deployment considerations and generalizability to other GAN architectures. Addressing these points would not only enhance the papers contributions but also provide clearer insights into the broader applicability and limitations of the proposed methods. Overall the paper is wellstructured presents significant findings and importantly opens new pathways for research in GAN compression and distillation. I recommend its acceptance with minor revisions provided the authors can address the questions and feedback posed particularly about computational efficiency and the approachs applicability to other GAN architectures.", "n54Drs00M1": " Summarize the Paper This paper presents a novel approach to enhancing the Affect Control Theory (ACT) by utilizing advanced natural language processing (NLP) techniques specifically a finetuned Bidirectional Encoder Representations from Transformers (BERT) model. ACT a theory developed in the 1970s to model human behavior in social interactions through sentiments faces a significant limitation due to the small vocabulary of affective lexicons derived from costly and timeconsuming surveys. The authors propose BERTNN a framework that surpasses this limitation by generating a vast and accurate affective lexicon from synthetic data thereby allowing for a broader application of ACT across various fields including online messaging and humancomputer interaction. BERTNN was trained on synthetically generated data describing social events and resulted in stateoftheart accuracy in estimating affective meanings across three dimensions: Evaluation Potency and Activity (EPA).  Strong and Weak Points of the Paper  Strong Points: 1. Innovation in Approach: The paper excellently leverages the capabilities of BERT an advanced NLP model to address a significant limitation in ACT showcasing an innovative crossdisciplinary application. 2. Comprehensive Methodology: The detailed explanation of the methodology from data generation to model tuning provides clarity and insight into the process enabling reproducibility and further research. 3. Empirical Validation: It presents a thorough comparison with existing methods and demonstrates superior performance across several metrics establishing the efficacy of BERTNN.  Weak Points: 1. Data Generation and Representation: The reliance on synthetically generated data might not capture the nuanced realworld variability in human sentiment and social interaction. This limitation could impact the models applicability in complex social scenarios. 2. Evaluation and Correlation Analysis: While the model achieves high accuracy and correlation with existing affective lexicons the paper could further benefit from a qualitative evaluation or case study demonstrating its effectiveness in practical ACT applications. 3. Generalizability: The focus on enhancing ACT through BERTNN is innovative yet the findings applicability to different cultures or languages remains unclear potentially limiting its general utility.  Questions and Feedback to the Author: 1. Could you provide insight or examples on how BERTNN might perform or adapt in realworld complex social scenarios beyond the synthetically generated data 2. Is there potential for incorporating unsupervised learning methods to supplement the models ability to identify and understand novel or nuanced affective meanings within text 3. Regarding generalizability are there plans or possibilities to test and tune BERTNN across various cultural contexts or languages ensuring broader applicability  Review Summary This paper showcases a significant advancement in the application of Affect Control Theory (ACT) through the integration of a finetuned BERT model addressing a longstanding limitation of ACT  the scope of affective lexicons. The innovative approach BERTNN not only broadens the affective lexicon but also improves the accuracy of sentiment analysis in social behavior modeling. The strong empirical validation against existing methods and the detailed explanation of the methodology are commendable aspects of the paper contributing to its reliability and potential for future research. However the paper could be strengthened by addressing the limitations related to the synthetic datas representation of realworld complexity evaluating the models practical application in diverse scenarios and exploring the frameworks adaptability across different cultures or languages. Addressing these points would not only enhance the papers contribution to the field but also broaden its impact and applicability. Considering the innovative approach strong empirical foundation and the potential for significant contributions to both the fields of ACT and NLP I recommend this paper for publication provided the authors address the identified weaknesses and consider the questions raised regarding its broader application and evaluation.", "uVTp9Z-IUOC": " Peer Review of \"Improving Robustness via TestTime Adaptation with NonVanishing Gradients\"  1) Summary of the Paper This paper presents a novel approach to fully testtime adaptation for improving the robustness of pretrained models against common corruptions and domain shifts without requiring source data or target domain labels. The authors address limitations of entropy minimization which tends to produce vanishing gradients for highconfidence predictions by proposing two new loss functions based on the negative log likelihood ratio that maintain nonvanishing gradients for improved selfsupervision during adaptation. The paper also introduces a diversity regularizer update and a parametrized input transformation module designed to counteract domain shifts by preprocessing inputs in a manner that can be learned entirely during testtime adaptation. Experimental results demonstrate the superiority of their method over existing techniques such as TENT and TENT in adapting models to perform well on corrupted versions of ImageNet (ImageNetC and ImageNetR).  2) Strong and Weak Points Questions and Feedback Strong Points:  The introduction of nonvanishing gradientbased loss functions (Lslr and Lhlr) for addressing the shortcomings of entropy minimization represents a significant and novel contribution to the field of testtime adaptation.  The inclusion of a diversity regularizer that uses a moving average which stabilizes model adaptation and prevents the convergence of the model to trivial solutions is both innovative and practically useful.  The parametrized input transformation module that can be learned completely during the testtime adaptation phase without source domain data or target labels is a novel concept that adds considerable value to their method. It shows a deep understanding of the challenges present in testtime adaptation and offers a tangible solution. Weak Points:  The experiments mainly focus on image classification tasks related to domain adaptation and corruption robustness. It would be beneficial if the methods efficacy could also be demonstrated on other tasks or with other types of neural networks to highlight its versatility.  While the paper presents strong empirical results there is limited theoretical analysis on why the proposed nonsaturating losses exhibit superior performance in testtime adaptation. Providing theoretical insights or more indepth explanations could further strengthen the paper.  The methods dependency on correct initialization and hyperparameter settings (e.g. the weight of Lconf and the \u03ba in the running estimate pt(y)) might pose challenges in practice and the paper lacks a detailed sensitivity analysis of these aspects. Questions and Feedback:  Have the authors considered evaluating the approach in a broader set of applications beyond image classification such as natural language processing or speech recognition where domain shifts and data corruptions are also prevalent challenges  Could the authors elaborate on the potential limitations of the proposed input transformation module especially in scenarios with extreme domain shifts or highly novel corruptions not seen during training  It would be valuable for the authors to discuss more explicitly how the proposed methods can be integrated into existing systems or pipelines and any computational overhead they introduce.  3) Review Summary Overall this paper makes a substantial contribution to the area of testtime adaptation by introducing novel loss functions that maintain nonvanishing gradients for highconfidence predictions and by adding a diversity regularizer and a parametrized input transformation module. These innovations collectively result in significant improvements in model robustness to common corruptions and domain shifts as demonstrated by empirical evaluations on challenging benchmarks like ImageNetC and ImageNetR. However the paper would benefit from a broader evaluation across task types and further theoretical analysis or explanations of the observed improvements. Addressing these points could enhance the papers impact and provide readers with a deeper understanding of the mechanisms at play. The strength of the experimental results and the novelty of the proposed methods strongly support the paper\u2019s contributions to the field. Nonetheless more detailed discussions about the methods applicability limitations and potential integration into existing systems would further solidify this works value. Overall this paper represents a significant step forward in testtime adaptation research offering promising avenues for future work in achieving more robust and adaptable models.", "o8iGesI9HN-": " Review of \"Optimized Separable Convolution for Efficient Deep Learning Architectures\"  Summary The paper introduces a novel convolutional operator optimized separable convolution designed to minimize representational complexity and improve efficiency in deep convolutional neural networks (DCNNs). The mainstream convolution operation being a cornerstone in most DCNNs is resourceintensive due to its quadratic dependency on the channel and kernel sizes. Previous efforts to mitigate this issue such as depth and spatial separable convolutions have achieved some level of success but are criticized for their ad hoc nature limiting their capacity for optimal efficiency. The authors propose a sophisticated method for optimizing separable convolutions which significantly reduces complexity to O(C 32 K) for general cases and even further to O(C \u00b7 log(CK)) when all constraints are relaxed. Their approach involves a principled design for determining the internal number of groups and kernel sizes for separated convolutions underpinned by a novel constraint called the volumetric receptive field condition. This ensures that the separable convolution retains the same effective input space consideration as the original unoptimized convolution. Experimental evaluations on benchmark datasets such as CIFAR10 CIFAR100 and ImageNet demonstrate the superiority of the proposed method over existing convolution types in terms of the accuracyparameter tradeoffs.  Strong Points 1. Innovativeness: The proposed optimized separable convolution presents a principled and theoretically grounded approach to reducing convolutional neural network complexity. It fills a gap in the literature by addressing the inefficiencies of existing separable convolution methods with a scientifically robust solution. 2. Experimental Validation: Comprehensive experiments conducted on widely recognized datasets offer strong empirical evidence of the methods effectiveness. The comparison against standard convolution and its known separable variants substantiates the authors claims about the proposed methods superior efficiency and performance. 3. Practical Relevance: By significantly reducing model complexity while maintaining or even enhancing performance the proposed method has substantial implications for the deployment of advanced neural network models in resourceconstrained environments.  Weak Points 1. Lack of Comparison with Recent Innovations: Although the paper provides an extensive comparison with conventional methods it does not benchmark the proposed method against the very latest innovations in neural network efficiency improvements. Inclusion of such comparisons could provide clearer positioning in the current research landscape. 2. Implementation Details: The paper could benefit from more detailed discussion on the practical implementation of the proposed convolution including potential challenges and how they can be mitigated. This would be valuable for practitioners looking to adopt the method. 3. Analysis of Computational Overhead: While the paper emphasizes the reduction in representational complexity and parameter counts it does not thoroughly examine the computational overhead introduced by the optimization process itself. A detailed analysis of this aspect would be beneficial for a holistic understanding of the methods efficiency.  Questions and Feedback to the Author 1. Could the authors elaborate on how the proposed volumetric receptive field condition compares with traditional receptive field considerations in terms of maintaining feature extraction capabilities 2. Have the authors considered the potential of integrating their proposed method with other model optimization techniques such as network pruning or quantization for further efficiency gains 3. The paper mentions that the proposed method can be generalized into an Nseparable case. Could the authors provide more insights into how the choice of N impacts performance and efficiency and how it might be optimized  Review Summary The paper presents a significant contribution to the field of deep learning by introducing a novel method for optimizing separable convolutions. This work tackles the paramount issue of computational and parameter efficiency in convolutional neural networks with a novel principled approach that outperforms existing methods. The experiments rigorously validate the proposed method underscoring its potential for broad adoption in the design of efficient neural network architectures. However the paper could be enhanced by addressing the latest innovations in neural network efficiency providing more detailed implementation insights and analyzing the computational overhead of the optimization process. Addressing these points would strengthen the papers contributions and provide clearer guidance for practitioners. In summary the proposed optimized separable convolution constitutes an exciting advancement in efficient neural network design with compelling experimental results to support its efficacy. With further elaboration on the outlined weak points this work could set a new standard in the field.", "mQxt8l7JL04": " Review of \"GeometryPreserving Regularized Autoencoders for Representation Learning\"  1. Summary This paper introduces a novel regularized autoencoder framework that aims to learn highdimensional data representations by preserving the geometry of the data manifold. The authors propose a hierarchy for geometrypreserving mappings emphasizing conformal maps that preserve angles and relative distances which they argue are better suited for representation learning than isometric mappings. The framework uses a relaxed distortion measure as a regularization term to ensure the learned representations maintain these geometric properties. The approach is tested on diverse image and motion capture datasets showing improved performance over existing methods notably in an unsupervised human face retrieval task using the CelebA dataset.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Technical Depth: The paper presents a unique perspective on representation learning by focusing on the preservation of geometric properties of data manifolds in the latent space. The formulation of a hierarchy for geometrypreserving mappings and the introduction of a relaxed distortion measure for regularization are both technically deep and innovative.  Comprehensive Experiments: The authors conducted extensive experiments on multiple datasets (image and motion capture data) to validate their approach. The use of the CelebA dataset for an unsupervised information retrieval task and achieving a significant improvement over related methods highlights the practical relevance of the proposed framework.  Reproducibility and Open Source: Providing code and detailed experimental setup in appendices and on GitHub enhances the papers reproducibility and potential for impact within the machine learning community. Weak Points:  Assumption of Identity Metric: The assumption that the ambient data space uses the identity metric (H(x)  I) might limit the application of the proposed method in cases where the data manifold could benefit from a more sophisticated metric that incorporates domainspecific knowledge or labels.  Computational Complexity: While the authors mention the computational requirements for calculating the Jacobian and backpropagation through the regularization term a detailed discussion on the computational efficiency (time and memory requirements) compared to existing methods is missing. Questions and Feedback:  Scalability to Larger Datasets: Can the authors elaborate on how their approach scales to larger and more complex datasets Given the computational considerations mentioned insights into the scalability of their method would be valuable.  Impact of Data Augmentation: The use of mixup data augmentation is briefly discussed. Could the authors provide more details on how different data augmentation techniques might impact the performance of their approach  Robustness to Noise: How robust is the proposed geometrypreserving regularization to noisy or incomplete data Additional experiments or discussions on this aspect would strengthen the papers contributions.  3. Review Summary The paper presents a highly innovative approach to representation learning focusing on preserving the geometry of data manifolds within the latent space through regularized autoencoders. By introducing a hierarchy of geometrypreserving mappings and a novel regularization term that encourages conformal mappings the authors address a gap in the current literature on unsupervised representation learning. The experimental results particularly in the unsupervised human face retrieval task demonstrate the effectiveness of the proposed method achieving significant improvements over recent related work. However the paper could be enhanced by addressing the limitation of assuming an identity metric for the ambient data space and providing a more comprehensive discussion on the computational efficiency of the proposed approach. Further exploration of the methods scalability robustness to data quality issues and the impact of differing data augmentation techniques would also be valuable contributions. In conclusion the paper makes significant strides in advancing the field of representation learning by prioritizing geometric preservation within learned representations. With improvements in addressing the outlined weak points the work has the potential to influence future research directions and applications in machine learning and beyond. Recommendation: Accept with minor revisions.", "tm9-r3-O2lt": "Summary of the Paper: The paper introduces a novel framework leveraging Generative Adversarial Networks (GANs) particularly StyleGAN1 and StyleGAN2 to modify the memorability of face images while preserving identity and other facial features such as age and emotion. It addresses the natural phenomenon where certain images especially faces are more memorable than others and explores what attributes contribute to this memorability. The authors posit that memorability is an intrinsic feature of an image consistent across different observers. They create datasets using StyleGANs pretrained on the FFHQ dataset to generate reallooking faces which are then categorized into high and low memorability groups based on scores predicted by computational models. Through logistic regression a hyperplane in the latent space is identified to separate these groups. Modifying the distance of a faces latent vector from this hyperplane allows for control over the memorability score. The method does not require training an auxiliary network and is applied to both generated (unreal) and real human faces. The paper also explores how different layers of StyleGANs latent space influence face memorability and verifies the realness of modified images using FID and KID scores. Strong Points: 1. Innovative Approach: The method for modifying face memorability using GANs specifically without the need for an auxiliary network is novel and represents a significant advantage over previous methods that required manual feature manipulation or additional training. 2. Comprehensive Analysis: The paper effectively uses the largest dataset of human faces with memorability annotations and employs a stateoftheart model (StyleGAN) for generating and modifying images. It also carefully analyses how different layers of the augmented latent space contribute to face memorability showing a thorough understanding of the GAN architecture. 3. Practical Implications: The proposed method has significant potential applications in fields like education advertising and social media where image memorability can be crucial. By demonstrating the effectiveness of the method on real human faces the authors open up new avenues for practical realworld applications of their research. Weak Points: 1. Lack of Comparison: The paper does not provide a detailed comparison with existing methods for modifying image memorability. Including such comparisons could have strengthened the argument for the proposed methods superiority. 2. Limited Discussion on Failures: The paper could benefit from a more detailed discussion on the limitations or potential failures of the method especially when applied to real human faces. Understanding the conditions under which the method might fail or be less effective is crucial for practical applications. 3. Ethical Considerations: Modifying the memorability of faces especially without consent can have ethical implications. The paper lacks a discussion on the ethical considerations of using such technology. Questions and Feedback to the Author: 1. Have the authors considered a more detailed comparison with existing techniques for modifying image memorability including those that do not use GANs 2. Can the authors expand on the limitations of their method particularly in terms of its application to real human faces 3. It would be beneficial if the authors could include a discussion on the ethical considerations and potential misuse of the technology presented in this paper. Review Summary: The paper presents a novel and effective method for modifying the memorability of face images using GANs specifically StyleGAN1 and StyleGAN2 without the need for training an auxiliary network. The comprehensive analysis of how different layers of StyleGANs latent space influence face memorability coupled with the practical applications of the method in various fields demonstrates the papers significant contributions to the understanding and manipulation of image memorability. However the paper would benefit from a more detailed comparison with existing methods a deeper exploration of its limitations especially when applied to real faces and a discussion on the ethical implications of its use. Overall the paper provides valuable insights and a promising approach to modifying image memorability using GANs with potential for broad practical applications.", "y7tKDxxTo8T": " Review of \"ZEroShot Recommenders (ZESREC): Enabling Generalization in Recommender Systems\"  Summary: This paper tackles the significant challenge of recommender systems (RecSys) in datascarce environments particularly new startups or products without substantial useritem interaction data. It introduces ZEroShot Recommenders (ZESREC) an algorithm leveraging zeroshot learning for RecSys. Unlike traditional methods that rely on overlapping users or items between datasets or utilize categorical item identifiers (IDs) ZESREC employs generic item features (e.g. descriptions images) as continuous indices and takes advantage of recent advances in sequential RecSys for representing users. The paper evaluates ZESREC across three pairs of realworld datasets demonstrating its potential in providing recommendations in zeroshot settings where both the users and items of the target domain are unseen during training.  Strong Points: 1. Innovation in Addressing Data Scarcity: The paper presents a novel approach to circumvent the lack of data in earlystage RecSys through zeroshot learning a significant breakthrough for startups and new products. 2. Utilization of Generic Features for Items: This methods departure from traditional categorical identifiers to using natural language images and videos for items is commendable allowing for a more versatile application across different domains. 3. Empirical Evidence and RealWorld Application: The authors provide extensive empirical results that confirm ZESRECs effectiveness across different datasets. The inclusion of case studies further substantiates its practical utility in understanding and generalizing user behavior patterns across domains.  Weak Points: 1. Assumption of Domain Commonality: The algorithms effectiveness might be significantly limited by the prerequisite of some commonality between the source and target domains which isnt always guaranteed or quantifiable. 2. Potential for Bias in User Representation: Relying solely on user interactions with items for representation might introduce biases especially in scenarios where user interaction data is minimal or not fully representative of their preferences. 3. Computational Complexity and Scalability: The paper does not address the algorithms computational demands or scalability especially pertinent when dealing with largescale datasets typical of RecSys applications.  Questions and Feedback: 1. Quantification of Domain Commonality: How does the algorithm perform in scenarios with minimal commonality between domains and is there a methodology to quantify this commonality to predict ZESRECs effectiveness 2. Mitigation of Potential User Representation Bias: Have alternate strategies been considered or explored to complement user interaction data for a more robust and unbiased user representation 3. Computational Resources and Scalability: Could the authors shed light on the computational resources required for ZESREC especially when applied to large datasets and discuss potential scalability solutions  Review Summary: ZEroShot Recommenders (ZESREC) stands out for its innovative approach to overcoming the data scarcity problem in RecSys through zeroshot learning. By employing generic item features and advancements in sequential RecSys the algorithm demonstrates a commendable ability to generalize across different domains without the need for overlapping users or items. The empirical validation across realworld datasets and insightful case studies significantly bolster its applicability and effectiveness. However the reliance on domain commonality potential biases in user representation and the lack of discussion on computational demands and scalability present areas for improvement. Questions regarding the quantification of domain commonality strategies for enhancing user representation and details on algorithmic resource demands are crucial for further validating and strengthening ZESRECs utility and wider adoption. Overall ZESREC represents a promising advancement in tackling the perennial issue of data scarcity in RecSys. With further exploration and refinement in the highlighted areas it has the potential to significantly benefit startups and new products in leveraging Recommender Systems effectively right from their inception.", "hUr6K4D9f7P": " Peer Review of \"Weighted Truncated Adversarial Weight Perturbation for Graph Neural Networks\"  1. Summary The paper introduces and rigorously analyzes Weighted Truncated Adversarial Weight Perturbation (WTAWP) a novel technique aimed at enhancing the generalization of Graph Neural Networks (GNNs) through the identification and mitigation of flat local minima. In light of the substantial evidence suggesting that flatter minima are indicative of better generalization in neural networks the authors set out to explore this relationship within the context of noni.i.d. graph data marking a pioneering study in this direction. Their approach WTAWP seeks to counteract a vanishinggradient issue identified in existing AWP formulations by applying perturbations selectively and weighting the standard loss alongside the AWP loss. Theoretical insights are provided alongside comprehensive experimental evaluations demonstrating the efficacy of WTAWP in improving both natural and robust generalization across various graph learning tasks and models with minimal computational overhead.  2. Evaluation  Strong Points:  Innovative Approach: The paper proposes an original solution (WTAWP) to a previously unnoticed problem (vanishinggradient issue in AWP for GNNs) advancing our understanding and methodology in enhancing neural network generalization.  Theoretical Foundation: The theoretical analysis including a generalization bound for noni.i.d. graph classification and an indepth examination of the proposed WTAWP method\u2019s impact on local extrema provides a robust underpinning for the empirical findings.  Comprehensive Experiments: Extensive experimental work covering various datasets GNN architectures and performance metrics (clean and robust accuracy certified robustness etc.) thoroughly validates the proposed approach.  Practical Relevance: Addressing the challenge of finding flat minima in GNNs especially given their susceptibility to adversarial attacks has significant implications for deploying safe and reliable AI systems in realworld scenarios.  Weak Points:  Discussion on Limitations: While the paper provides a detailed analysis and empirical evaluation a more explicit discussion on the limitations and potential downsides of WTAWP would enhance the reader\u2019s understanding of where the method might fall short or how it might be further improved.  Comparison with Baselines: The paper could further strengthen its evaluation by comparing WTAWP not only against vanilla AWP and natural training but also against other SOTA regularization or adversarial training methods that could be applicable to GNNs.  Insight into Feature Importance: Although the paper focuses on weight perturbation incorporating an analysis on how WTAWP influences the importance or role of different node features or graph properties in the learning process could provide deeper insights into its operation.  Questions and Feedback to the Author:  Have the authors considered exploring the scalability of WTAWP to larger more complex graph structures such as those encountered in social network analysis or proteinprotein interaction networks  Given the computational efficiency of WTAWP highlighted in the paper could the authors comment on its compatibility with online learning scenarios or dynamic graphs where retraining frequency might be high  A detailed comparison with related approaches in adversarial training for GNNs would be informative. How does WTAWP compare in terms of effectiveness and efficiency  3. Review Summary The paper makes a significant contribution to the literature on GNN generalization and adversarial robustness by introducing a novel and theoretically sound approach WTAWP. Through rigorous theoretical analysis and comprehensive empirical evaluation the authors convincingly demonstrate the effectiveness of their method in finding flat minina thereby improving the generalization of GNNs on a wide range of tasks. The paper is wellorganized thoroughly researched and presents a compelling case for the utility of WTAWP in addressing a critical issue in GNN training. However it would benefit from a more detailed discussion on the methods limitations a broader comparison with existing techniques and further insights into its impact on feature importance within graphs. Overall given the importance of the problem tackled the originality of the proposed solution and the quality of the analysis presented I recommend this paper for publication with minor revisions to address the aforementioned points. The study not only advances our understanding of flat minimas role in GNN generalization but also provides practical tools for improving model robustness and reliability with implications for a wide range of applications.", "ghTlLwlBS-": " 1) Summarize the Paper The authors introduce a novel Feudal Reinforcement Learning (FRL) framework that addresses the challenge of semantic mismatch in readingtoact tasks. These tasks require agents to comprehend highlevel natural language instructions and execute associated lowlevel actions within an environment. The proposed FRL model consists of two components: a manager agent and a worker agent. The manager agent processes highlevel textual information formulating a series of subgoals through a multihop reading model. The worker agent on the other hand focuses on achieving these subgoals through interaction with the environment at a low level. This separation of concerns allows the model to navigate the gap between highlevel instruction understanding and lowlevel action execution efficiently. The paper validates the models effectiveness on two benchmarks Read to Fight Monsters (RTFM) and Messenger showing superior performance compared to existing models without the need for humandesigned curriculum learning.  2) Strong and Weak Points Questions and Feedback  Strong Points:  Innovative Framework: The introduction of the FRL framework is a significant advancement in bridging the semantic gap in readingtoact tasks. The dualagent system effectively decomposes complex instructions into actionable steps.  Generalization: The model demonstrates strong generalizability across different environments instruction forms and manuals highlighting the frameworks versatility.  Performance: The FRL framework outperforms existing stateoftheart models on challenging tasks specifically RTFM and Messenger even without relying on curriculum learning showcasing its effectiveness.  Weak Points:  Complexity in Practical Applications: While the model shows promising results in controlled benchmarks the adaptation to realworld applications with far more complex environments and instructions might introduce unforeseen challenges.  Error Propagation Concerns: The paper mentions potential error propagation in the multihop planning process but does not explore solutions extensively. Error correction mechanisms could be critical for realworld applications.  Lack of Insight into WorkerManager Interaction: Although the division of tasks between the manager and worker agents is clear the paper could benefit from a deeper analysis of the dynamics and potential bottlenecks in their interaction.  Questions and Feedback:  Q1: How scalable is the FRL framework when dealing with realworld scenarios that involve more complex and diverse instructions and environments  Q2: Could the authors elaborate on potential strategies to mitigate error propagation in the multihop planning executed by the manager agent  Feedback: Future research could focus on enhancing the interaction between the manager and worker agents possibly by incorporating feedback mechanisms that allow for dynamic adjustment of plans based on realtime environmental feedback or the workers performance.  3) Review Summary The proposed Feudal Reinforcement Learning (FRL) framework represents a significant leap forward in addressing the complex challenge of semantic mismatch in readingtoact tasks. By segregating the processing of highlevel instructions and lowlevel actions into two distinct agents the model not only showcases outstanding performance on challenging benchmarks but also introduces a promising approach to generalizing across various task environments without the need for manually designed curricula. The innovative architecture demonstrating generalizability and superior performance positions the FRL framework as a potentially transformative solution in the field. However further exploration into realworld application scalability error propagation mitigation and the intricate dynamics between the manager and worker agents will be crucial in fully realizing its potential. Overall the paper presents a solid contribution with its comprehensive methodology and convincing experimental results. With additional research focused on addressing the outlined concerns the FRL framework could pave the way for more sophisticated and adaptable readingtoact systems in the future.", "wk5-XVtitD": " Review  1) Summary This paper investigates the utility of language model (LM) pretraining for autonomous decisionmaking in a simulated household environment challenging the LMs with tasks featuring partial observability large action spaces complex states and long dynamics. Specifically the study explores the effectiveness of leveraging a pretrained GPT2 LM for policy initialization which is then finetuned via imitation learning. The tasks are encoded as templated English strings representing observations goals and action history which the LM predicts subsequent actions from. The study consists of several experiments showing that initializing policies with LM pretraining enhances task completion rates significantly for novel task scenarios. Additionally experiments delving into encoded interfacing mechanisms reveal that languagebased environment representations are not strictly necessary for harnessing the benefits of LM pretraining as effective encodings can be learned from scratch. These findings suggest the potential broader applicability of LM pretrained representations in structuring generalization for machine learning tasks outside pure language processing realms.  2) Strengths and Weaknesses Strong Points:  Innovative Application: The paper explores a novel avenue of using LM pretraining within the domain of embodied decisionmaking extending the benefits observed in language tasks to more general machine learning problems.  Comprehensive Experimental Setup: The experiments are welldesigned covering multiple facets of how LMs can be utilized and scrutinizing the role of languageinformed encodings in policy effectiveness.  Significant Improvements Demonstrated: Demonstrating a nearly 20 task completion rate improvement using LMinitialized policies for novel tasks presents a strong case for the utility of LMs beyond conventional language processing tasks. Weak Points:  Scalability and Flexibility: The paper does not address scalability issues or the extensibility of this method when applied to environments vastly different from VirtualHome or to tasks with significantly higher complexity.  Comparison with Stateoftheart: While comparisons are made against various models its unclear how this LM pretraining approach stacks against the latest possibly more sophisticated architectures or policy learning methods.  CrossDomain Generalizability: The leap from the demonstrated success in VirtualHome to broader machine learning applications is promising but speculative. It lacks empirical support on how these findings might translate to distinctly different problem domains. Questions and Feedback:  How does the performance of the LMinitialized policies change with increasing environment complexity or with environments other than VirtualHome  It would be beneficial to compare the proposed method with stateoftheart policy learning approaches directly including those without LM pretraining. How does this method stack up in terms of efficiency generalization and computational resources  The paper suggests broader applications of the technique. Are there preliminary results or additional experiments planned to test the approach in other contexts such as robotics in unstructured environments or decisionmaking in dynamically changing scenarios  3) Review Summary This paper presents an intriguing investigation into the applicability of LM pretraining for enhancing policy learning in nonlinguistic tasks demonstrating a promising avenue for broadening the utility of large language models. The methodology is sound and the experiments are wellcrafted providing clear evidence of the benefits of leveraging pretrained LMs for task generalization in simulated environments. However the study also leaves open questions regarding scalability direct comparison with stateoftheart policy learning methods and the generalizability of the approach across diverse problem domains. Despite these areas for further exploration the paper contributes valuable insights into the potential of LMs to inform and improve decisionmaking models beyond traditional languagecentric applications. The findings encourage continued exploration of the intersection between language processing techniques and broader machine learning challenges suggesting a fertile ground for future research in autonomous systems and AI.", "oMI9PjOb9Jl": " 1. Summary of the Paper The paper introduces a novel approach named DABDETR (Dynamic Anchor Box DETR) that innovatively employs dynamic anchor boxes as queries in the DETR (Detection Transformer) model for object detection. By leveraging the spatial coordinates (x y w h) of anchor boxes as queries the proposed method significantly improves the performance of DETR models by providing explicit positional priors thereby facilitating querytofeature similarity and dealing with the slow convergence issue associated with DETR. This is achieved through the modulation of the positional attention map and an iterative update of the anchor boxes across layers. Extensive experimentation demonstrates that DABDETR achieves superior performance on the MSCOCO benchmark outperforming existing DETRlike models under similar conditions.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Innovative Query Formulation: The approach of using dynamic anchor boxes as queries is novel and addresses the significant limitation of slow convergence in DETR models effectively.  Improvement in Performance: Demonstrating the best performance among DETRlike models on the MSCOCO benchmark is a strong testament to the efficacy of the proposed method.  Comprehensive Experiments: The extensive experimentation and analysis provided substantiate the claims made about the effectiveness of DABDETR.  Weak Points:  Limited Comparison with NonDETR Models: The paper majorly focuses on comparing DABDETR with DETR and its variants but a broader comparison with nonDETR especially the latest models in object detection could provide a clearer picture of its relative standing.  Overlooked Computational Efficiency: While improvements in accuracy metrics are substantial theres a lack of discussion on the impact of these changes on computational efficiency inference time and resource requirements.  Potential for Bias in Hyperparameter Tuning: The selection process and impact of hyperparameters such as the temperature for positional encoding have not been elaborated upon which could bias the performance favorably without extensive validation.  Questions and Feedback:  How does DABDETR perform in scenarios with highly overlapping objects or extreme object scales Given the focus on positional priors and dynamic updates the paper could benefit from a detailed analysis in these challenging scenarios.  Could the authors elaborate on the choice of the temperature parameter for positional encoding Providing insights into how different values affect model performance might help in understanding its sensitivity and optimization process.  It would be beneficial to see results on computational efficiency metrics such as GFLOPs inference time and memory usage alongside accuracy metrics. This would provide a more holistic view of the models performance and applicability in resourceconstrained environments.  3. Review Summary DABDETR represents a significant advancement in the use of Transformers for object detection addressing a critical flaw in DETR models \u2014 slow convergence through a novel query formulation using dynamic anchor boxes. This approach not only improves performance metrics on benchmark datasets but also provides a deeper understanding of the role of queries in transformers. Despite its notable contributions the paper could be strengthened by offering a broader comparative analysis with nonDETR models and detailing the impact on computational efficiency as well as elaboration on the optimization of key hyperparameters. Overall the proposed method showcases promising advancements in leveraging Transformers for object detection tasks meriting consideration for publication after addressing the highlighted concerns regarding comparisons computational efficiency discussions and hyperparameter optimizations.", "gJLEXy3ySpu": " 1) Summary of the Paper This paper addresses the need to certify robustness in machine learning models against \\xe2\\x84\\x930norm adversarial perturbations for topk predictions. Adversarial examples created by perturbing input features to mislead classifiers pose a significant threat especially in applications that rely on topk predictions (e.g. machine learning services recommender systems and web searches). Unlike most existing research that focuses on either top1 predictions under \\xe2\\x84\\x930norm perturbations or topk predictions under \\xe2\\x84\\x932norm perturbations this work aims to provide an almost tight \\xe2\\x84\\x930norm certified robustness for topk predictions using the technique of randomized smoothing via randomized ablation. The method derives a certified radius within which the topk predictions remain robust against adversarial perturbations and proves its almost tightness under the \\xe2\\x84\\x930norm. Empirical evaluations on CIFAR10 and ImageNet show significant improvements in certified top3 accuracy against perturbations compared to existing methods.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The paper tackles a relatively unexplored area by focusing on \\xe2\\x84\\x930norm adversarial perturbations for topk predictions. The innovation lies in bridging the gap between \\xe2\\x84\\x930norm robustness in top1 predictions and \\xe2\\x84\\x932norm in topk predictions using randomized ablation.  Theoretical Contribution: The derivation of an almost tight \\xe2\\x84\\x930norm certified robustness guarantee is a significant theoretical contribution that advances our understanding of model robustness in adversarial settings.  Empirical Evidence: Demonstrating substantial improvements in certified top3 accuracy on wellknown datasets (CIFAR10 and ImageNet) provides strong empirical support for the proposed method. Weak Points:  Practicality of Randomized Ablation: The paper could better address the practical limitations or challenges of implementing randomized ablation especially in realtime or resourceconstrained environments.  Analysis of Failure Cases: While the method shows notable improvements an analysis of conditions or scenarios where it fails or performs suboptimally would have been valuable.  Comparison with NonRandomized Methods: The emphasis on randomized methods for ensuring \\xe2\\x84\\x930norm robustness could overshadow potential benefits of nonrandomized adversarial defense strategies. A more diverse comparison might reveal interesting insights. Questions and Feedback:  Scalability and Performance: Can the authors elaborate on the scalability of the proposed method particularly in highly dimensional input spaces or under stringent latency requirements  Impact of Hyperparameters: The choice of parameters (e.g. e n and \u03b1) significantly impacts the models performance. Could the authors provide more comprehensive guidance or automated strategies for setting these hyperparameters in different application contexts  Future Directions: The conclusion mentions exploring other noise types for \\xe2\\x84\\x930norm robustness and incorporating base classifier information for larger certified radii. Can the authors speculate on how these future directions might concretely materialize or which specific avenues look most promising  3) Review Summary Overall the paper presents a compelling approach to certifying \\xe2\\x84\\x930norm robustness for topk predictions through the innovative use of randomized smoothing via randomized ablation. Its theoretical contributions bolstered by strong empirical evidence set a foundation for future research in creating robust machine learning models against adversarial attacks. However to enhance the depth and applicability of the findings addressing the practical considerations of implementing the proposed method exploring failure cases and expanding the comparative analysis with nonrandomized strategies would be beneficial. Nonetheless this work significantly advances the scholarly discourse on adversarial robustness in machine learning and opens up numerous avenues for further investigation. My recommendation leans towards acceptance provided the authors can address the weak points and consider the feedback for future work or revisions.", "m716e-0clj": " Review of \"Decentralized Augmented Gradient Adam: Converging to the Stationary Point with Less Communication Overhead\"  1. Summary of the Paper The paper presents a novel decentralized adaptive gradient method named Decentralized Augmented Gradient Adam (DAGAdam) designed to address the limitations of existing decentralized adaptive methods in handling data heterogeneity without increasing communication overhead. The traditional adaptthenwhilecommunicate structure used in decentralized adaptive methods such as DAdam and QGDAdam has been identified to cause a deviation from the desired stationary solution due to the high sensitivity to data heterogeneity. The proposed DAGAdam employs a communicatethenadapt structure integrating the concept of a decentralized augmented gradient (DAG) that allows each node to incorporate neighbors information before the adaptive step promoting convergence to the stationary solution in nonconvex scenarios. Theoretical analyses and experimental results across various computer vision (CV) and natural language processing (NLP) tasks demonstrate DAGAdam\u2019s superiority over existing methods including its ability to converge to more accurate solutions with equivalent or lesser communication costs.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The communicatethenadapt strategy is a conceptual advancement over the traditional adaptwhilethen communicate methods offering a promising direction for decentralized optimization.  Theoretical Justification: The paper provides a robust theoretical framework supporting the proposed DAGAdam method including convergence analyses and proofs of improvement over existing methods.  Comprehensive Experimental Validation: The experiments are thorough covering a wide range of deep learning tasks across CV and NLP effectively demonstrating the practical benefits of DAGAdam. Weak Points:  Dependency on Hyperparameter \u03bd: While demonstrating flexibility the performance is significantly sensitive to the choice of \u03bd which can be a limitation in practice without guidance on setting this parameter.  Limited Comparison with NonAdaptive Methods: The focus is primarily on comparing with other adaptive methods. Including nonadaptive decentralized methods could provide a fuller picture of the algorithm\u2019s performance spectrum. Questions and Feedback:  On Hyperparameter \u03bd: Could the authors provide more insight or a heuristic for choosing \u03bd especially in heterogeneous data scenarios to mitigate the sensitivity issue mentioned  Scalability Discussion: How does DAGAdam scale with extremely large networks Although there is some discussion on network sizes up to 64 nodes insights on hundreds or thousands of nodes would be valuable given practical largescale applications.  Extension to Federated Learning: The methods potential applicability to federated learning settings where privacy concerns and data distribution heterogeneity are paramount is intriguing. Could the authors elaborate on adaptions or considerations for such contexts  3. Review Summary The paper introduces DAGAdam an innovative decentralized adaptive gradient method that successfully addresses key limitations of previous methods by adjusting the communication and adaptation sequencing. This work is theoretically sound and supported by extensive experiments demonstrating its efficacy in practice. However the reliance on a sensitive hyperparameter and the lack of broader comparisons slightly diminish its immediate applicability without further guidance on parameter settings and a wider performance benchmark. Overall the contributions are significant marking a substantial advancement in decentralized optimization methodologies. The proposed method not only opens new avenues for research in decentralized deep learning but also provides a viable tool for practical applications pending further examination of parameter sensitivity and scalability. I recommend this paper for publication suggesting that addressing the highlighted concerns could enhance its impact and utility for a broader audience.", "nnU3IUMJmN": "Review of \"Incorporating Structural Locality in NonParametric Language Models\" 1) Summary This paper presents an innovative approach to enhance nonparametric language models (LMs) by integrating structural locality features aiming to improve the relevance of retrieved examples for sequence generation tasks. The core hypothesis is that the contexts structural attributes\u2014such as belonging to the same project hierarchy in source code or being within the same topical section in text\u2014can significantly influence the effectiveness of example retrieval and consequently the overall model performance. By defining and embedding locality features within the model the authors aim to refine the distance metrics used for example retrieval thus ensuring more contextually and structurally relevant suggestions. This approach is empirically tested across two distinctly different domains: Java source code and Wikipedia text with both domains showing performance improvements when locality features are incorporated. The analysis provided delves into how these features contribute to better performance and why existing models which rely solely on contextual similarity fall short of capturing the nuanced structure of data inherent in realworld datasets. 2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Relevance: The paper tackles a critical and often overlooked aspect of LMs especially nonparametric ones\u2014how to improve retrieval quality by leveraging inherent data structure. This exploration is novel and timely given the increasing reliance on large heterogeneous datasets for language modeling.  Empirical Evaluation: The quantitative analysis across two different domains (Java code and Wikipedia text) provides a robust ground for evaluating the effectiveness of the proposed method. The use of realworld largescale datasets enhances the credibility and generalizability of the findings.  Thorough Analysis: The discussion goes beyond just stating performance improvements it investigates where and how the locality features contribute to these enhancements. This analysis provides deeper insights into the models inner workings and the role of structural locality in understanding data. Weak Points:  Feature Selection and Generalization: The paper could benefit from a more comprehensive discussion on the selection process for locality features especially considering the diversity of possible structural attributes across different domains. A systematic guide or framework for feature selection in unexplored domains might enhance the papers utility for future research and practical implementations.  Impact on Model Complexity and Efficiency: While the proposed method shows performance gains there is limited discussion on its impact on computational efficiency and model complexity. Given that nonparametric LMs can already be quite resourceintensive understanding the tradeoffs involved in incorporating locality features is essential for practical applications. Questions and Feedback:  Could the authors provide more insights into how the locality features were selected for each domain Specifically were alternative features considered and if so what was the rationale behind the final selection  How do the proposed enhancements affect model training and inference time Assessing the computational overhead introduced by incorporating locality features would be valuable for potential adopters of the method.  The paper mentions that the method applies where locality effects are present and sufficient data is available. Can the authors elaborate on the limitations or challenges when applying this approach to domains with sparse or less structured data 3) Review Summary The paper introduces an effective method to augment nonparametric LMs with structural locality features demonstrating empirical success in improving model performance across different domains. This work addresses a gap in the current language modeling research by focusing on the structural aspects of datasets which are often overlooked in favor of purely contextual similarity measures. The novel approach combined with rigorous empirical evaluation makes a significant contribution to the field. However the paper could be further strengthened with a detailed discussion on feature selection model complexity and computational efficiency implications. Overall the paper presents a promising direction for enhancing nonparametric LMs with clear evidence of its benefits and thoughtful consideration of its limitations and potential applications. Given the solid experimental grounding and the relevance of the addressed problem I recommend publication with minor revisions to address the raised questions and concerns.", "zXne1klXIQ": "Review of \"Learning Invariant Representations via Selective Augmentation (LISA): Addressing Distribution Shifts by Eliminating Spurious Correlations\" 1) Summary of the Paper This paper introduces LISA an innovative machine learning approach designed to mitigate the challenges posed by distribution shifts particularly domain and subpopulation shifts which are prevalent in realworld scenarios. LISA operates by selectively performing data augmentation (mixup) on samples either within the same label across different domains or within the same domain across different labels. The core hypothesis is that such selective augmentation can effectively cancel out domainrelated spurious correlations thus learning more invariant representations. The method is exhaustively evaluated across nine benchmark datasets covering both types of distribution shifts mentioned with findings indicating that LISA generally outperforms seven prior stateoftheart methods in addressing these shifts. Additionally the paper supports these empirical results with theoretical analysis demonstrating LISA\u2019s ability to achieve smaller worstdomain error compared to ERM and vanilla mixup. 2) Strengths and Weaknesses Strengths:  Comprehensiveness of Evaluation: The thorough empirical testing of LISA across a broad array of datasets and the positive results reported highlight the method\u2019s robustness and versatility in mitigating distribution shifts.  Innovative Approach: LISAs strategy of leveraging selective augmentation to eliminate spurious correlations is a notable departure from traditional methods that focus on regularization and domainspecific alignment offering a new perspective on addressing the challenge of distribution shifts.  Theoretical Underpinning: Providing a theoretical analysis that backs the empirical observations lends credibility and depth to the study situating LISA not only as an empirically validated solution but also as theoretically sound. Weaknesses:  Lack of Comparison With Latest Methods: Although the paper compares LISA with several stateoftheart methods it does not mention or compare with the very latest works in this rapidly evolving field. This comparison could have highlighted LISAs effectiveness more decisively.  Hyperparameter Tuning and Scalability: The efficacy of LISA seems to depend significantly on the appropriate selection of hyperparameters such as the sample selection probability (psel). The paper does not fully address the practical challenges of tuning these parameters especially in very large datasets or complex domain structures.  Explanation on Failures and Limitations: While the paper aptly discusses the strengths and successes of LISA there is a relative lack of detail on when LISA might not perform well or on its limitations which is crucial for readers to gauge the method\u2019s applicability to their specific problems. Questions and Feedback to the Author:  Could the authors provide more insights or empirical evidence on how LISA performs in scenarios with highly complex domain structures or a massive number of subpopulations  The selection of hyperparameters particularly psel appears crucial for the methods success. Could the authors elaborate on strategies for hyperparameter tuning especially in less straightforward scenarios  It would be beneficial for the readers if the authors could discuss any observed or potential limitations of LISA including scenarios where it might not be the best choice. 3) Review Summary LISA represents a significant step forward in addressing the pervasive issue of distribution shifts in machine learning applications. Its novel approach of using selective sample augmentation to eliminate spurious correlations thereby learning invariant representations is both innovative and demonstrably effective across various benchmarks. Furthermore the theoretical grounding provided enhances the contribution\u2019s robustness. However the paper could benefit from a broader comparison with the very latest methods in this area a deeper discussion on the challenges of hyperparameter tuning and a clearer delineation of the methods limitations. Overall LISA\u2019s promising results combined with its novel approach make it an important contribution to the field offering a new avenue for research and application in addressing distribution shifts in machine learning. This methodology not only advances the understanding of how to tackle domain and subpopulation shifts but also opens up new possibilities for future research directions.", "xp2D-1PtLc5": " Review of \"ClsVC: A Novel Framework for Voice Conversion Using a Single Encoder\"  Summary The paper introduces ClsVC an innovative voice conversion (VC) framework designed to address the challenges of separating speaker and content information from speech signals. Unlike previous methods that rely on dual encoders or bottleneck networks to disentangle linguistic and timbre data ClsVC utilizes a single encoder to partition the latent space into content and timbre information supported by specific constraints to maintain this separation. The proposed framework is demonstrated through experiments on the VCTK dataset showing superior performance in both naturalness and speaker similarity when compared to existing models. The paper elaborates on the mathematical foundations underlying the framework discusses the rationale and mechanism of the proposed method through a detailed analysis of the encoder and classification tasks and validates the approach through comprehensive experiments and ablation studies.  Strengths 1. Innovative Approach: The singleencoder framework with partitioned latent space is a novel approach that simplifies the voice conversion process while maintaining or even improving the quality of conversion. 2. Comprehensive Experiments: The paper includes thorough experimental validation including comparisons with stateoftheart methods across different conditions (traditional manytomany VC and oneshot VC) showcasing ClsVCs superiority. 3. Clear Mathematical Formulation: The authors provide a solid mathematical foundation for their approach presenting clear arguments and evidence supporting their models effectiveness in separating content and timbre information. 4. Loss Function Design: The design and explanation of the loss functions for the model showcase thoughtful consideration of the voice conversion problem addressing potential pitfalls and ensuring the model focuses appropriately on different aspects of the VC challenge.  Weak Points and Questions 1. Model Complexity and Efficiency: The paper does not thoroughly discuss the computational efficiency and model complexity of ClsVC. Given that it surpasses other models in performance it would be beneficial to understand its resource demands especially in comparison to dualencoder systems. 2. Generalizability to Other Languages: The experiments are conducted on an English dataset. Questions arise regarding the frameworks performance on other languages and datasets with more diverse phonetic and prosody characteristics. 3. Impact of Data Quality and Quantity: There is limited discussion on how data quality and the quantity of training data affect the models performance. Insights into these aspects would have added depth to the evaluation. 4. Realworld Applicability Feedback: While the paper presents impressive results in a controlled setting it lacks an examination of the models performance in realworld applications such as in noisy environments or for speakers with significant variances in speech patterns.  Review Summary The paper presents ClsVC a promising new voice conversion framework that efficiently separates speaker and content information using a single encoder setting new standards in speech conversion quality. The experimental results convincingly demonstrate its superiority over existing methodologies backed by a rigorous mathematical foundation. Nevertheless the paper could have been strengthened by addressing the computational demands of the model explicitly discussing its performance across languages beyond English and examining the impact of training data characteristics on the model. Furthermore a deeper exploration into its realworld applicability would make a compelling case for ClsVCs practical utility. These aspects while not detracting from the novelty and current achievements of the proposed method suggest directions for future work and exploration. Overall ClsVC represents a significant advancement in voice conversion technology offering a simpler yet more effective framework for achieving highquality voice conversion. The approach is not only theoretically sound but also empirically validated setting a benchmark for future research in the field. Despite the minor weaknesses identified the papers contributions are substantial warranting recommendation for publication contingent on addressing the mentioned concerns to further solidify its findings and implications.", "xbu1tzbjvd": " Peer Review of \"DYNAMO: Dynamical Model Embeddings for Interpreting Deep Neural Networks\"  1. Summary The paper introduces DYNAMO an innovative algorithm designed to interpret and analyze deep neural networks by mapping networks onto a lowdimensional manifold. Each point on this manifold represents a neural network and proximity between points signifies similarities in the highlevel computational processes they perform. This approach facilitates the identification of common computational strategies across different networks even when these networks have varied architectures or are trained with different parameters. Notable applications of DYNAMO include clustering of neural networks based on their computational processes averaging models for improved task performance and facilitating semisupervised learning through optimization on the model embedding space. The paper includes empirical results showcasing the effectiveness of DYNAMO in understanding the commonalities and differences in the computational processes across a range of recurrent neural networks (RNNs) and convolutional neural networks (CNNs).  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovation: The idea of creating a manifold that represents neural networks based on their computational processes is novel and intriguing. It offers a promising avenue for understanding deep learning models beyond traditional methods.  Broad Applicability: DYNAMO is shown to be applicable to different network types (RNNs and CNNs) indicating its versatility and potential for widespread adoption in the study of neural networks.  Insightful Empirical Results: The paper presents comprehensive experiments that not only validate the effectiveness of DYNAMO but also offer new insights into the computational dynamics of neural networks. Weak Points:  Lack of Comparison: There is minimal comparison with existing neural network interpretation methodologies such as knowledge distillation leaving the reader uncertain of DYNAMOs advantages or improvements over these methods.  Complexity and Accessibility: The mathematical and conceptual density of the paper might make it challenging for a broader audience to grasp the nuances of the proposed method. Simplifying the exposition or providing more intuitive examples could enhance accessibility.  Scalability Concerns: While the paper discusses the application of DYNAMO to specific cases it does not thoroughly address scalability issues or computational costs which are critical for evaluating the feasibility of applying the algorithm to largescale networks. Questions and Feedback: 1. How does DYNAMO perform compared to traditional neural network interpretation methods A detailed comparative analysis would strengthen the claim of DYNAMOs novelty and effectiveness. 2. Can the authors elaborate on the scalability of DYNAMO particularly regarding computational costs and memory requirements when applied to largescale neural networks 3. The paper would benefit from a section discussing the potential limitations of DYNAMO and strategies for mitigating these limitations.  3. Review Summary The paper introduces DYNAMO a novel and promising approach for interpreting deep neural networks by constructing a manifold that represents networks based on their computational processes. The applications demonstrated including clustering model averaging and facilitating semisupervised learning showcase the algorithms potential to provide new insights into neural network dynamics. The empirical results are compelling and serve as strong evidence of DYNAMOs efficacy. However the paper could be improved by addressing certain weaknesses including providing more direct comparisons with existing interpretation techniques simplifying the presentation to reach a broader audience and discussing scalability in more detail. Addressing these points would strengthen the papers contributions and make a more convincing case for DYNAMOs significance in the field of neural network interpretation. Overall the paper presents a significant advancement in the interpretability of neural networks offering a novel framework that promises to provide deeper insights into the computational strategies of diverse neural network architectures.", "qTTccuW4dja": "Summarize the Paper: The paper introduces Arithmetic coding and kd trees for Language (AriEL) a novel method for efficiently encoding variablelength sequences such as sentences into continuous space volumes for improved retrieval and language generation from random sampling. This approach bridges the gap between traditional variational autoencoders that map inputs to points and struggle with balancing prediction accuracy and smooth representation and the need for a more effective use of latent space. AriEL utilizes arithmetic coding combined with kd trees to partition the latent space into volumes based on the prevalence of sequences thus allowing for more efficient sampling and retrieval of learned patterns without additional loss terms. The method is evaluated through experiments on a toy grammar dataset and a human dialogue dataset with comparisons made to standard deep learning models like autoencoders and Transformers demonstrating AriELs superior ability in randomly generating a wider variety of correct language patterns. The study underscores the potential of volume encoding in enhancing information retrieval and generalization capabilities of language models. Strong Points: 1. Novelty of Approach: AriEL introduces an innovative method of combining arithmetic coding with kd trees to encode sequences into volumes rather than points addressing the limitations of existing models in efficient retrieval and generalization from latent space. 2. Comprehensive Evaluation: The paper conducts extensive experiments on both a constructed toy grammar and a real human dialogue dataset providing a thorough analysis of AriELs performance in comparison to wellestablished models. The inclusion of GPT2 within AriEL for evaluation further strengthens the validity of the proposed method. 3. Improved Random Sampling: Unlike traditional methods that struggle with producing a diverse range of correct sentences AriELs unique approach to encoding and decoding demonstrates significant improvements in generating varied and grammatically correct sentences through random sampling. Weak Points: 1. Complexity and Scalability: The paper could further discuss the computational complexity and scalability of AriEL especially in comparison to the models it is benchmarked against. Details on runtime and resource requirements would provide a more comprehensive understanding of its practical applications. 2. Detailed Analysis on Failures: While the paper presents success metrics a more indepth analysis of the instances where AriEL fails or underperforms would offer valuable insights into its limitations and areas for improvement. 3. Comparative Analysis: The comparison primarily focuses on the quantity and diversity of generated language. A deeper dive into the qualitative differences including the semantic coherence and contextual relevance of the generated sentences across models would enrich the evaluation. Questions and Feedback to the Author: 1. Can the authors elaborate on the computational complexity of AriEL especially in comparison to the baseline models used in this study Understanding the tradeoffs between performance and computational costs is crucial for practical adoption. 2. The paper would benefit from a more detailed discussion on instances where AriEL does not perform as expected. What are the common characteristics of these instances and how might they inform future improvements 3. How does AriEL perform in scenarios with highly contextdependent language patterns such as sarcasm or idiomatic expressions Exploring these aspects could illuminate the models ability to handle complex language nuances. Review Summary: The presented paper offers an innovative method AriEL for improving language generation and retrieval through the novel use of volume encoding in the latent space. By leveraging arithmetic coding and kd trees AriEL demonstrates superior performance in generating a wide variety of grammatically correct sentences through random sampling compared to traditional deep learning approaches. The comprehensive evaluation across different datasets including the novel integration with GPT2 showcases the methods effectiveness and the potential benefits of adopting volume codes for language models. However the paper would benefit from a deeper exploration into the computational demands a more nuanced analysis of performance limitations and a qualitative comparison of the generated language. Overall AriEL presents a promising direction for future research in efficient language processing and generation suggesting a fertile ground for further exploration and refinement.", "tvwNdOKhuF5": " Peer Review on the Paper: Enhancing FPS Game Performance with a MultiStage Learning Framework  1. Summary of the Paper This paper introduces a sophisticated multistage learning framework aimed at improving agent performance in FirstPerson Shooter (FPS) games specifically within the ViZDoom environment. The authors propose an approach that leverages general reinforcement learning (RL) techniques instead of relying on larger neural networks more precise fragging skills or manually engineered strategies. Key components of this approach include Hindsight Experience Replay (HER) for better navigation Hindsight Proximal Policy Optimization (HPPO) RuleGuided Policy Search (RGPS) Prioritized Fictitious SelfPlay (PFSP) and Diversified Strategic Control (DSC). The resulting agent termed the DSCAgent demonstrates superior performance over topranked agents from previous ViZDoom AI Competitions by employing diversified and humanlike strategies. Through thorough analysis and experiments the paper shows the effectiveness of each component in the framework and argues for its applicability not only in FPS games but also in other gaming environments and realworld applications that require spatial navigation and diversified behavior.  2. Strengths and Weaknesses Strengths:  Innovative Approach: The paper presents a novel multistage learning framework that intelligently combines several advanced RL techniques setting a new benchmark in FPS game performance.  Comprehensive Analysis: A detailed examination of each component\u2019s contribution to the overall performance provides clarity on the frameworks complexity and its operational mechanism.  Realworld Applicability: The authors convincingly argue how the approaches used can be extended beyond gaming to realworld applications enhancing the papers relevance and impact. Weaknesses:  Complexity for Replication: The multistage framework while effective is complex. For future researchers or practitioners wanting to replicate or build upon this work the intricacies involved might pose significant challenges.  Limited Comparative Analysis: While the paper showcases superior performance against past competitors a more extensive comparison with contemporary RL approaches could provide a clearer picture of the frameworks standing in the current research landscape.  Potential Overfitting Concerns: With the DSCAgent performing significantly better than previous agents theres a lack of discussion on whether the approach could lead to overfitting especially when faced with unpredictable gameplay scenarios not covered in the training set. Questions and Feedback to the Author: 1. Could the authors discuss potential overfitting issues and how they can be mitigated within the proposed multistage framework 2. Is there a possibility for simplifying the framework without significantly compromising performance to aid replication and adoption by the broader research community 3. How does the framework perform against human players and what are the implications of these results for the development of AI in the real world  3. Review Summary The paper presents a groundbreaking approach to improving AI agent performance in FPS games relying on a sophisticated mix of RL techniques within a multistage learning framework. The introduction of HPPO alongside techniques like HER for navigation and RGPS represents a significant step forward in the development of AI strategies for complex dynamic environments like FPS games. The comprehensive analysis and the demonstration of superior performance in the ViZDoom environment underscore the papers contributions to the field. However the complexity of the approach might hinder its broader application and replication. Further comparative analysis with current RL methods and a discussion on the potential for overfitting would enhance the papers credibility and applicability. Overall the paper sets a new standard in FPS AI performance and opens up intriguing possibilities for applying these advanced RL techniques to both other gaming contexts and realworld scenarios. The authors are encouraged to address the highlighted weaknesses and consider the posed questions and feedback to strengthen the papers impact and applicability further.", "rS9t6WH34p": " Peer Review for \"ObSuRF: ObjectBased Superposition of Radiance Fields for Unsupervised 3D Scene Decomposition\"  1) Summary of the Paper This paper introduces ObSuRF a novel method for decomposing a 3D scene from a single input image into separate object components using Neural Radiance Fields (NeRF). Rather than modeling the whole scene holistically ObSuRF treats each object in the scene as an individual NeRF allowing for more granular control and understanding of the scenes composition. This is achieved through an encoder network that outputs a set of latent vectors each corresponding to an object within the scene. These vectors then condition a set of NeRF decoders that independently generate the geometry and appearance for each object. The authors propose a novel training strategy featuring an RGBD based loss that bypasses the computationally expensive ray marching process typical of NeRF models. They demonstrate the effectiveness of ObSuRF on segmentation tasks in 2D image space and extend its application to 3D scenarios showcasing its capacity to handle complex geometries and appearances without explicit supervision for individual object identification or geometry.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper offers an innovative method for decomposing 3D scenes into objectbased components using NeRF. This approach presents a significant advancement in both unsupervised image segmentation and 3D scene understanding.  Efficiency Gains: By leveraging a novel training objective that uses RGBD data for supervision ObSuRF avoids the intensive computational demands of traditional ray marching making it notably more efficient.  Broad Implications: The capacity of ObSuRF to infer compact objectspecific representations has farreaching implications for various AI and robotics applications including dynamics modeling robot grasping and visual question answering. Weak Points:  Generalization to RealWorld Data: While ObSuRF shows impressive results on synthetic datasets its application and performance on realworld data remain unclear especially considering robustness against noisy depth information.  Model Complexity: Despite its computational efficiency improvements the methodologys complexity particularly in the encoderdecoder architecture and the latent vector conditioning mechanism could pose challenges in deployment or further development without a comprehensive understanding.  Evaluation Depth: Although the model is evaluated on novel 3D benchmarks comparisons with a broader range of existing approaches in both 2D and 3D segmentation could provide a clearer perspective on its relative performance and advancements over stateoftheart methods. Questions and Feedback: 1. Generalization Capability: How does the model perform on realworld datasets especially in scenarios with occlusions lighting variations or complex textures Is there a plan to test and adapt ObSuRF for these conditions 2. Scalability: The paper discusses the models performance on scenes with a relatively small number of objects. How does ObSuRF scale to scenes with significantly higher complexity and object count 3. Integration with Downstream Tasks: Can the authors provide more insights or preliminary results on integrating ObSuRFs objectspecific representations into downstream tasks such as dynamic modeling or robotics applications  3) Review Summary and Recommendation ObSuRF represents a significant step forward in unsupervised 3D scene decomposition offering innovative solutions to segment scenes into individual object components effectively. The proposed models efficiency facilitated by a novel training objective that uses RGBD data marks a notable advancement in leveraging NeRF for complex scene understanding without requiring explicit geometrical or identity supervision. However the models performance and adaptability on realworld data its scalability and its deployment in downstream applications remain areas for further exploration and development. Additionally expanding the evaluation to include a wider range of benchmarks and comparisons with existing methods could strengthen the papers contributions and claims further. Given the strong foundational approach ObSuRF presents coupled with the identified areas for enhancement and exploration I recommend this paper for publication with minor revisions. The authors are encouraged to address the feasibility of realworld application model scalability and broader evaluation to solidify ObSuRFs standing as a significant contribution to the field of 3D scene understanding and segmentation.", "vA7doMdgi75": "Review for \"Robust Subspace Recovery Without Knowledge of the True Subspace Codimension\" 1) Summary of the paper: This paper tackles the problem of robust subspace recovery (RSR) specifically focusing on recovering a subspace from data corrupted by outliers without prior knowledge of the subspaces dimension. The authors build on the Dual Principal Component Pursuit (DPCP) method which has been shown to perform well in highdimensional settings with a known subspace dimension. The core contribution is a proposed algorithm that extends DPCP to handle unknown subspace dimensions by relaxing orthogonality constraints and employing random initializations. The method involves running multiple instances of a projected subgradient descent method (PSGM) aimed at finding vectors in the null space of the subspace thereby recovering the subspaces orthogonal complement without prespecifying its dimension. The paper theoretically demonstrates that under mild conditions this approach can recover the correct subspace with high probability and provides empirical evidence supporting these claims. 2) Strong and weak points Questions and Feedback: Strong Points:  The paper addresses a significant gap in the literature by proposing a method for RSR in highdimensional settings without requiring a priori knowledge of the subspaces dimension a common limitation in existing approaches.  The theoretical analysis is comprehensive providing clear conditions under which the proposed method succeeds and showing how it implicitly regularizes the rank of the solution contributing to understanding the behavior of the PSGM algorithm in RSR problems.  Empirical results on simulated and real datasets demonstrate the practical effectiveness of the approach especially its robustness and capacity to correctly estimate the subspace codimension. Weak Points:  The discussion on related work could be expanded to more thoroughly situate this study within the current research landscape highlighting how this approach compares to or improves upon existing methods in more direct terms.  While empirical results are promising the paper would benefit from a broader range of datasets including more challenging realworld scenarios to further validate the approachs effectiveness and its generalizability.  The theoretical conditions presented could be accompanied by more intuitive explanations or examples to make the paper more accessible to readers not deeply familiar with the technical aspects of RSR or optimization theory. Questions:  Can the authors elaborate on the potential impact of outlier distribution on the algorithms performance Specifically how robust is the algorithm to outliers that are not uniformly distributed in the ambient space  How does the computational complexity of the proposed method compare to existing approaches and what implications does this have for its applicability to largescale datasets  Is there a theoretical or empirical limit to the dimensionality of the subspace and the ambient space for which this method remains effective Feedback:  It would be insightful to include a more detailed discussion on the implications of relaxing orthogonality constraints specifically regarding how this affects the solution space and convergence of the algorithm.  Considering further investigation on the choice of parameters for the PSGM algorithm specifically how different initialization strategies or stepsize rules might impact the performance could provide valuable insights for practitioners.  A comparison with stateoftheart methods on a benchmark set of datasets would strengthen the empirical section offering readers a clearer perspective on the proposed methods competitive advantages and potential limitations. 3) Review Summary: The paper presents an innovative approach to robust subspace recovery that does not require knowledge of the subspaces dimension addressing a significant limitation in existing methods. The proposed algorithm based on an extension of the DPCP method with relaxed orthogonality constraints and randomized initialization is theoretically sound and empirically promising. However the impact of this work could be amplified through a more exhaustive comparison with related work a detailed examination of computational complexity and further empirical validation across diverse datasets. Additionally enhancing the papers accessibility through intuitive explanations of the theoretical results and more detailed discussions on the algorithms design choices would be beneficial. Overall the paper makes a valuable contribution to the field and with refinements it has the potential to significantly advance the stateoftheart in robust subspace recovery.", "mFpP0THYeaX": " Summarize the Paper The paper introduces a novel approach to unsupervised domain adaptation called GIFT (Gradual Interpolation of Features toward Target) which addresses the challenge of adapting a model from a source to a target distribution especially when direct samples from intermediate distributions between source and target are unavailable or unannotated. The paper builds on the premise that for effective domain adaptation through selftraining access to such intermediate samples can significantly lower the generalization gap. In scenarios where such samples are not available GIFT proposes creating virtual samples by interpolating between representations from the source and target distributions thereby simulating a gradual domain adaptation process. The method was evaluated against other selftraining approaches showing its advantages in settings with synthetic and natural distribution shifts. The experiments prove that GIFT by applying this progressive interpolation aids in achieving better adaptation by guiding the model through a curriculum of gradually shifting samples towards the target distribution.  Strong and Weak Points Questions and Feedback  Strong Points: 1. Innovative Approach: The paper presents a novel and logically sound method of addressing domain adaptation through the lens of curriculum learning which is both innovative and relevant to current challenges in machine learning. 2. Empirical Support: The thorough experimentation including benchmarks with synthetic and natural distribution shifts provides robust evidence in support of GIFT\u2019s efficacy over traditional selftraining methods when intermediate samples are not readily accessible. 3. Practical Relevance: By focusing on a realworld problem where samples from intermediate distributions might not always be annotated or even available the research offers a practical solution that could be widely applicable across various domain adaptation scenarios.  Weak Points: 1. Limited Scope: The paper primarily focuses on image classification tasks. While this provides a clear framework for evaluation it also limits the understanding of how GIFT might perform across other data modalities (e.g. text audio). 2. Interpolation Mechanics: While the concept of interpolating between source and target representations is captivating the paper could benefit from a deeper exploration into the impact of different interpolation techniques or functions beyond linear interpolation. 3. Scalability and Computational Cost: There is limited discussion on the computational costs or scalability of applying GIFT especially when dealing with largescale datasets or more complex models.  Questions and Feedback: 1. Could the authors elaborate on how different interpolation techniques might affect the efficacy of GIFT It would be interesting to see if alternative approaches could further optimize the interpolation process or offer advantages in specific adaptation scenarios. 2. How do the authors envision extending GIFT to nonimage data modalities Given the promising results in image classification insight into potential challenges and considerations for applying GIFT to other types of data would be valuable. 3. Could the paper provide more detail on the computational implications of implementing GIFT especially in resourceconstrained environments Information on the computational overhead and potential optimization strategies would be beneficial for practitioners evaluating the feasibility of deploying GIFT in various settings.  Review Summary The paper proposes a compelling and innovative method for unsupervised domain adaptation addressing a significant challenge in machine learning by introducing GIFT. Through wellstructured experiments and logical argumentation it convincingly demonstrates that GIFT can outperform traditional selftraining approaches in scenarios lacking intermediate samples between source and target distributions. The novel application of curriculum learning through virtual sample interpolation presents a meaningful advancement in domain adaptation strategies. However to strengthen the paper further addressing the outlined weak points and considering the posed questions could broaden the researchs implications and applicability. Expanding the scope beyond image classification exploring different interpolation techniques and discussing the practical considerations of scalability and computational cost in more depth would greatly enhance the papers contribution. Overall the paper presents a valuable addition to the field of unsupervised domain adaptation. GIFTs methodology is sound and its empirical validation is robust marking a significant step forward in addressing the challenges of domain adaptation when direct intermediate samples are not available.", "sTkY-RVYBz": " PeerReview:  1) Summarize the paper The paper examines the impact of Batch Normalization (BN) on deep neural network models specifically focusing on its effect on outofdomain (OOD) generalization. The authors present evidence that while BN accelerates convergence and increases indomain accuracy it causes models to rely on lowvariance frequent features that do not generalize well to OOD examples. To address this they propose the Counterbalancing Teacher (CT) method a robust representation learning framework that uses a BNfree version of the network as a teacher to regularize the student network through a consistency loss function. The CT method is shown to enhance performance on standard robustness benchmarks (CIFAR10C CIFAR100C and VLCS) without relying on target domain information or additional data augmentation. Theoretical insights in an overparameterized linear regression context explain why BN leads to dependency on frequent lowvariance features and empirical results demonstrate the effectiveness of the CT method over existing approaches.  2) Strong and Weak Points Questions and Feedback Strong Points:  The investigation into BNs downside is a valuable contribution addressing an overlooked aspect of a widely used technique in deep learning.  The Counterbalancing Teacher (CT) method is innovative leveraging insights from knowledge distillation literature to improve OOD generalization without additional data or domain information.  Comprehensive empirical evaluation is conducted across different datasets and architectures presenting a strong case for the effectiveness of CT over existing methods. Weak Points:  The theoretical justification while insightful is limited to an overparameterized linear regression setting which might not encompass the complexity of networks used in practice.  It is unclear how well the CT method scales to very large datasets or more complex models beyond the ones tested.  The paper could benefit from a more detailed discussion on the impact of varying the \u03bb parameter in the CT methods loss function and guidelines on setting it for new tasks. Questions:  Can the authors provide insights on the computational overhead introduced by training and maintaining a separate teacher model in the CT framework  How does the CT method perform in cases of extreme domain shift or with datasets that inherently contain highvariance features  Is there any empirical evidence on how the CT method impacts model interpretability given its altered reliance on input features Feedback:  Expanding the theoretical analysis beyond linear regression could strengthen the foundation of the proposed CT method.  Including a sensitivity analysis on the \u03bb parameter in the CT method would provide practical guidance for deploying the approach in different scenarios.  Consider exploring the scalability of the CT method to larger models and datasets as well as its interaction with other normalization techniques beyond BN.  3) Review Summary The paper introduces an important critique of Batch Normalizations effect on outofdomain generalization and proposes a compelling solution through the Counterbalancing Teacher (CT) method. The innovative approach and comprehensive empirical validation make a significant contribution to the field. However the work could be further strengthened by extending the theoretical analysis beyond linear models examining the computational efficiency and scalability of the CT method and providing practical guidelines for its implementation. Despite these suggestions for enhancement the paper presents a valuable perspective on the tradeoffs associated with BN and offers a novel framework for improving robustness in deep learning models.", "xa6otUDdP2W": "Review for \"Scheduled GrowandPrune Method for Training Deep Neural Networks\" 1) Summary: The paper introduces a novel technique for model sparsification named Scheduled GrowandPrune (GaP) aiming to reduce the computational and memory requirements of Deep Neural Networks (DNNs) without sacrificing model quality. Unlike traditional sparsification methods that either prune from dense models or fix sparse masks early in the training the GaP methodology cyclically or in parallel grows partitions of a model to a dense state and then prunes them after some training epochs. This process does not require a pretrained dense model thus saving memory. The authors demonstrate that the proposed method outperforms stateoftheart sparsification techniques achieving higher or comparable model quality at significant levels of sparsity across various tasks (image classification object detection 3D object part segmentation and translation). They also provide theoretical justification for the convergence of their method and share their code for further research exploration. 2) Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses a significant problem in the deep learning community i.e. the high computational and memory costs of DNNs by introducing an innovative and effective method for model sparsification.  The proposed methods ability to outperform stateoftheart sparsification techniques in various tasks without requiring a pretrained dense model is a crucial advancement.  Providing theoretical justification for the convergence properties of the GaP method adds credibility to the empirical findings.  The inclusion of code makes the method reproducible and facilitates further research and practical applications. Weak Points:  The paper does not thoroughly discuss the limitations or potential drawbacks of the GaP methodology. Understanding the conditions under which the method might underperform or fail is crucial for researchers and practitioners.  The hyperparameter tuning process is not extensively covered. Insights into how the partition numbers partition strategies and other significant hyperparameters were chosen could be beneficial for implementing the method in new contexts.  There is a lack of comparison with other nonpruningbased model reduction techniques such as knowledge distillation or quantization. A nuanced understanding of how GaP stands in the broader landscape of model compression could provide an added layer of depth to the analysis. Questions and Feedback:  Could the authors elaborate on how the GaP method performs in scenarios with limited data availability Is there a risk of overfitting during the grow phases  How does the choice of hyperparameters (e.g. partition number and growprune scheduling) affect the models final sparsity and quality Would an automated hyperparameter optimization approach be feasible here  It would be informative to see more analysis on the impact of the method on the inference speed and power consumption on actual hardware platforms given that different sparsity levels and patterns can significantly vary in their practical benefits. 3) Review Summary: This paper presents a wellthoughtout and meaningful contribution to the field of neural network sparsification and optimization. The proposed Scheduled GrowandPrune methodology addresses fundamental challenges in existing sparsification techniques by eliminating the need for pretrained dense models and introducing a flexible and robust mechanism for achieving highquality sparse models. The empirical results across a range of tasks are impressive significantly when the GaP method surpasses dense models performance at 80 sparsity levels. Furthermore providing theoretical convergence guarantees enriches the papers value. However to strengthen the paper the authors should consider discussing the potential limitations and failure modes of their method more comprehensively. An expanded discussion on hyperparameter tuning and insights into decisionmaking regarding the partitioning strategies would enhance the papers practical applicability. Lastly a broader comparison with other model reduction techniques beyond pruning would provide readers with a clearer understanding of where GaP stands in the model compression landscape. Based on the significant contributions and the potential impact of the Scheduled GrowandPrune methodology I recommend the acceptance of this paper provided that the authors address the minor concerns and suggestions outlined above.", "pz1euXohm4H": " PeerReview of \"Augmenting TargetSide Inputs in Autoregressive Sequence Generation\"  1) Summary This paper addresses the underexplored area of targetside data augmentation in autoregressive (AR) sequence generation tasks which is an integral part of machine learning and natural language processing. Traditionally data augmentation methods have focused on manipulating the source input to generate additional training examples. However the proposed method Soft Sequencelevel Targetside Inputs Argumentation (SSTIA) innovatively enhances the targetside input by generating soft pseudotoken sequences. These sequences are created by multiplying decoder output probability distributions with target token embeddings and further \"mixedup\" with original ground truth tokens to generate synthetic targetside sequences for training. The approach does not demand extra labeled data or introduce new model parameters during inference maintaining efficiency. Experiments conducted across several sequence generation tasks\u2014dialog generation machine translation and abstractive summarization\u2014show that the proposed method significantly outperforms strong baselines.  2) Strong and Weak Points Question and Feedback to the Author Strong Points: 1. Originality of Research Focus: The paper delves into the relatively unexplored area of targetside augmentation in AR sequence generation presenting an innovative approach to improve sequence generation tasks without extra data or computational costs during inference. 2. Sound Methodology: The methodology for creating soft pseudotokens and leveraging mixup for targetside augmentation is both novel and thoroughly described providing clarity on the proposed enhancement mechanism. 3. Comprehensive Experiments: The authors conducted extensive experiments across multiple datasets and tasks providing a robust validation of their methods effectiveness compared to strong baselines and previous data augmentation techniques. Weak Points: 1. Lack of Theoretical Justification: The paper would benefit from a deeper theoretical discussion on why mixing soft pseudotokens with ground truth targets yields better training outcomes. A theoretical framework or analysis could strengthen the papers foundations. 2. Comparison with Alternative Targetside Augmentation Methods: While the paper states that targetside augmentation is an underexplored area some exploration of or comparison with other potential methods\u2014even if they are not directly related to sequence generation\u2014would provide context and justify the novel approach more solidly. 3. Details on Hyperparameter Selection: More insights into the selection process for critical hyperparameters such as the softmax temperature and mixup ratio along with their impact on different datasets would add depth to the methodological understanding. Question and Feedback: Could the authors detail potential implications or adjustments of the SSTIA method for nonautoregressive (NAR) sequence generation tasks Since NAR generation presents a different set of challenges understanding how the proposed method might adapt or influence such paradigms would be beneficial for future research directions.  3) Review Summary The paper \"Augmenting TargetSide Inputs in Autoregressive Sequence Generation\" presents an intriguing exploration into enhancing targetside inputs for sequence generation tasks. The proposed SSTIA method focusing on generating soft pseudotokens through mixing decoder output probability distributions with target token embeddings stands out for its novelty and the lack of additional inference costs. Empirical evidence across multiple tasks and datasets demonstrates the methods efficacy and potential to improve sequence generation quality significantly. However the research could be intensified with a deeper theoretical rationale behind the effectiveness of the proposed targetside augmentation method and a broader comparison with other augmentation techniques including those outside the AR generation spectrum. Additionally a more detailed discussion on hyperparameter optimization and its variance across tasks or datasets would enrich the papers methodological exposition. Overall this paper contributes meaningfully to the field of machine learning and natural language processing by highlighting a previously underexplored aspect of data augmentation. Its findings could pave the way for further research into targetside data augmentation and its application to other sequence generation frameworks including NAR tasks. I recommend publication provided the authors address the outlined areas for improvement thus enhancing the paper\u2019s contribution and clarity.", "mk0HzdqY7i1": " 1) Summary of the Paper This paper aims to evaluate the reproducibility of results for combinatorial optimization problems focusing on the MAXIMUM INDEPENDENT SET (MIS) problem utilizing both traditional and deep learningbased approaches. The authors introduced an opensource benchmark suite for evaluating MIS solvers  traditional algorithmic and machine learningbased performed an indepth analysis of the guided tree search algorithm by Li et al. reimplemented for better code quality and compared various solvers\u2019 performance. Their findings highlight that the graph convolutional network in the guided tree search algorithm does not significantly contribute to solving the MIS problem beyond what traditional algorithmic techniques offer. They also show that classical solvers often outperform deep learningbased solvers both in speed and solution quality except for a reinforcement learning solver which showed promise.  2) Strong and Weak Points Questions and Feedback Strong Points: 1. Comprehensive and Reproducible Benchmarking Suite: The introduction of an opensource unified benchmark suite is a substantial contribution allowing for fair and reproducible comparisons across different solvers. 2. Indepth Analysis: The detailed evaluation of various configurations of the guided tree search algorithm and comprehensive comparisons to classical algorithmic solvers and a reinforcement learning solver provide valuable insights. 3. Transparency and Reproducibility: Efforts to reproduce results and the transparent discussion of issues encountered as well as attempts to contact original authors add to the papers credibility. Weak Points: 1. Limited Scope of Comparisons: The vast majority of the paper focuses on the INTELTREESEARCH and variations thereof. Expanding the scope to include more diverse machine learning approaches might offer a broader understanding of the domain. 2. Dependence on Graph Reduction and Local Search Techniques: The strong performance of certain configurations heavily relies on external algorithmic techniques. This may imply that the success of ML models in this case comes more from traditional optimization methods rather than learned strategies which could be clearer in the discussion. 3. Performance Metrics: While solution quality and speed are crucial evaluating the models scalability memory usage and energy efficiency might present a fuller picture of their practical applicability. Questions and Feedback: 1. Impact of Problem Size on ML Models: How do the size and complexity of the graph instances affect the performance of machine learning models particularly those based on reinforcement learning A deeper analysis here could be insightful. 2. Generalizability of Results: To what extent can the findings of this paper generalize to other NPhard combinatorial optimization problems It would be beneficial to discuss the potential implications for problems beyond the MIS. 3. Future Directions: The paper touches upon the promise shown by reinforcement learning solvers. Could the authors elaborate on how future research might enhance these models effectiveness and efficiency or explore other machine learning paradigms  3) Review Summary and Recommendation This paper makes a notable contribution to the field of combinatorial optimization especially in creating a reproducible and fair comparison framework. Its thorough analysis uncovers critical insights notably the limited value of graph convolution networks in this context and the comparative effectiveness of traditional optimization methods. However the paper could benefit from a broader comparison across a more diverse set of machine learning models and further exploration of scalability generalization and future directions for incorporating machine learning in combinatorial optimization. Considering both its significant contributions and areas for improvement I recommend accepting this paper with minor revisions. Addressing the weak points and considering the feedback provided could enhance the papers impact and broader relevance to the field.", "sOK-zS6WHB": " Peer Review on \"Model Fingerprinting for Responsible Disclosure of Generative Models\"  1) Paper Summary The paper presents a novel approach towards responsible disclosure of generative models specifically GANs by embedding unique fingerprints in generated data enabling effective detection and attribution of generated samples back to their source models. The authors introduce a mechanism that efficiently generates a large population of generator instances each with a distinct fingerprint without significant compromise on generation quality. This method aims to address the growing concern of misuse in deep fakes and misinformation providing a responsible model release framework that traces back misused data to its source. The approach achieves almost perfect detection accuracy of fingerprints and demonstrates robustness against image perturbations making it a significant step forward in mitigating the risks associated with generative model misuse.  2) Strong and Weak Points Question and Feedback to the Author Strong Points:  Novelty and Relevance: The paper tackles an increasingly crucial issue with the wide availability and capabilities of generative models. The novel fingerprinting mechanism is a proactive step towards responsible disclosure setting it apart from reactive deep fake detection methods.  Effectiveness and Scalability: The method achieves high detection accuracy with an impressive capacity (1038 identifiable models) and scalability demonstrating practical viability for wideranging applications.  Comprehensive Validation: The paper thoroughly evaluates the mechanism across various datasets and scenarios including robustness and immunizability against perturbations which strengthens the claims made. Weak Points:  Potential for Evasion: While the paper addresses robustness and secrecy theres less focus on potential adversarial attacks designed explicitly against fingerprint detection. A dedicated section discussing and evaluating the resilience of fingerprints against sophisticated modelbased reverse engineering or evasion techniques would be valuable.  Limited Discussion on Ethical Considerations: The paper rightly touches on responsible disclosure but could expand on the ethical implications of fingerprinting especially concerning privacy consent and the potential for misuse in surveillance or censorship.  Generalization Beyond GANs: The paper focuses primarily on GANs. It would be beneficial to discuss how the proposed fingerprinting mechanism could be generalized or adapted to other forms of generative models enhancing its applicability. Questions and Feedback:  Could the authors elaborate on potential countermeasures against sophisticated adversarial attacks aimed at obfuscating or removing embedded fingerprints  Given the mechanisms reliance on the unique generation of fingerprints during training is there a mechanism in place to prevent collisions or ensure the uniqueness of fingerprints across a large scale of models  How might the embedding of fingerprints affect the legal and ethical aspects of generative model usage particularly concerning digital rights management and user privacy  Feedback: It would enhance the paper to include a deeper discussion on the broader implications of model fingerprinting exploring both the potential societal benefits and risks alongside technical solutions to mitigate ethical concerns.  3) Review Summary The paper introduces a crucial and timely solution to a growing concern within the AI community around the misuse of generative models. Its innovative approach to embedding detectable and attributable fingerprints within generated data marks a significant advancement in responsible model disclosure. The methods effectiveness scalability and thorough validation underscore its potential utility across various applications. However the paper could benefit from a deeper exploration into the defense against deliberate adversarial evasion techniques ensuring the robustness of the fingerprinting mechanism. Further expanding the discussion on ethical considerations and broader applicability beyond GANs would strengthen its contribution to the field. In conclusion the paper presents a compelling solution with significant practical implications for the responsible use of generative models. With further exploration into its limitations and broader impacts this work could set a foundation for ethical standards in AI model disclosure and usage paving the way for more secure and accountable generative technologies.", "htWIlvDcY8": " Peer Review of \"FALCON: Fast Learning of Novel Visual Concepts\"  1) Summarize the paper The paper introduces FALCON a metalearning framework designed for quick learning of new visual concepts from minimal examples utilizing a multistream data approach including image viewing sentence descriptions and supplemental relations with known concepts. Concepts are modelled as box embeddings in a highdimensional space facilitating the representation of visual aspects like colors and shapes with their geometric relationships. Through neurosymbolic learning FALCON interprets natural language descriptions and derives an optimal box embedding for new concepts based on visual appearance and conceptual relations. The models effectiveness and superior performance in learning accuracy and efficiency are demonstrated through evaluations on synthetic and realworld datasets providing a significant advancement in the field of visual concept learning and reasoning.  2) Strong and Weak Points Questions and Feedback  Strong Points:  Innovative Concept Representation: The use of box embeddings to represent visual concepts is novel and intuitively aligns with how concepts encompass objects with varying properties in a geometric sense.  Comprehensive Learning Framework: The integration of image interpretation sentence reading and conceptual relating in a unified framework mimics human learning processes closely presenting a holistic approach to concept learning.  Improved Performance: FALCONs performance on synthetic and realworld datasets particularly its superior learning efficiency and accuracy over existing baselines underscores the models practical effectiveness and potential applicability.  Weak Points:  Generalization Concerns: While promising the papers focus on specific datasets raises questions about the models generalizability to a wider array of visual contexts and more complex or abstract concepts.  Computational Demands: The paper does not deeply discuss the computational overhead or scalability of FALCON especially when dealing with a large number of novel concepts or high visual complexity.  Lack of RealWorld Testing: Although realworld datasets are utilized the paper could benefit from a more extensive analysis or case study on actual applications giving insights into the models performance in practical scenarios.  Questions:  How does FALCON handle ambiguous concepts or images with unclear context that may lead to misinterpretation of the concept  Are there limitations to the types of relationships or properties that can be modelled with the box embedding approach  Feedback:  Greater emphasis on FALCONs generalizability across diverse visual and conceptual domains would bolster its appeal and application scope.  A more detailed examination of the computational demands possibly benchmarking against other models could provide insights into the feasibility of deploying FALCON in resourceconstrained settings.  Future work could focus on extending the models applicability to abstract concepts not directly observable in visual data opening avenues for broader utility in AI systems.  3) Review Summary The paper presents FALCON a significant advancement in the field of fast visual concept learning through a metalearning framework that excellently integrates multiple data streams for learning novel concepts. Its utilization of box embeddings for concept representation is both innovative and apt demonstrating enhanced learning efficiency and superior performance on benchmark datasets. However the model could benefit from further exploration regarding its generalizability computational demands and applications to more abstract concepts. Addressing these aspects could amplify FALCONs utility and applicability in realworld scenarios marking its potential as a cornerstone in visual concept learning research. The recommendations for future work if heeded could substantially augment the models relevance and efficacy in broader AI applications.", "u7PVCewFya": " Peer Review for \"Improving Differentially Private Deep Learning with Tailored Loss Function\"  1. Summary of the Paper This paper investigates the impact of the loss function on the performance of Differentially Private Stochastic Gradient Descent (DPSGD) in training deep neural networks. The canonical DPSGD approach guarantees differential privacy but at the cost of reduced accuracy primarily due to the gradient clipping and noise addition procedures intrinsic to DPSGD. The authors argue that optimizing the loss function for DPSGDs constraints\u2014namely the sensitivity of the learning algorithm and the limited number of iterations permissible under strong privacy guarantees\u2014can mitigate the accuracy drop. To this end they propose a novel loss function that combines Sum Squared Error (SSE) Focal Loss and a regularization penalty aiming at different aspects of the optimization challenge posed by DPSGD. Specifically the proposed loss seeks to encourage fast convergence through SSE focus on hardtolearn examples in later training stages with the Focal Loss and control the sensitivity of learning via a regularization term. The authors report that their approach achieves new stateoftheart tradeoffs between privacy and accuracy on benchmarks such as MNIST FashionMNIST and CIFAR10 with a particularly notable 4 accuracy improvement for CIFAR10 under a DP guarantee of \u03b5  3.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The paper is the first to tackle the optimization problem in DPSGD learning by focusing on the loss function a direction not previously explored according to the literature review.  Comprehensive Analysis: The paper presents an extensive analysis of the effects of the proposed loss function on the sensitivity of DP models the gradient norms and directions the smoothness of the loss surface and convergence rates.  Practical Impact: Achieving stateoftheart privacyaccuracy tradeoffs in popular datasets indicates the significant potential impact of this work in actual DP machine learning applications. Weak Points:  Generalizability of Results: The experiments focus on image datasets and classification tasks. It would be beneficial to see the performance of the proposed loss function in other types of data and tasks such as regression or sequence prediction to assess its broader applicability.  Discussion on Hyperparameters: Although the paper mentions a hyperparameter search it provides limited discussion on the sensitivity of the proposed method to these hyperparameters. A more detailed discussion could aid in understanding the robustness of the approach. Questions and Feedback:  Extension to Other Privacy Mechanisms: The authors focused on DPSGD. Could the proposed loss function be effectively applied to other differential privacy mechanisms such as those involving objective perturbation  Computational Overhead: Is there any significant computational overhead introduced by the introduced loss function especially considering the additional complexity of DPSGD compared to regular SGD  3. Review Summary The paper presents a novel and meaningful contribution to the field of differentially private deep learning by shifting focus to the loss function used in DPSGD. The proposed loss function combining SSE Focal Loss and a regularization penalty represents a comprehensive approach targeting the unique challenges introduced by the privacy constraints in DPSGD. The results reported in the paper are promising indicating significant improvements over stateoftheart privacyaccuracy tradeoffs on widelyused benchmarks. However the paper could be strengthened by addressing the generalizability of the proposed method beyond image classification tasks and providing a more detailed analysis of the sensitivity to hyperparameters. Additionally exploring the applicability of the proposed loss function to other differential privacy mechanisms and assessing its computational overhead would further enhance the papers contribution. Based on the strengths of the novel approach comprehensive analysis and practical significance as well as considering the areas for improvement the recommendation for this paper would lean towards acceptance provided that the authors can address the questions and feedback highlighted regarding generalizability hyperparameter sensitivity and broader applicability.", "u6sUACr7feW": " PeerReview:  1) Summarize the Paper This paper introduces DPPTTS a novel TexttoSpeech model leveraging Determinantal Point Processes (DPPs) to diversify prosody in speech synthesis. The model employs a prosody diversifying module (PDM) alongside a sequencetosequence architecture with stochastic duration and pitch predictors based on normalizing flows. PDM generates diverse prosodic feature candidates and a new similarity metric soft dynamic time warping (SoftDTW) is proposed for evaluating the similarity between variablelength prosodic features. The paper demonstrates through experiments that DPPTTS can generate more expressive and naturalsounding speech than stateoftheart models while maintaining performance in realtime applications.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovativeness: The application of DPPs for prosody diversification in TTS models is novel. The inclusion of a prosody diversifying module (PDM) to regulate the amount and variety of prosodic expression introduces a unique approach to enhancing speech naturalness and expressiveness.  Thorough Evaluation: The paper presents a comprehensive evaluation setup comparing DPPTTS with leading models on prosody diversity and naturalness. Sidebyside evaluations human evaluations and additional quantitative metrics provide a robust assessment of DPPTTS\u2019s performance.  RealTime Application Feasibility: Demonstrating that the model can be used in realtime applications without significant loss in performance or quality is a compelling argument for its practicality. Weak Points:  Complexity and Transparency: The paper introduces a complex model architecture and training procedure. However it does not thoroughly discuss the computational complexity making it challenging to assess the feasibility of applying this model in various realworld scenarios.  Limited Evaluation on Diverse Datasets: The evaluation is primarily conducted on the LJ Speech dataset. Expanding the evaluation to include more diverse datasets in multiple languages could validate the models effectiveness across different linguistic and prosodic characteristics.  Lack of Discussion on Failure Cases: The paper does not address potential limitations or failure cases of DPPTTS. Insights into situations where DPPTTS might underperform or its adaptability challenges would provide a more balanced understanding. Questions and Feedback:  How does the models performance scale with the size of the dataset and are there specific characteristics of datasets that significantly impact performance  The use of SoftDTW is intriguing. Could the authors elaborate on the decision process for selecting this measure over others particularly regarding its impact on the models performance  The paper could benefit from a deeper discussion on the implications of the chosen hyperparameters including the quality weight and the number of candidates in PDM. How sensitive is the models output to these parameters  3) Review Summary The paper presents a significant contribution to the TexttoSpeech domain by proposing a novel method for prosody diversification using Determinantal Point Processes. The innovative aspects of the model coupled with comprehensive evaluations underscore its potential to generate more expressive and naturalsounding speech than current stateoftheart models. However the models complexity the predominance of evaluations on a single dataset and the lack of discussion on potential limitations or failure cases are areas that need addressing. The authors are encouraged to provide more details on the impact of dataset characteristics on performance the rationale behind specific methodological choices such as SoftDTW and an exploration of the models sensitivity to hyperparameters. Overall despite these areas for improvement the model represents a promising advancement in the field of speech synthesis potentially paving the way for more natural and expressive TTS systems.", "iMH1e5k7n3L": " Peer Review for \"Integrating Rank Coding into Artificial Neural Networks for Improved Efficiency and Speed\" 1) Summary of the Paper The paper presents an innovative approach to enhancing the computational efficiency and speed of artificial neural networks (ANNs) by incorporating temporal coding techniques inspired by biological spiking neural networks (SNNs). It focuses on rank coding (RC) a method where the temporal order of neuron firing encodes information. By applying RC within the context of conventional ANNs specifically long shortterm memory (LSTM) networks the authors demonstrate significant computational savings and speedups during both training and inference phases. The approach involves modifying the backpropagation through time (BPTT) process to initiate from a strategically chosen early timestep based on a thresholdcrossing event effectively shortening the forward and backward passes. The proposed method showcased through experiments is shown to achieve high accuracy on timedependent tasks including sequence classification and spokenword recognition while allowing for a tunable speedaccuracy tradeoff through an innovative regularization term that balances output timing optimization with accuracy. 2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Integration of SNN Concepts in ANNs: The introduction of temporal coding such as rank coding into ANNs is a novel endeavor that bridges the gap between the computational models of biological neural systems and artificial neural systems.  Demonstrable Improvements in Efficiency and Speed: The paper provides concrete evidence through experiments that the RC training method improves computational efficiency and speed without significantly sacrificing accuracy.  Flexibility in SpeedAccuracy Tradeoff: The introduction of a regularization parameter that allows tuning the speedaccuracy tradeoff provides significant flexibility making the approach applicable to a wide range of realworld problems with different requirements. Weak Points:  Limitations in Applicability Disclosure: The paper lacks a detailed discussion on the limitations of the proposed method such as potential applicability issues in more complex or nontemporal tasks.  Comparison with Existing Methods: While the paper mentions surpassing stateoftheart SNN results and provides a comparison with nonRC LSTMs it lacks a direct and detailed comparison with existing ANN techniques that aim for similar goals such as early exit strategies.  Details on Training Complexity: The paper could benefit from a more detailed discussion on the training complexity especially concerning the training time and resource requirements compared to conventional methods. Questions and Feedback:  Have the authors considered the impact of their method on model interpretability especially given the implicit nature of timing optimization  It would be beneficial if the authors could elaborate on the potential applicability of their method to nonsequenced tasks or more complex sequential tasks that might not exhibit clear temporal patterns.  Feedback includes a suggestion for future work to explore the integration of RC with other ANN architectures beyond LSTMs such as Transformers to assess the generalizability of the approach across different model types. 3) Review Summary The paper offers a significant contribution to the field of neuromorphic computing by successfully integrating rank coding a temporal coding mechanism inspired by biological neural networks into conventional artificial neural networks. This integration not only aligns with the ongoing efforts to bridge the gap between biological and artificial neural systems but also addresses practical concerns of computational efficiency and speed in processing temporal data. The demonstration of the RCtrained models achieving superior performance in efficiency and maintaining high accuracy on sequence classification and temporallyencoded tasks is commendable. However for the paper to have a stronger impact it would benefit from addressing the noted weak points such as providing a more indepth discussion on the limitations of the proposed method offering direct comparisons with other existing ANN techniques targeting similar goals and elaborating on the training complexity involved. Despite these areas for improvement the novel approach of incorporating temporal coding into ANNs presents a promising direction for future research in neuromorphic computing and artificial intelligence at large.", "r88Isj2alz": " Review of \"Exploiting Vulnerabilities of Neural ODEs via Adversarial EnergySurging Attacks (NODEAttack)\"  1) Summary The paper introduces an innovative adversarial attack technique NODEAttack targeted at Neural Ordinary Differential Equation (ODE) models. These models have garnered attention for their memory efficiency making them ideal for deployment in resourceconstrained environments like IoT devices. The research highlights a previously unexplored vulnerability in these models: their adaptive energy consumption during inference which can be exploited to significantly increase their energy usage through carefully crafted adversarial inputs. The paper demonstrates that such attacks can lead to a 168 increase in energy consumption over benign inputs and even reduce the efficiency of a mobile application by 50. This study is the first of its kind to focus on energybased adversarial attacks on Neural ODEs offering insights into both the problems formulation and evaluation across different solvers and datasets.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The research tackles a unique aspect of Neural ODEs i.e. their energy consumption during inference which has not been previously explored. It introduces a novel type of adversarial attack that can have significant implications for the deployment of such models in realworld applications.  Comprehensive Evaluation: The extensive evaluation across two datasets and popular ODE solvers coupled with a practical case study provides robust evidence of the effectiveness and realworld impact of NODEAttack.  Transferability Insights: The exploration of attack transferability across different solvers and architectures increases the studys relevance and applicability demonstrating the broader implications of the findings. Weak Points:  Lack of Comparison with Other Adversarial Attacks: While the paper introduces an innovative attack method it does not compare its approach with existing adversarial attack methods beyond energy consumption such as those focusing on accuracy degradation.  Limited Discussion on Mitigation Strategies: The paper does an excellent job of identifying and demonstrating a significant vulnerability but falls short in offering detailed insights into potential mitigation strategies or countermeasures. Questions and Feedback:  How does the energy consumption of Neural ODE models under NODEAttack compare to traditional DNN models under similar adversarial conditions Is there a differential impact that warrants specific mitigation strategies for Neural ODEs  Could the authors elaborate on potential mitigation strategies for reducing the susceptibility of Neural ODE models to such energysurging attacks Understanding whether existing adversarial example detection or mitigation techniques could be adapted for this purpose would be valuable.  The paper focuses on the CIFAR10 and MNIST datasets for evaluation. Would the authors expect similar findings across other datasets or in scenarios involving more complex or highdimensional data such as video processing  3) Review Summary This paper presents groundbreaking work in the field of adversarial attacks against Neural ODE models focusing on a largely unexplored vulnerability: the adaptive energy consumption of these models during inference. The proposed NODEAttack method significantly increases the energy consumption of affected models presenting a serious concern for their deployment in energysensitive environments. The research is wellconducted with a comprehensive evaluation that convincingly demonstrates the attacks effectiveness and implications. While the novelty and relevance of the work are commendable the paper could be strengthened by a detailed comparison with other forms of adversarial attacks and a deeper exploration into potential mitigation strategies. Such insights would not only contextualize the unique challenges posed by NODEAttack but also guide future research in securing Neural ODE models against adversarial threats. Overall this paper makes a valuable contribution to the field by highlighting a critical vulnerability in Neural ODEs and setting the stage for further research on energyefficient and secure deployment of these models in practical applications.", "uHq5rHHektz": " Review of the Paper: \"Improving Adversarial Robustness through Contextual Fusion in Deep Neural Networks\"  Summary: This paper presents a novel method to enhance the adversarial robustness of deep learning models by using a fusion model that integrates both background and foreground features extracted from two different Convolutional Neural Networks (CNNs) specifically PlacesCNN for background and ImageNetCNN for foreground. The authors compare this approach against traditional models on CIFAR10 and MS COCO datasets under various adversarial attacks including Gaussian blur and FGSM attacks. They demonstrate that the fusion approach does not decrease performance on clean data while simultaneously improving robustness to adversarial perturbations. The paper also explores the benefits of regularization to bias the decisionmaking process in the presence of adversarial attacks showing that it can improve performance beyond conventional adversarial training methods.  Strong Points: 1. Innovative Approach: The paper introduces a biologically inspired multimodal fusion strategy that mimics the human brains ability to integrate information from different sensory inputs for decisionmaking. This represents a significant departure from conventional singular focus CNNs towards a more integrated and robust approach to image classification. 2. Comprehensive Evaluation: The methodology section details a robust evaluation framework that tests the model across different types of adversarial attacks and datasets. This helps in affirming the versatility and effectiveness of the proposed model in various scenarios. 3. Realworld Applicability: By addressing the limitations of current deep learning models in adversarial settings without compromising on performance on clean data the paper contributes practical insights that can be immediately utilized in enhancing the security and reliability of AI systems in realworld applications.  Weak Points: 1. Limited Exploration of Fusion Techniques: While the paper provides a solid foundational approach to multimodal fusion it somewhat narrowly focuses on a single method of fusion (late fusion strategy). Exploring and comparing other fusion techniques could provide a more holistic understanding of the potential of multimodal approaches. 2. Lack of Comparison with StateoftheArt Models: Although the paper compares the proposed model with traditional models under adversarial and clean conditions it does not benchmark against current stateoftheart models specifically designed for adversarial robustness. This comparison could significantly strengthen the argument for the superiority of the proposed method. 3. Dataset Limitation: The paper focuses primarily on the visual modality using CIFAR10 and MS COCO datasets. Incorporating additional datasets or modalities (e.g. auditory) could help in validating the generalizability and scalability of the fusion methodology across different sensory inputs.  Questions and Feedback to the Author: 1. Fusion Techniques: Could the authors elaborate on why the late fusion strategy was chosen over other potential fusion approaches Additionally would incorporating early or hybrid fusion strategies improve the robustness and generality of the model 2. StateoftheArt Comparison: How does the proposed model perform in comparison to other stateoftheart adversarial defense mechanisms Specifically metrics on robustness accuracy on clean data and computational efficiency would be beneficial. 3. Extension to Other Modalities: The paper mentions the potential for extending this approach by adding richer contexts and modalities. Could the authors speculate on the challenges and potential strategies for integrating nonvisual modalities such as auditory or textual information into the fusion model  Review Summary: The paper presents a compelling and innovative approach towards improving adversarial robustness in deep neural networks through a biologically inspired fusion of background and foreground information. This multimodal strategy not only aligns with natural cognitive processes but also demonstrates significant improvements in image classification under adversarial attacks without compromising performance on clean data. The authors provide a rigorous evaluation of the model across different datasets and attack types bolstering the validity of their findings. However the paper could benefit from a broader exploration of multimodal fusion techniques and a direct comparison with stateoftheart adversarial defense models to contextualize its position within current research. Additionally extending the approach to include more diverse datasets and sensory modalities would greatly enhance the scope and applicability of the findings. Overall this work marks a considerable step forward in the pursuit of robust AI systems capable of withstanding adversarial perturbations drawing valuable insights from biological systems. The proposed fusion model not only represents a methodological advancement but also opens avenues for further research in multimodal learning and adversarial defense strategies.", "kHNKTO2sYH": " Peer Review  1) Summary of the Paper The paper introduces a novel semisupervised model named Clean Subspace Variational Autoencoder (CLSVAE) for detecting and repairing systematic errors in datasets. Systematic errors unlike random errors follow deterministic transformations that make them predictable but challenging for models to distinguish from inliers (uncorrupted instances). The proposed CLSVAE tackles this by partitioning the latent space into clean and dirty subspaces to model inliers and outliers separately. This approach allows for efficient detection and repair of systematic errors with minimal labelled data outperforming existing baseline models in experiments with three image datasets across different corruption levels and labelled set sizes.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality: The paper addresses the detection and repair of systematic errors with a novel semisupervised approach which has not been explored sufficiently in existing literature. The partitioning of latent space into clean and dirty subspaces is an innovative solution.  Practicality: The requirement for minimal labelled data (less than 2 of the dataset) makes CLSVAE highly practical for realworld applications where labelling can be expensive and timeconsuming.  Comprehensive Evaluation: The model is tested comprehensively across different datasets corruption levels and sizes of the labelled subset showing its robustness and effectiveness compared to baselines. Weak Points:  Lack of Diversity in Data Types: The experiments solely focus on image datasets. It remains unclear how the model performs on other types of data (e.g. tabular textual or sensor data) which could have different characteristics and types of systematic errors.  Assumption of Error Patterns: The models performance relies on the assumption that systematic errors have distinct patterns distinguishable from clean patterns. This might not always be the case especially in complex or noisy datasets where such patterns are less obvious.  Model Complexity and Scalability: The partitioning of latent space and additional penalty for minimizing mutual information could increase the models complexity. Theres little discussion on the scalability of CLSVAE to very large datasets or its computational efficiency. Questions and Feedback:  How does the model perform on nonimage data types Are there plans to test the model on tabular textual or sensor data  Can the authors elaborate on how CLSVAE handles cases where systematic error patterns are not easily distinguishable from clean patterns Are there any mechanisms to improve detection in such scenarios  A detailed discussion on the computational requirements and scalability of CLSVAE would be beneficial. How does the models performance (both in terms of accuracy and execution time) scale with the size of the dataset  3) Review Summary The paper presents an innovative semisupervised approach to the detection and repair of systematic errors in datasets. The proposed CLSVAE model shows promising results significantly outperforming baseline models across various experimental conditions. The approach of partitioning the latent space into clean and dirty subspaces is particularly noteworthy as it allows for the efficient handling of systematic errors with minimal labelled data. However the paper would benefit from expanding the scope of experiments to include nonimage data types and providing a more indepth discussion on the models performance in scenarios with less distinct error patterns. Additionally elaborating on the computational efficiency and scalability of the model would enhance its practical applicability. Overall the paper contributes valuable insights to the field of data cleaning and presents a potent tool for practitioners dealing with dirty datasets. However addressing the highlighted concerns could broaden the utility and applicability of CLSVAE reinforcing its significance in the domain.", "kUtux8k0G6y": " Review of \"Boosting the Robustness of Deep Learning Models Without Sacrificing Accuracy\"  1) Summarize the paper. This paper tackles the problem of robust inaccuracy in stateoftheart deep learning models proposing a novel training method that optimizes for both robust accuracy and minimizing robust inaccuracy without negatively impacting natural accuracy. The key innovations include a new loss function for training models to be both accurate and robust leveraging robustness as a principled mechanism for model abstention and employing a compositional model architecture to enhance overall model robustness without sacrificing accuracy. Through exhaustive experiments across various datasets and stateoftheart models the authors demonstrate that their technique notably reduces robust inaccuracy while maintaining or even slightly improving natural accuracy thus providing a more balanced tradeoff between the two.  2) List strong and weak points of the paper Question and Feedback to the author.  Strong Points:  Novelty and Importance: The paper addresses a critical and often overlooked problem in the field of deep learning: robust inaccuracy. Its novel training methodology provides a significant leap towards practical applications of neural networks especially in safetycritical areas.  Comprehensive Evaluation: The authors provide a thorough experimental analysis using various datasets and models effectively demonstrating the effectiveness of their approach across different conditions.  Clear Presentation: The concepts methodology results and implications are presented in a clear and wellstructured manner making the paper accessible and informative.  Weak Points:  Lack of Theoretical Analysis: While the paper provides empirical evidence supporting its claims it lacks a rigorous theoretical analysis or explanation of why the proposed method improves robust accuracy without sacrificing natural accuracy.  Generalizability Concerns: The paper could benefit from a discussion on the generalizability of the proposed method across broader datasets and tasks beyond those evaluated. Its uncertain how the method performs in completely unseen or more complex data settings.  Evaluation on Runtime Overhead: There is minimal discussion on the computational cost and runtime efficiency of the proposed methods which is crucial for practical adoption.  Questions and Feedback:  Can the authors provide more insight into why their training method outperforms existing approaches possibly through a theoretical lens  Would the authors comment on the scalability of their methodology in terms of computational resources required and how it scales with dataset and model size  A discussion on the limitations of the proposed abstain mechanism including potential scenarios where it might not perform as expected would be valuable. How does the abstain mechanism behave in the presence of highly sophisticated adversarial attacks  Exploring the impact of the choice of datasets and models on the methods effectiveness would provide deeper insights into its generalizability. Are there specific characteristics of data or models where the proposed method excels or falls short  3) Review Summary (Supporting arguments for your recommendation). The paper makes a significant contribution to the field of robust deep learning by proposing an innovative method that balances the tradeoff between robust and natural accuracy. The strong empirical results across various datasets and models coupled with the novelty and practical importance of the work make it a valuable addition to the literature. However the paper would benefit from a more rigorous theoretical analysis to complement its empirical findings. Furthermore addressing the generalizability of the proposed method to a wider array of tasks and datasets as well as providing a detailed discussion on the computation costs and the specifics of the abstain mechanism would strengthen the paper. Given the strengths of the proposed method and the comprehensive evaluation provided I recommend this paper for publication contingent upon the authors addressing the points related to theoretical foundations computational efficiency and generalizability.", "ljxWpdBl4V": " Review of \"A Principled Approach for Generative Model Training with Sample Probing in ZeroShot Learning\"  1. Summarize the paper The paper introduces a novel approach for training generative models specifically targeted at improving ZeroShot Learning (ZSL) and Generalized ZeroShot Learning (GZSL) tasks. The core idea is to employ a metalearning scheme called \"sample probing\" whereby a taskspecific closedform ZSL model is trained on synthetic samples generated by the model itself. These samples are then evaluated on novel samples within a compute graph allowing the generative model to receive direct feedback based on the utility of its samples for model training making the training process endtoend. The paper substantiates its claims through extensive experiments on four benchmark datasets demonstrating that integrating sample probing with stateoftheart generative models yields significant improvements in ZSL performance.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Innovativeness: The paper proposes a unique metalearningbased approach for enhancing generative models in ZSL tasks. The use of sample probing represents an innovative strategy to directly measure and improve the utility of generated samples for ZSL model training. b. Comprehensiveness of Experiments: Experiments are thorough spanning multiple benchmark datasets and comparisons with a variety of stateoftheart models. This provides a solid empirical foundation for the paper\u2019s claims. c. Theoretical Foundation: The introduction of a closedform solution for the probe model and its integration into the training process is elegantly derived and strengthens the papers theoretical underpinning. Weak Points: a. Complexity in Understanding: The paper is dense and may be challenging for readers unfamiliar with the intricate details of generative models and ZSL. Simplifying the explanation or providing a more detailed walkthrough of the process could enhance readability. b. Discussion on Limitations: While the paper presents its approachs benefits it falls short in discussing potential limitations or scenarios where sample probing might not yield expected improvements. Questions and Feedback: a. Scalability and Computational Costs: How does the introduced sample probing method scale with larger models and datasets especially considering the computational complexity added by training the probe model within the generative model\u2019s compute graph b. Extension to Other Domains: The paper primarily focuses on visual tasks. Could you speculate on the applicability of your approach to other domains such as natural language processing or audio processing c. Impact of the Quality of Class Embeddings: The method assumes the availability of highquality class embeddings. Could you discuss how variations in the quality of these embeddings could affect the performance of your approach  3. Review Summary The paper presents a significant leap in the training of generative models for ZSL by proposing a sample probing approach that measures sample utility directly for model training purposes. The approach is both innovative and theoretically sound bolstered by extensive experimental validation showcasing its effectiveness over current stateoftheart models. However potential readers might struggle with the papers complexity and further discussion on limitations and scalability could strengthen its contributions. Recommendation: I recommend accepting this paper after minor revisions particularly focusing on addressing the clarity of the presented method and discussing its scalability computational costs and potential limitations. The papers novel contributions and solid experimental validation make it a valuable addition to the ZSL and generative modeling body of research.", "tlkMbWBEAFb": " Peer Review of the Paper on Steerable FeedForward LearningBased Approach for Point Clouds  Summary The paper presents a novel framework for classifying 3D point clouds through a steerable feedforward learningbased model that uses spherical decision surfaces. This approach builds upon the concepts of steerable filters and spherical neurons to achieve rotation equivariance and with further optimization rotation invariance. The proposed model leverages conformal embedding to manipulate hypersphere neurons allowing for more effective representation and classification of point sets in varied orientations. Experiments conducted with synthetic Tetris data and realworld 3D skeleton data from the UTKinectAction3D dataset validate the effectiveness of this method in handling rotational equivariance and demonstrating robustness against noise and orientation variations.  Strong Points 1. Novelty: The paper introduces a novel approach to 3D point cloud classification that uniquely combines conformal embedding with spherical filtering offering a new direction in improving classification accuracy in the face of rotational discrepancies. 2. Theoretical Foundation: The derivation of a 3D steerability constraint for hypersphere neurons is both rigorous and innovative providing a solid theoretical underpinning for the proposed approach. 3. Empirical Validation: The extensive experimentation including tests with both synthetic and realworld data offers strong empirical support for the model\u2019s effectiveness. The results convincingly demonstrate the advantages of the approach in terms of rotation equivariance and noise robustness. 4. Practical Implications: The reduction in data augmentation requirements due to rotation invariant classification capability presents a significant practical benefit potentially reducing computational costs and improving model efficiency.  Weak Points 1. Complexity of Methodology: The sophistication of the theoretical framework while a strength could also be a barrier to understanding and broader adoption. A simplified interpretation or visualization of the key concepts might aid in comprehension. 2. Comparison with StateoftheArt: While the paper presents impressive results a more comprehensive comparison with other leading methods in 3D point cloud classification especially nonsteerable approaches would provide a clearer perspective on the method\u2019s relative performance. 3. Discussion on Limitations: The paper could benefit from a deeper discussion on the limitations of the proposed approach such as potential computational overheads introduced by conformal embedding and steering operations.  Questions and Feedback to the Author 1. Method Complexity: Can the authors provide a simplified overview or visual guide to help in understanding the complex mathematical operations involved particularly conformal embedding and the steerability constraint derivation 2. Comparison Basis: On what basis were the other methods selected for comparison Including more recent or directly competitive approaches might strengthen the argument for the proposed method\u2019s novelty and performance. 3. Addressing Limitations: Could the authors comment on any identified limitations or potential tradeoffs involved in applying their model especially in terms of computational efficiency and scalability to larger datasets  Review Summary The paper introduces a compelling and theoretically wellfounded approach to 3D point cloud classification that addresses the challenge of rotational variations through the novel use of steerable spherical neurons. The method\u2019s theoretical innovation combined with strong empirical validation marks a significant contribution to the field. However the complexity of the methodology could benefit from further elucidation for broader accessibility. Additionally expanding the comparison with other stateoftheart methods and a more thorough discussion of potential limitations would enhance the papers impact and practical relevance. The combination of theoretical rigor and practical effectiveness sets this work apart suggesting a valuable direction for future research in 3D object classification. Given its contributions the paper is recommended for publication with minor revisions to address the highlighted aspects for clarification and expansion.", "hW2kwAcXq5w": " Peer Review  1) Summarize the paper The paper presents a novel approach to offline Imitation Learning (IL) named DiscriminatorWeighted Behavioral Cloning (DWBC) which learns optimal expert behavior from a dataset composed of both optimal and suboptimal demonstrations. By integrating a discriminator that can distinguish between expert and nonexpert data the proposed method optimizes a generalized Behavioral Cloning (BC) objective where discriminator outputs serve as weights. This method aims to alleviate the need for reward learning and offline Reinforcement Learning (RL) training addressing problems in environments where collecting optimal demonstrations is challenging. Experimental results demonstrate the superiority of DWBC over baseline methods in terms of training speed and returns offering an efficient and effective solution to offline IL challenges.  2) Strong and Weak Points Questions and Feedback Strong points:  Novelty: The introduction of a discriminator to weight the BC objective based on the quality of demonstrations is innovative. It cleverly leverages information within suboptimal data enhancing learning efficiency and performance.  Theoretical justification: The paper provides a sound theoretical foundation for the proposed method clearly explaining its correlation with generalized BC objectives and demonstrating its equivalence under certain conditions.  Empirical validation: The extensive experimental setup across multiple environments and comparison with baseline models robustly validates the effectiveness of DWBC. Weak points:  Lack of comparison with the latest methods: While the paper compares DWBC with several baseline algorithms it does not consider the latest advancements in offline IL or offline RL which might have provided a more challenging comparison.  Limited exploration of hyperparameters: The paper does not fully explore the sensitivity of DWBC to its hyperparameters specifically the weighting factor \u03b1 and the discriminator\u2019s clipping range. A more detailed analysis could offer insights into the robustness of the method.  Potential overfitting concerns: There is a brief mention of overfitting in the section discussing ORIL but no detailed discussion on how DWBC mitigates potential overfitting especially considering its reliance on discriminator outputs which could potentially introduce bias. Questions and feedback for authors:  Could you provide more insights into the choice of environments for your experiments How do these choices reflect the anticipated realworld applications of DWBC  How does DWBC perform under conditions of extreme suboptimality in demonstrations Is there a threshold beyond which the method\u2019s performance degrades significantly  A discussion on the computational complexity of DWBC compared to baseline methods would be beneficial. Can the increased training speed compensate for any additional computational demands introduced by the discriminator  It would be advantageous to see a more detailed exploration of how the method scales with larger and more complex datasets potentially highlighting limitations and scalability challenges.  3) Review Summary The paper introduces an innovative offline IL algorithm DWBC which leverages a discriminator for weighting the BC objective facilitating effective learning from datasets containing a mix of optimal and suboptimal demonstrations. The method is underpinned by solid theoretical explanations and compelling empirical evidence demonstrating its superiority over baseline algorithms in terms of returns and training speed. However the paper could be strengthened by a broader comparison with the latest methods in offline IL and offline RL more extensive exploration of hyperparameter sensitivity and a thorough examination of the potential for overfitting. Further discussion on scalability computational complexity and realworld applicability would significantly enhance the papers contribution to the field of offline IL. Overall the paper presents a promising direction for addressing key challenges in offline IL particularly in leveraging suboptimal data without the need for reward function learning or extensive environment interactions. With improvements in the areas highlighted for feedback this work could have a substantial impact on the development of efficient and effective IL methods suitable for a wide range of applications.", "vruwp11pWnO": " Review of \"Scaling OutofDistribution Detection for SafetyCritical Applications\"  1. Summary The paper addresses the critical challenge of outofdistribution (OOD) detection in machine learning (ML) specifically targeting safetycritical applications such as autonomous driving and novel organism identification. Recognizing the limitations of existing research that primarily focuses on smallscale datasets the authors propose a shift towards more realistic largescale settings to better evaluate OOD detection models. They introduce three benchmarks tailored to largescale settings: a dataset for ImageNet multiclass anomaly detection featuring anomalous species a multilabel anomaly detection setup leveraging ImageNet21K for evaluating PASCAL VOC and COCO datasets and a novel benchmark for anomaly segmentation using the StreetHazards dataset. The key finding is the effectiveness of a simple detector based on maximum logit (MaxLogit) which outperforms stateoftheart methods across all tasks establishing a new baseline for future research in largescale OOD detection.  2. Strong and Weak Points Questions and Feedback Strong Points: a. Novel Datasets and Benchmarks: The creation of new benchmarks especially in unexplored largescale settings is a significant contribution. The Species dataset and the StreetHazards dataset for anomaly segmentation provide valuable resources for the community to test OOD detection methods in more realistic scenarios. b. Simplicity and Effectiveness of MaxLogit Detector: The identification and evaluation of the MaxLogit detector as a superior method for OOD detection across various settings is commendable. The simplicity of the approach coupled with its performance makes a strong case for its adoption as a new baseline. c. Comprehensive Evaluation: The extensive experimental setup across multiple datasets and settings including multiclass multilabel and segmentation tasks provides a thorough validation of the authors claims. Weak Points: a. Lack of Theoretical Justification: While the MaxLogit detector shows excellent empirical results the paper lacks a theoretical analysis explaining why the MaxLogit approach is more effective than other methods especially the MSP baseline. b. Generalization Beyond Image Data: The focus is primarily on image data. It would be beneficial to see if the proposed MaxLogit detector can be equally effective in other domains such as text or audio. c. Comparison with Very Recent Approaches: Given the rapidly evolving nature of ML research it would be advantageous to compare the MaxLogit detector with very recent OOD detection methods not covered in the paper. Questions and Feedback: 1. Can the authors provide any theoretical insights or hypotheses to explain the superior performance of the MaxLogit detector 2. Are there any known limitations or drawbacks of the MaxLogit approach when applied to datasets with an extremely high number of classes or in domains other than images 3. The paper would benefit from a discussion on the computational cost of the MaxLogit detector compared to other methods especially considering the largescale settings proposed.  3. Review Summary This paper makes significant contributions to the field of outofdistribution detection by proposing new benchmarks tailored for largescale settings and introducing the MaxLogit detector which outperforms existing methods across various tasks. The datasets introduced including the Species and StreetHazards datasets are valuable assets for the community enabling further research in more realistic scenarios. While the paper is strong in its experimental validation of the MaxLogit detector it falls short in providing theoretical explanations for its effectiveness and exploring its generalization beyond image data. Despite these minor weaknesses the papers contributions are substantial and its findings are likely to have a lasting impact on the field. The simplicity and effectiveness of the MaxLogit approach combined with the introduction of new largescale benchmarks pave the way for future research on outofdistribution detection in safetycritical environments. I recommend acceptance suggesting the authors address the outlined questions and feedback to strengthen the paper further.", "wgR0BQfG5vi": "Summarize the Paper This paper addresses the problem of overconfidence in neural networks predictions specifically in the context of machine translation. It critically discusses the limitations of static label smoothing parameters and introduces a novel regularization scheme that dynamically adjusts the smoothing parameter based on the entropic level of the models prediction probability distribution making the degree of label smoothing samplespecific and adaptive throughout training. This method also incorporates selfknowledge distillation as a means of providing a prior label distribution for softening target labels. The authors present theoretical support for their approach showing how it can lead to better gradient scaling and direction thereby averting overconfidence and improving model calibration. Experiments conducted across multiple machine translation datasets demonstrate significant improvements in model performance and calibration over baseline methods. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The adaptive nature of the proposed regularization method is a clever solution to the static parameter issue in label smoothing making it a significant contribution to the field. 2. Comprehensive Validation: The method is validated across several datasets and metrics providing a solid empirical foundation for the claims of improved performance and calibration. 3. Theoretical Underpinning: The theoretical analysis detailing the interaction between adaptive smoothing selfknowledge distillation and gradient dynamics adds depth to the paper ensuring the reader understands why the method works. Weak Points: 1. Domain Specificity: The experiments are confined to machine translation. It would be beneficial to understand how this method performs on a broader range of tasks including those outside NLP. 2. Complexity in Explanation: While the theoretical grounding is strong the explanation of gradient analysis and the operation of the adaptive mechanism could be challenging for readers not already familiar with these concepts. 3. Evaluation of Adaptive Mechanism: The paper could benefit from a more detailed exploration of the behavior of the adaptive smoothing parameter throughout training beyond its endpoint convergence. Questions and Feedback: 1. Beyond NLP: Can the authors speculate or provide preliminary results on the applicability of this regularization scheme to tasks outside NLP such as image classification or reinforcement learning 2. Adaptive Parameter Analysis: Could the paper include a more detailed analysis of the dynamics of the adaptive smoothing parameter throughout training perhaps using case studies of specific instancessamples 3. Explanation Clarity: While the theoretical basis is comprehensive the authors might consider simplifying the explanation or providing more intuitive examples to help readers less familiar with the underlying mathematics. Review Summary This paper makes a substantial contribution to addressing overconfidence in neural networks through an innovative adaptive label smoothing method augmented with selfknowledge distillation. The empirical validations across several benchmarks show marked improvement in both model performance and calibration underscoring the methods effectiveness. The theoretical insights into gradient dynamics offer a convincing explanation of why the method works enriching the papers value. However the scope of experiments is narrowly focused on machine translation within NLP which may limit the perceived applicability of the method. Additionally the complexity of the descriptions around the adaptive mechanisms operation and its theoretical justification may pose barriers to understanding for a portion of the intended audience. Given these strengths and areas for enhancement the paper is recommended for publication. It is suggested that the authors consider expanding the scope of their empirical validation to include tasks outside of NLP and strive for greater clarity and accessibility in their explanations of the core concepts and theoretical analysis.", "hjlXybdILM3": "Summarize the paper: The paper introduces SimpleBits an innovative method aiming to synthesize simplified inputs for neural network classifiers by reducing their information content. The simplification process is quantified by minimizing the encoding bit size derived from a pretrained generative image model under the hypothesis that a lower bit size correlates with simpler visual complexity. SimpleBits is tested in various scenarios including conventional training dataset condensation and posthoc explanations demonstrating its capability to reduce superfluous information without significantly compromising task performance. It successfully identifies and removes redundant information in settings with introduced distractors and elucidates the tradeoff between input simplification and task performance across different realworld datasets. Furthermore when applied to dataset condensation SimpleBits effectively maintains model accuracy with significantly simplified datasets. In the posttraining context it offers insights into model prediction processes by indicating potential reasons for misclassifications. Strengths and Weaknesses: Strengths: 1. Innovative Approach: SimpleBits introduces a novel way of creating simplified inputs that maintain taskrelevant information filling a gap in understanding neural networks operational mechanisms. 2. Versatility: The application of SimpleBits across different scenarios (training condensation and posthoc analysis) showcases its flexibility and utility in diverse domains. 3. Interpretability: By minimizing input complexity while retaining essential information SimpleBits serves as a robust tool for interpretability presenting plausible reasons for misclassifications and uncovering taskrelevant features. 4. Extensive Evaluation: The method is thoroughly evaluated across multiple datasets including realworld and medical imaging datasets providing comprehensive insights into its effectiveness and limitations. Weaknesses: 1. Lack of Comparison: The paper does not compare SimpleBits with existing input simplification methods or baselines. Including comparative analyses could strengthen the argument for SimpleBits effectiveness. 2. Domain Generality: While the paper demonstrates SimpleBits applicability to image data its effectiveness and adaptability to nonvisual data types or text are unexplored. 3. Hyperparameter Sensitivity: The methods dependence on the hyperparameter \u03bbsim could limit its ease of use necessitating extensive tuning for optimal performance across different datasets. Questions and Feedback: 1. Comparison to Existing Methods: Could the authors provide comparisons with existing input simplification or information reduction methods to contextualize SimpleBits performance and utility 2. Application to NonVisual Data: Have the authors considered extending SimpleBits to nonvisual or textbased datasets If so what adjustments if any would be necessary 3. Hyperparameter Optimization: Can the authors offer guidance or strategies for selecting or optimizing the \u03bbsim hyperparameter to generalize across varied datasets Review Summary: SimpleBits represents a significant advance in the field of neural network interpretability and model simplification. By focusing on the reduction of information content in input data while preserving taskrelevance it offers a novel lens through which neural network decisions can be understood and refined. The methods application across different contexts including dataset condensation and error analysis illustrates its versatility and potential for widespread adoption in tasks requiring model transparency and simplicity. However to strengthen the conclusions drawn from this study comparative analyses with existing simplification methods would be beneficial providing clear benchmarks against which SimpleBits effectiveness can be measured. Additionally exploring the methods adaptability to varied data types and domains would enhance its applicability. On the whole SimpleBits is a promising approach to demystifying neural network operations encouraging further investigation into how information simplification impacts model learning and interpretability. Its development is timely and responds to the growing need for transparent understandable models in AI and machine learning.", "hqkhcFHOeKD": " Peer Review  1) Summary of the Paper The paper presents a comprehensive study aimed at improving feature representation in deep neural networks (DNNs) for classification tasks. It addresses the limitations of the conventional softmax loss function which does not adequately encourage discriminative feature learning by proposing a principled optimization approach focused on maximizing class and sample margins. The authors define class margin as a measure of interclass separability and sample margin as a measure of intraclass compactness. They introduce a Generalized Margin Softmax (GMSoftmax) loss and derive specific formulations for classbalanced and classimbalanced scenarios including sample margin regularization and zerocentroid regularization respectively. The theoretical foundation for these approaches is thoroughly discussed and the paper demonstrates the effectiveness of the proposed methods across several tasks: visual classification imbalanced classification person reidentification and face verification.  2) Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Rigor: The paper presents a strong theoretical foundation for the proposed methods discussing the necessity of class and sample margins and rigorously defining the optimization objectives. The derivations and proofs provided in the appendix add credibility and depth to the study.  Comprehensiveness: Addressing both classbalanced and classimbalanced scenarios the paper covers a wide range of applications. The extensive experiments across different tasks and datasets showcase the versatility and wide applicability of the proposed approach.  Novelty and Insight: The introduction of generalized and largest margin softmax losses along with novel regularization terms for balancing classimbalanced datasets provide fresh perspectives on designing and understanding marginbased loss functions. The insights into the necessity of normalization are particularly valuable. Weak Points:  Implementation Details: While the experimental results are impressive the paper could benefit from more detailed discussion on the implementation aspects especially concerning the choice of hyperparameters and their impact on the performance of the proposed methods.  Comparison with StateoftheArt: The paper presents a comparison with several baseline methods but lacks a direct comparison with the latest stateoftheart approaches that might employ different strategies for discriminative learning or dealing with class imbalance.  Discussion on Limitations: The paper thoroughly discusses the proposed methods strengths but falls short in discussing their limitations or potential drawbacks which would have provided a more balanced view and suggested directions for future work. Questions and Feedback:  How sensitive are the proposed methods (GMSoftmax LMSoftmax and the regularization terms) to the choice of hyperparameters It would be helpful to have a sensitivity analysis or a discussion on the robustness of these methods to hyperparameter variations.  Could the authors expand on the potential limitations or scenarios where the proposed methods might not perform as expected Understanding these aspects can significantly contribute to future research directions.  It would be interesting to see a direct comparison with the latest stateoftheart discriminative learning or class imbalance correction methods. How do the proposed methods fare in such a comparison  Adding more discussion on the practical implementation challenges and how to address them could make the paper more useful for practitioners looking to adopt these methods.  3) Review Summary The paper provides a significant contribution to the field of deep learningbased classification by introducing a principled optimization framework that focuses on maximizing class and sample margins. The theoretical rigor comprehensive experimentation and novel insights presented are commendable. However the paper could be strengthened by providing more detailed implementation guidelines addressing potential limitations more thoroughly and offering a direct comparison with the latest stateoftheart methods. These enhancements would make the paper even more valuable to both researchers and practitioners in the field. Given the strong theoretical foundation and the compelling experimental results demonstrated across various tasks I recommend this paper for publication provided that the authors address the mentioned feedback and questions which could enrich the discussion and implications of the study.", "tUa4REjGjTf": " Review of \"Analyzing and Enhancing the Robustness of Ensemble Models Against Adversarial Attacks\"  Summary The paper addresses the vulnerability of deep neural networks (DNNs) to adversarial examples by focusing on the robustness of ensemble machine learning (ML) models. Unlike existing works that primarily concentrate on single ML models this study explores the idea that ensemble ML models could potentially offer additional benefits in terms of certified robustness against adversarial attacks. Through theoretical analysis and empirical validation the authors find that standard ensemble approaches only offer marginal improvements in certified robustness over individual models. They identify diversified gradients and large confidence margins as crucial factors for enhancing the robustness of ensemble models. Based on these insights they introduce a novel training strategy called Diversity Regularized Training (DRT) aimed at improving the certified robustness of ensemble models. The effectiveness of DRT is demonstrated through experiments on benchmark datasets like MNIST CIFAR10 and ImageNet where DRTenhanced ensembles outperform existing approaches in terms of certified robustness.  Strong Points 1. Theoretical Depth: The paper provides a significant theoretical contribution by establishing the necessary and sufficient conditions for the certified robustness of ensemble models which fills a gap in current literature. 2. Practical Significance: By formulating the DRT approach and testing it on wellknown datasets the paper not only advances theoretical understanding but also offers practical tools to enhance the robustness of ensemble ML models against adversarial attacks. 3. Comprehensive Experiments: The extensive experiments provide solid evidence supporting the theoretical claims and demonstrate the superiority of DRT over stateoftheart certified robustness techniques. 4. Relevance: Considering the growing application of DNNs in sensitive areas the paper tackles a highly relevant problem \u2013 improving the robustness of ML models against adversarial attacks.  Weak Points 1. Complexity and Accessibility: The heavy theoretical content may make the paper less accessible to practitioners or researchers from adjacent fields who might benefit from the practical aspects of the DRT approach. 2. Generalization of Results: Though extensive the experiments are limited to three datasets. Additional experiments could help validate the generalizability of the DRT approach across different domains and data types. 3. Comparison with Baselines: While the paper establishes the superiority of DRT over certain baselines a more detailed discussion on how and why DRT outperforms these methods could provide deeper insights to readers.  Questions and Feedback 1. Model Complexity: How does the introduction of DRT affect the computational complexity of training ensemble models especially in the context of larger datasets and models 2. Robustness vs. Accuracy Tradeoff: The paper primarily focuses on certified robustness. It would be informative to discuss any observed tradeoffs between robustness and model accuracy or performance on nonadversarial examples. 3. Extension to Other Threat Models: The paper focuses on L2norm bounded adversarial attacks. How applicable is the DRT approach to other types of adversarial threats or perturbations  Review Summary The paper makes substantial theoretical and empirical contributions by exploring the certified robustness of ensemble ML models to adversarial attacks a topic of growing importance given the widespread application of machine learning. It successfully identifies significant conditions for improving the robustness of ensembles and introduces the DRT method which significantly improves ensemble robustness as verified through comprehensive experimental evaluation. Despite some limitations regarding the broadness of experimental validation and the potential for increased model complexity the findings are robust widely applicable and constitute a significant step forward in the quest for more secure AI systems. Given its strong theoretical foundation practical implications and successful empirical demonstration the paper presents a valuable addition to the field of secure AI.", "ffS_Y258dZs": " Review  Summary of the Paper The paper introduces a novel benchmark aimed at evaluating artificial agents ability to learn and exhibit compositional learning behaviors which are crucial for agents expected to collaborate with humans. This ability allows agents to generalize compositionally learning from a limited set of stimuli and applying that learning to novel situations. The authors propose exploring this capability through metareinforcement learning in the context of discriminative referential games leveraging a new symbolic continuous stimulus (SCS) representation. This representation enables vast semantic structures without altering the representations shape beneficial for metalearning. Baseline results using current reinforcement learning agents on this benchmark indicate a significant challenge underscoring the need for research on new inductive biases and methods to enhance agents compositional learning abilities.  Strong Points 1. Innovative Approach for Testing Compositional Learning: The paper proposes a creative and novel method to test artificial agents compositional learning behaviors a critical aspect of AI expected to collaborate effectively with humans. 2. Introduction of SCS Representation: The symbolic continuous stimulus representation is a significant contribution offering a flexible versatile encoding method that bridges a gap in current representational strategies accommodating infinitely many semantic structures. 3. Comprehensive MetaLearning Framework: The metareinforcement learning framework leveraging discriminative referential games is thoroughly developed providing a solid conceptual foundation for investigating compositional learning behaviors in AI systems.  Weak Points 1. Limited Exploration of Existing Solutions: While the paper introduces a novel benchmark and representation it lacks a detailed comparison with existing approaches or elaboration on how this benchmark outperforms or complements other benchmarks in compositional learning and AI research. 2. Baseline Results Lack Depth: The discussion on baseline results is somewhat shallow lacking indepth analysis or interpretation of these results in the broader context of AIs compositional learning capabilities. 3. Absence of Discussion on Practical Implications: The paper does not discuss the practical implications of its findings or how they could be applied to realworld applications where compositional learning behaviors in AI systems are crucial.  Questions and Feedback to the Author 1. How does the proposed benchmark and SCS representation complement or improve upon existing benchmarks and representations used in studying compositional learning in AI 2. Can the authors provide a more detailed analysis of the baseline results possibly comparing them with results obtained using other benchmarks or representations 3. The paper could benefit from a section discussing potential realworld applications or scenarios where the ability to learn compositional learning behaviors is critical. Could the authors expand on this  Review Summary The paper presents a novel and intellectually stimulating approach to evaluating and fostering compositional learning behaviors in artificial intelligence systems highlighting the importance of this capability for AI agents expected to collaborate with humans. The introduction of the symbolic continuous stimulus representation is a notable contribution potentially revolutionizing how we approach representational strategies in AI research. However the paper could be strengthened by providing a more detailed comparison with existing benchmarks and representations including an indepth analysis of the baseline results within this context. Additionally a discussion on the practical implications of this research for realworld applications would make the findings more impactful. Given the significance of the contributions and the potential for future research that this paper opens up I recommend its publication. However I encourage the authors to address the mentioned weaknesses particularly the need for a more detailed analysis and discussion of the practical implications of their work to further elevate the papers value to the field.", "w7Nb5dSMM-": " Review of the Paper: Equivalence of GillespieOrr Evolutionary Algorithms to Stochastic Gradient Descent in Machine Learning Models: Insights and Implications for Transfer Learning  Summarize the Paper: This paper explores the relationship between Stochastic Gradient Descent (SGD) and a class of evolutionary algorithms (EA) known as GillespieOrr Evolutionary Algorithms (GOEA) within the context of machine learning particularly focusing on the process of transfer learning (TL). The authors aim to establish the theoretical equivalence between SGD and GOEA under certain conditions drawing upon the Fisher Geometric Model in the OrrGillespie formalization as a groundwork. Leveraging this equivalence the paper uncovers several implications for the training of Artificial Neural Networks (ANNs) via SGD including aspects of generalization and minima flatness which are then paralleled with natural evolutionary mechanisms. Through a detailed exploration of evolutionary algorithms and their historical context the paper reviews the advancements in evolutionary perspectives towards optimizing artificial neural network parameters. The core contribution lies in proving the theoretical equivalence between SGD and GOEA under specific assumptions with potential computational advantages for SGD highlighted. This equivalence also paves the way for practical insights into ANN training model robustness generalizability and hypotheses concerning the characteristics of optimization landscapes.  Strong and Weak Points of the Paper: Strengths: 1. Innovative Theoretical Contribution: The theoretical equivalence between SGD and GOEA is novel bridging a significant gap in machine learning and evolutionary algorithm research. This insight has potential implications across a broad spectrum of ML applications especially in understanding and improving transfer learning techniques. 2. Interdisciplinary Approach: The papers interdisciplinary nature integrating concepts from evolutionary biology with machine learning is commendable. This approach not only enriches the theoretical foundation of machine learning models but also opens avenues for crossdisciplinary research. 3. Implications and Heuristics: The discussion on the implications of the central results and the proposed heuristics for accelerating the finetuning process is particularly strong. These elements provide actionable insights that could guide future empirical work in model optimization and transfer learning. Weaknesses: 1. Lack of Empirical Validation: The papers primary weakness lies in the absence of experimental validation of the proposed equivalence and the resultant hypotheses. While the theoretical framework is compelling empirical evidence is crucial for validating and grounding the theoretical propositions in practical machine learning scenarios. 2. Complexity and Accessibility: The paper is dense and technical possibly limiting its accessibility to a broader audience. Simplifying the presentation or providing more intuitive explanations and visualizations could enhance comprehension. 3. Specificity of Assumptions: The conditions under which SGD and GOEA are equivalent (low learning rate for SGD and sufficiently large sampling population for GOEA) may not be universally applicable across all machine learning tasks limiting the generalizability of the findings. Questions and Feedback for the Author: 1. Could you elaborate on potential strategies for empirical validation of the theoretical equivalence between SGD and GOEA What specific experimental setups do you envision 2. The discussion on computational advantages of SGD over EAs highlights a critical practical consideration. How significant do you think the computational cost difference is in realworld scenarios particularly with the evolution of more efficient parallel computing techniques 3. Regarding the proposed heuristics based on evolutionary theory to accelerate finetuning could you provide more concrete examples or case studies where these have been (or could be) successfully applied  Review Summary: The paper makes a significant theoretical contribution by establishing an equivalence between Stochastic Gradient Descent and GillespieOrr Evolutionary Algorithms under certain conditions. This equivalence sheds light on potential improvements and understandings in the training processes of Artificial Neural Networks especially concerning transfer learning. The interdisciplinary approach enriches the theoretical discussions and offers novel perspectives on model optimization. However the lack of empirical validation is a notable limitation. Future work should aim to test the hypotheses and heuristics proposed through rigorous experimental setups to bolster the theoretical claims. Additionally simplifying the complex technical explanations could enhance the papers accessibility and impact. Overall this paper pioneers a promising theoretical framework potentially transformative for machine learning research particularly in optimizing transfer learning processes. The insights derived from understanding SGD and GOEA equivalence are valuable encouraging further investigation into evolutionary algorithms application in machine learning.", "uy602F8cTrh": " Peer Review  1. Summary of the Paper This paper introduces CausalDyna a Dynastyle modelbased reinforcement learning (MBRL) algorithm that aims to enhance the generalization capabilities of deep reinforcement learning agents in robotic manipulation tasks. This enhancement comes from the algorithms use of Structural Causal Models (SCMs) to model state dynamics and generate episodes with counterfactually varied object property values. Such a novel application is intended to help policies generalize to objects with properties that are rare or unseen during the training phase thereby improving both sample efficiency and generalization when interacting with a diverse set of object properties. The algorithms performance is evaluated against two benchmarks the stateoftheart modelbased algorithm MBPO and the modelfree algorithm SAC within the CausalWorld roboticmanipulation environment. Here CausalDyna showcases a significant improvement in performance regarding both sample efficiency and generalization abilities.  2. Strong and Weak Points Questions and Feedback to the Author: Strong Points:  Novelty: The integration of SCMs for counterfactual reasoning within a Dynastyle MBRL framework is innovative and wellmotivated aiming to directly tackle the challenge of generalization within the realm of robotic manipulation.  Empirical Results: The empirical evaluation shows that CausalDyna outperforms existing stateoftheart MBRL and modelfree algorithms in terms of sample efficiency and the ability to generalize across a range of object properties.  Practical Relevance: The realistic motivation behind avoiding damaging exploration and improving the diversity of training scenarios is commendable and addresses a genuine concern in robotic manipulation tasks. Weak Points:  Model Complexity: The introduction and application of SCMs add a layer of complexity to the model. The paper would benefit from a more detailed discussion of the practical implications of implementing such a model including computational overhead and scalability.  Limited Evaluation: While the current evaluation in the CausalWorld environment is thorough the generalization of CausalDyna to other domains or tasks outside robotic manipulation is not addressed.  Hyperparameter sensitivity: The methods performance might be sensitive to the choice of hyperparameters (e.g. the counterfactual generation ratio \u03b1) yet there is minimal discussion on this aspect. Questions and Feedback:  Model Complexity and Interpretability: Could the authors elaborate on the interpretability of the causal models used Given the complexity added by SCM understanding how changes in object properties causally affect the environment could be crucial.  Counterfactual Reasoning Scope: Is the scope of counterfactual reasoning limited to object properties or can it be extended to other aspects of the environment or the agents actions Additionally the implications of this extension for model complexity and performance would be valuable.  World Model Design: The paper mentions the performance degradation in the case of manipulating very heavy objects (0.1kg). Could the authors speculate on potential approaches or modifications to the world model design that might address this issue  3. Review Summary and Recommendation This paper presents a significant contribution to the field of modelbased reinforcement learning particularly in the context of generalizing robotic manipulation tasks across a varied range of object properties. The innovative use of Structural Causal Models for generating counterfactual episodes represents a promising direction for enhancing the adaptability and efficiency of learning algorithms. The experimental results provide strong evidence for the methods effectiveness underpinning its potential for broader application beyond the tested scenarios. However details concerning the models complexity practical implementation considerations and potential for domain generalization warrant further exploration. Addressing these aspects would strengthen the papers contributions and provide clearer guidance for future research and applications. Given the strong empirical results and the novelty of the approach I recommend this paper for publication contingent on a more rigorous discussion of the mentioned concerns to fully realize and communicate its contributions and applicability.", "p4H9QlbJvx": " Review of \"Dynamical Isometry and Its Impact on Structured Neural Network Pruning\"  Summary This paper revisits the debate on the efficacy of inheriting weights in structured neural network pruning challenging recent studies that suggest starting from scratch could be equally or more effective than finetuning a pruned model. The authors argue that the underperformance of pruned models in past studies stemmed from an inadequately low finetuning learning rate (LR). Through extensive experimentation across various networks (ResNets VGG11) and datasets (MNIST CIFAR10 ImageNet) they demonstrate that with appropriately larger finetuning LRs pruning consistently outperforms training from scratch particularly over a range of pruning ratios. Theoretical backing is provided via the concept of dynamical isometry which the authors claim is disrupted by the act of pruning. To address this a regularizationbased technique for recovering dynamical isometry in residual convolutional neural networks is proposed showcasing its simplistic implementation yet effective outcomes. The paper sheds light on the critical role of finetuning LR in the performance of pruned models and the significance of dynamical isometry recovery promoting a revisited perspective on the inherited value in structured pruning.  Strong Points 1. Empirical Evidence: The exhaustive array of experiments across different models and datasets robustly supports the authors\u2019 claim that a higher finetuning LR brings significant performance improvements in pruned models over training from scratch. 2. Theoretical Insight: The paper dives into the theoretical underpinnings of why such a discrepancy exists in performance due to LR variations intelligently linking it to the concept of dynamical isometry which is novel in the context of neural network pruning. 3. Practical Solution: The introduction of a simple yet effective regularizationbased method for recovering dynamical isometry is a commendable contribution. This adds a pragmatic tool for practitioners in the field potentially enhancing the performance of pruned models.  Weak Points 1. Generalizability Across Architectures: While the paper presents results on multiple networks it focuses primarily on CNNs. The impact of the proposed methods on other architectures like RNNs or Transformers is not addressed which could limit the applicability of findings. 2. Complexity of Implementation: Despite claiming the simplicity of the proposed regularization technique the practical aspects of implementing and tuning this approach in different settings (networks tasks) were not extensively discussed. 3. Lack of Broader Discussion on Limitations: The paper could benefit from a more comprehensive discussion on the limitations or potential pitfalls of prioritizing dynamical isometry recovery in the pruning process including any tradeoffs it might entail.  Questions and Feedback for the Author 1. Could the authors elaborate on how their findings might apply or extend to architectures beyond CNNs such as recurrent neural networks or Transformer models 2. More detailed guidance on finetuning the proposed regularization technique for dynamical isometry recovery across various setups would enrich the paper. Could the authors provide insights on optimizing this approach for different network complexities 3. A broader discussion on any observed or theoretical limitations of focusing on dynamical isometry recovery in the pruning process would be valuable. Are there circumstances where this approach might inadvertently compromise other aspects of network training or performance  Review Summary The paper effectively challenges the viewpoint that structured pruning benefits little from inheriting weights offering substantial empirical and theoretical contributions to support its claims. The emphasis on finetuning LRs role and dynamical isometry recovery presents valuable insights potentially reshaping how pruning methods are conceptualized and implemented. However the discussion could be deepened by exploring the applicability of findings across a broader range of neural architectures and providing more practical details on the proposed regularization technique. Moreover a nuanced consideration of the limitations or tradeoffs associated with the authors methods would present a more balanced view. Overall due to its strong empirical evidence theoretical contributions and practical solutions the paper represents a significant step forward in understanding and improving structured neural network pruning.", "z7DAilcTx7": " Summary The paper presents a novel framework for adversarial training termed as Adversarial Transport Framework. This method considers the adversarial examples distribution as a slight shift from the original distribution and addresses adversarial training within the scope of distributional robustness optimization (DRO) with an \\xe2\\x88\\x9eWasserstein constraint. By establishing a formal connection between adversarial training and optimal transport the authors propose using an entropic regularizer for distributional robustness in adversarial training. The key contribution lies in proving the existence and uniqueness of an optimal distribution of adversarial examples for a given classifier which can be sampled via Langevin Monte Carlo. This new training technique compared to the standard baseline results in classifiers that offer enhanced robustness and accuracy along with significant speed improvements during training on MNIST and CIFAR10 datasets.  Strong Points 1. Theoretical Foundation: The paper lays out a solid theoretical foundation by connecting adversarial training with distributional robustness optimization and optimal transport theory. This connection allows for a richer understanding of adversarial examples and offers a new perspective on adversarial training. 2. Innovation in Training Method: Utilizing entropic regularization and Langevin Monte Carlo sampling for adversarial training is innovative. This method is both practically effective as evidenced by empirical results and theoretically sound. 3. Significant Performance Improvements: The proposed framework not only improves the robustness and accuracy of the classifiers but does so with considerable speedups in training time (\u00d7200 for MNIST and \u00d78 for CIFAR10) which is critical for practical adoption.  Weak Points 1. Assumptions and Practicality: Some assumptions such as the condition for \\xe2\\x88\\x9eWasserstein distance and the specific form of coupling measures might limit the general applicability or practical feasibility in different contexts or on more complex datasets. 2. Computational Complexity: While the paper highlights speedups in training there is less emphasis on the computational complexity and resources required for implementing Langevin Monte Carlo sampling in practice which could be a barrier for wider adoption. 3. Lack of Broader Evaluation: The evaluation is limited to MNIST and CIFAR10 datasets against specific types of adversarial attacks. A broader evaluation across more varied datasets and against a wider array of attack types would strengthen the claims of robustness and generalizability.  Questions and Feedback to Author 1. Beyond \\xe2\\x88\\x9eWasserstein: How does the framework adapt or perform when considering other forms of distances or metrics especially in datasets where the \\xe2\\x88\\x9enorm might not fully capture the adversarial perturbations 2. Scalability to Larger Datasets: Could the authors provide insight into the scalability of the Adversarial Transport Framework for larger and more complex datasets beyond MNIST and CIFAR10 3. Comparison with Recent AT Methods: How does this framework compare to other recent advances in adversarial training particularly those that might use different forms of regularization or optimization strategies  Review Summary The Adversarial Transport Framework introduces a refreshing perspective on adversarial training by leveraging concepts from distributional robustness optimization and optimal transport. This work stands out for its theoretical rigor and the practical efficacy of the proposed training method which significantly enhances classifier robustness and training efficiency. However the assumptions made and the scope of the evaluation may limit the immediate generalization of the findings. Further exploration and comparative analysis particularly in more diverse and challenging settings would be beneficial. Despite these caveats the paper presents a compelling contribution to the field of adversarial machine learning offering both novel theoretical insights and tangible improvements in adversarial training methodologies.", "qrdbsZEZPZ": " Peer Review of \"Certified Robustness of Differentially Private Federated Learning Against Poisoning Attacks\"  1. Summary This paper investigates the relationship between differential privacy (DP) settings in federated learning (FL) and the robustness of FL against poisoning attacks. In particular it explores whether the inherent privacy features of DPFL can certify robustness against these adversarial manipulations. The study introduces novel randomization mechanisms and analyses to enhance the differential privacy in FL at both user and instance levels. It also proposes two robustness certification criteria: certified prediction and certified attack cost under different attack scenarios. The theoretical contributions are supported by extensive experiments across various datasets demonstrating the tradeoff between privacy protection utility loss and certified robustness against poisoning attacks.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novel Approach:  The paper addresses a significant gap in research by providing a theoretical framework to certify the robustness of DPFL systems against poisoning attacks leveraging differential privacy properties.  Comprehensive Theoretical Analysis:  The proposed criteria for certified robustness and the rigorous mathematical proofs provide a strong theoretical foundation for the study.  Extensive Experimental Validation:  The authors conducted thorough experiments across multiple datasets validating the theoretical insights and demonstrating the practical applicability of their findings. Weak Points:  Tradeoff Consideration:  While the paper discusses the tradeoff between privacy guarantees and certified predictions it seems to lack a deeper analysis of how to optimally balance these aspects for different practical scenarios.  Generalization of Results:  The robustness certification and experimental validations are conducted under specific attack models and DP settings. Its somewhat unclear how these results generalize beyond the studied scenarios or datasets.  Clarity and Accessibility:  The paper is heavily theoretical which makes it less accessible to practitioners or researchers unfamiliar with the technical aspects of DP or FL. Simplifying the explanations or providing more intuitive insights could enhance its readability and impact. Questions and Feedback:  How do the proposed randomization mechanisms compare against existing methods in terms of computational complexity and scalability especially in largescale FL systems  The paper could benefit from a discussion on potential limitations or side effects introduced by the proposed approaches such as increased training time or degraded model performance in nonadversarial settings.  Could the authors elaborate on how their findings might adapt or extend to other types of adversarial attacks not specifically considered in this paper such as evasion attacks or inference attacks  Providing practical guidelines or heuristics for selecting the DP parameters and noise levels considering the tradeoffs between privacy utility and robustness certification would be greatly beneficial for applying these insights in realworld scenarios.  3. Review Summary Overall this paper makes a significant contribution to the field of differential privacy and federated learning by exploring the relationship between DP guarantees and robustness against poisoning attacks. The theoretical analysis is sound and wellsupported by experimental results illustrating the practical applicability of the proposed methods. However the paper could be improved by addressing the optimal balance between privacy and utility making the results more accessible and generalizable and providing more guidance for practical implementation. Despite these areas for enhancement the innovative approach and comprehensive validation make this work an important step forward in developing secure and trustworthy federated learning systems.", "hcMvApxGSzZ": " 1. Summary The paper introduces a novel algorithm for image steganography named Fixed Neural Network Steganography (FNNS) which leverages the sensitivity of neural networks to tiny perturbations to hide secret messages in images with significantly lower error rates compared to previous methods. The method achieves 0 error reliably for hiding up to 3 bits per pixel (bpp) of secret information evades detection by existing steganalysis systems and can be adapted to resist neural networkbased steganalysis. The technique is particularly useful in applications requiring the encryption of secret messages offering an example use case for anonymized and secure image sharing. The approach contrasts with prior works by not requiring the training of encoderdecoder networks on specific datasets but rather utilizing adversarial perturbations or pretrained networks for message hiding.  2. Strong and Weak Points Questions and Feedback Strong Points: 1. Innovative Approach: The use of adversarial perturbations to hide information within images is a novel application of adversarial machine learning concepts outside their typical use case of fooling classifiers. 2. High Accuracy: Achieving a 0 error rate for up to 3 bpp is a significant achievement that surpasses current stateoftheart methods making FNNS a potentially transformative approach for steganography. 3. Robustness: The methods ability to evade existing steganalysis methods including neural networkbased systems adds a layer of security and practical applicability for realworld scenarios. 4. Versatility: FNNSs compatibility with pretrained encoderdecoder networks for further error reduction is an excellent approach offering flexibility and ease of integration with existing deep learning steganography solutions. Weak Points: 1. Optimization Time: The longer optimization times required for FNNS especially for FNNSR (random network) may limit its realtime applications. However the tradeoff might be acceptable for highsecurity applications. 2. JPEG Compression Vulnerability: While attempts have been made to increase robustness against JPEG compression achieving high bit rates with low error after compression remains a challenge. Questions and Feedback: 1. Scalability: How does FNNS scale with larger images and messages Are there practical limitations in terms of image size or message length that impact its applicability 2. Error Rates for Higher BPP: The paper successfully demonstrates 0 error for up to 3 bpp. What are the error rates when attempting to hide messages at higher bpp say 4 or more This information could help gauge the methods limits. 3. Impact of Image Content: Does the content of the image (e.g. complexity texture) affect the performance of FNNS especially in terms of imperceptibility and error rates Feedback: The paper could benefit from an expanded discussion on the potential impacts of using different color spaces or nonspatial spaces for optimization. Exploring these avenues could address some of the weaknesses noted (e.g. JPEG compression vulnerability) and open new paths for optimization and application of FNNS.  3. Review Summary The paper proposes an innovative and highly effective method for image steganography that outperforms existing techniques in error rate robustness against steganalysis and adaptability to different use cases. This research represents a significant step forward in the field of steganography leveraging adversarial machine learning insights to achieve remarkable results. Despite some limitations related to optimization time and challenges with JPEG compression FNNS introduces a promising direction for secure and undetectable message hiding within images. The paper is wellstructured with clear demonstrations of the methods superiority over existing approaches and thoughtful consideration of potential use cases particularly in privacysensitive applications. I recommend this paper for publication as it contributes valuable insights and tools to the field of steganography and opens numerous avenues for further research and application development.", "nD9Pf-PjTbT": " Paper Summary The paper investigates the convergence behavior of the generalized belief propagation (BP) algorithm when applied to ferromagnetic Ising models on graphs with motifs such as triangles and loops. The primary focus is on demonstrating that with proper initialization (all messages initialized to one) generalized BP converges to the global maximum of the Bethe free energy. This result is significant as it extends existing theories about the performance of BP on loopy graphs to more complex structures characterized by higherorder interactions among vertices within motifs. The paper also delves into the mathematical foundations of Ising models on graphs with motifs the formulation of the Bethe free energy for such models and the optimization landscape related to the convergence properties of the BP algorithm.  Strong Points 1. Novelty: The paper addresses a gap in existing research by focusing on the behavior of BP on graphs with motifs a more complex structure compared to previously studied treelike or simple loopy graphs. 2. Mathematical Depth: The authors provide a rigorous theoretical framework including proofs and lemmas to support their findings about the convergence of generalized BP. This mathematical rigor strengthens the paper\u2019s contributions. 3. Practical Relevance: Understanding BPs behavior on complex graph structures has significant implications for various fields such as computer vision bioinformatics and social network analysis where Ising models find extensive application.  Weak Points 1. Empirical Validation: The paper lacks empirical results to complement its theoretical findings. Experiments demonstrating the convergence behavior on realworld datasets or synthetic examples matching their theoretical setup would provide practical insights into the algorithms performance. 2. Comparison to Alternative Methods: While the focus on BP is justified the paper could benefit from a discussion comparing BP\u2019s performance to other inference methods in the context of graphs with motifs. 3. Accessibility: The high level of mathematical detail while a strength might also limit the papers accessibility to readers not wellversed in advanced statistical physics or graph theory.  Questions and Feedback for the Author 1. Empirical Evidence: Could the authors provide empirical examples or simulations that validate the theoretical predictions made about the generalized BPs convergence on graphs with motifs 2. Scalability: How does the convergence behavior and computational complexity of the generalized BP algorithm scale with the size and complexity of the graph (e.g. number of nodes motifs) 3. Initialization Sensitivity: The paper mentions that convergence is achieved under a certain initialization. Could the authors discuss the sensitivity of the algorithms performance to different initial conditions 4. Applicability Limits: Are there any practical limitations or constraints on the applicability of the generalized BP algorithm to specific types of Ising models or graph structures  Review Summary The paper presents a significant step forward in understanding the generalized belief propagation algorithms behavior on complex graph structures with motifs. Its theoretical contributions grounded in rigorous mathematical analysis extend the knowledge frontier on BPs convergence properties particularly for ferromagnetic Ising models. However the absence of empirical validation and discussion on the algorithms performance compared to alternative methods leave room for further development. Including experimental results would greatly enhance the papers contributions offering practical insights into the algorithms realworld applicability and performance. Furthermore a discussion on the initialization sensitivity and scalability of the algorithm could provide a more comprehensive overview of its practical implications. Overall the papers strengths in novelty and mathematical depth are noteworthy but addressing the identified gaps could make it an even stronger contribution to the field.", "qyTBxTztIpQ": " Peer Review  Summary of the Paper This paper introduces CrowdPlay an innovative framework that bridges the gap between crowdsourcing and reinforcement learning (RL) for generating largescale human demonstration data to advance research in imitation learning (IL) and offline learning. By leveraging crowdsourcing methods CrowdPlay collects a diverse dataset of human gameplay in Atari 2600 games. This dataset includes multimodal behavior and multiagent data from both humanhuman and humanAI interactions. A significant aspect of CrowdPlay is its ability to provide realtime feedback to participants aiming to enhance the quality of the collected data. The authors also delve into the design and impacts of different incentive structures on data quality. Furthermore the paper presents benchmarks using the dataset across various offline learning algorithms offering insights into the challenges and potentials of learning from human demonstrations.  Strong Points 1. Novelty and Utility: The CrowdPlay framework and dataset represent a significant addition to research resources in RL particularly in the context of IL and offline RL. Its opensource nature and compatibility with standard RL environments like OpenAI Gym enhance its accessibility and potential for widespread adoption. 2. Comprehensive Approach: The authors have meticulously designed the framework to address several key aspects \u2013 multimodal and multiagent data collection incentive design realtime feedback and integration with existing RL environments. 3. Indepth Analysis of Incentives: The detailed study on the impact of various incentive models on data quality is particularly commendable. This analysis provides valuable insights that can guide future crowdsourcing endeavors in similar contexts. 4. Rigorous Benchmarking: The benchmark results contribute to understanding the complexities involved in offline learning from human demonstrations revealing clear challenges and setting the stage for future improvements in algorithm design.  Weak Points and Questions 1. Dataset Balance and Representation: Despite efforts to collect diverse data the paper notes some imbalance in the dataset especially in multimodal Space Invaders data. How might this imbalance affect the generalizability of IL and offline RL models trained on this dataset 2. Comparison with Other FrameworksDatasets: While the paper positions CrowdPlay as a novel contribution a more explicit comparison with existing datasets or frameworks especially in terms of data diversity and size could further clarify CrowdPlays unique value proposition. 3. Algorithm Performance: The benchmark results show a significant gap between human performance and the performance of offline learning algorithms. Can the authors speculate on the primary factors contributing to this gap Are there limitations in the current offline RL algorithms or do these results indicate inherent challenges in learning from diverse human behaviors  Review Summary and Recommendation The paper presents CrowdPlay a pioneering framework that effectively merges crowdsourcing with RL environments for the generation of a significant dataset comprising human gameplay demonstrations. Its focus on multimodal and multiagent settings alongside an extensive analysis of incentive structures sets a robust foundation for advancing IL and offline RL research. While the framework and dataset are undeniably valuable further discussion on dataset balance a more comparative analysis with existing tools and deeper investigation into the challenges faced by offline RL algorithms in mimicking human gameplay would enhance the papers impact. Additionally exploring avenues to address the highlighted algorithmic challenges and dataset limitations could be beneficial for future work. Given the contributions novelty utility and the comprehensive effort in the design implementation and analysis phases I recommend this paper for publication. The feedback and questions raised should be considered for future work or as potential areas for revision to strengthen the current submission.", "vcUmUvQCloe": "Summarize the Paper The paper introduces a novel concept the joint Shapley value extending traditional Shapley values a key component in game theory and explainable AI for assessing feature importance within models. It aims to measure the collective contribution of a set of features to a models prediction upholding Shapleys original axioms while incorporating sets of features rather than treating them individually. This approach addresses the limitations of conventional interaction indices and individual significance measures which may not accurately represent feature importance in the presence of correlation. The paper rigorously establishes the mathematical foundation of the joint Shapley value proving its uniqueness and demonstrating its practical application through game theoretical models and machine learning attribution problems particularly in binary feature scenarios offering a presenceadjusted global value for more intuitive interpretation. Strong Points 1. Theoretical Rigor: The extension of Shapleys axioms to sets of features and the formal proof of the uniqueness of joint Shapley values is mathematically rigorous. This foundational work solidifies the paper\u2019s contribution to the field. 2. Practical Relevance: By addressing the limitations of existing feature importance measures in correlated feature scenarios the paper tackles a significant challenge in explainable AI enhancing interpretability and reliability. 3. Comprehensive Experiments: The experiments spanning game theory models and application to real and simulated datasets (Boston housing movie reviews) illustrate the utility of joint Shapley values and allow for a direct comparison with existing measures. Weak Points 1. Computational Complexity: The paper partially addresses computational concerns but does not fully explore efficiency optimizations or provide benchmarks against other methods which may be critical for application in largescale problems. 2. Limited Exploration of Extensions: While the paper lays a strong theoretical foundation it could benefit from a deeper examination of the potential extensions or variations (beyond binary features) and their implications for various types of models. 3. Lack of Discussion on Limitations: The paper robustly presents the innovation and application of joint Shapley values but could provide a clearer discussion on potential limitations or scenarios where joint Shapley values may not offer significant advantages over traditional methods. Questions and Feedback to the Author 1. Computational Efficiency: Could the authors elaborate on the computational efficiency of calculating joint Shapley values especially in comparison to conventional Shapley values and interaction indices Are there specific scenarios where the computational cost may outweigh the benefits 2. Extension beyond Binary Features: The paper presents a presenceadjusted global value for binary features. How might the joint Shapley value concept be extended or adapted to continuous features or more complex data types 3. Impact on Model Interpretability: How do the joint Shapley values alter the interpretation of model predictions in practice Could the authors provide more detailed case studies or examples where joint Shapley values offer substantial new insights Review Summary The paper is a significant contribution to the field of explainable AI addressing a critical gap by extending Shapley values to sets of features which is pivotal for models with correlated features. The theoretical rigour combined with practical experiments underscores the potential of joint Shapley values to enhance model interpretability. However to strengthen the papers impact and applicability the authors should address the computational efficiency in more depth explore the formulation\u2019s extensions beyond binary features and discuss potential limitations. Remarkably the introduction of a presenceadjusted global value and the application across varied datasets demonstrate the utility of joint Shapley values in providing more intuitive and comprehensive interpretations of model predictions. As such this paper advances our understanding and methodology for assesssing feature importance in complex models paving the way for further research and application in explainable AI.", "vPK-G5HbnWg": " Review of \"Parallelizable Attentionbased Computation structure Encoder (PACE) for Directed Acyclic Graph (DAG) Optimization\"  Summary This paper introduces PACE a novel approach towards the optimization of Directed Acyclic Graphs (DAGs) structures used in applications like neural architecture search and probabilistic graphical model learning. PACE addresses the limitation of sequential processing inherent in current popular DAG encoders by proposing a parallel processing strategy. Utilizing a Transformerbased architecture underpinned by a positional encoding scheme tailored for DAGs which they named dag2seq PACE is capable of embedding DAG structures in parallel. The paper demonstrates the superiority of PACE over existing approaches through various experiments highlighting its enhanced encoding efficiency the generation of smooth latent DAG spaces advantageous for optimization routines and significant speed improvements in training and inference.  Strong Points 1. Innovative Parallelization Strategy: The introduction of a parallelizable encoder for DAGs which bypasses the sequential processing limitations of existing models is highly commendable. This addresses a core challenge in DAG optimization and is likely to inspire further research in this direction. 2. Comprehensive Experimental Validation: The paper presents extensive experiments comparing PACE with stateoftheart DAG encoders and graph encoders for undirected graphs. The findings convincingly demonstrate PACEs enhanced performance in generating smooth latent spaces and boosting computational efficiency. 3. Theoretical Contribution: The development of the dag2seq framework which efficiently transforms DAGs into sequences while preserving their unique characteristics represents a significant theoretical advancement. The frameworks ability to injectively encode DAGs ensuring distinct DAGs produce distinct sequences is particularly noteworthy.  Weak Points 1. Computational Complexity Clarification: While the paper highlights the speed improvements with PACE theres little discussion on the computational complexity of PACE itself especially in comparison to sequential models. A deeper exploration into the tradeoffs involved including memory consumption would provide a more balanced view of PACEs practical applicability. 2. Limited Analysis on Scalability: The experiments while thorough do not explicitly address PACEs scalability especially when dealing with very large DAGs. Further experiments highlighting performance in such scenarios would enhance the papers contributions. 3. Ablation Study Scope: The ablation study effectively demonstrates the importance of the attention mask and the dag2seq framework. However it would be beneficial to include more variations such as the impact of varying the number of Transformer blocks or the embedding dimensionality to better understand the models sensitivity to these parameters.  Questions and Feedback to the Author 1. Could the authors provide more insights or theoretical analysis on the computation complexity of PACE compared to existing DAG encoders 2. Are there scenarios or specific types of DAG structures where PACE might not perform as well A discussion on potential limitations or challenges in applying PACE to varying DAG complexities could be enlightening. 3. How does PACE handle very large DAGs where the number of nodes and edges might significantly impact memory and computational requirements An exploration into the scalability aspects of PACE would be valuable.  Review Summary PACE represents a significant step forward in the optimization of DAG structures offering a novel parallelizable approach that substantially improves upon the efficiency and effectiveness of existing encoders. The introduction of the dag2seq framework is a notable theoretical advancement enabling the injective encoding of DAGs into sequences. The comprehensive experimental validation further underscores PACEs superior performance in generating conducive latent spaces for optimization routines and achieving significant speed improvements. However the paper could benefit from a more detailed discussion on the computational complexities involved an indepth analysis of PACEs scalability and an expanded ablation study to underscore the models robustness under varying configurations. Addressing these aspects would enhance the papers contributions and provide a more comprehensive understanding of PACEs potential and limitations. Overall the presented work is a substantial contribution to the field of DAG optimization promising to inspire further research and applications. With further exploration and clarification on the points raised this paper has the potential to set a new standard in the domain.", "l3SDgUh7qZO": " Review of \"SphereFace2: Bridging the Training and Evaluation Gap in Deep Face Recognition via Binary Classification Framework\"  1. Summary of the Paper The document presents a novel deep face recognition (FR) framework called SphereFace2 aimed at addressing the limitations of existing deep FR methods that predominantly use softmaxbased multiclass classification for training. The authors identify a fundamental discrepancy between the training (multiclass classification) and evaluation (pairwise verification) stages in current methods leading to suboptimal empirical performance. Specifically they critique the \u201ccompetitive\u201d nature of softmax normalization and its consequent limitations such as the closedset assumption during training which does not accurately reflect the openset nature of realworld FR tasks. SphereFace2 introduces a binary classification training framework avoiding softmax normalization and instead employing a \u201conevsall\u201d approach where each identity is treated as a separate binary classification problem. This new framework effectively allows the model to bridge the gap between training and evaluation treating training as an openset learning problem and thereby improving the generalizability of the model. The authors propose a specifically designed loss function and articulate general principles for designing effective loss functions within this binary classification framework. Experiments on popular benchmarks demonstrate that SphereFace2 consistently outperforms stateoftheart deep FR methods.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The shift from a multiclass to a binary classification framework presents an original approach to solving the discrepancy between training and evaluation in deep FR. This could significantly impact future research directions in the field.  Comprehensive Experimentation: The paper thoroughly evaluates the proposed SphereFace2 on multiple benchmarks providing strong empirical evidence of its effectiveness over stateoftheart methods.  General Design Principles: Beyond proposing a specific solution the paper outlines general principles for designing loss functions within the binary classification framework adding valuable insights for future research. Weak Points:  Complexity and Implementation Details: The paper introduces a complex framework with multiple components and hyperparameters (\u03bb r m t). However it lacks detailed discussion on the ease of implementation and sensitivity to hyperparameter settings which might be crucial for practitioners.  Comparison with Similar Approaches: While the paper mentions related works it does not provide a direct and detailed comparison against other binary classification approaches in deep FR which could strengthen the argument for SphereFace2\u2019s superiority.  Impact of Binary Classification on Training Efficiency: Transitioning to binary classification significantly increases the number of binary classifiers needed. The paper could benefit from a discussion on the implications for training efficiency and computational resources. Questions and Feedback:  Could the authors elaborate on how SphereFace2 handles the increase in the number of classifiers due to the binary classification approach particularly concerning training time and computational resource requirements  How sensitive is SphereFace2 to its hyperparameters (\u03bb r m t) and what recommendations do the authors have for their selection in practice  The methodology primarily targets the discrepancy between training and evaluation scenarios in deep FR. Could the authors discuss the potential of SphereFace2 in domains other than face recognition where similar discrepancies exist  3. Review Summary SphereFace2 introduces a compelling novel framework for deep face recognition effectively addressing key limitations of the prevalent multiclass classification approaches. By proposing a binary classification framework that better aligns with the openset nature of realworld face recognition tasks this work stands to make a substantial impact on future research and applications in the field. The outlined general principles for designing loss functions within this framework further enhance the paper\u2019s contribution. However the complexity of the framework potential challenges in implementation and the impact on training efficiency warrant further discussion. A more detailed comparison with other binary or alternative classification approaches could also strengthen the papers position. In conclusion SphereFace2 represents a significant and innovative step forward in deep face recognition research with convincing empirical results supporting its effectiveness. Future work could benefit from addressing the raised points to fully harness the potential of this promising approach in wideranging applications beyond facial recognition.", "zfmB5vgfaCt": " 1. Summary of the Paper This paper investigates a novel security threat to Neural Machine Translation (NMT) systems characterized by adversarial efficiency attacks. The authors identify an inherent property of NMT systems where their computational efficiency is determined by the output length rather than the input length. Leveraging this property the authors introduce an adversarial attack method TranSlowDown which aims to significantly degrade the efficiency of NMT systems by applying minimal unnoticeable perturbations to the input text. The paper demonstrates the effectiveness of TranSlowDown through exhaustive experiments on three public NMT systems: Google T5 Facebook Fairseq and HelsinkiNLP translators. The results showcase substantial increases in response latency and computational resource consumption including battery power on mobile devices. This study emphasizes the need for future research to enhance the efficiency robustness of NMT systems against such adversarial attacks.  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Originality: The paper addresses a less explored yet crucial aspect of NMT system security  efficiency robustness against adversarial attacks. This novel perspective enriches the current understanding of NMT systems vulnerabilities.  Significance of Findings: The paper unveils a significant security threat where adversarial inputs can drastically reduce the efficiency of leading NMT systems. The potential implications for both commercial applications and endusers highlight the urgency of addressing these vulnerabilities.  Thorough Evaluation: The experimental design encompassing three distinct NMT systems and both tokenlevel and characterlevel attacks provides a comprehensive validation of the proposed methods effectiveness. Weak Points:  Limited Discussion on Defense Mechanisms: While the paper successfully establishes the presence of efficiencyoriented adversarial threats it offers limited insight into potential defense strategies. A more balanced treatment would enhance its relevance.  Generalizability of Findings: The study focuses on three specific NMT systems. Expanding the evaluation to include more diverse architectures and settings could reinforce the generalizability of the findings and conclusions.  Impact Assessment: The practical implications of such attacks in realworld scenarios especially beyond battery consumption on mobile devices could be further explored. Understanding these aspects would add depth to the studys significance. Questions and Feedback: 1. Defense Strategies: Could the authors elaborate on potential defense mechanisms against efficiencyoriented adversarial attacks Understanding whether existing accuracyfocused defenses can be adapted or if entirely new strategies are needed would be beneficial. 2. Generalizability: How confident are the authors that TranSlowDowns effectiveness extends beyond the tested NMT systems Testing on systems with significantly different architectures could validate the universality of the attack. 3. Realworld Impact: Beyond battery usage on mobile devices what are other potential realworld consequences of efficiency attacks on NMT systems Insights into these would underscore the broader significance of the findings.  3. Review Summary The paper presents a pioneering analysis of an underexplored yet critical vulnerability of NMT systems to efficiencyoriented adversarial attacks. Through developing and systematically evaluating TranSlowDown the authors convincingly demonstrate the feasibility and potential consequences of such attacks. The significant increase in computational resource demands showcased through exhaustive experiments underscores the need for further research on defensive measures. However the paper could be enhanced by a deeper examination of defense strategies broader testing to affirm findings generalizability and a more comprehensive exploration of realworld impacts. Despite these areas for improvement the paper substantially contributes to the field by highlighting a novel security threat and setting the stage for future research on bolstering NMT systems efficiency robustness.", "iRCUlgmdfHJ": " Summary The paper investigates the feature representation bottleneck of Deep Neural Networks (DNNs) through the lens of input variables multiorder interactions. It uncovers a widespread phenomenon across different DNN architectures and tasks where DNNs tend to capture both very simple and highly complex interactions but struggle with those of intermediate complexity. This discrepancy highlights a significant cognitive gap between DNNs and human visual processing as humans can extract meaningful information from moderate levels of complexity that DNNs overlook. To explain and theoretically prove the existence of this representation bottleneck the authors introduce a novel methodology using multiorder interaction utilities. Furthermore they suggest modifications to the training process incorporating losses to either promote or penalize learning interactions of specific complexities. These experimental interventions validate the theory and demonstrate the potential for refining DNNs feature representation capabilities.  Strong Points 1. Theoretical Contribution: The paper effectively bridges theory with practical observations by providing a theoretical framework for understanding DNNs\u2019 representation bottlenecks. The utilization of multiorder interaction utilities is a novel approach to analyzing the complexity of features encoded by DNNs. 2. Comprehensive Experiments: The experiments are welldesigned and thorough covering a variety of DNN architectures and datasets. The analysis helps in generalizing the observation of the representation bottleneck phenomenon across different DNNs. 3. Practical Implications: By introducing losses that influence the learning of specific orders of interactions the paper not only highlights the problem but also suggests a potential pathway for enhancing DNNs representation capabilities making it a valuable contribution toward the goal of achieving more humanlike processing in DNNs.  Weak Points 1. HumanDNN Cognition Comparison: While the paper provides insightful observations on the representation gap between DNNs and humans it lacks a rigorous comparison or quantification of this cognitive gap. The explanatory power might have been stronger with detailed experiments or evidence showing precisely how human cognition differs in processing the complexity of visual interactions. 2. Generalization of Findings: Although experiments are extensive the general applicability of the proposed losses in realworld scenarios or complex tasks beyond the tested datasets and architectures may require further exploration to ensure that these findings are broadly applicable. 3. Computational Efficiency: The methods scalability and computational efficiency are not clearly addressed. Given the increased complexity with calculating multiorder interactions particularly for large networks and datasets the practicality of implementing the proposed losses in training DNNs might be challenging. 4. Adversarial Vulnerability: The observation that DNNs mainly encoding highorder interactions are more susceptible to adversarial attacks is fascinating. However a deeper analysis into why this vulnerability occurs and how it might be mitigated would enhance the papers contribution to creating robust DNNs.  Questions and Feedback 1. Could the authors provide more insights or data on the human cognitive process for understanding intermediate complexity which might help in drawing a clearer parallel with DNNs\u2019 learning patterns 2. How does the computational cost of implementing the proposed losses compare with traditional training methods especially for larger networks and datasets 3. Are there potential methods or strategies to mitigate the increased adversarial vulnerability observed in DNNs encoding highorder interactions  Review Summary This paper presents an innovative and thoughtprovoking exploration into the feature representation bottlenecks of DNNs shedding light on an essential aspect of artificial intelligence that diverges from human cognitive processes. The theoretical grounding and empirical evidence supporting the existence of this phenomenon are strong and constitute a significant step forward in understanding the complexities of DNN representation capabilities. However some areas such as the comparison with human cognition potential scalability issues and adversarial vulnerability could benefit from further clarification and exploration. Despite these minor shortcomings the papers strengths in theory experimentation and potential implications for improving DNN design are compelling. Therefore the paper makes a valuable contribution to the field and opens up new avenues for research into making DNNs\u2019 learning processes more efficient and aligned with human cognitive patterns.", "zKbMQ2NY1y": " Peer Review for \"Enhancing BlackBox Transferability of Adversarial Attacks via Augmented Intermediate Level Attack (AugILA)\"  Summary The paper introduces AugILA a novel approach that extends the Intermediate Level Attack (ILA) framework by incorporating image augmentation techniques to improve the transferability of adversarial attacks across different deep neural network models. The authors systematically explore the effects of common image transformation operations and propose a combination of augmentations including random cropping reverse adversarial update and attack interpolation to enhance attack transferability. Experimental results show significant improvements in attack success rates against a range of undefended models with varying perturbation budgets thereby outperforming both the original ILA and other stateoftheart transferbased attack methodologies.  Strong Points 1. Novel Approach: The paper presents a unique perspective by focusing on the augmentation of input images in the context of ILA which is a relatively unexplored area. The idea of using reverse adversarial updates and attack interpolation is innovative and contributes to the novelty of the work. 2. Extensive Evaluation: The authors conduct a thorough experimental analysis comparing AugILA against not only the baseline ILA method but also other leading transferbased attack strategies. The use of nine different models for evaluation and consideration of both small and large perturbation budgets add to the robustness of the findings. 3. Clear Presentation: The paper is wellstructured with a clear progression from the motivation and related work to the detailed explanation of the proposed method and experimental analysis. The inclusion of visualizations for adversarial examples and detailed tables for attack success rates aids in understanding the impact of AugILA.  Weak Points 1. Lack of Discussion on Computational Overhead: While the paper demonstrates the effectiveness of AugILA there is a notable absence of discussion regarding its computational efficiency. The introduction of multiple augmentations could potentially increase the time and resources required for generating attacks which is a crucial aspect for practical applications. 2. Limited Exploration of Augmentation Techniques: The paper focuses on a specific set of image transformation operations (e.g. random cropping and reverse adversarial update). Expanding the scope to include more varied and potentially sophisticated augmentation techniques could uncover further improvements in attack transferability. 3. Potential for Defended Model Analysis: The primary focus is on the attack success rates against undefended models. Including a comprehensive analysis on defended models could provide a more complete picture of AugILA\u2019s effectiveness considering the constant arms race between attack methodologies and defense mechanisms.  Questions and Feedback to the Author 1. Computational Efficiency: Could the authors provide insights into the computational overhead introduced by the proposed augmentations Comparing the runtime and resource requirements of AugILA with ILA and other methods would be beneficial. 2. Broader Set of Augmentations: What was the rationale behind selecting the specific augmentation techniques described in the paper Have other augmentation methods been explored and if so why were they not included in the final proposal 3. Performance on Defended Models: The paper mentions experiments on defended models in the appendices. Could the authors elaborate on the performance of AugILA in these contexts particularly against stateoftheart defense mechanisms  Review Summary The paper makes a significant contribution to the field of adversarial machine learning by introducing AugILA a method that enhances the transferability of adversarial attacks via augmented image transformations. The approach is novel and the experimental results strongly support the authors claims of AugILA\u2019s superiority over existing methods. However the paper could benefit from a more detailed analysis of the computational efficiency of AugILA an exploration of a broader range of augmentation techniques and additional experiments on defended models to assess the methods comprehensive applicability. Despite these areas for improvement the paper provides valuable insights and constitutes a meaningful advancement in adversarial attack methodologies. Given the strong empirical results and the novelty of the approach I recommend acceptance contingent upon the consideration and potential addressal of the presented weak points and questions.", "zPLQSnfd14w": " Review of \"Generalization Guarantees for Neural Network Metric Learning\"  1. Summary This paper provides an indepth analysis of uniform generalization guarantees for metric learning when using neural networks for nonlinear embeddings. The authors establish a theoretical framework to evaluate how well a model generalizes from training to unseen data divided into two regimes linked to the sparsity of the networks last layer. Key contributions include two novel bounds on the Rademacher complexity of the family of distance functions induced by neural network embeddings. These bounds differ in their dependence on the structural properties of the weight matrices particularly the last layer showing that denser layers result in stronger guarantees. Additionally the paper rigorously empirically evaluates these bounds on conventional datasets (MNIST and 20newsgroups) demonstrating the existence and impact of the dense and sparse regimes in practical scenarios.  2. Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Contribution: The paper successfully addresses a significant gap in the literature by extending generalization guarantees to nonlinear metric learning with neural networks a substantial technical achievement.  Clarity and Depth of Analysis: The dual regime approach (sparse vs. nonsparse) is a nuanced insight that allows for a comprehensive understanding of how network architecture impacts metric learning generalization.  Empirical Validation: The inclusion of empirical results especially the careful analysis of learning regimes across different datasets and architectures provides strong evidence for the practical relevance of the theoretical findings. Weak Points:  Complexity of Notation and Definitions: While necessary for the depth of analysis the papers dense notation and rapid introduction of concepts may pose barriers to accessibility for readers not already wellversed in the field.  Broader Practical Implications: The paper could further explore the practical implications of its findings particularly how they could inform architectural decisions in applied settings beyond the MNIST and 20newsgroups datasets.  Discussion on Limitations: There is a lack of discussion surrounding the limitations of the proposed bounds especially in contexts where data may not adhere strictly to the assumptions made in the analysis. Questions and Feedback: 1. Applicability to Other Neural Network Architectures: How applicable are the derived bounds to modern architectures that employ mechanisms beyond simple depth and layer width such as attention mechanisms or residual connections 2. Sensitivity to Activation Functions: The paper mentions the use of ReLU and leaky ReLU activations. Could the authors comment on how sensitivity to the choice of activation functions might affect the generalization bounds 3. Impact of Regularization: The experiments with the 20newsgroups dataset involved `L2` regularization. It would be beneficial to understand how different regularization strategies (e.g. dropout `L1` regularization) interact with the sparsitydensity regimes and the resulting implications for the bounds.  3. Review Summary The paper represents a significant theoretical advancement in the understanding of generalization in metric learning via neural network embeddings addressing a crucial gap by extending analysis beyond linear embeddings to encompass nonlinear cases. The dualregime approach provides nuanced insight into how the structural properties of neural networks influence their generalization capabilities. The empirical evaluation further underscores the relevance of these insights to practical applications highlighting the occurrence of dense and sparse weight regimes in commonly used datasets and architectures. However the work could be improved by addressing the accessibility of its dense theoretical exposition expanding on the practical implications of its findings and including a discussion on the limitations of the proposed generalization bounds. Questions remain regarding the applicability of these bounds to a broader range of neural network architectures the impact of different activation functions and how various regularization techniques might influence the sparsitydensity of the last layer and by extension the generalization bounds. Overall the paper makes a valuable contribution to the metric learning literature opening new avenues for research in the generalization properties of neural networks in this context. Future work that builds on these foundations to address the posed questions and feedback could further enhance our understanding and ability to harness the power of metric learning in deep learning frameworks.", "gRCCdgpVZf": "Peer Review of \"A Model for ZeroShot Domain Adaptation with MultiDimensional Task Descriptors\" 1. Summary: The paper introduces a novel approach to tackle zeroshot domain adaptation (ZSDA) an area critical for applications where data collection across all possible domain configurations is infeasible due to high costs or practical constraints. The authors formulate each domain as indexed by a multidimensional array and propose a twolayer model comprising a domaininvariant latent representation layer and a domainspecific linear prediction layer structured as a lowrank tensor. This design choice is backed by theoretical analyses providing sample complexity bounds that illuminate how prediction error on unseen domains correlates with the number of observed domains and samples per domain. Empirical evaluations are conducted on modified MNIST GTOS and a fourway fiber sensing dataset showcasing the models superiority over existing ZSDA approaches. 2. Strong Points:  Theoretical Contribution: The paper presents novel theoretical contributions that provide a deeper understanding of ZSDA under multilinear structure assumptions. The explicit sample complexity bounds are particularly valuable for understanding the models efficiency in unseen domain adaptation.  Practical Relevance: The study addresses a highly relevant challenge in machine learning particularly for applications in fields where exhaustive data collection is unrealistic. The specified motivating examples (fiber sensing in smart cities digital pathology) convincingly illustrate the problems significance.  Solid Empirical Validation: The selection of diverse datasets (including realworld applications) for experimentation showcases the models broad applicability and effectiveness in improving generalization to unseen domains compared to baseline and existing stateoftheart methods. 3. Weak Points:  Assumptions and Realizability: While the paper lists assumptions for their theory (section 4) theres a lack of discussion on the practicalities of meeting these assumptions in realworld scenarios. For instance the realizability assumption (w\\xe2\\x88\\x97t and \\xcf\\x95\\xe2\\x88\\x97 belong to the model classes W and \\xce\\xa6) may not hold in all applications.  Complexity and Scalability Concerns: Though theoretical bounds are provided there is limited discussion on the computational complexity of the proposed model especially concerning tensor completion in highdimensional spaces. More insight into scalability and runtime implications would be valuable.  Model Interpretability: Given the models application in domains with potentially high stakes (e.g. digital pathology) discussions around the interpretability of the domaininvariant and domainspecific components would have been beneficial. This is crucial for trust and adoption in practice. 4. Questions and Feedback to the Author:  Could you elaborate on the impact of violations of the realizability assumption on model performance and how such situations could be mitigated in practice  It would be helpful to provide more details on the computational complexity of the tensor completion step and any strategies employed to ensure scalability to highdimensional task descriptors.  Have interpretability aspects of the model been considered especially in terms of understanding learned domaininvariant representations and domainspecific linear predictors 5. Review Summary: This paper represents a significant step forward in zeroshot domain adaptation offering both theoretical and practical insights into tackling this challenging problem. The proposed model leverages a novel multilinear structure to efficiently adapt to unseen domains supported by robust theoretical analyses and compelling empirical evidence across various datasets. However there are areas where further clarification and extension could enhance the work such as addressing realizability assumptions computational complexity and model interpretability. Despite these minor concerns the paper contributes meaningful theoretical foundations and practical strategies for ZSDA likely to spur future research and applications in this domain. Recommendation: Accept with minor revisions addressing the discussed weak points.", "sRZ3GhmegS": " Peer Review of \"Contrastive BERT for Reinforcement Learning (COBERL)\"  Summary of the Paper The paper proposes a novel reinforcement learning (RL) agent Contrastive BERT for RL (COBERL) which integrates a new contrastive loss with a hybrid LSTMtransformer architecture to improve data efficiency in learning from pixelbased inputs in various domains. Utilizing bidirectional masking prediction inspired by BERT and extending the contrastive method RELIC into the time domain COBERL aims to learn robust selfattentionconsistent representations without relying on manually designed data augmentations. The agents efficacy is demonstrated through comprehensive testing across the Atari suite a set of control tasks and a challenging 3D environment showing notable improvements in data efficiency and in many cases performance scores.  Strong Points 1. Innovative Approach: COBERLs integration of contrastive learning and a hybrid LSTMtransformer architecture is a notable innovation addressing the challenge of sample efficiency in RL trained on highdimensional raw inputs. 2. Comprehensive Evaluation: The paper provides a thorough evaluation of COBERL across a diverse set of environments and tasks demonstrating its versatility and effectiveness. 3. EndtoEnd Design: The methods ability to operate endtoend without additional encoders or manual data augmentations is a significant advantage simplifying the training process and making it more accessible.  Weak Points and Questions 1. Comparison with StateoftheArt: While the paper demonstrates COBERLs effectiveness a detailed comparison with the very latest stateoftheart methods in similar domains would provide a clearer picture of its competitive advantages or limitations. 2. Potential for Overfitting: The paper could discuss potential risks of overfitting due to the complexity of the model especially given the extensive use of transformers and LSTMs. 3. Scalability and Computational Efficiency: Further details on COBERLs computational requirements compared to other methods would be beneficial especially considering the potential for increased computational complexity due to the hybrid architecture. 4. Generalization to Other Domains: While the paper showcases COBERLs performance across several domains the effectiveness in other potentially less visuallydriven environments (e.g. abstract decisionmaking tasks) is unclear.  Feedback to the Author  The innovative approach of combining contrastive learning with a hybrid architecture is commendable. Future works could explore modifications to further enhance model robustness and prevent potential overfitting.  A deeper comparison with current stateoftheart methods including those that may have emerged after the development of COBERL will enrich the papers contributions.  It would be insightful to include more detailed analyses on the computational efficiency and scalability of COBERL particularly in comparison to simpler models or those that the paper outperforms.  Exploring the applicability and effectiveness of COBERL in environments that rely less on visual inputs could provide a broader understanding of its generalization capabilities.  Review Summary COBERL represents a significant leap forward in promoting data efficiency and robust learning in reinforcement learning from pixels. Its novel integration of contrastive learning with a hybrid LSTMtransformer architecture addresses critical challenges in the field. However a more detailed comparison with the latest stateoftheart methods discussions on potential risks of overfitting and clarity on computational efficiency are necessary for a more comprehensive evaluation. Furthermore investigating its applicability across a broader spectrum of RL domains could highlight its versatility and limitations more accurately. Despite these areas of improvement COBERLs demonstrated effectiveness across diverse tasks and its endtoend design present an exciting step towards more efficient and generalizable RL agents.", "mqIeP6qPvta": "Peer Review of \u201cFoveated Transformer for Efficient Object Classification\u201d 1. Summarize the paper: The paper introduces a novel model named FoveaTer (Foveated Transformer) inspired by the biological concept of foveated vision found in humans and many animals. This model incorporates the principle of deploying varying spatial resolution across the visual field  higher at the center (fovea) and lower at the periphery. The FoveaTer exploits this concept by utilizing a transformer architecture coupled with a mechanism that pools image features based on squared pooling regions simulating the foveated visions efficiency. The model uses attentionbased mechanisms to decide on subsequent fixation locations on the image dynamically focusing computational resources on regions of interest to decide on object classifications. It implements a confidence threshold to limit scene exploration to essential fixations. This proposed method demonstrates computational efficiency by achieving similar accuracy to fullresolution models with significantly reduced computation and showing enhanced robustness against adversarial attacks. The performance and advantages were validated on the ImageNet100 dataset comparing the Foveated model against a conventional fullresolution model. 2. Strong and Weak Points Questions and Feedback: Strong Points:  Innovativeness: The blend of concepts from biological vision and cuttingedge transformer architecture is creative and unexplored. This innovation potentially opens up new avenues in efficient image processing.  Computational Efficiency: Demonstrating that the Foveated model requires significantly fewer transformer computations (35) and overall computations (73) in comparison to the fullresolution model without compromising accuracy is a significant achievement.  Adversarial Robustness: The improved resistance against adversarial attacks is a crucial contribution given the increasing concerns about the vulnerability of neural networks in securitysensitive applications. Weak Points:  Generalization: The experiments focused on the ImageNet100 dataset which might limit the perceived generalizability of the model across diverse image datasets and tasks.  Comparison Range: The paper mainly compares the Foveated model with a fullresolution model. Including comparisons with other stateoftheart efficiencyfocused models could provide a broader understanding of the FoveaTers positioning in the field.  Realworld Application: Theres limited discussion on the practical implications of the proposed model. Insights into deployment scenarios realworld efficiency or scalability could enhance the papers relevance to applied domains. Questions  Feedback:  Could the authors detail how the model would perform on datasets with more complex or varied imagery such as medical images or satellite photos  It would be beneficial to elaborate on potential limitations or challenges in scaling the FoveaTer model for larger datasets or in realtime applications.  Feedback: Providing more analysis on the contribution of each module to the overall performance could offer insights into potential areas for future improvements or simplifications. 3. Review Summary: The paper presents a compelling and innovative approach towards integrating the biological concept of foveated vision with modern transformer architecture to achieve computationally efficient imaging processing. The FoveaTer model leverages attentionbased mechanisms and dynamic resource allocation to perform object classification tasks showing promise with reduced computational load and enhanced robustness against adversarial attacks. While the results are promising further exploration into the generalizability of the model broader comparisons and practical applications would enhance the papers contribution to the field. The strengths of the proposed model notably its innovative approach and the efficiency gains provide a strong argument for its significance. However addressing the identified gaps could solidify its position and utility within the computer vision community. Based on the merits of innovation demonstrated efficiency and the potential impact of this work I recommend the paper for publication contingent on the consideration and potential incorporation of the feedback and questions highlighted.", "noaG7SrPVK0": "Summarize the paper: This paper addresses the generation of counterfactual plans under uncertainty in machine learning models specifically in the context of consequential decisionmaking domains like loan approvals and university admissions. Counterfactual plans offer explanations by suggesting modifications to alter model predictions favorably but their effectiveness can diminish with model updates due to new data. The authors propose a framework that considers model uncertainty using first and secondmoment information of the model parameters to both assess and enhance the robustness of counterfactual plans. This includes a diagnostic tool for evaluating the probability bounds of a plans validity a correction approach to modify plans for higher validity and the COunterfactual Plan under Ambiguity (COPA) framework for creating robust counterfactual plans that balance validity proximity and diversity. The methods were validated through experiments on synthetic and realworld datasets demonstrating their effectiveness in improving counterfactual plan robustness against future parameter shifts. Strong Points: 1. Novel Approach to Uncertainty: The paper addresses the critical issue of model uncertainty in counterfactual explanations an area not extensively explored by existing methodologies. The consideration of model parameters uncertainty is innovative and highly relevant for realworld applications where models are frequently updated. 2. Comprehensive Framework: The proposal of diagnostic corrective and constructive tools (COPA framework) offers a comprehensive solution spanning the evaluation improvement and generation of counterfactual plans. This endtoend approach is both practical and theoretically appealing. 3. RealWorld Applicability and Validation: The experiments conducted on both synthetic and realworld datasets underscore the frameworks effectiveness and provide convincing evidence of its applicability and utility in real scenarios. 4. Mathematical Rigor and Clarity: The mathematical formulations including the formulation of the problem under model parameter uncertainty and the proposed semidefinite programming solutions are precise and clearly communicated. The theoretical underpinnings are solid providing a firm foundation for the proposed approach. Weak Points: 1. Assumption Limitations: The scope is primarily limited to linear classification settings which might restrict applicability to more complex models notably deep neural networks used in many consequential decisionmaking domains today. 2. Computational Complexity: While the paper claims computational efficiency the actual computational demands especially for highdimensional data and complex models are not thoroughly discussed. This could be a critical factor in realworld applications. 3. Lack of Comparative Analysis: Although the paper compares the proposed framework with the DiCE framework a broader comparison with other stateoftheart methods in counterfactual explanations under uncertainty could have provided a clearer picture of the method\u2019s advantages and limitations. 4. Limited Exploration of Model Dynamics: The paper could further explore the dynamics of model updates (e.g. frequency extent of changes) and their implications for counterfactual plan robustness over time. Questions and Feedback for the Author: 1. Could the authors discuss the applicability of the proposed framework to more complex nonlinear models beyond linear classifiers Are there specific adaptations or considerations required 2. Can the authors provide more insight into the computational complexity of the proposed methods especially when dealing with highdimensional data or more complex model structures 3. Could further comparisons with additional methods in counterfactual explanations or robust optimization be integrated to highlight the unique advantages or potential shortcomings of the proposed framework more comprehensively 4. Are there insights or preliminary thoughts on how the proposed diagnostic and correction tools might adjust to accommodate frequent model updates particularly in rapidly evolving data environments Review Summary: The paper introduces an innovative and comprehensive framework addressing counterfactual plan generation under model parameter uncertainty an underexplored yet critical aspect in the field of interpretable machine learning. By incorporating diagnostic corrective and constructive components within the COPA framework the authors provide robust tools for enhancing counterfactual explanation applicability and reliability amidst model updates\u2014a common scenario in realworld applications. The mathematical rigor alongside empirical validations on synthetic and realworld datasets solidifies the contribution of this work. However concerns regarding the frameworks applicability to more complex models its computational demands and broader comparisons with existing methods suggest areas for further exploration and enhancement. Encouraging deeper investigations into these dimensions could significantly elevate the works impact and applicability. Overall this paper makes a valuable contribution to the literature on interpretable machine learning opening avenues for further research on counterfactual explanations under uncertainty.", "iMSjopcOn0p": " Review of \"Unified Training for MultiTask Multitrack Music Transcription Using a GeneralPurpose Transformer Model\"  Summary This paper presents a significant advance in Automatic Music Transcription (AMT) by introducing a unified training framework that leverages a generalpurpose Transformer model to transcribe music from a diverse range of datasets encompassing multiple instruments. The authors tackle the inherent challenges of AMT namely the simultaneous transcription of multiple instruments with finescale pitch and timing lack of largescale datasets and inconsistent evaluation metrics. They overcome the domainspecific architecture limitation by enabling their model referred to as MT3 to learn transcriptions across different instrument combinations and styles using a compact flexible output vocabulary inspired by the MIDI specification. The MT3 model is trained on an assembly of six multitrack AMT datasets marking a step towards addressing the scarcity of training data by leveraging a mixture of resources. The paper introduces consistent evaluation metrics for multiinstrument transcription and showcases significant improvements in transcription quality especially for lowresource instruments thus setting a new State Of the Art (SOTA) across multiple datasets.  Strong Points 1. Novelty and Generalization: The papers approach to treating AMT as a sequencetosequence problem using a Transformer model capable of learning across various datasets represents a significant methodological advancement. This generalpurpose application breaks new ground by moving away from instrumentspecific architectures. 2. Comprehensive Dataset Assembly: By collating six distinct multitrack AMT datasets the authors address the pressing challenge of limited AMT data resources. This diversified dataset forms the largest known collection for multitask AMT training providing ample ground for the models robust performance across various instruments. 3. Consistent Evaluation Framework: The introduction of consistent evaluation metrics including a novel multiinstrument F1 score addresses the longstanding issue of metric inconsistency in AMT research. This framework allows for a fair comparison across models and datasets facilitating a clearer understanding of model performance in multiinstrument settings.  Weak Points 1. Implementation Details: While the paper offers a highlevel overview of the Transformer architecture employed more detailed explanation or rationale behind certain choices (e.g. token vocabulary modifications embedding techniques) could enrich the readers understanding of the models functionality. 2. DatasetSpecific Challenges: The discussion somewhat glosses over the inherent difficulties in unifying datasets that were not originally designed to be compatible. More insight into the challenges faced and how they were resolved would provide valuable knowledge for future research aiming to replicate or build upon this work. 3. Limited Consideration of Alternative Approaches: The paper strongly focuses on the proposed Transformer model without much comparison to or discussion of alternative machine learning models or architectures that could potentially be applied to AMT. This limitation narrows the scope of the findings potentially overlooking other promising approaches.  Questions and Feedback to the Author 1. Can the authors provide more insights into the decision process behind the specific modifications made to the token vocabulary compared to the original MIDI specification 2. How do the authors foresee the integration of generative adversarial networks (GANs) or other novel deep learning approaches in further enhancing the MT3 models transcription capabilities 3. The paper mentions potential future directions including self or semisupervised training on unlabeled data. Could the authors elaborate on the envisioned approaches for integrating such methodologies within the current framework  Review Summary This paper represents a significant step forward in the field of Automatic Music Transcription through the introduction of a generalpurpose Transformer model that advances the state of the art across a variety of datasets and musical instruments. The unified framework extensive dataset assembly and consistent evaluation metrics address several longstanding challenges within AMT research. However the paper could benefit from deeper insights into the architectural choices more extensive discussion of dataset integration challenges and broader consideration of alternative machine learning approaches for AMT. Despite these minor shortcomings the papers contributions are substantial providing a strong foundation for future research in multiinstrument music transcription. The proposed MT3 model not only showcases exceptional performance improvements especially for lowresource instruments but also paves the way for exploring new applications and methodologies in music understanding and transcription.", "t2LJBsPxQM": " Summarize the Paper This paper introduces a groundbreaking theoretical framework for designing orthogonal convolutions in convolutional neural networks (CNNs) addressing the challenges associated with enforcing orthogonality in deep neural architectures. Orthogonal convolutions have been identified as a remedy for gradient vanishingexploding problems sensitivity to adversarial perturbations and for enhancing the generalization capabilities of deep networks. The authors establish a compelling equivalence between orthogonal convolutions in the spatial domain and paraunitary systems in the spectral domain providing a novel perspective on the design of orthogonal convolutions. By leveraging this insight the paper presents a comprehensive and exact parameterization for 1D and separable 2D orthogonal convolutions showcasing its application in scaling up deep orthogonal networks including popular architectures like ResNet and ShuffleNet and extending its utility to residual flowbased models. The theoretical contributions and experimental results presented underscore the significant potential of exact orthogonality in convolutional layers for both improving the performance of deep networks and ensuring their robustness.  Strong and Weak Points Questions and Feedback  Strong Points: 1. Originality and Theoretical Contribution: The paper presents a novel approach to designing orthogonal convolutions by establishing their equivalence with paraunitary systems in the spectral domain. This contribution is significant as it lays a theoretical foundation that was previously missing in the literature. 2. Practical Implications: By demonstrating how to achieve exact orthogonality in deep orthogonal networks and showing substantial performance improvements in architectures like ResNet and ShuffleNet the paper proves the practical applicability and advantages of its approach. 3. Comprehensive Approach: The authors do not stop at theoretical contributions but extend their study to important architectural considerations for deep orthogonal networks such as skip connections initialization stride and dilation making the work comprehensive.  Weak Points: 1. Complexity and Accessibility: Given the theoretical depth and the novel concepts introduced the paper might be challenging for readers who are not already familiar with spectral domain analyses or the mathematical foundation underlying orthogonal and paraunitary systems. 2. Comparative Evaluation: While the paper presents experimental results showcasing the superiority of their approach a more detailed comparison with stateoftheart methods especially in terms of computational efficiency and memory usage would be beneficial. 3. Generalization beyond CNNs: The focus is predominantly on CNNs and specific deep learning architectures. Discussion on the applicability or limitations of the proposed approach in other types of neural networks or different learning paradigms would add value.  Questions and Feedback: 1. On Generalizability: Can the authors comment on the potential application of their framework to other network architectures beyond CNNs such as recurrent neural networks (RNNs) or Transformers 2. Regarding Computational Efficiency: While the proposed method is said to be computationally efficient providing actual computational cost comparisons against existing methods would solidify this claim. Could the authors provide such benchmarks 3. Clarification on Realworld Applications: The experiments focus on demonstrating the theoretical effectiveness of the proposed approach. Can the authors provide insights into how their method performs in realworld applications or datasets  Review Summary The manuscript presents a significant advancement in the design of orthogonal convolutions for CNNs by introducing a novel theoretical framework that correlates orthogonal convolutions with paraunitary systems in the spectral domain. This approach not only addresses several persistent challenges in deep learning such as gradient instability and sensitivity to adversarial attacks but also enhances the generalization capacity of deep networks. The authors extend their theoretical contributions to practical applications by showcasing improvements in deep neural network architectures like ResNet and ShuffleNet and opening pathways to incorporating orthogonal convolutions in residual flows. Despite its theoretical and practical strengths the paper could improve its accessibility deepen its comparative analysis and explore its applicability to a broader range of neural network architectures. Nonetheless the presented work marks a significant step forward in the design and understanding of deep orthogonal networks offering new insights and tools for both researchers and practitioners in machine learning and deep learning domains.", "twgEkDwFTP": " Peer Review  Summary of the Paper This paper addresses the overfitting issue observed in machine learning models that utilize reweighting algorithms to improve fairness by enhancing worstgroup performance. Previous empirical findings have shown that such algorithms when applied to overparameterized models tend to overfit diminishing their effectiveness. The authors present a theoretical analysis that confirms and explains this phenomenon showing that reweighting algorithms invariably lead to overfitting by making the model converge to the same empirical risk minimization (ERM) interpolator regardless of the reweighting. The study further explores the role of regularization in mitigating overfitting concluding that effective regularization must significantly limit the model\u2019s training accuracy to prevent overfitting. Through theoretical proofs and empirical validation on real datasets the paper illustrates the inherent limitations of reweighting algorithms in achieving high worstgroup performance without overfitting recommending large regularization and data augmentation as necessary strategies.  Strong Points 1. Theoretical Depth: The paper offers rigorous theoretical proofs to support the claims about the inherent overfitting tendency of reweighting algorithms in overparameterized models. This theoretical foundation significantly contributes to the understanding of fairness in machine learning. 2. Empirical Validation: The validation of theoretical claims with experiments on real datasets Waterbirds and CelebA strengthens the papers arguments providing a comprehensive view of the issue. 3. Practical Implications: By elucidating the limits of reweighting algorithms and proposing actionable strategies like large regularization and data augmentation the paper provides valuable insights for practitioners aiming to improve worstgroup performance in machine learning models. 4. Clarity and Structure: The paper is wellstructured logically progressing from the introduction of the problem through theoretical analysis to empirical validation and conclusions. The use of clear language and appropriate mathematical formalism makes complex concepts accessible.  Weak Points 1. Lack of Discussion on Alternative Approaches: While the focus on reweighting algorithms is thorough the paper could benefit from a broader discussion on alternative methods for improving worstgroup performance that may not suffer from the same overfitting issues. 2. Generalization of Theoretical Results: The theoretical analysis while profound is primarily focused on overparameterized settings. It would be beneficial to discuss the applicability or adaptation of these findings to less overparameterized models. 3. Impact of Data Quality: The role of data quality especially regarding spurious correlations and minority group representation in exacerbating or mitigating the overfitting problem is not sufficiently explored.  Questions and Feedback for the Author 1. On Alternative Methods: Can the authors discuss whether their findings could guide the development of new reweighting algorithms or alternative methods that are less prone to overfitting in overparameterized models 2. Regarding Data Quality: How might variations in data quality such as the presence of noise or the representation of minority groups impact the conclusions about reweighting algorithms and the effectiveness of regularization 3. Applicability to Different Model Architectures: The paper primarily addresses fullyconnected neural networks. How might the findings apply to other model architectures such as convolutional or recurrent neural networks  Review Summary The paper makes a significant contribution to the field of machine learning fairness by providing a comprehensive theoretical and empirical analysis of the limitations of reweighting algorithms in improving worstgroup performance without leading to overfitting. The authors\u2019 recommendations for large regularization and data augmentation are practical and valuable for practitioners. However the paper could be strengthened by addressing the generalization of its findings beyond overparameterized models exploring the impact of data quality on overfitting and discussing alternative approaches for achieving fairness in machine learning models. Overall the paper provides important insights and guidance for practitioners and researchers focused on building fair and effective machine learning systems.", "u4C_qLuEpZ": " Peer Review for \u201cLeveraging Graph Neural Networks for Enhanced Program Analysis via Assembly Code Embeddings\u201d  1. Summary The paper introduces an innovative approach to program analysis by developing a new model leveraging the capabilities of graph neural networks (GNN) to embed program information at the assembly code level thus capturing both semantic and structural aspects of programs. This method allows for the analysis of both source coderelated tasks such as program classification and compilationrelated problems like detecting binary similarity. By abstracting programs into multiple graph structures (control flow graph call graph and data flow graph) and utilizing a combination of BERT and Gated GNN models the authors aim to achieve a more comprehensive understanding of program semantics and structure. The model has been evaluated on two program analysis tasks yielding accuracies of 82.58 and 83.25 respectively and demonstrating improvements over stateoftheart approaches.  2. Strong Points  Innovative Approach: The paper explores a novel pathway by using assembly code for program embedding which uniquely positions it to capture deeper structural and semantic program features not fully accessible through source code or AST analysis.  Comprehensive Model Architecture: The dual model training process encompassing BERT for initial node embeddings and Gated GNN for whole program embedding effectively leverages the strengths of both machine learning architectures and represents a wellthoughtout approach to tackling the program analysis problem.  Significant Improvement over Existing Methods: Demonstrating more than 8 improvement over current stateoftheart methods in program classification and binary similarity detection tasks is a notable achievement. This highlights the potential of the proposed model to significantly advance the field of program analysis.  Welldesigned Experiments: The experiments are carefully planned and executed showcasing the effectiveness of the model on realworld datasets and providing a clear comparison with existing methodologies.  Weak Points  Lack of Qualitative Analysis: The paper primarily focuses on quantitative results leaving a gap in qualitative analysis that could shed light on why the model performs better and provide deeper insights into its operation.  Limited Explanation on Model Choice: While the methodology is comprehensive the paper could benefit from a more detailed rationale on why specific models (BERT and Gated GNN) were chosen and how they complement each other in the context of program analysis.  Generalizability Concerns: The paper evaluates performance on two specific tasks with a particular dataset. Additional evaluations across different datasets and program analysis tasks would help establish the model\u2019s generalizability.  Model Complexity and Computational Demand: Given the sophisticated architecture involving multiple graph structures and deep learning models there is a concern about the computational demand and scalability of the proposed solution. This aspect is not sufficiently addressed in the paper.  Questions and Feedback  Model Selection and Configuration: Could you elaborate on the process for selecting BERT and Gated GNN as the core components of your model What were the alternatives considered and why were they deemed less suitable  Impact of Different Assembly Languages: Does the models performance vary significantly with assembly code derived from different programming languages or architectures If so how do you propose to tackle these discrepancies  Scalability: How does the model scale with increasingly large program sizes and more complex program structures Are there any strategies in place to mitigate potential declines in performance or efficiency  Review Summary The paper introduces a compelling and novel approach to program analysis using graph neural networks on assembly code marking a significant potential advancement in the field. The innovative methodology substantial improvement over existing methods and wellconducted experiments are commendable. However the paper would benefit from addressing the mentioned weak points particularly in providing a deeper qualitative analysis rationalizing model choices more thoroughly assessing generalizability across broader datasets and addressing scalability and computational demands. Despite these areas for improvement the strengths and contributions of this work are clear and present a solid foundation for future research in program analysis. I recommend the acceptance of the paper contingent on the consideration and possible inclusion of the suggested improvements which I believe would make the paper even stronger.", "wv6g8fWLX2q": " Review of \"TimeAware Multipersistence SpatioSupra Graph Convolutional Networks for Multivariate Time Series Forecasting\"  1. Paper Summary: This study introduces a novel architecture TimeAware Multipersistence SpatioSupra Graph Convolutional Network (TAMPS2GCNets) for forecasting multitime series data. The paper bridges two emerging research areas: timeaware deep learning and multipersistence (MP) a tool in topological data analysis that assesses data shape changes across multiple dimensions. The core proposition is to enhance Graph Convolutional Networks (GCNs) by incorporating timeaware MP to capture latent temporally conditioned interrelations among data points which traditional methods miss. This approach constructs a dynamic EulerPoincar\u00e9 surface to summarize topological features over time and applies a new supragraph convolution module for modeling intra and interdependencies in data. Extensive experiments with highway traffic flow Ethereum token prices and COVID19 hospitalization datasets demonstrate TAMPS2GCNets superior predictive performance over stateoftheart methods.  2. Strong and Weak Points Questions and Feedback: Strong Points:  Innovative Integration: The papers integration of multipersistence and timeaware deep learning presents a novel approach. This combination leverages the strengths of both methods for enhanced time series forecasting.  Stability Proof: Providing proof of stability for the dynamic EulerPoincar\u00e9 surface contributes to the theoretical rigor of the paper.  Extensive Experimentation: The broad range of datasets (covering traffic Ethereum token prices and COVID19 hospitalizations) showcases the models versatility and its superior performance compared to existing methods. Weak Points:  Computational Complexity: While the paper mentions that the model has lower computational costs than other MP approaches detailed comparisons or benchmarks on computational efficiency are not provided. Given MPs known high computational demands concrete metrics on computational performance would be valuable.  Lack of Baseline Comparisons for Model Components: The paper could benefit from a more detailed comparison between the newly proposed model components (e.g. DEPS) and existing methodologies. Specifically demonstrating the unique advantages of each component through ablation studies would strengthen the argument for the proposed architectures novelty.  Limited Discussion on Practical Implications: Although the paper briefly touches on the potential ethical implications and applications in biosurveillance expanding on practical challenges limitations and the realworld applicability of the method would provide a more comprehensive understanding of its utility and impact. Questions and Feedback:  Can the authors elaborate on the computational efficiency of TAMPS2GCNets in comparison to other stateoftheart models particularly in terms of time complexity and resource utilization  How does the model handle noisy or incomplete data especially in highly dynamic and unpredictable domains like cryptocurrency and pandemic spread forecasting  Feedback: To increase the utility and approachability of the model it would be beneficial to include case studies or examples illustrating the application of TAMPS2GCNets in addressing specific forecasting challenges.  3. Review Summary: The paper presents a groundbreaking approach by integrating multipersistence and timeaware deep learning within GCNs for enhanced multivariate time series forecasting. The novel use of a dynamic EulerPoincar\u00e9 surface for summarizing timeconditioned topological properties and the introduction of a supragraph convolution module for learning data dependencies offer significant contributions to the field. The experimental results convincingly demonstrate the models efficacy. However the paper could be improved by offering deeper insights into the models computational efficiency providing more detailed componentwise comparisons and expanding the discussion on realworld applications and limitations. Despite these minor shortcomings the innovative methodology and the comprehensive experimental validation strongly support the papers contributions making it a valuable addition to the literature on time series forecasting and topological data analysis.", "t7y6MKiyiWx": "Review of \"Pyramidal Circuit: A Pathway to Training Orthogonal Neural Networks in Quadratic Time\" 1. Summary: The paper introduces a novel training method for orthogonal neural networks (OrthoNNs) called the Pyramidal Circuit inspired by quantum computing principles but applicable to classical computing as well. The Pyramidal Circuit aims to maintain perfect orthogonality of weight matrices during training significantly reducing the computational overhead associated with obtaining orthogonal matrices through Singular Value Decomposition (SVD) in classical gradient descent methods. This is achieved by replacing the direct optimization of matrix elements with the optimization of parameters defining the pyramidal quantum circuit specifically the angles of Reconfigurable Beam Splitter (RBS) gates. The method claims to run in quadratic time comparable to traditional neural network training times and provides a classical simulation approach alongside potential for implementation on quantum computers. The authors also undertake numerical experiments on the MNIST dataset to validate the efficacy of their approach. 2. Strong and Weak Points Questions and Feedback: Strong Points: a. Innovation: The concept of using a quantuminspired structure for improving the efficiency of training orthogonal neural networks is highly innovative. It cleverly bridges quantum computing insights with classical neural network training. b. Orthogonality Preservation: The paper addresses a significant challenge in neural network training by ensuring perfect orthogonality without the computational cost of repeated SVD potentially enhancing the stability and performance of deep learning models. c. Efficiency: The proposed method operates in quadratic time for both forward and backward passes which is a substantial improvement over existing methods that approximate orthogonality. d. Experimental Verification: The authors support their claims with numerical experiments showing the practicality of their approach even on current quantum computing hardware. Weak Points: a. Scalability and Practicality: While numerical experiments are conducted the paper somewhat lacks a deep exploration of scalability issues. Given that realworld applications of neural networks often involve very large matrices a thorough analysis of runtime and resource implications on classical and quantum hardware would be beneficial. b. Comparison with Existing Methods: The presentation would benefit from a more detailed comparison against other stateoftheart methods in orthogonal neural network training especially in terms of training time and model accuracy. c. Limitations and Constraints: The requirement for input normalization and its implications on learning have been briefly mentioned. More discussion on the potential limitations or constraints this and other aspects of the proposed method introduce would make for a more balanced view. Questions and Feedback: 1. Can the authors provide more insight into how the Pyramidal Circuit approach scales with increasing network depth and input size especially in practical applications 2. It would be interesting to see a discussion on how the proposed method affects the learning capacity and generalization performance of the trained models compared to traditional methods. 3. Are there specific scenarios or problem domains where the authors anticipate the Pyramidal Circuit will have the most significant impact 4. Feedback: Expanding on the points mentioned above particularly scalability and detailed comparisons would strengthen the paper. Additionally exploring potential limitations and providing more comprehensive data on the practical implications of input normalization could offer a more nuanced perspective on the utility of the proposed method. 3. Review Summary: The paper presents an innovative and theoretically promising approach to training orthogonal neural networks efficiently addressing a known challenge in deep learning. The quantuminspired Pyramidal Circuit is a creative solution and the initial experimental results are promising. However the paper could be significantly strengthened by addressing scalability in more detail offering greater insight into the comparison with existing methods and discussing potential limitations more comprehensively. Despite these areas for enhancement the work is a valuable contribution to the field potentially paving the way for more efficient and stable training of deep neural networks. The concepts novelty and the intersection with quantum computing principles make it a compelling read and a significant step forward in the exploration of orthogonal neural networks.", "tG8QrhMwEqS": " Review of \"An Adaptive ActivationBased Structured Pruning Approach for Deep Learning Model Compression\"  1) Summary of the Paper This paper addresses the challenge of deploying deep learning models on resourceconstrained edge devices by proposing an adaptive activationbased structured pruning approach to compress these models efficiently. It outlines the limitations of existing unstructured pruning methods which are inefficient on commodity hardware and require extensive manual tuning. The proposed methodology combines iterative structured pruning with an activationbased attention feature map to identify and prune unimportant filters effectively. Additionally it introduces adaptive pruning policies tailored for various application requirements including accuracy memory and latency constraints. The authors evaluate their method on CIFAR10 and ImageNet datasets using prominent models like ResNet and VGG showcasing significant improvements in parameter and FLOPs reduction with minimal to no accuracy loss.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty of ActivationBased Approach: The utilization of activationbased attention to identify filters for pruning is innovative. It provides a more accurate criterion for determining the importance of filters compared to traditional weightbased methods.  Adaptive Pruning Policies: The paper successfully addresses diverse deployment scenarios by introducing adaptive pruning policies. This flexibility is crucial for practical applications with varying requirements.  Comprehensive Evaluation: The extensive evaluation across different models and datasets convincingly demonstrates the effectiveness of the proposed method setting new benchmarks in structured pruning. Weak Points:  Comparative Analysis: While the paper compares its results with a wide range of existing methods it lacks a deeper analysis of why the proposed method outperforms others substantially. Insights into the comparative efficiency of the activationbased approach versus other criteria for pruning could strengthen the argument.  Implementation Details: The paper does not provide sufficient details on the practical implementation of the adaptive pruning policies particularly on how these policies dynamically adjust during the pruning process across different layers.  Reproducibility Concerns: Without the publication of code or a more detailed explanation of the algorithmic steps reproducing the reported results may be challenging for other researchers. Questions and Feedback:  How does the proposed activationbased attention mechanism compare in computational overhead to traditional weightbased pruning criteria during the pruning process  Can the authors provide insights or preliminary results on how their method performs on other types of neural networks beyond the ones tested such as recurrent neural networks (RNNs) or Transformer models  It would be beneficial if the paper could delve into the tradeoffs encountered when adjusting the various adaptive pruning policies providing guidance on balancing these aspects for optimal results.  3) Review Summary The paper introduces a significant advancement in structured model pruning addressing the acute need for deploying deep neural networks on resourceconstrained devices. By innovatively leveraging activationbased attention for filter importance determination and proposing adaptive pruning policies it markedly outperforms current stateoftheart methods in reducing model parameters and computational requirements without sacrificing accuracy. Despite its strengths the paper could be enhanced by offering a more detailed comparative analysis with existing methods elaborating on the implementation of adaptive pruning policies and addressing reproducibility concerns by detailing the algorithmic steps or releasing the code. Additionally expanding the evaluation to include other neural network architectures would validate the generalizability of the proposed approach. In conclusion the proposed method presents a significant step forward in structured pruning promising substantial practical benefits for deploying efficient deep learning models. Nonetheless addressing the discussed weaknesses and questions could further solidify the methods value and facilitate its adoption in realworld applications.", "v6s3HVjPerv": " Peer Review on \"Exploring the Efficacy of Interpretability Methods in Bias Discovery: A User Study\"  Summary The paper investigates the effectiveness of various interpretability methods in assisting users to uncover biases in image classification models. The core of this research is a user study with 240 participants that compares a baseline explanation technique (analyzing model predictions directly) with two advanced interpretability approaches: conceptbased explanations and counterfactual explanations generated by an invertible neural network (INN). The study utilizes a novel synthetic dataset TWO4TWO designed to contain controllable and quantifiable biases across different attributes providing a rigorous testing ground for interpretability techniques. The results reveal that the baseline method outperforms conceptbased explanations while INN counterfactuals show a similar performance to the baseline but aid in the identification of certain attributes more accurately. The paper makes significant contributions by presenting a carefully designed user study protocol the TWO4TWO dataset and opening new pathways for evaluating interpretability methods realworld utility.  Strong Points 1. Novel Dataset: The creation of the TWO4TWO synthetic dataset is a major contribution to the field of interpretable machine learning. It allows for controlled experiments to test the efficacy of different interpretability techniques in a bias discovery task bridging a gap in existing research materials. 2. Empirical Validation: Conducting an extensive user study with 240 participants provides a solid empirical foundation to evaluate the effectiveness of interpretation methods in realworld scenarios. This significantly advances the understanding of how such techniques can support bias identification. 3. OpenSource Resources: By opensourcing the dataset model and study design the authors facilitate reproducibility and further research. This transparency is commendable and enhances the papers impact. 4. Comprehensive Evaluation: The paper thoroughly evaluates the interpretability methods considering not just their technical capabilities but their usability and effectiveness in aiding users to identify biases. This holistic approach is crucial in understanding the practical utility of interpretable ML methods.  Weak Points and Suggestions 1. Comparative Depth: While the paper includes a baseline and two stateoftheart interpretability methods a broader comparison including more diverse types of interpretability techniques (e.g. saliency maps examplebased explanations) could enrich the findings and offer a more comprehensive view of the landscape. 2. Attribute Selection Bias: The focus on specific attributes (e.g. legs\u2019 position animal color) in assessing feature relevance might introduce a form of selection bias. It would be informative to include a wider variety of attributes or to evaluate the interpretability methods capacity to reveal unexpected biases. 3. Qualitative Feedback Integration: The paper could benefit from a deeper analysis of participant feedback beyond accuracy metrics. Insights into user experience misunderstandings and perceived reliability of the explanations might uncover additional aspects affecting the utility of interpretability methods. 4. Generalization to RealWorld Data: While TWO4TWO is designed to simulate realworld challenges further studies employing actual datasets are necessary to validate the findings applicability beyond a synthetic setting.  Review Summary The paper presents a valuable contribution to the field of interpretable machine learning by conducting an extensive user study comparing different interpretability techniques in bias identification tasks. The introduction of the TWO4TWO dataset is particularly noteworthy offering a novel tool for future research in this area. However the studys focus on a somewhat narrow range of interpretability methods and attributes might limit the findings generalizability. Expanding the range of evaluated techniques and explored biases could provide a fuller understanding of the interpretability tools strengths and weaknesses. Furthermore a more detailed exploration of qualitative participant feedback and the methods applicability to realworld data would enhance the papers insights. Nonetheless the research represents a significant step forward in empirically assessing interpretability techniques usefulness particularly in uncovering biases and lays a solid foundation for future explorations in this domain.", "vrW3tvDfOJQ": " Review of \"Inverse Variance Reinforcement Learning: Enhancing Deep Reinforcement Learning through Uncertainty Estimation\"  Summary This paper introduces InverseVariance Reinforcement Learning (IVRL) a novel framework designed to improve the sample efficiency of modelfree deep reinforcement learning (DRL) algorithms. The authors identify the primary challenge in DRL as the reliance on noisy value estimates for supervision which is particularly detrimental to sample efficiency due to the heteroscedastic nature of this noise. They advocate for the adoption of an uncertaintybased weighting mechanism in the optimization process to mitigate this issue. Unlike existing methods which predominantly utilize sampled ensembles for uncertainty estimation IVRL employs a Bayesian framework combining probabilistic ensembles with Batch Inverse Variance (BIV) weighting. This approach allows IVRL to capture a broader spectrum of uncertainties leading to significant enhancements in sample efficiency across various discrete and continuous control tasks.  Strong Points 1. Innovative Approach: The paper presents a novel approach to incorporating uncertainty estimation into DRL. By combining probabilistic ensembles with BIV weighting IVRL addresses the limitations of sampled ensembles and provides a comprehensive framework for improving learning efficiency. 2. Theoretical Justification: The authors offer a solid theoretical foundation for their approach thoroughly analyzing sources of uncertainty and detailing how IVRL estimates and mitigates them. This rigorous analysis strengthens the credibility of the proposed framework. 3. Empirical Evidence: The paper supports its claims with extensive empirical results demonstrating IVRL\u2019s superiority over current stateoftheart methods in terms of sample efficiency. The conducted experiments cover a wide range of environments adding to the generalizability of the findings. 4. Environmental Consideration: The inclusion of a carbon footprint analysis is commendable showing a commitment to environmental sustainability and encouraging responsible research practices.  Weak Points 1. Computational Cost: While the paper acknowledges the computeintensive nature of IVRL due to its use of ensembles it could provide a more detailed discussion on potential tradeoffs especially in resourceconstrained settings. 2. Exploration Strategies: The paper touches upon how IVRL could potentially influence explorationexploitation tradeoffs but does not delve deeply into possible interactions. A more comprehensive analysis in this area could enhance the understanding of IVRL\u2019s applicability. 3. Hyperparameter Sensitivity: There is limited discussion on the sensitivity of IVRL to hyperparameter settings particularly the choice of \\xce\\xbe and \u03bb. Insights into how these parameters were tuned and their impact on performance would greatly benefit reproducibility and applicability.  Questions and Feedback to the Author 1. Could you provide more insights into the computational overhead of IVRL compared to traditional methods and any potential strategies to alleviate this concern 2. How does IVRL interact with different exploration strategies and could certain strategies further enhance its sample efficiency 3. Could the paper discuss the sensitivity of IVRL to its hyperparameters Specifically how robust is its performance across a range of settings for \\xce\\xbe and \u03bb  Review Summary Inverse Variance Reinforcement Learning (IVRL) represents a significant advancement in the domain of deep reinforcement learning. Through a novel combination of probabilistic ensembles and BIV weighting it effectively addresses the sample inefficiency problem exacerbated by noisy supervision. The theoretical and empirical analyses provided in the paper are both comprehensive and convincing demonstrating notable improvements over existing methods across various tasks. However the paper could benefit from a deeper exploration of the computational costs associated with this method the interaction with exploration strategies and the sensitivity to hyperparameters. Despite these areas for further investigation IVRL holds considerable promise for enhancing the applicability and efficiency of DRL in realworld scenarios. Its consideration of the environmental impact of computational research further underscores the importance of sustainable practices in machine learning. Overall this paper makes a compelling case for the adoption of uncertaintybased weighting in DRL and is a valuable contribution to the field.", "swbAS4OpXW": " Review of \"GenDA: Oneshot Generative Domain Adaptation with Enhanced Diversity and Quality\"  Summary This paper introduces GenDA a novel approach for oneshot Generative Domain Adaptation that enables the adaptation of a Generative Adversarial Network (GAN) pretrained on a large dataset to a new domain defined by as few as one target image. The method focuses on preserving synthesis quality and diversity by introducing an attribute adaptor for the generator and an attribute classifier for the discriminator without extensive finetuning of the original model parameters. By leveraging prior knowledge from the source model and constraining the diversity in the generative domain GenDA achieves superior performance in producing photorealistic and diverse images across various tests surpassing stateoftheart methods especially in terms of diversity under conditions of limited supervision.  Strong Points 1. Innovative Approach: GenDA addresses the challenge of maintaining image synthesis diversity with limited data by introducing an attribute adaptor and classifier a novel solution that differs substantially from the vanilla finetuning strategy used by existing methods. 2. Preservation of Quality and Diversity: The method\u2019s emphasis on preserving both the synthesis quality and diversity by reusing the pretrained models knowledge and introducing diversity constraints is compelling and addresses a significant gap in the current literature on oneshot domain adaptation. 3. Extensive Evaluation: The paper presents an extensive evaluation of the approach across multiple settings including oneshot and fewshot domain adaptation and crossdomain adaptation. The comparative analysis demonstrates significant improvements over stateoftheart methods substantiating the efficacy of GenDA. 4. Reproducibility and Utility: The detailed description of the methodology the base model used (StyleGAN2) and the datasets for evaluation along with the commitment to public code release enhance the paper\u2019s reproducibility and potential utility for future research and applications.  Weak Points 1. Limited Exploration of Failure Cases: The paper lacks a detailed discussion on the limitations or potential failure cases of GenDA. While it briefly mentions the methods difficulty in adapting models to completely different domains a more thorough examination of these limitations and potential remedies would enhance the papers contribution. 2. Attribute Selectivity: The method\u2019s current design treats all characters of the reference image as a whole which might limit the precise control over which specific attributes to transfer. A discussion on future directions for enabling selective attribute adaptation could be valuable. 3. Domain Specificity and Generalization: Further exploration on how GenDA performs across a wider range of domains particularly those with sparse or complex attributes could strengthen the argument for its versatility and general applicability.  Questions and Feedback to the Author 1. Potential for Selective Attribute Adaptation: Can the authors elaborate on potential strategies for enabling selective adaptation of attributes from the target domain This could address the methods mentioned limitation and open avenues for more controlled domain adaptation. 2. Adaptation to Drastically Different Domains: The authors note that GenDA struggles with adapting models to completely different domains. Can they suggest potential enhancements or alternative approaches that might overcome this limitation 3. Impact on Computational Resources: Given the lightweight design of the proposed modules can the authors comment on the computational efficiency of GenDA compared to traditional finetuning approaches This would be valuable for understanding its practicality in resourceconstrained scenarios.  Review Summary GenDA represents a significant advance in the field of oneshot generative domain adaptation by introducing a novel approach that effectively preserves both the quality and diversity of synthesized images with minimal supervision. The methods innovative use of attribute adaptors and classifiers thorough evaluation and superior performance over existing methods underscore its contribution to the domain adaptation literature. However further exploration of its limitations particularly regarding attribute selectivity and domain specificity could enhance its robustness and applicability. The paper is wellstructured provides a comprehensive methodological description and adheres to reproducibility standards making it a valuable contribution to both academic and practical pursuits in generative model adaptation.", "nc0ETaieux": "Review of \"On the Limitations of Learning Distributions with Generative Adversarial Networks\" 1) Summary of the paper This paper delves into an essential question surrounding Generative Adversarial Networks (GANs): whether achieving a population minimax optimal solution in the Wasserstein GAN objective implies learning the target distribution. The authors critically examine this assumption and provide theoretical evidence against it by establishing a connection between GANs and cryptographic primitives known as local pseudorandom generators (PRGs). They prove that under standard cryptographic assumptions and for a broad class of distributions there exist simple ReLU network generators which despite producing an output that no polynomially large Lipschitz discriminator can statistically distinguish from the target distribution are still significantly distant from the target in Wasserstein distance. Additionally the paper complements the theoretical findings with empirical evidence by constructing a specific generator based on Goldreich\u2019s PRG and showing that it can evade detection by standard neural network discriminators. Furthermore the work suggests possible ties between the theory of GANs and circuit lower bounds hinting at profound implications for computational complexity theory. 2) Strengths and Weaknesses Strong Points: a) Theoretical Depth: The paper provides a rigorous analysis and a significant theoretical contribution by connecting the fields of GANs cryptography and computational complexity. b) Novel Insight: The demonstration that cryptographic assumptions lead to limitations in learning distributions with GANs presents a novel and thoughtprovoking insight into the inherent challenges in the field. c) Empirical Validation: By offering empirical evidence that complements the theoretical findings the paper not only grounds its theoretical constructs in practical observation but also illustrates the feasibility of constructing generators that are hard for discriminators to distinguish. Weak Points: a) Specificity of Generators: The papers major conclusions are based on a very specific class of generators potentially limiting the general applicability of the findings across broader GAN architectures and settings. b) Practical Implications: While the theoretical insights are profound the paper could more explicitly discuss the implications for practice particularly how these limitations might affect the design of GAN architectures or training methods. c) Complexity for Practitioners: The heavy reliance on cryptographic and computational complexity theory might render the paper challenging for practitioners in the field of machine learning and deep learning to fully grasp and apply these insights. Questions and Feedback: 1) Extension to Other Architectures: Can the authors speculate on how these findings might generalize beyond ReLU networks and constant depth generators 2) Practical Recommendations: Based on these findings what practical advice would the authors offer to researchers or practitioners aiming to design GANs for learning distributions 3) Limitations on Distributions: The paper focuses on a \"large family of natural target distributions\"  can the authors specify any types of distributions that might be immune to the limitations discovered 3) Review Summary This paper makes a significant theoretical contribution to our understanding of GANs by revealing fundamental limitations in using these networks to learn distributions predicated on cryptographic assumptions. Through a wellstructured argument that ties together GANs local PRGs and computational complexity theory the work provides a comprehensive perspective on why achieving the population minimax solution might not suffice for learning distributions in the statistical sense. Despite its theoretical depth and novel insights the paper could benefit from a broader discussion on the practical implications of these findings and a more explicit consideration of their applicability across different types of GAN architectures and target distributions. Given its solid theoretical foundation and the potential implications for both the theory and practice of GANs my recommendation is to accept this paper. However I would encourage the authors to address the questions raised regarding the generalization of their results and to offer more explicit guidance for practitioners. This would greatly enhance the papers impact and utility for a wider audience in the machine learning community.", "vds4SNooOe": " Review of \"Learning FineGrained Embeddings for CrossGranularity FewShot Learning with SuperclassConditional Gaussian Mixture Models\"  1. Summary This paper addresses the challenge of CrossGranularity FewShot (CGFS) learning where a model pretrained with coarse labels (superclasses) needs to adapt to finegrained (subclass) tasks with only a few training samples. The main contribution is the introduction of a SuperclassConditional Gaussian Mixture Model (SCGM) designed to learn finegrained embeddings by modeling the generative process of samples across class hierarchies. The SCGM is encoderagnostic adding minimal computational overhead and allowing efficient endtoend training via maximum likelihood estimation with an ExpectationMaximization algorithm. The method is validated through extensive experiments on benchmark image datasets and a reallife medical dataset demonstrating its ability to outperform stateoftheart approaches in terms of effectiveness and efficiency.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty and Theoretical Foundation: The SCGM approach introduces a novel perspective by incorporating superclass information into the generative process of Gaussian mixture models providing a strong theoretical foundation for modeling the hierarchical nature of classes. This is a significant step forward in addressing CGFS problems.  Practical and Flexible Framework: The models flexibility to work with any encoder and the minimal overhead introduced by its additional parameters make it highly practical for various applications especially in domains where finegrained annotations are expensive or difficult to obtain.  Comprehensive Validation: The thorough experimental setup including evaluations on both image datasets and a reallife medical dataset effectively demonstrates the models versatility. The comparison with stateoftheart methods and the detailed analysis of results provide strong evidence of the superiority of the proposed method. Weak Points:  Lack of Broader Contextualization: While the paper does a good job of situating the work within the landscape of fewshot learning and embedding methods it could benefit from a broader discussion on potential limitations and the specific scenarios where SCGM may not perform as well. For instance how does the model deal with extremely imbalanced subclasses or superclasses with high intraclass variability  Complexity in Understanding the Model and Algorithm: The complexity of the proposed SCGM model and its optimization algorithm might be a barrier for some readers. While the technical explanations are detailed additional illustrations or stepbystep examples could help in making the concepts more accessible. Questions and Feedback: 1. Scalability: How does the SCGM model scale with extremely large datasets or a high number of classes Would there be significant challenges in terms of computational resources or optimization stability 2. Applicability to NonImage Data: While the paper demonstrates the models effectiveness on image data and a medical dataset it would be beneficial to see its performance on other types of data such as text or audio. Are there specific modifications or considerations required for these domains 3. Adaptability to Dynamic Class Hierarchies: How adaptable is the SCGM model to scenarios where the class hierarchy may evolve over time Would the model need retraining from scratch or are there mechanisms for incremental updates  3. Review Summary The paper presents a significant contribution to the CGFS learning problem by proposing the SCGM model which stands out for its novel theoretical foundation practical applicability and comprehensive validation. The use of a superclassconditional Gaussian mixture model to generate finegrained embeddings offers a new perspective that addresses the limitations of existing methods effectively. Despite the complexity of the model and its optimization algorithm the strong experimental results demonstrate its effectiveness and efficiency across different domains. However the paper could benefit from a broader discussion of potential limitations and more detailed exploration in diverse data types beyond images. Questions about scalability adaptability to nonimage data and dynamic class hierarchies remain suggesting avenues for future work. Overall the paper makes a compelling case for the SCGM model as a flexible and efficient solution for CGFS learning tasks. Its contributions to both the theoretical and practical aspects of finegrained learning are likely to stimulate further research and applications in this area.", "rwEv1SklKFt": "Peer Review of \u201cOn the Vulnerability of Poisoned Classifiers to Alternative Trigger Attacks\u201d 1) Summarize the paper This paper addresses a critical flaw in the prevailing understanding of how backdoor attacks against classification models operate. Typically its believed that only the attacker who embeds a trigger into the training data of a classifier can manipulate it. Challenging this notion the authors introduce a novel threat model where a third party ignorant of the original trigger can still hijack the classifier using alternative triggers. By employing a method dubbed Denoised Smoothing alongside human analysis theyve crafted a novel attack mechanism that construct alternative triggers from adversarial examples. The efficacy of this method was demonstrated across highresolution datasets like ImageNet and TrojAI showing superior performance and scalability over previous techniques. Additionally a user study underscored the intuitiveness and generalizability of this attack suggesting no backdoor in a poisoned classifier remains hidden from those determined to find it. 2) List strong and weak points of the paper Questions and Feedback to the author Strong Points: a) Innovative Approach: The paper pioneers in illustrating how poisoned classifiers can be commandeered using alternative triggers by a party unaware of the original backdoor widening the scope of threat models in adversarial machine learning. b) Extensive Experiments: The authors back their hypotheses with comprehensive experiments on large and complex datasets affirming the robustness and scalability of their method. c) Humanintheloop Design: This approach not only enhances the interpretability of the attack but also demonstrates its practical applicability empowering users to identify and exploit backdoors in poisoned classifiers. Weak Points: a) Scope of Triggers: The study primarily focuses on patchbased backdoor attacks which may limit its applicability to more sophisticated or subtle forms of triggers. b) Manual Intervention Requirement: While human involvement enhances interpretability it also introduces subjectivity and scalability issues potentially limiting the method\u2019s applicability in automated or largescale settings. Questions: a) Have the authors considered extending their method to automatically identify and extract alternative triggers reducing human dependency b) Can the proposed method be adapted or extended to identify and mitigate nonpatchbased backdoor triggers c) How does the presence of diverse and complex triggers in a dataset affect the effectiveness of the proposed attack method Feedback: The paper presents a significant advancement in the understanding and manipulation of poisoned classifiers. However exploring automated mechanisms to reduce or eliminate human involvement might improve scalability and objectivity. Further expanding the scope to encompass a broader range of trigger types could enhance the generalizability of this approach. Lastly a deeper analysis into the methods robustness against classifiers poisoned with varied or multiple triggers would enrich the findings. 3) Write a review summary (Provide supporting arguments for your recommendation) The paper \"On the Vulnerability of Poisoned Classifiers to Alternative Trigger Attacks\" provides a compelling exploration into a previously underexamined threat against backdoored classification models. By debunking the conventional belief that only the original attacker can exploit the backdoor the authors have significantly broadened the landscape of adversarial threats to poisoned classifiers. Their proposed humanintheloop method utilizing Denoised Smoothing to generate effective alternative triggers is novel and demonstrates considerable promise in both efficacy and efficiency over existing approaches. Despite this strong foundation the studys focus on patchbased triggers and reliance on manual intervention may restrict its application. To enhance the papers value and impact investigating automated techniques for trigger identification and broadening the attack applicability to a wider variety of backdoor mechanisms are recommended avenues for future work. The inclusion of extensive empirical validation and a user study further strengthens the papers claims providing a solid proof of concept that invites additional scrutiny and replication within the community. Overall given its innovative approach strong empirical support and significant implications for the security of machine learning models I recommend this paper for publication conditional upon considerations into expanding its method versatility and exploring avenues to mitigate its current limitations.", "tsg-Lf1MYp": " Peer Review  1. Summary of the Paper The paper introduces a novel task Natural Attributebased Shift (NAtS) detection which aims to identify samples that deviate from a training distribution due to changes in natural attributes (e.g. age time and brightness). The authors highlight the importance of NAtS detection over traditional OutofDistribution (OOD) detection for ensuring the reliability of deep learning models in realworld applications. They create benchmark datasets across vision language and medical domains by adjusting natural attributes in existing datasets to simulate NAtS. The paper conducts an extensive analysis of conventional OOD detection methods on these datasets and observes inconsistent performances. Through a detailed exploration of the behavior of NAtS samples in feature space three NAtS categories are proposed. Additionally a novel training objective combining classification distance and entropy losses is suggested to enhance the performance of OOD detection methods across all NAtS categories showing considerable improvement especially for the Mahalanobis detector.  2. Strong Points  Originality: The paper addresses a very relevant and underexplored problem. The introduction of NAtS detection as a task along with the creation of benchmark datasets is a substantial contribution.  Comprehensive Analysis: The extensive evaluation of existing OOD methods on NAtS datasets and the detailed analysis of NAtS samples behavior in feature space provide valuable insights into the challenges of NAtS detection.  Methodological Novelty: The novel training objective that incorporates classification distance and entropy losses is an innovative approach that directly responds to the limitations identified in traditional OOD detection methods.  Weak Points  Dataset Generation: Its unclear how the age time and brightness attributes were precisely adjusted for creating the NAtS datasets. More details on the process would strengthen the reproducibility and the understanding of the potential biases introduced during dataset creation.  Generalizability: The papers focus on three specific natural attributes may limit the generalizability of the findings. It would be beneficial to test the proposed method on NAtS samples affected by a wider range of attributes.  Comparison with StateoftheArt: While the paper compares the proposed method against baselines it lacks a direct comparison with the latest stateoftheart OOD detection methods that could have similar or complementary objectives.  Questions and Feedback  Can the authors provide more details on the criteria used for adjusting natural attributes while creating the NAtS datasets  It would be helpful if the authors could discuss the potential limitations of their proposed training objective especially concerning the scalability to largescale datasets and different types of natural attributes not covered in this study.  Have the authors considered the impact of class imbalance within the NAtS datasets on the performance of OOD detection methods  3. Review Summary This paper tackles the novel and relevant problem of Natural Attributebased Shift (NAtS) detection in deep learning models. The introduction of benchmark datasets and the comprehensive analysis of NAtS samples in feature space are significant contributions that address an important research gap. The proposed training objective is an innovative solution that demonstrates potential for improving the reliability of machine learning models in realworld settings. However the paper could be strengthened by providing more details on the dataset generation process discussing the generalizability of the findings across a broader range of attributes and including a comparison with the latest OOD detection methods. Overall the paper makes a valuable contribution to the field and with some improvements it could have a substantial impact on research and applications involving deep learning in realworld scenarios.", "fy_XRVHqly": " Review of \"StructureaWAre Transformer (SWAT) for Modular Reinforcement Learning\"  1. Summary This paper introduces the StructureaWAre Transformer (SWAT) a novel approach to Modular Reinforcement Learning (MTRL) that efficiently incorporates morphological information into policy learning. It specifically addresses the challenges in inhomogeneous MTRL where agents with variable state and action space dimensions across tasks necessitate adaptable policy models. Traditional methods while leveraging Graph Neural Networks (GNNs) to account for agent morphology face limitations in multihop message passing leading to information oversmoothing. The proposed SWAT model through traversalbased positional embedding (PE) and graphbased relational embedding (RE) injects structural biases related to agent morphology into a transformerbased policy model enabling direct message communication while preserving morphological information. Empirical evidence demonstrates SWATs superiority over existing stateoftheart techniques in multitask learning and transfer learning scenarios across diverse robotic locomotion tasks.  2. Strong and Weak Points Questions and Feedback Strong Points: a) Innovation: The innovative incorporation of traversalbased PE and graphbased RE into the transformer model for leveraging morphological information represents a significant advancement in the field of MTRL. It addresses the critical gap left by previous works that either suffered from information oversmoothing or forfeited the morphological information entirely. b) Empirical Validation: The comprehensive experimental setup including tests in multitask learning transfer learning and even zeroshot settings solidly validates the proposed method. The performance comparison with existing methods (SMP and AMORPHEUS) on benchmark tasks convincingly illustrates SWATs effectiveness. c) Theoretical Motivation: The paper presents a convincing theoretical motivation for structureaware policy modeling in modular reinforcement learning. The detailed discussion on the limitations of existing approaches and the rationale behind the proposed solution provide a clear understanding of SWATs significance. Weak Points: a) Complexity and Scalability: The paper lacks an explicit discussion on the computational complexity of the SWAT model. Given the sophisticated structure of the model concerns about scalability and computational demands across very large action and state spaces or extremely diverse morphologies remain unaddressed. b) Generalization beyond Robotics: While the paper demonstrates superior performance in robotic locomotion tasks its unclear how SWAT performs in nonrobotic domains or tasks with less pronounced morphological structures. Expanding the scope of validation could enhance the general applicability of the proposed method. c) Interpretability of Embeddings: The paper provides limited insight into the interpretability of the positional and relational embeddings. Understanding how SWAT explicitly utilizes morphology for decisionmaking could offer more profound insights into its learning mechanism. Questions and Feedback: 1. Can the authors elaborate on the computational complexity of the SWAT model particularly in comparison to the baseline methods 2. It would be beneficial if the paper could discuss the applicability of SWAT in nonrobotic or less morphologycentric domains. Are there any limitations or adaptations required 3. An exploration into the interpretability of the embeddings used in SWAT could provide valuable insights. Can the authors discuss any efforts made or future plans to analyze what the model learns or how it makes decisions based on morphological embeddings  3. Review Summary The paper presents a highly innovative and empirically validated approach to Modular Reinforcement Learning (MTRL) introducing the StructureaWAre Transformer (SWAT). SWAT represents a significant step forward in addressing the challenges of inhomogeneous MTRL by effectively leveraging agent morphological information. The strengths of the paper lie in its novel methodology extensive empirical validation and clear theoretical motivation. However further exploration into the models computational complexity generalization capabilities beyond robotic tasks and interpretability of the embeddings could strengthen the work. The potential impact of this research is substantial posing SWAT as a new stateoftheart in MTRL with promising directions for future research in understanding and improving upon the integration of morphological information in policy learning. Given these considerations I recommend this paper for publication contingent on addressing the discussed weaknesses particularly clarifying its computational demands and broader applicability.", "oSP1hwZB24": "Summarize the paper: The paper introduces a novel method for clickthrough rate (CTR) prediction in recommendation systems by proposing a Dynamic Parameterized Operation (DPO). This method is designed to capture both explicit and implicit interactions of features on an instancewise basis enhancing the adaptiveness and effectiveness of featurebased modeling and user behavior modeling. The authors demonstrate that incorporating DPO into deep neural networks (DNNs) and Attention modules significantly outperforms existing stateoftheart methods on public and realworld datasets. The methodology is grounded in dynamically generating kernels conditioned by the instance which allows for a more nuanced capturing of feature interactions. The deployment of this model in a large ecommerce companys ranking system indicates its practical value. The paper also discusses the theoretical formulation of DPO and its various implementations for feature and user behavior modeling showcasing its versatility and efficiency across different tasks. Strong Points: 1. Novelty and Practical Impact: The introduction of DPO is innovative addressing the inefficiencies and inflexibilities in modeling feature relations in CTR predictions. Its successful deployment in a realworld production system indicates its practical viability and potential for largescale impact. 2. Comprehensive Methodology: The paper provides a detailed formulation of DPO and explores its applications in both featurebased modeling and user behavior modeling. This comprehensive approach not only showcases the versatility of DPO but also strengthens the paper\u2019s contribution to the field. 3. Extensive Evaluation: The authors conducted thorough experiments on both public and realworld datasets outperforming existing methods. The inclusion of an online AB test further authenticates the models effectiveness in a live environment. Weak Points: 1. Complexity and Scalability Concerns: Despite the models effectiveness the complexity of DPO especially in terms of computational resources might pose scalability issues in some contexts. The paper could benefit from a deeper discussion on handling largescale data and reducing computational cost. 2. Comparative Analysis: While the paper does a great job of comparing DPO with stateoftheart methods a more detailed comparison with recent advancements in the field highlighting the unique advantages and possible limitations of DPO would provide readers with a clearer understanding of its position within the research landscape. 3. Practical Implementation Details: While the deployment in a realworld system is mentioned the paper could provide more insights into the operational challenges latency considerations and model maintenance over time. This would be valuable for readers interested in the practical aspects of implementing such a system. Questions and Feedback to the Author: 1. Could you elaborate on the scalability of the DPO method particularly in contexts with extremely large datasets Are there any strategies to mitigate potential computational costs 2. Can you provide more comparative insights into how DPO stands against the very latest methods not covered in the paper This could include discussions on efficiency accuracy and model complexity. 3. More details on the practical deployment of the system would be appreciated. Specifically how does the model adapt to evolving user behaviors and item features over time in a live environment Review Summary: The paper makes a significant contribution to the field of CTR prediction by introducing Dynamic Parameterized Operation (DPO) which effectively captures both explicit and implicit feature interactions instancewisely. The novelty of the approach combined with comprehensive methodology and extensive evaluation strengthens the papers impact. The models deployment in a realworld system further validates its practical applicability and effectiveness. However discussions on scalability more detailed comparative analysis and deeper insights into practical implementation details could further enhance the paper. Despite these minor areas for improvement the strengths of the paper clearly outweigh its weaknesses suggesting a strong recommendation for its publication. The novel insights provided along with the demonstrated practical impact set a valuable foundation for future research in the field of recommendation systems and CTR prediction.", "nRCS3BfynGQ": " Review of the Paper: Novel Graph Network Architectures with Equivariance to Transformations  Summary This paper introduces two innovative graph network architectures designed to leverage symmetries in data specifically targeting improvements in generalization and efficiency in learning. The first architecture termed the Distance Preserving Graph Network (DGN) achieves equivariance to transformations preserving distances between node coordinates effectively handling operations within the Euclidean group. The second architecture the Angle Preserving Graph Network (AGN) extends this concept to anglepreserving transformations aligning with the conformal groups requirements. Both architectures aim to utilize inherent data symmetries to promote data efficiency intrinsic inductive bias and superior generalization capabilities. The authors experimentally validate their approaches using a synthetic dataset of ndimensional geometric objects illustrating the advantages of their models over traditional graph network approaches and acknowledging limitations when applicable symmetries are absent.  Strong Points 1. Innovative Approach: The paper explores the relatively underexplored concept of exploiting symmetries in graphstructured data presenting a compelling argument for its potential in enhancing learning efficiency and generalization. 2. Theoretical Foundation: The authors provide a robust theoretical framework for their architectures grounding their designs in group theory and equivariance principles. This theoretical rigor adds credibility to their approach. 3. Experimental Validation: Through experimental evaluation using synthetic datasets the paper successfully demonstrates the practical benefits of the proposed architectures in terms of data efficiency and generalization capabilities. 4. Acknowledgment of Limitations: The authors fairly acknowledge cases where their architectures may not be beneficial showing an understanding of the scope and applicability of their methodologies.  Weak Points 1. Breadth of Experimental Evaluation: While the synthetic dataset experiments provide initial validation the study would benefit from a broader experimental evaluation involving more diverse and complex realworld datasets to thoroughly assess the proposed architectures capabilities and limitations. 2. Comparison with StateoftheArt: The paper could strengthen its argument by including a more comprehensive comparison with existing stateoftheart graph networks particularly those that have incorporated some form of symmetry exploitation. 3. Implementation Details: For reproducibility and further examination by the community the paper could provide more detailed insights into the implementation of the proposed architectures including hyperparameter settings and training protocols.  Questions and Feedback to the Author 1. Extension to RealWorld Datasets: Could the authors provide insights or preliminary results on the applications of their architectures to realworld datasets especially those with less explicitly defined symmetries 2. Impact of Symmetry Misidentification: How sensitive are the proposed architectures to cases where assumed symmetries do not exactly match the data characteristics Could such misidentification significantly impair performance or learning efficiency 3. Interoperability with Other Architectures: How easily can the proposed equivariant architectures be integrated with or adapted to other existing graph network models especially those already deployed in practical applications  Review Summary This manuscript presents a significant contribution to the field of graph network architectures through its novel approach to leveraging data symmetries for enhanced learning efficiency and generalization. The theoretical underpinnings are solid and initial experimental results are promising. However for a more comprehensive understanding of the architectures capabilities and limitations further experimentation with diverse realworld datasets is necessary. Additionally a more detailed comparison with existing stateoftheart methods could better position the proposed architectures within the broader research landscape. Despite these areas for enhancement the work substantially advances our understanding and application of equivariance in graph network models and opens up interesting directions for future research.", "l_amHf1oaK": " Review of \"A MultiNeuron RelaxationBased BaB Framework for Neural Network Verification\"  Summary This paper introduces a novel neural network verification approach termed MultiNeuron Constraint Guided BaB (MNBAB) that integrates the strengths of multineuron convex relaxation and BranchandBound (BaB) techniques. This hybrid strategy significantly improves the verification process particularly for larger and less regularized networks by leveraging multineuron relaxations to reduce subproblem generation and employing a GPUbased dual optimizer for solving these problems efficiently. The authors claim that MNBAB surpasses the current stateoftheart in terms of certified accuracy by up to 28 on challenging benchmarks indicating a substantial advance towards verifying practically relevant neural networks.  Strong Points 1. Innovation: The paper presents an innovative approach by combining multineuron convex relaxations with a BaB procedure addressing the scalability and precision challenges in neural network verification. 2. Empirical Evaluation: The extensive experiments and benchmark comparisons offer solid evidence of MNBABs effectiveness and efficiency over existing methods. The inclusion of challenging networks in the evaluation underscores the practical relevance of the proposed verifier. 3. Reproducibility: The authors provide all necessary code and scripts for reproducing the experiments which is commendable and supports the transparency and the academic value of the work.  Weak Points 1. Theoretical Foundation: While the empirical results are impressive the paper could benefit from a more indepth theoretical analysis of why the proposed multineuron constraints and the GPUbased dual optimizer lead to such significant improvements. Such analysis could provide deeper insights into the methods efficacy and potential limitations. 2. Comparison with Other Methods: Although the paper compares MNBAB with several stateoftheart methods it lacks a detailed discussion on how MNBABs components individually contribute to its superior performance. A more granular comparison or an ablation study could offer valuable insights into each components significance. 3. Complexity and Performance Tradeoffs: The paper reports substantial gains in certified accuracy but provides limited discussion on the computational resources required (other than mentioning GPU utilization) and how these may affect the methods applicability in different practical scenarios.  Questions and Feedback to the Author 1. Theoretical Insights: Can the authors provide more theoretical insights or intuitions behind the specific design choices of MNBAB particularly concerning the efficiency gains from the multineuron constraints 2. ComponentSpecific Contributions: Could the authors elaborate on how each key component of MNBAB (multineuron constraints GPUbased dual optimizer branching heuristics) individually contributes to its overall performance 3. Resource Utilization: How does MNBABs resource utilization (e.g. memory and GPU requirements) compare with existing methods especially when verifying largescale networks Are there any scalability limitations 4. Impact of Network Architecture: The paper demonstrates impressive results on fullyconnected convolutional and residual networks. How does MNBAB perform on other architectures such as transformers or recurrent neural networks  Review Summary The paper presents MNBAB a highly innovative and effective method for neural network verification that cleverly integrates multineuron convex relaxations with a BranchandBound procedure. The extensive empirical evaluation showcases notable improvements over existing stateoftheart verifiers especially on challenging networks marking a significant step forward in the field. However the paper could be strengthened by providing deeper theoretical analysis more detailed comparisons with other methods and discussing the implications of computational resource requirements. Given the clear presentation thorough experimentation and significant contributions to neural network verification I recommend this paper for publication contingent on addressing the aforementioned feedback to solidify its contributions further.", "hJk11f5yfy": " Review of \"Hierarchical Deep Learning for Improved Robustness against Subpopulation Shift in Image Classification\"  Summary This paper presents an innovative approach to addressing subpopulation shift in image classification tasks by incorporating hierarchical structures into deep learning models. The authors argue that standard neural networks while excelling in image classification tasks often fail to account for hierarchical relationships among classes leading to decreased performance when faced with novel unseen classes. To overcome this limitation the paper introduces a conditional supervised training framework that leverages hierarchical information through labels enabling the model to better adapt to subpopulation shifts. The proposed method is evaluated using custom datasets derived from ImageNet and a subpopulation benchmark dataset showing improved accuracy and reduction in catastrophic mispredictions compared to traditional flat models.  Strong Points 1. Innovative Approach: The papers novel contribution of integrating hierarchical information into deep learning models addresses a significant challenge in image classification\u2014subpopulation shift. This approach is both logical and innovative building on the intuition that understanding the highlevel structure among classes can aid in generalizing to unseen classes. 2. Comprehensive Methodology: The methodology is thorough starting from the motivation and theoretical foundation to the practical implementation involving hierarchical encoding conditional training and performance evaluation. The use of a conditional supervised learning framework to enforce hierarchical learning is particularly wellconceived. 3. Empirical Validation: The empirical evaluation is robust with experiments conducted on custom datasets and an existing subpopulation benchmark dataset. The improvements reported in accuracy and reduction in catastrophic coefficient serve as strong evidence for the efficacy of the proposed approach. 4. Clarity and Organization: The paper is wellwritten with a logical flow that smoothly transitions from explaining the motivation behind the study to detailing the proposed method and discussing the experimental results. The use of figures to illustrate hierarchical structures and the encoding mechanism helps in understanding the complex concepts introduced.  Weak Points 1. Lack of Comparison with Related Works: While the paper mentions related works in hierarchical learning and subpopulation shift it lacks a direct comparison of the proposed method with these existing approaches. Specifically a benchmarking against stateoftheart methods in handling hierarchical information or addressing subpopulation shift could have further validated the superiority of the proposed framework. 2. Generalization to Other Datasets and Tasks: The experiments are limited to image classification tasks on datasets derived from ImageNet. The generalizability of the proposed method to other types of datasets (e.g. text audio) and different tasks beyond classification (e.g. object detection segmentation) remains unexplored. 3. Detailed Explanation of Hierarchical Encoding: The paper could benefit from a more detailed explanation or a dedicated section on the process of hierarchical encoding detailing how exactly the hierarchical information is integrated into the learning process. This would aid readers in understanding and possibly replicating the methodology. 4. Consideration of Computational Costs: There is no discussion on the computational costs or increased complexity associated with the proposed hierarchical learning approach. Given the importance of computational efficiency in deploying deep learning models insights into the tradeoffs involved would have been valuable.  Questions and Feedback to the Author 1. Could the authors provide a direct comparison of the proposed method with at least two existing approaches that address subpopulation shift or incorporate hierarchical information in deep learning models 2. How generalizable is the proposed method to other domains of data (e.g. text audio) and tasks beyond classification Are there specific adaptations required for such generalization 3. Can the authors elaborate on the hierarchical encoding process particularly on how the hierarchical information is integrated into the training process 4. What are the computational costs associated with the proposed hierarchical learning method compared to traditional flat models How does the increase in model complexity impact training time and inference speed  Review Summary The paper makes a significant contribution to the field of deep learning for image classification by focusing on the problem of subpopulation shift. The proposed method of incorporating hierarchical information into the learning process is both innovative and effectively addresses a known limitation of standard neural networks. The empirical results provide strong evidence for the methods efficacy demonstrating improvements in classification accuracy and a reduction in the impact of catastrophic mispredictions. However the paper could be strengthened by including direct comparisons with existing methods that tackle similar challenges offering insights into the generalizability of the proposed approach across different domains and tasks and providing a more detailed description of the hierarchical encoding process. Additionally a discussion on the computational costs would offer a more comprehensive view of the proposed methods practical implications. Overall the paper presents a promising avenue for enhancing the robustness of deep learning models against subpopulation shifts through hierarchical learning. With further validation and elaboration on the mentioned concerns the proposed method has the potential to significantly impact the field.", "rq1-7_lwisw": "Review of \"Challenging Object Concept Learning for Embodied AI: A Causal Intervention Approach\" 1. Summary This paper introduces a novel task in the field of artificial intelligence particularly focusing on embodied AI termed Object Concept Learning (OCL). The task aims to enhance the understanding of objects beyond mere recognition it necessitates machines to deduce objects affordances\u2014what actions can be performed with them\u2014and the attributes facilitating these affordances. To support the OCL task the authors have constructed an extensively annotated knowledge base categorizing objects their attributes affordances and the causal relations binding these concepts. Additionally they propose a baseline model called Object Concept Reasoning Network (OCRN) that uses causal intervention and concept instantiation techniques to infer these relations effectively. The OCRN demonstrates notable performance in inferring object knowledge while adhering closely to the causal structures designed. 2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper addresses a crucial gap in embodied AI by pushing for an understanding of objects beyond recognition. The proposal of the OCL task is innovative aiming for a deeper semantic and functional understanding of objects.  Comprehensive Dataset: The construction of an extensively annotated dataset encompassing categories attributes affordances and their causal relations is a commendable effort. This dataset not only supports the OCL task but also sets a benchmark for future research in the field.  Effective Baseline Model: The OCRN with its causal intervention and concept instantiation mechanisms exemplifies a significant step towards addressing the OCL task. Its ability to adhere to causal relations while performing inference is a commendable strength. Weak Points:  Complexity and Accessibility: The complexity of implementing causal reasoning models might pose a barrier to entry for researchers without a strong background in causal inference. Also the paper could further detail the practical challenges and limitations encountered during OCRN development and dataset annotation.  Evaluation Metrics: While the use of mean Average Precision (mAP) and Total Direct Effect (TDE) for evaluation are apt additional insights could be gained by including more diverse metrics especially those evaluating the qualitative aspects of affordance and attribute reasoning. Questions and Feedback:  Scalability of OCRN: How well does OCRN scale with increasingly diverse and larger datasets Are there any computational constraints or efficiency concerns with the proposed causal intervention technique  Generalizability Across Contexts: How does the model perform in varied or unforeseen contexts given that affordances can significantly vary with environmental changes  Further Exploration on Negative Causality: The paper mentions annotating and understanding positive causal relations. However could further insights be provided on how negative causality (e.g. why an object cannot perform a certain affordance due to an attribute) is treated in the OCRN model 3. Review Summary This paper presents a significant contribution to the advancement of embodied AI by introducing the novel Object Concept Learning task addressing a key limitation in current AI systems understanding of objects. By constructing an extensive dataset and proposing a baseline model that effectively employs causal reasoning for object understanding the authors set a solid foundation for future research in this domain. The OCRN model with its commendable performance in adhering to causal relations represents an important step towards deeper reasoningbased object understanding. However the paper would benefit from a more indepth discussion on the challenges and limitations associated with the OCL task OCRN model development and dataset annotation. Questions remain regarding the scalability of the OCRN model its performance across diverse and unforeseen contexts and its handling of negative causality which could be areas for future investigations. In conclusion the paper is a commendable effort towards enhancing machine understanding of objects by linking attributes affordances and causal reasoning. With potential improvements and further exploration the work presented could significantly impact the field of embodied AI and beyond.", "o0ehFykKVtr": " Review Draft 1) Summary of the paper The paper introduces a novel approach termed \"robotaware control\" (RAC) specifically designed for transferring visual control policies from one robot to another without the necessity for robotspecific data collection. The core innovation lies in the separation of visual dynamics models into robotaware world dynamics and robotspecific dynamics which when combined significantly enhance the transferability of visual modelbased reinforcement learning (MBRL) policies. This mechanism permits the application of zeroshot transfer of visual manipulation skills to new robots. The methodology includes a visual planning cost that segregates the robot agent from its environment enabling policy application to different robots without additional data. Experiments were conducted using tabletop manipulation tasks both in simulation and with real robots providing proof of the concepts effectiveness in transferring skills across robots with diverse configurations. 2) Strong and weak points Questions and Feedback to the author Strong Points:  The paper tackles an important challenge in robotics and RL: reducing the need for extensive robotspecific data collection for visual control policy training. The proposed solution addresses a clear gap in the literature.  The \"robotaware control\" paradigm is wellconceptualized and represents an innovative approach by integrating robotaware world dynamics with robotspecific dynamics for skill transfer.  Experiments showcase the practical applicability of the proposed method across both simulated and realworld settings enhancing the papers credibility.  The zeroshot transfer capability is a significant achievement marking a step forward in the field of robotic manipulation tasks. Weak Points:  The paper lacks a detailed discussion on the limitations of the RAC paradigm particularly concerning its scalability to more complex tasks beyond tabletop manipulation or in environments with dynamic changes.  There is minimal analysis on the impact of the choice of the robotspecific dynamics model on the overall systems performance especially in realworld scenarios.  The comparison with other stateoftheart methods is not exhaustive. It would be beneficial to have a broader comparison to highlight the advantages of RAC over existing methodologies. Questions and Feedback:  Could the authors provide insight into how the RAC approach might be adapted or scaled to more complex manipulation tasks or dynamic environments  How dependent is the method on the accuracy of the robotspecific dynamics model Would inaccuracies in this model significantly detract from performance  A broader comparison with existing methods including domain randomization techniques and simulationtoreal transfer methods could enhance the papers impact. Can the authors discuss how RAC might perform in comparison to these existing strategies 3) Review Summary The paper presents a promising approach towards facilitating the transfer of visual control policies across different robots without necessitating substantial robotspecific data collection. The introduction of the \"robotaware control\" concept is both innovative and relevant addressing a critical challenge in the field of robotics and reinforcement learning. Through the decoupling of world and robot dynamics the authors demonstrate the potential for zeroshot transfer of manipulation skills which is convincingly supported by experimental results. However the paper would benefit from a deeper discussion on its limitations and scalability especially concerning more complex tasks and dynamic environments. Additionally providing a broader comparison to existing methods could better position RAC within the landscape of current research delineating its unique advantages and potential areas for further exploration. Ultimately the paper contributes a valuable perspective to the ongoing discourse in robotic manipulation and policy transfer. With further refinement and elaboration on the points raised it has the potential to significantly impact the field.", "rUwm9wCjURV": " Summary The paper explores the advancement of deep reinforcement learning (DRL) agents tasked to execute outofdistribution (OOD) multitask instructions expressed in temporal logic (TL). The novelty lies in proposing a deep learning configuration that integrates inductive biases to aid agents in generating latentgoal representations for enhanced OOD generalization performance. The proposed latentgoal architectures are tested within a neurosymbolic framework across different stateoftheart neural networks to evaluate their efficacy in generalizing unseen instructions in OOD scenarios. The research demonstrates that latentgoal networks significantly outperform traditional architectures providing a new avenue for developing DRL agents capable of comprehending and acting upon complex multitask instructions in dynamic environments.  Strong Points 1. Innovative Approach: The introduction of latentgoal architectures represents a significant advancement in aiding DRL agents to generalize instructions in OOD scenarios. This novel approach addresses the primary challenge of enabling agents to understand and execute complex tasks they have not encountered during training. 2. Comprehensive Evaluation: The paper thoroughly assesses the performance of the proposed architectures against various stateoftheart networks across different benchmarks. The inclusion of ablation studies further elucidates the internal mechanics of the novel configuration showcasing its superiority and the factors contributing to its effectiveness. 3. Practical Implications: By demonstrating how agents can better generalize instructions in unseen environments this research has profound implications for developing autonomous agents in the real world where scenarios are dynamic and unpredictable.  Weak Points 1. Limited Exploration of Relational and Attentional Mechanisms: While the paper highlights the integration of relational networks and attention mechanisms the exploration into how these components specifically contribute to the observed improvement in generalization is limited. More detailed analysis or comparisons could enrich the understanding of their roles. 2. Scalability Concerns: The experiments are confined to specific benchmarks and scenarios. For widespread application it would be crucial to assess the scalability of the proposed architecture to more complex environments and a broader variety of tasks. 3. Discussion on Computational Overheads: There is minimal discussion on the computational costs associated with implementing latentgoal architectures particularly in comparison with traditional models. Given the complexity of the proposed approach understanding its computational efficiency is essential for practical applications.  Questions and Feedback 1. Clarification on the Role of Relational and Attention Mechanisms: Can the authors delve deeper into how relational networks and attention mechanisms specifically contribute to the superior performance of latentgoal architectures 2. Scalability and Generalization Beyond Benchmarks: How do the authors foresee the scalability of the proposed architecture in more complex realworld scenarios Can the model generalize across drastically different environments and tasks 3. Computational Efficiency: Could the authors comment on the computational efficiency of latentgoal architectures How do the computational costs compare to traditional models especially in larger more complex environments  Review Summary This paper contributes significantly to the field of DRL by introducing latentgoal architectures for better instruction generalization in OOD scenarios. The proposed method marks a notable advancement in teaching agents to execute complex multitask instructions unseen during training. The thorough evaluation against stateoftheart networks across various benchmarks demonstrates the effectiveness and superiority of the proposed approach. However the research could be strengthened by a more indepth analysis of the relational and attention mechanisms roles within the architecture a detailed discussion on scalability and generalization capabilities beyond the benchmarks presented and an examination of the computational overheads involved. Addressing these aspects would provide a more comprehensive understanding of the latentgoal architectures potential and limitations. Overall the findings contribute valuable insights into DRL generalization strategies opening new avenues for research and application in autonomous agents capable of navigating and acting in dynamically changing environments.", "y_op4lLLaWL": "Review of \"Implicit Bias of Gradient Descent in Training Variational Autoencoders\" 1. Summary of the Paper This paper offers a comprehensive examination of the training dynamics of Variational Autoencoders (VAEs) especially in the context of data supported on lowerdimensional manifolds. Specifically it revisits conjectures made by Dai and Wipf (2019) concerning the training outcomes of VAEs particularly whether the generator\u2019s covariance converges to zero and if the training leads to a generator that learns the correct support equal to the ground truth manifold. Through a blend of theoretical analysis and empirical studies the paper demonstrates that for linear encodersdecoders VAEs under standard gradient descent training do converge to generators with support identical to the data manifold attributed to the implicit bias of gradient descent rather than the VAE loss structure alone. Conversely it shows that in nonlinear cases the trained VAEs typically converge to learn a higherdimensional manifold than the actual data manifold presenting an avenue where the initial conjecture does not hold. 2. Strong Points  Novelty and Depth of Analysis: The paper delves into an understudied aspect of VAE training providing a rigorous theoretical framework complemented by empirical evidence. This dual approach significantly contributes to the understanding of VAE behaviour particularly addressing the implicit bias of gradient descent in training dynamics.  Relevance: Given the ubiquitous application of VAEs in generative modeling and the wide acknowledgment of training challenges this investigation is not only timely but also addresses a critical gap in existing literature.  Clarity and Organization: The paper is wellstructured with a clear exposition of concepts methodological rigor in theoretical derivations and a thoughtful presentation of empirical results. The separation of discussions into linear and nonlinear VAEs aids in digestibility and clarity. 3. Weak Points  Generalizability of Findings: While the paper sheds valuable light on the behavior of linear and certain nonlinear VAE architectures the specific types of nonlinearities and the manifolds examined are somewhat constrained. Expanding the variety of nonlinear mappings and exploring more diverse data manifolds could enhance the generalizability of the findings.  Broader Implications: Although the paper meticulously covers the proposed scope a more indepth exploration of how these findings translate to practical suggestions for improving VAE training particularly in challenging realworld scenarios could make the conclusions more impactful.  Questions and Feedback to the Author  Could further elaboration be provided on whether the findings have specific implications for the design of encoder and decoder architectures especially in practical settings where the data manifold may not be well understood ahead of training  The conclusion points toward a need for future research into the interplay between VAE architecture and optimization techniques. Could the authors speculate on potential approaches or modifications to current practices that might stem from this work 4. Review Summary This paper provides significant insights into the implicit bias of gradient descent in VAE training challenging and extending upon previous conjectures with compelling theoretical and empirical evidence. Its strength lies in its rigorous analysis which combines theoretical derivations with empirical validation to explore the nuances of VAE behavior in both linear and nonlinear settings. However the paper could benefit from a broader consideration of the practical implications of its findings and further exploration of the effects of a wider variety of nonlinear mappings and data manifolds. Overall this work marks a meaningful advancement in our understanding of VAEs suggesting directions for future research aimed at refining training methodologies for improved generative modeling. Recommendation: Accept provided the authors can address or at least discuss the broader implications of their findings for practical VAE training scenarios and possibly expand upon the types of nonlinearities and data manifolds studied to enhance the generalizability and impact of their work.", "zFlFjoyOW-z": " Review of \"A Framework for Learning InterestBased Item Representations in Recommendation Systems Using User Multi Interests Capsule Network (MICN)\"  1. Summary The paper presents an innovative framework designed to enhance item representation learning for recommendation systems by focusing on capturing user interests more accurately through a User Multi Interests Capsule Network (MICN). Traditional methods in the domain have predominantly been itemcentric emphasizing the optimization of itembased mechanisms between user interaction sequences and candidate items. These methods have not adequately addressed the need to directly model user interests leading to potential inefficacies in reflecting users multifaceted interests within recommendation systems. To address this the authors introduce MICN as an auxiliary task within a generic framework that can be easily integrated with existing recommendation models avoiding the need for extensive model redesign or augmentation with external datasets. The proposed system aims to conjointly learn itembased and interestbased item representations thereby providing a more nuanced understanding of user interests which as demonstrated through experiments on benchmark datasets results in significant performance improvements across various recommendation models. By integrating MICN the paper endeavors to refine the recommendation process making it more responsive to varied user interests without complicating the underlying model architecture.  2. Strong and Weak Points Questions and Feedback  Strong Points:  Innovative Approach: The introduction of a User Multi Interests Capsule Network (MICN) for learning interestbased item representations is novel and addresses a significant gap in current recommendation systems.  Compatibility and Flexibility: The design of MICN as an auxiliary task ensures compatibility with existing recommendation models facilitating easy adoption without necessitating major architectural modifications or additional datasets.  Empirical Validation: The authors have conducted comprehensive experiments across multiple benchmarks and recommendation models demonstrating the efficacy of MICN in enhancing recommendation performance.  Weak Points:  Generalization and Scalability Concerns: While the paper mentions the framework\u2019s adaptability it lacks a detailed exploration of its scalability across very large datasets and its generalization capabilities across diverse domains beyond the ones tested.  Limited Discussion on User Privacy: Incorporating user interest in such a detailed manner might raise concerns regarding user privacy and data security. The paper does not address how these concerns are mitigated within the framework.  Hyperparameter Optimization Detailing: Although the paper discusses hyperparameter optimization it does not provide indepth insights or guidelines on how these parameters were chosen or can be finetuned in practical applications.  Questions and Feedback: 1. Can the authors elaborate on the scalability of the MICN framework when applied to extremely large datasets or realtime recommendation scenarios 2. How does the MICN approach handle user privacy and data security especially considering the detailed user interest modeling 3. More details on hyperparameter selection and optimization methodologies would be beneficial for practitioners looking to implement or extend the MICN framework. Can the authors provide these insights  3. Review Summary The paper introduces a significant advancement in recommendation systems through the development of a framework that leverages User Multi Interests Capsule Network (MICN) to learn interestbased item representations. This approach not only fills a crucial gap in the literature by moving beyond itemcentric models to directly incorporate user interests but also does so in a manner that is flexible and easily integrated with existing systems. The empirical evidence provided strongly supports the effectiveness of the proposed method in improving recommendation accuracy across a variety of models and benchmarks. However the paper could benefit from addressing potential scalability challenges detailing the handling of user privacy concerns and providing more comprehensive guidelines on hyperparameter optimization. Given the strong performance improvements demonstrated and the novelty of the approach this paper represents a valuable contribution to the field of recommendation systems. However for a more holistic assessment and to facilitate practical application addressing the highlighted considerations would further strengthen the papers impact and utility.", "kj0_45Y4r9i": " Review of \"Clustering by Discriminative Similarity (CDS)\"  Summary This paper presents a novel framework Clustering by Discriminative Similarity (CDS) which aims to improve data clustering by learning discriminative similarities between data points. The core idea is to model the data clustering problem as a multiclass classification problem and search for an optimal data partition where the associated unsupervised classifier trained on cluster labels has a low generalization error. The discriminative similarity is derived from the generalization error bound for an unsupervised similaritybased classifier which is expressed as the sum of pairwise similarity between data from different classes. The paper introduces CDS via unsupervised Kernel classification (CDSK) a clustering method using a PSD kernel as the similarity function. This method outperforms current clustering algorithms by learning adaptive kernel weights for different data points thereby enhancing clustering performance.  Strong Points 1. Novel Framework: The introduction of a new clustering framework that models data clustering as a multiclass classification problem is innovative. It leverages discriminative similarity derived from the unsupervised classifiers generalization error bound to guide cluster formation. 2. Theoretical rigor: The paper provides a comprehensive theoretical analysis including a generalization error bound for the similaritybased classifier through Rademacher complexity and the relationship between discriminative similarity and kernel density classification. This rich theoretical foundation lends credibility to the proposed method. 3. Experimental Validation: Through a series of experiments on various datasets the paper convincingly demonstrates the superiority of the CDSK clustering method over existing techniques. The detailed comparison with other stateoftheart methods supports the effectiveness of the proposed approach.  Weak Points 1. Assumptions and Model Complexity: The proposed approach assumes a mild classification model which requires an unknown joint distribution over data and its labels. This assumption might limit the applicability of the method to realworld scenarios where such a distribution is not easily assumed. Additionally the models complexity including the need for parameter tuning (e.g. \u03bb kernel bandwidth) could pose difficulties in practical implementations. 2. Computational Efficiency: While the paper discusses the computational complexity the increased computation cost due to iterative coordinate descent optimization especially for large datasets could be a significant drawback. The paper could benefit from a more indepth discussion on scalability and potential optimization strategies. 3. Evaluation Metrics and Baselines: The paper primarily uses Accuracy (AC) and Normalized Mutual Information (NMI) as performance measures. Inclusion of additional metrics such as clustering purity or silhouette score could provide a more rounded evaluation. Furthermore while existing clustering methods serve as baselines the comparison with deep learningbased clustering approaches is missing which might have offered insight into how the proposed method fares against these newer methods.  Questions and Feedback to the Author 1. Could the authors elaborate on how the assumption of a mild classification model affects the methods applicability to datasets with complex or unknown distributions 2. Regarding computational efficiency are there strategies that could be employed to reduce the computational burden especially for larger datasets 3. Have the authors considered comparing CDSK with deep learningbased clustering methods and if so how does CDSK perform in such comparisons  Review Summary Clustering by Discriminative Similarity (CDS) introduces an innovative framework for data clustering by leveraging a novel concept of discriminative similarity derived from generalization error bounds. The theoretical analysis is robust and experimental results demonstrate the methods effectiveness compared to existing clustering techniques. However concerns regarding the models assumptions computational efficiency and evaluation against deep learningbased methods need to be addressed. Overall the paper presents a significant contribution to the clustering domain proposing a method with strong theoretical foundations and empirical validations. Given the potential impact and novel approach I lean towards a positive recommendation subject to the authors addressing the outlined concerns particularly around computational efficiency and broader comparisons with contemporary methods.", "gSdSJoenupI": " Review of \"PolyLoss: A Polynomial Expansion Perspective on Loss Functions\"  Summary The paper introduces a novel loss function framework named PolyLoss which decomposes common classification loss functions such as crossentropy and focal loss into a series of weighted polynomial bases. This decomposition allows for easy adjustment of the importance of different polynomial bases depending on the task and dataset offering a more flexible approach to loss function design. The paper demonstrates that by adjusting a single polynomial coefficient their Poly1 formulation can significantly outperform traditional crossentropy and focal loss in a variety of tasks including 2D image classification instance segmentation object detection and 3D object detection. The papers contributions are significant in that they offer a new unified framework for understanding and improving upon existing loss functions and show the potential for simple modifications to these functions to achieve improved performance across a range of computer vision tasks.  Strong Points 1. Innovative Framework: The PolyLoss framework is a novel approach that unifies the understanding of existing loss functions and provides a systematic way to design and adjust them for various tasks. 2. Simplicity and Effectiveness: The Poly1 formulation is praised for its simplicity\u2014requiring only one extra hyperparameter and one line of code change\u2014while still offering significant improvements over existing loss functions. 3. Extensive Evaluation: The paper is thorough in its experimental evaluation providing extensive results across multiple tasks and datasets which convincingly demonstrate the effectiveness of PolyLoss. 4. Insightful Theoretical Analysis: The mathematical derivation and theoretical insights provided especially on how polynomial coefficients impact the loss function and training process are both clear and enlightening.  Weak Points 1. Lack of Comparison with Other Recent Loss Functions: While the paper does an excellent job comparing PolyLoss to traditional loss functions like crossentropy and focal loss it lacks comparison with other recent developments in loss function design. 2. Potential for Overfitting Not Explored: The paper does not discuss whether the increased performance might lead to overfitting especially given that Poly1 targets performance on specific datasets. 3. Limited Discussion on Intuition behind Optimal Coefficients: While the results show that adjusting the leading polynomial coefficient yields improvements the paper could benefit from a deeper exploration of the underlying reasons why certain coefficients work better than others for given tasks.  Questions and Feedback to the Author 1. Comparison with Other Loss Functions: Could you provide comparisons with other recent advancements in loss function designs to see how PolyLoss stacks up against the cutting edge 2. Overfitting Concerns: Was there any analysis done to check if models trained with PolyLoss are more prone to overfitting especially when hyperparameters are optimized for specific datasets 3. Intuition Behind Coefficient Choices: Can more details be provided on the intuition or guiding principles for choosing the optimal coefficients for different tasks This might help practitioners in adapting PolyLoss more easily to new tasks. 4. Broad Applicability: Your experiments focus primarily on visual tasks. Do you foresee PolyLoss being applicable to other domains such as natural language processing or speech recognition  Review Summary The paper presents a significant advancement in the design of loss functions for deep learning models in various computer vision tasks. The proposed PolyLoss framework through its simple yet effective Poly1 formulation convincingly outperforms traditional loss functions across a wide range of experiments. The frameworks ability to unify and extend existing loss functions using a polynomial basis is innovative and provides new insights into loss function design. However there are areas in which the paper could be strengthened particularly in comparing PolyLoss with other recent loss function advancements exploring the potential for overfitting and providing more intuition behind the selection of polynomial coefficients. Given the clear presentation extensive experimental work and the potential impact of PolyLoss in improving model performance with minimal changes required I highly recommend the acceptance of this paper. The authors are encouraged to address the aforementioned points to further strengthen the paper.", "kezNJydWvE": "Peer Review of the Paper on Dynamic Scene Deblurring Using Reblurring Loss 1) Summary of the Paper This paper introduces a novel approach for dynamic scene deblurring by leveraging the concept of reblurring. Traditional learningbased deblurring methods often result in blurred predictions due to limitations in minimizing the L1L2 pixelwise distances. As a solution the authors propose a model that uses reblurring loss functions both supervised at the training stage and selfsupervised at the inference stage to effectively minimize remaining blurriness in deblurred images. This model not only addresses the perceptual quality issues faced by previous methods but also offers a new perspective on optimizing deblurring tasks by focusing on image sharpness rather than highlevel contextual features. Empirical results demonstrate the effectiveness of the proposed reblurring losses in achieving superior visual sharpness and better perceptual quality metrics on largescale benchmarks and real images. 2) Strengths and Weaknesses Strengths: a. Innovative Approach: The concept of using reblurring as an inverse task to enhance deblurring performance is novel and logically sound. It introduces a fresh perspective in the realm of image deblurring by directly targeting the minimization of blurriness diverging from relying heavily on highlevel semantics or context. b. Comprehensive Evaluation: The paper thoroughly evaluates the proposed method across multiple datasets and compares it against stateoftheart alternatives. By providing extensive test cases including realworld images the authors solidly demonstrate the practical applicability and effectiveness of their approach. c. Generality of the Method: The proposed reblurring loss can be integrated into any learningbased deblurring method which indicates its flexibility and potential widespread applicability. This adaptability can significantly contribute to future research and practical applications in image deblurring. Weaknesses: a. Lack of Theoretical Foundation: While the empirical results are impressive the paper provides limited theoretical insight into why reblurring effectively contributes to better deblurring. A deeper analysis or proof demonstrating the underlying principles of this phenomenon would strengthen the paper. b. Computational Efficiency Concerns: The addition of reblurring modules and the iterative optimization process might introduce significant computational overhead. The paper does not address the computational impact which is crucial for assessing the practical feasibility of the method especially for realtime applications or devices with limited processing capabilities. c. Limited Discussion on Limitations: There is a lack of discussion regarding the potential limitations or scenarios where the proposed method may not perform optimally. An acknowledgment of these aspects and suggestions for future work to overcome them would provide a more balanced view. Questions and Feedback to the Author: 1. Could the authors provide more theoretical background or intuition behind why the process of reblurring can lead to a more effective deblurring strategy 2. How does the computational complexity of the proposed method compare to existing stateoftheart deblurring approaches especially in the context of realtime processing 3. The paper could benefit from a more detailed discussion on any observed limitations or challenges during the experiments. Are there specific conditions or types of blur where the proposed method may struggle 3) Review Summary The paper presents a novel and promising approach to dynamic scene deblurring by introducing reblurring loss functions. This method not only shows superior performance in terms of visual sharpness and perceptual quality on standard benchmarks and real images but also brings a fresh perspective to the field. Despite its strengths including innovation comprehensive evaluation and generality the paper falls short in providing theoretical support for its findings and discussing the methods computational efficiency and limitations. Nonetheless considering the substantial empirical evidence of the methods effectiveness and its potential to be universally applied to various deblurring models I recommend the acceptance of this paper with the suggestion that the authors address the mentioned weaknesses particularly by expanding on the theoretical foundation detailing the computational requirements and discussing potential limitations in future work. The proposed approach has significant implications for the advancement of deblurring techniques and could stimulate further research in this direction.", "gf9buGzMCa": " Review of \"Exploring the Limits of Width for Neural Networks in Universal Approximation with Activation Functions\"  1) Summary of the Paper This paper investigates the boundaries of width in neural network functions concerning their capability for universal approximation under different activation functions. It posits that for neural networks to achieve universal approximation across arbitrary compact sets their width must exceed the input dimension. Specifically it focuses on widthconstrained networks (equal to or less than the input dimension) to explore the extent to which such networks can approximate continuous functions. The authors demonstrate through a maximum principle that networks with continuous and monotonic activation functions cannot universally approximate arbitrary continuous functions on specific compact sets. However they prove that exact fitting of partially constant functions across disjoint compact sets is possible under particular conditions using ReLU activations. The paper also delves into the background of approximation properties of widthbounded neural networks contributing theoretical insights into the kind of feature extraction necessary for the earlier layers of a neural network to solve specific machine learning tasks.  2) Strong and Weak Points Questions and Feedback Strong Points:  Rigorous Mathematical Analysis: The paper provides a comprehensive and rigorous mathematical analysis supporting its claims regarding the limitations and capabilities of widthbounded neural networks.  Indepth Review of Related Work: The paper presents a thorough examination of the existing literature which forms a solid foundation for the presented research highlighting its novelty and contributions.  Practical Insight for Network Design: By examining the effects of network width the paper offers valuable insights for designing neural networks especially concerning feature extraction capabilities needed in the early layers. Weak Points:  Limited Practical Applicability: The focus on narrow networks might limit the practical applicability of the findings as most current applications leverage wider and deeper networks.  Clarity on Assumptions and Generalization: While the paper specifies conditions under which the presented results hold it could improve clarity on how these assumptions might affect the generalization of the findings to more complex realworld tasks.  Experimental Validation: Although the paper presents numerical experiments a more comprehensive experimental section possibly including realworld datasets or comparing performance with wider networks could strengthen its claims. Questions and Feedback: 1. Generalization to Deeper Networks: How might the results presented extend to deeper networks with varying layer width 2. Impact of Different Activation Functions: The paper focuses largely on monotonic and continuous activation functions like ReLU. Could the authors provide insights into how different activation types especially nonmonotonic ones might affect the results 3. RealWorld Applicability: Can the authors discuss the implications of their findings for practical network design particularly in applications requiring complex feature extraction and highdimensional data representation  3) Review Summary The paper makes a significant contribution to the understanding of widthbounded neural networks\u2019 approximation capabilities specifically addressing the universal approximation limitations and possibilities under certain conditions. The rigorous mathematical foundation and thorough literature review stand out as strengths. However the paper could benefit from clearer explanations regarding the broader implications of its assumptions and more extensive experimental validation to bolster its conclusions. Considering the depth of theoretical analysis and the relevance of the findings to neural network design I recommend acceptance but suggest addressing the mentioned weaknesses particularly the expansion of experimental validation and clarification on the papers general applicability to practical neural network design.", "uut_j3UrRCg": " Summarize the Paper The paper introduces a modular lifelong learning architecture designed to efficiently handle hierarchically structured tasks by leveraging learnings from simpler tasks to tackle more complex ones. The architecture capitalizes on a unique internal data structure termed \"sketches\" which encode inputs outputs and module interactions. These sketches facilitate the recognition of frequently occurring patterns leading to the instantiation of new modules or the adjustment of existing ones based on the patterns recurrence and relational mapping. The system is grounded in localitysensitive hashing (LSH) to organize and retrieve these sketches streamlining the process of task identification and module execution. The approach is proven theoretically capable of learning complex tasks through this hierarchical and modular method with empirical successes highlighted in situations where traditional training methods falter. Additionally the architecture autonomously identifies tasks without explicit external annotations by analyzing the sketchbased encoding of data and module interactions. Experiments demonstrate the architectures effectiveness in learning complex conceptual tasks like intersections of halfspaces and multidigit recognition that benefit from a modular learning strategy over traditional endtoend approaches.  Strong and Weak Points Questions and Feedback  Strong Points 1. Innovativeness: The architectures modular design centered around sketches and LSH is a novel approach to lifelong learning. Its ability to learn and leverage simpler tasks to solve more complex ones is a significant advancement. 2. Theoretical Foundation and Empirical Validation: The paper provides a solid theoretical underpinning for the proposed architectures capabilities complemented by empirical evidence showcasing its practical effectiveness. 3. Automatic Task Identification: The architectures ability to autonomously identify and classify tasks without explicit task descriptors demonstrates a high degree of adaptability and intelligence making it a robust lifelong learning system.  Weak Points 1. Complexity and Scalability: The architecture while powerful appears complex in its implementation and operation. Concerns regarding scalability especially in handling a vast number of tasks and the associated computational overhead are not thoroughly addressed. 2. Generalizability: The tested scenarios though illustrating the architectures strengths are limited in scope. Further experiments across diverse domains would bolster confidence in its generalizability. 3. Integration with Existing Systems: The paper does not discuss how this architecture could be integrated with or complement existing machine learning frameworks which could be crucial for practical applications and broader adoption.  Questions and Feedback 1. Scalability: Can the authors elaborate on the scalability of the system especially in terms of computational and memory requirements as the number of tasks and modules grows 2. Domain Generalization: How well does the architecture generalize across significantly different domains or tasks with minimal structural similarity 3. Integration with Existing Frameworks: Could the authors discuss potential strategies for integrating this architecture with current machine learning frameworks and platforms  Review Summary The paper presents a compelling modular lifelong learning architecture that innovatively utilizes sketches and localitysensitive hashing to learn and apply knowledge from simpler tasks to solve more complex tasks hierarchically. This work significantly advances the field of lifelong learning by introducing a theoretically grounded and empirically validated method for automatic task identification and learning. However questions regarding scalability generalizability and integration into existing systems remain. Addressing these concerns would enhance the architectures applicability and adoption in broader machine learning and artificial intelligence domains. Overall the strengths of the proposed approach particularly its novel use of sketches and its autonomous task identification capability outweigh its current limitations. With further development and detailed responses to the raised questions this work has the potential to make a substantial impact on lifelong learning research and applications.", "tJCwZBHm-jW": " Review of \"Exploring the Transferability of 2D Image Models to 3D PointClouds\"  Summary: This paper investigates the transferability of 2D image models to 3D pointcloud understanding by leveraging the fundamental visual similarities between 2D imagery and 3D pointclouds. The authors propose a novel approach called Finetuned Imagepretrained Models (FIP) which involves inflating 2D convolutional filters to 3D and minimally finetuning certain layers (input output and optionally batch normalization layers) of a pretrained image model for pointcloud tasks. Their experimental results across various tasks (classification indoor and outdoor scene segmentation) and datasets (ModelNet 3D Warehouse S3DIS SemanticKITTI) indicate that such transferred models not only achieve competitive performance but also offer significant improvements in data efficiency and training speed.  Strong Points: 1. Originality and Significance: The paper tackles an important problem in computer vision by exploring the crossmodal transfer from 2D images to 3D pointclouds. This approach is both novel and significant due to the scarcity of large annotated pointcloud datasets and the computational expense involved in training 3D models from scratch. 2. Comprehensive Empirical Evaluation: The authors conduct extensive experiments to validate their hypotheses covering various architectures and datasets. The evaluation rigorously compares the performance of FIP models to stateoftheart pointcloud models and against different training paradigms (from scratch imagepretrained fully finetuned). 3. Significant Findings: The paper documents substantial gains in performance data efficiency and training speed from using imagepretrained models in 3D pointcloud understanding. These results could have a wideranging impact on how future 3D models are developed especially in resourceconstrained scenarios.  Weak Points: 1. Lack of Theoretical Analysis: While the empirical results are compelling the paper lacks a theoretical foundation explaining why 2Dto3D model transfer is so effective. A deeper investigation into the underlying mechanics of convolutional filters and their interpretability across modalities would strengthen the paper. 2. Generalizability Concerns: The study primarily focuses on ConvNet architectures and does not explore the transferability of newer architectures such as Graph Neural Networks (GNN) that are also popular in pointcloud processing. This could raise questions about the methods applicability across different 3D model architectures. 3. Potential for Bias in Dataset Selection: The paper bases its findings on a few selected datasets. While these datasets are reputable expanding the experimental validation to include more varied datasets could help establish the robustness and generalizability of the proposed method.  Questions and Feedback to the Author: 1. Rationale Behind Selective FineTuning: What is the theoretical or empirical reasoning behind only finetuning the input output and batch normalization layers Could finetuning additional layers potentially improve performance 2. Impact on Other 3D Architectures: Have the authors considered extending their method to more recent or alternative 3D model architectures such as GNNs How would the transferability principle apply in these cases 3. Addressing Overfitting Concerns: Given the competitive performance on benchmark tasks did the authors encounter overfitting during experiments especially in the fewshot scenarios If so how was it mitigated  Review Summary: The paper presents a compelling case for the transferability of 2D image models to 3D pointcloud understanding through its Finetuned Imagepretrained Models (FIP) approach. By showcasing improvements in performance data efficiency and training speed across several benchmarks the work sets a promising direction for future research in 3D computer vision. However a deeper theoretical examination of why such transferability exists and an exploration of the methods effectiveness across a broader array of architectures and datasets would be beneficial. Additionally addressing potential concerns about the selectivity in finetuning and its impact on model generalizability would solidify the contributions further. Overall the paper makes an important contribution to the field opening new avenues for leveraging 2D image models in 3D pointcloud tasks with reduced computational and data requirements.", "keeCvPPd3vL": "Summarize the paper: The paper addresses a fundamental aspect of Generative Adversarial Networks (GANs) in the field of image synthesis specifically focusing on the generator component which is often treated as a blackbox. Building on the premise that existing generators implicitly rely on sparsityinspired models the authors propose a novel interpretation that generators can be viewed through the lens of Convolutional Sparse Coding (CSC) and its MultiLayered version (MLCSC). By explicitly enforcing sparsityinspired regularization on selected activation layers within the generator the paper demonstrates improved performance in image synthesis tasks. This method is validated across a range of GAN architectures and also shown to enhance performance in solving inverse problems using the Deep Image Prior (DIP) method. The authors conduct extensive experiments and present their findings which support the argument that such regularization leads to better emerging properties and performance in both synthetic image generation and in the application of GANs to inverse problem solving. Strong and Weak Points Questions and Feedback to the author: Strong Points: 1. Novel Approach: The paper introduces an innovative perspective on understanding and enhancing GANs particularly focusing on the generator\u2019s architecture and its regularization. The idea of interpreting and integrating CSC and MLCSC models into GANs is creative and promising. 2. Comprehensive Validation: The exhaustive experimentation across various GAN architectures and application scenarios (image synthesis and solving inverse problems) provides a robust validation of the proposed approach. The use of both Fre\u0301chet Inception Distance (FID) and Inception Score (IS) as metrics further solidifies the empirical evaluation. 3. Practical Significance: By addressing the blackbox nature of generators within GANs the paper potentially opens up new avenues for research in deep learning and image processing making the synthesis process more interpretable and efficient. Weak Points: 1. Lack of Theoretical Underpinning: While the experimental results are compelling the paper could benefit from a deeper theoretical analysis of why sparsityinspired models lead to improved performance in GANs. A closer examination of the mathematical properties and implications of CSC and MLCSC in the context of deep generative models might enrich the discussion. 2. Generalization and Scalability Concerns: The paper focuses primarily on image synthesis and specific inverse problems. It remains unclear how easily the proposed regularization techniques can be adapted or scaled to other domains of generative modeling beyond images such as text or audio synthesis. 3. Comparison with StateoftheArt: Although the paper mentions improvements over nonregularized models a direct comparison with other contemporary methods that aim to improve the interpretability and performance of GANs could provide a clearer picture of the proposed method\u2019s standing. Questions and Feedback:  How does the proposed sparsityinspired regularization affect the training time and computational resources needed for GANs  Can the authors elaborate on potential limitations or drawbacks of their approach when applied to more complex or highdimensional synthesis tasks  Would it be possible to extend the proposed methodology to other types of deep generative models such as Variational Autoencoders (VAEs) Review Summary: The paper presents a groundbreaking approach to understanding and enhancing the performance of GANs by interpreting the generator component through the framework of CSC and MLCSC models followed by the application of sparsityinspired regularization. This novel interpretation and modification strategy not only shed light on the inner workings of GAN generators but also demonstrate improved performance in image synthesis and in solving inverse problems through empirical validation across multiple GAN architectures. While the approach is innovative and the results are promising the paper could benefit from further theoretical explanations and broader considerations regarding the scalability and adaptability of the method. Despite these areas for enhancement the proposed method stands as a significant contribution to the field of deep learning and image synthesis offering a more interpretable and potentially more efficient path forward for GAN development and research. Given the compelling evidence provided through extensive experimentation this work is recommended for publication with minor revisions primarily aimed at addressing theoretical depth and broader applicability discussions.", "hRVZd5g-z7": " Review of \"Joint Subspace View of Convolutional Filters across Network Layers\"  Summary: The paper presents a novel approach to modeling convolutional neural networks (CNNs) by exploiting the crosslayer filter redundancies via a joint subspace view. The authors suggest decomposing convolutional filters into layerspecific filter atoms and shared atom coefficients across layers revealing a lowrank structure that indicates significant parameter redundancy within CNNs. This discovery leads to the proposal of a new CNN architecture (ACDC) where the majority of parameters are shared across layers drastically reducing the models parameter count while preserving and in some instances enhancing its performance on various tasks including image classification and fewshot learning. The method compatible with numerous network architectures presents a structural regularization approach that improves model interpretability training adaptation and compaction. Extensive empirical evidence demonstrates the effectiveness of this method across different architectures and datasets providing a significant step towards more compact and interpretable CNN models.  Strong Points: 1. Innovative Approach: The paper introduces a pioneering method to exploit the intrinsic redundancies within CNNs leading to significant parameter efficiency without compromising the models expressiveness. This could potentially revolutionize the way we think about network design. 2. Extensive Validation: The authors provide comprehensive empirical evidence across various datasets and CNN architectures. The method demonstrates not only a reduction in parameter size but also competes favorably in performance with stateoftheart models. 3. Versatility and Compatibility: The approach is compatible with a wide range of CNN architectures making it a versatile tool for network design and optimization. 4. Interpretability: The joint subspace modeling offers a pathway towards better model interpretability as demonstrated by the extended class activation mapping which could contribute significantly to understanding CNN decisionmaking processes.  Weak Points: 1. Complexity in Implementation: Despite the benefits the method introduces complexity in terms of implementation particularly in decomposing and aligning the atomcoefficient filters across layers. Practical consideration for ease of adoption was not thoroughly discussed. 2. Theoretical Underpinnings: While extensive empirical results are provided the paper lacks a deeper theoretical discussion on why the lowrank structure exists across layers and its implications. A theoretical framework could provide insights into the limits and potential of the approach. 3. Potential Impact on Training Time: The effect of the proposed method on training dynamics and time efficiency is not clear. Given the novel architecture understanding its impact on model training times and convergence behavior would add value. 4. Scalability and Generalization: The paper shows impressive results on several benchmarks. However discussions on scalability to larger models and datasets or generalization to tasks beyond image classification and fewshot learning are limited.  Questions and Feedback to the Author: 1. Can the authors elaborate on the practical considerations for implementing the atomcoefficient decomposition particularly in optimizing the decomposition process to minimize computational overhead 2. Are there theoretical insights or hypotheses on why the shared atom coefficients across layers lead to significant parameter reduction without loss of expressiveness Further discussion on this could be enlightening. 3. How does the proposed ACDC architecture affect the training time and convergence behavior in comparison to traditional CNNs Insights into training dynamics would be helpful. 4. The methods impact seems highly positive on image classification and fewshot learning tasks. How well does it generalize to other domains such as object detection or segmentation  Review Summary: The paper introduces a remarkable new concept in CNN design by leveraging crosslayer filter redundancy offering a promising avenue towards more efficient and interpretable networks. The proposed joint subspace view significantly reduces parameter count while maintaining or even improving performance across tasks and architectures. Despite these strengths clarifications on the implementation complexity theoretical foundations training dynamics and generalization capabilities would enhance the papers contribution. Considering the extensive empirical evidence and the novel approachs potential impact on network design I recommend this paper for acceptance pending minor revisions that address the raised concerns on theoretical underpinnings and broader applicability discussions. The paper stands as a substantial contribution to the field pushing forward our understanding and capabilities in designing efficient and interpretable CNN architectures.", "iEvAf8i6JjO": " 1. Summary of the Paper The paper introduces a novel approach Trust Region Gradient Projection (TRGP) aiming to mitigate catastrophic forgetting in continual learning (CL) by facilitating forward knowledge transfer. This is achieved through an innovative use of a trust region a layerwise selection of correlated old tasks based on gradient projection norms and a scaled weight projection which allows for reusing old tasks weights without alteration. TRGP operates by optimizing scaling matrices and model updates in directions orthogonal to the subspaces of old tasks effectively protecting previous knowledge while promoting knowledge transfer. The proposed method was exhaustively evaluated on standard CL benchmarks and demonstrated substantial improvements over stateoftheart methods in terms of mitigating forgetting and enhancing new task performance.  2. Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The concept of a trust region and scaled weight projection provides a creative and logically sound method for selecting and leveraging correlated tasks addressing the issue of catastrophic forgetting effectively.  Comprehensive Evaluation: The extensive experimentation and comparison with stateoftheart methods showcase TRGPs superior performance across diverse benchmarks underscoring its effectiveness and adaptability.  Theoretical Justification: The paper provides a solid theoretical foundation for its techniques including clear definitions and mathematical formulations which helps in understanding the underlying principles of TRGP. Weak Points:  Complexity and Scalability: The additional layers of computation including the determination of the trust region and scaled weight projection for each new task could potentially increase the computational complexity. The scalability of TRGP especially in scenarios involving a very large number of tasks remains unclear.  Generalization to Unrelated Tasks: While TRGP shows impressive performance on correlated tasks its effectiveness when applied to tasks with minimal or no correlation is not thoroughly discussed. Understanding how TRGP handles vastly diverse tasks is critical for realworld applications.  Limited Discussion on Parameter Selection: The paper briefly mentions the sensitivity of TRGPs performance to the threshold l but does not provide detailed insight or guidelines on selecting such parameters for optimal performance. Questions and Feedback:  Can the authors elaborate on the computational overhead introduced by TRGP compared to conventional methods and discuss potential strategies to mitigate these costs  How does TRGP perform in scenarios where new tasks are highly unrelated to previous ones Including such results could provide a more complete picture of TRGPs applicability.  Providing more details or insights on choosing the threshold l and other hyperparameters could be immensely helpful for practitioners looking to apply TRGP in different contexts.  3. Review Summary and Recommendation The paper presents a highly innovative approach TRGP to address the challenge of catastrophic forgetting in continual learning. By intelligently leveraging task correlations and reusing the knowledge of old tasks TRGP not only minimizes forgetting but also improves the learning of new tasks as evidenced by its strong performance across multiple benchmarks. The introduction of trust regions for old task selection and scaled weight projections for knowledge transfer is theoretically sound and shows significant promise. However the potential increase in computational complexity the effectiveness of the method on unrelated tasks and the lack of detailed guidance on parameter selection are areas that could benefit from further exploration and clarification. Addressing these points would strengthen the papers contributions and its utility in practical applications. Given the methods innovative contributions and demonstrated performance advantages I recommend acceptance. Nonetheless addressing the highlighted weak points and questions could further enhance the papers quality and its impact on the field of continual learning.", "k6F-4Bw7LpV": " Review  1) Summary of the Paper The paper proposes a novel perspective on classifier generalization in machine learning asserting that conventional methods focusing solely on test error are inadequate for capturing the complete behavior of classifiers. The authors suggest a more comprehensive approach termed \"Distributional Generalization\" that looks at the entire joint distribution of classifier inputs and outputs. Experimentally they demonstrate that interpolating classifiers\u2014those achieving zero training error\u2014can reproduce training set label noise on unseen data conditioned on specific input subgroups. Through a series of experiments with different classifiers (neural networks kernel machines and decision trees) and datasets they provide empirical support for their \"Feature Calibration\" conjecture which posits that an interpolating classifiers output distribution can match the true label distribution for certain subgroups. They contrast their findings with classical generalization theories propose extending their results beyond interpolating methods and discuss the implications of their work for understanding overparameterization implicit bias and model scaling.  2) Strong and Weak Points Questions and Feedback Strong Points:  Novel Approach: The paper introduces a groundbreaking perspective on understanding classifiers through Distributional Generalization which could significantly enhance how models are evaluated beyond simple error metrics.  Empirical Validation: The authors substantiate their claims with rigorous experimental evidence across various settings and classifier types providing a solid foundation for their conjectures.  Theoretical and Practical Implications: Their work has profound implications for deep learning theories regarding implicit bias and overparameterization and it opens new avenues for research on model generalization. Weak Points:  Limited Scope in Classifier Types: While the paper examines a range of classifiers the focus seems heavily tilted towards neural networks. The inclusion of more traditional classifiers or hybrid models could enrich the analysis.  Lack of Formal Theoretical Foundation: The conjectures while compelling are primarily supported by empirical evidence. A more rigorous mathematical formalism or proof underpinning the Feature Calibration Conjecture would enhance the papers credibility.  Potential Overfitting Concerns: The emphasis on interpolating classifiers achieving zero training error might evoke concerns regarding overfitting especially in scenarios not covered by the experiments. Discussions on mitigating overfitting while maintaining distributional closeness would be beneficial. Questions and Feedback:  Could the authors elaborate on how their approach might adapt to continuous output spaces as in regression tasks  Is there potential for the Feature Calibration Conjecture to be generalized across tasks beyond classification such as generative models  Feedback: It would be insightful to explore the tradeoffs between achieving distributional generalization and the risk of overfitting especially in the context of highly complex or noisy data. Further the impact of regularization techniques on distributional closeness warrants discussion.  3) Review Summary The paper introduces the concept of Distributional Generalization a nuanced approach to understand and evaluate classifiers beyond traditional error metrics. By empirically demonstrating that interpolating classifiers can replicate training set label statistics on test sets within specific subgroups the authors provide a fresh lens to gauge model generalization. The proposed Feature Calibration Conjecture supported by experiments across various machine learning models underlines the importance of considering the whole inputoutput distribution in model assessment. Despite the innovative approach and substantial empirical backing the paper would benefit from a more robust theoretical underpinning for its conjectures and an expanded scope to include a broader range of classifier types and tasks. Additionally addressing potential overfitting risks and the impact of regularization on achieving distributional closeness could enhance the paper\u2019s practical relevance. Overall the paper represents a significant contribution to the field challenging existing paradigms of model generalization and opening new avenues for research on evaluating and understanding machine learning models. Further theoretical development and exploration of the proposed concepts will be crucial for validating and extending the paper\u2019s findings.", "vwLLQ-HwqhZ": " Review of \"Investigating Batch Normalization in Online Continual Learning and Proposing Continual Normalization\"  Summary This paper addresses a critical issue in online continual learning (OCL) environments: the negative impact of Batch Normalization (BN) on the performance of learned tasks over time due to the noni.i.d and nonstationary nature of continual learning data. The authors investigate the crosstask normalization effect where BN normalizes data biased towards the current task leading to increased catastrophic forgetting of older tasks. To counter this the authors propose a novel normalization technique Continual Normalization (CN) designed to maintain the benefits of BN while mitigating its drawbacks in OCL settings. CN operates by initially applying group normalization to the feature map followed by batch normalization aiming to balance across minibatch and withinsample normalization. The paper provides extensive empirical validation of CN across various continual learning algorithms and scenarios demonstrating its effectiveness as a direct substitute for BN showing notable performance improvements with minimal computational overhead.  Strong Points 1. Novel Approach: The paper presents a creative solution to a welldocumented problem in continual learning. By introducing CN it addresses the limitations of BN in an OCL context providing a theoretically sound and practically effective methodology. 2. Extensive Experiments: The comprehensive experimentation across different algorithms and OCL scenarios strongly supports the claims made about CN. This broad empirical coverage not only establishes CN\u2019s versatility but also its potential applicability in realworld OCL applications. 3. Opensource Implementation: Providing the implementation code is a significant advantage facilitating further research and adoption in the community. This transparency in methodology enables reproducibility and critical evaluation.  Weak Points 1. Potential Computational Overhead Concerns: While the paper mentions minimal computational overhead with CN it lacks a detailed comparative analysis of the computational resources and time required across all normalization techniques examined. Such an analysis would be valuable for practical applications especially in resourceconstrained environments. 2. Discussion on Hyperparameter Sensitivity: The paper introduces an additional hyperparameter (number of groups in CN) without extensively discussing its sensitivity and the process for optimal selection in different scenarios. Given the impact hyperparameters can have on model performance insights into their tuning would enhance the paper\u2019s practical utility. 3. Limited Exploration of Theoretical Foundations: Although the paper provides empirical evidence supporting CNs effectiveness it could benefit from a deeper theoretical discussion explaining why CN outperforms other normalization techniques in OCL settings thereby strengthening the foundational understanding of the proposed method.  Questions and Feedback to the Author 1. Can the authors provide more insights into the computational overhead comparison between BN CN and other normalization methods in more detail particularly in terms of training time and resource utilization 2. How sensitive is CNs performance to the choice of the number of groups Are there recommendations or guidelines for selecting this hyperparameter across different types of OCL tasks 3. Could the authors expand on the theoretical underpinnings that make CN particularly effective in combating the crosstask normalization effect observed with BN in OCL environments  Review Summary The paper presents Continual Normalization (CN) as an innovative and effective solution to the problem of catastrophic forgetting in online continual learning caused by the limitations of Batch Normalization. The proposal is supported by extensive empirical validation and a clear articulation of the method\u2019s benefits over existing approaches. However the work would greatly benefit from a more indepth discussion on the computational efficiency of CN sensitivity analysis regarding the newly introduced hyperparameter (number of groups) and a stronger theoretical foundation explaining CNs effectiveness. Despite these areas for improvement the paper makes a significant contribution to the field of continual learning offering a promising avenue for future research and application.  Recommendation I recommend accepting this paper with minor revisions addressing the identified weak points and questions to enhance the clarity depth and utility of the presented work. The novel approach comprehensive empirical validation and practical implications of this research signify a substantial advancement in the continual learning domain.", "xOeWOPFXrTh": " Review of \"Optimizing for HigherOrder Dynamics in VideoBased Cardiac Measurement\"  1) Summary This paper addresses a significant gap in the field of computer vision specifically in the context of videobased physiological monitoring. It argues that existing methods which predominantly focus on firstorder dynamics like optical flow fall short when it comes to capturing more subtle yet clinically relevant higherorder dynamics such as acceleration. This is especially pertinent in cardiac monitoring where parameters derived from the second derivative of the cardiac pulse signal can provide insights into blood pressure and arterial health. The authors propose a deep learning model that explicitly incorporates secondderivative information both in the inputs (video frames) and in the optimization target (cardiac signal) showing that such a design considerably enhances the models ability to estimate clinically meaningful parameters like left ventricle ejection time (LVET). The model is trained on a novel dataset generated from highlyparameterized synthetic avatars and evaluated against clinicalgrade contact sensor measurements demonstrating improved accuracy in capturing waveform morphology necessary for clinical applications.  2) Strong and Weak Points Questions and Feedback Strong Points:  Originality and Clinical Relevance: The paper addresses a neglected yet vitally important aspect of videobased physiological measurement  the accurate estimation of waveform morphology through higherorder dynamics. Its focus on LVET estimation for potential applications in diagnosing and monitoring cardiac health is both novel and clinically relevant.  Methodological Rigor: Incorporating secondderivative information into both the model inputs and optimization targets is an innovative approach. The systematic evaluation including ablation studies to pinpoint the contribution of secondderivative inputs and targets demonstrates methodological rigor.  Use of Synthetic Data for Training: Generating training data from synthetic avatars to capture a wide range of physiological and motion dynamics is a smart solution to the problem of acquiring labeled realworld datasets. This could pave the way for more diverse and inclusive training sets in medical image analysis. Weak Points:  Generalization Concerns: While the use of synthetic data is innovative there are concerns about the model\u2019s ability to generalize to realworld scenarios. The paper could benefit from a more detailed examination of how well models trained on synthetic data perform on real human subjects across different races ages and under varied lighting conditions.  Comparison with StateoftheArt: The paper lacks a direct comparison with existing methods particularly those that may not explicitly optimize for secondorder dynamics but have been applied to similar clinical parameters estimation.  Details on Model Architecture: The description of the deep learning architecture could be more detailed especially concerning the specific innovations that facilitate the integration of secondderivative information. Questions and Feedback:  Could the authors comment on the potential for the model to generalize to realworld applications including variations in subject demographics and environmental conditions  It would be informative to see a direct comparison with current stateoftheart methods in both videobased physiological measurement and specifically in LVET estimation. How does the proposed model compare in terms of accuracy and computational efficiency  Providing more detailed architecture diagrams and a more thorough explanation of the neural network design choices could enhance readers\u2019 understanding of the models functionality and potential for adaptation to other applications.  3) Review Summary The paper introduces a highly innovative approach to videobased cardiac measurement emphasizing the critical role of higherorder dynamics in estimating clinically relevant parameters such as LVET. This work opens up new avenues in noninvasive cardiac health monitoring and has the potential to significantly impact telehealth and remote patient monitoring. The utilization of synthetic data for training addresses a common challenge in medical imaging  the acquisition of diverse and representative training datasets. However for the paper to fully establish the model\u2019s effectiveness and applicability a more detailed treatment of its generalization capabilities and comparisons with existing methods is necessary. Despite these areas for enhancement the strengths of the paper\u2014namely its originality methodological rigor and potential clinical impact\u2014warrant a strong recommendation for publication. Further development and validation work particularly involving realworld data and broader comparisons could substantially solidify its contributions to the field of medical image analysis and computational health.", "pVU7Gp7Nq4k": " Peer Review on \"Representation Mitosis in Deep Neural Networks\"  Summary of the Paper The paper presents a groundbreaking study on the phenomenon termed \"representation mitosis\" observed in the last hidden layers of wide deep neural networks (DNNs) particularly convolutional neural networks (CNNs) and its implications for understanding \"benign overfitting.\" Utilizing a variety of neural network architectures (e.g. DenseNet40 WideResNet28 ResNet50) and datasets (e.g. CIFAR10100 ImageNet pMNIST) the study systematically investigates how beyond a critical width of the network neurons in the last hidden layer begin to form groups or \"clones\" that represent identical features of the input data differentiated only by statistically independent noise. This process analogous to cellular mitosis is attributed to training the network to zero training error and results in improved generalization performance as the network width increases defying the traditional biasvariance tradeoff.  Strong Points 1. Novel Concept: The paper introduces an innovative concept of \"representation mitosis\" expanding our understanding of overparameterization and its role in the generalization capabilities of DNNs. 2. Comprehensive Empirical Validation: The authors provide extensive empirical evidence across various network architectures and datasets to support the hypothesis of representation mitosis making their findings robust and wellsubstantiated. 3. Clarity in Presentation: The explanation of representation mitosis its mechanisms and implications are presented with commendable clarity and are supported by illustrative figures that aid in comprehension. 4. Potential for Broad Impact: The insights regarding overfitting and the role of network width in generalization could influence future studies in deep learning potentially guiding more effective network design and training methodologies.  Weak Points 1. Lack of Theoretical Framework: While the paper excels in empirical analysis it falls short in providing a rigorous theoretical framework to back the hypotheses and findings related to representation mitosis. 2. Generalizability Concerns: Although representation mitosis is demonstrated across several networks and datasets the phenomenons applicability to other neural network types such as recurrent neural networks (RNNs) or transformers remains unexplored. 3. Overlooking Alternative Explanations: The paper could benefit from addressing or ruling out alternative mechanisms that may lead to a similar improvement in generalization performance with increased network width. 4. Reproducibility and Accessibility: While the authors provide a link to the code repository a more detailed explanation of experimental setups and parameters in the main text might enhance reproducibility for a broader audience.  Questions and Feedback to the Author 1. Theoretical Underpinnings: Could the authors elaborate on any existing theoretical models or work to support the concept of representation mitosis or provide insights into potential avenues for developing such theoretical support 2. Impact of Regularization Techniques: How might different regularization techniques affect the onset and properties of representation mitosis considering that they are integral to reaching zero training error 3. Application to Other Architectures: Have the authors considered investigating representation mitosis in other architectural frameworks such as transformers to assess the generality of their findings 4. Robustness to Adversarial Attacks: Does the presence of clones in the last hidden layer affect the networks robustness to adversarial attacks given that these clones represent redundant information  Review Summary and Recommendation The paper presents a compelling empirical study on representation mitosis in deep neural networks offering a novel perspective on the phenomenon of benign overfitting and challenging traditional notions of the biasvariance tradeoff. The studys strengths lie in its innovative concept extensive empirical validation and potential impact on future research directions. However the paper would benefit from addressing theoretical considerations exploring broader applicability across different network types and providing a more detailed discussion on the experimental setups for enhanced reproducibility. Given the novel insights provided and the potential impact on the field I recommend the paper for publication contingent upon addressing the mentioned weak points particularly the development or discussion of a theoretical framework to support the empirical findings. The resolution of these issues would significantly strengthen the paper and contribute to its relevance and utility in the broader deep learning research community.", "gEZrGCozdqR": " 1) Summary of the Paper The paper presents a novel method to enhance the zeroshot learning capabilities of large language models by leveraging a process termed \"instruction tuning.\" This method involves finetuning a preexisting 137B parameter language model (LaMDAPT) on a diverse set of more than 60 NLP tasks each described via natural language instructions. The resulting model named FLAN (Finetuned Language Net) demonstrates significant improvement in zeroshot performance across a variety of NLP tasks when compared to its unmodified counterpart and even outperforms the 175B parameter GPT3 model in many instances. The paper also explores the effects of the number of finetuning datasets model scale and the utilization of natural language instructions on the success of instruction tuning through ablation studies.  2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The papers method of instruction tuning is novel and addresses a significant challenge in NLP by improving zeroshot learning which could make large language models more versatile and accessible.  Comprehensive Evaluation: The authors rigorously test FLAN across multiple datasets and task types providing a thorough comparison to not only its unmodified version but also to other stateoftheart models like GPT3 and GLaM showcasing the methods effectiveness.  Use of Natural Language Instructions: Leveraging natural language instructions as a way to guide the models learning process is both intuitive and scalable enabling the model to better generalize across unseen tasks. Weak Points:  Potential Overfitting on Instruction Format: While instruction tuning improves zeroshot performance there might be a concern about the models potential overfitting to the format of the instructions rather than truly understanding the tasks.  Resource Intensity: The resourceintensive nature of training a 137B parameter model on over 60 datasets might limit the reproducibility and accessibility of the method for a wider range of researchers and practitioners.  Limited Improvement on Certain Tasks: The paper acknowledges that instruction tuning does not significantly improve performance for tasks that are closely aligned with the models original language modeling training suggesting a limitation of the methods effectiveness. Questions and Feedback:  How well does FLAN generalize to languages other than English given the predominant use of English in both pretraining and instruction tuning  Would the authors consider exploring the effectiveness of instruction tuning with smaller models which could be more accessible and costeffective for the wider research community  It would be beneficial for the paper to discuss potential strategies for mitigating any observed overfitting to the instructional format ensuring the model truly grasps the underlying tasks.  3) Review Summary The paper introduces a compelling method for improving zeroshot learning in large language models through instruction tuning backed by extensive evaluations that demonstrate FLANs superiority over its predecessors in various tasks. The approach is innovative leveraging the vast capabilities of large models while mitigating their traditional limitations in zeroshot settings. However concerns regarding the methods scalability cost and potential limitations in certain task types are noted. Furthermore questions about the models generalization capabilities outside of Englishcentric tasks and its performance with smaller models are raised. Despite these areas for future inquiry and improvement the paper significantly contributes to the field of NLP by showcasing the potential of instruction tuning to enhance zeroshot learning paving the way for more versatile and effective language models. Thus the paper is a valuable addition to the corpus of research in NLP opening up several avenues for future research and application.", "qDx6DXD3Fzt": " Review of \"Provable OutofDistribution Detection for Reliable Classification with Deep Neural Networks\"  1. Summarize the paper This paper addresses a critical limitation of deep neural networks (DNNs) \u2014 the issue of overconfident predictions on outofdistribution (OOD) data especially when such data are manipulated adversarially. Existing methods for OOD detection are shown to be susceptible to adversarial manipulation leading to a misclassification with high confidence by the model. The authors introduce ProoD a novel architecture that integrates a certifiable OOD detector with a standard classifier to achieve robust OOD detection without sacrificing prediction accuracy or detection performance on unmanipulated OOD samples. ProoD ensures adversarially robust OOD detection through confidence upper bounds around OOD samples and avoids the asymptotic overconfidence problem intrinsic to standard neural networks. The approach achieves competitive performance with stateoftheart methods without incurring a loss in model accuracy or OOD detection capability.  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Innovative Solution: The paper presents a novel and principled solution to integrate a certifiable OOD detector with a classifier addressing a significant gap in current ML safety mechanisms.  Robustness and Accuracy: ProoD not only addresses the overconfidence issue of DNNs on OOD data but does so without compromising the prediction accuracy \u2014 a notable achievement that balances safety with performance.  Comprehensive Evaluation: The experimental setup including comparisons to existing methods (OE ACET ATOM etc.) provides a thorough validation of ProoD\u2019s utility and robustness across different datasets and scenarios. Weak Points:  Complexity and Scalability: While ProoD demonstrates impressive performance the paper might benefit from a more detailed discussion on the computational complexity and scalability of the model especially in realworld applications involving largescale data.  Generalization across domains: The evaluation focuses on image datasets (CIFAR R.ImgNet). Insights into how ProoD performs across different data modalities or more complex OOD scenarios could enrich the analysis.  Hyperparameter Sensitivity: The models performance in relation to hyperparameter settings (e.g. the binary shift in the binary discriminator) could be better addressed providing insights into model robustness against parameter selection. Questions:  Can the authors comment on the potential challenges or limitations of applying ProoD in realtime safetycritical systems where computational efficiency is crucial  How does ProoD handle highly complex OOD samples that are closer to the decision boundary of the indistribution and outdistribution Are there scenarios where ProoD might fail and how can such cases be mitigated Feedback:  It would be valuable to include a deeper analysis of the computational requirements of ProoD and its scalability in practical applications. Specifically providing benchmarks on computational resources and latency could offer clearer insights for adoption.  Exploring the application of ProoD beyond image classification tasks to other domains such as text or audio could demonstrate the versatility and broader impact of the proposed solution.  A more detailed discussion on the impact of hyperparameter settings on the overall model performance and robustness could enhance the papers contribution offering guidelines for practitioners in tuning the model efficiently.  3. Review Summary The paper introduces ProoD a compelling approach to ensuring reliable and robust classification by DNNs in the presence of OOD data addressing a crucial challenge in deploying ML models in safetycritical systems. The methodology combines the benefits of certified OOD detection and standard classification performance without the tradeoffs that typically accompany robustness enhancements. The experimental results demonstrate ProoDs effectiveness and superiority in both adversarially robust OOD detection and maintenance of high classification accuracy. However the paper could be strengthened by addressing the scalability and computational efficiency of ProoD exploring its applicability across different domains and offering a more nuanced discussion on the sensitivity of the model to hyperparameter adjustments. Addressing these areas can significantly bolster the papers contribution providing a more comprehensive understanding of ProoDs practical implications and limitations. Nonetheless the paper presents a significant step forward in creating safer and more reliable ML systems with the potential for wideranging impact across various applications where safety and reliability are paramount. Further exploration and refinement of ProoD could pave the way for its adoption in critical areas such as healthcare autonomous driving and other domains where the cost of failure is high.", "nKZvpGRdJlG": " Peer Review  Summarize the paper This paper introduces ROCO (Robust Combinatorial Optimization) an innovative adversarial framework aimed at evaluating and enhancing the robustness of combinatorial optimization (CO) solvers. The authors address the vulnerability of CO solvers to adversarial attacks by proposing mechanisms that modify the underlying graph structure of CO problems. They develop attack models that weaken solver performance by altering graph edges within a constraint budget and propose defensive strategies to counteract these attacks and improve solver robustness. The methodology encompasses both reinforcement learning and heuristic approaches for attack and defense applied to three realworld CO tasks: Directed Acyclic Graph Scheduling Asymmetric Traveling Salesman Problem and Fraud Coverage. Notably the framework is applicable to both traditional and learningbased solvers without requiring knowledge of the solvers internals (blackbox setting). Experimental results demonstrate the effectiveness of the proposed attacks in deteriorating solver performance and the ability of the defensive measures to recover or even improve on the original solver outputs.  Strong points 1. Originality and Contribution: The paper pioneers the study of adversarial attacks on CO solvers and introduces a novel adversarial framework (ROCO) for testing and improving solver robustness. This contribution is significant considering the prevalent neglect of vulnerability assessments in CO solvers. 2. Applicability across Solver Types: A notable strength of the proposed methodology is its applicability to a wide range of solvers whether learningbased or traditional without needing to access the solvers internal mechanisms. This versatility is important for broad applicability in realworld scenarios. 3. Comprehensiveness of Experiments: The experiments cover three different CO tasks employ various attack and defense models and compare results across multiple solvers including commercial ones like Gurobi. Such comprehensive testing validates the frameworks effectiveness and generality.  Weak points 1. Lack of Theoretical Analysis: The paper primarily focuses on empirical results without providing a rigorous theoretical foundation or analysis to explain why certain attack or defense strategies work. A deeper theoretical insight could strengthen the understanding of the observed phenomena. 2. Limited Scope in Attack and Defense Strategies: The paper concentrates on attacks and defenses that revolve around modifying graph edges. Expanding the scope to include other forms of structural perturbations such as node modifications or changes in nodeedge attributes could uncover additional insights into solver robustness and vulnerability. 3. Potential Ethical Concerns: While the intention behind exploring adversarial attacks is to improve solver robustness theres an underlying ethical consideration regarding the publication of such vulnerabilities without ensuring widespread adoption of defense mechanisms first. The paper could benefit from a discussion on ethical implications and guidelines for responsibly disclosing such vulnerabilities.  Questions and Feedback to the Author 1. Have you considered expanding the attack and defense models to include modifications beyond graph edges such as node deletions or attribute changes If yes what were the challenges or outcomes of those explorations 2. Can the authors provide more insight into the theoretical underpinnings that might explain the efficacy of certain attack and defense strategies observed in the experiments 3. Given the potential misuse of adversarial attacks highlighted in the paper could the authors elaborate on ethical guidelines or considerations for researchers and practitioners exploring this area  Review Summary This paper presents a pioneering exploration into the adversarial robustness of combinatorial optimization solvers a crucial yet underexplored area. By proposing an adversarial framework (ROCO) that enables systematic testing and reinforcement of solver robustness through graph structure manipulations the authors make significant contributions to both the theory and practice of combinatorial optimization. However the strengths of the paper such as its originality broad applicability and extensive experimental validation are somewhat tempered by its limitations including the lack of a strong theoretical foundation a focus on edge modifications to the exclusion of other graph modifications and potential ethical implications of exposing solver vulnerabilities. Questions to the authors revolve around theoretical insights the scope of attack and defense mechanisms and ethical considerations. In conclusion while the paper takes an important step towards understanding and mitigating adversarial vulnerabilities in CO solvers further work is encouraged to broaden the scope of attack and defense mechanisms delve into theoretical explanations and address ethical considerations in adversarial research.", "kamUXjlAZuw": " Review of the Paper on Learning Fairness Tradeoffs: Probabilistic and Asymptotic Frameworks  Summary This paper embarks on an exploration to establish a mathematical framework aimed at enhancing generalization within the realm of fairness in algorithmic decisionmaking. Emphasizing the interplay between statistical loss and fairness mitigation the study delves into two primary methodologies: a PACtype setup yielding probabilistic upper bounds and an asymptotic framework that determines the limiting distribution of tradeoffs between empirical and true fairness considerations. The investigation sheds light on the critical role of class imbalance in the learnability of fairness underscoring the difficulties in obtaining reliable fairness metrics with inadequate categorylevel samples. The methodology interweaves fairness definitions loss functions and the leveraging of statistical tools like Rademacher complexities to pave the way for new insights into bias mitigation while balancing statistical performance with fairness constraints. The empirical analyses supplemented by real data experimentation critically assess the frameworks efficacy especially under conditions of class imbalance and partial debiasing.  Strong Points 1. Comprehensive Theoretical Framework: The paper excels in constructing a detailed theoretical analysis that merges wellestablished statistical tools with fairness considerations. The dual approach involving PAC learning and asymptotic behavior provides a robust foundation for understanding the learnability and generalization of fairness metrics in machine learning models. 2. Empirical Verification: By applying the derived theoretical insights to realworld datasets the paper not only grounds its claims in practical considerations but also showcases the applicability and potential limitations of its proposed framework in real scenarios. This strengthens the papers overall contribution by bridging the gap between theory and practice. 3. Clarity in Presentation: The paper is wellstructured and articulates complex concepts in a manner that is accessible to readers with a decent understanding of the subject matter. The use of defined terms propounded theorems and empirical findings are systematically presented enhancing comprehension and engagement.  Weak Points 1. Limited Scope of Experimental Validation: While the paper provides empirical evidence from real datasets the scope of these experiments appears somewhat limited. A broader array of datasets particularly those exhibiting diverse forms of bias and class imbalances would enrich the papers findings and validate its applicability across different scenarios. 2. Treatment of Class Imbalance: Although the paper recognizes class imbalance as a critical factor affecting the learnability of fairness it might benefit from a more indepth examination of strategies to address this issue. Alternative approaches or methodologies to better handle class imbalance within the papers proposed framework could enhance its practical utility. 3. Discussion on Computational Complexity: The paper lacks a detailed discussion on the computational implications of implementing its proposed methods especially in largescale applications. Insights into the computational costs associated with the PAC and asymptotic frameworks as well as recommendations for efficient implementation would be valuable for practitioners.  Questions and Feedback to the Author 1. Expanding Experimental Validation: Could the authors consider including additional datasets that cover a wider range of class imbalances and bias characteristics to validate their frameworks versatility and robustness 2. Addressing Class Imbalance: What are the authors thoughts on potential methodologies or modifications to the proposed framework that could more effectively deal with the challenges posed by class imbalance 3. Computational Considerations: Could the authors provide insights into the computational demands of their proposed methodologies especially when applied to large datasets and suggest strategies for optimizing computational efficiency  Review Summary The paper makes a significant theoretical contribution to the field of fairness in AI providing a solid mathematical foundation for understanding and improving the generalization of fairness metrics. The adoption of PAC learning and asymptotic analyses offers a novel perspective on navigating the tradeoffs between statistical performance and fairness considerations. While the empirical validations strengthen the papers claims expanding these experiments and providing further discussions on addressing class imbalance and computational demands would greatly enhance its utility and impact. Overall the paper represents a valuable step forward in the pursuit of developing fair and generalizable machine learning models warranting its recommendation for publication with minor revisions.", "fXHl76nO2AZ": " Review of \"Gradient Importance Learning (GIL) for ImputationFree Inference with Incomplete Data\"  Summarization The paper introduces Gradient Importance Learning (GIL) a novel methodology aimed at training deep learning models specifically Multilayer Perceptrons (MLPs) and Long ShortTerm Memories (LSTMs) for inference on datasets with missing entries without necessitating imputation. By employing a reinforcement learning (RL) framework the method dynamically adjusts the gradients during the training phase exploiting the information inherent in the patterns of missingness. This is in stark contrast to conventional methods that rely on generating estimates for missing data which may not only introduce inaccuracies but also compound errors in subsequent prediction tasks. The authors extensively evaluate their approach on three distinct datasets including MIMICIII involving realworld electronic health records demonstrating substantial performance improvements over traditional imputationfollowedbyprediction methodologies.  Strengths and Weaknesses Strengths: 1. Novelty: GIL represents a significant departure from existing imputationprediction frameworks addressing the limitations by directly leveraging missingness patterns through a reinforcement learningenhanced backpropagation method. 2. Comprehensive Evaluation: The methods efficacy is thoroughly validated across a range of datasets with varying types of missingness thereby substantiating its applicability and robustness in realworld scenarios. 3. ImputationFree Inference: By circumventing the need for imputation GIL curtails the risk of propagating imputation errors into the prediction phase which is a critical advancement over existing methodologies. 4. Flexibility and Generalizability: The approach does not rely on additional data assumptions domain expertise or modelingspecific preprocessing which significantly broadens its utility across various applications. Weaknesses: 1. Complexity and Reproducibility: The integration of reinforcement learning for gradient adjustment introduces complexity which may pose challenges in terms of computational resources and replicability by practitioners. 2. Limited Exploration of Comparative Models: While GIL outperforms traditional models an exploration of its performance compared to other stateoftheart deep learning models that do not use RL for handling missing data would provide a more nuanced understanding of its position within the landscape. 3. Potential Overfitting Concerns: The reinforcement learning aspect while innovative might lead to overfitting especially in scenarios with extremely high missingness rates which was not thoroughly discussed or addressed. Questions and Feedback to the Author: 1. Could the authors provide further insights into the computational overhead introduced by the RL component especially in comparison to traditional imputation methods 2. It would be beneficial to understand how GIL performs in the presence of missing data mechanisms beyond MAR and MNAR particularly in adversarially crafted missingness patterns. 3. How does GIL address or mitigate potential overfitting risks particularly in data with very high missingness rates or small sample sizes 4. Expanding on the comparative analysis to include latest DL models that inherently handle missing data without imputation could offer a clearer picture of GILs novelty and efficiency.  Review Summary GIL represents a pioneering approach to handling missing data by directly embedding information about missingness into the model training process through an RLenhanced gradient adjustment mechanism. This imputationfree methodology not only negates the risk of error propagation inherent in twostep imputationprediction frameworks but also demonstrates superior performance across diverse realworld datasets. Despite its novel contributions and evident strengths some areas like potential computational complexity comparative analysis with a broader set of contemporary models and safeguards against overfitting would benefit from further clarification and exploration. Nonetheless GILs generalizability flexibility and robust performance strongly advocate for its utility in tackling the perennial challenge of missing data across numerous applications.", "vxlAHR9AyZ6": " Peer Review of \"\u03b1Weighted Federated Adversarial Training (\u03b1WFAT) for Enhanced Model Robustness\"  1) Summary of the Paper This paper addresses a critical performance deterioration issue observed in Federated Adversarial Training (FAT) where the straightforward combination of Federated Learning and Adversarial Training exacerbates data heterogeneity among local clients and adversely impacts model robustness against adversarial attacks. The authors propose a novel framework named \u03b1Weighted Federated Adversarial Training (\u03b1WFAT) to mitigate this challenge by introducing a reweighting mechanism that relaxes the innermaximization of Adversarial Training into a lower bound rendering it more compatible with Federated Learning. They provide theoretical analyses to support their approach and conduct extensive experiments across various datasets and settings demonstrating that \u03b1WFAT significantly improves model performance in both IID and NonIID scenarios compared to existing FAT approaches.  2) Strong and Weak Points Question and Feedback  Strong Points: 1. Innovative Approach: The proposal of an \u03b1weighted relaxation mechanism to address the inherent data heterogeneity issue in FAT is novel and shows a deep understanding of the problem space. 2. Comprehensive Evaluation: Extensive experiments including ablation studies and comparisons with existing methods across multiple datasets provide strong empirical evidence of the effectiveness of \u03b1WFAT. 3. Theoretical Foundations: The theoretical analysis accompanying the proposed method adds credibility and helps in understanding the potential of \u03b1WFAT to balance robustness and convergence efficiently.  Weak Points: 1. Limited Exploration of \u03b1 and K\\xcc\\x82 Parameter Space: Although the paper conducts ablation studies on both parameters a more detailed investigation into how different ranges of \u03b1 and K\\xcc\\x82 values affect performance in various scenarios could provide deeper insights. 2. Lack of Discussion on Practical Implementation Challenges: The paper could improve by discussing potential implementation challenges scalability issues and the computational cost associated with deploying \u03b1WFAT in realworld federated systems.  Question: How does the choice of \u03b1 and K\\xcc\\x82 values impact the performance of \u03b1WFAT in highly skewed NonIID distributions Is there a methodological approach to select these parameters optimally based on the dataset characteristics  Feedback to the Author: The paper presents a compelling solution to a critical issue in the intersection of Federated Learning and Adversarial Training. To further strengthen the paper consider providing more insights into the practical applicability of \u03b1WFAT such as realworld scenarios where this approach could be particularly beneficial. Additionally expanding the discussion around the parameter selection process for \u03b1 and K\\xcc\\x82 could help readers better understand how to deploy the proposed method effectively in different settings.  3) Review Summary and Recommendation The paper introduces a significant advancement in the domain of Federated Learning and Adversarial Training by mitigating the negative impacts of data heterogeneity on model robustness through the proposed \u03b1WFAT framework. The innovative approach backed by both theoretical insights and comprehensive empirical evidence represents a considerable contribution to the field. However the study would benefit from a more nuanced exploration of the parameter space and discussions on realworld deployability and scalability. Given the strengths far outweigh the weaknesses and the feedback provided aims to further enhance an already solid research work I recommend the acceptance of this paper. It opens avenues for future work in optimizing federated adversarial training methods and has the potential to inform the development of more robust and efficient distributed machine learning systems.", "reFFte7mA0F": "Review of \"Advancing OnDemand Ride Pooling with Conditional Expectation Based Value Decomposition\" 1) Summary: The manuscript presents a novel approach to solving the RidePool Matching Problem (RMP) a complex computational challenge at the core of ondemand ride pooling services like Uber Pool Grab Share etc. The proposed method named Conditional Expectation based Value Decomposition (CEVD) aims to improve the efficiency and performance of matching algorithms by incorporating the impacts of actions of other agents (vehicles) on an individual agents future value. This is accomplished through computing conditional expectations using joint conditional probabilities addressing a significant limitation in the current stateoftheart method Neural Approximate Dynamic Programming (NeurADP) which does not consider interactions among agents. The authors demonstrate using a realworld taxi dataset that CEVD outperforms NeurADP by up to 9.76 in terms of overall requests served signaling a substantial improvement in the domain where even a 1 increase is deemed significant. 2) Strong and Weak Points Questions and Feedback: Strong Points: a) Significant Improvement: The paper clearly demonstrates a sizable improvement over the existing best method. In a domain where marginal gains are considered significant the reported improvements are compelling. b) RealWorld Applicability: Employing a realworld taxi dataset for the evaluation gives concrete evidence of the methods applicability and effectiveness in practical settings. c) Novel Approach: The methods novelty lies in addressing the nonconsideration of other agents actions a critical limitation in existing frameworks. The conditional expectation concept to account for interagent impacts without escalating computational complexity is innovative. Weak Points: a) Experimental Transparency: While the results are promising a more detailed description of the experimental setup (e.g. dataset size computation times across different methods specifics of the implementation) would enhance reproducibility and trust in the results. b) Comparison with Baselines: Beyond NeurADP a comparison with other baseline methods especially those incorporating some aspects of agent interaction could better contextualize CEVDs improvement. c) Broader Applicability: Though the improvement in ondemand ride pooling is well argued a discussion on the broader applicability of CEVD such as in other domains or settings with similar characteristics would enrich the papers impact. Questions and Feedback: 1) Scalability Concerns: Could the authors elaborate on how the method scales with the increasing number of agents and requests especially in larger cities or peak times 2) Impact on Environmental Goals: The introduction briefly mentions environmental benefits. Can the authors expand on how CEVD impacts these broader goals perhaps in terms of reduced emissions or decreased traffic congestion 3) Future Improvements: What are the identified limitations of CEVD and what potential improvements do the authors foresee 3) Review Summary: The manuscript presents a compelling advancement in the field of ondemand ride pooling addressing a significant gap in existing methodologies by factoring in the competitive yet interconnected nature of agents (vehicles). The proposed CEVD method significantly outperforms the leading method NeurADP demonstrating not only the importance of including interactions among agents in calculations but also achieving this without compromising scalability. The realworld data evaluation offers solid evidence supporting CEVDs effectiveness and potential applicability in practical scenarios. However the paper could gain from a more thorough experimental setup description broader comparisons to additional baselines and a discussion on scalability and broader applicability including potential environmental impacts. Addressing these points would enhance the papers completeness and provide clearer guidance for future research directions. Overall considering the substantial improvements demonstrated the detailed theoretical foundation and the practical evaluation the paper represents a significant advance in the domain of ondemand ride pooling. With some improvements especially regarding the experimental details and broader contextualization this work could serve as a cornerstone for future research and applications in the field.", "qY79G8jGsep": " PeerReview on \"DISSECT: Disentangling Simultaneous Explanations via Concept Traversals\" 1) Summary of the Paper The paper introduces DISSECT a novel approach for generating counterfactual explanations in deep learning models through Concept Traversals (CTs). These CTs are series of generated examples that systematically increase the degree of concepts influencing a classifier\u2019s decision. Unlike previous methods DISSECT simultaneously trains a generator a discriminator and a concept disentangler with minimal supervision to effectively disentangle concepts maintain realism preserve relevant information and ensure stability across similar inputs. The utility of DISSECT is demonstrated through various experiments on synthetic and realworld datasets showing its superiority in disentangling and providing meaningful counterfactual explanations compared to baseline methods. The method also shows promise in identifying biases and spurious features influencing a classifiers decisions. 2) Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The introduction of a concept disentangler in the generation process which allows for minimal supervision represents an innovative step forward in explainable AI. It addresses the intricate issue of concept disentanglement effectively.  Comprehensive Evaluation: The paper presents a thorough evaluation of DISSECT across various datasets and compares its performance with existing methods. By covering aspects like importance realism distinctness substitutability and stability the evaluation framework is notably comprehensive.  Realworld Applicability: The application of DISSECT for uncovering biases and spurious artifacts in classifiers is both relevant and significant. This demonstrates the methods practical utility beyond theoretical contributions.  Public Resources: Releasing the code and a new synthetic dermatology dataset (SynthDerm) is commendable promoting transparency and facilitating further research in the community. Weak Points:  Lack of Humancentric Evaluation: While the quantitative metrics are comprehensive the lack of humancentric studies or user feedback on the interpretability and utility of the generated explanations could be a limitation. Understanding user perspectives could further validate the effectiveness of DISSECT.  Computational Complexity: The paper does not discuss the computational requirements of DISSECT especially given the joint training of multiple components. Insights into its scalability and computational efficiency compared to simpler methods would be valuable.  Hyperparameter Sensitivity: There is limited discussion on the sensitivity of DISSECTs performance to its hyperparameters particularly the disentanglers weight (\u03bbr). Given the significant impact of \u03bbr on the distinctness and realism of generated explanations more analysis on hyperparameter tuning would enhance reproducibility and practical applicability.  Broader Impact and Ethical Considerations: While the ethics statement acknowledges limitations a deeper discussion on the potential misuse of DISSECT (e.g. generating misleading explanations) and safeguards would be prudent. Questions and Feedback: 1. Could the authors elaborate on the realworld implications of using DISSECT in sensitive areas like healthcare or criminal justice in light of its ability to surface biases and potentially sensitive information 2. How does DISSECT handle multimodal data or instances where distinct concepts are inherently intertwined 3. It would be beneficial to provide more details on the computational resources required for DISSECT particularly for largescale datasets or complex models. Can DISSECT be efficiently applied in such scenarios 3) Review Summary This paper presents DISSECT a novel methodology for generating counterfactual explanations through Concept Traversals showing significant improvements in disentanglement realism and relevance of generated explanations compared to existing approaches. DISSECTs potential in uncovering biases and enhancing model transparency underscores its value in advancing explainable AI. Despite its strengths incorporating humancentric evaluations discussing computational requirements and providing a deeper analysis on hyperparameter sensitivity would further solidify the papers contributions. Overall DISSECT represents a significant step forward in generating meaningful and interpretable explanations for deep learning models with promising applications across various domains.", "sk63PSiUyci": " Review of \"AISARAH: An Adaptive Insightful Variant of the SARAH Algorithm for Stochastic Optimization\"  Summary The manuscript introduces AISARAH a variant of the Stochastic Recursive Gradient Algorithm (SARAH) designed for unconstrained finitesum optimization problems prevalent in machine learning. This papers key innovation lies in its adaptive stepsize adjustment methodology which leverages local geometry to implicitly compute stepsizes and efficiently estimate the local Lipschitz smoothness of stochastic functions. Unlike traditional methods requiring intensive hyperparameter tuning AISARAH is presented as a fully adaptive tunefree solution that simplifies implementation and enhances computational efficiency. Through extensive numerical experiments comparing SARAH SARAH and other firstorder methods AISARAH is shown to offer competitive if not superior performance across various datasets without the need for finetuned hyperparameters.  Strengths and Weaknesses Strengths: 1. Innovation in StepSize Adaptivity: The paper successfully addresses a significant challenge in stochastic optimization  the efficient and adaptive selection of stepsizes without prior knowledge or extensive hyperparameter tuning. 2. Comprehensive Empirical Validation: The extensive numerical experiments involving comparisons with both classical counterparts and stateoftheart methods across multiple datasets provide strong evidence of AISARAH\u2019s effectiveness. 3. Practicality and Ease of Implementation: The algorithms design is highlighted as straightforward to implement and tunefree making it highly accessible for a wide range of applications. Weaknesses: 1. Lack of Theoretical Depth in Convergence Analysis: While the paper provides intuitive insights and empirical evidence supporting AISARAHs performance it lacks a rigorous theoretical analysis of convergence properties particularly in nonconvex scenarios. 2. Evaluation Metrics and Benchmarking: The paper mainly focuses on convergence speed and the optimality gap. Additional evaluation metrics such as robustness to noisy gradients or analysis of computational complexity would enrich the discussion. 3. Comparison with Contemporary Adaptive Methods: Although comparisons are made with several firstorder methods including ADAM and SGD with Momentum a more indepth discussion on how AISARAH stands against other adaptive methodologies (like AMSGrad) would provide a clearer positioning of AISARAH\u2019s contribution. Questions and Feedback: 1. Can the authors elaborate on the theoretical convergence rate of AISARAH especially in the context of nonconvex optimization problems 2. Given the computational efficiency of the algorithm could the authors comment on its scalability particularly in distributed computing environments 3. The paper asserts that the performance of AISARAH is not sensitive to the choice of \u03b3. Could more details or empirical evidence be provided to substantiate this claim  Review Summary This manuscript presents AISARAH an innovative variant of the SARAH algorithm that introduces adaptive stepsize adjustment based on local geometry. This feature allows the algorithm to dynamically adapt without the intensive tuning of hyperparameters that characterizes many contemporary optimization methods. The practicality and computational efficiency of AISARAH are among its notable strengths supported by extensive empirical evaluation demonstrating its competitive performance. However the paper could benefit from a more thorough theoretical analysis especially regarding convergence properties in nonconvex scenarios. Moreover additional benchmarks against recent adaptive methods and further elaboration on the algorithms robustness and scalability would strengthen the manuscript. Overall AISARAH represents a significant step forward in stochastic optimization particularly in making adaptive stepsize selection more accessible and less reliant on hyperparameter tuning. Its strong empirical performance as demonstrated across various datasets underscores the potential impact of this work in the field of machine learning optimization. However to solidify its contribution a deeper theoretical foundation and broader comparative analysis would be beneficial.", "o86_622j0sb": " Summary of the Paper This paper tackles the challenge of improving the imperceptibility of adversarial examples in blackbox attacks on deep neural networks while maintaining high query efficiency and success rates. The authors propose the use of segmentation priors to restrict perturbations within salient regions of an image thereby enhancing imperceptibility without significantly sacrificing query efficiency and success rates. They introduce the Saliency Attack a novel gradientfree blackbox strategy that further refines perturbations within these salient regions resulting in adversarial examples that are more imperceptible and interpretable. The proposed method also shows enhanced robustness against detectionbased defenses. Through experiments including attacks on ImageNet with an Inception v3 target model the authors demonstrate that their approach outperforms stateoftheart blackbox attacks in terms of imperceptibility as measured by L0 L2 and MAD metrics while maintaining competitive performance in success rate and query efficiency.  Strong Points 1. Originality and Innovation: The introduction of segmentation priors for limiting perturbations to salient regions in blackbox attacks is a novel approach that addresses the crucial aspect of imperceptibility in adversarial examples. The Saliency Attacks method of refining perturbations adds a new dimension to attacks focusing on minimizing perceptibility while being effective. 2. Robustness to DetectionBased Defense: Demonstrating the proposed attacks robustness against detectionbased defenses like Feature Squeezing indicates practical applicability by showing that the adversarial examples generated can evade some existing countermeasures. 3. Extensive Empirical Evaluation: The paper provides a comprehensive experimental setup including comparisons with multiple stateoftheart attacks across different metrics and under various query budgets. This thorough evaluation strengthens the credibility of the proposed methods effectiveness and efficiency.  Weak Points 1. Lack of Theoretical Analysis: While the empirical results are impressive the paper could benefit from a more detailed theoretical analysis or discussion on why limiting perturbations to salient regions leads to more imperceptible yet effective adversarial examples. 2. Generalization across Diverse Models and Datasets: The experiments predominantly focus on ImageNet and an Inception v3 model. Additional experiments on other datasets and models would help in assessing the generalizability of the proposed method. 3. Impact of Segmentation Algorithm Choice: The choice of the segmentation algorithm (Pyramid Feature Attention Network) is not extensively justified and its impact on the final results is not discussed. Insights into how different segmentation methods could affect the attack\u2019s efficiency and success would be valuable.  Questions and Feedback to the Author 1. Theoretical Insights: Could you provide more theoretical insights or intuitive explanations on why focusing attacks on salient regions leads to significantly more imperceptible adversarial examples 2. Impact of Segmentation Quality: How sensitive is the proposed Saliency Attack to the quality of the segmentation Could poor segmentation significantly affect the success rate or imperceptibility 3. Extensions to Other Models and Datasets: Are there plans to test the proposed methods effectiveness across a wider range of models and datasets especially where salient features might be less pronounced or harder to segment  Review Summary The paper presents an innovative and effective approach to generating imperceptible adversarial examples for blackbox attacks targeting the limitations of existing methods that often compromise on imperceptibility. By leveraging segmentation priors the proposed Saliency Attack method refines perturbations within salient image regions achieving notable success in creating adversarial examples that are both effective and difficult to detect. The robust empirical evaluation and the focus on enhancing imperceptibility without sacrificing performance are significant strengths of this work. However the paper would benefit from a deeper theoretical analysis and broader validation across various models and datasets to fully establish the generalizability and theoretical underpinnings of the proposed approach. Overall the paper contributes meaningfully to the field by introducing a novel effective method for improving adversarial attack imperceptibility. With some enhancements particularly in theoretical rationale and broader validation this work could set a new standard for developing imperceptible adversarial examples in blackbox settings.", "zaALYtvbRlH": " Review of SPANDROP: A Data Augmentation Technique for Sparse Supervision in Long Sequences  Summary The paper introduces SPANDROP a novel data augmentation technique designed for machine learning tasks that involve long sequences particularly in the context of natural language processing (NLP). The primary motivation behind SPANDROP is to address challenges in sequence inference tasks where not all parts of the input sequence are equally important for predicting the output. The authors present SPANDROP as a method that enhances model performance by ablating (removing) random parts of the input sequence thereby generating counterfactual examples that maintain the integral supervision signal with minimal data. This approach is meant to help models better identify the core elements that contribute to the desired output even in underspecified scenarios where the precise segments of interest cannot be directly annotated. A variant of SPANDROP based on the betaBernoulli distribution referred to as BetaSPANDROP is also proposed to offer diverse sequence augmentations while ensuring more consistency with the original learning objective. The efficacy of SPANDROP is demonstrated through experiments on both synthetic and realworld NLP tasks showing notable improvements in sample efficiency and generalization capabilities regardless of data abundance.  Strengths and Weak Points Strong Points: 1. Novelty: The introduction of SPANDROP and its variant provides a new perspective on handling longsequence inference problems by focusing on input manipulation rather than model architecture changes. 2. Theoretical Underpinning: The authors provide a comprehensive theoretical analysis of SPANDROPs properties which not only validates the methods efficacy but also helps understand its robustness. 3. Empirical Validation: The wide range of experiments from synthetic tasks like FINDCATS to complex realworld NLP datasets thoroughly demonstrate the benefits of SPANDROP across different settings. 4. General Applicability: SPANDROPs independence from specific model architectures and its simplicity in implementation make it widely applicable across various sequencebased learning tasks. Weak Points: 1. Scalability and Computational Efficiency: The paper lacks a detailed discussion on the computational implications of SPANDROP especially regarding its scalability to very long sequences or extremely large datasets. 2. Comparison with StateoftheArt: While improvements over baseline models are shown there is limited direct comparison with current stateoftheart methods tailored for longsequence processing or data augmentation in NLP. 3. Insights on Hyperparameter Selection: More detailed guidelines or insights on choosing optimal hyperparameters for SPANDROP especially the dropout probability `p` and parameters for BetaSPANDROP could enhance its practical applicability. Questions and Feedback to the Author: 1. Can the authors elaborate on the computational costs associated with SPANDROP especially in comparison to other augmentation or sequence processing techniques 2. How does SPANDROP scale with increasingly longer sequences typical in certain domains such as genomics or detailed document analysis 3. Could the authors provide additional insights or empirical findings on how SPANDROP interacts with different model architectures particularly nontransformerbased models  Review Summary SPANDROP is a compelling approach aiming to refine the learning process for long sequence tasks by focusing on the crucial segments contributing to the desired output. Its theoretical basis and empirical demonstrations across both toy and realworld tasks highlight its potential to significantly improve data efficiency and model generalization without necessitating complex modifications to model architectures. However for wider adoption and understanding further exploration regarding its computational efficiency applicability across various domains and sequence lengths and interactions with diverse model architectures would be beneficial. Additionally a deeper comparative analysis against stateoftheart methods could better position SPANDROP within the existing landscape of sequence inference and data augmentation strategies. Overall this paper contributes valuable insights and a new technique to the field promising improvements in handling long sequences across multiple domains.", "oU3aTsmeRQV": " Peer Review 1) Summary of the Paper This paper introduces a novel method named SelfEnsemble Adversarial Training (SEAT) to enhance the robustness of Deep Neural Networks (DNNs) against adversarial attacks. The authors address the vulnerabilities of DNNs to adversarial examples which are slight imperceptible modifications to input data that can lead to incorrect predictions. SEAT is a selfensemble method that averages the weights of models from different stages of the training process to yield a singular robust classifier. This approach takes advantage of the beneficial properties of models at various points in their training trajectory as opposed to relying on a single model or an ensemble of divergent models. The paper presents a theoretical and empirical analysis highlighting the superiority of SEAT in providing smoother loss landscapes and improved robustness to adversarial attacks compared to existing methods. Experiments on CIFAR10 and CIFAR100 datasets demonstrate the effectiveness of SEAT against several types of adversarial attacks. 2) Strong Points Weak Points Questions and Feedback Strong Points:  Innovative Approach: The concept of selfensembling by averaging weights from the models own training history is novel and creative. It effectively leverages the beneficial aspects of models throughout their training lifecycle.  Theoretical Foundation: The authors provide a solid theoretical basis for why SEAT works discussing the relationship between ensemble predictions and weightensembled models and backing their claims with empirical evidence.  Comprehensive Evaluation: The paper thoroughly evaluates the proposed method against various adversarial attacks on two widelyused datasets offering a detailed comparison with stateoftheart adversarial training techniques. Weak Points:  Lack of Diversity in Experimentation: The experiments mainly focus on CIFAR10 and CIFAR100 datasets. Including more varied datasets or realworld scenarios could strengthen the paper by demonstrating the methods broader applicability.  Computational Efficiency Concerns: While the paper addresses the computational burden of traditional ensemble methods it does not provide detailed insights into the computational efficiency of SEAT especially concerning the overhead of maintaining and updating a history of model weights during training. Questions and Feedback:  Could the authors elaborate on how SEAT would perform in situations with dynamically evolving data distributions such as in realtime learning scenarios  The paper would benefit from a discussion on the potential limitations of SEAT especially regarding scenarios where ensemble methods might not be as effective.  How does the choice of the hyperparameters particularly the exponential decay rates for ensembling \u03b1 affect the overall performance and robustness of the SEAT model 3) Review Summary The paper presents a compelling and innovative approach to enhancing the robustness of deep neural networks against adversarial attacks. The idea of selfensembling through weight averaging across the training trajectory is both novel and theoretically sound offering significant improvements in model robustness with less computational overhead compared to traditional ensemble methods. Experimental results further consolidate the effectiveness of SEAT portraying it as a strong contender in the adversarial defense landscape. However the paper could be improved by addressing the computational efficiency of the proposed method in more detail discussing limitations and expanding the scope of experimentation to include a wider variety of datasets and realworld scenarios. Addressing these points would provide a clearer understanding of SEATs performance and applicability across different domains and conditions. Overall the contribution of this work to the field of adversarial defense is significant with the potential to inspire further research into efficient and effective defense mechanisms. The proposed SEAT method represents a promising step forward in securing DNNs against adversarial threats warranting further exploration and development.", "gpp7cf0xdfN": " 1. Summary of the Paper This paper introduces a novel adversarial learning framework termed Reverse Engineering of Deceptions (RED) aimed at reverseengineering adversarial perturbations to recover the original images and understand the adversarys objectives behind the adversarially altered images. The authors highlight the challenge in recovering finelycrafted tiny adversarial perturbations which are not effectively handled by standard image denoising methods. To address this they propose a ClassDiscriminative Denoising based RED framework (CDDRED) which incorporates class discriminative ability and data augmentation strategies (specifically spatial transformations) into image denoising. Extensive experiments demonstrate that CDDRED effectively estimates and reverses adversarial perturbations across various evaluation metrics and adversarial attack methods outperforming existing image denoising baselines.  2. Strong and Weak Points Questions and Feedback Strong Points:  Novelty: The approach to reverseengineer adversarial images to recover both the perturbations and the original images represents a novel and meaningful direction in adversarial defense research.  Comprehensive Evaluation: The paper employs a wide array of metrics and adversarial attack methods to validate the effectiveness of CDDRED ensuring the robustness and generalizability of the proposed framework.  Practical Relevance: By enabling the recovery of original images from adversarially perturbed instances this research has significant applications in enhancing the security and trustworthiness of imagebased neural network models especially in critical applications. Weak Points:  Complexity and Scalability: The paper does not provide detailed insights into the computational complexity and scalability of the CDDRED framework which is crucial for its applicability to realworld largescale datasets.  Lack of Theoretical Underpinning: While the empirical results are promising the paper lacks a theoretical analysis to explain why the CDDRED approach works effectively across different types of adversarial attacks and datasets.  Comparison with StateoftheArt Techniques: The paper mainly compares CDDRED with denoising baselines but a comparison with stateoftheart adversarial defense methods could provide a better understanding of its relative performance. Questions and Feedback: 1. Scalability and Efficiency: Could the authors elaborate on the computational complexity of CDDRED and its scalability to large datasets or highresolution images 2. Theoretical Analysis: A theoretical explanation or analysis for the effectiveness of combining classdiscriminative denoising with data augmentation in reversing adversarial attacks would be valuable. 3. Broader Comparison: Including a comparison with stateoftheart adversarial defense methods not limited to denoising approaches could strengthen the papers claims on the superiority of CDDRED. 4. Potential Failure Cases: Discussing potential limitations or scenarios where CDDRED might fail or perform suboptimally would provide a more balanced view of the proposed framework.  3. Review Summary The paper presents an intriguing and novel concept of Reverse Engineering of Deceptions (RED) for adversarial image perturbations and proposes the CDDRED framework to address this challenge effectively. The comprehensive experimental evaluation across a variety of metrics and attack methods demonstrates the potential of CDDRED in recovering original images from their adversarially perturbed versions. Despite these strengths the paper would benefit from a discussion on the computational complexity and scalability of their approach a deeper theoretical understanding of why their method succeeds and an expanded comparison with other stateoftheart adversarial defense methods. Addressing these points could enhance the paper\u2019s contribution and provide clearer insights into the practical applicability and theoretical foundations of the CDDRED framework. In conclusion the paper makes a valuable contribution to adversarial machine learning literature by proposing a novel approach to reverseengineer adversarial attacks. However addressing the aforementioned recommendations could further underscore the significance and robustness of the proposed framework.", "kQMXLDF_z20": " Review of the Paper \"Topologyguided Graph Contrastive Layer for Deoversmoothing in Graph Neural Networks\"  1. Summary This paper addresses the critical issue of oversmoothing in graph neural networks (GNNs) where increased depth (i.e. more layers) leads to less discriminative node representations subsequently hindering tasks like node classification and link prediction. The authors provide a survey of existing deoversmoothing methods identifying three key metrics for evaluating such methods: a constant divergence indicator an easytodetermine divergence indicator and a modelagnostic strategy. They propose the Topologyguided Graph Contrastive Layer (TGCL) a novel method that satisfies all three criteria by employing contrastive learning to guide the divergence of latent representations based on the topology of the input graph. The effectiveness of TGCL is theoretically analyzed and empirically demonstrated through extensive experiments where it outperforms stateoftheart baselines.  2. Strong and Weak Points Questions and Feedback to the Author Strong Points:  Innovative Approach: The TGCL method is novel leveraging contrastive learning based on graph topology to address the oversmoothing problem effectively. Its emphasis on all three identified metrics is particularly commendable.  Comprehensive Evaluation: Extensive experimentation with realworld datasets and comparisons with stateoftheart methods provide strong empirical support for TGCLs effectiveness.  Theoretical Support: The paper offers a theoretical analysis of the proposed contrastive layer adding depth to the empirical findings. Weak Points:  Comparison of Metrics: While the paper identifies three crucial metrics for evaluating deoversmoothing methods and claims TGCL satisfies all it lacks a comparative analysis of how these metrics directly impact the models performance on various tasks.  Scalability Concerns: The paper does not address the computational complexity implications of incorporating the TGCL especially for very large graphs which could limit its applicability in more extensive realworld scenarios.  Lack of Ablation Study: Although the paper provides extensive experimentation it would benefit from an ablation study to understand the individual contributions of the proposed layers components. Questions and Feedback:  Impact of Graph Density on Performance: How does the density of the input graph (i.e. the number of edges relative to nodes) impact the effectiveness of TGCL in mitigating oversmoothing  Adaptability to Various GNN Architectures: Could the authors elaborate on how TGCL integrates with different GNN architectures and whether any specific types benefit more from this method  Comparison with Noncontrastive Approaches: How does TGCL compare to noncontrastive deoversmoothing methods especially in terms of computational efficiency and scalability  3. Review Summary The paper \"Topologyguided Graph Contrastive Layer for Deoversmoothing in Graph Neural Networks\" presents a significant advancement in the field of GNN research by addressing the critical issue of oversmoothing. The novel use of contrastive learning guided by graph topology represents a significant step forward in developing more effective GNN models capable of deeper learning without sacrificing the distinctiveness of node representations. The comprehensive experimental evaluation alongside the theoretical analysis provides a strong case for the proposed TGCL methods effectiveness marking it as a valuable contribution to the community. However to further strengthen the work addressing the scalability concerns and providing a detailed ablation study would be beneficial. Additionally exploring the methods adaptability across different GNN architectures could broaden its applicability. By tackling these points future research building on this work could expand the possibilities for applying deep GNN models across a wider range of complex realworld graph datasets. Given the strong theoretical foundation novel approach and comprehensive empirical validation I recommend the publication of this paper suggesting that addressing the mentioned weak points could make a strong paper even stronger.", "faMcf0MDk0f": " Review of \"BoolNet: Towards Efficient Binary Neural Networks Without 32bit Components\"  Summary: The paper introduces BoolNet a novel binary neural network (BNN) architecture designed to minimize energy consumption while attempting to maintain reasonable accuracy levels. Traditional BNNs despite their promise for efficiency often include 32bit components such as Batch Normalization PReLU and scaling factors which can significantly inflate energy usage and are not amenable to environments with stringent resource constraints. BoolNet addresses these limitations by eliminating most 32bit operations and employing a fully binary information flow throughout the network. Key innovations include integrating BatchNorm into the Sign function for inference removing 32bit multiplicationdivision operations and utilizing a Multislice strategy to preserve representational capacity without necessitating 32bit values. Experimental results on the ImageNet dataset demonstrate that BoolNet achieves a competitive 63.0 Top1 accuracy with a 2.95\u00d7 reduction in energy consumption compared to stateoftheart BNNs. The paper also underscores the disparity between theoretical instruction counts and practical performance metrics advocating for a broader consideration of memory usage and energy efficiency in BNN design and evaluation.  Strengths: 1. Innovative Architecture: BoolNets design is a significant departure from conventional BNN strategies that rely heavily on 32bit components to bridge the accuracy gap with fullprecision networks. This innovative approach advances the field toward truly efficient BNNs. 2. Comprehensive Methodology: The authors multifaceted strategy including the fusion of BatchNorm with the Sign function and the Multislice strategy shows a deep understanding of the principal challenges in designing efficient BNNs. 3. Solid Experimental Validation: The experiments conducted on the ImageNet dataset and the subsequent analysis provide strong evidence of BoolNets improved energy efficiency and reasonable accuracy performance. 4. Relevance and Timeliness: With the growing concern over the environmental impact and computational costs of deep learning the energy efficiency of neural networks is a timely topic. BoolNet directly addresses these concerns.  Weaknesses: 1. Comparison with StateoftheArt: While the paper makes a compelling case for BoolNets performance relative to certain BNN architectures a broader comparison with other compact and efficient network designs (beyond BNNs) could contextualize its advantages more effectively. 2. Lack of Details on Hardware Implementation: The paper mentions hardware accelerator simulations to support energy efficiency claims but does not delve into the specifics of the hardware setup or the potential challenges of realworld deployment. 3. Impact on Other Tasks and Datasets: The evaluation is primarily focused on ImageNet classification. Additional insights into BoolNets adaptability and performance across different tasks and datasets would be valuable.  Questions and Feedback for the Author: 1. Could you provide more details on the hardware accelerator simulations specifically the assumptions made and how broadly applicable the results might be to typical deployment scenarios 2. How does BoolNet perform on tasks beyond image classification such as object detection or segmentation Understanding its versatility could help in assessing its broader impact.  Review Summary: BoolNet represents a significant step forward in the design of energyefficient BNNs by eliminating reliance on 32bit components and introducing strategies to mitigate accuracy loss. The approach is both innovative and timely addressing critical concerns in the deep learning community regarding computational and environmental costs. However the paper could benefit from a broader comparison with nonBNN architectures additional details on hardware implementation considerations and exploration of BoolNets performance across a wider range of tasks and datasets. Nonetheless the paper provides valuable contributions to the field of efficient deep learning offering insights and methodologies that could facilitate the development of even more efficient BNNs and potentially influence future research directions in compact network designs.", "pgir5f7ekAL": " Draft Peer Review  Summarize the paper This paper investigates principal component analysis (PCA) within the framework of generative modeling particularly focusing on a scenario where the observed matrixs principal eigenvector is closely approximated by a generative models output. The authors introduce a novel quadratic estimator promising a statistical rate of order \\xe2\\x88\\x9ak logL m where L is the Lipschitz constant of the generative model k is the dimensionality of the models input and m is the sample size. Furthermore they adapt the classic power method to include projections onto the generative models range during iterations achieving exponential convergence under specific conditions. They compare their methods against established algorithms like the traditional power method and truncated power method in spiked matrix recovery and phase retrieval contexts using various image datasets showcasing significant performance improvements.  Strong Points 1. Innovative Approach: The integration of generative modeling with PCA to tackle highdimensional data and the curse of dimensionality is novel and addresses a pertinent problem in data science and machine learning. 2. Theoretical Contributions: The paper provides a comprehensive theoretical analysis including the statistical rate of the proposed estimator and the convergence properties of the adapted power method. 3. Practical Relevance: By demonstrating their methods efficacy on image datasets for applications like spiked matrix recovery and phase retrieval the authors bridge the gap between theory and practice emphasizing their contributions applicability. 4. Generality of Model: The proposed methods are not constrained to specific types of generative models or assumptions about data distribution enhancing their applicability across various domains.  Weak Points 1. Complexity and Implementational Details: The paper mentions projections onto the generative models range but leaves out specifics about how these projections are computationally achieved especially given the nonconvexity of typical generative model ranges. 2. Comparison with StateoftheArt: While the comparison with traditional and truncated power methods is valuable an evaluation against more recent or specialized methods in generative modelbased PCA or related areas would provide a clearer picture of the proposed methods standing. 3. Impact of Hyperparameters: The paper could better address how sensitive their methods are to hyperparameters such as the choice of initial vector or iteration number and guidelines for setting these in practice.  Questions and Feedback to the Author 1. Could you provide more details on the projection steps computational implementation given the usual nonconvexity of generative model ranges 2. It would be beneficial to include a comparison or discussion related to recent advancements in generative modelbased PCA or eigenvalue problems to contextualize your methods performance further. 3. More insight into the choice and impact of hyperparameters such as the initialization strategy and the number of iterations would aid in understanding the practical application of your methods.  Review Summary This paper presents a technically solid and innovative approach to addressing PCA in highdimensional settings through generative modeling assumptions. The introduction of a quadratic estimator and an adapted power method supported by theoretical analyses and empirical validations marks a significant contribution to the field. However certain areas would benefit from further clarification such as the computational aspects of required projections and more comprehensive comparisons with the current stateoftheart methods. Additionally providing a deeper insight into the practical application of the proposed methods particularly regarding hyperparameter sensitivity would enhance the papers impact. Overall the paper stands as a noteworthy advancement in integrating generative models with PCA algorithms opening avenues for future research in the area and application in tackling realworld highdimensional data challenges.", "zzk231Ms1Ih": " Review of \"A Theory of Parametric Tournament Representations\"  1. Summary The paper delves deeply into the structure and representation of tournaments in highdimensional space. By defining tournaments as complete directed graphs the authors aim to bridge the gap between structural knowledge and algebraic representation focusing chiefly on understanding how tournaments can be depicted in d dimensions. Two core questions guide this research: identifying the structural characteristics enabling tournaments to be represented in d dimensions and determining the minimum dimension necessity for a given tournament representation. Through a series of theorem formulations the authors unveil the forbidden configurations for tournament representations in d dimensions specifically characterizing rank 2 tournaments as locally transitive tournaments which can be solved using a Quicksort algorithm for the minimum feedback arc set problem. For higher dimensions the authors extend their investigation linking their findings with the Hadamard conjecture and present novel upper and lower bounds on the dimensionality needed to represent tournaments further exploring the implications on the signrank of matrices and applications in learning from pairwise comparison models.  2. Strong and Weak Points Questions and Feedback Strong Points:  Theoretical Contributions: The paper significantly advances our theoretical understanding of tournament representations particularly via the notion of forbidden configurations and their connection to flip classes. The decomposition strategy proposed is novel and opens new pathways for analyzing tournaments in higher dimensions.  Characterization of Rank2 Tournaments: The explicit characterization of rank 2 tournaments as locally transitive tournaments is both novel and insightful providing a clear bridge between algebraic and structural properties of tournaments.  Practical Implications: The application of findings to determine the lower and upper bounds on the dimensionality needed to represent tournaments has clear implications for machine learning especially in ranking and preference learning. Weak Points:  Empirical Validation: The paper primarily focuses on theoretical formulation and though it discusses realworld tournaments in passing there seems to be a lack of comprehensive empirical validation. Future work could benefit from a more detailed exploration of how these theoretical bounds hold up in practice across various domains.  Practical Algorithmic Contributions: While the paper successfully ties the theoretical constructs to the problem of ranking in tournaments it stops short of developing practical algorithmic tools that could be directly applied to realworld datasets. Questions and Feedback to the Author:  Empirical Evaluation: Could the authors elaborate on how the theoretical findings have been or could be empirically validated in realworld datasets Specific examples or case studies would be beneficial.  Extension to NonTournament Graphs: How extendable are the proposed theories and methods to nontournament graphs or to incomplete graphs that may arise in practical scenarios  Impact on Machine Learning Models: The paper hints at applications in machine learning models particularly around pairwise comparisons. Could the authors discuss any potential implications or insights their findings might offer for designing better machine learning algorithms for preference learning or ranking  3. Review Summary This paper makes significant theoretical contributions to understanding the structure of tournaments in highdimensional spaces and their representations. By structuring tournaments around forbidden configurations and offering a novel characterization of rank 2 tournaments the work lays a solid foundation for deeper explorations into tournament representations. While the paper excels in its theoretical foundation it leaves room for future explorations particularly in applying these concepts more concretely to realworld datasets and extending the insights to practical algorithm development. Moreover the work poses intriguing questions about the minimum dimensionality requirements for tournament representation and its broader implications including signrank and machine learning models. Hence while recommending publication its also recommended that the authors consider incorporating more empirical validations and exploring the practical applicability of their findings in subsequent works.", "gVRhIEajG1k": "Peer Review for \"Enhancing Adversarial Transferability from a Data Distribution Perspective\" 1) Summary of the Paper The paper presents a novel perspective on improving the adversarial transferability of attacks on deep neural networks (DNNs) by examining it from a data distribution standpoint. By observing that certain samples deemed LowDensity Data (LDD) are more susceptible to small Gaussian noise and thus have stronger transferability for their adversarial counterparts the authors propose the concept of Intrinsic Adversarial Attack (IAA). IAA seeks to align the direction of adversarial attacks with that of decreasing ground truth data density employing score matchingbased optimization. They achieve this through two main strategies: smoothing the activation functions with Softplus to decrease the impact of later layers and adjusting the influence of residual modules in networks like ResNet to encourage the use of early layer distributionrelevant information. The paper reports significant improvements in transferability across various DNN models surpassing current methods thus offering new insights into adversarial transferability and potential angles for defense against blackbox attacks. 2) Strong and Weak Points Questions and Feedback Strong Points:  Novelty of Perspective: The approach of studying adversarial transferability from the data distribution angle is innovative and opens up new avenues for research in adversarial example generation and defense mechanisms.  Comprehensive Methodology: The combination of smoothing activation functions and adjusting residual modules influence for better alignment of adversarial directions is wellthoughtout and the theoretical basis is soundly established.  Extensive Experimental Validation: The authors have conducted a wide range of experiments across multiple source and target models providing substantial evidence for the effectiveness of their method. Weak Points:  Model Dependency: While the method shows remarkable results theres a concern regarding its general applicability beyond the models tested (e.g. outside DNNs or nonimagebased tasks). It\u2019s unclear how well IAA performs on nonvision tasks or with fundamentally different architectures.  Potential for Evasion: The paper could benefit from a deeper exploration into how easily adversarial examples generated by IAA could be detected or mitigated considering the arms race in adversarial attack and defense.  Hyperparameter Sensitivity: The method involves tuning several hyperparameters (\u03b2 and \u03bb values). However the paper does not extensively discuss the sensitivity of these parameters or provide guidelines for their selection in different scenarios. Questions and Feedback:  Could the authors elaborate on the potential applicability of IAA to nonvision tasks or with architectures significantly different from those tested (e.g. transformers)  It would be beneficial to discuss the potential countermeasures that could mitigate the effectiveness of IAAgenerated adversarial examples and how they might affect current defense strategies.  Providing insight into the process of hyperparameter selection and its impact on the performance across different models would enhance the papers utility for practical implementation. 3) Review Summary and Recommendation The paper introduces a significant advancement in understanding and enhancing the adversarial transferability of attacks on DNNs by focusing on the data distribution perspective. The novel methodology combining activation function smoothing with the adjustment of residual modules to align with intrinsic attack directions demonstrates a clear improvement in adversarial example transferability across a range of models. However there are areas that could be strengthened such as providing a clearer pathway to applicability beyond the tested domains a thorough analysis of potential countermeasures and detailed guidance on hyperparameter sensitivity. Given the strong theoretical foundation substantial experimental validation and the fresh perspective provided the paper makes a significant contribution to the field. With improvements addressing the mentioned weaknesses this work can pave the way for future research in adversarial machine learning especially regarding the generation of transferable adversarial examples and the development of robust defense mechanisms. Recommendation: Accept with Revisions", "mdUYT5QV0O": "Summarize the paper: The paper introduces a novel algorithm Regularized Modernized Dual Averaging (RMDA) for efficiently training neural networks (NNs) with a regularization term that promotes desired structures within the neural networks aiming specifically at maintaining structured sparsity. Traditional stochastic gradient descent methods and their variants have shown empirical success in achieving sparse structures in NNs but have lacked theoretical guarantees that these structures persist throughout training iterations. RMDA addresses this by assuring that after a finite number of iterations the trained NN maintains the desired sparse structure confirmed by manifold identification theory. Through rigorous experimentation RMDA not only demonstrates its ability to maintain structured sparsity without compromising accuracy but also outperforms contemporary methods in terms of both structured and unstructured sparsity. This papers contribution lies in its principled analysis using manifold identification variance reduction without additional cost and empirical demonstration of superior performance over existing methods including stateoftheart pruning techniques. Strong Points: 1. Theoretical Foundation: The paper provides a comprehensive theoretical foundation for the RMDA algorithm utilizing manifold identification theory to guarantee that the desired structured sparsity is achieved and maintained. This addresses a significant gap in the literature by ensuring that the sparse structure isnt just a transient characteristic of the training process but a stable feature of the model. 2. Empirical Validation: The authors extensively test their algorithm across a variety of settings and show that it outperforms existing methods not only in achieving structured sparsity but also in maintaining competitive accuracy levels. This robust empirical investigation significantly strengthens the papers claims. 3. Practicality and Efficiency: The RMDA algorithm is designed with practical constraints in mind achieving variance reduction without the need for additional computational costs or intricate hyperparameter tuning. This makes the method particularly appealing for largescale deep learning applications as confirmed by the provided efficiency comparisons in terms of training time. Weak Points: 1. Complexity of Theoretical Concepts: The papers heavy reliance on advanced optimization and manifold identification concepts could make it inaccessible to readers who are not wellversed in these areas. Some sections may benefit from additional explanations or simplifications to make the paper more approachable to a broader audience. 2. Comparison with a Wider Range of Methods: While the paper does provide comparisons with several existing methods including stateoftheart pruning techniques there might be other contemporary methods or approaches developing concurrently that could provide a more varied context for evaluating RMDAs performance. 3. Discussion on Limitations and Future Work: The paper could benefit from a more detailed discussion on the potential limitations of the RMDA algorithm and the conditions under which it might not perform as expected. Additionally suggesting avenues for future research based on the observed results could add value. Questions and Feedback to the author: 1. Accessibility for a Broader Audience: Could the authors consider providing a more simplified explanation or perhaps a brief tutorial section on manifold identification for readers not familiar with the concept to make the paper more accessible 2. Evaluation Against Emerging Methods: Are there other possibly concurrent methods or algorithms being developed that also address structured sparsity in neural networks If so how might RMDA compare to these techniques 3. Limitations and Generalization: Could the authors elaborate on any limitations encountered during the development or testing of RMDA Furthermore how well does the algorithm generalize across different types of neural networks or machine learning tasks beyond those tested Review Summary: This paper presents a significant and novel contribution to the field of neural network training algorithms by introducing RMDA a method that guarantees the achievement and maintenance of structured sparsity within neural networks through manifold identification theory. The theoretical grounding of the paper is robust and wellarticulated supported by extensive empirical evidence showcasing the superiority of RMDA over existing methods in various aspects including maintaining high accuracy levels and improving upon stateoftheart pruning techniques. Despite the depth and complexity of the theoretical content which may limit accessibility the practical implications and efficiency of RMDA are clear offering a significant advancement in the training of structured neural networks. The paper would benefit from a broader comparison with emerging methods and a more detailed discussion on limitations and potential future research directions. Overall the contributions of this paper are highly valuable providing both theoretical insights and practical tools for the development of efficient and structured neural networks.", "gxk4-rVATDA": " Peer Review:  1) Summary of the Paper: The paper presents an innovative algorithm for training neural networks by directly learning the individual bits of the weights rather than the weights themselves. This approach allows for training with integer weight values at arbitrary bit depths facilitating the natural discovery of sparse networks without the need for additional regularizations or constraints. The authors empirically demonstrate superior performance with this method over standard training techniques in fully connected networks and comparable results in residual networks. A key finding is that the first three most significant bits contribute most significantly to high accuracy with the remaining bits providing intrinsic regularization. Remarkably over 90 of a networks capacity can encode arbitrary data without impacting performance suggesting a novel avenue for data storage within neural networks. The paper also delves into connections with other works demonstrating how this method relates to and extends upon existing techniques in neural network quantization and sparsity.  2) Strong and Weak Points of the Paper: Strong Points:  Novelty: The proposed bitwise training method introduces a unique perspective to training neural networks focusing on the bits of weights. This approach can potentially transform network quantization and sparsity strategies.  Empirical Validation: The authors provide extensive empirical evidence supporting their claims including experiments with different networks and datasets. The fact that over 90 of the network can store arbitrary codes without affecting performance is particularly intriguing.  Potential for Practical Applications: This method opens up new potential applications including efficient network compression and the embedding of large datasets or even other neural networks within a single network without loss of performance. Weak Points:  Lack of Theoretical Foundation: While the empirical results are strong the paper lacks a deep theoretical analysis explaining why training the most significant bits leads to high performance and why the remaining bits act as intrinsic regularization.  Potential Security Concerns: The ability to encode arbitrary data within a network without affecting its performance might pose unforeseen security risks particularly if used to embed malicious data. This aspect is only briefly touched upon in relation to malware storage.  Generalization across Various Architectures: The paper primarily discusses results with fully connected and residual networks. It would be beneficial to understand how this method performs across a wider variety of architectures including modern attentionbased models. Questions and Feedback to the Author:  Could the authors provide theoretical insights or a more detailed hypothesis on why the first three most significant bits predominantly determine network performance  How does the bitwise training approach impact the interpretability of neural networks if at all  Given the potential security implications of storing arbitrary codes in neural networks what measures do the authors suggest to mitigate misuse of this feature  3) Review Summary: The paper introduces a highly innovative method for training neural networks at the bit level providing a fresh perspective on weight quantization and sparsity. The empirical results are compelling demonstrating not only superior or comparable performance to traditional training methods but also revealing the capacity for networks to store extensive amounts of arbitrary data without performance degradation. This work opens new avenues for research in neural network efficiency and data storage. However the paper could benefit from a deeper theoretical understanding of the observed phenomena and a broader investigation into the methods applicability across different neural architectures. Additionally the potential security implications of embedding arbitrary data within networks warrant further discussion and exploration. Overall the paper provides a significant contribution to the field of deep learning challenging traditional notions of neural network training and capacity. I recommend publication provided the authors address the above feedback particularly regarding theoretical explanations and the exploration of potential security measures for the data embedding capabilities of their method."}