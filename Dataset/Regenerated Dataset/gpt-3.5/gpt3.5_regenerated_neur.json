{"wS23xAeKwSN": " Review of \"PolarMix: Effective Point Cloud Augmentation for LiDAR Data\" 1. Summary: The paper introduces PolarMix a novel point cloud augmentation technique designed for LiDAR data. PolarMix addresses the challenge of collecting and annotating large amounts of point clouds by proposing two augmentation strategies: scenelevel swapping and instancelevel rotation and paste. Extensive experiments demonstrate the effectiveness of PolarMix across various perception tasks and datasets showing consistent performance gains and applicability to different deep architectures. 2. Strengths:  Innovative Approach: PolarMix introduces a unique point cloud augmentation technique tailored for LiDAR data focusing on preserving point cloud fidelity and enriching distributions effectively.  Performance Evaluation: The paper conducts comprehensive experiments demonstrating the superiority of PolarMix in enhancing network training and performance across different perception tasks and datasets.  Generalizability: PolarMix is shown to be applicable across various deep architectures tasks and datasets showcasing its versatility and usability as a plugandplay tool. 3. Weaknesses:  Clarity in Presentation: The technical details of the augmentation strategies particularly the scenelevel swapping and instancelevel rotation and paste could be further elaborated to enhance understanding.  Comparison: While the paper compares PolarMix with baseline methods and showcases its performance across different tasks a more detailed comparison with existing stateoftheart methods in point cloud augmentation could strengthen the evaluation. 4. Questions and Suggestions for Authors:  Technical Detail Clarification: Could the authors provide more insights into the computational complexity of the proposed PolarMix technique compared to other stateoftheart augmentation methods  Transferability: How does the performance of PolarMix vary when applied to realworld LiDAR data with noise and outliers Addressing realworld challenges would enhance the practical applicability of the technique. 5. Limitations: The paper does not explicitly address potential negative societal impacts or ethical considerations arising from the use of data augmentation techniques. To improve the authors could include a section discussing any ethical implications and measures to mitigate negative societal impacts associated with PolarMix. Overall the paper presents a valuable contribution in the field of LiDAR data augmentation with the introduction of PolarMix. Addressing the suggested improvements and ethical considerations would enhance the credibility and applicability of the proposed technique.", "l2CVt1ySC2Q": " Peer Review:   1) Summary: The paper investigates the excess capacity of deep networks in the context of supervised classification. It extends and unifies prior Rademacher complexity bounds to accommodate modern architectures such as residual networks. The study aims to understand the retention of empirical error while reducing the capacity of networks. Experiments on multiple datasets demonstrate substantial excess capacity per task and a similarity in capacity levels across tasks.  2) Strengths and Weaknesses: Strengths:  Theoretical Contributions: The paper introduces novel convolutionspecific covering number bounds and extends Rademacher complexity analysis to handle modern network architectures effectively.  Experimental Validation: Extensive experiments on benchmark datasets support the theoretical findings demonstrating the compressibility of networks with respect to weight norms.  Methodological Rigor: The clear methodology for controlling the empirical Rademacher complexity through weight norms and Lipschitz constants adds credibility to the findings. Weaknesses:  Complexity of Analysis: The intricate mathematical derivations and technical language could potentially limit accessibility for readers without advanced theoretical expertise.  Experiment Design: While the experiments demonstrate the concept of excess capacity limitations in data augmentation and network architectures might affect the generalizability of the findings.  3) Questions and Suggestions for Authors:  Clarification on Constraints: Could the authors elaborate on the specific constraints enforced during optimization especially how they affect the training dynamics and model performance  Impact on Pruning Techniques: Have the authors considered how their findings on compressibility with weight norms could inform or be integrated with existing model compression techniques like weight pruning  4) Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work. To enhance the paper they could provide further insights into the interpretability of the capacitydriven quantities and their impact on practical applications in machine learning domains.  In conclusion the paper presents a thorough investigation into the excess capacity of deep networks through a combination of theoretical analysis and empirical experiments. Addressing the raised questions and suggestions could further strengthen the clarity and applicability of the research findings. The work holds promise in advancing the understanding of neural network capacity and its implications for model optimization and generalization.", "tlUnxtAmcJq": " PeerReview Draft  Summary: The paper introduces a novel approach SO3KRATES for efficient modeling of nonlocal electronic effects in quantum chemistry using a modified attention mechanism based on spherical harmonic coordinates (SPHCs). The proposed model utilizes selfattention based message passing neural networks to uncouple atomic features from higherorder geometric information achieving stateoftheart results on benchmarks while requiring fewer parameters and offering substantial speedup compared to previous models. The work addresses the limitations of current architectures in capturing nonlocal effects efficiently.  Strengths: 1. Innovative Approach: The use of SPHCs and the proposed attention mechanism are innovative and address the challenge of nonlocal electronic effects in molecular simulations effectively. 2. Model Performance: SO3KRATES demonstrates superior performance on benchmarks exceeding stateoftheart results while requiring fewer parameters and providing significant speedups during training and inference. 3. Theoretical Rationale: The paper is theoretically sound grounding its methodology in established concepts in machine learning and quantum chemistry. 4. Clarity and Organization: The paper is wellstructured with detailed explanations of the methodology results and implications.  Weaknesses: 1. Complexity: The proposed model introduces additional complexity in managing spherical harmonic coordinates which might impact the interpretation and implementation by other researchers or practitioners. 2. Evaluation: While the paper extensively evaluates the model performance on benchmarks a more detailed comparison with existing architectures and a broader range of datasets could strengthen the validation of the proposed approach. 3. Scalability: The potential scalability issues of the current implementation especially in terms of computational resources for largescale applications are not thoroughly addressed in the paper.  Questions and Suggestions: 1. Clarifications on Implementation: Provide a more detailed explanation of how the spherical neighborhoods N are computed and address the computational efficiency implications of this approach. 2. Scalability: Can the authors discuss potential strategies to enhance the scalability of SO3KRATES for large molecular systems or datasets in future developments 3. Data Efficiency and Generalization: Could the authors elaborate on how SO3KRATES adapts to new larger structures not seen during training and its performance in scenarios with limited training data  Limitations: The paper needs to further address the potential negative societal impact by discussing ways to prevent misuse of the developed methods for harmful applications such as designing chemical warfare agents or other irresponsible uses.  Overall Evaluation: The paper presents a novel approach SO3KRATES for efficient modeling of nonlocal electronic effects in quantum chemistry demonstrating promising results. The methodological contribution is significant although further clarification on scalability and practical implementation aspects is needed. Addressing the limitations and societal impact of the work would strengthen the overall contribution.", "nosngu5XwY9": " Summary: The paper introduces a novel framework called Dynamic Inverse Reinforcement Learning (DIRL) that addresses the limitation in traditional Inverse Reinforcement Learning (IRL) models by allowing for timevarying reward functions. The authors demonstrate DIRL in simulated experiments and on real mouse decisionmaking data in a labyrinth environment. The method successfully recovers interpretable reward functions for mice cohorts providing insights into exploratory behavior and goaldriven decisionmaking strategies.  Strengths: 1. Innovative Contribution: The development of DIRL fills a gap in traditional IRL models by accommodating timevarying rewards making it particularly valuable for characterizing decisionmaking behavior in complex environments. 2. Demonstrated Application: The authors effectively demonstrate DIRL in both simulated gridworld and real mouse data scenarios showcasing its adaptability and utility in understanding and interpreting animal behavior. 3. Interpretability: The method provides interpretable results allowing for the identification of distinct goal maps and timevarying weights enhancing the understanding of the decisionmaking processes of the subjects. 4. Comparative Analysis: Comparing the performance of DIRL with existing IRL frameworks highlights the superior performance of DIRL in explaining and modeling animal behavior.  Weaknesses: 1. Limitation in Scalability: The paper mentions concerns regarding scalability to highdimensional state spaces due to learning separate rewards for each state in each goal map. Further exploration on scaling this approach may be needed. 2. Uncertainties in Model Generalization: The requirement for a significant number of trajectories to accurately infer an animals timevarying reward function might limit the generalizability of the model in situations with sparse data.  Questions and Suggestions for Authors: 1. Can the authors elaborate on the computational efficiency of the proposed DIRL framework compared to traditional IRL methods especially when dealing with larger datasets or more complex environments 2. How does DIRL handle noisy or incomplete trajectory data and what steps were taken to ensure robustness against variations in data quality 3. Considering the potential application of DIRL in fields beyond neuroscience such as healthcare have the authors considered the ethical implications of utilizing animal behavior data for training artificial agents  Limitations: The authors have discussed the limitations associated with the scalability and generalizability of the approach. To improve the authors should explore methodologies to reduce the dependency on a high number of trajectories for robust inference and should develop strategies for handling highdimensional state spaces efficiently.  Constructive Suggestions for Improvement: To enhance the paper: 1. Conduct further experiments to test the performance of DIRL in different and more complex environments. 2. Address the scalability issue by exploring approaches that reduce the number of trajectories required for inference while maintaining model accuracy. 3. Provide a discussion on potential ethical considerations and societal impacts of utilizing DIRL in various applications outside of neuroscience. The paper presents an innovative contribution with strong applicability and insightful results. However addressing scalability and robustness issues and discussing broader societal implications would further strengthen the work.", "uNYqDfPEDD8": " Summary: The paper proposes a novel neural pipeline for joint placement and routing in chip design using reinforcement learning and conditional generative models. It addresses challenges in mixedsize macro placement and oneshot routing. Experimental results demonstrate the effectiveness of the approach.  Strengths: 1. Innovative Approach: The paper introduces a novel neural pipeline for joint placement and routing without traditional solvers filling gaps in existing literature. 2. Mixedsize Friendly Approach: The proposed RLbased model for mixedsize macro placement improves realism and reduces postprocessing requirements. 3. Efficient Routing: The oneshot conditional generative routing model optimizes routing efficiency showing a competitive performance. 4. Thorough Methodology: The paper provides detailed methodology enhancing reproducibility and understanding of the proposed techniques.  Weaknesses: 1. Limited Generalizability: The evaluation focuses on specific benchmarks limiting the generalizability of the results. 2. Lack of Scalability Analysis: The scalability of the proposed approach to larger design sizes or complexities is not discussed. 3. Limited Comparison: The comparison with existing methods is somewhat limited and more extensive benchmarking against stateoftheart solutions is needed. 4. Societal Implications: The potential negative impact on job loss in the EDA industry due to AIbased automation should be addressed.  Questions and Suggestions for Authors: 1. Can you provide insights into the scalability of your approach to handle larger chip designs or more complex routing scenarios 2. How does the proposed method perform on different datasets beyond the ones presented It would be interesting to see the robustness of the model across a variety of benchmarks. 3. Have you considered any realworld applications or industry collaborations to validate the effectiveness of your approach in practical chip design scenarios 4. To enhance the papers impact can you discuss potential strategies to mitigate the negative societal impact such as reskilling programs for displaced workers or collaborative research efforts with industry partners  Limitations: The authors should further address the potential negative societal impact of their work particularly job loss in the EDA industry by providing constructive suggestions for mitigation such as reskilling programs or collaborative partnerships with industry stakeholders.", "q__FmUtPZd9": " Summary The paper introduces a new problem in decisionmaking called \"inverse decisionmaking with task migrations\" specifically focused on social contagion management tasks. The main idea is to solve one decisionmaking task by using querydecision pairs from another task even if the underlying models are unknown. The paper presents a generic framework called SocialInverse provides theoretical analysis and conducts empirical studies to validate the approach.  Strengths 1. Novel Problem: The paper addresses a novel and interesting problem in decisionmaking showcasing the potential for crosstask learning without knowing the underlying models. 2. Theoretical Analysis: The paper provides a rigorous theoretical analysis including generalization analyses and upper bounds on the generalization error presenting a solid foundation for the proposed framework. 3. Empirical Validation: Empirical studies demonstrate the effectiveness of the proposed framework showcasing promising results and comparing favorably with other learningbased and graphbased methods. The experimental setup is welldocumented and thorough.  Weaknesses 1. Complexity: The paper is heavily theoretical and at times may be challenging for readers unfamiliar with advanced topics in decisionmaking and social contagion management. More intuitive explanations or examples could enhance readability. 2. Limitations Clarity: While mentioning limitations in passing a dedicated section identifying specific limitations and potential negative societal impacts would add clarity and completeness to the discussion.  Questions and Suggestions for Authors 1. Clarifications: Could you provide more intuitive examples or visuals to aid in understanding the proposed framework and problem setting 2. Generalization Performance: How does the generalization performance of SocialInverse compare to other stateoftheart methods in decisionmaking tasks with or without task migrations 3. Scalability: Have you considered the scalability of the proposed framework to larger graphs or datasets How does the method fare in realworld scenarios with complex and massive networks  Limitations The authors briefly mentioned limitations and potential negative societal impacts of their work. However a more detailed discussion on addressing these limitations and mitigating any negative impacts could be beneficial. For further improvement the authors could consider:  Providing a comprehensive assessment of potential ethical privacy or bias issues in deploying the proposed framework.  Offer suggestions on how to enhance transparency and interpretability of the decisionmaking process to address societal concerns.", "pgF-N1YORd": " Review of the Paper: \"Understanding Transfer in Continual Reinforcement Learning\" 1. Summary and Contributions: The paper aims to enhance the understanding of transfer in continual reinforcement learning by studying the impact of various components of the soft actorcritic (SAC) algorithm on transfer efficacy. The study uses Continual World as the suite of continuous control tasks focusing on critic and actor networks exploration and data handling. The best approach named ClonExSAC achieves a 87 final success rate and significantly improves transfer performance compared to existing methods. 2. Strengths:  Comprehensive Analysis: The paper conducts a thorough and systematic study on transfer in continual reinforcement learning.  Novelty: The research fills a gap in understanding transfer mechanisms in reinforcement learning providing valuable insights for the community.  Methodological Rigor: By utilizing a suite of tasks and performing experiments with multiple CL methods the study ensures a robust evaluation.  Concrete Recommendations: The recommendations provided such as using behavioral cloning and efficient exploration policies offer practical guidance for improving transfer in CRL. 3. Weaknesses:  Scope Limitation: The study focuses primarily on SAC and Continual World limiting the generalizability of findings to other RL algorithms or environments.  Potential Overlooking of Factors: There might be additional factors influencing transfer not addressed in the study such as task complexity or network architecture modifications.  Lack of Realworld Application: The paper could benefit from discussing potential realworld applications and implications of the proposed ClonExSAC method. 4. Questions and Suggestions for Authors:  The authors could clarify how the findings might extend or differ in more complex environments or with different RL algorithms.  It would be beneficial to explore the scalability and applicability of ClonExSAC in practical RL scenarios or benchmark it against more diverse benchmarks.  How might the proposed method adapt to nonstationary environments with evolving tasks or dynamics 5. Limitations: The authors should provide a comprehensive discussion on the potential societal impacts of their work especially concerning data retention for continual learning systems. Suggestions for improvement could include privacypreserving mechanisms or data anonymization strategies to address these limitations. Overall the paper presents a significant contribution to the understanding of transfer in continual reinforcement learning and proposes a promising method ClonExSAC to improve transfer performance. Addressing the identified limitations could further enhance the practical relevance and societal acceptance of the proposed research.", "rUc8peDIM45": " Review 1) Summary: The paper explains the phenomenon of stochastic gradient descent (SGD) favoring flat minima by relating SGDs noise structure to its linear stability. The authors consider training overparameterized models with square loss and prove that stable minima for SGD are characterized by bounded sharpness as per the Frobenius norm of the Hessian matrix. The alignment property of SGD noise is emphasized for linear and random feature models supported by empirical results on CIFAR10 dataset. 2) Strengths:  The paper addresses a significant topic in understanding the implicit regularization of SGD by linking noise structure to flat minima selection.  The theoretical approach is detailed and provides a quantitative characterization of how learning rate and batch size impact flatness of minima accessible to SGD.  The empirical validation on CIFAR10 dataset reinforces the practical relevance and validity of the theoretical findings. 3) Weaknesses:  The discussion on the implications of the results in realworld scenarios or broader applications beyond CIFAR10 dataset could be further elaborated. How can these findings impact other machine learning tasks or model architectures  While the theoretical framework is elaborate the practical implications or recommendations for practitioners could be more explicitly stated. How can these insights guide model training or optimization practices 4) Questions and Suggestions for Authors:  Can the authors elaborate on how the alignment property of SGD noise could be leveraged in the optimization process or model design to enhance generalization capabilities  How do the findings of this research contribute to the broader understanding of optimization strategies in deep learning and their impact on model performance  Further exploration into the scalability of the proposed approach to larger datasets or more complex model architectures could provide insights into its applicability in realworld scenarios. 5) Limitations and Suggestions for Improvement: The authors could further elaborate on the limitations of their study including the generalizability of their findings across different datasets model architectures and optimization scenarios. Additionally discussing the potential negative societal impacts or ethical considerations related to the application of the presented methods could enhance the manuscripts completeness and relevance. In conclusion the paper presents a thorough analysis of the relationship between SGD noise structure and flat minima selection with strong theoretical foundations and empirical validation. Clarifications on the practical implications and broader applications of the findings could further enhance the impact of the research.", "nRcyGtY2kBC": " Peer Review  Summary: The paper introduces heterogeneous transfer causal effect (HTCE) learners to enhance the estimation of conditional average treatment effects (CATE) in a target domain by leveraging information from a source domain with different feature spaces. The study addresses a crucial problem in causal inference for personalized treatment recommendations and provides insights into the performance of various CATE learners in a transfer learning scenario. The experimental evaluation on a newly proposed semisynthetic data simulation benchmark demonstrates the effectiveness of the HTCElearners in diverse settings.  Strengths: 1. Innovative Framework: The paper introduces a novel approach to tackle the challenging problem of heterogeneous transfer learning in CATE estimation catering to the need for personalized treatment interventions. 2. Clear Presentation: The paper is wellorganized with detailed explanations of the proposed methodology enhancing understanding for readers familiar with both causal inference and machine learning. 3. Experimental Validation: The thorough experimental evaluation on multiple datasets and the comparison against benchmarks provide substantial evidence of the effectiveness of the proposed HTCElearners. 4. Insightful Analysis: The detailed analysis of the impact of different components and settings on performance such as information sharing and selection bias adds depth to the study and guides potential improvements.  Weaknesses: 1. Theoretical Foundations: The paper lacks extensive theoretical analysis of the proposed HTCElearners warranting further discussion on the convergence properties robustness and generalization capabilities of the method. 2. Limitation Discussion: While limitations are briefly addressed a more comprehensive exploration of potential negative societal impacts ethical considerations and generalizability of the proposed method would enhance the papers contribution. 3. Application Scenarios: The study showcases the methodology on synthetic data and real datasets but could benefit from additional realworld application scenarios to validate the practical utility of the HTCElearners.  Questions and Suggestions for Authors: 1. Could the authors elaborate on the scalability of the proposed method in handling largescale datasets or streaming data scenarios 2. Can the authors discuss the computational complexity and training times of the HTCElearners compared to standard CATE learners especially in scenarios with limited data 3. How does the proposed framework cater to cases where the feature spaces between the source and target domains have significant discrepancies Are there methods proposed to handle drastic differences in features  Limitations: The authors have briefly addressed the limitations in the paper. To further enhance the studys impact it is recommended to provide a comprehensive discussion on the potential negative societal impacts by considering ethical implications healthcare disparities and biases that the proposed method may introduce.  Suggestions for Improvement: 1. Include a section discussing the ethical considerations potential biases and societal implications of the HTCElearners to ensure responsible deployment and decisionmaking. 2. Extend the theoretical analysis to provide guarantees on the methods performance and robustness in various transfer learning scenarios. Overall the paper presents a valuable contribution to the field of causal inference and transfer learning but further theoretical analysis realworld applications and ethical considerations would strengthen the studys impact and relevance. Great work on introducing the HTCElearners", "s1yaWFDLxVG": " Summary The paper investigates the sensitivity of firstorder optimization analyses to assumptions on objective functions emphasizing the limitations of traditional smoothness and strong convexity assumptions. The authors propose a new set of conditions lower restricted secant inequality (RSI\u2212) and upper error bounds (EB) and demonstrate that under these conditions gradient descent is exactly optimal among all firstorder algorithms.  Strengths 1. Innovative Contribution: The paper introduces novel conditions (RSI\u2212 and EB) that address the limitations of traditional assumptions showcasing an innovative approach to firstorder optimization analysis. 2. Theoretical Rigor: The paper provides a rigorous mathematical analysis establishing necessary and sufficient interpolation conditions and proving lower bounds on convergence rates enhancing the theoretical foundation of the study. 3. Practical Relevance: Empirical verification of RSI\u2212 \\xe2\\x88\\xa9 EB conditions on neural network optimization demonstrates practical applicability bridging theory with realworld scenarios.  Weaknesses 1. Complexity: The papers depth and technicality might pose challenges for readers unfamiliar with advanced optimization theory requiring a high level of mathematical expertise to fully grasp the content. 2. Limited Contextualization: The paper could benefit from clearer connections to existing literature on optimization assumptions and a more detailed discussion on the broader implications of the proposed conditions in the field of machine learning and optimization.  Questions and Suggestions for Authors 1. Clarity for NonExperts: How can the authors simplify the presentation to make the concepts more accessible to a broader audience including researchers new to optimization theory 2. Comparative Analysis: Could the authors elaborate on how the proposed conditions (RSI\u2212 and EB) compare to other existing assumptions in terms of practical performance on various optimization problems 3. Future Directions: What are potential extensions of this work such as exploring other classes of functions or investigating more complex optimization algorithms under the proposed conditions  Limitations The paper does not adequately address the potential negative societal impact or ethical considerations of its theoretical findings. Authors could consider including a section discussing the broader implications and ethical considerations of their work to ensure responsible research practices.  Constructive Suggestions for Improvement 1. Inclusion of Ethical Considerations: Incorporate a discussion on the societal implications of the research and any potential ethical considerations arising from the optimization assumptions proposed. 2. Clear Practical Relevance: Emphasize the practical implications of the findings for practitioners in machine learning and optimization highlighting how these conditions can enhance algorithm performance in realworld applications. 3. Broader Contextualization: Provide a thorough discussion on how the proposed conditions align with existing optimization paradigms and offer novel insights into algorithm efficiency and convergence rates.", "plu6AK3qs5T": " Summary: The paper proposes a method called AutoClipping that eliminates the need to tune the clipping threshold R for differential private (DP) training of deep learning models. The authors demonstrate that this method is as private and efficient as existing DP optimizers while being easier to use. They provide rigorous convergence analysis and show superior performance on various tasks compared to stateoftheart techniques.  Strengths: 1. Innovative Contribution: Introducing AutoClipping as a method to automate the selection of clipping threshold R is a novel and impactful contribution. 2. Theoretical Analysis: The paper provides comprehensive theoretical analysis and convergence results for the proposed automatic DP training method. 3. Empirical Results: Strong experimental results across various vision and language tasks demonstrate the effectiveness of AutoClipping. 4. Reproducibility: The authors provide code snippets and detailed settings for reproducibility.  Weaknesses: 1. Need for Formal Evaluation: The paper could benefit from a more formal evaluation of potential drawbacks or limitations of AutoClipping. 2. Additional Comparisons: More detailed comparisons with existing methods especially in terms of computational efficiency could further strengthen the paper.  Questions and Suggestions: 1. Efficiency Comparison: Have the authors considered including efficiency comparisons with existing methods especially in terms of computational resources used 2. Scalability: How does AutoClipping scale with very large models in terms of training time and resource requirements 3. Robustness Analysis: Have the authors evaluated the robustness of AutoClipping to various adversarial attacks or noise in the input data  Limitations: The authors have not adequately addressed potential negative societal impacts. To improve they could discuss the implications of their work on privacy data security and potential misuse by malicious parties. They should also consider addressing how to safeguard against any misuse of their algorithm in harmful applications.", "uLYc4L3C81A": " PeerReview:  1) Summary: The paper introduces the Confident Adaptive Language Modeling (CALM) framework aiming to dynamically allocate compute resources during the generation process of large language models to enhance efficiency while maintaining high performance. The study addresses challenges related to early exiting from Transformerbased large language models and performs a theoretical analysis proposing confidence measures for pertoken exit decisions. Empirical experiments across multiple text generation tasks demonstrate the efficacy of CALM in reducing compute with a speedup of up to three times while maintaining performance quality.  2) Strengths:  Innovative Framework: Introducing a novel CALM framework for adaptive language modeling.  Theoretical Grounding: The study provides a theoretical analysis of the early exiting paradigm and proposes efficient strategies for maintaining text and risk consistency in generated outputs.  Empirical Validation: Empirical experiments on diverse tasks demonstrate the effectiveness of the CALM framework in reducing compute complexity with substantial speedup gains.  Efficiency Gains: The approach showcases promising efficiency gains while controlling for performance consistency through different exit confidence measures.  3) Weaknesses:  Confidence Measures: The paper could benefit from a more detailed analysis of the tradeoffs and impacts of different confidence measures used for earlyexit decisions.  Limited Experimental Analysis: While the empirical experiments demonstrate the efficacy of CALM a deeper analysis of potential failure cases or scenarios where the framework might encounter challenges could provide a more comprehensive understanding.  4) Questions and Suggestions:  Question: How do the different confidence measures used in the CALM framework impact the overall performance and efficiency gains across various text generation tasks  Suggestion: Further investigate and compare the performance of earlyexit decisions based on different confidence measures providing insights into their relative effectiveness and tradeoffs.  Question: How does CALM handle scenarios where early exits may lead to suboptimal predictions or inconsistency in the generated outputs  Suggestion: Consider conducting additional experiments to evaluate CALMs performance resilience in challenging scenarios addressing potential failure cases or model limitations.  5) Limitations: The paper adequately addresses the limitations and potential negative societal impact of their work by focusing on efficiency gains without compromising performance quality. To further enhance the discussion on limitations the authors could consider exploring the interpretability and robustness of the confidence measures in realworld applications as potential areas for improvement. Overall the paper presents a compelling framework for achieving efficiency gains in large language models while maintaining performance consistency. Further exploration of confidence measures and robustness analysis could strengthen the studys contribution to the field of language modeling and text generation.", "opw858PBJl6": " Summary: The paper focuses on the evaluation of saliency methods used for explainability in deep learning. It highlights the limitations of current evaluation methods that focus on completeness but overlook soundness. The authors introduce new evaluation metrics that incorporate both completeness and soundness in an intrinsic framework. A simple saliency method is proposed which performs well in both completeness and soundness evaluations. The experiments also provide insights regarding the intrinsic justifications for popular heuristic tricks such as TV regularization and upsampling.  Strengths: 1. Novel Contribution: The paper introduces a new perspective on evaluating saliency methods by incorporating both completeness and soundness in intrinsic evaluations which fills a gap in the current research. 2. Methodological Rigor: The paper provides a detailed explanation of the proposed metrics and methods along with rigorous mathematical formulations enhancing the credibility of the research. 3. Experimental Evaluation: The experiments conducted demonstrate the efficacy of the proposed completeness and soundness metrics showcasing the utility of these evaluation criteria.  Weaknesses: 1. Human Interpretability: The paper lacks discussion on the human interpretability aspect of explanations which is crucial for realworld applications and user understanding. 2. Limited Dataset Coverage: The study mainly employs CIFAR10 and Imagenette for evaluations limiting the generalizability of the findings. Employing diverse datasets could enhance the robustness of the proposed metrics.  Questions and Suggestions for Authors: 1. Dataset Diversity: Have you considered extending the experimental evaluation to diverse datasets including those with varying complexities and domains to validate the effectiveness of the proposed metrics across different scenarios 2. HumanCentric Consideration: How can the proposed completeness and soundness metrics be integrated with human interpretability assessments to provide a comprehensive evaluation of saliency explanations  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To improve they could consider including a section discussing ethical considerations implications for bias and fairness in the deployment of saliency methods to mitigate any adverse effects.", "pFqgUJxXXz": " Summary: The paper proposes Adversarial Task Upsampling (ATU) framework as a new approach for metalearning task augmentation. ATU aims to generate tasks that are taskaware taskimaginary and modeladaptive to improve metalearner performance. The paper provides theoretical justifications introduces an algorithm and conducts experiments on regression and classification tasks to validate the effectiveness of ATU in improving generalization capability.  Strengths: 1. Innovative Approach: The proposed ATU framework addresses the challenge of task augmentation in metalearning by generating tasks that are taskaware imaginary and adaptive. This is a novel and promising approach. 2. Theoretical Justifications: The paper provides a clear theoretical analysis about the ATU framework explaining how it maximizes taskawareness compared to existing approaches. This adds depth to the proposed method. 3. Comprehensive Experiments: The paper conducts extensive experiments on regression and classification tasks covering various datasets and metalearners to show the effectiveness of ATU in improving metalearning performance. The results consistently demonstrate the superiority of ATU. 4. Visualization: The paper includes visualizations that showcase the quality of upsampled tasks generated by ATU providing insights into the taskawareness and taskimaginary properties of the generated tasks.  Weaknesses: 1. Complexity of the Method: The proposed ATU framework with its multistage generation strategy and adversarial loss might be complex and challenging to implement in practical scenarios. Clearer explanations or simplifications could benefit readers. 2. Simplification of Experiment Results: While the paper presents detailed experimental results a more concise and visual representation of key findings could enhance the readability and impact of the results section.  Questions and Suggestions for Authors: 1. Clarity on Implementation: Can the authors provide a more detailed explanation or visual representation of the ATU algorithm to enhance understanding and help readers implement the method effectively 2. Scalability: How does ATU perform when the number of tasks in the metatraining set increases significantly Does the effectiveness of ATU diminish with a larger number of tasks 3. Comparison with StateoftheArt: Are there additional stateoftheart methods or benchmarks that could be included in the comparison to further highlight the superiority of ATU  Limitations: The authors have not adequately addressed the negative societal impact of their work. A constructive suggestion for improvement would be to discuss how the proposed method could be applied responsibly to avoid reinforcing biases or creating unintended consequences for realworld applications.", "mSiPuHIP7t8": "Summary: The paper introduces GraphDE a probabilistic generative framework for graph debiased learning and outofdistribution (OOD) detection. It addresses the issues of outliers in training data and OOD samples in testing data using a unified model to probabilistically characterize graph data shifts. The framework includes a recognition model structure estimation model and classification model aiming to identify outliers during training and provide an OOD detector. The model is validated on diverse datasets outperforming strong baselines for both debiasing and OOD detection tasks. Strengths: 1. Innovative Approach: The paper introduces a novel probabilistic generative framework addressing critical but less explored issues of debiased learning and OOD detection in graph data. 2. Theoretical Justification: The paper provides theoretical analyses to support the effectiveness of GraphDE explaining how the model components mutually promote each other during training. 3. Experimental Evaluation: Extensive experiments demonstrate the superior performance of GraphDE over robust GNN baselines and other stateoftheart methods on various datasets for debiasing and OOD detection tasks. 4. Thorough Methodology: The paper thoroughly outlines the methodology including model instantiations learning objectives and inference processes enhancing the reproducibility and implementation clarity of the proposed approach. Weaknesses: 1. Limited Comparison: While the performance of GraphDE is compared against strong baselines and robust models a more comprehensive comparison with a broader range of existing methods for debiased learning and OOD detection could provide a clearer understanding of GraphDEs relative performance. 2. Additional Insights: While the theoretical analysis and experimental results provide convincing support for GraphDE more insights into the limitations or failure cases of the proposed approach would enhance the overall comprehensiveness of the paper. Questions  Suggestions for Authors: 1. Further Validation: How robust is GraphDE to noisy or adversarial outliers in realworld scenarios Further validation on complex and dynamic graph datasets with varied characteristics could strengthen the applicability of GraphDE. 2. Scalability: How does GraphDE handle scalability issues with largescale graph data sets Ensuring efficiency and scalability are crucial for practical deployment in realworld applications. 3. Interpretability: Could the authors provide insights into model interpretability and how the decisions of GraphDE can be explained to users or stakeholders in reallife applications Clear interpretability enhances trust and adoption of such models. Limitations: The authors have sufficiently addressed the limitations and potential negative societal impacts of their work by focusing on technical aspects and performance evaluation. However providing a discussion on ethical considerations and societal implications explicitly would enhance the depth of the paper. Constructive Suggestions for Improvement:  Enhance the discussion on the limitations and societal implications of the proposed approach.  Include a broader comparison with existing methods and explore the generalizability of GraphDE across various graph data domains.  Consider incorporating explanations or interpretability features in the model for better understanding by endusers or stakeholders.", "vExdPu73R2z": " PeerReview of \"Robust Referring Video Object Segmentation with Semantic Consensus Discrimination\"  Summary: The paper introduces a novel task called Robust Referring Video Object Segmentation (R2VOS) that addresses the semantic consensus issue in Referring Video Object Segmentation (RVOS). The authors propose a method that leverages relational cycle consistency constraints to enhance semantic alignment between visual and textual modalities. Extensive experiments on RYoutubeVOS RefDAVIS17 and the novel R2YoutubeVOS dataset show superior performance over existing methods.  Strengths: 1. Innovative Task Definition: Introducing the R2VOS task without the semantic consensus assumption is a significant contribution. 2. Methodology: The proposed approach that utilizes relational cycle consistency and early grounding medium is novel and effective. 3. Performance: Achieving stateoftheart results on multiple datasets demonstrates the effectiveness of the proposed method. 4. Experimental Rigor: Detailed comparisons quantitative results and ablation studies provide a comprehensive evaluation.  Weaknesses: 1. Complexity: The method appears complex which might hinder broader adoption and practical implementation. 2. Generalization: The paper mainly focuses on the proposed task and method without much discussion on generalization to other related tasks or domains.  Questions and Suggestions for Authors: 1. Clarifications: Provide more insights into how the proposed method overcomes semantic misalignment and false alarm issues in unpaired inputs. 2. Implementation Clarifications: Include more information on the specific hyperparameters chosen dataset preparation details and any implementation challenges faced. 3. Future Work: Discuss potential avenues for future research such as exploring the applicability of the proposed approach in related tasks beyond RVOS.  Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work by discussing challenges like false positives due to occlusion and providing an ablation study on the effectiveness of different components in their method. However they could enhance this section by suggesting remedies or workarounds for the identified limitations. Overall the paper presents a significant contribution to the field of referring video object segmentation through the introduction of the R2VOS task and the proposed method. The authors have conducted thorough experiments and analysis to validate the effectiveness of their approach. Minor enhancements in addressing the complexity of the method and including more implementation details could further strengthen the paper.", "uKYvlNgahrz": " Review of \"Building Monotonic Neural Networks with NonSaturated Activation Functions\"  1. Summary: The paper presents a method for constructing monotonic deep neural networks using nonsaturated activation functions. It addresses the limitation of traditional constructions by proposing a modification using a combination of concave and convex monotone activation functions. The method is found to outperform stateoftheart techniques in approximating monotone functions while maintaining simplicity and efficiency.  2. Assessment of Strengths and Weaknesses: Strengths:  The paper addresses an important practical need for monotonic neural networks.  The proposed method is simple efficient and does not require additional modifications in the learning process.  Experimental results show superior accuracy compared to existing stateoftheart methods.  The approach is wellexplained and supported by theoretical proofs. Weaknesses:  The paper lacks a detailed discussion on the interpretability of the proposed method.  More insights could be provided on the scalability of the proposed technique for larger datasets.  It would be beneficial to explore the impact of the proposed modification on model generalization.  3. Questions and Suggestions for Authors:  Could you elaborate on the applicability of the proposed method to realworld scenarios with large datasets  How does the proposed modification affect model interpretability and explainability considering the importance of these aspects in critical domains such as finance and healthcare  Have you considered conducting experiments on datasets with varying levels of noise to evaluate the robustness of the method  It would be beneficial to include a discussion on the computational efficiency of the proposed technique and its scalability to larger models.  4. Limitations: The authors could further discuss the interpretability of the proposed method and the scalability of the technique to larger datasets. It is recommended to address these limitations to enhance the practical applicability of the proposed approach.  Checklist:  Main claims in the abstract and introduction align with the papers contributions and scope [Yes]  Description of limitations of the work is included [Yes]  Discussion on potential negative societal impacts is not provided [NA]  Ethical guidelines are followed [Yes]  Assumptions and proofs of theoretical results are stated and included [Yes]  Code data and experimental details necessary for reproducibility are provided [Yes]  Error bars are reported [Yes]  Total compute resources are detailed [Yes]  Existing assets are cited and the license is not mentioned [No]  New assets including code are included in the supplementary material [Yes]  Full dataset descriptions for comparison are provided [Yes]  Additional Proofs and Experiment Details: The provided detailed proofs are comprehensive and help in understanding the theoretical foundations of the proposed approach. The additional experiments shed light on the methods robustness and performance in noisy and small datasets enhancing the papers credibility. Overall the paper presents a valuable contribution to the field of monotonic neural networks but improvements in interpretability and scalability discussions would further strengthen the work.", "qPb0m0NXt4j": " Summary: The paper introduces a novel approach to model and simulate the evolution process of graphical conventions in visual communication games. It focuses on emergent communication through sketching exploring the tradeoff between iconicity and symbolicity. The research presents a learning framework for neural agents defining and evaluating properties of the emergent conventions. Experimental results demonstrate the emergence of conventions consistent with human graphical communication studies.  Strengths: 1. Original Contribution: The research presents a unique approach to model graphical conventions in visual communication distinguishing itself from prior work. 2. Innovative Framework: Introducing a learning framework for neural agents in visual communication games offers a novel perspective on emergent communication. 3. Thorough Evaluation: Defining and evaluating emergent conventions based on iconicity symbolicity and semanticity provides a robust analysis of the proposed framework.  Weaknesses: 1. Limited Generalization: The paper could benefit from discussing the potential limitations in generalizing the findings beyond the experimental setup. 2. Assumptions and Constraints: Addressing the assumptions and constraints of the model and simulation could enhance the papers comprehensiveness. 3. Impact Clarification: Further elucidation on the practical implications or applications of the research findings could enrich the discussion.  Questions and Suggestions for Authors: 1. Can you elaborate on the implications of the research findings for realworld applications or future studies in emergent communication systems 2. How do you acknowledge and address the limitations of the pretrained sketching module and its impact on the discriminative nature among selected classes 3. Could you provide insights on how the findings of the paper contribute to advancing the understanding of the evolutionary processes of human graphical communication systems  Limitations Addressed: While the paper offers a comprehensive analysis of the emergent graphical conventions in neural agents it could further enhance the discussion by acknowledging and addressing limitations in generalizability assumptions and practical implications. By robustly considering these aspects the authors can provide a more wellrounded evaluation of the societal impact and the applicability of the research findings.  Suggestions for Improvement: 1. Provide a more extensive discussion on the limitations and assumptions of the model to enhance the papers credibility. 2. Offer practical examples or scenarios where the proposed framework could be beneficial to showcase the practical relevance of the research findings. 3. Consider integrating insights from related fields to enrich the discussion on emergent communication systems and their significance in cognitive science and AI research.", "wXNPMS11aUb": "Summary: The paper introduces a bioinspired developmental algorithm DevFly to improve performance in localitysensitive hashing by developing binary connections between input and coding layers. The study explores the effectiveness of this process in improving the accuracy of nearest neighbor search tasks demonstrating enhanced performance compared to the traditional FlyLSH method. Experimental results on MNIST CIFAR10 SIFT10M and GloVe datasets showcase the efficiency of DevFly in enhancing search accuracy and query time. Strengths: 1. Innovative Approach: Introducing a bioinspired developmental algorithm to enhance hash performance is a novel and intriguing concept. 2. Experimental Validation: Extensive experiments on various datasets provide a thorough analysis showcasing the efficacy of DevFly compared to traditional methods. 3. Efficiency: The proposed method shows competitive performance with faster query times making it suitable for large datasets and realtime applications. Weaknesses: 1. Limited Baseline Comparison: The paper lacks a comprehensive comparison with stateoftheart methods making it challenging to assess the true effectiveness of DevFly. 2. Dataset Dependency: Performance variation across datasets raises concerns about the methods generalizability and adaptability to diverse data structures. 3. Biological Plausibility: While inspired by biological neural circuits the study does not delve deeply into the biological underpinnings leaving room for further exploration. Questions  Suggestions: 1. Generalization Concerns: How does DevFly perform on highly complex datasets with diverse feature distributions 2. Computational Efficiency: Could the authors provide insights into the computational complexity of DevFly especially in comparison to existing methods 3. Biological Fidelity: How closely does DevFly mimic real neurobiological developmental processes and are there potential enhancements to increase biological plausibility Limitations: The authors should further address the generalizability of DevFly across diverse datasets and provide insights into scaling the method for complex highdimensional data spaces. Overall the paper presents an innovative approach with promising results but further elaboration on generalizability comparative analysis and biological relevance would enhance the impact and applicability of the proposed DevFly algorithm.", "x0LCDsbJ5JF": " Review of \"SpatiallyAdaptive SqueezeExcitation Networks for Image Synthesis and Recognition\"  Summary: This paper introduces SpatiallyAdaptive SqueezeExcitation (SASE) networks for image synthesis and recognition tasks. The proposed SASE module aims to reinforce dataspecificity by learning both channelwise and spatial attention leading to better performance in lowshot and oneshot learning scenarios for image synthesis tasks. Additionally in image recognition tasks SASE is used to replace convolution layers in ResNets achieving improved accuracy on the ImageNet1000 dataset.  Strengths: 1. Innovative Approach: The paper introduces a novel SASE module that enhances the dataspecificity of neural networks contributing towards improved performance in both image synthesis and recognition tasks. 2. Comprehensive Experiments: The paper provides detailed experiments across different tasks datasets and model configurations demonstrating the effectiveness of the proposed SASE module. 3. Clear Explanations: The paper effectively explains the design and implementation of the SASE module making it easy to understand for readers.  Weaknesses: 1. Evaluation Metrics: While the paper uses metrics like FID KID Density and Coverage for evaluation incorporating more widelyaccepted metrics would strengthen the evaluation and comparison with existing methods. 2. Overfitting: The paper should provide more analysis on the potential overfitting of the proposed SASE module especially in the context of object detection and instance segmentation tasks. 3. Computational Efficiency: Although the paper mentions the lightweight design of the SASE module a deeper analysis of the computational efficiency and scalability would be beneficial.  Questions and Suggestions for Authors: 1. Comparison with Transformer Models: How does the SASE module compare to the Transformer models in terms of computational efficiency and complexity 2. Transfer Learning: Have you explored the transfer learning capabilities of the SASE module especially in scenarios with limited labeled data 3. Generalization: How does the SASE module generalize to diverse datasets beyond those mentioned in the paper 4. Robustness: Have you analyzed the robustness of the SASE module to adversarial attacks or noisy inputs 5. Future Work: What are the potential research directions or extensions based on the outcomes of this study especially in the context of emerging trends like selfsupervised learning  Limitations: 1. Limited Negative Societal Impact Discussion: While the paper briefly mentions addressing negative societal impacts further elaboration on potential ethical considerations and biases in the proposed work could enhance the discussion.  Overall Assessment: The paper presents a novel approach with the SASE module for image synthesis and recognition tasks backed by comprehensive experiments. Strengthening the evaluation metrics analyzing computational efficiency and addressing potential overfitting issues could further enhance the papers impact and contribution to the research community.", "kUnHCGiILeU": " Paper Summary The paper proposes a novel framework called CageNeRF for deforming and animating neural radiance fields on arbitrary objects. It introduces a categoryagnostic cagebased representation as a deformation prior enabling the deformation and animation of general objects. The framework operates in a decoupled manner utilizing cage deformation while maintaining flexibility. Extensive experiments demonstrate the frameworks effectiveness in geometry editing object animation and deformation transfer.  Strengths 1. Innovative Approach: Introducing a cagebased deformation scheme for neural radiance fields is a novel and innovative concept. 2. Generalization: The categoryagnostic nature of the proposed framework allows for application to a wide range of objects. 3. Extensive Experiments: The paper provides comprehensive experiments showcasing the effectiveness of the framework in various tasks. 4. Quality Results: The qualitative and quantitative results demonstrate the capability of CageNeRF in deforming and animating implicit objects.  Weaknesses 1. Complexity: The paper introduces a complex framework that may require additional effort for implementation and understanding. 2. Comparative Analysis: More direct comparative analysis with existing methods would strengthen the validation of the proposed approach. 3. Limitations Acknowledgment: The paper briefly addresses limitations but could benefit from a more indepth discussion on potential constraints.  Questions and Suggestions 1. Clarity on Deformation Transfer: Can the authors provide more details on how deformation transfer is achieved within the framework 2. Evaluation Metrics: Could the authors explain the choice of evaluation metrics used in the experiments 3. Future Work: What are the potential directions for extending the framework to address more complex deformations or dynamic objects  Limitations  Suggestions The authors could further address the limitations by:  Discussing the tradeoffs between explicit and implicit representations in capturing complex deformations.  Offering suggestions on incorporating material color and lighting control for dynamic object rendering.  Conclusion In conclusion the paper presents a promising approach in deforming and animating neural radiance fields. By addressing the weaknesses and further exploring the frameworks limitations this work has the potential to make significant contributions to the field of 3D rendering and animation.", "zfQrX05HzBO": " Summary The paper addresses the challenge of continuous category discovery referred to as the Continuous Category Discovery (CCD) problem. It introduces a framework called Grow and Merge (GM) which alternates between a growing phase and a merging phase to effectively tackle the conflict between maintaining performance on known categories and discovering novel categories. The proposed GM framework achieves significant improvement over stateoftheart approaches in continuous category discovery.  Strengths 1. Innovation: Introducing the concept of Continuous Category Discovery (CCD) in contrast to conventional novel category discovery in static settings. 2. Framework Design: The GM framework is wellstructured with a clear approach of alternating between growing and merging phases to balance classification tasks and novel category discovery effectively. 3. Experimental Validation: Extensive experiments are conducted across various scenarios and datasets to demonstrate the superiority of the proposed GM framework over existing methods in terms of less forgetting and better category discovery accuracy.  Weaknesses 1. Technical Clarity: The paper includes intricate technical details that may be challenging for readers without a strong background in the field to comprehend fully. 2. Evaluation Metrics: While Mf and Md metrics are introduced more explanations may be needed to help readers understand their implications and significance. 3. Baseline Comparison: The paper lacks detailed comparisons with a wider range of baseline methods to provide a more comprehensive understanding of the effectiveness of the proposed GM framework.  Questions and Suggestions for Authors 1. Can you elaborate on the choice of specific hyperparameters (e.g. \\xcf\\xb5 \u03c4 \u03b1) in the proposed GM framework and discuss their impact on performance 2. How does the GM framework scale with the complexity of datasets (e.g. largerscale datasets like ImageNet) and can it handle scenarios with a larger number of classes or categories 3. Considering the sensitivity of hyperparameters in deep learning models have you conducted a sensitivity analysis to evaluate the robustness of the GM framework across various configurations  Limitations The paper provides detailed insights into addressing the CCD problem through the GM framework. However the authors could further enhance the paper by:  Explicitly discussing potential negative societal impacts of the proposed method such as data biases or unintended consequences.  Addressing limitations related to scalability and efficiency when applied to realworld datasets with high dimensionality and class diversity.  Constructive Suggestions for Improvement 1. Clarify and simplify technical descriptions making the paper more accessible to a broader audience. 2. Include a more indepth comparison with existing methods focusing on various performance metrics and scenarios for a comprehensive evaluation. 3. Discuss the practical implications and future research directions to highlight the significance and potential applications of the GM framework in realworld settings.", "pfI7u0eJAIr": " Summary: The paper introduces the concept of embeddings for numerical features in deep learning models for tabular data. It explores two novel approaches for constructing embeddings: piecewise linear encoding and periodic activation functions. The study shows that these embedding schemes can significantly enhance the performance of deep learning models bridging the performance gap with Gradient Boosted Decision Trees (GBDT) on GBDTfriendly benchmarks. The experiments demonstrate that proper embeddings for numerical features benefit various deep architectures not just Transformerbased ones.  Strengths: 1. Innovative Contribution: The paper addresses an underexplored research question in tabular deep learning and provides novel approaches to construct embeddings for numerical features. 2. Empirical Validation: The strengths of the proposed methods are backed by comprehensive empirical evaluations on public datasets demonstrating significant performance improvements over traditional approaches. 3. Comparison with GBDT: The study shows that with the proper embeddings deep learning models can compete effectively with GBDT on various benchmarks highlighting the potential of the proposed techniques.  Weaknesses: 1. Complexity: The paper introduces several new concepts and approaches which may make it challenging for readers to grasp all the details and implications without sufficient background knowledge. 2. Limited Discussion on Optimality: While the performance improvements are demonstrated the paper lacks an indepth discussion on the theoretical foundations and optimality of the proposed embedding schemes.  Questions and Suggestions: 1. Evaluation Metrics: Could you provide insights into the choice of evaluation metrics used in comparing the different embedding schemes and backbones How do these metrics capture the nuances of performance improvements in tabular deep learning 2. Scalability: Have you considered the scalability of the proposed embedding schemes to larger datasets and more complex tabular data problems How do the embeddings behave with increasing data dimensionality 3. Interpretability: In practical applications how interpretable are the learned embeddings for numerical features especially in scenarios where model explainability is crucial Are there methods to enhance the interpretability of the embeddings  Limitations: While the paper presents valuable insights and contributions limitations include the need for further discussion on model scaling interpretability and theoretical underpinnings of the proposed embedding schemes.  Suggestions for Improvement: 1. Increase clarity in explaining the theoretical foundations of the embedding schemes to provide a better understanding for readers. 2. Extend the discussion on the scalability and interpretability of the embeddings considering practical implications in realworld scenarios. 3. Consider incorporating additional experiments on larger datasets to evaluate the robustness and generalizability of the proposed embedding techniques. Overall the paper presents innovative methods for enhancing tabular deep learning models using embeddings for numerical features but addressing the above suggestions could further strengthen the research findings and implications.", "xl39QEYiB-j": " Summary: The paper proposes a novel approach for embodied sceneaware human pose estimation using a simulated agents proprioception and scene awareness. The method recovers global 3D human poses in a simulated environment by disentangling camera pose utilizing a multistep projection gradient and controlling a physically simulated agent equipped with environmental sensors. The proposed method achieves stateoftheart results on the challenging PROX dataset while remaining subrealtime causal and trained only on synthetic data.  Strengths: 1. Innovative Approach: The method introduces a unique approach by incorporating embodied sceneawareness for human pose estimation in a simulated environment. 2. Causal Inference: The method is causal allowing for realtime performance which is crucial in timecritical applications. 3. StateoftheArt Performance: Achieving highquality pose estimation on the challenging PROX dataset without using PROX motion sequences for training is impressive. 4. Detailed Explanations: The paper provides detailed explanations of the proposed method including the multistep projection gradient and the embodied human pose estimation process.  Weaknesses: 1. Limitation to Synthetic Data: The reliance solely on synthetic datasets may limit the generalization of the proposed method to realworld scenarios. 2. Societal Impact Consideration: The paper could further detail the potential implications and risks associated with the use of such technology in realworld applications. 3. Evaluation Metrics: While the paper discusses the metrics used for evaluation a comparative analysis with existing methods would enhance the assessment of the proposed technique. 4. Failure Cases Analysis: Providing a detailed analysis of failure cases and proposing solutions could strengthen the paper.  Questions and Suggestions for Authors: 1. Can you elaborate on how the proposed method can be applied or adapted to realworld scenarios beyond the simulated environment 2. Have you considered the computational complexity of your method and potential challenges in scaling it up for realtime applications 3. How does the proposed method handle variations in scene geometries and lighting conditions which are common in realworld settings  Limitations: The authors could enhance their analysis by considering:  Addressing the applicability of their method to realworld scenarios with diverse scene complexities.  Incorporating discussions on potential ethical considerations and societal implications arising from deploying such technology.  Suggestions for Improvement: 1. Realworld Data Integration: Consider incorporating realworld datasets to train and validate the method for improved generalization. 2. Robustness Analysis: Perform robustness testing on the proposed method to validate its performance under diverse environmental conditions. 3. Ethical Assessment: Include a dedicated section discussing the societal impacts and ethical considerations of deploying such technology in practical applications.", "xL7B5axplIe": " Peer Review 1) Summary: The paper introduces Conservative Dual Policy Optimization (CDPO) as a provably efficient ModelBased Reinforcement Learning (MBRL) algorithm. It addresses the limitations of optimistic algorithms like Posterior Sampling RL (PSRL) in complex environments with poor model generalizability. CDPO achieves global optimality and iterative policy improvement simultaneously by combining Referential Update and Conservative Update steps. The empirical results validate CDPOs exploration efficiency. 2) Strengths:  The paper provides a clear problem statement a detailed discussion of related work and a wellstructured algorithmic solution in CDPO.  The theoretical analysis is rigorous including the statistical equivalence with PSRL policy iterative improvement and proof of global optimality.  Empirical evaluation on various MuJoCo tasks demonstrates the superior performance of CDPO compared to both MBRL and modelfree baselines.  Ablation studies enhance the understanding of CDPO components and the impact of the constraint hyperparameter \\xce\\xb7. 3) Weaknesses:  The paper could benefit from a more detailed explanation of the complexities associated with nonlinear models and the reasoning behind certain assumptions like Lipschitz continuity.  The empirical evaluation could be extended to more diverse environments or stateoftheart benchmarks for a more comprehensive comparison.  The presentation of the paper could be improved by providing clearer figures or diagrams to aid in understanding the algorithm and results. 4) Questions and Suggestions:  Could the authors elaborate more on the selection criteria for the reference model in CDPO and its impact on algorithm performance  How scalable is CDPO to larger and more complex problems beyond those tested in the empirical study  What are the computational and memory requirements of CDPO compared to existing MBRL algorithms and are there any scalability issues in practical implementation 5) Limitations: The authors have adequately addressed the limitations and societal impact of their work. Constructive suggestions for improvement include providing a detailed discussion on the practical implications and ethical considerations of CDPOs efficient exploration mechanism in diverse realworld applications. Overall the paper presents a novel and promising MBRL algorithm in CDPO supported by strong theoretical analysis and empirical validation. Addressing the minor clarity and thoroughness issues highlighted could further strengthen the papers contribution in the field of reinforcement learning.", "ptUZl8xDMMN": "Summary: The paper presents a novel framework for designing and analyzing graph scattering networks extending beyond traditional graph wavelets. The framework includes variable branching ratios generic functional calculus filters and spectrumagnostic stability guarantees. The authors introduce new methods for graphlevel feature aggregation and extend scattering transforms to higherorder tensorial input. Theoretical results are validated through numerical experiments showcasing superior performance in social network graph classification and regression of quantumchemical energies on QM7 datasets. Strengths: 1. Innovative Framework: The paper introduces a novel and flexible framework for graph scattering networks addressing limitations in traditional graph wavelet approaches. 2. Theoretical Rigor: The authors provide indepth theoretical analysis and derive spectrumagnostic stability guarantees expanding the understanding and applicability of graph scattering networks. 3. Experimental Validation: Results from numerical experiments demonstrate the effectiveness of the proposed framework in graph classification and regression tasks outperforming traditional methods in certain scenarios. 4. Clear Communication: The paper is wellstructured with detailed explanations of concepts methods and results making it accessible to readers. Weaknesses: 1. Limitation in Network Topologies: The paper does not address the inability of scattering networks to emulate nontreestructured network topologies which could limit its applicability in realworld scenarios. 2. Generalizability: While the framework shows promise in the presented experiments the broader impact and generalizability across diverse datasets and applications need further exploration. 3. Bias Considerations: The paper should include a thorough discussion on potential biases inherent in the utilized datasets ensuring transparency and ethical considerations. Questions and Suggestions for Authors:  Can you elaborate on potential strategies or enhancements to address the limitation in emulating nontreestructured network topologies within scattering networks  How do you plan to validate the generalizability of the proposed framework across a wider range of datasets and realworld applications  Given the caution against overinterpretation of mathematical guarantees have you considered conducting a sensitivity or bias analysis to evaluate the impact of biases present in the datasets used for validation  Could you provide insights into how the proposed framework could be extended to address the interpretability and explainability aspects especially in complex regression tasks like quantumchemical energy predictions Limitations Addressed Adequately: The paper does address limitations and negative societal impacts by cautioning against overinterpretation of mathematical guarantees and the presence of biases in datasets. To further enhance this the authors could include a dedicated section discussing the ethical implications of biases potential unintended consequences and measures taken to mitigate them. Moreover providing transparency on dataset sources biases and ethical considerations would strengthen the paper.", "lXuZaxEaI7": " Peer Review  Summary: The paper presents a method to make policy optimization algorithms specifically Proximal Policy Optimization (PPO) batch sizeinvariant by decoupling the proximal policy from the behavior policy. The authors show how this decoupling improves the stability and efficiency of such algorithms especially in environments with stale data. Experimental evidence is provided to support the effectiveness of this approach.  Strengths:  The paper addresses an important issue in batch size sensitivity in policy optimization algorithms.  The authors present a novel method to achieve batch sizeinvariance and provide theoretical insights into the workings of PPO.  Extensive experiments are conducted to validate the proposed approach and demonstrate its effectiveness across environments.  Clear and structured presentation with detailed explanations of concepts and methods.  Weaknesses:  While the paper introduces innovative modifications to improve policy optimization some aspects of the proposed approach such as the computation overhead of maintaining an exponentiallyweighted moving average (EWMA) could impact practical implementation.  The theoretical foundation and empirical results are wellexplained but a comparison with existing stateoftheart methods for addressing batch size sensitivity could provide additional context.  The discussion on the negative impacts of stale data and potential limitations of the proposed method could be further expanded.  Questions and Suggestions for Authors: 1. How does the proposed EWMA modification impact convergence speed and stability in training compared to traditional PPO 2. Have you explored the potential negative effects of decoupling the proximal policy from the behavior policy such as increased variance in policy updates 3. Could you further investigate the tradeoffs between achieving batch sizeinvariance and computational overhead particularly in resourceconstrained settings 4. Considering the practical implications of stale data how generalizable is the proposed method to different reinforcement learning tasks with varying data characteristics  Limitations: The authors have adequately discussed the limitations and potential negative societal impact of their work. However to enhance the papers impact it would be beneficial to include discussions on the resource implications of implementing the proposed method in realworld applications and potential challenges in scaling the approach to more complex environments.  Suggestions for Improvement:  Provide a more detailed discussion on the practical implications and scalability of the proposed method.  Include a comparative analysis with existing batch sizeinvariance approaches in policy optimization to highlight the unique contributions of the proposed method. Overall the paper presents a promising approach to addressing batch size sensitivity in policy optimization and provides valuable insights into improving the robustness and efficiency of reinforcement learning algorithms. Further experimental validations and practical considerations could strengthen the impact of the proposed method.", "wlEOsQ917F": "1) Summary: The paper introduces a novel framework for stochastic bilevel optimization addressing the challenge of minimizing a value function involving the argminimum of another function. The authors propose two algorithms  SOBA and SABA  to solve bilevel optimization problems efficiently. SOBA is an extension of SGD while SABA is an extension of the variance reduction algorithm SAGA. The methods are supported by theoretical convergence analyses and benchmarked on hyperparameters selection and data hypercleaning tasks. 2) Strengths and Weaknesses: Strengths:  Innovative Framework: The proposed joint evolution of inner outer variables and linear system solutions is innovative.  Theoretical Analysis: The theoretical analyses for SOBA and SABA demonstrate strong convergence properties matching or exceeding existing methods.  Benchmarking: Extensive benchmarking on hyperparameter selection and data hypercleaning tasks strengthens the empirical validation. Weaknesses:  Complexity: The method involves complex computations such as resolving linear systems which can hinder practical implementation.  Assumption Dependency: The method relies on assumptions such as strong convexity of G which may not be applicable in all scenarios. 3) Questions and Suggestions:  Clarification: Could the authors provide more insights into the practicality and efficiency of the proposed framework in realworld scenarios with largescale datasets and complex optimization landscapes  Robustness: How robust are the proposed algorithms to variations in hyperparameters and data characteristics  Generalizability: Can the framework be extended to more diverse optimization problems beyond the ones considered in the paper 4) Limitations: The authors should address potential limitations in the practical implementation of their methods such as computational complexity scalability to large datasets and potential issues in dealing with nonconvex optimization landscapes. Suggestions for improvement could include providing computational efficiency analyses and scalability tests on larger datasets. This peer review acknowledges the paper\u2019s innovative contributions strong theoretical foundations and empirical validations while urging the authors to address implementation challenges robustness concerns and potential extensions to broaden the impact of their work.", "vgIz0emVTAd": " Summary: The paper introduces a novel adversarial defense method DISCO based on local implicit functions aiming to remove adversarial perturbations from images by projecting them back into the natural image manifold using deep features. Extensive experiments demonstrate DISCOs superiority over prior defenses showcasing robustness efficiency and transferability across datasets classifiers and attacks.  Strengths: 1. Innovative Approach: Proposes a novel defense strategy based on local implicit functions showcasing efficiency and robustness in defending against adversarial attacks. 2. Extensive Experimentation: Conducted thorough experiments demonstrating DISCOs superior performance in comparison to existing defenses across various datasets and attack scenarios. 3. Parameter Efficiency: Shows that DISCO has a lightweight design with significantly fewer parameters making it computationally efficient. 4. Defense Transferability: Demonstrates strong transferability of the defense across different attacks classifiers and datasets showcasing its flexibility.  Weaknesses: 1. Limited Evaluation Scenarios: While the paper covers a comprehensive set of experiments there is a need to evaluate the defense against a wider range of attack types to ensure its robustness. 2. Societal Impact Consideration: The discussion on potential negative societal impacts is brief and may benefit from a more indepth analysis and suggestions on how to mitigate these risks.  Questions and Suggestions for Authors: 1. Further Evaluation: Have you considered testing DISCO against a wider variety of attacks including more sophisticated ones like patch attacks or functional adversarial attacks 2. Generalization and Robustness: How does DISCO perform in scenarios where the nature of attacks changes such as unseen types of adversarial perturbations or novel attack strategies 3. Computational Efficiency: Can you provide additional insights into the computational overhead of implementing DISCO and how it scales with larger datasets or more complex models 4. Explanation Clarity: Are there plans to provide a clearer explanation of how DISCO addresses the limitations of traditional adversarial defenses in future revisions of the paper  Limitations Impact: The authors should further investigate the robustness of DISCO against diverse attack types beyond normbounded attacks to enhance the defenses applicability in realworld scenarios. Additionally elaborating more on potential societal impacts and suggesting proactive measures for mitigation would enhance the papers ethical considerations and overall quality.", "mjVZw5ADSbX": "Summary: The paper proposes a new framework CONT for contrastive neural text generation to address the limitations of previous contrastive learning methods in text generation tasks. CONT leverages contrastive examples from predictions utilizes an Npairs contrastive loss and incorporates learned similarity scores into the inference stage. Experimental results across various generation tasks demonstrate that CONT outperforms conventional training frameworks and previous contrastive methods achieving new stateoftheart results. Strengths: 1. Innovative Framework: The paper introduces a novel approach to contrastive learning in text generation tasks addressing key challenges and achieving superior performance. 2. Comprehensive Experiments: Extensive experiments on five generation tasks with ten different benchmarks demonstrate the effectiveness of CONT over existing methods. 3. Detailed Methodology: The paper provides a clear and detailed description of the proposed framework highlighting each key component and its benefits. 4. StateoftheArt Performance: CONT achieves new stateoftheart results on summarization code comment generation and datatotext generation showcasing its potential in improving various text generation tasks. Weaknesses: 1. Training Efficiency Issue: The paper mentions a potential limitation related to the training efficiency of CONT requiring 2 to 4 times more training time compared to MLEbased models. Efficient training strategies could be explored to mitigate this limitation. 2. Limited Discussion on Negative Aspects: The paper does not delve into potential negative societal impacts of their work which could be considered to ensure ethical implications are addressed. 3. Incomplete Addressing of Limitations: While the CONT framework enhances contrastive learning in text generation tasks further elaboration on potential limitations and their mitigation strategies could add depth to the discussion. Questions and Suggestions: 1. How does the proposed framework compare to other postgeneration reranking methods in terms of accuracy and efficiency 2. Can the authors elaborate on any potential biases introduced by the training data or model architecture 3. Further insights into the scalability and generalizability of CONT to different languages or domains could strengthen the impact of the paper. 4. Exploring interpretability aspects of the learned representations and how they aid in generating more coherent and humanlike text could be a valuable addition to the discussion. Limitations: The authors should provide a more thorough discussion on the potential limitations and societal impacts of their work. Suggestions for improvement could include conducting a comprehensive analysis to address potential biases and negative impacts along with proposing proactive solutions to mitigate such effects.", "wwWCZ7sER_C": " Summary: The paper explores algorithms with multiple predictors and their application in fundamental optimization problems such as matching load balancing and nonclairvoyant scheduling. The study introduces new algorithms taking advantage of multiple predictors and proves bounds on their performance. The presented results are primarily theoretical with some preliminary empirical validation for mincost perfect matching.  Strengths: 1. Novel Contribution: The paper addresses an important gap in the existing literature by exploring algorithms with multiple machinelearned predictions rather than just one. 2. Theoretical Depth: The study provides detailed theoretical analyses and bounds for the performance of the proposed algorithms enhancing the understanding of utilizing multiple predictors in optimization problems. 3. Technical Rigor: The paper presents a systematic and detailed approach in designing and analyzing algorithms for multiple predictors in various optimization problems. 4. Empirical Validation: The inclusion of preliminary empirical validation for mincost perfect matching adds practical relevance to the theoretical findings.  Weaknesses: 1. Complexity: The technical complexity and depth of the analysis might make it challenging for readers without a strong background in optimization theory to fully comprehend the paper. 2. Limited Empirical Results: While there is some empirical validation included a more extensive validation across all proposed algorithms and scenarios would strengthen the practical implications of the work. 3. Clarity: Some sections especially the algorithm descriptions and proofs could benefit from clearer explanations or examples to aid understanding.  Questions and Suggestions for Authors: 1. Algorithm Efficiency: Can you provide insights into the computational complexity of the proposed algorithms especially for scenarios with a large number of predictors 2. Robustness to Noise: How do the algorithms perform in the presence of noise or inaccuracies in the predictions Have you considered mechanisms to enhance robustness 3. Generalizability: Do the findings and algorithms presented in the paper have applicability beyond the specific optimization problems studied How transferable are the solutions to other domains or scenarios  Limitations:  Addressing Limitations: The paper could benefit from a more explicit discussion on potential limitations of the proposed algorithms and the negative societal impacts of using multiple predictors. Providing an ethical perspective on the application of these algorithms would enhance the completeness of the work. Overall the paper presents a valuable contribution to the field of algorithms with predictions by exploring the utilization of multiple predictors in optimization problems. Strengthening the empirical validation and enhancing clarity in certain sections would further enhance the impact and accessibility of the research.", "zzDrPqn57DL": "PeerReview: Summary: The paper proposes a novel LiDARcamera fusion framework named BEVFusion that disentangles the dependency on LiDAR data for predicting the 3D bounding boxes of objects. This new framework demonstrates superior performance compared to stateoftheart methods especially under robust training settings simulating LiDAR malfunctions. Extensive experiments on the nuScenes dataset validate the generalization ability and robustness of BEVFusion. Strengths:  Novel Framework: BEVFusion proposes a unique approach to disentangle the LiDARcamera fusion dependency thus addressing the limitations of existing methods when LiDAR data is unavailable.  Empirical Results: The paper provides extensive experiments demonstrating the superiority of BEVFusion over stateoftheart methods both in normal training settings and under robustness training settings.  Generalization Ability: The framework shows strong generalization ability and outperforms multiple LiDARonly detectors showcasing its versatility.  Robustness Analysis: The robustness of BEVFusion against LiDAR and camera malfunctions is thoroughly analyzed providing valuable insights for realworld deployment. Weaknesses:  Lack of Theoretical Justification: The paper lacks indepth theoretical analysis to support the effectiveness of the proposed framework. Providing theoretical insights or proof of concept would enhance the robustness of the proposed method.  Limited Dataset Variety: The evaluation is primarily conducted on the nuScenes dataset and expanding the experiments to other datasets could provide a more comprehensive evaluation of the proposed frameworks effectiveness in different scenarios.  Lack of Realworld Deployment Analysis: The paper does not delve into the practical deployment aspects of BEVFusion in realistic autonomous driving systems. Addressing the practical implications and potential limitations in deployment would strengthen the impact of the research. Questions and Suggestions for Authors: 1. Can you provide more insights into the theoretical foundations supporting the disentanglement of the LiDARcamera fusion dependency in BEVFusion 2. Have you considered expanding your experiments to other datasets to evaluate the generalization ability across diverse scenarios 3. How do you foresee the realworld deployment of BEVFusion in autonomous driving systems considering potential challenges and limitations that might arise Limitations: The authors should consider:  Providing a more comprehensive theoretical foundation to strengthen the robustness of the proposed framework.  Expanding the evaluation to multiple datasets to ensure the generalization ability across different scenarios.  Analyzing the practical implications and potential deployment challenges of BEVFusion for autonomous driving systems.", "lYHUY4H7fs": "Summary: The paper explores the concept of envyfree policy teaching in a multiagent environment where a teacher aims to modify agents reward functions to incentivize learning a target policy while ensuring fairness. The study investigates the existence of envyfree (EF) solutions cost minimization and the price of fairness (PoF) in this context. Key findings include the existence of EF solutions under certain conditions efficient computation of costminimizing EF solutions and asymptotic bounds on PoF. Strengths: 1. Innovative Concept: The paper introduces a novel approach by applying envyfreeness to policy teaching contributing to the intersection of policy teaching and fair resource allocation. 2. Theoretical Rigor: The study provides a thorough theoretical analysis offering proofs theorems and clear formalizations of fairness notions. 3. Practical Relevance: The implications for multiagent incentivization and fair policy teaching can have practical applications in various domains. Weaknesses: 1. Applicability: The paper focuses heavily on theoretical frameworks and may benefit from more concrete examples or realworld applications to enhance understanding. 2. Complexity: The technical nature of the paper might pose a challenge for readers not deeply versed in the subject matter suggesting the need for clearer explanations or additional illustrative examples. 3. Limited Discussion: While the study addresses the existence of fair solutions and cost minimization a broader discussion on potential implications and practical implementation strategies would enhance the papers relevance. Questions and Suggestions: 1. Can the authors elaborate on the practical implications of their findings within specific realworld scenarios to provide more context for readers 2. How could the concepts of fairness and envyfreeness explored in this study be leveraged in practical settings such as education business or AI decisionmaking systems 3. Are there plans to conduct empirical evaluations or simulations to validate the theoretical results presented in the study and demonstrate the effectiveness of the proposed envyfree policy teaching approach Limitations: While the paper thoroughly explores envyfree policy teaching it could further address the potential negative societal impact of introducing fairness constraints in multiagent teaching environments. The authors might consider discussing the ethical implications and potential tradeoffs associated with incorporating fairness considerations in policy teaching. Constructive Suggestions for Improvement: 1. Consider including case studies or simulations to illustrate the practical implications and effectiveness of envyfree policy teaching in realworld scenarios. 2. Provide a more detailed discussion on the ethical considerations and societal impacts of incorporating fairness constraints in multiagent teaching. 3. Enhance the papers accessibility by simplifying complex concepts and technical language where feasible to cater to a wider audience.", "vRwCvlvd8eA": " PeerReview 1. Summary and Contributions: The paper introduces chefs random tables (CRTs) a novel class of nontrigonometric random features aiming to approximate Gaussian and softmax kernels. It presents various CRT variants with a focus on optimal positive random features (OPRFs) that offer unbiased softmaxkernel estimation with positive and bounded random features achieving significantly lower variance. The authors demonstrate the effectiveness of CRTs in a range of tasks especially in lowrank attention Transformers for text speech and image data. 2. Strengths and Weaknesses:  Strengths:  Novelty: The paper introduces innovative methods addressing the limitations of existing random feature approaches.  Theoretical Rigor: Theoretical results on variance reduction and convergence are provided enhancing the credibility of the proposed mechanisms.  Empirical Evaluation: The extensive evaluation on various tasks showcases the superior performance of CRTs over existing methods.  Weaknesses:  Practical Implementation: While the theoretical aspects are solid practical implementation details and computational efficiency may need further clarification.  Comparison: While the empirical results are promising a more detailed comparison with existing stateoftheart methods could add value. 3. Questions and Suggestions for Authors:  Address Practical Implementation: Can the authors provide insights into the computational complexity and memory requirements of the proposed CRTs compared to traditional methods  Compare with StateoftheArt: Have the authors considered comparing the performance of CRTs with the most recent stateoftheart methods in Transformer training for a comprehensive evaluation  Practical Applications: How do the authors envision the adoption of CRTs in realworld applications considering ease of implementation and scalability 4. Limitations: The authors have not clearly addressed the potential negative societal impacts of their work especially in light of concerns surrounding energy consumption and environmental impact associated with largescale model training. To improve the authors could incorporate a section discussing the environmental implications of their methods and suggest ways to optimize energy usage for largescale training. In conclusion the paper makes significant contributions to the field of random features for kernel approximation demonstrating promising results on various tasks. However addressing practical implementation aspects conducting further comparisons and considering societal impacts can enhance the overall impact of the research.", "lTZBRxm2q5": " Summary: The paper introduces a novel class of neural SDEs with generalized white noise and proposes an efficient approximation method for noise sample paths using classical integration methods and sparse Gaussian processes. The experimental results demonstrate the models effectiveness in capturing noise characteristics and improving model fittings over existing models particularly in the context of scorebased generative models.  Strengths: 1. Innovative Approach: Introducing a new class of neural SDEs with generalized white noise adds value to the field by addressing a significant gap in existing models. 2. Experimental Validation: Demonstrating the models effectiveness through empirical results on various datasets enhances the credibility of the proposed approach. 3. Theoretical Analysis: Providing theoretical foundations and convergence analyses contributes to the rigor and depth of the paper.  Weaknesses: 1. Complexity: The paper involves complex mathematical derivations and concepts which may pose a challenge for readers unfamiliar with stochastic differential equations and Gaussian processes. 2. Limited Explanation: Some parts of the methodology particularly related to sparse Gaussian processes and convergence analysis could benefit from a more detailed explanation for clarity. 3. Generalization: The limitations of the proposed model for cases with Hurst exponents less than 12 should be further addressed to enhance the models applicability.  Questions and Suggestions for Authors: 1. Clarity: Can the authors provide more intuitive explanations or visual aids to help readers better understand the intricate concepts involved especially in the white noise approximation and sparse Gaussian process components 2. Scalability: How scalable is the proposed method in terms of computational efficiency especially when handling large datasets or highdimensional problems 3. Realworld Applications: Have you considered applying the model to specific realworld problems or datasets to showcase its practical utility beyond the presented experiments  Limitations: The paper addresses the limitations of the proposed model particularly related to handling Hurst exponents less than 12. Further work on improving the methods performance for such cases could enhance the models robustness in various scenarios.  Suggestions for Improvement: 1. Provide more detailed explanations of key concepts for better accessibility. 2. Include visual aids or examples to illustrate complex methodologies. 3. Consider extending the experiments to diverse realworld applications for broader validation.", "unb1wyXf-aC": " Summary: The paper introduces a novel perceptual superresolution (PSR) method called concurrent super resolution and segmentation (CSRS) for 3D neuroimaging. The CSRS method aims to improve the resolution of both image intensity and associated segmentation labels concurrently making it applicable to existing labeled datasets. The study evaluates the effectiveness of CSRS in detecting brain changes due to neurodegenerative diseases by quantifying crosssectional and longitudinal changes across disorders exhibiting patterns of brain atrophy.  Strengths: 1. Innovative Methodology: Introducing a novel PSR method for 3D neuroimaging addressing a crucial need for higher resolution in clinical neuroimages. 2. RealWorld Evaluation: Conducting a comprehensive realworld evaluation of MRI PSR for the quantification of neurodegenerative diseases. 3. Comparison of Loss Functions: Evaluating different PSR loss functions and demonstrating the impact on detection power in natural history studies. 4. Transparent Methodology: Providing detailed information on the software platform data sources and evaluation metrics for reproducibility.  Weaknesses: 1. Lack of Benchmark Comparison: The paper lacks comparison with existing stateoftheart superresolution methods in the field of 3D neuroimaging. 2. Limited Explanation: Some technical details such as the training process of the models and loss functions could be explained in more depth for clarity. 3. Dataset Issues: Addressing concerns related to the lack of highresolution ground truth in clinical neuroimages could be further explored and discussed.  Questions and Suggestions for Authors: 1. Benchmarking: Have you considered benchmarking CSRS against other cuttingedge superresolution methods in the field for a more comprehensive evaluation 2. Model Interpretability: Can you provide insights into the interpretability of the CSRS models and how the feature maps are transformed in the upscaling process  Limitations: The authors have mentioned the limitations related to the lack of highresolution ground truth in clinical neuroimages. To improve they could enhance the discussion on potential negative societal impacts of their work and provide suggestions for addressing the limitations more explicitly. Overall the paper presents a significant advancement in the field of 3D neuroimaging with the introduction of CSRS but further validation and depth in certain technical aspects would strengthen its impact. The transparent methodology and realworld evaluation are commendable aspects of the study.", "wk5zDkuSHq": " Review of \"Online Agnostic Multiclass Boosting via Online Convex Optimization\" 1. Summary: The paper extends the concept of boosting to online agnostic multiclass classification through a reduction to online convex optimization. It introduces the first boosting algorithm for online agnostic multiclass classification and addresses the settings of statistical agnostic online realizable and statistical realizable multiclass boosting. The work showcases empirical results demonstrating the efficiency and competitiveness of the proposed online boosting algorithms. 2. Strengths:  Novel Contribution: The paper makes a significant contribution by extending boosting to online agnostic multiclass classification addressing a gap in the literature.  Theoretical Underpinning: The work is theoretically grounded with a clear reduction to online convex optimization providing a strong foundation for the proposed algorithms.  Algorithm Elegance: The presented boosting algorithm is intuitive and efficient facilitates easy implementation and shows competitive performance against stateoftheart algorithms. 3. Weaknesses:  Weak Learning Conditions: The paper introduces weak learning conditions for online agnostic boosting but the formal definitions for AWOL AWL and RWOL could be further clarified.  Comparative Analysis: While the empirical results are promising a more comprehensive comparison with existing methods especially in terms of accuracy and computational efficiency would enhance the evaluation. 4. Questions and Suggestions:  Clarification on Weak Learning Conditions: Can the authors provide more detailed insights into the practical implications of the introduced weak learning conditions especially in terms of scalability and performance tradeoffs  Evaluation Scope: Could the authors elaborate on the choice of datasets for experimentation and provide insights into the dataset characteristics that influence the algorithms performance  Further Comparative Analysis: It would be beneficial to include a comparison with additional existing boosting algorithms to provide a broader perspective on the proposed approachs effectiveness. 5. Limitations: The authors need to further discuss the scalability and generalizability of the proposed algorithms across diverse datasets. It would be beneficial to elaborate on any potential negative societal impacts of online agnostic multiclass boosting and suggest ways to mitigate them. Overall the paper presents a significant advancement in online agnostic multiclass boosting algorithms but some areas such as weak learning condition clarity and comparative analysis could be enhanced for a more comprehensive evaluation.", "voV_TRqcWh": " Review of \"Poisson Flow Generative Models\"  Summary: The paper introduces a novel generative model called Poisson Flow Generative Model (PFGM) that maps a uniform distribution on a highdimensional hemisphere into any data distribution. It leverages concepts from physics specifically the Poisson equation to model the generation process. Through empirical evaluations on CIFAR10 the model achieves stateoftheart performance compared to normalizing flow and scorebased models.  Strengths: 1. Innovative Approach: The use of the Poisson equation and the interpretation of data points as electrical charges demonstrate a novel and innovative approach to generative modeling. 2. Experimental Results: Achieving current stateoftheart performance on CIFAR10 dataset with high Inception and FID scores showcases the effectiveness of the proposed PFGM. 3. Efficiency: The model shows superior speed compared to existing SDE approaches offering 10\u00d7 to 20\u00d7 acceleration on image generation tasks without compromising quality. 4. Robustness: PFGM is demonstrated to be more tolerant of estimation errors on a weaker network architecture and robust to the step size in the Euler method showing high stability across varying conditions.  Weaknesses: 1. Complexity: The depth and complexity of the model involving multiple differential equations and additional dimensions may limit its widespread applicability and understanding. 2. Generalizability: The efficacy of PFGM on more diverse datasets beyond CIFAR10 is not extensively discussed limiting its generalizability and applicability to other domains.  Questions and Suggestions: 1. Interpretability: How can the authors enhance the interpretability of the model given its intricate physical basis and mathematical formulations 2. Extension to Other Domains: Have the authors considered applying PFGM to other domains besides image generation such as audio synthesis or text generation to showcase its versatility 3. Scalability: How does the model scale with increasing dataset sizes especially for highresolution images or complex data distributions  Limitations: The paper might benefit from further elaborating on the interpretability aspects of the model discussing its limitations in interpretive understanding and robustness in various scenarios. Additionally addressing the generalizability to other datasets and domains would enhance the impact of the work.  Improvement Suggestions: 1. Enhance clarity in explaining the theoretical aspects and implications of the Poisson Flow Generative Model. 2. Incorporate additional experiments on diverse datasets to validate the models performance across various domains. 3. Provide detailed discussions on the potential realworld applications and societal impacts of the proposed model to stimulate further research and adoption.", "sWNT5lT7l9G": " Summary: The paper introduces constrained Generalized Policy Improvement (GPI) as a testtime method to improve zeroshot transfer in reinforcement learning by constraining actionvalue approximation errors on new target tasks. The approach is based on bounding the optimal values for novel task vectors expressible as linear combinations of source task vectors.  Strengths: 1. Novel Contribution: The paper introduces a novel theorem on bounding optimal values for tasks expressed as linear combinations of source tasks extending previous work. 2. Thorough Experiments: The authors conducted experiments in Scavenger and DeepMind Lab environments demonstrating significant performance improvements over existing methods. 3. Clear Presentation: The paper provides detailed explanations of concepts theorems and methodologies making it accessible to readers.  Weaknesses: 1. Complexity: Theoretical derivations especially in Theorem 1 and the explanation of constrained GPI could be challenging for readers less familiar with the subject. 2. Evaluation Metrics: While the experimental results are promising a comparison with additional baselines or a sensitivity analysis to hyperparameters could strengthen the study.  Questions and Suggestions: 1. Clarification: Could the authors provide an intuitive example elucidating the practical implications of the lower and upper bound constraints derived from Theorem 1 2. Robustness: How sensitive is the constrained GPI approach to different target task distributions or source task configurations Have you tested under varying conditions to assess its robustness 3. Scalability: Is there potential for scalability issues when applying constrained GPI in more complex environments with larger state or action spaces  Limitations and Suggestions: The authors should consider discussing potential negative societal impacts of their work such as biases in reinforcement learning applications and provide strategies to mitigate these risks. By addressing these points the paper can enhance its clarity robustness and societal impact consideration. Great job on the detailed study and experimentation", "ok-SB1kz67Z": " Summary: The paper introduces the concept of introspection in neural networks with a twostage decisionmaking process: sensing patterns in data and reflecting on all possible choices. The reflection stage uses introspective questions to improve robustness and calibration of network predictions. The paper demonstrates improved performance in various downstream tasks and applications.  Strengths: 1. Innovative Concept: Introducing introspection in neural networks can enhance generalizability and calibration. 2. Theoretical Foundation: The differentiation between sensing and reflection stages is wellfounded in philosophy enhancing the papers academic contribution. 3. Experimental Validation: Extensive experiments demonstrate the effectiveness of introspective learning in recognition tasks and downstream applications. 4. Clear and Detailed Explanations: Detailed explanations and demonstrations of the methodology and results enhance understanding and reproducibility.  Weaknesses: 1. Complexity: The twostage architecture and theoretical framework may be challenging for nonexperts to grasp. 2. Limited Evaluation: While the paper illustrates the benefits of introspection a more comparative analysis with existing methods could provide better context. 3. Dataset Limitation: The discussion lacks an indepth evaluation of the datasets used and their specific characteristics.  Questions and Suggestions for Authors: 1. Scalability: How does the proposed method scale with larger datasets and more complex neural network architectures 2. External Factors: Have external factors like dataset biases or adversarial attacks been considered in the introspective decisionmaking process 3. Interpretability: How can the introspective process be made more interpretable to users and stakeholders in practical applications 4. Human Intervention: How can human intervention be effectively integrated into the introspective learning framework to address biases and enhance decisionmaking  Limitations: The authors have not fully addressed the potential negative societal impacts of their work. To mitigate this it is recommended to provide ethical considerations related to biases in introspective decisionmaking privacy concerns and algorithmic transparency. Suggestions include incorporating fairnessaware techniques and promoting transparency in introspective decisions.  Final Comments: Overall the paper presents a novel approach to neural network decisionmaking through introspection showing promising results in improving model robustness and calibration. Addressing the weaknesses and questions raised could enhance the papers impact and applicability in realworld scenarios.", "o4neHaKMlse": " Summary: The paper introduces FIBER a VisionLanguage (VL) model architecture that can handle both imagelevel and regionlevel understanding tasks efficiently by deeply integrating multimodal fusion into the model. FIBER utilizes a twostage pretraining strategy consisting of coarsegrained pretraining on imagetext data and finegrained pretraining on imagetextbox data. The model showcases improved performance across various VL tasks compared to strong baselines demonstrating the effectiveness of the proposed architecture and pretraining approach.  Strengths: 1. Innovative Model Architecture: FIBERs deep multimodal fusion in the backbone by inserting crossattention modules is a novel and effective approach to handle both imagelevel and regionlevel understanding tasks. 2. TwoStage PreTraining Strategy: The twostage pretraining pipeline efficiently leverages both coarse and finegrained data providing consistent performance improvements. 3. Comprehensive Experiments: The paper conducts thorough experiments across a wide range of VL tasks demonstrating superior performance over strong baselines. 4. Efficiency: FIBER shows better efficiency in training time on object detection tasks compared to existing methods.  Weaknesses: 1. Evaluation on LargerScale Datasets: The paper lacks evaluation on extremely largescale datasets which could provide insights into the scalability of the proposed method. 2. Societal Impact Consideration: Although briefly mentioned in the conclusion a more indepth analysis of potential societal biases in the pretraining data and mitigation strategies should be incorporated.  Questions and Suggestions for Authors: 1. Evaluation on Larger Datasets: Have you considered evaluating FIBER on datasets with significantly larger scales to further demonstrate scalability 2. Robustness Testing: How robust is FIBER to noisy or biased data and have you tested the models performance in such scenarios 3. Finetuning Strategy: Could you elaborate more on the finetuning process and its impact on the overall performance of FIBER 4. Comparison with StateoftheArt: Have you compared the efficiency and performance of FIBER with the very recent stateoftheart models in the field of VL pretraining  Limitations: The paper briefly acknowledges potential societal biases in the pretraining data but could benefit from a more detailed discussion on addressing and mitigating biases. Consider providing specific steps or approaches to better address societal impacts and environmental costs induced by the model.", "mamv07NQWk": " Summary: The paper addresses the theoretical understanding of upper and lower regret bounds for two prevalent multilabel classification (MLC) performance metrics namely Hamming loss and Precision\u03ba. It aims to provide insights into the statistical properties of MLC by analyzing nonparametric and parametric settings under different assumptions including label sparsity and margin conditions. The work builds on prior research and extends the analysis to provide novel regret bounds for MLC classifiers.  Assessment of Strengths and Weaknesses: Strengths: 1. Comprehensive Analysis: The paper provides a thorough examination of upper and lower regret bounds for popular MLC metrics which adds significant value to the theoretical understanding of MLC. 2. Innovative Contributions: By incorporating label sparsity and margin assumptions the paper extends the research landscape and introduces novel results for MLC settings. 3. Reference to Prior Work: By building on existing contributions and comparing with related studies the paper establishes a strong foundation and context for its analysis. Weaknesses: 1. High Complexity: The intricate theoretical framework and mathematical derivations may limit the accessibility and applicability for practitioners without a strong background in theoretical machine learning. 2. Assumption Dependency: The effectiveness of the presented regret bounds heavily relies on the fulfillment of specific assumptions which may not always hold true in realworld MLC scenarios. 3. Limited Practical Insights: While the paper enhances the theoretical insights into MLC it lacks practical applications or realworld case studies to demonstrate the utility of the proposed regret bounds.  Questions and Suggestions for Authors: 1. Clarification on Assumptions: Could the authors provide further insights into the practical implications or relevance of the assumptions made in the study especially regarding label sparsity and margin conditions 2. Practical Applicability: How do the theoretical findings and regret bounds proposed in the paper translate into actionable insights for practitioners working on MLC problems in realworld applications 3. Future Research Directions: Are there plans to validate the theoretical findings through empirical experiments or case studies to validate the effectiveness of the proposed regret bounds in practical MLC scenarios  Limitations: The authors could enhance the paper by incorporating practical case studies or empirical assessments to validate the theoretical findings and ensure the relevance and applicability of the proposed regret bounds in realworld MLC applications. Additionally addressing the robustness and generalizability of the assumptions made in the study could strengthen the practical utility of the research findings.", "rWgfLdqVVl_": " Review of \"Unsupervised TransformerBased Visual Concept Tokenization for Disentangled Representation Learning and Scene Decomposition\"  Summary: The paper proposes an unsupervised transformerbased Visual Concepts Tokenization (VCT) framework for learning visual concepts from images. VCT extracts visual concept tokens using crossattention and a Concept Disentangling Loss achieving stateoftheart results in disentangled representation learning and scene decomposition tasks. The paper demonstrates the effectiveness of VCT with extensive experiments on various datasets and aligns well with language representations using CLIP.  Strengths: 1. Original Contribution: The paper introduces a novel approach for visual concept extraction and achieves impressive results in disentangled representation learning and scene decomposition tasks. 2. Innovative Framework: The use of crossattention and the Concept Disentangling Loss to ensure disentanglement of visual concept tokens is a unique and effective strategy. 3. Comprehensive Experiments: The extensive experiments conducted on various datasets demonstrate the effectiveness and superiority of VCT over existing methods.  Weaknesses: 1. Clarity of Description: The paper could benefit from clearer explanations of certain technical details especially in the methodology section related to Concept Tokenizer and Detokenizer. 2. Comparison with Baselines: While the paper mentions comparisons with existing methods a more detailed comparison table or visual aids could enhance the readers understanding of VCTs performance relative to other approaches.  Questions and Suggestions: 1. Generalization: How does VCT perform on larger and more complex datasets Is there potential for scalability to handle realworld applications with diverse visual concepts 2. Interpretability: Can the authors provide more insights into the interpretability of the learned visual concepts using VCT How can users understand and trust the extracted visual concept tokens 3. Efficiency: What is the computational overhead of VCT compared to traditional methods for visual concept extraction Are there any proposed optimizations to enhance efficiency without compromising performance  Limitations and Suggestions: The authors should consider addressing the following aspects to enhance the paper:  Adequately discuss the potential negative societal impacts especially related to privacy concerns bias or misuse of the visual concept extraction technology.  Include recommendations on how to mitigate any negative societal impacts such as incorporating privacypreserving mechanisms bias detection tools or ethical guidelines in the deployment of VCT. Overall the paper presents a compelling framework for visual concept extraction but improvements in clarity comparisons and addressing potential societal impacts could further strengthen its contribution.", "lArVAWWpY3": " Peer Review:  1) Summary of the Paper and Contributions: The paper introduces the concept of Sliced Wasserstein distances as a scalable alternative to classic Wasserstein distances particularly in highdimensional spaces. The contributions are summarized as follows:  Empirical Convergence Rates: The paper establishes empirical convergence rates for the sliced distances with explicit dimension dependence providing precise understanding of the scaling behavior with respect to sample size dimensionality and distance order.  Robust Estimation: It characterizes minimax optimal robust estimation rates with improved dimensional dependence for sliced distances along with equivalence to robust mean estimation for W1 boosting statistical and algorithmic guarantees in the sliced setting.  Computational Guarantees: Formal guarantees are provided for methods to compute the average and maxsliced Wp resolving the lack of such guarantees in previous work.  2) Strengths and Weaknesses: Strengths:  Rigorous Analysis: The paper provides a thorough analysis with explicit dimensionwise dependencies enhancing the understanding of the scalability of sliced Wasserstein distances.  Comprehensive Coverage: It covers empirical rates robust estimation and computational aspects addressing multiple facets of the scalability question.  Validation: The theory is supported by numerical experiments strengthening the validity of the proposed methodology. Weaknesses:  Complexity: The paper involves complex mathematical derivations and formalisms which may pose challenges for readers not wellversed in the specific mathematical domain.  Technical Content: The technical content may limit the accessibility of the paper to a broader audience outside the specialized field of study.  3) Questions and Suggestions: Questions:  Could the paper elaborate on the practical implications of the empirical convergence rates and robust estimation guarantees How might these findings impact realworld applications utilizing sliced Wasserstein distances  The paper mentions the equivalence between robust estimation for mean and W1. Could this equivalence potentially lead to novel insights or applications in statistical inference or machine learning Suggestions:  Clarify the practical relevance of the computational guarantees provided. How might these guarantees aid in enhancing computational efficiency in applications utilizing sliced Wasserstein distances  Consider providing a brief overview or intuitive explanation of the significance of logconcavity assumptions for the theoretical results to aid readers with limited background in the domain.  4) Limitations: The limitations of the paper include:  Accessibility: The technical complexity of the paper may limit its accessibility to a broader readership. Simplification or additional explanatory notes could enhance understanding for nonexperts.  RealWorld Applications: The paper could further discuss the specific realworld applications or scenarios where the scalability and robustness of sliced Wasserstein distances would be particularly beneficial.  5) Suggestions for Improvement: To enhance the papers impact and accessibility the authors could consider the following improvements:  Practical Insights: Provide more concrete examples or case studies illustrating the practical utility of the proposed methodology in relevant applications.  Clarified Explanations: Offer intuitive explanations or visual aids to elucidate key concepts and results increasing the papers accessibility to a broader audience.  Discussion on Future Directions: Include a section discussing potential future research directions or extensions building upon the current findings enhancing the papers significance and relevance in the field.", "klElp42K9U0": " Summary: The paper introduces a novel Offline Reinforcement Learning (RL) pipeline SSRRRS for automated algorithmhyperparameter selection and policy deployment in small data regimes. It addresses the challenge of selecting the best policy from limited historical data by utilizing repeated random subsampling for algorithmhyperparameter training and evaluation. The proposed pipeline demonstrates substantial performance improvements across various domains in healthcare education and robotics compared to existing approaches.  Strengths: 1. Innovative Pipeline: The SSRRRS pipeline provides a systematic and effective solution for algorithmhyperparameter selection in offline RL. 2. Empirical Validation: The paper supports its claims with comprehensive experiments across diverse domains showcasing the superior performance of the proposed pipeline. 3. Theoretical Contributions: The paper offers theoretical insights into the challenges of offline RL proposing a robust technique to address data partitioning variance. 4. Clear Presentation: The paper is wellstructured clearly presenting the problem methodology experiments and results.  Weaknesses: 1. Limited Comparison: While the paper compares SSRRRS with some existing methods a more extensive comparison with a wider range of stateoftheart approaches could provide a more comprehensive evaluation. 2. Complexity: The pipeline may involve considerable computational complexity due to repeated subsampling which could limit its applicability in realtime decisionmaking scenarios. 3. OPE Method Selection: The reliance on specific OPE methods could introduce bias the paper would benefit from discussing the implications and potential limitations of this choice.  Questions and Suggestions: 1. Generalization: How does the SSRRRS pipeline generalize to other offline RL algorithms beyond those studied in the paper 2. Scalability: Have you considered the scalability of the proposed pipeline to larger datasets or more complex domains 3. Practical Implementation: How feasible is the implementation of the SSRRRS pipeline in realworld applications considering computational requirements 4. Additional Evaluation: Could the paper include sensitivity analysis on key hyperparameters or conduct more extensive experiments in challenging scenarios to further validate the pipelines robustness 5. Ethical Considerations: Have you discussed potential ethical implications or biases that could arise from automated algorithmhyperparameter selection in critical domains like healthcare and robotics  Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work. However to further enhance the paper they can provide additional insights on the ethical implications and consequences of automated policy selection in sensitive applications. Overall the paper makes a significant contribution to the field of offline RL and presents a promising approach for algorithmhyperparameter selection in limited data scenarios. Incorporating the above suggestions can further strengthen the research findings and implications.", "p0LJa6_XHM_": " Summary: The paper introduces a new hidden trigger backdoor attack called \"Sleeper Agent\" that is effective against neural networks trained from scratch addressing the challenge of crafting backdoor poisons that hide triggers and compromise deep models. The attack employs gradient matching data selection and target model retraining during the crafting process demonstrating its effectiveness on ImageNet and in blackbox settings.  Strengths: 1. Innovative Approach: The paper introduces a novel hidden trigger backdoor attack that is effective against models trained from scratch filling a critical gap in existing research. 2. Comprehensive Experiments: The authors conduct empirical tests on multiple datasets including CIFAR10 and ImageNet demonstrating the effectiveness of Sleeper Agent in both graybox and blackbox settings. 3. Robustness: The attack is shown to be robust against various defenses showcasing its resilience in challenging scenarios. 4. Detailed Methodology: The paper provides a detailed methodology including the poison crafting procedure and optimization techniques contributing to the understanding of the attack.  Weaknesses: 1. Lack of Ethical Consideration: The potential societal impact of the attack particularly in securitycritical applications needs to be further elaborated. 2. Variance in Success Rates: The high variance in the attack success rates should be addressed to enhance the reliability and consistency of the method.  Questions and Suggestions for the Authors: 1. Generalization: How does the Sleeper Agent attack perform on datasets beyond CIFAR10 and ImageNet Can the attack be extended to more diverse datasets 2. Scalability: Are there scalability concerns when applying the attack to larger models or datasets How does the attack perform on models with different architectures or complexities 3. Defense Mechanisms: Have you considered potential countermeasures or defenses against the Sleeper Agent attack How could the attack be mitigated or detected in practical settings  Limitations: The authors should provide a more comprehensive discussion on the potential negative societal impact of the Sleeper Agent attack particularly in securitysensitive applications. Constructive suggestions for mitigating such impacts could include proposing ethical guidelines for responsible use of backdoor attacks and advocating for transparency in model training and validation processes.", "sFapsu4hYo": " Peer Review  1) Summary: The paper introduces a new dynamic sparse training algorithm that focuses on sparse models with shuffled block structures. The proposed algorithm adapts the sparse model by dropping small weights and adding new parameters based on input value and output gradient at each layer. The experiments demonstrate that the algorithm achieves high accuracy on various networks and datasets showcasing the effectiveness of sparse models with finegrained block structures.  2) Strengths and Weaknesses: Strengths:  Novel Contribution: Introducing dynamic sparse training for models with shuffled block structures is a novel approach that hasnt been explored extensively in previous literature.  Experimental Validation: The experiments on different networks and datasets provide strong evidence of the effectiveness of the proposed algorithm in achieving high accuracy with reduced training time.  Efficiency Focus: The emphasis on hardwarefriendly finegrained structured models that accelerate both forward and backward pass is a critical strength. Weaknesses:  Relatively Limited Comparison: While the comparison with existing methods is informative a broader comparison with more stateoftheart techniques could enhance the papers impact.  Complexity: The blockaware drop criteria and grow criterion may be challenging to implement and understand for a general audience potentially limiting the algorithm\u2019s adoption.  Societal Impact Discussion: The paper lacks a comprehensive consideration of potential societal impacts or ethical considerations stemming from the proposed algorithm.  3) Questions and Suggestions for Authors:  Questions:  How does the proposed algorithm compare with leading sparse training techniques in terms of convergence speed and computational efficiency  Have you considered the impact of hyperparameter choices on the algorithms performance and stability  Suggestions:  Provide a detailed explanation or visualization to help readers understand the intricate aspects of the blockaware drop criterion and grow criterion.  Consider elaborating on the practical implications and realworld applications of the proposed sparse training algorithm.  4) Limitations: The authors have not extensively addressed the potential societal impact or ethical considerations of their work. Including a dedicated section to discuss these aspects and suggesting ways to mitigate any negative impacts can enhance the paper\u2019s transparency and credibility.  Overall the paper presents a promising approach to dynamic sparse training using shuffled block structures. However addressing the mentioned weaknesses and incorporating the suggested enhancements can further strengthen the papers quality and impact.", "nrksGSRT7kX": " Summary: The paper introduces a novel modelbased offline RL algorithm called Robust Adversarial ModelBased Offline RL (RAMBO). RAMBO formulates the problem as a twoplayer zerosum game against an adversarial environment model training the model to minimize the value function while accurately predicting transitions in the dataset. By alternate optimization of the policy and adversarial model RAMBO enforces conservatism to outperform existing stateoftheart baselines on offline RL benchmarks.  Strengths: 1. Novel Algorithm: RAMBO introduces a new approach to modelbased offline RL addressing the problem through a theoretically grounded and adversarial dynamics model. 2. Theoretical Grounding: The paper provides a strong theoretical foundation with probable approximately correct (PAC) performance guarantee and a lower bound pessimistic value function for the true environment. 3. Empirical Results: Demonstrates superior performance over existing stateoftheart algorithms on widely studied offline RL benchmarks. 4. Comparison and Ablation: Conducts thorough comparisons with existing approaches as well as ablation studies to test the significance of adversarial training.  Weaknesses: 1. Complexity: The algorithm complexity might hinder its practical deployment in realworld scenarios where simplicity and efficiency are important factors. 2. Offline Tuning: While successful offline hyperparameter selection is demonstrated offline tuning may still pose challenges in complex realworld applications due to manual intervention. 3. AntMaze Results: Modelbased algorithms including RAMBO exhibit lower performance in AntMaze domains suggesting potential limitations in certain environments.  Questions and Suggestions for Authors: 1. How does RAMBO scale with larger datasets or more complex environments Have scalability considerations been addressed 2. Can the authors provide insights into why RAMBO performs less effectively on highquality datasets like MediumExpert in comparison to other algorithms 3. It would be beneficial to discuss potential strategies for reducing complexity and enhancing the algorithms applicability in diverse realworld scenarios.  Limitations: The authors have adequately addressed limitations and potential negative societal impacts by providing a detailed analysis of the algorithms performance across various benchmarks. A constructive suggestion would be to explore strategies to enhance robustness and scalability for application in broader and more complex domains.", "rOimdw0-sx9": " Review of \"Adaptive Distributionally Robust Meta Reinforcement Learning\"  Summary: The paper introduces a novel framework called DiAMetR for meta reinforcement learning that is resilient to testtime distribution shifts. The framework leverages distributional robustness to train a population of metapolicies for different levels of distribution shifts and employs a bandit algorithm for adaptive metapolicy selection at test time. The paper highlights the importance of adapting to distribution shifts and shows promising results on simulated robotics tasks.  Strengths: 1. Innovative Framework: DiAMetR introduces a unique approach to address distribution shifts in meta reinforcement learning which is supported by theoretical insights and empirical results. 2. Clear Exposition: The paper provides a detailed and structured explanation of the proposed framework making it easy to understand the methodology. 3. Experimental Evaluation: The empirical evaluation is extensive and demonstrates the effectiveness of DiAMetR in handling varying levels of distribution shift in metaRL tasks. 4. Theoretical Analysis: The theoretical analysis of the algorithm provides valuable insights into the properties of adaptive distributional robustness enriching the understanding of the approach.  Weaknesses: 1. Complexity: The framework involves multiple components such as training a population of metapolicies and testtime selection which may make it computationally intensive and difficult to implement in practice. 2. Lack of Realworld Evaluation: While the simulated robotics tasks provide a foundation for the framework realworld application and validation could strengthen the practical relevance of DiAMetR. 3. Limited Comparison: The comparison with existing metaRL algorithms is mainly focused on adaptation to distribution shift and a broader comparison in various scenarios could enhance the paper\u2019s impact.  Questions and Suggestions for Authors: 1. How does the computational complexity of training a population of metapolicies in DiAMetR compare to other metaRL algorithms 2. Could the authors discuss potential realworld applications and scenarios where DiAMetR could be applied to showcase its practical relevance 3. How sensitive is the performance of DiAMetR to hyperparameters especially the number of policies in the population and the uncertainty levels chosen 4. Could the robustness of DiAMetR be further validated through adversarial attacks or perturbations in the test tasks to assess its generalizability  Limitations: The paper adequately addresses potential limitations and negative societal impacts. To further enhance this aspect authors could emphasize the ethical implications of deploying machine learning algorithms in safetycritical domains and propose mechanisms to mitigate negative consequences. The paper presents a novel and promising framework for addressing distribution shifts in meta reinforcement learning supported by theoretical insights and empirical validation. Addressing the noted weaknesses and questions could further strengthen the impact and practical applicability of DiAMetR.", "nJt27NQffr": "This paper proposes a novel selfsupervised learning method called Maximum Entropy Coding (MEC) to address biases in learned representations introduced by existing pretext tasks. MEC optimizes representations based on the principle of maximum entropy aiming for better generalization to unseen downstream tasks. The authors leverage the minimal coding length in lossy data coding as a computationally tractable surrogate for entropy and provide a scalable reformulation of the objective for efficient computation. Extensive experiments across various image and video tasks demonstrate that MEC outperforms previous methods on downstream tasks including ImageNet linear probe semisupervised classification object detection instance segmentation and object tracking. Strengths: 1. The paper addresses a crucial issue in selfsupervised learning by explicitly optimizing for representations with maximum entropy aiming for better generalization. 2. The proposal of MEC and its derivation from the maximum entropy principle are novel and theoretically wellgrounded. 3. Extensive experiments across various tasks demonstrate the effectiveness of MEC in achieving stateoftheart performance in comparison to existing methods. 4. The paper provides a clear and detailed description of the method the formulation process and experimental results. Weaknesses: 1. The paper lacks a detailed comparison with related work in the introduction. A more thorough discussion about how MEC differs and improves upon existing methods would enhance the papers clarity. 2. The authors could include more qualitative illustrations or discussions to help readers better understand the theoretical foundation and the intuition behind the proposed method. 3. While the results are comprehensive more indepth analysis of the limitations and failure cases of MEC on specific tasks could provide a more wellrounded assessment. Questions and Suggestions for Authors: 1. Could you provide more insights into the computational complexity of MEC compared to existing methods How scalable is MEC to largescale pretraining 2. Have you considered how the performance of MEC might be impacted by different dataset characteristics or domain shifts It would be interesting to see discussions on the robustness of MEC to different data distributions. 3. Could you elaborate on the interpretability of the learned representations by MEC How does the maximum entropy principle affect the interpretability of the representations 4. It would be beneficial to add a discussion on the limitations and potential negative societal impacts of MEC along with suggestions for mitigating these issues. 5. Have you thought about conducting human studies or benchmarks to evaluate the perceptual quality of the learned representations by MEC beyond quantitative evaluation on downstream tasks Limitations: The authors have not adequately addressed the limitations and potential negative societal impacts of their work. It would be beneficial for the authors to include a section discussing these aspects and provide constructive suggestions for improvement.", "uIXyp4Ip9fG": " Review: 1. Summary of the Paper: The paper proposes Active Surrogate Estimators (ASEs) for labelefficient model evaluation. The method utilizes a surrogatebased estimation approach actively learning underlying surrogate models and introduces a novel acquisition strategy XWED tailored for the estimation task. The study shows that ASEs outperform the current stateoftheart in evaluating deep neural networks when labels are expensive. 2. Strengths:  Innovative Approach: The introduction of ASEs and the XWED acquisition strategy presents a novel and promising approach to labelefficient model evaluation.  Theoretical Analysis: The theoretical analysis provided in the paper is comprehensive breaking down the errors of ASEs and providing insights into the sources of error.  Experimental Results: The experimental results demonstrate the effectiveness of ASEs in different scenarios showing significant improvements over baselines.  Flexibility and Adaptability: ASEs offer flexibility in acquisition strategies and address noisy label scenarios effectively. 3. Weaknesses:  Distribution Shift Experiment: The paper could further elaborate on the results obtained in the distribution shift experiment providing more insights into the robustness of ASEs.  Surrogate Updates: It would be beneficial to discuss further the impact of retraining the auxiliary model \u03c0 in different settings and scenarios.  Sensitivity Analysis: The sensitivity of the results to hyperparameters or changes in the model architecture could be explored for a more indepth analysis. 4. Questions and Suggestions for Authors:  Further Analysis Required: Consider delving deeper into the justification and implications of using an unchanging surrogate model especially in scenarios with noisy label distributions.  Additional Experiments: Perform additional experiments to evaluate the robustness of ASEs in diverse datasets or complex model architectures.  Comparative Analysis: Comparing the performance of ASEs with other stateoftheart methods in scenarios with varying levels of label noise could provide valuable insights. 5. Limitation Addressing Suggestions:  Limited Dataset Scenarios: Authors could address the limitation of using fixed surrogate models in scenarios where the dataset is limited or dynamic by proposing adaptive strategies or discussing potential implications. Overall the paper presents a significant contribution to labelefficient model evaluation and with further exploration and analysis it can enhance its impact and applicability.", "l1WlfNaRkKw": " Summary: The paper studies learning under transformation invariances in three settings: invariantly realizable relaxed realizable and agnostic settings. It analyzes the performance of data augmentation (DA) theoretically and proposes optimal algorithms based on combinatorial measures. Contributions include showing that DA outperforms vanilla Empirical Risk Minimization (ERM) but is suboptimal in some settings. The paper provides complexity measures characterizing the optimal sample complexity and offers adaptive algorithms.  Strengths: 1. Comprehensive Analysis: The paper thoroughly explores the theoretical aspects of learning under transformation invariances. 2. Clear Contributions: The paper clearly outlines its contributions regarding the performance analysis of data augmentation and optimal algorithms. 3. Novel Approaches: The introduction of combinatorial measures and adaptive algorithms add depth to the analysis and provide a unique perspective.  Weaknesses: 1. Complexity: The theoretical nature of the paper might make it challenging for readers without a deep mathematical background to understand. 2. Applicability: The direct practical implications of the proposed theoretical results on realworld applications are not explicitly discussed.  Questions and Suggestions for Authors: 1. Clarity on Practical Implications: Can the authors provide some insights into how their theoretical findings can be applied in realworld machine learning scenarios 2. Generalizability: How do the results generalize to different types of transformations beyond group transformations 3. Validation: Have the proposed algorithms been validated with empirical experiments to confirm the theoretical findings  Limitations: The authors may consider providing clearer explanations on the practical implementation and potential realworld impact of their theoretical findings in order to bridge the gap between theory and application more effectively.", "tVbJdvMxK2-": " Summary The paper proposes a novel approach Global Pseudotask Simulation (GPS) to address the challenge of catastrophic forgetting in continual learning using experience replay (ER). The key contribution lies in formulating dynamic memory construction in ER as a combinatorial optimization problem and proposing GPS to optimize the global loss across all experienced tasks. The paper conducts experiments on vision benchmarks demonstrating that GPS consistently improves accuracy compared to baselines and serves as a unified framework for enhancing various memory construction policies in ER variants.  Strengths 1. Innovative Approach: The paper introduces a unique perspective on memory construction in ER and presents a practical solution GPS to optimize memory allocation dynamically. 2. Empirical Evaluation: The experiments conducted on four vision benchmarks showcase the effectiveness of GPS in improving accuracy across different tasks and its potential for integration with existing ER variants. 3. Thorough Analysis: The paper provides detailed explanations of the problem formulation solution tactics simulation methods and experimental results offering a comprehensive understanding of the proposed approach.  Weaknesses 1. Limitation in RealWorld Application: The paper focuses mainly on vision benchmarks raising questions about the generalizability of GPS to diverse domains beyond vision tasks. 2. Complexity and Scalability: The proposed approach involves combinatorial optimization and simulation techniques which may pose challenges in scalability and realtime applications.  Questions and Suggestions 1. How does GPS adapt to scenarios with varying data distributions or concept drifts over time Could the authors provide insights into the adaptability of GPS to nonstationary environments 2. Can the authors elaborate on the computational overhead introduced by using GPS in comparison to standard ER approaches Clear benchmarks or metrics for assessing the computational efficiency of GPS would be valuable.  Limitations The paper focuses on vision benchmarks potentially limiting the understanding of how GPS performs in diverse domains. To address this limitation the authors could consider evaluating GPS on a broader range of datasets to demonstrate its applicability across various domains.  Overall the paper presents a novel and promising approach with notable contributions to the field of continual learning. Further validations and enhancements in scalability and realworld applications would strengthen the impact of the proposed Global Pseudotask Simulation (GPS) method.", "ttQ_3CiZqd3": "PeerReview Summary: The paper presents a novel principle for learning equilibrium systems using a temporally and spatiallylocal rule. The principle adopts an optimal control approach where learning is formulated as minimizing the control needed to reach a solution state. The authors demonstrate that their approach achieves competitive performance in various neural network architectures including recurrent networks feedforward networks and metalearning tasks. The results shed light on the brains learning mechanisms and offer new insights into approaching machine learning problems. Strengths: 1. Innovative Approach: The paper introduces a novel principle that departs from traditional gradientbased optimization and frames learning as a leastcontrol problem providing a fresh perspective on neural network learning. 2. Theoretical Contributions: The theoretical analysis provided including the optimal control formulation and relationships with predictive coding theories adds depth to the understanding of learning dynamics in neural systems. 3. Applicability: The paper demonstrates the applicability of the proposed principle to a wide range of neural network architectures showcasing its versatility and effectiveness in various settings. 4. Experimental Validation: The results from applying the principle to different tasks including metalearning feedforward and recurrent neural networks are compelling and demonstrate the practical utility of the proposed approach. Weaknesses: 1. Limitation to Equilibrium Systems: The focus on equilibrium systems could be seen as a limitation as it may restrict the generalizability of the proposed principle to more dynamic systems. Consideration of outofequilibrium dynamics could enhance the broader applicability of the approach. 2. Computational Cost: The authors acknowledge that the computational cost of their approach often exceeds that of common methods like backpropagation which could be a limitation in practical applications. Suggestions for addressing this issue could strengthen the paper. Questions and Suggestions: 1. Generalization to NonEquilibrium Systems: Have you considered extending your principle to nonequilibrium systems where dynamics play a crucial role in neural computations Exploring how the principle could be adapted to dynamic environments would provide a more comprehensive understanding of its applicability. 2. Computational Efficiency: Given the computational cost mentioned are there strategies you could propose to optimize the efficiency of implementing the leastcontrol principle in practical applications Streamlining the computational process could improve the feasibility of adopting this approach in realworld scenarios. 3. Comparison with Other Methods: It would be valuable to compare the performance and computational efficiency of your approach with other stateoftheart methods on a broader set of benchmarks to showcase its advantages in diverse scenarios and potentially identify its limitations more precisely. Limitations: While the paper acknowledges the restriction to equilibrium systems addressing the computational cost and extending the principle to nonequilibrium environments are crucial for enhancing the practical utility and generalizability of the proposed approach. Adapting the method to dynamically changing systems could provide more comprehensive insights into its effectiveness.", "mn1MWh0iDCA": " Peer Review of \"Understanding the Behaviors of SOTA Offline Reinforcement Learning Agents\" Summary: The paper addresses the gap in understanding the behaviors of StateOfTheArt (SOTA) offline RL agents. It introduces experiments to evaluate agents on representations value functions and policies. Surprisingly the study reveals that more performant agents can have lowquality representations and inaccurate value functions. This leads to the introduction of a novel offline RL algorithm and exploration of when learned dynamics models are helpful. Overall the research offers valuable insights into the behaviors of offline RL agents. Strengths: 1. Comprehensive Experiments: The paper conducts extensive experiments to compare the representations value functions and policies of offline RL agents providing a thorough evaluation of SOTA agents. 2. Unique Findings: Revealing that a more performant agent can possess lowerquality representations challenges existing assumptions and adds a unique perspective to the field. 3. Empirical Support: The conclusions are wellsupported with empirical results and detailed analysis enhancing the credibility of the findings. 4. Case Study (RIQL): The case study introducing RIQL to show the effectiveness of different policy evaluationimprovement methods is a valuable addition and contributes to the practical implications of the research. Weaknesses: 1. Lack of Comparison with Benchmark: While the paper compares different SOTA offline RL agents a comparison with benchmark traditional RL algorithms or online RL methods could provide further context and insights. 2. Model Noise Handling Limitation: The research addresses the issue of model noise with USS but further discussion on the limitations and potential improvements of this approach could enhance the completeness of the study. 3. Societal Impact Discussion: The paper lacks a discussion on the potential societal impact of the findings and the introduced algorithms which is essential in AI research. Questions and Suggestions: 1. Generalization: How well do the findings and proposed methods generalize to a wider range of offline RL problems and environments 2. Behavior Interpretation: Can the study offer more insights into why more performant agents exhibit lowerquality representations and inaccurate value functions 3. Policy Evaluation Diversity: How do diverse policy evaluationimprovement methods influence the performance and behavior of offline RL agents 4. Future Directions: In future research could you explore the integration of learned dynamics models in a more effective way to improve the performance of modelfree offline RL agents Limitations: The paper lacks a discussion on the potential societal impact of the introduced algorithms. Authors are encouraged to include a brief section addressing the societal implications of the research and emphasize responsible AI development practices. Overall the study provides a valuable contribution to understanding offline RL agent behaviors. By addressing the weaknesses and responding to the questions and suggestions the authors can further enhance the quality and impact of their research. Well done on the thorough investigation and insightful findings.", "uP9RiC4uVcR": " Peer Review  1) Summary: The paper introduces the novel challenge of MoralExceptQA a task aimed at evaluating the moral flexibility of large language models (LLMs) by assessing their ability to predict human moral judgments in cases where a rule should be permissibly broken. The authors propose the MORALCOT prompting strategy inspired by cognitive science to improve LLM performance on MoralExceptQA. The experiments show that MORALCOT outperforms existing LLMs on this challenge showcasing the importance of modeling human reasoning for capturing human moral judgment flexibility.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important and timely issue of AI safety related to human moral judgment and flexibility.  The introduction of the MoralExceptQA challenge set and the MORALCOT strategy are novel contributions to the field.  Conducting a detailed error analysis provides valuable insights into the limitations of existing LLMs in understanding human moral reasoning.  The paper is wellstructured and clearly presents the methodology results and implications of the work.  The experimental results show a significant improvement by the proposed MORALCOT model over existing LLMs. Weaknesses:  The paper could benefit from more extensive comparison with the larger body of related work in moral reasoning and AI ethics.  The dataset size for MoralExceptQA is mentioned as a limitation future work should focus on expanding the dataset.  While the error analysis is detailed more emphasis on addressing specific error types and proposing solutions could further enhance the paper.  3) Questions and Suggestions for the Authors:  How do you plan to address the dataset size limitation for MoralExceptQA in future work  Could the MORALCOT model be applied to other domains beyond moral flexibility to evaluate its generalizability  Considering the limitations identified in the error analysis what specific improvements or adaptations could be made to enhance the performance of LLMs in moral reasoning tasks  4) Limitations: While the authors have acknowledged the dataset size limitation of MoralExceptQA further work should include a larger dataset to ensure robustness and generalizability of the findings. Additionally exploring how the MORALCOT model can be extended to address a broader range of moral judgment scenarios would enhance the impact of the work.  Overall the paper presents a significant and innovative contribution to the field of AI ethics and safety offering valuable insights into the challenges and opportunities in modeling human moral judgment for AI systems. Addressing the suggested improvements would further strengthen the impact and relevance of the research.", "m6DJxSuKuqF": "1) Summary: The paper introduces a novel KeyPointGuided model by ReLation preservation (KPGRL) for leveraging annotated keypoints to guide optimal transport matching. The KPGRL model enforces the matching of keypoint pairs and maintains the relations of data points to keypoints during transport. It can handle distributions in different spaces and is extended to partial optimal transport. The proposed model is applied to heterogeneous domain adaptation tasks showing promising results. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important gap in optimal transport methods by introducing a novel approach that leverages annotated keypoints for guiding correct matching.  The proposed KPGRL model is theoretically wellmotivated and has practical applications in heterogeneous domain adaptation tasks.  The paper provides a comprehensive comparison with existing optimal transport models and heterogeneous domain adaptation methods.  Detailed explanations formalism and illustrations are provided to help understand the proposed model and its applications. Weaknesses:  The paper lacks a detailed discussion on the computational complexity and scalability of the proposed models especially when dealing with largescale datasets.  More empirical evaluations on diverse datasets and comparison with stateoftheart methods from related fields could further strengthen the papers empirical validation.  The paper at times may be too technical requiring readers to have a solid understanding of optimal transport and related concepts. 3) Questions and Suggestions:  How does the efficiency of the proposed KPGRL model compare to existing optimal transport methods in terms of computational complexity and memory usage  Additional experiments on larger and more diverse datasets would help generalize the efficacy of the model. Have you considered such scalability evaluations  Can you provide more insights into how the proposed model generalizes to different domains beyond the examples provided in the paper It would be beneficial to understand its applicability in broader applications. 4) Limitations: The authors partially address the limitations and potential negative societal impact of the proposed work by highlighting the dataset restrictions and the generalization of the model. To enhance this aspect:  More detailed discussions on the ethical considerations and potential biases in the use of annotated keypoints could be included.  Acknowledgment and discussion on possible challenges or limitations in realworld applications of the model would improve the consideration of societal impact. Consider providing insights on possible biases that could arise if the keypoints are not correctly annotated or biased towards specific features.", "xWvI9z37Xd": " Summary: The paper introduces a new efficient unsupervised method for feature selection called WAST based on sparse autoencoders. It optimizes the sparse topology of an autoencoder to quickly detect informative features. Extensive experiments on various datasets demonstrate the superiority of WAST in selecting informative features reducing training iterations and computational costs substantially.  Strengths: 1. Innovative Approach: Introducing a novel method WAST for efficient unsupervised feature selection based on sparse autoencoders is a significant contribution. 2. Performance: Outperforming stateoftheart methods in selecting informative features while reducing computational costs is commendable. 3. Experimental Rigor: Comprehensive experiments on various datasets demonstrate the effectiveness and robustness of WAST. 4. Efficiency: Reducing memory and computational costs substantially showcasing the practical applicability of the proposed method. 5. Impact: Addressing the importance of feature selection in machine learning tasks and demonstrating the applicability in different domains adds value to the research.  Weaknesses: 1. Clarity: Certain sections could benefit from more clarity and better explanations to enhance understanding especially for readers not familiar with neural network terminology. 2. Limitations: While the results are promising a more detailed discussion on the limitations and potential challenges of implementing WAST in realworld scenarios could be beneficial. 3. Comparison: The comparison with supervised feature selection methods is insightful but could be further expanded or discussed in a separate section for better clarity. 4. Ablation Study: The ablation study could be expanded to provide deeper insights into the contribution of individual components in WAST.  Questions and Suggestions for Authors: 1. Implementation: How scalable and userfriendly is the implementation of WAST for practitioners and researchers seeking to apply it in their work 2. Generalization: How well does WAST generalize to unseen datasets or domains beyond those considered in the experiments 3. Future Work: Are there any plans to explore the application of WAST in more complex scenarios or extend its capabilities to address additional challenges in feature selection 4. Validation: How does the proposed method perform against more challenging benchmarks or datasets known for their complexity noise or inherent biases 5. Interpretability: Can the authors provide insights into the interpretability of the selected features by WAST and how they align with domainspecific knowledge or requirements  Limitations: The authors could expand on the future implications of hardware advancements and discuss potential negative societal impacts more explicitly to emphasize the ethical considerations of deploying such methods responsibly.", "l7aekTjF6CO": " PeerReview of the Paper \"Representation and Fairness in SortitionBased Democracy\"  Summary: The paper introduces a theoretical framework to analyze the representation and fairness of sortition particularly in the context of citizens assemblies. It presents a representation metric based on the distance between individuals and panel members evaluating the representation of selection algorithms. The study shows that uniform random selection achieves good representation and fairness for certain values of the parameter q. Additionally a new algorithm RANDOMREPLACE is proposed to balance representation and fairness tradeoff.  Strengths: 1. Novel Conceptual Framework: The paper offers a fresh perspective on the evaluation of representation and fairness in sortition. 2. Theoretical Rigor: The study is theoretically sound establishing lower bounds on representation and providing explicit algorithms. 3. Empirical Validation: Empirical experiments with realworld datasets provide practical insights into the behavior of different algorithms. 4. Clear Presentation: The papers structure is wellorganized transitioning logically from theory to experiments.  Weaknesses: 1. Clarity of Exposition: The theoretical concepts introduced such as qcost and social cost may be challenging for readers not wellversed in the field. Providing clearer explanations and examples could enhance accessibility. 2. Limited Scope: The study primarily focuses on a specific representation metric. Exploring other metrics or cost functions could enrich the analysis and offer a broader perspective. 3. Complexity of Algorithms: The proposed RANDOMREPLACE algorithm while effective may be computationally intensive for large datasets. Addressing scalability concerns could enhance its practical utility.  Questions and Suggestions for Authors: 1. Algorithm Complexity: Could you provide insights into the computational complexity of the proposed algorithms especially in scenarios with large population sizes 2. Sensitivity Analysis: Have you explored the sensitivity of your results to variations in the representation metric or the choice of features A sensitivity analysis could provide robustness to your findings. 3. RealWorld Applicability: How do you envision the practical adoption of your algorithms in realworld democratic processes Consider discussing the feasibility and implementation challenges. 4. Generalization: How transferable are your results across different sociopolitical contexts or countries Have you considered variations in cultural factors impacting representation  Limitations: The authors have effectively addressed the limitations and potential societal impact concerns in their paper. Suggestions for improvement include providing clearer explanations of complex concepts for a broader audience and considering scalability issues in algorithm development.  Conclusion: Overall the paper presents a thoughtprovoking analysis of representation and fairness in sortitionbased democracy providing valuable insights and a solid foundation for further research in the field. Strengthening the clarity of exposition exploring algorithmic complexity and considering broader applicability would enhance the impact of this work.", "yZ_JlZaOCzv": "Peer Review: Summary: The paper investigates the vulnerability of stateoftheart Go agents particularly AlphaZerobased agents to adversarial attacks in the form of adding meaningless actions to the game states. The study presents a novel method to efficiently find adversarial states where the agents perform poorly as evidenced by both PVNN and MCTSbased agents making trivial mistakes in the presence of subtle perturbations. The contributions include revealing the agents weaknesses developing an effective attack method and highlighting the potential to generalize adversarial examples to improve AI agents. Strengths: 1. Innovative Methodology: The paper introduces a unique approach to detect vulnerabilities in Go AI agents by strategically perturbing game states with meaningless actions. 2. Comprehensive Experiments: The extensive experimental evaluation on different agents datasets and game scenarios provides a thorough analysis of the proposed attack method\u2019s efficacy. 3. HumanVerifiable Examples: The study ensures that the identified adversarial examples are easily understandable and verifiable by amateur human players enhancing the robustness of the findings. 4. Efficiency and Speed: The proposed search algorithm efficiently reduces the search space making the attack method more practical and applicable in real scenarios. 5. Potential for Agent Improvement: The paper discusses the implications of the findings for enhancing AI agents by addressing the identified weaknesses through training with the discovered examples. Weaknesses: 1. Limited Generalizability: While the paper demonstrates the vulnerability of Go agents to adversarial perturbations the generalizability of the attack method to other domains or games needs further validation. 2. Verification of Results: The study could benefit from additional verification methods to confirm the accuracy of the identified adversarial examples and the impact of the proposed attack on the agents overall performance. 3. Theoretical Grounding: The paper could further elaborate on the theoretical underpinnings of the attack method and provide more indepth insights into the principles guiding the identification of adversarial examples. Questions and Suggestions: 1. Validation of Adversarial Examples: How did the authors ensure the correctness of the identified adversarial examples and their impact on the agents\u2019 decisionmaking processes 2. Scalability: How scalable is the proposed attack method to larger state spaces or more complex games beyond Go and what challenges might arise in extending the approach 3. Ethical Considerations: Have the authors considered the potential misuse or negative implications of their findings on game integrity or AI development and how could ethical considerations be addressed in future research Limitations: While the paper extensively evaluates the vulnerability of AI agents in Go to adversarial attacks there is a need for further exploration of the attack\u2019s implications on the agents decisionmaking processes and performance in diverse game scenarios. The authors should continue to address the limitations of the study and the potential societal impacts of their work to ensure responsible research practices and ethical considerations are consistently upheld.", "ogNrYe9CJlH": " Peer Review  1) Summary: The paper proposes a novel ReductiontoBinary (R2B) approach for achieving demographic parity in multiclass classification with nonbinary sensitive attributes. The approach reduces the debiasing task into a sequence of binary debiasing jobs on each class separately. The paper provides theoretical guarantees empirical validations on synthetic and realworld datasets and comparison with existing baselines.  2) Strengths:  The paper addresses a significant gap in algorithmic fairness literature by tackling multiclass classification with nonbinary sensitive attributes.  The proposed R2B approach provides theoretical guarantees and empirical validations showing competitive performance against existing baselines.  The study covers a wide range of datasets from social science computer vision and healthcare showcasing the methods applicability.  3) Weaknesses:  The paper lacks a detailed analysis of the computational complexity and scalability of the R2B method especially in large datasets.  The discussion on broader societal implications of bias mitigation is limited and the focus remains primarily on the technical aspects of debiasing algorithms.  4) Questions and Suggestions for Authors:  Can the authors provide insights into the computational complexity of the R2B algorithm and its scalability to large datasets  How does the proposed R2B method handle imbalanced class distributions in the sensitive attribute groups  Could the authors elaborate on the potential tradeoffs between debiasing accuracy and model performance in scenarios where biases are correlated with sensitive attributes  5) Limitations: The authors have adequately addressed limitations related to accommodating only demographic parity and categorical sensitive attributes. However to enhance the studys impact and societal relevance it is suggested that the authors consider:  Including a discussion on broader fairness metrics beyond demographic parity such as equalized odds.  Addressing the impact of debiasing on model accuracy especially in scenarios where biases are correlated with sensitive attributes.  Extending the method to handle continuous sensitive attributes without resorting to binning techniques for improved generalizability.  Overall Impression: The paper presents a novel and timely contribution to algorithmic fairness research by addressing the challenging problem of bias mitigation in multiclass classification with nonbinary sensitive attributes. The empirical validations and comparison with existing methods strengthen the studys credibility. However further discussions on computational complexity scalability tradeoffs between debiasing and model performance and broader fairness metrics are recommended for a more comprehensive evaluation.", "pOEN7dDC0d": " Summary: The paper investigates whether Harris\u2019s articulation scheme (HAS) holds in emergent languages. HAS is a statistical universal in natural languages for word segmentation based on phonemes alone. The study employs HAS as an unsupervised word segmentation method on emergent languages emerging from signaling games.  Strengths:  The paper demonstrates thorough experimental results that validate key claims made in the introduction and abstract.  The paper effectively presents complex concepts and methodologies related to emergent languages and word segmentation.  The work is wellstructured with clear sections and subsections that aid in understanding the research progression.  The authors have provided code data and instructions for reproducing the experimental results.  Weaknesses:  The limitations of the work are mentioned but the negative societal impact is briefly touched upon. A more detailed discussion on potential impact could enhance the completeness of the paper.  The paper could improve the clarity of some technical terms and explanations for readers less familiar with the specific field of study.  The discussion on bridging the gap between emergent and natural languages could be more detailed and provide clearer suggestions for future research directions.  QuestionsSuggestions: 1. Can you provide more insights into the significance of the findings regarding the meaningfulness of HASbased word boundaries in emergent languages 2. How do you envision addressing the limitations and enhancing the applicability of your work in practical scenarios 3. Could you elaborate more on the implications of the ZLAlike tendencies observed in the hyposegments as they relate to the broader field of emergent communication  Limitations: The authors have adequately addressed the limitations but further discussion on the potential negative societal impact and constructive suggestions for improvement could enhance the overall impact of the work. For example a more detailed exploration of how the findings could be applied in realworld scenarios would improve the practical relevance of the research.", "tro0_OqIVde": " Summary: The paper introduces the Recursive Gated Convolution (gnConv) operation as an alternative to selfattention for visual modeling. It aims to efficiently implement inputadaptive longrange and highorder spatial interactions in vision models. The proposed gnConv is used to construct a family of generic vision backbones named HorNet which outperforms Swin Transformers and ConvNeXt on tasks like ImageNet classification COCO object detection and ADE20K semantic segmentation.  Strengths: 1. Innovative Contribution: The paper introduces a novel operation gnConv to efficiently implement highorder spatial interactions in vision models presenting a new perspective. 2. Extensive Experiments: The paper provides detailed experiments on popular benchmarks showcasing the superiority of HorNet over existing models. 3. Thorough Comparison: The comparison with other models like Swin Transformers and ConvNeXt demonstrates the effectiveness of HorNet in different tasks. 4. Detailed Methodology: The paper thoroughly explains the formulation of gnConv model architectures and experiments making it easy to follow.  Weaknesses: 1. Complexity vs. Efficiency: The paper acknowledges that HorNet may be slower in GPU latency compared to ConvNeXt indicating a possible limitation in efficiency that could affect practical scalability. 2. HardwareFriendliness: There is a call for developing a more hardwarefriendly operation for highorder spatial interactions suggesting room for improvement in this aspect. 3. Societal Impact: The paper lacks a discussion on societal impact potential biases or ethical considerations which could be valuable to address.  Questions and Suggestions for Authors: 1. Scalability Concerns: How does HorNet perform in scenarios with constrained computational resources compared to CNN models considering efficiency and scalability 2. Future Directions: Are there plans to optimize the latency of HorNet on GPUs or explore hardwarefriendly implementations for practical deployment 3. Societal Impact: Considering the impact of AI models on society have you considered any ethical implications or biases associated with the utilization of HorNet in realworld applications  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. It is recommended to include ethical considerations and societal impact assessment in future publications for a more comprehensive evaluation.", "mfxq7BrMfga": " Summary: The paper introduces a novel framework for generalized oneshot GAN adaptation task focusing on both style and entity transfer. The proposed framework decouples style and entity adaptation minimizes divergence of internal distributions by sliced Wasserstein distance and uses variational Laplacian regularization for crossdomain correspondence.  Strengths:  Innovative Contribution: The paper addresses an important gap in oneshot GAN adaptation by considering both style and entity transfer which is a novel concept with potential impactful implications.  Effective Methodology: The proposed framework effectively decouples style and entity adaptation improves internal distribution learning and introduces manifold regularization which leads to impressive experimental results.  Thorough Experiments: Extensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method in various scenarios showcasing highquality syntheses and crossdomain correspondence.  Weaknesses:  Complexity: The proposed framework and methodology may be challenging for readers unfamiliar with GAN adaptation and internal distribution learning concepts.  Limitations Not Addressed: The paper mentions limitations related to entity complexity and control but does not provide explicit strategies or solutions to address these challenges.  Questions and Suggestions for the Authors: 1. Clarification on Entity Transfer: Can the authors provide more insights on how complex entities are handled in the adaptation process especially in cases where entities are intricate or pose changes significantly 2. Scalability: How scalable is the proposed framework for adaptation to various domains with diverse styles and entities 3. Comparison with Existing Methods: Could the authors provide a more detailed comparison with existing stateoftheart methods in terms of computational efficiency scalability and adaptability to different scenarios 4. Impact Analysis: Have the authors considered potential societal impacts or ethical considerations arising from the use of their adapted models especially in applications related to image manipulation and style transfer  Limitations: The authors have briefly touched upon limitations related to entity complexity and control however suggestions for improvement in handling these challenges are not explicitly provided. Enhancing the discussion on handling complex entities and improving control mechanisms could further strengthen the paper.  Suggestions for Improvement: The authors are encouraged to provide more detailed explanations and examples to make the paper more accessible to a broader audience particularly those not deeply familiar with GAN adaptation and internal distribution learning concepts. Additionally addressing the limitations more comprehensively and suggesting strategies for addressing these limitations would enhance the completeness of the paper.", "uPdS_7pdA9p": "Summary: The paper investigates the robustness of large pretrained foundation models (FMs) to group shifts highlighting that existing methods may not effectively handle these shifts. The authors propose a contrastive adapter training strategy to enhance FM group robustness without requiring finetuning. Their approach consistently improves worstgroup accuracy across multiple benchmarks efficiently and effectively. Strengths: 1. Innovative Approach: The proposed contrastive adapting strategy to improve FM group robustness is novel and addresses an important challenge in deep learning. 2. Rigorous Evaluation: The authors conduct thorough experiments across multiple benchmarks and provide detailed analyses to validate the effectiveness of their approach. 3. Efficiency: The paper highlights the efficiency of the proposed method by achieving significant improvements in group robustness with minimal parameters. 4. Clear Presentation and Structure: The paper is wellstructured with clear explanations experimental details and results. Weaknesses: 1. Lack of Comparison: While the paper compares the proposed method with various baselines there could be more indepth comparisons with stateoftheart methods to further demonstrate the superiority of the proposed approach. 2. Societal Implications: The paper lacks discussion on the potential negative societal impacts of their work which is crucial in AI research. 3. Clarity in Explanations: Some sections could be further simplified for better understanding especially for readers less familiar with the domain. Questions and Suggestions for Authors: 1. Network Scalability: How scalable is the contrastive adapting method across different architectures and tasks 2. Generalization: How does the proposed method perform on unseen datasets or realworld applications 3. UserFriendliness: How easy is it for practitioners to implement the contrastive adapting approach in their models 4. Ethical Considerations: Have you considered the ethical implications of improving group robustness in AI models and do you have any recommendations in this regard Limitations Improvement: Authors should consider discussing the potential societal impact of their work and offer suggestions for mitigating any negative consequences to ensure responsible AI research and deployment.", "n4wnZAdBavx": " Peer Review  Summary: The paper proposes a novel framework MultiAgent communication via Selfsupervised Information Aggregation (MASIA) to enhance coordination in Multiagent Reinforcement Learning (MARL) by efficiently aggregating messages from teammates. The framework utilizes selfsupervised objectives to optimize the aggregation process and incorporates a focusing network for extracting the most relevant information for individual decisionmaking. Empirical results demonstrate the superiority of MASIA over strong baselines across cooperative MARL tasks.  Strengths: 1. Clear Problem Statement: The paper effectively outlines the challenges in MARL coordination due to partial observability and proposes a relevant and novel solution. 2. Innovative Approach: The introduction of MASIA focusing on information aggregation and extraction is a novel contribution to enhance communication efficiency in MARL. 3. Theoretical Foundation: Grounding the work in recent advances in state representation learning and selfsupervised objectives adds theoretical strength to the proposed framework. 4. Comprehensive Experiments: The extensive experimental evaluation on various benchmarks and communication conditions provides robust evidence of MASIAs performance and efficacy.  Weaknesses: 1. Evaluation Metrics: The paper could benefit from a detailed discussion on the specific evaluation metrics used and their relevance to the proposed framework. 2. Ignored Related Work: While the related work is mentioned in passing a more comprehensive discussion and comparison with existing communication methods in MARL could add value to the paper. 3. Clarity of Some Sections: Certain technical sections particularly on the method and information representation optimization may require further clarification for better understanding. 4. Visualization: While the visualization results are highlighted as supporting evidence a more indepth analysis and interpretation of the visualizations could enhance the papers impact.  Questions and Suggestions for Authors: 1. Communication Efficiency: How does MASIA handle scenarios with a large number of agents Have you considered scalability issues in your experiments 2. Comparative Analysis: Can you provide more insights on how MASIA compares with existing communication methods like I2C ACML and SARNet in terms of communication efficiency and sample complexity 3. Future Direction: In future work do you plan to explore the application of MASIA in environments with image inputs or investigate ways to mitigate scalability challenges in MARL with numerous agents  Limitations: The paper adequately presents the limitations and societal impacts of the work. Considering the potential negative implications of inefficient communication in multiagent systems the authors could further explore approaches to mitigate the computational burden posed by communication processes for practical deployment. Overall the paper presents a significant contribution to the field of MARL communication and coordination. Addressing the raised points could further strengthen the papers impact and clarity.  This review is intended to guide the authors in enhancing the quality and impact of their research paper.", "zb-xfApk4ZK": " PeerReview  1) Summary: The paper introduces the ExploreExploit Markov chain Monte Carlo strategy (Ex2MCMC) that combines local and global sampling methods to improve sampling accuracy and efficiency. The study provides theoretical bounds on the accuracy and convergence speed of Ex2MCMC strategies proves Vuniform geometric ergodicity and analyzes FlEx2MCMC an adaptive version using a normalizing flow for proposal sampling. Experimental results on classical benchmarks and highdimensional distributions show the effectiveness of the proposed approach.  2) Strengths:  Comprehensive coverage: The paper addresses the limitations of existing sampling strategies and combines local and global samplers effectively.  Theoretical rigor: The study provides theoretical bounds and proves Vuniform geometric ergodicity enhancing the credibility of the proposed Ex2MCMC strategy.  Experimental validation: The paper includes detailed empirical evaluations on classical benchmarks and highdimensional distributions demonstrating the superiority of Ex2MCMC over traditional methods.  Novelty: The combination of local and global samplers along with adaptive sampling using normalizing flows represents a novel contribution to the sampling methodology.  3) Weaknesses:  Complexity: The paper contains advanced mathematical formulations and technical terminology that could be challenging for readers not wellversed in the field.  Lack of comparative analysis: While the experimental results show the benefits of Ex2MCMC a more detailed comparison with existing prominent methods like NUTS could enhance the papers strength.  4) Questions and Suggestions for the Authors:  Could the authors elaborate more on the computational complexity and runtime efficiency of the proposed Ex2MCMC and FlEx2MCMC algorithms compared to standard sampling methods  How robust is the proposed approach to different types of target distributions especially those with more complex geometries  5) Limitations: The authors have not explicitly addressed potential negative societal impacts associated with their work. Including a discussion on ethical considerations related to sampling techniques could enhance the papers overall impact and relevance. Overall the paper presents a significant advancement in the field of sampling methodologies especially in the context of highdimensional distributions. Strengthening the discussion on societal implications and incorporating additional comparative analyses with existing methods could further enhance the value of the study.", "zUbMHIxszNp": " Summary: The paper introduces a new multilevel framework that jointly models nodelevel properties and graphlevel statistics for graph generative models. It proposes a novel micromacro training objective that combines nodelevel and graphlevel losses. The approach is demonstrated on the GraphVAE model showing improved graph quality scores on benchmark datasets while maintaining fast training and generation times.  Strengths: 1. Innovative Approach: Introducing a principled joint probabilistic model for both micro and macro levels is a novel and valuable contribution. 2. Improved Realism: Micromacro modeling significantly enhances graph realism and diversity as demonstrated by improved graph quality scores. 3. Experimental Validation: The paper provides comprehensive experiments on benchmark datasets using stateoftheart evaluation metrics for assessing graph generation quality. 4. User Control: The approach allows users to specify target graph statistics simplifying the selection process for models based on the desired statistics.  Weaknesses: 1. Complexity and Overhead: The paper acknowledges computational overhead for evaluating graph statistics during training which could be a potential limitation for scaling to larger graphs. 2. Dependency on Specific Architecture: The proposed methodology is tailored for the GraphVAE model limiting its generalizability to other graph generative models. 3. Ethical Considerations: While addressed briefly further discussion on the ethical implications of network modeling and potential societal impacts could enhance the papers completeness.  Questions and Suggestions for Authors: 1. How does the proposed micromacro modeling approach compare with existing techniques in terms of scalability to larger graphs and datasets 2. Have you considered experimenting with other graph generative models beyond GraphVAE to validate the effectiveness of the micromacro objective 3. Could more details be provided on the computational efficiency of the proposed approach especially for realworld applications with dynamic graph data 4. Additionally further elaboration on the ethical considerations and societal impacts of employing graph generative models for network modeling would enhance the papers discussion.  Limitations: The authors could further elaborate on the potential negative societal impacts of their work such as ensuring ethical data collection addressing privacy concerns and mitigating biases in network modeling. Suggestions for improvement could include incorporating fairness and transparency measures in the model design and evaluation processes.", "tHK5ntjp-5K": " Paper Summary: The paper introduces the Hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is built on a variational autoencoder (VAE) framework with hierarchical latent space and two latent Denoising Diffusion Models (DDMs) which operate on point clouds. The model excels in highquality generation flexibility for various tasks and smooth surface reconstruction. Experimental results showcase stateoftheart performance on ShapeNet benchmarks.  Assessment of Strengths and Weaknesses: Strengths:  LION leverages a welldesigned VAE framework with hierarchical latent DDMs combining expressivity and flexibility for diverse tasks.  The model achieves stateoftheart performance on ShapeNet benchmarks.  It demonstrates highquality and varied 3D shape synthesis including voxelconditioned synthesis shape denoising and shape interpolation.  The shape interpolation and surface reconstruction capabilities enhance its utility for artists working with 3D shapes. Weaknesses:  The paper could benefit from further clarity in the explanation of complex technical concepts making it more accessible to a wider audience.  While the experimental results are strong comparative analysis with a broader range of existing models could provide a more comprehensive evaluation.  Given the complexity of the model and the range of applications it addresses a more detailed discussion on the computational efficiency and scalability would be valuable.  Questions and Suggestions for Authors: 1. Could you provide more insights into the computational efficiency and scalability of LION especially when dealing with larger datasets or complex multimodal synthesis tasks 2. How does LION compare with other stateoftheart models in terms of training time convergence speed and memory requirements 3. It would be beneficial to include a detailed discussion on the interpretability of the latent space representations in LION and how they contribute to the models performance. 4. Considering the diverse applications of LION have you explored any realworld use cases or collaborations with industry professionals to validate its practical utility  Limitations: The authors have done an impressive job addressing the limitations and potential negative societal impacts of their work. However to further enhance the societal impact it would be beneficial to include a section discussing ethical considerations related to the use of generative models like LION and provide guidelines on responsible usage.", "zkQho-Jxky9": " PeerReview Feedback  Summary: The paper introduces a formal definition of harm and benefit using causal models aiming to apply harmaverse decisionmaking in machine learning algorithms. It demonstrates the limitations of standard statistical methods in identifying harmful actions and proposes a framework for harmaverse decisionmaking using counterfactual objective functions. The authors illustrate their approach on the problem of identifying optimal drug doses and show promising results.  Strengths: 1. Unique Contribution: The paper makes a significant contribution by introducing a formal definition of harm and benefit and its application in harmaverse decisionmaking in machine learning. 2. WellStructured: The paper is wellorganized with clear sections that introduce and develop the concepts progressively. 3. Technical Depth: The paper goes into technical details like structural causal models counterfactual reasoning and decision theory demonstrating a strong understanding of the subject matter.  Weaknesses: 1. Complexity: The paper may be challenging for readers without a strong background in causal models and decision theory. Some parts could be simplified or explained in more accessible language. 2. Limited RealWorld Examples: While the drug dose response model example is insightful more realworld scenarios or practical applications of the proposed framework could strengthen the papers impact.  Questions and Suggestions: 1. Clarification: Can the authors provide a more intuitive explanation or example to better illustrate how the proposed harmaverse decisionmaking framework can be practically applied in complex realworld scenarios 2. Evaluation: How does the proposed methodology compare with existing approaches in terms of efficiency scalability and generalizability when applied to various domains beyond optimal drug dosing  Limitations: The authors should address the potential negative societal impact of their work by ensuring that the proposed methodology is ethically sound and does not inadvertently lead to harmful outcomes.  Suggestions for Improvement: 1. Simplify Complex Concepts: Where possible simplify technical explanations to make the paper more accessible to a broader audience. 2. Provide More RealWorld Applications: Include more diverse realworld examples to showcase the practical implications and advantages of the proposed harmaverse decisionmaking framework. Overall the paper presents a novel and valuable contribution to the field of machine learning ethics and causal reasoning but some improvements could enhance its readability and applicability.", "osPA8Bs4MJB": " Summary: The paper introduces the Local  Temporalaware Transformerbased Deepfake Detection (LTTD) framework to detect deepfake videos effectively by focusing on the local temporal inconsistencies leveraging transformers. The proposed approach achieves stateoftheart performance by emphasizing lowlevel cues and temporal information in deepfake detection.  Strengths: 1. Innovative Approach: The paper proposes a novel framework that effectively utilizes transformers to tackle deepfake detection showcasing a unique perspective in the field. 2. Robustness: The designs emphasis on lowlevel temporal information enables the model to be robust against common postprocessing methods and perturbations. 3. Generalizability: The LTTD framework demonstrates strong generalizability outperforming other methods on diverse datasets and deepfake types.  Weaknesses: 1. Complexity: The detailed architecture involving Local Sequence Transformer CPI loss and CPA module may pose challenges for implementation and understanding. 2. Evaluation Limitation: The methods performance in handling adversarial deepfakes and realworld deployment scenarios is not thoroughly addressed. 3. Validation Scope: The paper lacks evaluation against specific adversarial attacks common in deepfake generation to test its robustness comprehensively.  Questions and Suggestions for Authors: 1. Impact of Temporal Adversarial Attacks: How does the proposed framework perform against deepfakes adversarially enhanced in the temporal domain Conducting experiments in such scenarios would enhance the papers contributions. 2. RealWorld Deployment: Have you considered practical considerations for deploying this model in realworld scenarios Suggestions for enhancing deployment readiness would be valuable. 3. Explainability: Can you explain further how the models interpretability can aid in identifying regions of forgery 4. Scalability: How scalable is the proposed framework for deployment on a realtime deepfake detection system  Limitations: The authors could further address the limitations by:  Conducting adversarial testing against temporal manipulations to evaluate robustness.  Considering deployment challenges and offering solutions for practical realworld use.  Enhancing explainability and localizing forgery cues to strengthen model interpretability.", "wph_3smhuec": " Summary: The paper introduces a novel concept of Bilevel Games with Selection (BGS) to address ambiguity in bilevel optimization with nonconvex objectives. It proposes algorithms based on unrolled optimization and implicit differentiation to solve these games. The study extends existing analytical tools to handle degenerate critical points demonstrating algorithms capability to find equilibrium points with approximation errors.  Strengths: 1. Novel Concept: Introducing Bilevel Games with Selection is a significant contribution that addresses the ambiguity in bilevel optimization. 2. Detailed Analysis: Thorough analysis of the theoretical foundations and practical implications of using selection maps and algorithms to solve BGS. 3. Algorithmic Contributions: Proposed algorithms for solving BGS provide practical insights and corrections to existing approaches demonstrating the efficacy of the proposed methods.  Weaknesses: 1. Complexity: The theoretical derivations and analytical discussions might be challenging to follow for readers not wellversed in the mathematical concepts presented. 2. Limited Empirical Evaluation: The paper lacks extensive empirical evaluation or realworld application examples to validate the proposed methods in practical scenarios.  Questions and Suggestions for Authors: 1. Clarity in Presentation: Ensure the papers clarity by providing concrete examples or visual aids to facilitate understanding of the proposed concepts. 2. Validation: How will the authors validate the proposed algorithms empirically Are there specific benchmarks or datasets where these algorithms will be tested 3. Scalability: Have the authors considered the scalability of the proposed algorithms especially in largescale and highdimensional optimization problems Are there any limitations in terms of computational complexity or efficiency  Limitations: The authors have effectively addressed the limitations and negative societal impacts of their work. They could further enhance this aspect by discussing potential ethical implications of applying their algorithms in realworld scenarios and proposing strategies to mitigate any adverse effects.", "qx51yfvLnE": "Summary: The paper presents novel greedy Online Contention Resolution Schemes (OCRSs) for various combinatorial constraints like singleitem settings partition matroids and transversal matroids. The proposed schemes improve upon the existing stateoftheart greedy OCRSs and achieve optimal selectability guarantees against an almighty adversary. The study is significant for applications where online decisions under uncertainty are essential such as ad allocation and matching problems. Strengths: 1. Novel Contributions: The paper introduces optimal greedy OCRSs for fundamental constraints showcasing advancements in online stochastic optimization. 2. Theoretical Rigor: The proofs and analysis are detailed and thorough providing a solid theoretical foundation for the proposed schemes. 3. Practical Relevance: The study addresses realworld problems and the relevance of online decisionmaking in applications with hidden information. 4. Clear Explanation: The paper is wellstructured with detailed explanations of OCRSs constraints and the proposed algorithms. Weaknesses: 1. Limited Practical Validation: The paper lacks empirical validation of the algorithms on realworld datasets or practical applications. 2. Assumption Clarity: The assumptions made in the theoretical analysis could be explicitly stated to provide a clearer understanding of the models. 3. Comparative Analysis: A comparison with existing nongreedy OCRSs and a discussion of the tradeoffs between greedy and nongreedy approaches could enhance the completeness of the study. Questions and Suggestions: 1. Algorithm Performance: How do the proposed greedy OCRSs perform in comparison to existing nongreedy OCRSs in terms of selectability and competitive ratios 2. Generalizability: Are there extensions or variations of the proposed schemes that could cater to more complex combinatorial constraints or online decision scenarios 3. Practical Implementation: Have you considered practical challenges that may arise in implementing the proposed algorithms in realtime decisionmaking systems 4. Scalability: How do the proposed schemes scale with increasing problem sizes or constraints and are there scalability concerns that need to be addressed Limitations: The paper could benefit from addressing practical implementation aspects scalability considerations and empirical validations to enhance the applicability and impact of the proposed greedy OCRSs. Providing insights into these aspects would further strengthen the contributions of the study.", "zt4xNo0lF8W": " Summary: The paper introduces Mask Matching Transformer (MMFormer) as a new paradigm for fewshot segmentation. It decouples the learning of segmentation and matching modules allowing flexibility and reducing training complexity. The proposed MMFormer achieves competitive results on COCO20 and Pascal5 benchmarks showing its effectiveness and generalization ability.  Strengths: 1. Innovative Approach: Introducing a new paradigm for fewshot segmentation by decoupling the learning of segmentation and matching modules. 2. Effective Design: MMFormer efficiently matches support samples with query mask proposals resulting in competitive results on benchmark datasets. 3. Comprehensive Experiments: Extensive evaluation on COCO20i and Pascal5i demonstrates the methods robustness and transferability. 4. Thorough Ablation Studies: Componentwise ablations and training strategy analysis provide insights into the effectiveness of each module.  Weaknesses: 1. Performance Gap: The paper acknowledges a significant performance gap compared to the oracle results indicating room for improvement. 2. Training Efficiency: Despite proposing a twostage training strategy the efficiency could be further improved for better scalability. 3. Limited RealWorld Applications: The paper lacks a discussion on realworld applications or scenarios where MMFormer can be practically utilized.  Questions and Suggestions: 1. How does MMFormer handle noisy or ambiguous support samples during fewshot segmentation 2. Have you compared MMFormer with other stateoftheart methods on different datasets to assess its versatility 3. How does the performance of MMFormer vary when applied to complex or cluttered images for segmentation tasks 4. Can you provide insights into the computational efficiency and resource requirements of MMFormer compared to existing methods 5. How does MMFormer generalize to unseen classes or categories not present in the training datasets  Limitations: The authors should further address the significant performance gap compared to the oracle results. To mitigate this limitation they could explore additional design modifications or training techniques to bridge the performance difference. Overall the paper presents an innovative approach to fewshot segmentation with MMFormer supported by extensive experiments and insightful ablations. Further improvements could focus on enhancing the models performance and efficiency for practical applications.", "qZUHvvtbzy": " Summary The paper introduces a novel variational solution for the quantum manybody problem using symmetryprojected linear combinations of restricted Boltzmann machines. By incorporating a Lanczos recursion method the study achieves stateoftheart accuracy in representing the ground state of the Heisenberg J1 \u2212 J2 model on a square lattice.  Strengths 1. Innovative Approach: The paper presents a unique method by combining RBMs with Lanczos recursion to enhance the accuracy of variational wave functions. 2. StateoftheArt Accuracy: The results outperform previous numerical methods including convolutional neural networks in terms of accuracy for the Heisenberg model. 3. Comprehensive Analysis: The study includes detailed descriptions of the method formalism and implementation details providing a thorough understanding of the proposed approach.  Weaknesses 1. Computational Cost: The computational complexity could be a limiting factor especially for larger systems impacting the scalability of the method. 2. Error Analysis: While the paper demonstrates accurate results for ground state energy more insights into the error analysis particularly for correlation functions may enhance the completeness of the study.  Questions and Suggestions 1. Scalability: How does the computational cost scale with the system size Are there strategies to mitigate the computational burden for larger lattices 2. Error Analysis: Could you provide more insights into the errors associated with correlation functions and any strategies to improve their accuracy 3. Comparison: How does the proposed method compare to other stateoftheart techniques such as tensor product states in terms of accuracy and computational efficiency  Limitations The authors should address the computational cost and error analysis for correlation functions. Suggestions for improvement include optimizing computational resources and enhancing the accuracy of correlation function calculations.  Negative Societal Impact The work focuses on theoretical simulations without any observable negative societal impacts. This review highlights the innovative approach accuracy and computational considerations of the paper while suggesting further insights into error analysis and scalability for enhanced completeness.", "yts7fLpWY9G": " Summary: The paper investigates the effectiveness of adaptive readout functions in graph neural networks for learning graph structured data representations. It challenges the standard permutation invariant readout functions by proposing neural readouts and demonstrates the benefits in various domains. The study covers extensive experiments on more than 40 datasets showing consistent improvements over standard readouts.  Strengths: 1. Thorough Experimental Evaluation: The paper provides a comprehensive evaluation over multiple datasets and convolutional operators demonstrating the effectiveness of the proposed adaptive readouts. 2. Innovation: The introduction of adaptive readout functions challenges the standard permutation invariant readouts and presents a novel approach to learning graphlevel representations. 3. Relevance: The study explores practical implications in various domains and shows promise for enhancing graph neural network capabilities on challenging tasks with large datasets.  Weaknesses: 1. Limited Abstraction: The paper could benefit from more concise and clear explanations especially in the Experimental Results section to enhance readability and accessibility. 2. Hyperparameters Tuning: It would be useful to include a discussion on hyperparameter optimization for the adaptive readouts to showcase their full potential.  Questions and Suggestions for Authors: 1. Generalizability: Have you tested the generalizability of the adaptive readouts across different graph structures and problem domains to assess their robustness 2. Comparison Metrics: Apart from R2 and MCC could you include additional evaluation metrics to provide a more comprehensive comparison between standard and neural readouts 3. Model Complexity: How does the complexity of neural readouts impact training time and memory requirements compared to standard readouts 4. Interpretability: Consider discussing the interpretability of the neural readouts and potential implications for downstream tasks to add further insights.  Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. To improve they should consider discussing ethical implications such as data biases model fairness and the impact on vulnerable populations in their future studies.", "o762mMj4XK": " Summary: The paper introduces Balanced Neural Ratio Estimation (BNRE) a variant of the Neural Ratio Estimation algorithm designed to produce more conservative posterior approximations in simulationbased inference (SBI) tasks. BNRE enforces a balancing condition on the binary neural classifier to increase the reliability of posterior estimates. The paper provides theoretical arguments supporting the approach and evaluates BNRE on various benchmarks demonstrating its effectiveness in producing conservative posterior surrogates.  Strengths: 1. Novel Contribution: The introduction of BNRE to improve the reliability of SBI algorithms is a significant contribution to the field. 2. Theoretical Rigor: The paper provides comprehensive theoretical arguments backing the proposed method. 3. Empirical Validation: Extensive experiments validate BNRE on diverse benchmarks showcasing its effectiveness in producing conservative posterior approximations. 4. Clear Structure: The manuscript is wellorganized with detailed sections outlining the background method results and related work.  Weaknesses: 1. Limitations in HighDimensional Spaces: The performance and applicability of BNRE in highdimensional parameter spaces are not thoroughly addressed. 2. Uncertainty in Balancing Condition: The paper acknowledges that the balancing condition is enforced through a regularization penalty that may not result in strictly balanced classifiers. The impact of this uncertainty on reliability is not extensively discussed. 3. Limited Guarantee: While BNRE increases reliability the authors emphasize that it cannot guarantee 100 reliability in individual inferences.  Questions and Suggestions: 1. Clarifications on Balancing Condition: Could the authors elaborate on strategies to mitigate the uncertainty in the balancing condition and its impact on the reliability of posterior estimates 2. HighDimensional Analysis: How would BNRE perform and how reliable would the posterior estimates be in scenarios with highdimensional parameter spaces 3. Robustness Testing: Have the authors explored the robustness of BNRE against model misspecification or uncertainties in the simulator 4. Comparative Analysis: Could the authors include comparisons with other approaches beyond NRE to further highlight the superiority of BNRE in producing conservative surrogates 5. Scalability: Are there any insights on how BNRE would scale with larger datasets and more computationally demanding tasks  Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work highlighting potential uncertainties in the balancing condition in small simulation budget regimes and the need for careful consideration of reliability over statistical performance. To further improve the authors could provide detailed strategies for mitigating uncertainties in the balancing condition and explore the implications of BNRE in highdimensional spaces.", "zBlj0Cs6dw1": " Summary: The paper introduces RLCG the first Reinforcement Learning (RL) approach for Column Generation (CG) a key algorithm for solving linear programs with a large number of variables. RLCG treats CG as a sequential decisionmaking problem and utilizes Graph Neural Networks (GNNs) to enhance column selection leading to faster convergence in problems like the Cutting Stock Problem (CSP) and Vehicle Routing Problem with Time Windows (VRPTW). Extensive experiments show RLCG reduces the number of CG iterations significantly compared to traditional heuristics.  Strengths: 1. Innovative Approach: Introducing RL to enhance CG is a novel and impactful contribution to the field of mathematical optimization. 2. Extensive Experiments: Thorough experimentation on benchmark problems demonstrates the effectiveness of RLCG in accelerating CG convergence. 3. Clear Methodology: The paper provides a detailed explanation of RLCG including its formulation as an MDP and training process. 4. Generalization and Curriculum: The use of curriculum learning and generalization across different problem instances strengthen the validity of the proposed approach.  Weaknesses: 1. Limited Discussion on Negative Results: Lack of discussion on any scenarios where RLCG does not outperform traditional heuristics or the expert strategy. 2. Potential Overfitting: The paper does not extensively discuss potential overfitting issues or how the model generalizes to unseen instances. 3. Scalability Concerns: Adding multiple columns per iteration for faster convergence could lead to scalability challenges requiring further exploration.  Questions and Suggestions: 1. Generalization: How does RLCG perform on unseen instances or problem domains not considered in the experiments 2. Algorithm Complexity: Can the authors discuss the computational complexity implications of scaling RLCG to larger problem instances 3. Comparison with Other RL Methods: How does RLCG compare to other RL approaches in mathematical optimization problems 4. Practical Application: Could the authors discuss realworld applications where RLCG could make a significant impact  Limitations: The authors could enhance the paper by discussing potential overfitting issues and conducting further experiments to evaluate RLCGs performance on a wider range of problem instances.  EthicalSocietal Impact: The paper does not explicitly address negative societal impacts. Authors should consider discussing ethical considerations related to the deployment of RLCG in decisionmaking systems to ensure transparency accountability and fairness in algorithmic decisionmaking.", "vt516zga8m": " PeerReview:  Summary: The paper proposes a CacheAugmented Inbatch Importance Resampling (IR) approach for training recommender retrievers aiming to reduce bias from the softmax distribution by resampling items within the minibatch. The IR method not only offers distinct negatives to user queries but also adapts the softmax estimation dynamically during training. Experimental results on five realworld datasets show that IR outperforms competitive baselines in terms of effectiveness and convergence.  Strengths: 1. Original Contribution: The proposed IR method addresses an important issue in training recommender retrievers by effectively reducing bias during softmax estimation. 2. Theoretical Justification: The paper provides a comprehensive theoretical analysis of the proposed method supporting its effectiveness in approximating the softmax distribution. 3. Experimental Validation: Extensive experiments on realworld datasets demonstrate the superior performance of IR compared to competitive approaches showcasing its practical relevance and effectiveness.  Weaknesses: 1. Lack of Comparison Metrics: While the results show improved performance in terms of NDCG10 and RECALL10 it would be beneficial to include additional metrics for a comprehensive evaluation of the proposed method. 2. Validation on Additional Datasets: To strengthen the generalizability of the proposed IR approach experiments on a wider range of datasets could further validate its efficacy across diverse scenarios.  Questions  Suggestions for Authors: 1. Scalability: How does the proposed method scale with larger datasets or in scenarios with highdimensional features 2. Implementation Details: Can you provide more insights into the computational complexity and efficiency of the proposed IR approach compared to existing methods 3. Parameter Sensitivity: Have you performed sensitivity analysis on hyperparameters to demonstrate the robustness of the proposed method across different settings  Limitations: The authors have adequately addressed the limitations by conducting experiments on realworld datasets and providing theoretical justifications for the proposed method. To further enhance the study considering the above suggestions could contribute to a more comprehensive evaluation.  Suggestions for Improvement: 1. Additional Evaluation Metrics: Include metrics such as precision F1score or AUC to provide a holistic view of the methods performance. 2. Extension to Other Domains: Explore the applicability of the proposed approach in domains beyond recommender systems to demonstrate its versatility. 3. OpenSource Implementation: Consider releasing the code implementation of the proposed IR method to enable reproducibility and facilitate further research in the field. Overall the paper presents a promising approach for improving recommender retrievers backed by theoretical analysis and empirical results. Addressing the above aspects could further enrich the study and enhance its contribution to the research community.", "kyY4w4IgtM8": " Summary: The paper introduces a new metalearning method that leverages natural language feature descriptions to improve predictive performance on unseen tasks with limited labeled data. The proposed method uses BERT encoding to incorporate human knowledge from large text corpora into metalearning and achieves better performance than existing methods on realworld datasets.  Strengths: 1. Innovative Approach: Introducing feature descriptions in metalearning is a novel and significant contribution. 2. Empirical Validation: The method is supported by substantial experiments using realworld datasets. 3. Clear Presentation: The paper is wellstructured and effectively explains the proposed method enabling easy comprehension.  Weaknesses: 1. Lack of Comparisons: While comparisons with existing methods are provided a more detailed comparison and analysis could further strengthen the paper. 2. Abstract Terminology: Some technical terms might be challenging for readers unfamiliar with metalearning.  Questions and Suggestions: 1. Validation on Other Tasks: Have you considered testing the method on different domains to verify its generalizability 2. Evaluation Metrics: Are there specific metrics used to assess the translation quality from BERT sentence embeddings to feature embeddings 3. Impact of Dataset Size: How does the size of the metatraining datasets affect the performance of the proposed method  Limitations: The limitations of the paper include the need for additional empirical evaluation on variant methods for different tasks and exploring the usage of alternative neural network architectures for improved performance. This peer review provides feedback on the strengths weaknesses and potential areas for improvement in the paper. The work is commended for its novel approach and empirical validation with suggestions provided to enhance the clarity and robustness of the research.", "lgNGDjWRTo-": " PeerReview  Summary: The paper proposes a new deep generative model called PeriodicalGraph Disentangled Variational Autoencoder (PGDVAE) for generating periodic graphs. The model aims to disentangle and generate local and global graph patterns automatically. The proposed method is evaluated through extensive experiments on realworld and synthetic datasets showing promising results in generating diverse and novel periodic graphs.  Strengths: 1. Innovative Contribution: The paper addresses the challenging task of generating periodic graphs by proposing a novel deep generative model PGDVAE which disentangles local and global semantics. 2. Novel Model Architecture: The architecture of PGDVAE incorporating a localpattern encoder globalpattern encoder and decoder components is welldesigned and aligns with the problem requirements. 3. Comprehensive Experiments: Extensive experimental evaluations have been conducted using realworld and synthetic datasets to demonstrate the effectiveness and efficiency of the proposed method. 4. Scalability: The scalability analysis comparing PGDVAE and other stateoftheart models shows that PGDVAE has lower time complexity and space complexity making it a suitable choice for generating large periodic graphs.  Weaknesses: 1. Limited Performance Comparison: While the paper includes comparison with existing models a more indepth analysis and comparison with various metrics would enhance the validation of the proposed PGDVAE model. 2. Lack of Interpretability Analysis: The paper could benefit from providing more insights into how local and global graph patterns are disentangled and generated in different scenarios. 3. Complexity: The model architecture and training objectives might be complex for readers not familiar with deep generative models and graph representation learning techniques.  Questions and Suggestions for Authors: 1. How does the disentanglement process in PGDVAE ensure that the local and global patterns are accurately separated in the generated graphs 2. Can the authors provide a more detailed analysis on the interpretability of the latent representations learned by PGDVAE 3. How does PGDVAE perform in scenarios where the underlying graph structure is highly complex such as in molecular simulations with intricate connectivity patterns 4. Can the authors elaborate on any potential challenges or limitations faced during the model development and experimental evaluation  Limitations: While the paper delves into the modeling of periodic graphs using PGDVAE it could further address the limitations and societal impacts of the work. Suggestions for improvement include considering potential negative implications ethical considerations and broader societal impacts that may arise from the application of the proposed method. In conclusion the paper presents a novel approach for generating periodic graphs showcasing promising results through extensive experiments. Further enhancing the interpretability and providing a deeper analysis of model performance would strengthen the papers contribution to the research community.  Acknowledgments and Funding: The authors acknowledge the funding support provided by various sources including NSF grants Meta Research Award industry affiliations and GPU grants. This diverse range of funding sources highlights the collaborative nature and industry relevance of the research work.", "qVtbqSwOxy6": " Peer Review:  Summary: The paper introduces a novel framework Fast MultiView AnchorCorrespondence Clustering (FMVACC) to address the AnchorUnaligned Problem (AUP) in multiview anchor graph clustering. It focuses on establishing correct correspondences between anchor sets across views to enhance clustering performance. The proposed FMVACC method involves feature and structure information to align anchors and fusion graphs columnwisely. The paper showcases theoretical connections with existing methods and conducts comprehensive experiments on benchmark datasets to validate the effectiveness and efficiency of FMVACC.  Strengths: 1. Innovative Contribution: The paper addresses a crucial issue the AnchorUnaligned Problem in multiview anchor graph clustering and proposes a novel FMVACC framework to tackle it effectively. 2. Comprehensive Experiments: The authors conducted extensive experiments on benchmark datasets to demonstrate the superiority of their proposed method over existing stateoftheart algorithms. 3. Theoretical Analysis: The theoretical analysis provided on the FMVACC framework enhances the understanding of the proposed method and demonstrates its generality compared to existing strategies. 4. Efficiency: The proposed FMVACC algorithm showcases linear space and time complexity making it suitable for largescale applications. 5. Alignment Module: The effectiveness of the anchor alignment module is welldemonstrated through experiments highlighting the importance of correct correspondences for improved clustering performance.  Weaknesses: 1. Complex Terminology: The paper contains complex technical terms and mathematical expressions making it less accessible to readers with a basic understanding of the topic. Simplifying the language and providing more intuitive explanations could enhance readability. 2. Lack of Comparison Metrics: While the paper compares the proposed method with existing algorithms more detailed comparisons based on additional metrics or realworld case studies could provide a more comprehensive evaluation. 3. Limitations Clarity: The limitations of the proposed method in realworld scenarios or negative societal impacts are not explicitly discussed. Including these aspects could provide a more wellrounded evaluation.  Questions and Suggestions for Authors: 1. Can you provide more insights into the practical implications of FMVACC in realworld scenarios or specific applications 2. How does the proposed method handle noisy or incomplete data in multiview settings Is there a robustness analysis regarding data quality 3. Have you considered the interpretability of the clustering results obtained using FMVACC How can users interpret and utilize the generated clusters effectively  Limitations: The authors could enhance the discussion by explicitly addressing the limitations of FMVACC in realworld applications. Constructive recommendations for improving robustness interpretability or addressing potential negative impacts would add value to the paper.", "syU-XvinTI1": "Summary: The paper critiques recent deep learningbased models of grid cells in the entorhinalhippocampal circuit showing that they may not provide novel insights about neural circuits but rather reflect specific implementation choices. The authors argue that deep learning models in neuroscience require more circumspection transparency and biological knowledge for effective interpretation. Strengths: 1. The paper effectively challenges the overexuberant claims made by deep learningbased models in neuroscience. 2. Rigorous evaluation through largescale architectural and hyperparameter sweeps enhances the credibility of the findings. 3. Clear presentation of the methodology and results making it easy to follow the arguments. 4. The paper promotes cautious interpretation and understanding of the limitations of deep learning models in neuroscience research. Weaknesses: 1. While the paper highlights the limitations of current deep learning models it could provide more concrete suggestions for improving the interpretability and reliability. 2. The evaluation is focused on a specific case study of grid cells which may limit the generalizability of the findings to other neural circuits. 3. Lack of discussion on the potential positive implications of deep learning models in neuroscience beyond critiquing their limitations. Questions and Suggestions: 1. Can the authors elaborate on how their findings on grid cells can guide the development of more biologically plausible deep learning models for other neural circuits 2. How can researchers incorporate inductive biases into deep learning models to improve their reliability and applicability to neuroscience research 3. Are there potential applications or areas of neuroscience research where deep learning models have shown promise despite the limitations discussed in the paper Limitations: The authors could further address the potential negative societal impact by emphasizing the importance of ethical considerations and responsible deployment of deep learning models in neuroscience including transparency in model development and interpretation and ensuring that the public understands the limitations of such models. In conclusion the paper provides valuable insights into the limitations of deep learning models in neuroscience and emphasizes the need for a more nuanced and informed approach to their development and interpretation. Further discussion on the implications of the findings for future research directions and broader applications would enhance the impact of the paper.", "rg_yN3HpCp": " Summary: The paper introduces a novel approach called Graph Labelinvariant Augmentation (GLA) for graph contrastive learning. The goal is to address the limitations of current methods in generating effective augmentations for graph data without violating the labelinvariant assumption. GLA conducts augmentation in the representation space to improve model generalization achieving better performance compared to existing graph contrastive learning methods in semisupervised scenarios across eight benchmark datasets.  Strengths: 1. Innovative Approach: The paper introduces a unique method of conducting labelinvariant augmentation in the representation space for graph data addressing a critical issue in graph contrastive learning. 2. Empirical Experiments: The authors provide extensive experimental results on benchmark datasets demonstrating the effectiveness of GLA in improving generalization and outperforming existing methods. 3. Thorough Exploration: The paper goes beyond performance evaluation to explore factors like negative pairs augmentation space and strategy providing a deeper understanding of the proposed method.  Weaknesses: 1. Limited Comparison: While the paper compares GLA with existing methods a more detailed comparison on a wider range of datasets or tasks could further strengthen the validation of GLAs effectiveness. 2. Lack of Theoretical Analysis: The paper could benefit from a more indepth theoretical analysis of why the proposed method works providing deeper insights into the underlying principles and mechanisms. 3. Clarity: Some sections are dense with technical details making it challenging for readers not familiar with the domain to follow the explanations easily.  Questions and Suggestions for Authors: 1. Have you considered the scalability of GLA to larger and more complex graph datasets How does the method perform in scenarios with significantly larger graphs or higherdimensional feature spaces 2. Can you provide insights into the computational overhead of implementing GLA compared to existing graph contrastive learning methods especially when dealing with a large number of graphs or augmentations 3. How does the randomness in choosing the most difficult augmentation impact the stability and reliability of the training process Have you explored strategies to mitigate potential issues related to randomness  Limitations: The authors have adequately addressed the limitations in the proposed method that rely on the quality of the decision boundary indicated by the downstream classifier. However it would be beneficial to provide additional analysis on the robustness of the method under various decision boundary qualities.  Suggestions for Improvement: 1. Incorporate a broader range of datasets and tasks to further validate the effectiveness and robustness of GLA. 2. Enhance the theoretical explanations underlying the design choices and workings of GLA to provide a better understanding for readers. 3. Consider refining the presentation of the technical details to improve clarity and accessibility for a wider audience.", "tJBYkwVDv5": " Summary: The paper proposes a new algorithm Spectrum Thresholding Variance Estimator (STVE) for estimating variance parameters in dynamic linear regression problems. It introduces the first finite sample complexity guarantees for such estimators by analyzing the spectrum of the global system operator. The paper provides indepth theoretical analysis an algorithmic implementation and experimental validation on synthetic and realworld data.  Strengths: 1. Novel Algorithm: The introduction of STVE is a significant contribution to variance estimation in dynamic regression addressing a gap in existing literature. 2. Theoretical Analysis: The paper offers a rigorous theoretical foundation with detailed proofs and bounds supported by mathematical derivations. 3. Experimental Validation: The validation on synthetic and realworld data sets enhances the credibility of the proposed algorithm. 4. Clear Presentation: The paper is wellstructured and the results are presented coherently with suitable references to supplementary material.  Weaknesses: 1. Technical Complexity: The mathematical formalism and derivations might be challenging for nonexperts to grasp potentially limiting the accessibility of the paper. 2. Experimental Details: While experiments are conducted more comprehensive comparisons with existing methods and additional realworld datasets could have strengthened the experimental evaluation.  Questions and Suggestions: 1. Interpretability Metrics: Have interpretability metrics been considered for assessing the performance of the STVE algorithm particularly in realworld applications where explainability is crucial 2. Scalability: How does the algorithms computational complexity scale with large datasets Are there strategies to enhance scalability for practical applications 3. Hyperparameters Sensitivity: How sensitive is the algorithms performance to hyperparameter choices such as the cutoff parameter `p` Have sensitivity analyses been conducted 4. Deployment Considerations: Are there any considerations or limitations for deploying STVE in realtime or resourceconstrained environments What are the key factors to consider in practice  Limitations: The authors adequately addressed the limitations and discussed varying data characteristics and assumptions to account for missing or outlier values. To further improve incorporating a discussion on realtime processing constraints and potential negative societal impacts would enhance the completeness of the study. A thorough sensitivity analysis and interpretability metrics can provide deeper practical insights.", "wVc4Qg5Bhah": " Paper Summary: The paper introduces a universal and adaptive secondorder optimization algorithm called EXTRANEWTON for minimizing secondorder smooth convex functions. The algorithm achieves global convergence rates of O(\u03c3g \\xe2\\x88\\x9a T  \u03c3HT(32)  1T(3)) adapting to the variance in the gradient and Hessian oracles. EXTRANEWTON is parameterfree oblivious to problemspecific parameters and incorporates a novel adaptive stepsize mechanism. The algorithms versatility allows it to handle stochastic and deterministic oracles interchangeably without prior knowledge.  Strengths: 1. Novelty: EXTRANEWTON introduces a pioneering approach to universal and adaptive secondorder optimization bridging the gap in the literature on such algorithms. 2. Theory and Analysis: The paper provides a comprehensive theoretical framework including convergence guarantees and mathematical proofs establishing the algorithms efficacy. 3. Adaptive StepSize: The adaptive stepsize mechanism is a significant strength enabling the algorithm to adjust to noise levels in the oracles dynamically. 4. Universal Application: The ability of EXTRANEWTON to handle different types of oracles and converge efficiently under varying conditions is a standout feature.  Weaknesses: 1. Complexity: The paper contains technical jargon and elaborate mathematical derivations potentially making it less accessible to readers unfamiliar with advanced optimization concepts. 2. Implementation: The explicit algorithm derived from the implicit method may still pose challenges in realworld implementations due to the complexity of the optimization process. 3. Comparison: While the paper presents comparative results with other algorithms a more extensive comparison with a broader range of optimization methods could enhance the evaluation.  Questions and Suggestions for Authors: 1. Could you discuss the practical implications of implementing EXTRANEWTON in realworld optimization scenarios How does its performance translate to settings with noisy oracles and largescale problems 2. Further elaboration on the computational complexity and practical scalability of EXTRANEWTON would enhance readers understanding of its applicability in realistic optimization tasks. 3. Can you provide insights into the robustness of the algorithm under varying conditions such as nonconvex problems or different levels of noise in the oracle feedback 4. It would be valuable to include discussions on potential extensions or modifications to EXTRANEWTON that could improve its convergence rates or applicability in specific optimization domains.  Limitations: The paper does not extensively address the practical implementation challenges or the computational overhead associated with the proposed algorithm. It is essential to provide more insights into the feasibility of applying EXTRANEWTON in realworld optimization tasks to enhance its relevance and adoption.  Constructive Suggestions for Improvement: Authors could consider including more practical examples experimental results on larger datasets or realworld problems and a discussion on the computational efficiency of EXTRANEWTON to better illustrate its advantages and limitations in practical optimization scenarios. Additionally a more detailed comparison with existing stateoftheart algorithms would help establish the algorithms competitiveness and effectiveness in diverse optimization settings.", "oNWqs_JRcDD": "Summary: The paper introduces a novel framework for efficient scalable and generalizable optimizer search with the aim of democratizing research and application of automated optimizer design. The framework organizes the optimizer search space as a supertree of mathematical expressions allowing for pathfinding traversal methods to be used for search. The proposed Monte Carlo Samplingbased algorithm is equipped with rejection sampling and equivalentform detection to boost sample efficiency. The framework is evaluated across various tasks like digit and image classification adversarial attack graph learning and BERT finetuning outperforming humandesigned and prior optimizer search methods with a fraction of the search budget. Strengths: 1. Innovative Approach: The paper presents a novel and innovative framework for optimizer search that organizes the search space into a supertree based on mathematical expressions. 2. Efficiency: The proposed framework demonstrates high efficiency scalability and generalizability by surpassing humandesigned optimizers and prior search methods with only 128 evaluations. 3. Benchmark and Evaluation: The paper provides comprehensive evaluations on a diverse set of tasks showcasing the effectiveness of the proposed framework across various domains like image classification graph learning and NLP. 4. Sample Efficiency: The introduction of rejection sampling and equivalentform detection techniques enhances sample efficiency enabling the framework to efficiently navigate the search space. Weaknesses: 1. Lack of Comparison: While the paper compares the proposed framework against humandesigned and prior methods a direct comparison with other stateoftheart optimizer search frameworks could further establish the superiority of the proposed approach. 2. Complexity: The framework may be complex for practitioners or researchers unfamiliar with tree traversal methods or Monte Carlo Sampling potentially limiting its adoption outside of specialized research areas. 3. Limited Generalizability: While the framework demonstrates success across multiple tasks there could be challenges in applying it to novel or unconventional optimization problems not covered in the evaluation. Questions and Suggestions: 1. How does the proposed framework handle highdimensional or noncontinuous search spaces such as those encountered in reinforcement learning tasks or complex optimization landscapes 2. Can the authors provide insights on the computational requirements of the framework particularly in terms of memory usage and runtime to facilitate practical implementation on diverse computing platforms 3. Have the authors considered the interpretability of the discovered optimizers and their implications for understanding the underlying optimization strategies Limitations: The authors have partially addressed the limitations by proposing rejection sampling and equivalentform detection techniques. To further enhance the frameworks robustness the authors could explore methods for handling noisy or incomplete data during search. Overall the paper presents a promising framework for automated optimizer design with notable strengths in efficiency and sample efficiency. Further enhancements and broader validation could solidify the frameworks impact and applicability in diverse optimization domains.", "oprTuM8F3dt": " Peer Review  Summary: The paper proposes a novel coordinatebased model CoCoINR for implicit neural 3D representation by injecting codebook prior information into the network. The model consists of two attention modules: codebook attention and coordinate attention to enhance feature representation and improve rendering quality with fewer input views. Experimental results on various datasets demonstrate the robustness and fine detailpreserving capability of CoCoINR compared to existing methods.  Strengths: 1. Innovative Approach: The integration of codebook prior information into the network is a novel approach that enhances feature representation and improves rendering quality under sparse input views. 2. Thorough Experiments: Extensive experiments on different datasets demonstrate the effectiveness and robustness of the proposed CoCoINR showcasing its superiority over existing methods. 3. Clear Presentation: The paper is wellstructured providing a comprehensive explanation of the methodology implementation details and experimental results making it easy to follow.  Weaknesses: 1. Core Modules Clarity: While the paper describes the codebook attention and coordinate attention modules the details of their implementation and impact on the models performance could be elaborated for better understanding. 2. Comparison Metrics: The paper primarily focuses on PSNR SSIM and LPIPS as evaluation metrics. Including additional metrics or comparative analyses with other stateoftheart approaches could provide a more comprehensive evaluation.  Questions and Suggestions: 1. How does the integration of codebook prior information impact the overall computational complexity and training time of the model 2. Can the authors provide more insights into the interpretability of the codebook attention and coordinate attention modules in relation to scenespecific features 3. Have the authors considered potential realworld applications of CoCoINR beyond scene reconstruction and novel view synthesis  Limitations: While the paper addresses the limitations of existing methods and proposes an innovative solution more indepth discussions on the potential negative societal impact of deploying such advanced technologies should be considered. Suggestions for improvement could include ethical considerations bias mitigation strategies and implications for privacy and security. Overall the paper presents a novel and promising approach in implicit neural 3D representation that demonstrates improved performance under sparse input views. Addressing the weaknesses and limitations highlighted above will further enhance the quality and impact of the research.  This peer review provides feedback on the strengths weaknesses questions suggestions and limitations of the paper aiming to help enhance the quality of the proposed approach in implicit neural 3D representation.", "qSYVigfakqS": " Peer Review Summary: The paper introduces a novel method named SimFormer for weakshot semantic segmentation where novel classes are learned from cheaper imagelevel labels with the support of base classes having pixellevel labels. The proposed method disentangles the segmentation task into two subtasks and performs dual similarity transfer to learn proposalpixel and pixelpixel similarities along with a complementary loss to facilitate the learning of novel classes. Extensive experiments on challenging datasets demonstrate the effectiveness of the method. Strengths: 1. Innovative Contribution: The paper addresses an important and challenging problem in semantic segmentation proposing a novel dual similarity transfer framework. 2. Thorough Experiments: Extensive experiments are conducted on challenging datasets showcasing the effectiveness and practicality of the proposed method. 3. Clear Methodological Description: The paper provides detailed explanations of the proposed method making it easier for readers to understand the technical aspects. Weaknesses: 1. Complexity: The method involves multiple components (proposalpixel similarity transfer pixelpixel similarity transfer complementary loss) which might make it complex for implementation and understanding. 2. Generalization: The paper could provide more insights into how the proposed method generalizes to different datasets or scenarios beyond the ones used in the experiments. 3. Failure Cases: While the limitations are discussed providing more indepth analysis of failure cases would further enhance the papers discussion. Questions and Suggestions: 1. Scalability: How scalable is the proposed method when the number of classes increases significantly beyond the datasets used in the experiments 2. Robustness: How robust is the method to noise or errors in the imagelevel labels of novel classes especially in realworld scenarios 3. Comparison Metrics: Apart from mean IoU of novel classes are there any other evaluation metrics used to measure the methods performance comprehensively Limitations: The authors have addressed the limitations of the proposed method regarding failure cases and scalability. However further exploration into the generalization of the method and the impact of potential negative societal implications could strengthen the paper. Overall the paper presents a novel approach to weakshot semantic segmentation with promising results. Enhancing the discussion on failure cases scalability and generalization would amplify the impact and comprehensiveness of the work.", "pNEisJqGuei": "Peer Review: Summary: The paper introduces a novel approach of integrating value decomposition into actorcritic algorithms to aid in the iterative design process of reinforcement learning agents. The proposed SACD algorithm is demonstrated to maintain performance while learning a larger set of value predictions. The paper includes demonstrations of value decompositions use in identifying and addressing problems in both environment and agent design along with introducing new training methods leveraging the decomposed structure. Strengths: 1. Innovative Approach: The integration of value decomposition into actorcritic algorithms is a novel and promising approach to assist in the agentdesign process. 2. Comprehensive Demonstration: The paper provides a thorough demonstration of the utility of value decomposition in diagnosing and addressing challenges in RL environments and agents. 3. Performance Evaluation: The comparison with standard SAC algorithm on benchmark environments and the introduction of SACDCAGrad for optimization provide valuable insights. 4. Clarity and Structure: The paper is wellorganized with clear explanations of concepts and methodologies aiding readability and understanding. 5. Applications in Agent Design: The examples provided for diagnosing insufficient features mitigating value errors and adjusting component weights showcase practical applications of value decomposition. 6. Contributions and Extensions: The paper builds upon existing literature on explainable reinforcement learning and extends the utility of value decomposition in multiple dimensions. Weaknesses: 1. Simplification of Scenarios: The paper simplifies scenarios by considering linear reward decomposition and learning challenges in controlled environments which may limit realworld applicability and generalization. 2. Performance Comparison: While SACD shows promising results compared to SAC the paper could provide a more indepth analysis of dynamic performance across various scenarios. 3. Theoretical Emphasis: The focus on mathematical descriptions and algorithms might hinder broader accessibility for readers with less technical background. 4. Realworld Deployment: The impact of value decomposition on realworld deployment and scalability aspects could be further explored to understand practical implications better. Questions and Suggestions for Authors: 1. Can you elaborate on how the proposed method could be extended to handle nonlinear reward decomposition and more complex environments 2. It would be beneficial to discuss potential challenges in scaling up the approach to larger problems and deploying it in practical applications. How does the method handle scalability issues 3. Have you considered the interpretability of the value estimates in realworld decisionmaking contexts and potential biases they might introduce Can you suggest ways to mitigate these risks 4. Could you provide insights on the ease of implementation and computational requirements for incorporating value decomposition into existing RL workflows Constructive Suggestions for Improvement: 1. Provide a more detailed analysis of SACDs performance in varied scenarios to establish the algorithms generalizability and robustness. 2. Expand on the limitations section to address potential negative societal impacts more explicitly and offer actionable recommendations for mitigating biases and inaccuracies. 3. Consider incorporating a case study or practical demonstration to showcase the application of value decomposition in a realworld RL problem enhancing the papers practical relevance. Limitations: While the paper discusses limitations of the proposed method there could be more explicit acknowledgment and discussion of potential negative societal impacts including reinforcing biases or incorrect conclusions. Authors could consider providing more detailed guidelines for addressing these limitations in practice for ethical deployment.", "tbdk6XLYmZj": " PeerReview of \"Learning Best Combination for Efficient N:M Sparsity\"  Summary: The paper introduces a novel approach named Learning Best Combination (LBC) to achieve efficient N:M sparsity in neural networks. The key idea behind LBC is to treat the N:M sparsity problem as a combinatorial problem and optimize the selection of the best combination of weights by assigning learnable scores to each combination. The proposed method aims to reduce the training burden associated with N:M sparsity by gradually removing lowscored combinations during training. Extensive experiments demonstrate the superiority of LBC over existing N:M sparsity methods in terms of accuracy and efficiency across various tasks.  Strengths: 1. Innovative Approach: The paper presents a unique perspective on N:M sparsity as a combinatorial problem leading to the development of the LBC method which shows promising results compared to existing techniques. 2. Efficiency and Flexibility: LBC demonstrates efficiency in optimizing N:M sparsity without the need for heavy pretraining or densegradient calculations. The proposed method allows for joint optimization of weights and scores improving both training efficiency and performance. 3. Comprehensive Experimental Evaluation: The paper includes detailed experiments across multiple computer vision tasks showcasing the effectiveness of LBC in achieving high performance with reduced training costs compared to stateoftheart methods.  Weaknesses: 1. Limited Generalization: The paper mainly focuses on N:M sparsity in computer vision tasks limiting the generalizability of the proposed method to other domains. Future work could explore the applicability of LBC in tasks beyond computer vision such as natural language processing. 2. Early Training Stage Computation: The paper acknowledges that LBC still requires relatively dense computation in the early training stages due to the presence of numerous candidate subsets which could be a limitation in terms of efficiency.  Questions and Suggestions for Authors: 1. Scalability: Have the authors considered the scalability of LBC to larger and more complex neural network architectures beyond the ones tested in the experiments How does the method perform in scenarios with significantly larger models 2. Robustness: How robust is the LBC method to variations in dataset characteristics hyperparameters and network architectures Further insights into the robustness of LBC across different experimental settings would strengthen the credibility of the approach. 3. Comparison Metrics: Are there any additional metrics or benchmarks that could provide a more comprehensive evaluation of LBC in comparison to existing methods Consider discussing additional performance metrics beyond accuracy and training efficiency.  Limitations: Regarding limitations while the authors have provided a thorough exploration of the benefits and limitations of their proposed method they could further address the scalability aspects generalization beyond computer vision tasks and potential improvements in the efficiency of LBC in handling earlystage training computations in future work.  Conclusion: Overall the paper presents a novel and effective approach LBC for achieving efficient N:M sparsity in neural networks. The method shows promise in reducing training burdens while maintaining high performance across different tasks. Addressing the suggested questions and limitations can further enhance the robustness and applicability of LBC in various machine learning applications.", "zK6PjBczve": " Summary The paper introduces NeurHap a novel method for haplotype assembly in polyploid species and viral quasispecies. NeurHap combines graph representation learning with combinatorial optimization to improve the accuracy of haplotype assembly compared to existing approaches. Experimental results on both synthetic and real datasets demonstrate the superior performance of NeurHap.  Strengths  Innovative Approach: NeurHap proposes a unique formulation of the haplotype phasing problem as a graph coloring problem demonstrating a novel and effective method.  Performance: The experiments show that NeurHap outperforms stateoftheart baselines significantly in both polyploid species and viral quasispecies datasets.  Visualization: The visualization provided offers a clear and informative understanding of the phasing results showcasing the effectiveness of NeurHap.  Ablation Study: Conducting an ablation study to analyze the individual contributions of NeurHapsearch and NeurHaprefine provides valuable insights into the model components.  Weaknesses  Long Read Limitation: The paper acknowledges a limitation in handling long reads which could impact the application in scenarios where long reads are prevalent.  Autodiscovery of Haplotypes: NeurHap lacks the ability to automatically discover the number of haplotypes which could limit its adaptability to different datasets.  Questions and Suggestions 1. Scalability: How does NeurHap scale with increasing dataset sizes or complexities 2. Parameter Sensitivity: What sensitivity analysis was conducted on key parameters like lambda number of iterations and feature dimensions 3. Comparison to Traditional Methods: How does NeurHap compare to traditional combinatorial optimization methods in terms of accuracy and efficiency  Limitations The authors could improve the paper by addressing the limitations related to handling long reads and lack of automatic haplotype discovery. Providing potential solutions or directions for future research would enhance the completeness of the study.", "mxzIrQIOGIK": "Peer Review: Summary: The paper presents a systematic study on multiobjective online learning introducing a new framework called MultiObjective Online Convex Optimization (MOOCO) and a novel algorithm named Doubly Regularized Online Mirror Multiple Descent (DROMMD). The work includes theoretical analysis algorithm design regret bounds derivation and extensive experiments to demonstrate the effectiveness of the proposed algorithm. Strengths: 1. Contribution: The paper introduces a new framework and algorithm design for multiobjective online learning addressing a significant gap in the field. 2. Theoretical Foundation: The authors provide a thorough theoretical analysis including regret bounds derivation enhancing the credibility of their proposed algorithm. 3. Experimental Evaluation: Extensive experiments on both simulation and realworld datasets showcase the superiority of the DROMMD algorithm over baselines providing empirical evidence for its effectiveness. Weaknesses: 1. Complexity of Analysis: The nontrivial transformation for dynamic regret equivalence may pose challenges for readers to grasp the intricacies of the theoretical analysis. 2. Limited Discussion on Negative Impacts: While limitations are briefly mentioned a more indepth discussion on potential negative societal impacts of the work is recommended. Questions and Suggestions for Authors: 1. Can you provide more intuitive explanations or examples to help readers understand the complexities of the theoretical analysis especially the transformation for dynamic regret equivalence 2. Have you considered incorporating more detailed discussions around the potential negative societal impacts of the work and addressing any ethical considerations 3. How does the proposed algorithm handle scenarios with nonconvex optimization tasks in practical applications Providing insights into the generalizability of DROMMD to nonconvex settings would enhance the papers applicability. Limitations: While the authors briefly mention the limitations of their work a more thorough discussion on potential negative societal impacts and ethical considerations is recommended for more comprehensive coverage. Additionally providing more practical insights on the generalizability of the proposed algorithm to nonconvex settings would be beneficial. Overall the paper presents a novel and thorough investigation on multiobjective online learning with a solid theoretical foundation and promising experimental results. Addressing the above suggestions could further enhance the clarity and impact of the work.", "s_mEE4xOU-m": " Summary The paper explores the network load balancing problem in data centers using the multiagent reinforcement learning (MARL) framework. It addresses challenges such as limited observations dynamic environments and heterogeneous hardware. The proposed MARL algorithm aims to approximate the Nash equilibrium of a Markov potential game to optimize workload distribution.  Strengths 1. Novel Approach: The paper introduces a new distributed MARL algorithm for load balancing in data centers emphasizing potential games and fairnessbased rewards. 2. Realworld Evaluation: Both simulation and realworld experiments are conducted to evaluate the algorithms performance showcasing its effectiveness. 3. Theoretical Justification: Theorems and propositions provide theoretical backing for the proposed approach enhancing the papers academic rigor.  Weaknesses 1. Limited Comparative Analysis: While the proposed algorithm outperforms existing methods a more extensive comparison with a broader range of algorithms could strengthen the analysis. 2. Complexity: The papers technical content may be challenging for readers unfamiliar with reinforcement learning and network load balancing concepts necessitating more explanations for clarity.  Questions and Suggestions 1. Generalization: How can the proposed algorithms performance be generalized to different types of networks beyond the specific data center setup considered in this study 2. Fairness Metric: How robust is the fairnessbased reward function proposed in various workload scenarios especially under extreme load conditions 3. Scalability: What considerations have been made to ensure the proposed distributed MARL algorithm is scalable to much larger and more complex network environments in data centers  Limitations The paper adequately addresses the limitations and potential negative societal impact of their work providing a solid foundation for future improvements. To enhance the paper incorporating more insights on practical implementation challenges and potential ethical considerations could further improve its overall impact.", "s71h4wo9bFI": " 1. Summary The paper presents a study on optimal Bayesian data acquisition mechanisms for estimating the mean of a distribution from privacyconscious users. Users have varying sensitivities towards local and central privacy losses. The platform aims to design mechanisms to offer personalized data acquisition while minimizing estimation error and compensating users for privacy losses.  2. Assessment of Strengths and Weaknesses Strengths:  Theoretical Foundations: The paper establishes minimax lower bounds and optimality results for linear estimators showcasing a strong theoretical foundation.  Innovative Framework: Introducing a framework for data acquisition mechanisms considering heterogeneous privacy concerns is novel and valuable in the era of heightened privacy awareness.  Algorithm Development: Proposing a Polynomial Time Approximation Scheme for optimizing data acquisition mechanisms demonstrates computational significance. Weaknesses:  Complexity: The formulation involving nonconvex optimization may limit practical implementation and scalability in realworld applications.  Assumptions: Reliance on assumptions such as fixed functions for estimating privacy losses may oversimplify the model and lack flexibility.  3. Questions and Suggestions  Clarifications: Can the paper elaborate on how the proposed mechanism addresses incentive compatibility and individual rationality constraints effectively  Scalability: How adaptable is the proposed algorithm to larger datasets and more complex scenarios beyond mean estimation  Practical Implications: How does the proposed mechanism compare to existing privacypreserving mechanisms in realworld data markets  4. Limitations The authors should address the potential negative societal impact of the developed mechanisms such as reinforcing data asymmetry or compromising privacy through inadequately optimized schemes. Suggestions for improvement may include incorporating more robust privacy protection measures or considering ethical implications in the design process.", "ytnwPTrpl38": " Summary: In this paper the authors present a method called GeneRAlizing by DiscovERing (GRADER) that augments GoalConditioned Reinforcement Learning (GCRL) with a causal graph structure to improve generalization and reasoning capabilities. They propose a variational likelihood maximization framework with the causal graph as latent variables to solve the GCRL problem. The authors conduct experiments on nine tasks across three environments to demonstrate the effectiveness of their method against strong baselines.  Strengths: 1. Innovative Approach: The integration of causal reasoning with reinforcement learning for generalization is novel and shows promising results. 2. Theoretical Foundation: The authors provide a solid theoretical background for their method including uniqueness of the causal graph identification and performance guarantees. 3. Comprehensive Experiments: The paper conducts thorough experiments on multiple tasks and environments to demonstrate the effectiveness of the proposed method. 4. Data Efficiency: GRADER shows high data efficiency in learning the causal graph and achieving better tasksolving performance compared to baselines.  Weaknesses: 1. Scalability: The explicit estimation of causal structure may be limited in scaling to a higher number of nodes. 2. Abstraction Limitations: The factorized state and action space assumption might restrict the applicability of the method to semantic representations only.  Questions and Suggestions: 1. Scalability Improvement: How do you plan to address the scalability limitation of explicit estimation of the causal structure to accommodate a larger number of nodes 2. Abstraction Flexibility: Could you discuss potential approaches to relax the factorized state and action space assumption to make the method applicable to nonsemantic representations 3. Further Evaluation: Have you considered evaluating the model performance on more complex and largerscale environments to test the methods scalability and applicability to realworld scenarios  Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. It would be beneficial to include a section addressing this aspect highlighting any potential ethical concerns and providing recommendations for alleviating negative impacts.", "nyBJcnhjAoy": " Summary: The paper introduces ProtoX a prototypebased posthoc policy explainer that aims to provide humanunderstandable explanations for blackbox reinforcement learning agents working with unstructured data. ProtoX prototypes an agents behavior by learning prototypical scenarios. It considers both visual and scenario similarity utilizing selfsupervised training and an isometry layer. The innovative aspect of ProtoX lies in its focus on prototypes for explanation.  Strengths: 1. Innovative Approach: The concept of using prototypes for explaining blackbox reinforcement learning agents is novel and potentially impactful. 2. Detailed Methodology: The paper thoroughly describes the architecture of ProtoX including the encoder isometry layer prototype layer and learning objectives. 3. Empirical Evaluation: The authors provide a comprehensive evaluation of ProtoX comparing it with existing baselines on different game environments and showcasing its fidelity and interpretability.  Weaknesses: 1. Complexity: The methodology and training process of ProtoX are quite intricate which might make it challenging for readers to fully grasp without extensive knowledge in the domain. 2. Limited Interpretation Clarity: While ProtoX aims to provide interpretable explanations the paper acknowledges that understanding the prototypes might require a basic understanding of the game potentially limiting its usability in some scenarios. 3. Repetitive Prototypes: The paper notes that ProtoX may learn similar and redundant prototypes hinting at a potential for further optimization in the prototype learning process.  Questions and Suggestions for Authors: 1. Prototype Pruning: Have you considered incorporating pruning techniques to reduce the redundancy in learned prototypes and enhance the quality of explanations 2. Generalization: How well does ProtoX generalize to different types of tasks and environments beyond the ones tested in the experiments Have you conducted experiments on more diverse datasets  Limitations: The paper could benefit from addressing the potential negative societal impact of ProtoX. Constructively the authors could emphasize the importance of ensuring that the explanations provided by ProtoX do not reinforce biases or stereotypes. They should consider including guidelines or methods to mitigate any negative impact on users or society.", "nP6e73uxd1": " Summary The paper introduces a novel algorithm that bridges the gap between polylogarithmic and polynomial runtime bounds for sampling from a logconcave distribution on a convex body with infinitydistance error constraints. The algorithm takes continuous samples with totalvariation bounds and converts them into samples with infinity bounds. Furthermore the paper presents significant advancements in differentially private optimization applications.  Strengths  Groundbreaking algorithmic contribution to sampling from logconcave distributions.  Theoretical analysis showcases improved runtime complexities.  Clear and wellorganized presentation of the problem methodology and results.  Thorough discussion on related works that provides a solid context.  Weaknesses  The proof overview sections may be too detailed for some readers possibly impacting the readability.  Abrupt transitions between sections make the paper challenging to follow at times.  There might be room for more detailed illustrations or examples to enhance understanding.  Questions and Suggestions 1. Clarity on Algorithm 1: It would be beneficial to provide a stepbystep breakdown of Algorithm 1 in a supplementary material or appendix for better understanding. 2. Empirical Evaluation: Have the authors performed empirical evaluations to showcase the practical efficiency of the proposed algorithm in comparison to existing methods 3. Extension of Bounds: Can the algorithm be extended to handle scenarios where the logdensity function has varying smoothness or different types of constraints 4. Impact Assessment: While the authors mention the positive societal impact it would be beneficial to elaborate on the potential applications of the proposed algorithm in realworld scenarios and any implications for privacy.  Limitations The authors should include a section addressing potential limitations of their work and negative societal impacts to provide a more comprehensive assessment.  Suggestions for Improvement To enhance the quality of the paper I recommend the authors include:  Comparative empirical evaluations.  Additional explanatory examples within the main body.  Improvement on the general flow and coherence of the paper by refining transitions between sections. Overall the paper presents a significant advancement in sampling algorithms with potential applications in privacypreserving optimization. Strengthening the clarity and practical relevance of the findings will further improve the impact of this work.", "wt7cd9m2cz2": " Summary: The paper introduces a class of shallow neural networks for learning singleindex models via gradient flow. It discusses the advantages of neural networks over traditional methods highlighting their ability to efficiently approximate highdimensional functions with lowdimensional structures. The study establishes that the proposed neural network architecture can recover the unknown direction and approximate the link function nearoptimally. The results show how gradient flow on this architecture solves both univariate nonparametric kernel ridge regression and nonconvex optimization efficiently.  Strengths: 1. Novel Approach: The study introduces a unique architecture for learning singleindex models with shallow neural networks through gradient flow providing insights into highdimensional learning. 2. Theoretical Depth: The paper offers theoretical guarantees on recovering the unknown direction and link function with nearoptimal sample complexity showcasing a strong theoretical basis. 3. Contribution to Neural Network Understanding: The work bridges featurelearning and lazyregime concepts making it significant for advancing the understanding of neural network learning capabilities.  Weaknesses: 1. Complexity: The technical depth of the paper might hinder its accessibility to a broader audience not wellversed in highdimensional statistics and neural networks. 2. Practical Implementation: The emphasis on theoretical guarantees may lead to a lack of discussion on the practical implications and feasibility of implementing the proposed approach in realworld scenarios.  Questions and Suggestions for Authors: 1. Practical Relevance: How can the insights from this theoretical study be practically implemented in realworld applications such as deep learning models 2. Comparative Analysis: Could the authors compare the proposed architectures performance with existing neural network structures in terms of computational efficiency and accuracy 3. Robustness Study: Have robustness tests been conducted to assess the models performance under noisy or outlierladen datasets 4. Future Directions: Where do the authors see the next steps in their research regarding extending this study to more general function classes or more complex models  Limitations: The authors could further address the practical implications of their work detailing its realworld applicability. Additionally discussing the computational complexity of the proposed model and its scalability could enhance the papers practical relevance.", "xpdaDM_B4D": " Summary The paper introduces Few shot Learning with hard Mixup (FeLMi) for fewshot learning by utilizing manifold mixup to generate synthetic samples addressing data scarcity issues. The proposed approach incorporates entropybased filtering for pseudolabeled base examples and hard mixup using marginbased uncertainty. FeLMi achieves stateoftheart results on standard fewshot benchmarks and demonstrates improvements in crossdomain fewshot settings.  Strengths 1. Innovative Approach: FeLMi introduces a novel hard mixup strategy for fewshot learning addressing data scarcity issues effectively. 2. Thorough Evaluation: The paper extensively evaluates the proposed approach on various common fewshot benchmarks and crossdomain settings demonstrating significant improvements. 3. Detail Clarity: The methodology is welldefined and explained with clear descriptions of each stage and experimental setup. 4. Comparative Analysis: The paper includes comparisons with stateoftheart techniques and ablation studies to validate the efficacy of each proposed stage. 5. Ablation Studies: Ablation studies help demonstrate the contributions of individual components showcasing the impact of each stage on performance.  Weaknesses 1. Clarity in Methodology: Some parts of the methodology could be more clearly explained leading to potential confusion for readers. 2. Theoretical Justification: The paper lacks theoretical justification for some design choices potentially leaving gaps in the understanding of the proposed method. 3. Experimental Settings: The paper could benefit from further detailing the experimental settings including hyperparameter tuning process and justification for parameter choices.  Questions and Suggestions for Authors 1. How does the proposed hard mixup strategy compare to existing strategies in terms of computational efficiency and scalability 2. Have you considered comparing the proposed method with more recent stateoftheart techniques beyond those mentioned in the paper to ensure comprehensive evaluation 3. Can you provide insights into the robustness of FeLMi against noisy or mislabeled examples especially in scenarios with imperfect pseudolabeling 4. How do you plan to address the potential requirement of access to base examples during metatesting for largescale datasets in future work  Limitations The authors have not explicitly addressed potential concerns regarding the requirement of base examples during metatesting for very largescale datasets which could limit the generalizability of the proposed approach. It would be beneficial to explore alternative strategies to mitigate this limitation ensuring broader applicability. By addressing the suggested points and limitations the paper could further enhance the impact and robustness of the proposed approach in the field of fewshot learning.", "tIZtD2kZ6zx": " Summary: The paper introduces \"Drawing out of Distribution (DooD)\" a neurosymbolic generative model that learns generalpurpose representations from strokebased drawing tasks. Unlike prior works DooD operates directly on images without supervision inference is unsupervised and it outperforms baselines in generalization across data and tasks. The model comprises a generative model recognition model and learning setup that collectively capture stroke sequences.  Strengths: 1. Novel Model Design: DooDs neurosymbolic approach combines neural and symbolic processing efficiently. 2. Results and Performance: DooD excels in generalizing across datasets and tasks showing superior performance compared to baselines. 3. Evaluation Methodology: Thorough evaluation across various axes demonstrates the models effectiveness. 4. Innovative Features: Explicit spline rendering execution guidance and typetoken hierarchy contribute to the models robustness and utility.  Weaknesses: 1. Complexity: The technical details and mathematical formulations may be challenging for nonexperts to understand. 2. Lack of Comparison: While DooD is compared with baselines comparing it to more recent stateoftheart models in generative learning could further strengthen the evaluation. 3. Limited Practical Applications: The implications of the model outside strokebased drawing tasks are not thoroughly discussed.  Questions and Suggestions: 1. Model Interpretability: How can the authors enhance the interpretability of the models learned representations for practical applications 2. Scalability: How does the models performance scale with larger and more diverse datasets 3. Future Work: Could the authors elaborate on potential realworld applications of DooD beyond the strokebased drawing tasks discussed in the paper  Limitations: The authors have not fully addressed the potential negative societal impacts of their work. To improve they could assess and mitigate any unintended consequences of deploying a neurosymbolic generative model in various domains.", "prQT0gN81oG": "1. Summary: The paper introduces SHAKE a framework for bridging offline and online knowledge distillation techniques by leveraging a shadow head as a proxy teacher model. The proposed method aims to optimize the teacher model for the student by performing mutual distillation with reduced training budgets. Extensive experiments on classification and object detection tasks demonstrate stateoftheart results with various architectures and datasets. 2. Assessment of Strengths and Weaknesses: Strengths:  The paper presents a novel approach to knowledge distillation by addressing the performance gap between offline and online distillation methods.  Extensive experiments demonstrate consistent and significant accuracy improvements across multiple tasks and datasets showcasing the effectiveness of the proposed technique.  The frameworks simplicity and efficiency along with the reduction in training costs make it a practical solution for knowledge transfer tasks.  The paper provides a thorough comparison with existing offline and online distillation methods highlighting the advantages of the proposed SHAKE framework. Weaknesses:  Despite the promising results the paper could benefit from a more detailed theoretical analysis to further support the proposed approach.  The limitations of the proposed method could have been discussed more explicitly to provide a comprehensive view of its applicability across different scenarios.  The paper might lack sufficient analysis of potential failure modes or scenarios in which the SHAKE framework could underperform. 3. Questions and Suggestions for Authors:  It would be beneficial to elaborate more on the implications of the proposed shadow head as a proxy teacher model in terms of generalizability to diverse architectures and datasets.  How does the proposed framework handle cases with limited or no pretrained teacher models and are there potential strategies to address such scenarios effectively  Could the authors provide insights into the robustness of the SHAKE framework under noisy or adversarial settings considering the nature of knowledge transfer tasks in practical applications  Further discussions on the computational efficiency and scalability of the SHAKE framework when applied to largerscale tasks or models could enhance the practical utility of the proposed method. 4. Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work particularly in terms of the computational costs associated with training budgets. To improve they could provide more details on strategies to mitigate potential biases or unintended consequences that may arise from the deployment of the SHAKE framework. Overall the paper introduces a novel and practical approach to knowledge distillation demonstrating significant performance gains and efficiency improvements. Further theoretical and practical analyses along with addressing potential limitations could strengthen the impact and applicability of the proposed SHAKE framework.", "xZmjH3Pm2BK": " Summary: The paper introduces a novel approach called Wavelet Scorebased Generative Model (WSGM) that accelerates the synthesis of Scorebased Generative Models (SGMs) by generating normalized wavelet coefficients with the same number of time steps at all scales. It addresses the illconditioning issues of typical SGMs by factorizing probability distributions into wavelet coefficients providing mathematical analysis and empirical results on Gaussian distributions physical processes at phase transition and natural image datasets.  Strengths: 1. Innovative Approach: The introduction of WSGM represents a novel method to tackle the computational cost issue of typical SGMs showcasing advanced thinking in generative modeling. 2. Theoretical Foundation: The paper is wellgrounded in mathematical analysis providing theorems and empirical validations to support the proposed model. 3. Empirical Results: The inclusion of numerical experiments on various datasets strengthens the credibility of the method and demonstrates its effectiveness. 4. Clear Structure: The paper is wellorganized presenting a coherent flow from problem statement to theoretical analysis and practical implementation.  Weaknesses: 1. Complexity: The paper involves intricate mathematical concepts that might be challenging for readers without a strong background in mathematics and generative modeling. 2. Discussion Extension: While the limitations are mentioned further elaboration on potential negative societal impacts and ethical considerations could enhance the discussion. 3. Limited Scope: The focus on multiscale processes might limit the generalizability of the model to a wider range of applications broadening the scope could increase the papers impact.  Questions and Suggestions: 1. Can the authors provide a clearer explanation of how WSGM compares to other acceleration techniques highlighting the specific advantages it offers 2. In what scenarios or datasets do you foresee WSGM performing exceptionally well and are there potential applications outside of image generation where this approach could be beneficial 3. How does WSGM handle noise or error propagation in the generation process especially when dealing with more complex datasets or realworld applications  Limitations: The authors should consider including a robust discussion on the potential negative societal impacts and ethical considerations of deploying WSGM to generate synthetic data. Providing insights on how to address these concerns such as data privacy and bias mitigation strategies could enhance the papers completeness.", "tmUGnBjchSC": " Review of \"A Unified Framework for DecisionTheoretic Bayesian Optimization\"  Summary: The paper presents a novel framework that unifies decisiontheoretic and informationbased paradigms in Bayesian optimization (BO). The proposed framework introduces a family of decisiontheoretic entropies parameterized by a loss function and action set which generalizes the traditional Shannon entropy used in informationbased BO methods. By defining a generalized entropy measure the framework allows for the development of customized acquisition functions suited for diverse optimization tasks. The paper further explores gradientbased methods for optimizing the proposed family of acquisition functions and demonstrates strong empirical performance across various sequential decisionmaking tasks.  Strengths: 1. Innovative Framework: The paper introduces a novel decisiontheoretic view of Bayesian optimization introducing a family of decisiontheoretic entropies that provides a unified perspective on existing acquisition functions. 2. Customizable Acquisition Functions: The ability to tailor acquisition functions to diverse optimization tasks based on problemspecific loss functions and action sets is a significant contribution. 3. Empirical Performance: Strong empirical results across sequential decisionmaking tasks demonstrate the effectiveness of the proposed framework. 4. Gradientbased Optimization: The inclusion of gradientbased methods for acquisition optimization is a valuable addition improving computational efficiency.  Weaknesses: 1. Complexity: The technical depth and mathematical rigor of the framework may make it challenging for readers unfamiliar with Bayesian optimization and decision theory. 2. Limited Comparison: While the paper compares against common approaches like Knowledge Gradient and Uncertainty Sampling it lacks a comparison against stateoftheart algorithms in BO. 3. Practical Implementation: Practical considerations and limitations of implementing the proposed framework in realworld scenarios are not thoroughly discussed.  Questions and Suggestions for Authors: 1. Scalability: How does the proposed framework scale with increasing dimensions of the input space and complexity of optimization tasks 2. Robustness: Have the authors analyzed the robustness of the proposed framework to noise in the observations How does it perform under noisy conditions 3. Realworld Applications: Can the authors provide examples or case studies where the framework has been applied to realworld problems to showcase its practical utility 4. UserFriendliness: Have the authors considered userfriendliness and ease of implementation in realworld applications when designing this framework  Limitations: The authors should provide a more detailed discussion on the practical limitations and potential negative societal impacts of the proposed framework. It might be beneficial to address issues related to interpretability computational overhead and generalizability when implementing the framework in diverse applications. Overall the paper presents a sophisticated and innovative framework for decisiontheoretic Bayesian optimization with strong empirical results. Further clarity on practical implementation and comparison against advanced BO methods could enhance the papers impact.", "sn6BZR4WvUR": " Summary: The paper introduces a new optimization framework called `multiscale perturbed gradient descent (MPGD)` which incorporates chaotic perturbations to gradient descent in a controlled manner. The study analyzes MPGD from three different perspectives: 1) convergence to a stochastic differential equation (SDE) driven by a heavytailed L\u00e9vystable process 2) derivation of a generalization bound for the limiting SDE and 3) implicit regularization effects. The empirical results demonstrate the advantages of MPGD in various optimization tasks.  Strengths: 1. Novel Framework: Introducing MPGD as a controlled chaotic perturbation to gradient descent is innovative. 2. Theoretical Rigor: The paper provides a thorough theoretical analysis showcasing convergence to SDE generalization bounds and implicit regularization effects. 3. Empirical Validation: Empirical results on diverse tasks support the theoretical findings and demonstrate the advantages of MPGD. 4. Detailed Methodology: The detailed mathematical notations technical background and experimental setups provide a comprehensive understanding of the proposed framework.  Weaknesses: 1. Complexity: The mathematical rigor and technical terminology may make the content inaccessible to some readers. 2. Empirical Validation Limitation: The empirical results while supportive could benefit from comparisons with more stateoftheart optimization algorithms to strengthen the claims further. 3. Application Scenario: The paper could clarify which realworld scenarios or types of datasets may benefit most from MPGD to provide practical insights to readers.  Questions and Suggestions: 1. Clarification on Applicability: Could the authors provide examples of scenarios or datasets where MPGD might offer significant advantages over traditional optimization methods 2. Comparison to StateoftheArt: How does MPGD compare to recent advanced optimization algorithms in terms of convergence speed and performance on largescale datasets 3. Societal Impact Consideration: While the paper mentions no negative societal impacts could there be any ethical considerations or implications related to using chaotic perturbations in optimization algorithms that the authors should address  Limitations: The paper adequately considers limitations and societal impacts focusing on the theoretical framework empirical outcomes and mathematical analysis. To enhance the discussion authors could explore additional applications and realworld case studies to showcase the practical benefits of MPGD.", "wwW-1k1ljIg": " Summary The paper explores the detection of backdoor neurons in infected models by analyzing preactivation distributions. The proposed defense strategies are attackinvariant and efficiently locate potential backdoor neurons. Two detection strategies based on the differential entropy of neurons and the KullbackLeibler divergence between distributions are introduced. Experimental results indicate the effectiveness of the proposed methods against various backdoor attacks.  Strengths 1. Novel Approach: The paper introduces a novel approach to locate backdoor neurons by analyzing preactivation distributions enhancing the understanding of backdoor attacks and defenses. 2. Theoretical Basis: The paper lays out clear assumptions and corollaries for detecting backdoor neurons based on preactivation distribution properties providing a solid theoretical foundation. 3. Indepth Experiments: Extensive experiments on CIFAR10 and TinyImageNet datasets with various backdoor attacks demonstrate the effectiveness and superiority of the proposed strategies. 4. Comparison with Existing Methods: The paper thoroughly compares the proposed methods with existing defense strategies highlighting their superior performance efficiency and reliability in detecting backdoor neurons.  Weaknesses 1. Limitation in Practical Application: The proposed methods rely on assumptions about the distribution of preactivations which may not always hold true in realworld scenarios possibly limiting the practical application of the techniques. 2. Generalizability Concerns: The study focuses on specific attacks and datasets raising questions about the generalizability of the proposed methods to diverse scenarios and larger datasets. 3. Complexity: The paper introduces complex mathematical formulations and assumptions which might be challenging for readers not familiar with advanced statistical concepts in neural networks.  Questions and Suggestions for the Authors 1. How robust are the proposed methods to variations in poisoning rates or different attack scenarios beyond the ones tested 2. Can the assumptions made about preactivation distributions be validated in realworld settings considering data variability and model complexities 3. Are there any plans to extend the research to explore the impact of the proposed methods on different types of neural network architectures or datasets  Limitations The authors have provided a comprehensive discussion on the strengths and weaknesses of their work but they could further address the potential societal impact by considering the implications of false positives or inaccuracies in detecting backdoor neurons. Offering guidance on mitigating unintended consequences of backdoor defense strategies would be valuable.", "sQ2LdeHNMej": " Peer Review 1) Summary: The paper proposes FATHOM (Federated AuTomatic Hyperparameter OptiMization) a oneshot online procedure that combines hyperparameter tuning and optimization for federated learning. The method aims to minimize communication budget and local computational resources outperforming FedAvg. The paper provides theoretical analysis contributions and empirical results with datasets like FEMNIST and FSO. 2) Strengths:  Innovative Approach: FATHOM offers a unique online hyperparameter optimization solution for FL addressing important challenges.  Theoretical Analysis: The paper presents detailed theoretical convergence results and assumptions providing a solid foundation for the proposed method.  Empirical Validation: Extensive experiments using established frameworks and datasets validate the performance gains of FATHOM over FedAvg.  Relevance: The work addresses critical aspects of communication efficiency and local computation resources in federated learning making it highly relevant for the research community. 3) Weaknesses:  Lack of Comparison: The absence of comparisons with other oneshot FL HPO methods limits the comprehensive evaluation of FATHOMs effectiveness.  Societal Impact Discussion: While privacy concerns are touched upon a more explicit discussion on potential societal impacts could enhance the papers relevance.  Complexity: The algorithms complexity and tradeoffs especially regarding flexibility versus performance could be further discussed for clarity. 4) Questions and Suggestions:  Clarification on Adaptive Process: How does the adaptive process influence the convergence and practical benefits of FATHOM  Generalizability: How versatile is FATHOM in handling different FL models and datasets beyond the ones mentioned in the study  Scalability: How does FATHOM scale with larger datasets and more complex FL scenarios 5) Limitations: The authors have adequately addressed the limitations of their work such as the lack of comparisons with other methods. To enhance the societal impact assessment it is recommended to delve deeper into potential ethical considerations associated with FL applications. Overall the paper presents a novel approach in FL hyperparameter optimization backed by theoretical analysis and empirical results. Improving comparisons discussing societal impacts and explaining the adaptive process further would enhance the papers robustness and relevance.", "mhP6mHgrg1c": " Peer Review:  1) Summary: The paper proposes a subset selection framework ORIENT for enhancing efficiency in supervised domain adaptation methods using submodular mutual information functions for selecting source data subsets similar to the target domain. The framework is shown to complement existing SDA methods leading to faster training and improved performance on target data.  2) Strengths:  Innovative Framework: The introduction of ORIENT utilizing submodular mutual information for source data subset selection is novel and addresses the critical issue of efficiency in domain adaptation.  Empirical Validation: The paper provides empirical evidence demonstrating the effectiveness of ORIENT when combined with existing SDA methods showcasing faster training and improved classification performance.  Thorough Literature Review: The paper provides a detailed account of related works and clearly positions ORIENT within the existing domain adaptation techniques.  3) Weaknesses:  Technical Depth: The paper could benefit from further elaboration on the technical details of the proposed framework to enhance the understanding for readers less familiar with the domain.  Experimental Setup: While the experiments are comprehensive more detailed analysis and comparisons with additional baseline methods could strengthen the empirical evaluation.  Clarity of Presentation: Some sections could be more succinct and focused to improve the overall readability and coherence of the paper.  4) Questions and Suggestions for Authors:  It would be beneficial to provide insights into the impact of the subset size and subset sampling frequency on the performance of ORIENT. Could the authors elaborate on potential strategies to optimize these parameters effectively  How does the computational and memory resources required for implementing ORIENT scale with the size of the datasets Are there potential scalability challenges when working with larger datasets  5) Limitations: The authors have addressed limitations related to the reduction in training time and energy efficiency however more emphasis on the memory requirements and potential implications of heightened memory consumption could further enhance the papers thoroughness.  Limitations  Suggestions: To ensure the paper comprehensively addresses potential limitations and societal impacts the authors may consider expanding the discussion on memory utilization and providing recommendations for mitigating or optimizing memory requirements to augment the practical adoption of ORIENT. Additionally reinforcing the societal implications of energyefficient training and the overarching support towards Green AI initiatives would further underscore the societal benefits of the proposed approach.", "xK6wRfL2mv7": " Summary: The paper introduces a novel training algorithm called SharpnessAware Training for Free (SAF) to mitigate the sharpness issue in deep neural networks. SAF aims to reduce the sharpness of minima leading to improved generalization performance without additional computational cost. The proposed trajectory loss serves as a replacement for the existing sharpness measure enabling SAF to converge to flat minima efficiently. Empirical results on CIFAR10100 and ImageNet datasets demonstrate that SAF outperforms existing methods in terms of generalization capabilities with almost the same computational cost as the base optimizer.  Strengths: 1. Innovative Contribution: SAF introduces a novel trajectory loss to address the sharpness issue in neural networks without incurring additional computational overhead. 2. Efficiency: By avoiding the computational burden associated with existing methods like SAM SAF achieves stateoftheart results on ImageNet at the same computational cost as the base optimizer. 3. Empirical Validation: Extensive experiments on different datasets and models demonstrate the effectiveness of SAF in enhancing generalization performance while maintaining training speed.  Weaknesses: 1. Complexity: While SAF aims to reduce sharp minima efficiently the introduction of a new loss function and trajectory tracking may add complexity to the training process. 2. Scaling Limitations: The paper mentions potential memory issues on extremely large datasets which could limit the scalability of SAF in realworld applications. 3. Comparative Analysis: While the paper presents results comparing SAF with existing methods more detailed comparisons and ablation studies could further strengthen the analysis.  Questions and Suggestions: 1. Clarity: Could the paper elaborate on the specific scenarios where SAF may outperform existing methods and its limitations in practical applications 2. Scalability: How does SAF adapt to different network architectures and hyperparameters especially for largescale datasets beyond ImageNet 3. Evaluation Metrics: Apart from test accuracies are there any other metrics to evaluate the convergence and generalization capabilities of SAF comprehensively 4. Deployment: How feasible is the implementation and deployment of SAF in realworld deep learning applications considering its computational efficiency and memory requirements 5. Future Directions: Are there any plans to enhance the adaptability and robustness of SAF further in future research particularly in more diverse tasks like natural language processing  Limitations: The authors should provide more detailed discussion on the potential negative societal impact of their work especially concerning the scalability and practicality of SAF in realworld scenarios and suggest mitigation strategies if applicable.", "w1CF57sLstO": " Peer Review  1. Summary: The paper investigates the generalization properties of the ModelAgnostic MetaLearning (MAML) approach in the overparameterized regime using mixed linear regression models. The study provides upper and lower bounds for the excess risk of MAML trained with SGD highlighting the impact of various learning parameters and datatask distributions on generalization. The analysis explores the role of adaptation learning rate on both excess risk and early stopping time while validating theoretical findings through experiments.  2. Strengths:  The analysis provides a comprehensive theoretical understanding of MAML in the overparameterized regime.  Sharp characterization of excess risk with upper and lower bounds enhances the clarity of the results.  Exploration of the influence of data and task distributions on generalization offers valuable insights.  Investigating the adaptation learning rates role in generalization and early stopping time furthers understanding the dynamics of MAML.  3. Weaknesses:  The paper could benefit from further clarity in explaining the technical aspects of the model and assumptions made.  The presentation of the experimental validation could be improved for better visualization and interpretation.  A more extensive discussion on the practical implications and realworld applications of the findings would enhance the papers impact.  4. Questions and Suggestions for Authors:  Could you provide more detailed explanations of the technical aspects like effective meta weights for better understanding by readers  How do the results align with existing studies on MAML and overparameterized metalearning  Can you elaborate on the implications of the findings for practitioners and how they can be practically applied in realworld scenarios  5. Limitations: The paper could clarify the potential negative societal impact of its findings especially regarding the technical nature of the study and its direct relevance to practical applications. One potential suggestion is to provide a section discussing the ethical considerations or implications of the theoretical results in a more accessible manner. Overall the paper makes a significant contribution to the theoretical understanding of MAML in the overparameterized regime and addressing the minor weaknesses and suggestions could further enhance the impact and accessibility of the study.", "lCGYC7pXWNQ": " Summary The paper introduces FrameMaker a memoryefficient video classincremental learning approach that condenses videos into a single frame to reduce memory consumption while maintaining performance. The proposed method consists of two main components: Frame Condensing and InstanceSpecific Prompt. Experimental results show that FrameMaker outperforms existing methods while consuming only 20 memory.  Strengths 1. Innovative Approach: FrameMaker offers a novel method for memoryefficient video classincremental learning addressing the issue of memory overhead in storing videos. 2. Two Crucial Components: The integration of Frame Condensing and InstanceSpecific Prompt demonstrates the effectiveness of the proposed approach in reducing memory consumption. 3. Experimental Results: The results on challenging benchmarks show consistent improvement over existing methods while consuming significantly less memory. 4. Comparative Performance: FrameMaker surpasses stateoftheart methods in terms of accuracy under the same memory constraints.  Weaknesses 1. Complexity: The proposed method involves multiple components and loss terms potentially leading to a complicated implementation and training process. 2. Limited Detailed Analysis: The paper lacks indepth analysis of the effects of varying parameters and components on performance. 3. Memory Constraints: While FrameMaker reduces memory consumption it may face challenges in scenarios with highly constrained memory budgets.  Questions and Suggestions for Authors 1. Clarification on Loss Terms: Could the authors provide further insights into the impact of different loss terms on the overall performance 2. Optimization of Training Process: Are there any methods to simplify the training process and improve convergence for FrameMaker 3. Generalization: Have the authors tested the generalization capabilities of FrameMaker on other datasets or tasks to assess its robustness 4. Efficiency vs. Accuracy Tradeoff: How does the tradeoff between memory efficiency and classification accuracy evolve as the complexity of actions or datasets increases  Limitations The authors have adequately addressed the limitations of the work but could further explore the performance of FrameMaker in scenarios with highly constrained memory budgets.  Societal Impact The potential negative societal impact of FrameMaker is considered with privacy concerns given that condensed frames still retain some information from the original videos. Further exploration into privacypreserving measures may enhance the societal benefits of the approach.  Suggestions for Improvement 1. Simplicity: Simplify the training process and components to enhance usability and reduce complexity. 2. Detailed Analysis: Conduct a comprehensive study on the effects of varying parameters and components for a deeper understanding of the method. 3. Generalizability: Test FrameMaker on a wider range of datasets and tasks to assess its applicability in diverse scenarios. 4. Privacy Measures: Implement additional measures to ensure privacy preservation in video data processing. Overall the paper presents an innovative approach with promising results but further refinement and analysis can strengthen its impact in the field of video classincremental learning.", "oWqWiazEb62": " Summary: The paper introduces a method for training machine learning models with datadependent constraints. It focuses on reformulating datadependent constraints to ensure exact constraint satisfaction with a userprescribed probability demonstrating efficacy on a fairnesssensitive classification task.  Strengths: 1. Innovative Methodology: The proposed approach of guaranteeing exact constraint satisfaction with userprescribed probability is novel and addresses an important issue in machine learning. 2. Theoretical Rigor: The paper provides clear theoretical background and rigor to support their proposed method ensuring the validity of the algorithm. 3. Applied Use Case: The application of the method on fairnesssensitive classification tasks showcases realworld relevance and potential impact.  Weaknesses: 1. Complexity: The paper delves into technical details quite quickly making it challenging for readers not wellversed in optimization theory to follow. 2. Limited Empirical Validation: While the paper demonstrates efficacy on a specific task further comprehensive empirical validation on diverse datasets would strengthen the generalizability of the proposed method. 3. Discussion of Practical Implementation: More insights into the practical implementation challenges computational requirements and scalability considerations could enhance the usability of the proposed method.  Questions and Suggestions for Authors: 1. Can you provide more insights into the computational efficiency and scalability of the proposed method especially in comparison to existing approaches 2. How does the method perform on varied datasets with different levels of complexity and class imbalances 3. Have you considered the potential interpretability challenges that might arise due to the complexity introduced by these datadependent constraints  Limitations: The authors have not thoroughly addressed the potential negative societal impact of their work. To mitigate this they could include a discussion on ethical implications bias amplification risks and fairness considerations especially in deployment scenarios.  Constructive Suggestions: 1. Incorporate a more structured explanation of the algorithm and its implications for practitioners. 2. Enhance the clarity of the presentation especially in providing intuitive examples or visual aids to aid comprehension. 3. Address the broader ethical implications and limitations of the proposed method in the context of societal impact. Overall the paper contributes significantly to the niche area of datadependent constraint optimization and fairness in machine learning but could benefit from broader empirical validation clearer presentation and ethical considerations.  This is a general review based on the provided abstract excerpt. For a detailed critique additional information from the full paper would be required.", "r6_zHM2POTd": "Summary: The paper introduces a novel nonstationary dynamic meanfield theory to explain the impact of a tight balance of excitatory and inhibitory currents on information encoding in recurrent neural networks. The study shows that tighter balance enhances the accuracy of population encoding of timevarying stimuli leading to improved information transmission in the presence of noise and chaos. The theory is validated through both analytical calculations and numerical simulations demonstrating the linear relationship between tightness of balance and mutual information rate. Furthermore the paper examines the effects of training recurrent networks on specific tasks revealing the emergence of tightly balanced subnetworks and improved highfrequency encoding. Strengths: 1. Innovative Theory Development: Introducing a nonstationary dynamic meanfield theory to investigate the impact of balance on information encoding is a novel and valuable contribution. 2. Theoretical and Practical Relevance: The studys findings have important implications for neural network design and understanding noiserobustness and information encoding in recurrent neural networks. 3. Comprehensive Analysis: The paper offers a detailed examination combining theoretical analysis with numerical experiments to validate the theorys predictions across various scenarios and tasks. Weaknesses: 1. Complexity and Accessibility: The paper may be challenging for readers unfamiliar with neural network dynamics mathematical modeling and information theory limiting its accessibility to a broader audience. 2. Limited Experimental Validation: While numerical simulations support the theoretical findings additional experimental validation using biological data or realworld applications could strengthen the practical relevance of the study. 3. Addressing Network Size: The study emphasizes theoretical explanations for large network sizes however more insights on finitesize effects and scalability to smaller networks could enhance the papers applicability. Questions and Suggestions: 1. How does the proposed framework compare to existing theories or models in the field of neural network dynamics and information processing 2. Can the authors discuss potential applications of their findings in realworld scenarios such as braininspired computing or machine learning algorithms 3. Considering the implications of balance on information encoding how might factors like plasticity or learning mechanisms influence network dynamics and performance Limitations: The authors could further address potential limitations by integrating diverse neuron models exploring tasks with more complex temporal structures and considering biophysical constraints for a more comprehensive understanding of practical implications.", "pbILUUf_hBN": " Summary: The paper presents an algorithm for online learning with feedback graphs achieving nearoptimal regret bounds for both stochastic and adversarial environments. It combines ideas from EXP3 for bandits and EXP3.G for feedback graphs with a novel exploration scheme. The algorithm adapts to the graphs independence number and extends to timevarying feedback graphs.  Strengths:  Novel Algorithm: The proposed EXP3.G algorithm is wellmotivated and theoretically sound.  BestofBothWorlds Guarantee: Achieving nearoptimal regret bounds in both stochastic and adversarial settings is a significant contribution.  Technical Rigor: The paper provides a detailed analysis proofs and extensions to timevarying feedback graphs.  Extensive Related Work: The review of prior art in online learning with feedback graphs demonstrates the papers contribution within the research landscape.  Weaknesses:  Complexity: The paper involves sophisticated mathematical concepts that may be challenging for nonexperts to understand easily.  Limited Practical Examples: Realworld applications or empirical tests of the algorithm are not discussed which could enhance the paper\u2019s practical relevance.  Questions and Suggestions: 1. Clarity: Enhance clarity in explaining the algorithm and key concepts for better understanding by a wider audience. 2. Empirical Evaluation: Consider adding simulation results or case studies to demonstrate the algorithms performance in practical scenarios. 3. Robustness: Address the algorithms robustness to noise or imperfect data considering the practical challenges in realworld implementations. 4. Sensitivity Analysis: Conduct a sensitivity analysis to investigate the algorithms performance across different parameter settings or graph structures.  Limitations: The authors could further reflect on how the algorithms complexity might impact its practical implementation suggesting avenues for simplification and explore more practical use cases for validation. It would be beneficial for the authors to emphasize the practical implications of their work and potentially provide guidelines for practitioners looking to apply the algorithm in realworld scenarios. Overall the paper makes a significant contribution to the field of online learning with feedback graphs and presents a promising algorithm with notable theoretical guarantees.", "n0dD3d54Wgf": " Summary: The paper introduces Sparse Continual Learning (SparCL) a novel framework that leverages sparsity to enable costeffective continual learning on edge devices. SparCL focuses on training efficiency in continual learning systems providing learning acceleration and accuracy preservation through weight sparsity data efficiency and gradient sparsity. It outperforms existing stateoftheart methods in both efficiency and accuracy demonstrating practical potential on real mobile edge devices.  Strengths: 1. Innovation: SparCL introduces a unique approach by combining weight sparsity data efficiency and gradient sparsity to address the training efficiency in continual learning systems offering a novel perspective in the field. 2. Performance: The experimental results show that SparCL consistently outperforms existing methods in terms of training efficiency and accuracy demonstrating the effectiveness of the proposed framework. 3. Comprehensive Evaluation: The paper provides a detailed evaluation on multiple benchmarks including Split CIFAR10 and TinyImageNet along with comparisons with various existing CL methods and CLadapted sparse training methods.  Weaknesses: 1. Complexity: The proposed SparCL framework involves multiple components such as taskaware dynamic masking dynamic data removal and dynamic gradient masking which might make it challenging for practical implementation and deployment. 2. Limited Comparison with Existing Sparse Training Methods: While SparCL outperforms CLadapted sparse training methods a more indepth comparison with a wider range of sparse training techniques could further strengthen the papers findings. 3. Realworld Scalability: The evaluation on a single real mobile device may not fully capture the scalability and generalizability of SparCL on various edge devices with different configurations and constraints.  Questions and Suggestions for Authors: 1. Can you elaborate on the tradeoffs between the different components of SparCL and provide insights into scenarios where one component might be more critical than the others 2. How does SparCL handle concept drift or nonstationarity in continual learning settings especially when deploying on realworld edge devices with dynamic data streams 3. Have you considered the potential energy consumption implications of SparCL on edge devices especially in scenarios with limited battery life  Limitations: The authors should provide more detailed discussions on the scalability and efficiency tradeoffs of implementing SparCL on various edge device architectures especially considering the potential energy consumption impact on batterypowered devices. Additional insights on the potential negative societal impact of SparCL should also be addressed for a more comprehensive evaluation.", "x3JsaghSj0v": " Summary: The paper proposes a novel approach Adaptive Node Sampling for Graph Transformer (ANSGT) to address the limitations of existing graph transformers when dealing with large graphs. The paper identifies deficiencies in current graph transformers related to node sampling strategies that are agnostic to graph characteristics and neglect longrange dependencies. ANSGT formulates node sampling optimization as an adversary bandit problem and introduces a hierarchical attention scheme with graph coarsening to capture longrange interactions. Extensive experiments on realworld datasets exhibit the superiority of ANSGT over existing methods.  Strengths: 1. The paper presents a comprehensive analysis of the limitations of existing graph transformers and provides a clear motivation for proposing the ANSGT approach. 2. The proposed ANSGT method is wellfounded theoretically utilizing adaptive sampling strategies based on an adversary bandit problem and employing hierarchical attention to capture both local and global information. 3. The experimental evaluation on benchmark datasets demonstrates the superiority of ANSGT over existing graph transformers and popular GNNs showcasing the effectiveness of the proposed approach.  Weaknesses: 1. The paper could benefit from a more detailed explanation of the novelty of the proposed method compared to existing approaches. Clarifying the unique contributions and advantages of ANSGT would enhance the papers impact. 2. The paper could elaborate more on the computational complexity analysis of ANSGT to provide readers with a deeper understanding of the methods efficiency and scalability. 3. The limitations of the proposed approach potential scenarios where ANSGT may not perform optimally and directions for future improvements could be more explicitly discussed to provide a holistic view of the work.  Questions and Suggestions for Authors: 1. Can you provide more insights into the decisionmaking process behind selecting the specific sampling strategies used in ANSGT How were these strategies determined to be effective for different graph properties 2. How do you address the tradeoff between adaptability and computational complexity in the ANSGT method especially regarding the sampling weights update process 3. Have you considered the interpretability aspect of the proposed ANSGT method How can the decisions made by the adaptive node sampling module be explained to users or stakeholders for better understanding and trust in the models behavior  Limitations: The authors have not adequately addressed the negative societal impact of their work. To enhance social responsibility the authors could consider discussing the potential biases introduced by adaptive sampling strategies and how these biases can be mitigated to ensure fairness and inclusivity in applications. Additionally highlighting ethical considerations related to graph data privacy and security would strengthen the papers societal impact assessment.", "u4KagP_FjB": " PeerReview of \"Spartan: Training Sparse Neural Networks with a Soft Topk Masking Operation\" 1) Summary and Contributions: The paper introduces Spartan a novel sparse learning algorithm based on a soft topk masking operation to induce parameter sparsity during training. The algorithm offers an explorationexploitation tradeoff enabling a controlled transition between various sparsity patterns. Spartan is demonstrated on ResNet50 and ViT models on ImageNet1K achieving superior generalization accuracy over existing methods at high sparsity levels. 2) Strengths and Weaknesses: Strengths:  Spartan introduces a unique explorationexploitation scheme optimizing generalization accuracy in sparse neural networks.  The empirical evaluation on ResNet50 and ViT models demonstrates consistent performance improvements over existing stateoftheart methods.  The costsensitive soft topk masking scalability and training strategies enhance the adaptability of the algorithm.  The opensource implementation provided on GitHub promotes reproducibility and further research. Weaknesses:  The theoretical underpinnings supporting the explorationexploitation tradeoff could be elaborated further for better understanding.  While Spartan outperforms existing methods a deeper analysis of computational efficiency and potential tradeoffs in memory usage could strengthen the paper.  More comparative analysis and discussion on the negative societal impacts of energyintensive deep learning models and the potential benefits of sparsityaware algorithms could be included. 3) Questions and Suggestions:  How does Spartans explorationexploitation tradeoff influence the convergence behavior and computational complexity of the algorithm  Could a more detailed comparison with other methods in terms of memory efficiency and FLOP savings provide further insights into the algorithms practical utility  Have you considered evaluating Spartan on diverse datasets and architectures to assess its generalizability and robustness across different scenarios 4) Limitations:  While Spartan addresses memory usage and computational efficiency further exploration on the negative societal impact of energyintensive deep learning models and the potential benefits of sparsityaware algorithms could enhance the papers relevance to broader societal implications. In conclusion \"Spartan\" presents a valuable contribution to the field of sparse neural network training with its innovative explorationexploitation tradeoff. Strengthening the theoretical backing comparative analyses and societal impact considerations could enhance the comprehensiveness and impact of the work.  This peerreview provides a comprehensive evaluation of the paper \"Spartan: Training Sparse Neural Networks with a Soft Topk Masking Operation\" highlighting its innovative contributions and areas for improvement. Further clarity on theoretical aspects comparative analysis and societal implications could enrich the paper.", "ldl2V3vLZ5": " Summary: The paper introduces S3GC a novel method for scalable graph clustering with node feature sideinformation. S3GC uses contrastive learning along with Graph Neural Networks to learn clusterable features. The method is empirically demonstrated to outperform stateoftheart methods in terms of clustering accuracy achieving up to a 5 gain in NMI while being scalable to graphs of size 100M.  Strengths: 1. Innovative Approach: The use of contrastive learning and a onelayer Graph Convolutional Network (GCN) to combine graph and nodefeature information is novel and effective. 2. Scalability: The method is designed to scale well using efficient encoders and simple randomwalkbased samplers allowing it to handle graphs of size up to 100M nodes on a single VM. 3. Empirical Evaluation: The extensive empirical evaluation demonstrates the superiority of S3GC over existing methods across various benchmarks showcasing its effectiveness in realworld scenarios.  Weaknesses: 1. Theoretical Analysis: The paper lacks a detailed theoretical analysis of the method. The inclusion of theoretical results would enhance the understanding and credibility of the proposed approach. 2. Comparison with StateoftheArt Methods: While the paper compares S3GC with various baselines a more indepth comparison with the latest and more diverse stateoftheart methods could provide a richer evaluation context. 3. Limited Discussion on Limitations: The limitations of the proposed method are not discussed in depth a more thorough exploration of potential drawbacks would be beneficial.  Questions and Suggestions for Authors: 1. Theoretical Analysis: Have you considered providing theoretical analysis or convergence guarantees for S3GC 2. Comparison with Additional Baselines: How does S3GC compare with other recent cuttingedge methods beyond the ones included in the paper 3. Potential Drawbacks: Can you elaborate on any potential limitations or drawbacks of S3GC that were not specifically addressed 4. Scalability Concerns: How does the method handle even larger graph datasets and considerations for further scaling beyond the presented scale  Limitations: The authors could strengthen their work by addressing the theoretical analysis providing a more comprehensive comparison with stateoftheart methods discussing potential drawbacks in more detail and exploring scalability considerations for even larger graph datasets.", "mvbr8A_eY2n": "Summary: The paper addresses the problem of allocating items to multiple recipients with predefined fractions while minimizing envy among recipients. It formulates the problem as a semidiscrete optimal transport model allowing continuous tradeoffs between efficiency and envy. The contributions include a concise representation of optimal policies an efficient stochastic optimization algorithm and a PAClike sample complexity bound for approximating the optimal solution. It is especially applicable to largescale allocation problems like blood donation matching. Strengths: 1. Innovative Formulation: The paper introduces a novel formulation that offers a continuum of tradeoffs between efficiency and envy which is a departure from the traditional binary treatment of envy. 2. Efficient Algorithm: The proposed stochastic optimization algorithm demonstrates a provable convergence rate enhancing the practical applicability of the methodology. 3. Analytical Insights: The geometric interpretation of the Laguerre cells provides a clear understanding of the solution space and aids in sample complexity analysis. Weaknesses: 1. Complexity: The detailed mathematical formulations and optimization techniques might pose challenges for nonexperts to comprehend potentially hindering broader application and adoption. 2. RealWorld Validation: The performance evaluation is limited to simulations and realworld validation on diverse datasets could enhance the practical relevance and robustness of the proposed approach. Questions and Suggestions for Authors: 1. Clarification on Envy Definition: Provide further insights into how the proposed model captures envy and elaborate on the formal definition of Envy(yi) to enhance clarity for readers. 2. Practical Implementation: Discuss the feasibility and computational efficiency of implementing the proposed approach in realworld scenarios considering the complexity of the optimization framework. 3. Scaling Considerations: Address how the proposed methodology can scale to handle larger datasets and diverse allocation scenarios to ensure practical utility in complex allocation problems. 4. Impact Assessment: Consider discussing the potential societal impact of the proposed allocation policies and suggest ways to mitigate any negative consequences that may arise. Limitations: Authors should provide a detailed exploration of the potential negative societal impacts of their work and propose strategies to address or mitigate these impacts to ensure responsible deployment of the proposed allocation policies.", "riIaC2ivcYA": " Summary: The paper introduces a new neuralnetworkbased active learning algorithm INeurAL designed for the nonparametric streaming setting. It improves theoretical and empirical performance by introducing two new regret metrics optimizing the population loss leveraging neural networks for exploitation and exploration and updating parameters efficiently. The algorithm is evaluated against stateoftheart baselines in extensive experiments.  Strengths: 1. Contributions: The paper introduces novel regret metrics a neuralbased exploration strategy and efficient parameter updating improving the performance of active learning with neural networks. 2. Theoretical Analysis: The paper provides a rigorous theoretical analysis including regret bounds and instancedependent performance guarantees showing the effectiveness of the proposed algorithm. 3. Empirical Evaluation: Extensive experiments on realworld datasets demonstrate the superiority of INeurAL over existing baselines showcasing its improved performance.  Weaknesses: 1. Complexity: The papers content may be complex for readers unfamiliar with active learning and neural networks. Clearer explanations or visual aids could have been beneficial. 2. Comparison Baselines: The comparison does not include some recent active learning algorithms or other types of machine learning models limiting the comprehensiveness of the evaluation.  Questions and Suggestions for the Authors: 1. Explanation Clarity: Could the authors provide more intuitive explanations or examples to help readers understand the proposed algorithm and theoretical concepts better 2. Comparison Baselines: Have the authors considered including more diverse baselines for comparison to provide a broader evaluation of INeurAL 3. Scalability: How does the proposed algorithm scale with larger datasets or more complex neural network architectures Could the authors discuss the potential limitations in scaling up  Limitations: The authors could provide a more detailed discussion on the potential negative societal impact of active learning algorithms especially concerning data privacy bias and fairness implications. Integrating ethical considerations into the algorithm design and evaluation process could enhance the papers societal implications.", "q-FRENiEP_d": "Summary: The paper introduces SageMix a saliencyguided Mixup method for point clouds aiming to preserve salient local structures through smooth combination. It addresses the limited exploration of Mixupbased data augmentations in 3D vision particularly in point clouds. Extensive experiments show that SageMix outperforms existing Mixup methods in generalization performance robustness and uncertainty calibration across various benchmark datasets achieving competitive performance in part segmentation and 2D image classification tasks. Strengths: 1. Innovation: Introducing saliencyguided Mixup for point clouds is a novel and significant contribution addressing a gap in current research. 2. Performance: Extensive experiments show consistent and significant improvements over existing methods in various metrics demonstrating the effectiveness of SageMix. 3. Methodology: Detailed explanation of the sequential sampling based on saliency scores and shapepreserving continuous Mixup methodology is thorough and wellsupported. 4. Applications: Demonstrating the adaptability and competitive performance of SageMix across different tasks including part segmentation and 2D image classification adds value to the proposed method. Weaknesses: 1. Clarity: Some sections especially in the method description could benefit from simplification and clearer explanations for better understanding. 2. Comparison: While the paper compares SageMix with previous Mixup methods in various experiments a more indepth comparison and analysis with stateoftheart techniques could enhance the evaluation. 3. Dataset Diversity: The paper uses limited benchmark datasets and expanding the evaluation to more diverse datasets could provide a broader perspective on the generalizability of SageMix. 4. Code Availability: Although the code availability is mentioned more emphasis on the ease of reproducibility and accessibility for the research community could be beneficial. Questions and Suggestions: 1. How does the bandwidth parameter in SageMix affect the tradeoff between preserving local structures and creating continuous samples Is there an optimal range for different types of datasets 2. Have you considered the computational complexity of SageMix compared to other Mixup methods especially when scaling up to larger datasets or models 3. How does SageMix perform in scenarios with highly complex 3D structures or intricate point cloud patterns that might challenge the saliencyguided sampling approach 4. Could there be potential biases or limitations introduced by the use of saliency scores in the sampling process especially in scenarios with noisy or ambiguous features Limitations: The authors could enhance the discussion on potential societal impacts of the proposed method such as bias amplification through the saliencyguided sampling strategy and provide suggestions on mitigating these risks while deploying SageMix in realworld applications.", "thgItcQrJ4y": " Summary: The paper provides a comprehensive analysis of the Edge of Stability phenomenon in neural networks trained with fullbatch gradient descent. It divides the optimization trajectory into four phases based on sharpness dynamics empirically identifies the norm of the output layer weight as a significant indicator and offers theoretical and empirical explanations for the behavior observed in the EOS regime. The paper makes valuable theoretical contributions to understanding the optimization dynamics of deep neural networks.  Strengths: 1. Thorough Analysis: The paper conducts detailed theoretical and empirical investigations into the behavior of sharpness in neural networks providing a clear understanding of the Edge of Stability phenomenon. 2. Empirical Findings: The empirical identification of the output layer weight norm as an indicator of sharpness dynamics adds depth to the analysis. 3. Division of Trajectory: The division of the optimization trajectory into four distinct phases provides a structured approach to understanding the dynamics during training.  Weaknesses: 1. Assumptions: The paper relies heavily on specific assumptions such as the behavior of eigenvalues and the relationship between norms and sharpness. Ensuring the robustness of the conclusions under different assumptions could strengthen the findings. 2. Rigorous Explanations: While the paper presents valuable insights some theoretical arguments and assumptions could be further elaborated and made more rigorous. 3. HighFrequency Oscillations: The theory does not fully explain highfrequency oscillations in sharpness suggesting the need for further investigation into additional factors influencing sharpness dynamics.  Questions and Suggestions for Authors: 1. Validation of Assumptions: How sensitive are the conclusions to variations in the assumptions made about the behavior of eigenvalues and norms 2. Impact of Other Layers: Considering the observed relation between inner layers and sharpness could future work delve into the influence of layers beyond the output layer 3. Generalization: How applicable are the findings to deeper neural networks with nonlinear activations and are there plans to extend the analysis to more complex architectures  Limitations: The authors could expand on the potential negative societal impacts of their work such as biases in neural network training or ethical implications of edge of stability phenomenon to enhance the discussion and promote responsible research practices. Consider discussing ethical considerations when presenting findings that might have broader implications.", "rjbl59Qkf_": " Summary The paper investigates the limitations and implications of Generalized Reweighting (GRW) algorithms for addressing distributional shift in machine learning models. The main contribution is revealing that various GRW methods including importance weighting and Distributionally Robust Optimization (DRO) variants  commonly used to enhance empirical risk minimization (ERM)  fail to outperform ERM in achieving distributionally robust generalization. The study offers theoretical insights through detailed analyses of various scenarios especially focusing on linear models and wide neural networks emphasizing the ineffectiveness of GRW in improving model robustness under distribution shift.  Strengths 1. Theoretical Rigor: The paper provides comprehensive theoretical analyses of GRW approaches offering insights into the limitations of these methods. 2. Methodological Approach: The study carefully examines different scenarios and clearly presents the implications of its findings on model generalization under distribution shift. 3. Valuable Insights: By revealing the shortcomings of existing GRW algorithms the paper sets the stage for future research directions to enhance distributionally robust generalization.  Weaknesses 1. Empirical Validation: While the theoretical analysis is robust the lack of extensive empirical validation limits the immediate applicability of the findings. 2. Complexity: The intricate theoretical proofs may be challenging for general readers without a strong background in the subject matter.  Questions and Suggestions for Authors 1. Empirical Validation: Have you considered conducting more empirical experiments to further validate the theoretical findings and bridge the gap between theory and practice 2. Clarity: Can you simplify key theoretical concepts to improve accessibility for a broader audience considering the technical complexity of the paper 3. Comparative Analysis: How do the limitations and implications identified for GRW algorithms compare with other stateoftheart methods addressing distributional shift  Limitations and Societal Impact The authors have addressed limitations by highlighting strong assumptions and theoretical focus. To further enhance the credibility and relevance of the work consider incorporating empirical validation and expanding discussions on how these findings may impact realworld applications.  Conclusion The study provides valuable insights into the limitations of GRW algorithms laying the groundwork for future advancements in achieving distributionally robust generalization. Further empirical validation and clearer presentation of theoretical concepts could enhance the impact of the research.", "q85GV4aSpt": "Summary: The paper explores the theoretical understanding of square loss in classification for deep neural networks. It systematically investigates the properties of overparameterized neural networks trained using square loss highlighting aspects such as generalization error robustness and calibration error. The findings are backed by both theoretical analyses and empirical studies on synthetic and real data. Strengths: 1. Theoretical Depth: The paper delves into the theoretical underpinnings of square loss for deep classifiers offering insights into its convergence rates robustness and calibration error. 2. Empirical Validation: Extensive empirical studies on synthetic and real datasets reinforce the theoretical findings showcasing the effectiveness of square loss compared to crossentropy. 3. Novel Contributions: The paper makes novel contributions by bridging the gap between theoretical analysis and practical performance of square loss in deep learning classification tasks. 4. Clarity and Organization: The paper is wellstructured with clear section divisions and coherent flow of information aiding in understanding complex theoretical concepts. Weaknesses: 1. Limitation of RealWorld Applicability: While the theoretical analysis is rigorous the direct translation of findings to practical settings beyond the NTK regime may require further empirical validation. 2. Complexity: The technical nature of the paper might pose challenges for readers with limited statistical and mathematical background. Questions and Suggestions: 1. Extension Beyond NTK Regime: How do you plan to validate the applicability of the theoretical findings to more common neural network architectures that do not strictly fall within the NTK regime 2. Impact on LargeScale Datasets: Have you considered the scalability and computational efficiency of using square loss on largescale datasets such as ImageNet or for multiclass classification tasks 3. Generalization to Other Loss Functions: How do the proposed theoretical results compare with alternative loss functions beyond crossentropy and square loss especially in terms of robustness and calibration Limitations: Authors could expand on the discussion regarding the potential negative societal impact of their work and provide suggestions for mitigating any possible adverse consequences.", "zSkYVeX7bC4": "Summary: The paper investigates the ability of transformerbased language models to generalize from short to long problem instances termed \"length generalization.\" It explores the limitations of finetuning alone and introduces the concept of scratchpad prompting combined with incontext learning to improve length generalization capabilities. The study focuses on two algorithmic tasks parity and variable assignment to understand failure modes and learning strategies of large language models. Strengths: 1. Clear Problem Definition and Methodology: The paper clearly defines the problem of length generalization and provides a systematic methodology to evaluate model performance. 2. Empirical Analysis: The authors conduct thorough empirical studies using different techniques (finetuning scratchpad prompting incontext learning) and tasks to analyze the limitations and improvements in length generalization. 3. Novel Contribution: The introduction of scratchpad prompting combined with incontext learning as a solution to enhance length generalization is a novel and significant contribution to the field of language models. 4. Indepth Error Analysis: The paper provides detailed error analyses on the failure modes of different learning modalities which offers valuable insights into model behaviors. Weaknesses: 1. Lack of Realworld Application: The study primarily focuses on synthetic algorithmic tasks which may limit the generalizability of the findings to realworld applications. 2. Limited Dataset Variety: The paper does not discuss the diversity and complexity of datasets used for evaluation which could impact the generalizability of the proposed solutions. 3. Theoretical Analysis: The paper lacks theoretical discussions or analyses to complement empirical findings and provide a deeper understanding of the underlying mechanisms affecting length generalization. 4. Evaluation Metrics: The paper could benefit from including a broader range of evaluation metrics to assess model performance comprehensively. Questions and Suggestions for Authors: 1. Would it be beneficial to evaluate the proposed scratchpad prompting techniques on realworld language understanding tasks to validate their effectiveness outside synthetic tasks 2. How do the introduced methodologies handle variations in problem complexity and dataset diversity compared to traditional finetuning methods 3. Have you considered incorporating additional ablation studies to further isolate the impact of each technique on length generalization performance 4. In what practical scenarios do you envision the scratchpad prompting approach being most beneficial and how scalable is this methodology for tasks with varying levels of complexity Limitations: The authors could further address the potential negative societal impact of promoting highly complex language models that may inadvertently exacerbate biases or ethical concerns. They should also specify guidelines for responsible deployment and transparent reporting of results.", "ySB7IbdseGC": " 1. Summary: The paper introduces the Structured Recognition Variational Autoencoding framework focusing on incorporating structured recognition in unsupervised learning for complex data modeling. The framework extends the paradigm of unsupervised learning beyond density estimation by capturing posterior correlations induced by the \"explaining away\" effect. The proposed SRnlGPFA model successfully identifies latent signals from highdimensional neural spike data correlating with behavioral covariates.  2. Assessment of Strengths and Weaknesses: Strengths:  Novel approach: The introduction of structured recognition in the variational autoencoding framework for unsupervised learning represents a novel contribution.  Comprehensive evaluation: The paper includes experiments on synthetic data EEG datasets and neuronal firing data demonstrating the effectiveness of the proposed SRnlGPFA model compared to baselines.  Theoretical grounding: The paper provides a strong theoretical background addressing recognition potentials variational inference and structured probabilistic graphical models. Weaknesses:  Complexity: The framework seems complex and may require a high level of expertise to implement and replicate the experiments.  Limited generalizability: The paper focuses heavily on empirical results and may benefit from further discussion on the broader implications and applicability of the proposed framework.  3. Questions and Suggestions for Authors: Questions: 1. Can you elaborate on the computational complexity of the proposed SRnlGPFA model compared to existing methods 2. How scalable is the SRnlGPFA approach in terms of the size and complexity of datasets that it can effectively handle 3. Will the proposed framework be able to capture more complex latent structures beyond the experiments conducted in this paper Suggestions: 1. Provide further analysis on the interpretability of the latent signals identified by the SRnlGPFA model particularly in realworld applications. 2. Consider discussing the potential challenges in deploying the framework in practical scenarios and any recommended strategies for overcoming them. 3. Explore the possibility of extending the framework to address different types of data modalities and applications in the future.  4. Limitations: While the paper provides extensive empirical validation it would be beneficial if the authors explicitly address the potential negative societal impact of their work particularly concerning data privacy bias or misuse. Suggestions for improvement may include incorporating ethical considerations into the discussion section and outlining strategies to mitigate any negative impacts.", "zVglD2W0EAS": " Summary: The paper introduces DrugRec a novel causal inferencebased drug recommendation model that addresses three key factors in drug recommendation: bias elimination utilization of historical health condition and coordination of multiple drugs for safety. DrugRec leverages causal graphical models frontdoor adjustment multivisit modeling and a 2SAT algorithm for drugdrug interaction coordination. Extensive experiments on MIMICIII and MIMICIV datasets show stateoftheart performance in terms of recommendation quality and DDI rate reduction.  Strengths: 1. Novel Methodology: Introducing a causal graphical model for drug recommendation enriches the existing literature and addresses critical factors in drug recommendation tasks. 2. Comprehensive Approach: By considering bias elimination historical patient data and drugdrug interactions the proposed DrugRec model offers a holistic solution for more accurate and safer drug recommendations. 3. Experimental Results: Demonstrating significant performance improvements in key metrics like Jaccard score PRAUC and F1score along with substantially reduced DDI rates validates the effectiveness of the proposed method.  Weaknesses: 1. Exploration of Limitations: The paper lacks a thorough discussion on potential limitations of the proposed model such as scalability generalizability or realworld applicability. 2. TheorytoPractice Gap: While the theoretical framework and experimental results are robust practical implications including deployment feasibility and integration with existing systems are not explored. 3. Comparison with Baselines: The evaluation against baselines is essential for context but a deeper analysis of why DrugRec outperforms these methods would enhance the papers impact.  Questions and Suggestions: 1. Scalability: How does the proposed model scale with increasing dataset sizes or disparate healthcare settings 2. Realworld Applicability: Have you considered practical constraints like data availability interpretability and adoption by healthcare providers 3. Ethical Considerations: What steps are taken to ensure patient privacy and data security in the implementation of the drug recommendation model 4. Future Work: Consider discussing avenues for future research such as incorporating patient feedback realtime data updates or incorporating domainspecific knowledge to enhance the model further.  Limitations: The authors could enhance the paper by explicitly discussing the potential limitations of the proposed model including issues of data privacy interpretability and the risk of algorithmic bias. Providing suggestions for addressing these limitations would strengthen the papers impact. Your work showcases a novel approach with significant potential for advancing drug recommendation practices in healthcare. Addressing the mentioned points would further enhance the papers quality and make it more impactful in the research community.", "wZk69kjy9_d": "Summary: The paper introduces a practical method called Director for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. Director significantly outperforms exploration methods on tasks with sparse rewards demonstrating successful learning in various environments. It achieves this by leveraging representations of the world model to select goals in a compact discrete space aiding learning for the highlevel policy. Strengths: 1. Innovative Approach: The paper presents a novel method Director that addresses the limitations of hierarchical reinforcement learning without manual specification of subtasks or dense rewards. 2. Performance: Director outperforms existing exploration methods on tasks with sparse rewards and demonstrates success in a diverse range of environments. 3. Interpretability: The decisions made by Director are interpretable as the world model can decode goals into images for visualization providing insights into decision making. 4. Comprehensive Experiments: The paper provides thorough experiments across various benchmark suites showcasing the efficacy and generality of the proposed method. Weaknesses: 1. Complexity: The method involves multiple components and training steps which may make it challenging to implement and reproduce for researchers without a strong background in reinforcement learning. 2. Detailed Methodology: The detailed methodological explanation may be overwhelming for readers new to the field potentially impacting the ease of understanding. 3. Metrics Clarity: The paper lacks clarity regarding specific metrics or baselines for comparison which could make it challenging to quantify the improvement brought by Director over existing methods. Questions and Suggestions for the Authors: 1. Could you provide a more straightforward explanation of the key components of the Director method for readers less familiar with reinforcement learning 2. How does Director handle cases where the manager chooses suboptimal goals leading to inefficient task completion by the worker 3. Have you considered comparisons with other stateoftheart hierarchical reinforcement learning methods to demonstrate the superiority of Director comprehensively 4. Could you elaborate on the scalability of Director in more complex environments or with larger action spaces 5. Clarity on the computational requirements and efficiency of Director would enhance the applicability and practicality of the method for realworld applications. Limitations: The authors should provide further insights into the limitations and potential negative societal impacts of their work such as computational complexity action space scalability and the practicality of implementation in realworld scenarios. Implications on resource consumption and ethical considerations should be addressed to ensure responsible development and deployment of the proposed method.", "mux7gn3g_3": " Peer Review: 1) Summary: The paper investigates the effectiveness of metareinforcement learning (metaRL) approaches in diverse visionbased benchmarks compared to multitask pretraining with finetuning. The study evaluates three metaRL algorithms (Reptile PEARL RL2) on Procgen RLBench and Atari benchmarks. The findings indicate that multitask pretraining with finetuning performs equally well or better than meta pretraining with meta testtime adaptation on completely novel tasks. 2) Strengths:  Thorough Exploration: The paper addresses a significant gap in the literature by conducting a largescale study on visionbased metaRL across diverse tasks providing valuable insights into the effectiveness of different approaches.  Clarity and Detail: The paper effectively details the experimental setups algorithms and results across different benchmarks enhancing the reproducibility and understanding of the study.  Comparison of Approaches: By comparing metaRL algorithms to multitask pretraining with finetuning the study offers a comprehensive evaluation of different strategies highlighting the potential of simpler approaches in achieving competitive performance. 3) Weaknesses:  Limited Pretraining Tasks: A limitation of the study is the small number of pretraining tasks available for RLBench and Atari experiments potentially impacting the generalization of results. Additional pretraining tasks could enhance the robustness of the findings.  Assumption Clarification: The assumption of distinct nonoverlapping stateaction spaces between tasks should be further clarified to ensure the validity of evaluations especially in complex environments. 4) Questions and Suggestions:  Generalization and Robustness: How would the findings change with an increased number of pretraining tasks especially in scenarios with highly diverse environments Exploring the generalizability and robustness of different approaches under such conditions could provide further insights.  Task Complexity: How sensitive are the results to the complexity of the tasks and benchmarks chosen Investigating the scalability of the proposed methods to more complex scenarios beyond the studied benchmarks could extend the applicability of the study.  Further Performance Metrics: In addition to sample efficiency and final performance considering other metrics like computational costs scalability to larger datasets or ease of implementation could provide a more comprehensive evaluation of the approaches. 5) Limitations: The authors should address the limitation of the small number of pretraining tasks by expanding the pool of tasks to enhance the generalizability and reliability of the findings. Additionally further clarification on the assumptions made regarding stateaction spaces is recommended to ensure the robustness of the studys conclusions. 6) Constructive Suggestions: To improve the studys impact and relevance the authors could consider incorporating a broader range of pretraining tasks validating results in more complex environments providing additional performance metrics and refining the assumptions made to enhance the studys validity and applicability. Overall the paper presents a valuable contribution to the field of metareinforcement learning showcasing the comparative effectiveness of metaRL approaches and multitask pretraining. Addressing the identified weaknesses and limitations would strengthen the studys impact and reliability.", "tNXumks8yHv": " Peer Review of the Paper: \"Unifying Probabilistic Framework for Dimension Reduction Methods\" Summary: The paper introduces a unifying statistical framework for dimension reduction methods based on the coupling of hidden graphs using cross entropy. It establishes a probabilistic model that encompasses popular embedding methods and shows new connections with PCA. The contributions include revealing the limitations of current stateoftheart techniques and proposing solutions for improving the preservation of global structures in embedded spaces. Strengths: 1. Innovative Framework: The paper introduces a novel statistical framework that unifies various dimension reduction methods and provides insights into their limitations. 2. Clear Contributions: The contributions are clearly outlined and supported by theoretical analysis and empirical evaluations. 3. Theoretical Depth: The paper demonstrates a strong theoretical foundation by deriving the relationships between existing methods and proposing solutions based on probabilistic models. 4. Experimental Validation: Experiments particularly the ccPCA initialization approach provide practical evidence supporting the proposed frameworks effectiveness. 5. Relevance to the Field: Addressing the gap between theory and practice in dimension reduction methods the paper provides valuable contributions to the field. Weaknesses: 1. Complexity: The paper may be challenging for readers unfamiliar with advanced mathematical concepts and probabilistic models. 2. Lack of Comparative Analysis: While the paper extensively discusses the proposed framework a comparative analysis with other existing methods could further strengthen the evaluation. 3. Empirical Validation: While the experiments showcase promising results additional datasets and benchmarks could enhance the generalizability of the findings. Questions and Suggestions: 1. Could you elaborate on the practical implications of the proposed framework for realworld applications especially in fields where dimensionality reduction plays a critical role 2. Have you considered conducting experiments on a wider range of datasets including highdimensional data with varying complexities to demonstrate the generalizability of the framework 3. Can you provide more insights into the computational efficiency of the proposed approach compared to existing methods especially for largescale datasets Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work by focusing on methodological advancements in dimension reduction techniques. To improve they could discuss potential challenges in implementing the proposed framework in practical applications to enhance transparency and address ethical considerations. Overall the paper provides a significant contribution to the field of dimensionality reduction by introducing a unified probabilistic framework and offering solutions to address the limitations of current methods. Further refinements and validations could enhance the robustness and applicability of the proposed approach.", "wYGIxXZ_sZx": " Summary: The paper addresses the challenge of studying the generalization behavior of minimax learners in the context of stochastic optimization. It proposes a new metric the primal gap and provides theoretical bounds for studying the generalization of minimax learners. The authors analyze the performance of two algorithms gradient descentascent (GDA) and gradient descentmax (GDMax) in stochastic minimax optimization showcasing when GDA might generalize better than GDMax. The paper makes significant contributions by identifying the limitations of existing metrics and proposing a novel metric to address these shortcomings.  Strengths: 1. Original Contribution: The paper introduces a novel metric the primal gap to study generalization in minimax learners addressing a gap in the existing literature. 2. Theoretical Framework: The paper provides a rigorous theoretical analysis offering bounds for the new metrics generalization error in both convexconcave and nonconvexconcave settings. 3. Comparison of Algorithms: The comparison between GDA and GDMax adds value by demonstrating the practical implications of their generalization behavior in stochastic minimax optimization.  Weaknesses: 1. Complexity: The papers technical depth may pose a challenge for readers less familiar with optimization theory and machine learning. 2. Clarity: The detailed theoretical derivations and the abundance of assumptions might hinder the papers accessibility to a broader audience.  Questions and Suggestions for Authors: 1. Clarity on Assumptions: Could the authors provide a clearer explanation of the key assumptions made in the paper and their implications 2. Empirical Evaluation: Will the authors provide experimental results to validate the theoretical findings and practical implications of the proposed primal gap metric 3. Comparison Metrics: How does the proposed primal gap metric compare to existing metrics in terms of computational efficiency and ease of implementation  Limitations: The paper could benefit from a more straightforward presentation of complex theoretical concepts for wider dissemination and understanding. Consider providing practical examples or simplified explanations to improve accessibility to readers from diverse backgrounds.", "rP9xfRSF4F": " Summary The paper introduces the concept of Optimally Timed Intervention (OTI) policies for timely interventions before critical events in industrial settings. It proposes a machine learning approach to formulate the OTI problem as an optimal stopping problem on the hazard rate process linking to survival analysis and optimal stopping theory. The paper also presents a novel Dynamic Deep Recurrent Survival Analysis (DDRSA) architecture to model neural hazard rate processes and demonstrates superior performance over traditional intervention methods through experiments on realworld datasets.  Strengths 1. Innovative Contribution: The paper presents a novel problem formulation and a comprehensive machine learning approach to address timely interventions before critical events bridging optimal stopping and survival analysis. 2. Theoretical Soundness: The theoretical framework linking OTI to hazard rate processes is welldeveloped providing insights into optimal intervention policies. 3. Implementation and Experimentation: The proposed DDRSA architecture is effectively demonstrated to outperform static threshold policies in practical experiments on real datasets showcasing the effectiveness of the OTI approach.  Weaknesses 1. Complexity: The paper delves into intricate mathematical derivations and theoretical propositions which may be challenging for readers without a strong background in machine learning and optimization. 2. Empirical Evaluation: While the experiments show promising results additional comparative analyses against more baseline methods or sensitivity tests would enhance the robustness of the findings.  Questions and Suggestions 1. Clarification on Markovian Assumptions: How robust is the assumption of Markovian hazard rate processes in realworld scenarios Are there possible extensions to relax this assumption 2. Scalability Considerations: How does the proposed DDRSA architecture scale with larger datasets in terms of computational efficiency and model performance 3. Generalizability: Have you considered the transferability of the developed OTI policies to diverse industrial settings beyond the specific datasets used in the experiments  Limitations The authors could further discuss the following: 1. Limitation Acknowledgment: While the paper mentions the limitations related to hazard rate process accuracy a more indepth exploration of potential negative societal impacts or ethical considerations arising from automated intervention decisions could be beneficial. Overall the paper makes a significant contribution to the field by offering a novel approach to timely interventions supported by robust theoretical foundations and practical implementations. Further refinements addressing the identified weaknesses and limitations would enhance the impact and applicability of the proposed OTI framework.", "vQzDYi4dPwM": " Summary The paper discusses the interpolation problem for monotone data sets using monotone neural networks. It demonstrates that there are limitations to approximating monotone functions using monotone networks with nonnegative weights and explores the impact of restrictions on expressiveness and efficiency.  Strengths 1. Novel Contribution: The paper offers a novel perspective on the limitations and capabilities of monotone neural networks in approximating monotone functions. 2. Theoretical Rigor: The proofs and explanations provided are detailed and rigorous showcasing a deep understanding of the concepts. 3. Thorough Analysis: The paper thoroughly explores the complexities of monotone networks highlighting both their potential and limitations. 4. Clarity: The presentation of ideas definitions and results is clear aiding in understanding the intricate details of the study.  Weaknesses 1. Empirical Validation: The paper lacks empirical validation of the theoretical claims made which could enhance the practical relevance of the findings. 2. Limited Scope: The study focuses primarily on threshold activation functions neglecting the exploration of other activation functions commonly used in neural networks. 3. Practical Applications: The paper does not discuss practical implications or realworld applications of the theoretical findings limiting its broader relevance.  Questions and Suggestions for Authors 1. Empirical Testing: Have you considered conducting experiments to validate the theoretical claims made about the limitations of monotone neural networks in approximating functions 2. Generalization Bounds: How might the results on monotone neural networks impact the development of generalization bounds for such networks 3. Extensions to Other Activation Functions: Are there plans to explore the effects of nonnegative weights on the approximating capabilities of monotone networks with activation functions other than thresholds  Limitations The authors should address the lack of empirical validation in their study by conducting experiments to showcase the practical implications of their theoretical findings. Additionally they could discuss potential realworld applications of monotone neural networks to enhance the societal impact of their work.  Constructive Suggestions for Improvement 1. Empirical Validation: Include empirical results to validate the theoretical findings and demonstrate the practical implications of using monotone neural networks. 2. Discussion on Realworld Applications: Expand the discussion to include potential realworld applications of monotone networks to enhance the relevance of the study. 3. Exploration of Other Activation Functions: Consider extending the study to explore the impact of nonnegative weights on monotone networks with activation functions beyond thresholds for a broader understanding.", "lSfrwyww-FR": " PeerReview of \"Blackbox Attacks via Surrogate Ensemble Search\"  Summary: The paper introduces a novel method Blackbox Attacks via Surrogate Ensemble Search (BASES) which combines transfer and querybased approaches to generate highly successful blackbox attacks with minimal query requirements. The method employs a perturbation machine to generate perturbations based on weights assigned to surrogate models achieving high success rates with as few as 3 queries for targeted attacks. Extensive experiments demonstrate superior performance compared to stateoftheart methods including successful attacks on Google Cloud Vision API and object detection models.  Strengths:  Innovative Approach: The integration of transfer and querybased methods through surrogate ensemble search is a novel and effective strategy.  Superior Performance: Demonstrated superior performance with high success rates and significantly fewer queries compared to existing methods across a variety of models.  Extensive Experiments: Comprehensive experimentation on image classifiers and practical blackbox settings showcasing the methods robustness and effectiveness.  Code Availability: Providing access to the codebase on GitHub enhances reproducibility and promotes research transparency.  Weaknesses:  Computational Resources: The perturbation generation process over large surrogate ensembles can be computationally expensive which could limit scalability in certain scenarios.  Limited Access Settings: While the method excels in limitedaccess settings further exploration on its effectiveness in unrestricted query environments could provide a comprehensive understanding of its applicability.  Ensemble Diversity: The reliance on a diverse ensemble for successful attacks may present challenges when targeting models for which diverse surrogate models are unavailable.  Questions and Suggestions: 1. Ensemble Selection: How was the set of 20 models for the surrogate ensemble chosen Can you elaborate on the criteria for selecting these specific models 2. Targeted vs. Untargeted Attacks: Can you provide insights into the performance differences between targeted and untargeted attacks especially in scenarios where multiple target labels are considered 3. Generalization: How does the method perform when transitioning from image classification tasks to more complex tasks like natural language processing or medical imaging 4. Scalability: Are there plans to optimize the perturbation generation process for larger ensemble sizes or to enhance computational efficiency in future iterations of the methodology  Limitations: 1. Diverse Ensemble Requirement: While the methods lowdimensional search space is an advantage a diverse surrogate ensemble is crucial for attack success potentially limiting applicability to tasks with limited model diversity. 2. Computational Overhead: The computational cost of perturbation generation over large surrogate ensembles could be a limiting factor for scalability. In conclusion the paper presents a promising advancement in blackbox adversarial attacks offering a queryefficient solution with impressive success rates. Addressing scalability issues and further exploring its adaptability to diverse tasks could enhance the methods utility and impact. [Corresponding Authors Contact Information: Zikui Cai (zcai032ucr.edu) and M. Salman Asif (sasifucr.edu)]", "lTKXh991Ayv": " Summary: The paper introduces a novel adversarial spatiotemporal attack framework for disrupting spatiotemporal traffic forecasting models. The framework consists of two key steps: identifying timedependent victim nodes and generating adversarial traffic states using gradientbased methods. The paper provides theoretical underpinnings for the adversarial attack strategy and demonstrates its effectiveness through extensive experiments on realworld datasets showing significant performance degradation on spatiotemporal forecasting models.  Strengths: 1. Innovative Contribution: The paper presents a novel adversarial spatiotemporal attack framework to assess the vulnerability of spatiotemporal traffic forecasting models offering valuable insights into the robustness of such models. 2. Extensive Experiments: The work includes thorough experiments on realworld datasets demonstrating the practical effectiveness and robustness of the proposed framework. 3. Theoretical Analysis: The authors provide a theoretical upper bound analysis of the proposed attack strategy enhancing the understanding of the attack performance. 4. Practical Applicability: The proposed framework is modelagnostic and easily adaptable to different attack settings providing a versatile approach for disrupting spatiotemporal traffic forecasting models.  Weaknesses: 1. Limited Baseline Comparison: The paper could benefit from a more extensive comparison with existing adversarial attack strategies in similar domains to provide a broader perspective. 2. Lack of Ethical Considerations: The paper does not deeply address the ethical implications and potential negative societal impacts of adversarial attacks on critical systems like Intelligent Transportation Systems.  Questions and Suggestions: 1. Generalization: How do you ensure the generalizability of the proposed adversarial attack framework across different spatiotemporal forecasting models and datasets 2. Scalability: Have you considered the scalability of the framework concerning larger traffic networks and more complex spatiotemporal data structures 3. Adversarial Training: How does adversarial training with the proposed attacks impact the overall computational efficiency and training convergence of spatiotemporal forecasting models 4. RealWorld Implementation: What are the practical implications and challenges of deploying the proposed adversarial attack framework in realworld Intelligent Transportation Systems  Limitations: The authors have not extensively addressed the potential negative societal impact of their work. They could consider discussing ethical considerations related to adversarial attacks on critical systems and provide recommendations for mitigating these risks enhancing the sociotechnical implications of their research.  Constructive Suggestions: 1. Ethical Impact Assessment: Include a dedicated section exploring the ethical implications of adversarial attacks on spatiotemporal forecasting models and propose strategies to mitigate negative societal consequences. 2. Robustness Evaluation: Conduct additional experiments to evaluate the frameworks robustness against diverse adversarial attack scenarios and data perturbations to enhance the frameworks realworld applicability. Overall the paper presents an innovative approach to adversarial attacks on spatiotemporal traffic forecasting models with strong empirical validation and theoretical foundations. Addressing the identified weaknesses and expanding the discussion on ethical considerations could further enhance the papers impact and relevance in the research community.", "oMhmv3hLOF2": " Summary: The paper introduces a novel method StreamRF for reconstructing streaming radiance fields in dynamic scenes for novel view synthesis. It incrementally learns perframe model differences to adapt a base model onthefly. The approach leverages explicit grid representations narrowband tuning model difference compression and pilot model guidance to achieve efficient training and storage optimization. Experimental results demonstrate significant speedup in training efficiency over stateoftheart methods with competitive rendering quality.  Strengths: 1. Efficient Incremental Learning: The incremental learning paradigm enables onthefly handling of video sequences achieving fast training convergence. 2. Utilization of Explicit Grids: Leveraging explicit grid representations for dynamic scenes allows for reduced storage overhead and improved training efficiency. 3. Innovative Strategies: The use of narrowband tuning model difference compression and pilot model guidance showcases creative approaches for optimizing training and storage. 4. Experimental Results: Empirical evidence indicates substantial speedup in training efficiency rendering quality and storage reduction compared to existing methods.  Weaknesses: 1. Lack of Detailed Comparisons: While the results demonstrate efficacy a detailed comparison with existing methods on specific metrics could provide a more comprehensive evaluation. 2. Theoretical Analysis: Further theoretical analysis on the impact of different parameters and strategies could enhance the understanding of the proposed methods effectiveness. 3. Generalization: The paper lacks discussion on the generalization of the approach to diverse dynamic scenarios and potential challenges faced in realworld applications.  Questions and Suggestions for Authors: 1. How does the proposed method handle complex deformations in dynamic scenes and does it have limitations in capturing intricate movements 2. Have you considered the impact of noise or environmental factors in realworld scenarios on the training efficiency and rendering quality 3. Is there a plan to extend the framework to incorporate domainspecific priors or additional supervision for enhancing the representation of specific objects or scenes 4. Could the authors discuss the scalability of the method to longer sequences and potential challenges associated with handling extremely dynamic scenarios  Limitations: The authors should elaborate on the negative societal impact of biases in training data or algorithmic decisions especially in applications such as surveillance or AIpowered content generation to ensure ethical considerations are sufficiently addressed. Suggestions for improvement could include incorporating fairness and transparency assessments into the methodology or integrating explainable AI techniques to enhance interpretability.", "uzqUp0GjKDu": " Summary: The paper introduces unsupervised learning under Latent Label Shift (LLS) where label marginals can shift across domains but class conditionals remain consistent. The work establishes a new principle for identifying classes based on elements that shift together grouping together. The paper formalizes the LLS problem establishes an isomorphism with topic modeling for finite input spaces and proposes the DiscriminateDiscretizeFactorizeAdjust (DDFA) framework for highdimensional continuous input spaces. DDFA leverages domaindiscriminative models to improve upon competitive unsupervised classification methods.  Strengths: 1. Theoretical Foundation: The paper provides a solid theoretical framework establishing identifiability conditions and proposing a novel approach for handling the latent label shift problem. 2. Algorithm and Framework: The DDFA framework is wellreasoned and the steps of Discriminate Discretize Factorize and Adjust are logically structured to address the complexities of unsupervised learning under LLS. 3. Empirical Evaluation: The experiments on CIFAR datasets and the FieldGuide dataset demonstrate the effectiveness of the proposed DDFA framework in improving unsupervised classification accuracy compared to competitive baselines.  Weaknesses: 1. Evaluation vs. Baselines: While DDFA shows improvements in certain scenarios compared to SCAN baseline there are instances where it falls short of the performance achieved by the baseline models particularly on CIFAR10. 2. Scalability: The paper lacks discussion on the scalability and efficiency of the proposed DDFA framework especially when applied to larger datasets or highdimensional continuous input spaces. 3. Clarity and Accessibility: The technical language and complexity of the paper may hinder understanding for readers not specialized in the field of unsupervised learning or distribution shift.  Questions and Suggestions for Authors: 1. Could the paper provide more insights into the scalability of the DDFA framework particularly when dealing with larger datasets beyond CIFAR and highdimensional continuous input spaces 2. How does the DDFA framework handle noise and outlier points in the domain discrimination process and does it have any robustness metrics for noisy data 3. Would it be beneficial to include more visual aids or diagrams to enhance the understanding of the proposed LLS problem the DDFA framework and the experimental setup 4. Have you considered exploring the interpretability of the domains discovered by DDFA in order to provide more insights into the clustering and classification decisions  Limitations: The paper adequately addresses the limitations and negative societal impacts of the work. However for improvement authors could consider providing clearer explanations on the practical implications and applicability of the DDFA framework in realworld scenarios. Overall the paper presents a valuable contribution to unsupervised learning under Latent Label Shift with a strong theoretical foundation and promising empirical results through the DDFA framework. Further clarity and scalability considerations could enhance the impact and practical usability of the proposed framework.", "wSVEd3Ta42m": "Summary: The paper addresses the problem of learning a risksensitive policy based on the CVaR risk measure using distributional reinforcement learning. It critiques the standard actionselection strategy in distributional Bellman operator applications proposing modifications. The paper empirically shows that the proposed algorithm can learn better CVaRoptimized policies on synthetic and realworld data. Strengths: 1. Novel Contribution: The paper provides a novel approach to learning risksensitive policies using distributional reinforcement learning. This novel contribution enhances the utility of distributional RL for CVaR optimization. 2. Theoretical Grounding: The paper presents a strong theoretical foundation demonstrating the limitations of existing strategies and proposing a solution. 3. Empirical Evaluation: The empirical results showcase the effectiveness of the proposed algorithm on both synthetic and realworld data providing evidence of its potential practical utility. 4. Clear Presentation: The paper is wellstructured providing a thorough explanation of the problem domain theoretical underpinnings and empirical results. Weaknesses: 1. Theoretical Clarity: While the theoretical analysis is robust it may be challenging for readers less familiar with reinforcement learning and CVaR concepts to fully grasp the nuances. 2. Limited Comparison: The paper primarily compares the proposed approach with the Markov actionselection strategy lacking comparisons with a broader range of existing CVaR optimization methods. 3. Algorithm Complexity: The proposed algorithm although effective may be computationally expensive due to the high number of quantiles used for representation. Questions and Suggestions for Authors: 1. Can you provide additional insights into how the proposed algorithm compares with existing stateoftheart methods for CVaR optimization in reinforcement learning 2. Have you considered implementing the proposed algorithm on a wider range of RL tasks to evaluate its generalizability and robustness across diverse environments 3. How does the proposed approach perform in scenarios with highdimensional state and action spaces common in realworld applications 4. Could you discuss potential limitations or tradeoffs of the proposed algorithm in terms of computational overhead and scalability Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. To enhance societal awareness the authors could discuss the ethical implications of using risksensitive policies in decisionmaking contexts especially in highstakes domains. Overall the paper presents a significant advancement in learning risksensitive policies through distributional RL with strong theoretical foundations and promising empirical results. Addressing the above suggestions could further strengthen the impact and clarity of the research.", "slKVqAflN5": " Summary: The paper introduced a method for minimizing Lipschitz functions with an efficiency guarantee of O(\u03b5\u22124) building on Goldstein\u2019s subgradient method. The authors demonstrated that the method can work on any Lipschitz function without the need for directional differentiability. They also developed a new cutting plane algorithm for better efficiency in low dimensions.  Strengths: 1. Novel Contributions: The paper presents innovative modifications to existing algorithms for efficient minimization of Lipschitz functions expanding the applicability without requiring directional differentiability. 2. Efficiency Improvement: The new cutting plane algorithm improves efficiency in low dimensions offering competitive complexity results compared to existing methods. 3. Theoretical Grounding: The paper is welltheorized with Lemmas and Theorems supporting the algorithmic efficiency and correctness.  Weaknesses: 1. Complexity Analysis: The complexity analysis may need more detailed explanation or numerical examples to better showcase the efficiency gains. 2. Algorithm Implementation: It would be beneficial to provide insights or examples on how the proposed algorithms can be practically implemented in realworld scenarios.  Questions for Authors: 1. How does the proposed method compare in terms of convergence speed with existing stateoftheart algorithms for minimizing Lipschitz functions 2. Have realworld applications or datasets been used to validate the effectiveness and efficiency of the proposed algorithms 3. Can the authors elaborate on any computational or memory requirements for implementing the algorithms especially in scenarios with largescale optimization problems  Suggestions for Authors: 1. Provide illustrative examples or experiments to showcase the practical applicability and efficiency of the proposed algorithms in realworld optimization tasks. 2. Consider discussing the potential scalability of the algorithms to largescale optimization problems commonly encountered in machine learning and data analysis applications.  Limitations: The authors have outlined the limitations of their work focusing on the extension to the stochastic setting. Further exploration and research in this direction could enhance the practical utility and robustness of the proposed algorithms.", "tq_J_MqB3UB": " Summary The paper introduces Diverse Weight Averaging (DiWA) a new weight averaging (WA) strategy that aims to increase functional diversity across averaged models in order to improve outofdistribution (OOD) generalization. Through a biasvariancecovariancelocality decomposition the paper explores the success of WA in addressing diversity shifts. Empirical findings demonstrate that DiWA consistently outperforms existing methods on the DomainBed benchmark showcasing its effectiveness.  Strengths  Theoretical Foundation: The paper provides a robust theoretical analysis contributing a biasvariancecovariancelocality decomposition for WA and its expected error shedding light on the success of WA in OOD scenarios.  Novelty: Proposing DiWA as an approach to enhance diversity across models is a significant and innovative contribution to the field.  Empirical Validation: The empirical results provide strong evidence of the effectiveness of DiWA with consistent improvements over existing methods showcasing its practical applicability.  Weaknesses  Clarity: The paper is dense with technical terms and complex equations potentially making it challenging for readers to grasp the concepts easily.  Limitations: The limitations of DiWA such as its susceptibility to correlation shifts and the need for ideal hyperparameter ranges are discussed but could be further addressed.  Questions and Suggestions for Authors 1. Hyperparameters Impact: How sensitive is DiWA to hyperparameter choices across different datasets and how can this sensitivity be minimized 2. Scalability: Are there scalability concerns with regards to implementing DiWA on larger or more complex datasets 3. Correlation Shift Handling: How can DiWA be enhanced to address correlation shifts which are not effectively mitigated by current approaches  Limitations The paper addresses limitations well but authors could further discuss how to mitigate the negative societal impact of potentially relying on deep learning models for critical realworld applications. Constructive suggestions may include incorporating fairness considerations or ethical guidelines in model development. Overall the paper presents a valuable contribution to the field of neural network generalization under distribution shifts through the innovative DiWA strategy. Celebrating its theoretical underpinnings and empirical success further clarifications and expansions in certain areas could enhance the overall impact and accessibility of the paper.", "nQcc_muJyFB": " Summary: The paper focuses on exploring the role of the feature projector in knowledge distillation proposing a new feature distillation method based on a projector ensemble to improve performance. The study finds that adding a projector benefits the student network even when the student and teacher have identical feature dimensionalities. Empirical results highlight the importance of the projector in improving student performance. The proposed ensemble of projectors further enhances the quality of student features as demonstrated through experiments on different datasets.  Strengths: 1. Novel Approach: The paper introduces a novel feature distillation method based on a projector ensemble showing improvements in student network performance. 2. Experimental Results: Comprehensive experiments on various datasets and teacherstudent pairs demonstrate the effectiveness of the proposed method. 3. Empirical Evidence: The study provides empirical evidence supporting the hypothesis that projectors can mitigate overfitting issues and enhance the quality of student features.  Weaknesses: 1. Clarity of Explanation: Some sections particularly the technical descriptions and formulations may benefit from further clarification or illustrative examples for better understanding. 2. Comparison with Existing Methods: A more detailed comparison with existing feature distillation methods could provide a clearer perspective on the proposed methods advantages and limitations. 3. Theoretical Analysis: Though the empirical results are robust more theoretical analysis or insights into the mechanism underlying the effectiveness of the projector ensemble could strengthen the paper.  Questions and Suggestions for Authors: 1. Could you provide insights into the selection of hyperparameters especially the choice of \u03b1 in the objective function (Eq. 5) and the number of projectors in the ensemble 2. How generalizable is the proposed method to different network architectures and datasets beyond the ones tested in this paper 3. Have you considered the computational overhead of using multiple projectors How does the performance gain justify the increase in training costs 4. Could a more indepth analysis be conducted on the impact of different initializations of projectors in improving diversity and generalizability  Limitations and Suggestions for Improvement: The paper could enhance its impact by addressing the following:  Limitations: There could be more clarity in addressing limitations and potential negative societal impacts of the proposed method providing a more comprehensive assessment.  Improvement Suggestions: Including a section discussing potential limitations and societal impacts along with suggestions for mitigating any negative consequences could improve the papers overall completeness.", "rhdfTOiXBng": " Summary: The paper introduces NATURALPROVER a novel language model that aims to generate proofs in natural mathematical language by utilizing background references and constrained decoding. The model is evaluated on two tasks: suggesting next steps in a mathematical proof and generating full proofs. NATURALPROVER outperforms finetuned GPT3 in terms of quality of nextstep suggestions and generated proofs especially in short proofs with 26 steps. Human evaluations by mathematics students show promising results for the models performance.  Strengths: 1. Innovative Approach: The paper explores an important yet underexplored area of generating mathematical proofs using language models contributing to advancements in AIassisted mathematics. 2. Improvements Over Baseline: NATURALPROVER demonstrates superior performance compared to the conventional finetuned GPT3 in generating useful nextstep suggestions and correct proofs. 3. Human Evaluation: The inclusion of human evaluations by mathematics students provides valuable insights into the models feasibility and usefulness in a realworld scenario. 4. Clear Methodology: The paper provides a detailed methodology for training and evaluating the model making it replicable for future research in the field.  Weaknesses: 1. Logical Coherence on Longer Proofs: The paper acknowledges challenges the model faces with logical coherence in longer proofs which is a significant limitation that needs addressing for practical use. 2. Symbolic Derivations: The models struggles with performing multistep symbolic derivations point to potential limitations in handling more complex mathematical reasoning. 3. Limited Evaluation: While human evaluations are insightful broader and more diverse evaluative measures could provide a more comprehensive assessment of the models performance.  Questions and Suggestions: 1. Model Generalization: How does the model perform on unseen data or different mathematical domains beyond what was used for training and evaluation 2. Handling Diverse Mathematical Proofs: How does the model adapt to more diverse proof structures such as proofs by induction or complex mathematical reasoning beyond basic arithmetic 3. Scalability: Are there measures in place to ensure the scalability of NATURALPROVER to handle longer and more complex mathematical proofs efficiently 4. Future Applications: How do the authors envision deploying NATURALPROVER in realworld settings such as educational platforms or integrated into formal proof assistants  Limitations: The authors should further address the challenges related to logical coherence on longer proofs and symbolic derivations. Constructive suggestions for improvement could include refining the models training data and architecture to better accommodate these complex areas of mathematical reasoning.", "wsnMW0c_Au": " Summary: The paper proposes an algorithmic equivalence technique between nonconvex gradient descent and convex mirror descent. It addresses the problem of regret minimization in online nonconvex optimization and shows an O(T(23)) regret bound for nonconvex online gradient descent. Key contributions include introducing a new algorithmic equivalence method and extending results from continuoustime to discretetime algorithms.  Strengths: 1. Innovative Approach: The paper introduces a novel algorithmic equivalence technique that bridges the gap between nonconvex gradient descent and convex mirror descent providing valuable insights into nonconvex optimization. 2. Rigorous Analysis: The analysis is thorough and provides quantitative bounds under specific geometric and smoothness conditions contributing to a better understanding of the convergence of nonconvex optimization methods. 3. Generalizability: The results extend to arbitrary Lipschitz convex loss functions showcasing the broad applicability of the proposed method. 4. Clear Structure: The paper is wellstructured with defined sections that facilitate understanding and navigation through complex concepts.  Weaknesses: 1. Potential Practical Relevance: While the theoretical contributions are significant the practical implications or applicability of the findings to realworld scenarios could be better highlighted. 2. Clarity of Assumptions: The paper could benefit from a clearer explanation of some assumptions especially Assumption 1 to enhance the readers understanding. 3. Complexity: The technical nature of the material may pose challenges for readers unfamiliar with the specialized terminology in optimization theory.  Questions and Suggestions for Authors: 1. Practical Relevance: How do the findings of the study translate to practical applications in machine learning and optimization Providing examples or scenarios where this algorithmic equivalence technique can be applied would enhance the papers impact. 2. Relaxation of Assumptions: Are there opportunities to relax some of the assumptions made in the study to broaden the scope of the proposed algorithmic equivalence method 3. Comparison with Existing Methods: How does the proposed algorithmic equivalence technique compare to existing methods in terms of efficiency and convergence guarantees in nonconvex optimization problems 4. Experimental Validation: Are there plans to conduct empirical experiments to validate the theoretical results and demonstrate the performance of the proposed method in realworld optimization tasks  Limitations: The paper could benefit from a more robust discussion on the limitations and potential negative societal impacts of the work along with suggestions for improvement in these areas to ensure responsible and ethical research conduct.", "s7SukMH7ie9": "Summary: The paper explores the novel concept of adversarial training (AT) with complementary labels (CLs) aiming to equip machine learning models with adversarial robustness even when all given labels in a dataset are wrong. The authors identify challenges such as intractable adversarial optimization and lowquality adversarial examples in AT with CLs and propose a new learning strategy using gradually informative attacks. Extensive experiments on benchmark datasets demonstrate the effectiveness of their method. Strengths: 1. Innovation: The paper addresses a significant gap in the research by exploring AT with CLs a novel and practical setting. 2. Theoretical Insights: The authors provide theoretical analyses of the challenges faced in AT with CLs enhancing the understanding of the problem. 3. Proposed Method: The introduction of Warmup Attack and PseudoLabel Attack as components of a new learning strategy is wellmotivated and shows promising results. 4. Experimental Validation: Extensive experiments on various datasets and ablation studies confirm the performance of the proposed method. Weaknesses: 1. Complexity: The paper is dense with technical details potentially making it challenging for readers to follow without a strong background in the subject matter. 2. Limited Comparison: The comparison with existing methods could be more extensive to provide a comprehensive view of the proposed approachs performance relative to stateoftheart techniques. 3. Visualization: Figures and tables are crucial for understanding and some of the figures mentioned in the text are missing in the review. Questions and Suggestions: 1. Generalization: How robust is the proposed method to datasetspecific characteristics and variations in input data 2. Scalability: How does the proposed strategy scale with larger datasets and more complex models 3. Further Analysis: Could the authors provide more insights into the impact of hyperparameters on the performance and stability of the method 4. Practical Applications: Are there specific realworld applications where AT with CLs could be particularly beneficial Limitations: The authors have not fully addressed the potential negative societal impacts of their work. To enhance the social responsibility of the research it would be advisable to consider and acknowledge any ethical concerns or unintended consequences that may arise from implementing AT with CLs in realworld scenarios.", "kOIaB1hzaLe": " Summary: The paper proposes a new multiclass framework NREC for likelihoodtoevidence ratio estimation in simulationbased inference. The framework is designed to eliminate the bias inherent in the current NREB formulation allowing for better diagnostics and performance. The authors conduct experiments to benchmark different algorithms in various training regimes highlighting the performance and limitations of NREA NREB and the proposed NREC.  Strengths: 1. Novel Contribution: The paper introduces a novel framework NREC which addresses the bias issue present in existing frameworks providing a more reliable estimation of likelihoodtoevidence ratios. 2. Thorough Experiments: The authors conduct comprehensive experiments to evaluate the new framework in various training regimes providing valuable insights into its performance compared to existing algorithms. 3. Detailed Analysis: The paper offers detailed explanations and analyses of the algorithms loss functions and performance metrics enhancing the understanding of the proposed framework.  Weaknesses: 1. Complexity: The papers content is highly technical and may be challenging for general readers to grasp without a deep background in the field. 2. Lack of Simplicity in Presentation: The complex mathematical formulations and technical terms used throughout the paper might hinder the accessibility for broader readership. 3. Clarity Issues: Some sections especially in the Methods and Experiments may benefit from clearer organization and improved flow to enhance reader understanding.  Questions and Suggestions for Authors: 1. Clarification on Implementation Details: Could the authors provide more details on the practical implementation of the NREC framework How does it compare in terms of computational efficiency and scalability 2. Impact on Model Generalization: How does the proposed NREC framework impact the generalization capabilities of the model especially when applied to realworld problems with varying complexities 3. Robustness Evaluation: Have the authors conducted any robustness tests to assess the stability and consistency of the proposed framework under different conditions or noise levels  Limitations: The authors should ensure that the papers content is made more accessible to a wider audience by simplifying technical jargon and explaining complex concepts in a more approachable manner. Additionally providing practical examples or visual aids where possible could enhance comprehension.", "vmjckXzRXmh": " Peer Review: 1) Summary: Contrastive learning has proven to be an effective method for learning representations from unlabeled data. This paper delves into the phenomenon of linear transferability of contrastive representations in unsupervised domain adaptation settings. It provides theoretical analysis on when and why linear transferability occurs focusing on relationships between clusters in different domains. The paper introduces a novel linear head that leverages average representations within classes and a preconditioner matrix to achieve superior domain adaptation performance. 2) Strengths:  Theoretical Depth: The paper provides a rigorous theoretical analysis on linear transferability of contrastive representations extending the current understanding in unsupervised domain adaptation.  Innovative Approach: Introducing a novel linear classifier algorithm that incorporates both average representations and a preconditioner matrix is a significant contribution to the field.  Empirical Validation: The empirical results demonstrate the effectiveness of the proposed algorithm surpassing traditional linear probing methods on domain adaptation tasks.  Realistic Assumptions: The paper addresses the limitations of prior works by considering more realistic scenarios without stringent assumptions. 3) Weaknesses:  Complexity: The papers technical content may be challenging for readers without a deep background in mathematical analysis.  Limited Empirical Benchmarks: While the empirical results are promising a more extensive comparison with existing domain adaptation methods could provide a broader perspective on the performance of the proposed algorithm. 3) Questions and Suggestions for Authors:  Can you provide insights into the scalability of your proposed algorithm particularly in handling larger datasets or higherdimensional feature spaces  How do you plan to validate the generalizability of your findings across different types of datasets and domains 4) Limitations: The authors have adequately addressed the limitations of their work by acknowledging the potential complexity in empirical validation and the specialized technical nature of their theoretical analysis. To enhance the societal impact of their work they could consider providing a more accessible summary or visualizations to aid readers in understanding the key concepts. In conclusion this paper presents a novel perspective on contrastive learning and unsupervised domain adaptation backed by strong theoretical analysis and promising empirical results. Further work can focus on extending the algorithm to diverse datasets and realworld applications. Overall this research contributes significantly to the field of representation learning and domain adaptation.", "rApvGord7j": " Peer Review:  1) Summary: The paper investigates fair Bayesoptimal classifiers under predictive parity focusing on sufficiencybased measures like predictive parity. The authors show that when the overall performances of different protected groups vary moderately fair Bayesoptimal classifiers under predictive parity are groupwise thresholding rules. However if the performance levels vary widely predictive parity may lead to withingroup unfairness. They propose an algorithm FairBayesDPP to ensure predictive parity when their condition is satisfied. The algorithm aims to achieve fairness while maximizing test accuracy.  2) Strengths:  The paper addresses an important issue in fair machine learning by exploring predictive parity a less studied sufficiencybased fairness measure.  The theoretical results are welldeveloped proving the conditions under which fair Bayesoptimal classifiers are groupwise thresholding rules.  The FairBayesDPP algorithm is innovative and provides a practical solution for achieving predictive parity and maintaining accuracy.  3) Weaknesses:  The paper lacks a comprehensive comparison with existing stateoftheart methods for fairness in machine learning which could provide more context and insights into the proposed approachs effectiveness.  The experimental evaluation could be expanded to include more benchmarks and datasets to showcase the generalizability of the FairBayesDPP algorithm.  4) Questions and Suggestions:  Could the authors elaborate on how the FairBayesDPP algorithm adapts to different types of protected attributes and data distributions  It would be beneficial to include a detailed discussion on the computational complexity of the algorithm and its scalability to larger datasets.  How does the proposed algorithm handle cases where the sufficiencybased measure of predictive parity conflicts with other fairness metrics like demographic parity or equalized odds  5) Limitations: The paper could better address the potential negative societal impact of not considering other fairness metrics alongside predictive parity. Constructive suggestions for improvement could involve discussing how the algorithm can adapt to diverse fairness constraints to mitigate unintended consequences.  Overall Assessment: The paper provides valuable insights into fair machine learning by focusing on predictive parity and proposing an algorithm FairBayesDPP to achieve fairness. Strengthening the experimental evaluation with additional benchmarks and a broader comparison with existing methods would enhance the papers impact and practical relevance in the field. Addressing the limitations and societal impacts more explicitly would further strengthen the contribution of the work.", "ofRmFwBvvXh": " Summary: The paper presents new approximation error bounds with explicit prefactor for Sobolevregular functions using deep convolutional neural networks (CNNs). It introduces a novel error bound for approximating Sobolevregular functions and establishes a new approximation result for functions supported on an approximate lowerdimensional manifold. The study applies these results to establish nonasymptotic excess risk bounds for classification methods using CNNs.  Strengths: 1. Novel Contributions: The paper introduces new error bounds for approximating Sobolevregular functions with explicit prefactors and extends the results to functions supported on lowerdimensional manifolds which are significant contributions. 2. Application in Classification: The study applies the theoretical results to establish nonasymptotic excess risk bounds for classification using CNNs demonstrating practical implications. 3. Thorough Analysis: The paper provides detailed theoretical analysis discusses related work and presents clear derivations and results.  Weaknesses: 1. Limited Scope: The paper only focuses on binary classification problems using CNNs. It could benefit from considering broader applications such as multiclass classification or regression tasks. 2. Simplification of Realworld Data: The assumption of approximate lowerdimensional data may oversimplify realworld scenarios where multiple factors influence the data structure such as spatial invariance in image data. 3. Empirical Validation: While discussing the empirical successes of CNNs the paper lacks empirical validation to support the theoretical findings and demonstrate the practical relevance of the proposed bounds.  Questions and Suggestions for Authors: 1. Extension to Multiclass Classification: Have you considered extending the theoretical results to multiclass classification problems to enhance the applicability of the proposed bounds 2. Validation with Realworld Data: How do you plan to validate the proposed bounds using realworld datasets and scenarios to demonstrate the practical significance of the results 3. Incorporation of Structural Information: Have you explored incorporating additional structural information such as spatial invariance in the theoretical framework to improve the bounds for image classification tasks  Limitations: The paper could consider addressing limitations such as the oversimplification of data assumptions the need for empirical validation to support theoretical findings and the extension of results to broader applications like multiclass classification in order to enhance the studys impact and applicability. Consider incorporating these aspects for a more comprehensive analysis.", "yewD_qbYifc": "Summary: The paper introduces Priority Convention Reinforcement Learning (PCRL) to address challenges in tackling multiagent largediscreteactionspace (LDAS) problems. PCRL embeds priority conventions like positionbased agent priority and action priority to break symmetries and find optimal solutions efficiently. The efficacy of PCRL is demonstrated through experiments on multiagent applications showcasing its ability to handle LDAS problems more optimally than existing methods. Strengths: 1. Innovative Approach: Introducing priority conventions in RL to address LDAS problems is a novel and promising approach. 2. Theoretical Foundation: The paper provides theoretical proofs backing the effectiveness of the proposed method. 3. Experimental Validation: The experiments on multiagent applications validate the superiority of PCRL over existing methods demonstrating its efficacy and optimality. 4. Thorough Analysis: The paper extensively discusses the methodology related work and provides detailed experimental results. Weaknesses: 1. Lack of Comparative Analysis: While the experiments show the effectiveness of PCRL a more detailed comparison with existing stateoftheart methods could further reinforce the papers contributions. 2. Clarity in Methodology: The paper could provide a more detailed explanation of the learning schemes and actionselection strategies to enhance the understanding for readers. 3. Societal Impact Discussion: The paper lacks a detailed discussion on the societal impacts of the work which is an important aspect to consider in AI research. Questions and Suggestions for Authors: 1. Comparison: Can the authors provide a more detailed comparison with existing RL methods for multiagent systems to highlight the specific advantages of PCRL 2. Scalability: How does the performance of PCRL scale with an increasing number of agents in multiagent systems Has the method been tested on largerscale scenarios 3. Generalization: How well does PCRL generalize to different types of multiagent applications beyond the ones tested in the experiments 4. Future Directions: What are the next steps for further enhancing the performance and applicability of PCRL in realworld scenarios Limitations: The authors need to consider a more comprehensive discussion on the limitations and societal impacts of their work. They should provide constructive suggestions for addressing these aspects to enhance the overall quality and relevance of their research.", "utahaTbcHdP": " PeerReview  1) Summary: The paper delves into the phenomenon of Neural Collapse in deep neural networks beyond zero training error and its invariance to class imbalances. The study introduces SimplexEncodedLabels Interpolation (SELI) as an invariant characterization of the Neural Collapse phenomenon in the presence of class imbalances. Theoretical models and extensive experiments on synthetic and real datasets confirm convergence to the SELI geometry even with imbalances although convergence deteriorates with higher imbalances. The paper raises the importance of ridge regularization in adjusting the geometry under imbalances.  2) Strengths:  The paper makes a significant contribution by studying Neural Collapse and its invariance to class imbalances an area of growing interest in deep learning research.  The theoretical analysis is rigorous and insightful providing a deep understanding of how the learned embeddings and classifiers evolve with imbalances.  Extensive experiments on synthetic and real datasets support the theoretical findings adding weight to the conclusions drawn.  3) Weaknesses:  While the theoretical framework is welldeveloped the paper could benefit from further clarification on the practical implications of the findings. How can these insights be practically leveraged in realworld applications  The paper could provide more detailed comparisons with existing approaches and further discussion on the limitations of the proposed methodology.  4) Questions and Suggestions for Authors:  Could the authors elaborate on the practical implications of their findings in realworld scenarios with imbalanced data  Can the authors discuss the potential challenges in applying the proposed SELI geometry in practical deep learning models and suggest strategies for overcoming these challenges  How do the findings of the study contribute to improving the generalization performance of deep neural networks in the presence of class imbalances  5) Limitations: The authors could further address the potential negative societal impact of their work by discussing ethical considerations related to imbalanced data in machine learning models. Suggestions for improvement include considering bias mitigation techniques and fairnessaware algorithms for handling imbalanced data to prevent negative societal implications. In conclusion the paper presents a novel and intriguing investigation into the invariance of Neural Collapse to class imbalances. By strengthening the practical implications and addressing potential societal impacts the authors can enhance the significance and applicability of their work in the field of deep learning research.", "m16lH6XJsbb": " Summary: The paper introduces a novel approach Spectrum Random Masking (SRM) for data augmentation in imagebased reinforcement learning (RL) tasks. The main idea is to apply augmentation in the frequency domain instead of spatial domain to improve generalization. The paper provides a thorough theoretical foundation and conducted experiments on DMControl Generalization Benchmark to demonstrate the effectiveness of SRM in enhancing generalization potentials.  Strengths: 1. Innovative Approach: The paper introduces a novel approach of frequency domain augmentation which is a unique perspective in the field of RL. 2. Theoretical Rigor: The paper provides a solid theoretical foundation for the proposed method explaining the principles behind the approach with clarity. 3. Experimental Results: Extensive experiments are conducted on DMControl Generalization Benchmark showcasing the stateoftheart performance of SRM with strong generalization potentials.  Weaknesses: 1. Clarity in Presentation: While the theoretical concepts are well articulated the paper could benefit from improving the clarity of presentation especially in the methodology section. 2. Comparative Analysis: The paper would be enhanced by providing a more detailed comparison with existing methods and discussing limitations of those methods. 3. Reproducibility: More information on implementation details code availability and hyperparameter settings would enhance the reproducibility of the results.  Questions and Suggestions for Authors: 1. Can the authors provide more insights into the computational complexity of the proposed SRM method compared to spatialbased augmentation methods 2. How does the SRM method perform in scenarios with partial observability or where only state information is available instead of complete image observations 3. What are the implications of the random masking strategy on training dynamics and convergence behavior of the RL agent  Limitations: The authors have not explicitly addressed the negative societal impact of their work. To improve it is recommended to include a brief discussion on potential societal implications of the proposed method such as biases or ethical considerations related to the use of augmented data in RL systems. This can contribute to a wellrounded evaluation of the broader impact of the research.", "kgT6D7Z4Xv9": "Summary: The paper explores the concept of path independence in equilibrium models and its relation to upward generalization. It introduces the Asymptotic Alignment (AA) score as a metric for measuring path independence and shows a strong correlation between path independence and accuracy particularly on harder problem instances. Experimental interventions are conducted to promote or penalize path independence demonstrating a direct impact on generalization. The findings suggest that path independent models can better exploit additional testtime computation for improved accuracy on complex tasks. Strengths: 1. Innovative Concept: The paper presents a novel concept of path independence in equilibrium models shedding light on its importance for upward generalization. 2. Thorough Analysis: Extensive experiments including interventions and metrics like AA score provide a detailed investigation of the relationship between path independence and generalization. 3. Clear Structure: The paper is wellorganized with clear delineation of concepts experiments and results making it easy to follow. 4. Empirical Validation: The findings are supported by empirical results across various tasks reinforcing the significance of path independence for scalable testtime usage. Weaknesses: 1. Complexity: The technical nature of the paper may pose challenges for less experienced readers in understanding the intricate details of equilibrium models and path independence. 2. Ambiguity: The paper could benefit from clearer explanations and visual aids to enhance comprehension of the complex concepts discussed. 3. Limited Comparison: While the paper extensively discusses path independence it could benefit from comparing its approach and findings with existing methods for adaptively managing computational resources in neural networks. Questions and Suggestions for Authors: 1. Clarity on Training Methodology: Can the authors provide more details on the training methodology used for equilibrium models and the specific hyperparameters employed in the experiments 2. Generalization to Other Architectures: Are the findings regarding path independence specific to equilibrium models or could similar principles be applied to other types of neural network architectures 3. Validation on Larger Datasets: Have the experiments been validated on larger datasets to ensure the robustness of the proposed path independence metric across different scales of data and tasks 4. RealWorld Applicability: How feasible is it to implement interventions to promote path independence in practical applications and what implications could this have for realworld use cases Limitations: The authors have addressed the limitations and potential negative societal impacts of their work adequately. However providing further insights on the scalability and implications of adopting path independence as a general modeling principle could enhance the papers practical relevance and impact.", "xpR25Tsem9C": " PeerReview:  1) Summary: The paper presents HHVAEM a novel Hierarchical Variational Autoencoder model that addresses the limitations of existing VAE methods in handling mixedtype incomplete data. The model incorporates Hamiltonian Monte Carlo (HMC) with automatic hyperparameter tuning for improved approximate inference. The proposed HHVAEM outperforms existing baselines in tasks such as missing data imputation supervised learning with missing features and active information acquisition.  2) Strengths and Weaknesses:  Strengths:  Comprehensive review of related literature and clear motivation for the proposed model.  Novel approach of combining hierarchical VAE with HMC for improved inference.  Rigorous experimentation and quantitative and qualitative evaluations to demonstrate the effectiveness of HHVAEM.  Robust results across various tasks and datasets showcasing superior performance compared to existing methods.  Weaknesses:  Lack of comparison with a wider range of stateoftheart methods for handling mixedtype incomplete data.  The paper could provide more insights into the interpretability of the model and the learned representations.  3) Questions and Suggestions:  Questions:  How scalable is HHVAEM when applied to larger datasets with higher dimensions  Could the computational complexity of the model hinder its practical deployment in realtime applications  Suggestions:  Provide further analysis on the interpretability of the latent representations learned by HHVAEM.  Consider discussing potential scenarios or applications where HHVAEM might be particularly beneficial or challenging.  4) Limitations: The paper adequately addresses the limitations and societal impact of its work by highlighting the potential benefits of the proposed model in handling realworld complex datasets. For improvement the authors could provide a more detailed discussion on the ethical considerations related to the use of machine learning models for decisionmaking in critical domains. Overall the paper presents a valuable contribution to the field of deep generative models and provides a wellstructured and informative study on addressing the challenges of mixedtype incomplete data imputation and learning.", "wZEfHUM5ri": " Summary: The paper introduces a novel pretraining approach called Crossview Completion (CroCo) for learning representations tailored to 3D vision and lowerlevel geometric tasks. The method is based on training neural networks with pairs of images showing the same scene from different viewpoints where parts of one image are masked and need to be reconstructed using information from both images. The experiments demonstrate that CroCo leads to improved performance on monocular 3D vision tasks like depth estimation and can be directly applied to binocular tasks such as optical flow and relative camera pose estimation.  Strengths:  Innovative Contribution: The paper introduces a novel pretraining method specifically designed for 3D vision tasks filling a gap in the existing literature.  Experimental Rigor: The paper provides a detailed evaluation of the proposed method on various downstream tasks and datasets demonstrating its effectiveness.  Comparative Analysis: The comparison with existing stateoftheart methods and ablation studies adds depth to the evaluation of CroCo.  Clear Presentation: The paper effectively explains the methodology architecture and training details making it accessible to the reader.  Weaknesses:  HighLevel Semantic Tasks: The authors acknowledge that CroCo may underperform on highlevel semantic tasks compared to traditional MIM models due to the choice of the dataset. Addressing this limitation could strengthen the applicability of CroCo.  RealWorld Data: The reliance on synthetic renderings for training poses a limitation on the generalizability of the approach. The paper could benefit from discussing potential strategies for adapting CroCo to realworld images.  Questions and Suggestions for Authors: 1. Could you elaborate on potential strategies for adapting CroCo to realworld images and capturing diverse scenes without supervision 2. Have you considered exploring different datasets including realworld data to evaluate the robustness and generalizability of CroCo 3. Given the underperformance on highlevel semantic tasks have you thought about incorporating more objectcentric datasets like ImageNet for pretraining to enhance performance in these tasks 4. How does CroCo handle variations in lighting conditions and scene complexity Could augmentations or transformations be incorporated to improve robustness  Limitations: The authors have adequately discussed the limitation regarding the performance of CroCo on highlevel semantic tasks but could further enhance the discussion by exploring strategies to mitigate this limitation such as dataset augmentation or dataset selection.  Constructive Suggestions for Improvement: 1. Conduct experiments with realworld image data to evaluate the generalizability of CroCo. 2. Explore techniques to address the underperformance on highlevel semantic tasks such as incorporating additional datasets or training strategies. 3. Provide further insights into how CroCo handles variations in scene characteristics and lighting conditions and discuss potential strategies to improve robustness in diverse scenarios.", "ue4gP8ZKiWb": " Peer Review  Summary: The paper introduces a Prompt Certified Machine Unlearning algorithm PCMU which aims to efficiently handle the process of machine unlearning by simultaneously training and unlearning without the need for multiple iterations. The algorithm is based on randomized data smoothing and gradient quantization to ensure the model parameters and gradients remain unchanged against data removal requests. The paper provides a theoretical foundation and practical framework for PCMU showcasing its effectiveness in image classification tasks through empirical evaluation.  Strengths:  Innovative Approach: The PCMU algorithm proposes a novel method of simultaneous training and unlearning which can significantly improve efficiency over existing techniques.  Theoretical Foundation: The paper is wellgrounded in theoretical analysis establishing connections between randomized smoothing for robustness and unlearning providing a solid basis for the proposed algorithm.  Empirical Evaluation: The empirical evaluation on real datasets demonstrates the superior performance of PCMU against stateoftheart methods in image classification tasks.  Weaknesses:  Complexity: The papers content is highly technical and contains intricate theoretical derivations which may be challenging for readers without a strong background in machine learning and optimization.  Clarity: The text could benefit from clearer organization and more concise writing to enhance readability and accessibility to a broader audience.  Limited Comparison: While the empirical evaluation is commendable a more comprehensive comparison with a wider range of machine unlearning methods could provide a more robust assessment of PCMUs effectiveness.  Practical Implementation: Detailed discussions on the implementation aspects scalability and potential limitations in realworld scenarios could enhance the papers applicability.  Questions and Suggestions for Authors: 1. Algorithm Complexity: How does the computational complexity of the PCMU algorithm compare with existing unlearning methods particularly when dealing with largescale datasets 2. Generalizability: Can the proposed PCMU algorithm be extended to tasks beyond image classification and if so what aspects should be considered when applying it to different domains 3. Scalability: Have you considered the scalability of the algorithm to handle a large number of machine unlearning requests concurrently and what potential challenges might arise in such scenarios  Limitations: The authors could provide more insights into the practical implications and potential negative societal impacts of the PCMU algorithm. Offering suggestions for mitigating any adverse effects and ensuring responsible deployment would enhance the completeness of the paper.", "vRrFVHxFiXJ": " Peer Review of \"ChemCPA: Predicting Drug Perturbations with SingleCell RNAseq Data\"  Summary: The paper introduces ChemCPA a novel encoderdecoder model designed to predict drug perturbations using singlecell RNAseq data by leveraging information from bulk RNA HTS datasets. The model outperforms existing methods in predicting unobserved drugcovariate combinations and demonstrates high generalization capabilities. The authors present compelling evidence supporting the effectiveness of ChemCPA in drug discovery applications.  Strengths: 1. Innovative Approach: The integration of a perturbation network with a transfer learning scheme is a novel contribution that enhances the generalization capabilities of the model. 2. Comprehensive Evaluation: The paper provides a thorough evaluation of the models performance on both shared and unseen drugcovariate combinations showcasing its superiority over existing methods. 3. Clear Explanations: The detailed descriptions of the model architecture and training process aid in understanding the technical aspects of ChemCPA. 4. Practical Relevance: The work addresses a significant challenge in drug discovery and presents a practical solution that can accelerate the screening process and improve efficiency.  Weaknesses: 1. Limited Comparison: While the paper compares ChemCPA with existing methods a more extensive comparison with a broader range of stateoftheart models could provide a more comprehensive evaluation. 2. Societal Impact Discussion: The paper lacks a discussion on potential societal implications of accelerating drug discovery processes which should be considered in future research. 3. Data Limitations: The authors should address the limitations of datasets used such as biases or missing data and discuss potential implications on the models performance and generalization.  Questions and Suggestions for Authors: 1. Could the authors elaborate on the computational complexity of ChemCPA and discuss scalability issues for larger datasets or realtime applications 2. How robust is ChemCPA against noisy or incomplete datasets and how does it handle data imbalances or biases 3. Have the authors considered regulatory aspects related to drug screening and virtual predictions especially in terms of translating insilico findings to invivo experiments  Limitations: The authors should further address the potential negative societal impact of accelerating drug discovery processes ensuring proper ethical considerations and regulatory compliance in pharmaceutical research. Overall the paper presents a significant advancement in leveraging singlecell RNAseq data for drug perturbation predictions. Further discussions on societal implications robustness tests and model scalability would enhance the comprehensiveness and impact of the research.", "lXUp6skJ7r": " Summary The paper introduces an adversarial style augmentation (AdvStyle) approach for domain generalization in semantic segmentation. The aim is to improve the performance of a model trained on synthetic data when applied to realworld domains by dynamically generating hard stylized images during training. The proposed method is demonstrated to outperform existing augmentation techniques and achieve stateoftheart results on synthetictoreal domain generalization benchmarks.  Strengths 1. Novel Approach: The introduction of AdvStyle focusing on image style variation for domain generalization is a novel and promising contribution. 2. Thorough Experiments: The paper provides comprehensive experiments on synthetictoreal semantic segmentation benchmarks demonstrating the effectiveness of AdvStyle on different models and domains. 3. Model Agnostic: AdvStyle is modelagnostic and showcases consistent improvements across different backbone architectures. 4. Versatility: The versatility of AdvStyle is highlighted by its application to single domain generalized image classification where it also achieves stateoftheart performance.  Weaknesses 1. Limitation Discussion: The paper lacks a detailed discussion on the limitations of the proposed method and potential negative societal impacts. 2. Comparison Metrics: While the paper provides detailed comparisons with existing methods additional metrics or analyses could strengthen the comparisons further. 3. Technical Details: Some technical implementation details could be further elucidated for better understanding or reproducibility.  Questions and Suggestions 1. Clarification on Adversarial Learning: Could the authors elaborate on the rationale behind choosing the specific learning rate for the adversarial style learning in AdvStyle 2. Generalization to Other Tasks: Have the authors considered the potential applicability of AdvStyle to other computer vision tasks beyond semantic segmentation and image classification 3. Parameter Sensitivity: How sensitive is the performance of AdvStyle to different hyperparameters such as the architecture of the segmentation model or the choice of optimization techniques  Limitations As for the limitations section the authors should elaborate on potential negative societal impacts of the proposed AdvStyle method and provide recommendations or guidelines to mitigate any adverse consequences. Suggestions for addressing these aspects constructively could enhance the overall quality and societal relevance of the work.", "mUeMOdJ2IJp": " Peer Review 1) Summary The paper introduces a novel approach for recovering linear subspaces from data particularly focusing on Principal Component Analysis (PCA) in the presence of irregular noise. The work addresses challenges arising from heterogeneity in Federated Learning settings. By leveraging multiple data points from each user the proposed estimator is designed to be efficient and effective under various noise conditions. The upper bounds for estimation error and informationtheoretic error lower bounds are proven showcasing the estimators robustness to irregular noise. Furthermore the paper extends the analysis to a linear regression problem demonstrating the versatility of the proposed techniques. 2) Assessment of Strengths and Weaknesses Strengths:  The paper addresses an important and challenging problem in statistics and machine learning.  The contributions are welldefined with rigorous theoretical analysis providing error bounds.  The approach to leveraging multiple data points for subspace recovery is innovative and practical.  The extension to linear regression problem showcases the versatility of the proposed techniques.  Clear explanations and mathematical rigor enhance the understanding of the methodology. Weaknesses:  The paper may benefit from more empirical validation or realworld application examples to demonstrate the effectiveness of the proposed approach.  While the theoretical derivations are thorough more intuitive explanations could help improve accessibility for readers less familiar with the topic.  The limitations and potential challenges in the practical implementation of the proposed approach could be elaborated to provide a more comprehensive analysis. 3) Questions and Suggestions for Authors Questions:  How scalable is the proposed estimator in handling larger datasets with diverse noise profiles  Have you considered applying the methodology to realworld datasets to validate its performance outside the theoretical framework  What are the computational complexities involved in implementing the proposed approach especially in scenarios with a large number of users and data points Suggestions:  Provide more detailed examples or case studies to illustrate the practical utility of the proposed approach.  Consider discussing potential extensions or improvements to the methodology for addressing even more complex noise distributions or scenarios.  Clarify any assumptions made and their implications for the applicability of the proposed estimator in diverse settings. 4) Limitations The authors have not adequately addressed the potential negative societal impact of their work. To improve they should incorporate discussions on data privacy bias or fairness considerations especially in applications involving sensitive user information. Overall the paper presents a valuable contribution to the field and with some enhancements in empirical validation and societal impact considerations it could further solidify its significance and relevance.  This peer review aims to provide constructive feedback and suggestions for the authors to enhance the quality and impact of their work.", "qk1qpCN-k6": " Summary: The paper introduces a quantitative analysis of overfitting based on information theory specifically the information bottleneck method in the context of machine learning. The study focuses on the tradeoff between relevant and residual information in linear regression problems. It compares the information content of optimal algorithms and Gibbs regression analyzing their efficiency and information extraction behavior in various scenarios including isotropic and anisotropic settings.  Strengths: 1. Theoretical Framework: The paper presents a comprehensive theoretical framework based on the information bottleneck method providing a deep insight into the tradeoff between relevant and residual information in learning algorithms. 2. Novelty: The analysis of residual and relevant information beyond generalization error offers a fresh perspective on overfitting and model generalization contributing to the existing body of knowledge on learning algorithms. 3. Quantitative Analysis: The study quantifies the information efficiency of Gibbs regression in relation to optimal algorithms delineating how much more residual information is encoded by the Gibbs method which is valuable for assessing algorithm performance. 4. HighDimensional Analysis: By considering highdimensional settings and anisotropic covariates the paper explores important aspects of learning algorithms in complex scenarios providing insights that can be extrapolated to practical machine learning tasks.  Weaknesses: 1. Complexity: The paper may be challenging for readers not wellversed in information theory and machine learning concepts potentially limiting its accessibility to a broader audience. 2. Practical Applicability: While the theoretical analyses are robust the practical implications for realworld machine learning tasks are not explicitly discussed which could affect the relevance of the findings for practitioners.  Questions and Suggestions: 1. Clarification: Could the authors provide more intuitive examples or visual aids to help readers grasp the concepts and implications of the information bottleneck method especially for those less familiar with information theory 2. Application: How can the insights gained from this study be translated into practical applications in machine learning such as improving model performance or developing more efficient learning algorithms 3. Comparison: How does the information efficiency of Gibbs regression compare to other common machine learning algorithms and what are the implications for choosing between different algorithms in practice  Limitations: The authors could enhance the paper by discussing potential negative societal impacts of their work to ensure ethical considerations are adequately addressed. Suggestions for improvement include incorporating discussions on data privacy algorithm bias and societal implications of machine learning advancements.", "mq-8p5pUnEX": " Summary: The paper proposes a novel approach to improve the efficiency and generalization performance of neural networks in processing sequential data. The proposed method combines the advantages of Transformers and recurrent neural networks by dividing computation into two streams: a slow stream that compresses information into a Temporal Latent Bottleneck and a fast stream processed by a Transformer. The model shows improved sample efficiency and generalization performance across various tasks compared to competitive baselines as demonstrated in vision reinforcement learning and language tasks.  Strengths: 1. Innovative Architecture: The proposed division of computation into slow and fast streams leveraging different processing mechanisms is a novel and promising approach. 2. Improved Generalization: The model consistently outperforms competitive baselines across a range of tasks showcasing its effectiveness in enhancing sample efficiency and generalization capabilities. 3. Thorough Experimental Validation: Extensive experiments across multiple domains demonstrate the robustness and applicability of the proposed model.  Weaknesses: 1. Complexity: The model introduces additional complexity and may require finetuning of hyperparameters like the chunk size which could be challenging without clear guidelines. 2. Limited Exploration: The paper lacks a detailed comparison with a wider range of stateoftheart models beyond the selected competitive baselines. 3. Limited Explanations: Some sections particularly in the Methodology part are dense and need more clarity for better understanding.  Questions and Suggestions for Authors: 1. Performance Comparison: How does the proposed model compare with more recent advancements like Cross Transformers or Vision Transformer variants designed for diverse tasks 2. Interpretability: Can you provide more insights on how the model manages longrange dependencies and the tradeoffs between computational efficiency and information retention 3. Scalability: Have you explored the scalability limitations of the proposed model when dealing with larger datasets or realworld applications  Limitations:  Authors could provide more details on hyperparameter tuning and potential challenges in scaling the model for larger datasets or complex tasks. Consider discussing ways to automate chunk size selection or enhance generalizability without manual intervention. Overall the paper presents a compelling hybrid approach that effectively blends the strengths of recurrent neural networks and Transformers. However addressing complexity thorough comparative analysis and providing more interpretability will strengthen the papers impact and potential for broader applications.", "o4uFFg9_TpV": " Summary: The paper proposes a new approach called visual prompting inspired by prompting in NLP which adapts pretrained visual models to novel downstream tasks without taskspecific finetuning or model modification. By formulating tasks as simple image inpainting problems using masked autoencoders the authors demonstrate the effectiveness of visual prompting on a newly curated dataset from Arxiv for various imagetoimage tasks.  Strengths: 1. Innovative Approach: The concept of visual prompting is novel and offers a fresh perspective on adapting pretrained models for downstream tasks without traditional finetuning. 2. Effective Results: The paper provides solid evidence of the effectiveness of visual prompting on various computer vision tasks showcasing promising outcomes. 3. Thorough Experiments: The authors conduct comprehensive experiments comparing different models and datasets and provide both quantitative and qualitative results.  Weaknesses: 1. Data Bias: Limited experimental results on diverse datasets could raise concerns about the generalizability of the proposed method. 2. Complexity Limitations: The simplicity of tasks tackled compared to traditional supervised models raises questions about the scalability of visual prompting to more complex scenarios. 3. Lack of Comparison: While the proposed method demonstrates good results a direct comparison with stateoftheart finetuned models would enhance the papers credibility.  Questions and Suggestions: 1. Clarification on Training Data: How did the authors ensure the diversity and representativeness of the Figures dataset Was any data augmentation applied during model training 2. Model Robustness: How does the proposed method perform under noise or perturbations in input images Have robustness studies been conducted 3. Scalability: Are there plans to extend the study beyond imagetoimage tasks to more complex vision problems like object recognition or scene understanding  Limitations: The authors adequately address the limitations through transparency about the reduced competitiveness against supervised models and the dependency on the Figures dataset. To improve considering broader datasets and addressing ambiguities in the task definition would enhance the studys robustness.  Constructive Suggestions: To enhance the paper the authors could conduct more diverse experiments on challenging datasets explore the scalability of visual prompting and investigate methods to reduce dependency on taskspecific datasets for broader application. Overall the paper presents an intriguing approach with promising results but further exploration and validation on diverse and complex tasks are recommended for a stronger impact in the field.", "rYkGxHPnCIf": " Summary: The paper introduces a nonequilibrium importance sampling (NEIS) method for computing integrals with highdimensional distributions. The method involves generating samples from a base density propagating them along flowlines induced by a velocity field and calculating averages along these trajectories. The paper presents theoretical results showcasing the zerovariance estimator under specific conditions and illustrates the concept using deep learning to optimize the velocity field. Numerical experiments demonstrate significant variance reduction in estimators compared to traditional methods.  Strengths: 1. Theoretical Rigor: The paper provides a sound theoretical foundation for the NEIS method proving zerovariance estimators under specific conditions. 2. Methodological Innovation: Introducing deep learning for optimizing velocity fields is a novel and noteworthy approach that showcases a practical implementation of the NEIS method. 3. Thorough Experiments: The numerical experiments provide empirical evidence of the effectiveness of the proposed method showcasing substantial variance reduction compared to traditional approaches. 4. Clarity and Structure: The paper is wellstructured with a clear explanation of concepts methods and results making it accessible to readers.  Weaknesses: 1. Limited Dimensionality: The benchmark examples in the experiments are limited to dimensions up to 10. Extending the evaluation to higher dimensions would provide a more comprehensive assessment of the methods scalability. 2. Assisted Training Complexity: The assisted training method introduces additional complexity potentially making the implementation less straightforward. Clarifications on practical challenges and tradeoffs would enhance the papers applicability. 3. Impact of Parameters: Further discussion on the sensitivity of results to hyperparameters such as neural network architecture and training configurations would enhance the papers reproducibility and generalizability.  Questions  Suggestions for Authors: 1. Extension to Higher Dimensions: Have you considered scaling the method to higher dimensions beyond 10 to evaluate its performance and scalability accurately 2. Hyperparameter Sensitivity Analysis: Can you provide insights into the sensitivity of the methods performance to hyperparameters such as neural network structure learning rate or optimization algorithms 3. Practical Implementation Complexity: How does the complexity of implementing the assisted training method compare to direct training Are there specific considerations or challenges authors should be aware of in practical applications 4. Generalization to Realworld Applications: In which realworld applications or domains do you foresee the direct applicability and potential impact of the NEIS method beyond the presented numerical experiments  Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. Future research could incorporate discussions on ethical considerations related to algorithmic advancements and provide guidelines on responsible AI use.", "w0O3F4cTNfG": " Summary: The paper explores causal discovery in systems with measurement error focusing on linear systems where the mixing matrix is identified up to column permutations and scalings. A surprising connection is established between linear structural equation models with measurement error (SEMMEs) and those with unobserved root variables (SEMURs). The paper characterizes the identifiability of causal models under a twopart faithfulness assumption and proposes causal structure learning methods.  Strengths: 1. Novel Connection: The paper introduces a novel connection between linear SEMMEs and SEMURs providing insights into causal discovery under measurement error. 2. Identifiability Results: The paper provides detailed identifiability results under different faithfulness assumptions enhancing the understanding of causal structure learning. 3. Methodological Contribution: The proposed causal structure learning methods based on the identified mapping and faithfulness assumptions are a significant methodological contribution. 4. Performance Evaluation: Evaluation on synthetic data showcases the effectiveness of the proposed methods. 5. Clear Structure: The paper is wellorganized with clear sections discussing the models assumptions identifiability and evaluation.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers without a strong background in causal inference and linear structural equation models. 2. Assumption Clarity: Detailed explanations and examples clarifying the twopart faithfulness assumption and conditions for edge identifiability could enhance understanding. 3. Practical Relevance: While the theoretical advancements are significant discussing practical implications or applications of the findings could add context for a broader audience.  Questions and Suggestions for Authors: 1. Clarification on Models: Could the authors provide a clearer explanation or visual representation of how the mapping between the two models affects identifiability results 2. Practical Implementation: How feasible are the proposed methods for realworld applications with systems containing measurement errors 3. Comparison with Existing Methods: How does the proposed approach compare with existing algorithms for causal structure learning especially in scenarios with real data 4. Robustness: Have the authors considered the robustness of their methods to different levels of measurement error or model misspecifications 5. Scalability: How scalable are the proposed methods to larger systems with a higher number of variables and complex causal relationships  Limitations: The authors have adequately addressed the limitations of their work highlighting the need for accurate mixing matrix estimation for reliable structure recovery. To improve considering more practical applications and exploring robustness to various data scenarios could enhance the relevance of the work.", "nZRTRevUO-": " 1. Summary: The paper proposes LOLBO a novel approach that combines trust region optimization with Bayesian optimization over the latent spaces of deep autoencoder models. The method addresses the challenges of highdimensional latent spaces in structured optimization problems. LOLBO achieves significant improvements over stateoftheart methods in various realworld benchmarks by aligning local optimization in the latent space with local optimization in the original structured input space.  2. Assessment of Strengths and Weaknesses: Strengths:  Novel Approach: Introducing LOLBO which combines trust region optimization with Bayesian optimization in latent spaces is a significant innovation.  Empirical Results: The paper demonstrates significant improvements over existing methods in various benchmarks indicating the effectiveness of the proposed LOLBO approach.  Comprehensive Background: The paper provides a detailed background on Bayesian optimization deep autoencoder models and latent space optimization setting a solid foundation for its proposed method. Weaknesses:  Complexity: The method involves sophisticated joint training procedures and adaptation of trust regions which may increase computational complexity making the implementation challenging.  Evaluation Metrics: The evaluation primarily focuses on objective improvement additional metrics related to scalability convergence or robustness could provide a more comprehensive assessment of LOLBOs performance.  Interpretability: The paper lacks an indepth discussion on the interpretability of the results making it challenging to understand the underlying mechanisms of LOLBO.  3. Questions and Suggestions for the Authors: Questions: 1. How sensitive is LOLBO to hyperparameters and were sensitivity analyses conducted 2. Could the authors elaborate on the computational resources required for implementing LOLBO especially in largescale optimization tasks 3. How does LOLBO perform in comparison to nonBO optimization methods such as evolutionary algorithms or reinforcement learning in similar structured optimization tasks Suggestions: 1. Consider including visual aids such as diagrams or illustrations to enhance the reader\u2019s understanding of LOLBOs mechanisms. 2. Provide insights into potential applications beyond molecule design where LOLBO could be beneficial. 3. Discuss limitations in terms of scalability to even larger search spaces and suggest strategies to address this challenge.  4. Limitations: The authors should consider discussing the potential societal impact and ethical implications of using sophisticated optimization methods like LOLBO in domains like drug discovery. They could provide suggestions on ensuring transparency fairness and alignment with ethical guidelines when applying such advanced optimization techniques. Overall the paper presents a strong contribution with the LOLBO approach but further clarification on implementation details scalability and interpretability would enhance its impact and utility in realworld applications.", "omI5hgwgrsa": " PeerReview  1. Summary: The paper delves into decentralized stochastic variational inequalities over fixed and timevarying networks aiming to lower complexity bounds for communication and local iterations. It introduces optimal algorithms matching these bounds and outperforming existing literature across various scenarios. The research contributes significantly to the decentralized setting and provides experimental results to validate the proposed algorithms.  2. Strengths:  Novelty: The paper addresses an underexplored area of decentralized variational inequalities presenting lower complexity bounds and optimal algorithms which advance the field significantly.  Theoretical Rigor: Theorems and algorithms are wellstructured and supported by mathematical proofs enhancing the credibility and robustness of the research.  Comparative Analysis: Evaluation against existing methods and theories showcases the superiority of the proposed algorithms providing a comprehensive understanding of their effectiveness.  Experiments: Experimental validation confirms the efficacy of the algorithms enhancing the practical relevance of the research.  3. Weaknesses:  Clarity of Exposition: The paper could benefit from clearer explanations in some technical parts especially regarding the communication protocols and gossip matrices to aid understanding for readers less familiar with the topic.  Scalability Considerations: While experimental results look promising further investigation into the scalability of the algorithms with larger datasets or networks would strengthen the practical applicability of the proposed methods.  4. Questions and Suggestions for Authors:  Ensure clarity in defining and explaining concepts related to communication protocols over networks as these are crucial to understanding the algorithms performance.  Describe how the proposed algorithms would scale as the size of the dataset or network increases to evaluate their efficiency in handling larger computational tasks.  Consider providing more insights into the adaptability of the algorithms to various network topologies to determine their versatility in realworld applications.  5. Limitations: Authors could further discuss the societal impact of decentralized algorithms including potential implications for data privacy network security and computational resource distribution. Suggestions include conducting sensitivity analyses and exploring mechanisms to mitigate negative impacts. Overall the paper presents valuable contributions to the field of decentralized variational inequalities offering optimized algorithms and lower complexity bounds. Addressing clarity issues and scalability considerations would enhance the researchs impact and practical relevance.  End of PeerReview", "q0XxMcbaZH9": "1) Summary of the Paper and Contributions: The paper proposes a new training framework for boosting querybased instance segmentation models through discriminative query embedding learning. By exploring datasetlevel uniqueness and transformation equivariance the framework enhances the discriminative ability of querybased models on instance separation. The approach applied to four wellknown querybased models achieves significant performance gains on COCO and LVISv1 datasets. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important challenge in instance segmentation by focusing on crossscene level query embedding separation.  The proposed training algorithm significantly improves the performance of existing querybased models without requiring architectural changes.  The experimentation on multiple datasets and models demonstrates consistent performance gains across different backbones and instance sizes.  The methodology is methodical and wellstructured making it reproducible and easily implementable by other researchers. Weaknesses:  Some technical aspects of the proposed algorithm such as external memory utilization and sampling strategies could be further detailed for better understanding.  The paper could benefit from additional experimentation and analysis on the impact of the proposed framework across a wider range of instance segmentation scenarios to strengthen the generalizability of the approach. 3) Questions and Suggestions for Authors:  How do the authors plan to further validate the proposed training framework on more diverse datasets and model architectures to assess its general applicability  Can the authors elaborate on the computational implications of scaling the proposed method to larger datasets and different hardware setups  Are there plans to provide a comprehensive comparison of the proposed approach with other stateoftheart instance segmentation methods beyond the selected ones in the paper 4) Limitations: The authors could provide more explicit discussions on the potential negative societal impact of their work. Suggestions for improvement include integrating an analysis of societal implications within the discussion section or dedicating a subsection to address this aspect in future iterations of the paper. Overall the paper presents a compelling approach to enhancing querybased instance segmentation models with a clear methodology and notable performance gains. Further exploration and refinement of the proposed training framework could lead to significant advancements in instance segmentation research.", "pUPFRSxfACD": " Summary: The paper addresses the challenge of learning invariant features from heterogeneous data without explicit environment partition. It proves the theoretical impossibility of learning such features without additional information and inductive biases. The proposed framework ZIN jointly learns environment partition and invariant representation using auxiliary information. The paper provides sufficient and necessary conditions for identifying invariant features and experimental validation on synthetic and realworld datasets demonstrates the effectiveness of the proposed framework.  Strengths: 1. Theoretical Rigor: The paper provides a rigorous theoretical foundation proving the impossibility of learning invariant features without additional information and inductive biases while offering sufficient and necessary conditions for identifying invariant features. 2. Novel Contribution: The proposed framework ZIN offers a new approach to learning invariant models from heterogeneous data without explicit environment partition filling a gap in the current literature. 3. Experimental Validation: The experimental results on both synthetic and realworld datasets validate the effectiveness of ZIN showcasing improved performance over existing methods. 4. Clarity and Structure: The paper is wellstructured and clearly articulates the problem contributions and methodology making it easy for readers to understand the concepts.  Weaknesses: 1. Complexity: The theoretical aspects and conditions introduced may be challenging for readers unfamiliar with the field potentially limiting the accessibility of the paper to a broader audience. 2. Limited Empirical Validation: While the experimental results are promising the number of datasets used for validation is relatively limited. Including more diverse datasets could strengthen the generalizability of the findings.  Questions and Suggestions for Authors: 1. Clarification: Could the authors provide further clarification on how the proposed framework ZIN handles scenarios where auxiliary information is not readily available or cannot be effectively utilized 2. Comparative Analysis: How does the proposed ZIN framework compare to stateoftheart methods in terms of computational complexity and scalability 3. Robustness Analysis: Have the authors examined the robustness of the ZIN framework against noise or outliers in the data Conducting sensitivity analyses could further bolster the credibility of the proposed approach.  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work highlighting the need for explicit discussion on inductive biases when learning invariant models without environment partition. A suggestion for improvement would be to explore the impact of noise and outliers on the proposed framework for a more comprehensive evaluation.", "yI7i9yc3Upr": " Summary: The paper proposes a framework called NeurAllyDecomposed Oracle (NADO) for controlling autoregressive generation models. NADO decomposes a sequencelevel oracle function into tokenlevel guidance to steer the base model in text generation. The approach is efficient and does not require additional labeled data. The theoretical analysis and experiments on text generation and machine translation tasks demonstrate the effectiveness of NADO in guiding the base model towards specific control factors while maintaining high generation quality.  Strengths: 1. Innovative Framework: NADO introduces a novel approach to controlling autoregressive generation models leveraging decomposition of the oracle function for efficient text generation guidance. 2. Efficiency: The method does not require additional annotated data making it computationally efficient and costeffective. 3. Theoretical Analysis: The paper provides a thorough theoretical analysis of how the approximation quality of NADO impacts the controllable generation results. 4. Experimental Results: The experiments conducted on text generation with lexical constraints and machine translation with formality control showcase the effectiveness of NADO in maintaining highquality generation results while achieving control.  Weaknesses: 1. Complexity: The method involves several intricate components such as the training of the auxiliary neural model NADO and the theoretical derivations which might be challenging for readers to understand easily. 2. Comparative Analysis: While the paper compares NADO with existing methods a more detailed comparative analysis with a wider range of baselines could strengthen the evaluation. 3. Oracle Function: The necessity of a sequencelevel oracle for controlling the generation might limit the broader applicability of the framework in scenarios without explicit oracle functions.  Questions and Suggestions: 1. Oracle Function: How critical is the accuracy of the oracle function for the success of NADO Are there strategies to improve the accuracy of the oracle function in practical scenarios 2. Scalability: How scalable is NADO when applied to larger datasets or models Are there potential bottlenecks that may arise as the complexity of the models or datasets increases 3. Generalizability: How well does NADO generalize to tasks beyond text generation such as image generation or other nontextual data generation tasks  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. It could be beneficial for the authors to consider and discuss potential ethical implications of using NADO for controlling text generation. Providing guidelines to mitigate any potential negative impacts would enhance the papers completeness and societal responsibility.", "mmzkqUKNVm": "Peer Review Summary: The paper proposes an operatorlevel approach called Semantic Diffusion Network (SDN) to enhance semantic boundary awareness for deep semantic segmentation models. By formulating boundary feature enhancement as an anisotropic diffusion process and introducing SDN as a learnable solution the authors aim to improve boundary predictions by focusing on interclass boundary awareness. Extensive experiments on ADE20K and Cityscapes datasets show consistent improvements over baseline segmentation models. Strengths: 1. Innovative Approach: The paper introduces a novel operatorlevel solution to address the challenge of accurate boundary prediction in semantic segmentation models. 2. Efficiency: The proposed SDN offers an efficient and flexible module that can be easily integrated into existing encoderdecoder segmentation models. 3. Experimental Validation: Extensive experiments on challenging public benchmarks demonstrate the effectiveness of the approach showing consistent improvements over typical and stateoftheart segmentation models. Weaknesses: 1. Theoretical Grounding: While the paper presents an innovative approach a more detailed theoretical analysis of the proposed operatorlevel solution could enhance the understanding of the methodology. 2. Comparison with Existing Methods: The paper lacks a comprehensive comparison with existing boundaryaware segmentation methods and operatorlevel approaches limiting the context for the proposed SDN. 3. Visualization: While qualitative visualizations are mentioned additional visual comparisons with baseline models could further illustrate the improvement in boundary quality. Questions and Suggestions: 1. Clarification: Can the authors provide more insights into the adaptability of SDN to different network architectures and datasets 2. Scalability: How scalable is the proposed approach in terms of computational resources and model complexity as compared to existing methods 3. Generalizability: Have the authors investigated the generalizability of SDN to diverse semantic segmentation tasks beyond the datasets mentioned in the paper Limitations: The authors have not explicitly addressed the potential negative societal impacts of their work within the paper. It would be helpful to include a brief discussion on the ethical considerations related to the deployment of advanced segmentation techniques such as privacy concerns in image analysis tasks. Finally the paper offers a compelling contribution to the field of semantic segmentation with its innovative operatorlevel approach. Addressing the suggested improvements and providing additional context could further strengthen the papers impact and relevance in the research community. Note: The peer review is based on the information provided in the supplied paper draft.", "uzn0WLCfuC_": "Peer Review: 1) Summary: The paper introduces and explores the application of the Expected Improvement (EI) technique in the contextual bandit problem. Two novel EIbased algorithms are proposed: one for linear reward functions and the other for more general reward functions modeled by deep neural networks. The algorithms are analyzed both theoretically and empirically showing improved regret performance compared to existing approaches especially in high dimensions. 2) Strengths and Weaknesses: Strengths:  The paper provides a clear and comprehensive introduction to the contextual bandit problem and the significance of exploring the EI technique in this context.  The proposed algorithms are welldefined and supported by theoretical analyses providing insights into their performance guarantees.  Empirical experiments conducted on benchmark datasets demonstrate the efficacy of the proposed algorithms showcasing their superiority over existing approaches.  The paper bridges the gap between Bayesian optimization and contextual bandit problems by introducing the EI technique which could have significant implications for various applications. Weaknesses:  There is a lack of indepth discussion regarding the computational complexity of the proposed algorithms and any potential challenges in scaling them for largescale applications.  The explanation of the theoretical analyses could be further expanded to provide clearer insights into the key results and how they contribute to the overall understanding of the EIbased algorithms. 3) Questions and Suggestions:  How do the proposed EIbased algorithms perform in scenarios with varying degrees of noise in the reward function Have noisetolerant mechanisms been incorporated into the algorithms  Can the authors elaborate on the adaptability of the algorithms to dynamic contextual environments where the features of the arms change over time  It would be beneficial to provide insights into the generalizability of the proposed algorithms to realworld applications outside the specific benchmark datasets considered in the experiments.  Consider discussing the potential implications of the EI technique in reinforcement learning beyond the contextual bandit setting and avenues for future research in this direction. 4) Limitations and Suggestions for Improvement:  The authors have adequately addressed the limitations of their work such as the lack of discussion related to computational complexity and noise robustness. To enhance the paper it would be beneficial to elaborate on potential strategies to mitigate these limitations.  Constructive suggestions for improvement include conducting additional experiments on diverse datasets to further validate the proposed algorithms performance under various conditions. Overall the paper presents a valuable contribution to the field of contextual bandits by introducing and exploring the application of the EI technique. Expanding on the theoretical analyses and addressing potential limitations could enhance the impact and robustness of the proposed algorithms.", "upuYKQiyxa_": "PeerReview Summary: The paper proposes a method to enhance the robustness of visual classification models by directing the model to base its prediction on the foreground object rather than spurious background cues. This is achieved through a finetuning step using a small set of foreground masks leading to improved model performance across shifted domains. The method is applied to Vision Transformer (ViT) models and demonstrates significant improvements in robustness to distribution shifts. Strengths: 1. Innovative Approach: The paper introduces a novel method to improve the robustness of visual classification models by focusing on foreground object cues addressing a common issue in image classification. 2. Experimental Rigor: The study conducts extensive experiments across various datasets and models demonstrating the effectiveness of the proposed method in improving model performance under distribution shifts. 3. Clear Presentation: The paper provides a clear explanation of the proposed method the experimental setup and results making it easy for readers to understand the contributions and findings. 4. Code Availability: The authors provide a link to their code repository enhancing reproducibility and promoting transparency in research. Weaknesses: 1. Limited Comparison: The paper mainly compares the proposed method with baseline approaches that manipulate model saliency maps. A comparison with a broader range of stateoftheart methods for domain adaptation and robustness enhancement could provide more insights into the methods competitiveness. 2. Evaluation Metrics: While the paper reports improved performance on outofdistribution datasets a deeper analysis of the impact on specific error types interpretability of the model decisions or computational efficiency would enhance the completeness of the evaluation. 3. Theoretical Foundation: The paper could benefit from a deeper discussion on the theoretical underpinnings of the method linking it to relevant theories in explainable AI and domain adaptation. Questions and Suggestions: 1. Generalization: How does the proposed method generalize to different types of datasets beyond ImageNet Have you tested the method on more diverse and challenging datasets to assess its performance in different domains 2. Scalability: How does the method scale with the size and complexity of datasets and models Have you considered the computational efficiency and scalability of the proposed approach for largescale applications 3. Ethical Considerations: Have you considered potential biases introduced by directing the model to focus on specific object features How can the method address fairness and bias mitigation in visual classification tasks Limitations and Suggestions for Improvement: The authors could further address the potential negative societal impacts of their work by discussing ethical considerations such as fairness interpretability and potential biases introduced by the method. Providing guidelines on how to mitigate biases and ensure fairness in model predictions would strengthen the ethical foundation of the research.", "q_AeTuxv02D": " Summary The paper introduces the theoretical study on the ability of graph Message Passing Neural Networks (gMPNNs) for inductive outofdistribution (OOD) link prediction tasks with larger test graphs compared to training graphs. The study explores the convergence of link predictors based on structural node embeddings obtained by gMPNNs proposing a novel gMPNN for structural pairwise embeddings. Theoretical bounds and empirical validation on random graphs support the findings.  Strengths 1. Theoretical Framework: The paper provides a novel theoretical framework for OOD link prediction tasks filling a gap in the literature. 2. Novel Contribution: The proposal of a new family of structural pairwise embeddings gMPNNs\u2022\u2022 adds a unique approach to address OOD link prediction challenges. 3. Nonasymptotic Bounds: Demonstrating nonasymptotic bounds and convergence results enhance the theoretical understanding of OOD link prediction tasks. 4. Empirical Validation: Extensive empirical experiments on random graphs and ogblddi dataset validate the theoretical results adding credibility to the findings.  Weaknesses 1. Limited Realworld Applications: The study focuses largely on theoretical aspects and lacks practical realworld link prediction tasks for comparison with baselines. 2. Complexity: The theoretical formulations may be challenging for readers unfamiliar with graph neural networks and theoretical bounds.  Questions and Suggestions for Authors 1. Clarifications:  Could you elaborate on the practical implications of your findings for realworld applications of link prediction tasks  How do the proposed gMPNNs\u2022\u2022 models compare with stateoftheart methods in inductive link prediction tasks 2. Comparative Study:  Consider comparing the performance of gMPNNs with other types of neural networks in realworld scenarios to provide more depth to the empirical evaluation. 3. Clarity:  Enhance the clarity of the paper by providing intuitive examples or illustrations to aid in the understanding of the complex theoretical concepts.  Limitations The authors have provided a comprehensive analysis of the limitations and societal impact of the work with suggestions for future research expansion. However it would be beneficial to further emphasize the potential practical implications of the findings for endusers and decisionmakers in relevant domains.", "t4vTbQnhM8": " Summary: The paper introduces a novel method NonParametric Kernel Stein Discrepancies (NPKSD) to assess the quality of synthetic data generators especially implicit generative models. The method is designed to evaluate the goodness of fit of synthetic data to the original data distribution. Experimental results show improved power performance compared to existing approaches for synthetic and real datasets.  Strengths: 1. Novel Contribution: The paper introduces a new method NPKSD to assess generative models without requiring explicit density functions filling a gap in existing research. 2. Theoretical Guarantee: The paper provides theoretical guarantees for NPKSD enhancing its credibility and reliability in assessing synthetic data generators. 3. Experimental Validation: Extensive experiments on synthetic and real datasets demonstrate the effectiveness and improved power performance of NPKSD compared to existing approaches. 4. Detailed Methodology: The paper provides a comprehensive explanation of the NPKSD method including its theoretical underpinnings and algorithmic implementation.  Weaknesses: 1. Complexity: The paper delves into intricate mathematical concepts and methodologies potentially making it challenging for readers with limited expertise in statistical methods to grasp the intricacies of the proposed approach. 2. Experimental Interpretation: While the experimental results are promising more detailed explanation and discussion of the specific cases where NPKSD outperforms other methods would provide deeper insights into its advantages.  Questions and Suggestions: 1. Clarification on NPKSD Performance: Can the authors provide more insights into specific scenarios or datasets where NPKSD significantly outperformed existing methods 2. Scalability and Efficiency: How does the computational complexity of NPKSD compare to traditional goodnessoffit tests Are there any scalability issues when applied to large datasets 3. Validation on Diverse Datasets: Have the authors tested NPKSD on a wider range of datasets beyond MNIST and CIFAR10 to demonstrate its applicability across various domains 4. Comparison with Alternative Approaches: Could the authors provide a comparative analysis of NPKSD with other stateoftheart goodnessoffit testing methods for generative models particularly those that focus on implicit distributions  Limitations: While the paper introduces a valuable method for assessing synthetic data generators consideration should be given to the generalizability and interpretability of the NPKSD results across different datasets and applications. Additionally discussing the potential negative societal impact of inaccuracies in synthetic data assessment could further enhance the papers contribution.", "p3w4l4nf_Rr": " Summary: The paper investigates Nashregret minimization in congestion games a class of games with broad realworld applications. The authors propose a centralized algorithm based on optimism in the face of uncertainty and a decentralized algorithm using a novel combination of the FrankWolfe method and Goptimal design. They show that the algorithms have polynomial sample complexity in terms of the number of players and facilities.  Strengths: 1. Novel Algorithms: The paper introduces innovative centralized and decentralized algorithms for congestion games with (semi)bandit feedback addressing a gap in existing literature. 2. Theoretical Analysis: The sample complexity analysis reveals that the proposed algorithms achieve polynomial dependence on key parameters offering efficient solutions for congestion games. 3. Clear Presentation: The paper is wellstructured providing a thorough background problem formulation algorithmic details and theoretical guarantees.  Weaknesses: 1. Implementation and Empirical Validation: While the theoretical guarantees are solid empirical validation on benchmark datasets or simulations could strengthen the practical relevance of the proposed algorithms. 2. Clarity of Exposition: Some technical jargon and mathematical notations may be challenging for readers outside the specialization. Simplifying the language without compromising rigor could enhance accessibility.  Questions and Suggestions for Authors: 1. Scalability: How do the proposed algorithms scale with larger numbers of players and facilities Have you explored the scalability limits in terms of computational resources 2. Comparison: How do the proposed algorithms compare with stateoftheart methods in terms of convergence speed and performance in practical scenarios 3. Extensions: Are there plans to extend the algorithms to handle dynamic environments or other types of games beyond congestion games  Limitations: The authors have adequately addressed the limitations in terms of theoretical guarantees and computational complexity. To enhance the societal impact including discussions on realworld applications and practical implications could further enhance the study.", "wQVjGP5NbP9": " Review of \"Private Algorithm for Correlation Clustering\" 1. Summary: The paper introduces a novel differentially private algorithm for correlation clustering on unweighted graphs with provable privacy guarantees. The algorithm achieves a constant multiplicative approximation and an optimal O(n log\u00b2 n) additive approximation. It follows a refined approach compared to previous work emphasizing on differentially private synthetic graph release for privacy. 2. Strengths:  Novel Algorithm: Introduces a new differentially private algorithm for correlation clustering with strong approximation guarantees.  Theoretical Analysis: Provides a detailed theoretical analysis of privacy guarantees and approximation bounds.  Algorithm Design: Proposes an elegant approach that makes crucial steps of the clustering process differentially private. 3. Weaknesses:  Practical Implementation: Acknowledges limitations in practicality as the algorithm may face challenges in realworld scalability.  Additive Approximation: Admits a significant dependence on the threshold parameter T0 for outputting singletons which may limit applicability in diverse datasets.  Lack of Experimental Validation: Does not include experimental results to validate the performance of the algorithm in realworld scenarios. 4. Questions and Suggestions for Authors:  Scalability: How could the algorithm be optimized for improved scalability in handling large datasets  Improvements: Can the algorithms additive approximation bound be refined further reducing the dependence on the threshold parameter  Experimental Validation: Are there plans to conduct empirical evaluations to assess the performance of the algorithm in practical settings  Limitations: The authors have noted limitations in the practical scalability of the algorithm and its strong dependence on the threshold parameter for outputting singletons. To enhance the applicability of the algorithm the authors should focus on optimizing for scalability and improving the additive approximation without heavy reliance on specific parameters. I commend the authors for the innovative approach to differentially private correlation clustering and the comprehensive theoretical analysis. Addressing limitations and refining the algorithm for practical implementation would significantly enhance the impact and usability of the work.", "kImIIKGqDFA": "1) Summary: The paper introduces AGVM a novel algorithm designed to enable largebatch training of complex dense visual predictors addressing the challenge of heavy performance drops in tasks such as object detection and segmentation. AGVM aligns gradient variances between different network modules offering significant benefits in terms of training efficiency and generalization performance. Extensive experiments on COCO and ADE20K datasets demonstrate that AGVM outperforms existing methods enabling faster training without sacrificing performance. 2) Assessment of Strengths and Weaknesses: Strengths:  Novel Contribution: AGVM introduces a unique approach to tackle the challenges of largebatch training in complex visual prediction tasks.  Generalizability: AGVM is shown to work across various architectures optimizers and tasks highlighting its versatility.  Superior Performance: Extensive experiments demonstrate that AGVM achieves stateoftheart results with significantly reduced training times.  Theoretical Analysis: The paper provides a theoretical analysis of AGVMs convergence properties adding depth to the algorithm. Weaknesses:  Complexity: While effective the algorithms intricate structure might require additional effort for implementation and understanding by researchers.  Lack of Realworld Application: The paper focuses on performance metrics in controlled settings realworld applications and practical limitations could be further explored. 3) Questions and Suggestions:  Clarification on Implementation: More details on the practical implementation of AGVM including computational efficiency and scalability would enhance the paper.  Robustness Analysis: It would be valuable to include robustness tests under varying conditions to assess AGVM across different scenarios.  Transparency: Authors can elaborate on any potential challenges or limitations encountered during the algorithms development and experimental phases for a comprehensive understanding. 4) Limitations and Societal Impact:  The authors briefly addressed the limitations of module partitioning and the potential negative societal impact related to fraudulent applications. Suggestions include detailed investigations into such limitations and proactive measures to mitigate negative impact. Overall the paper presents a valuable contribution to the field of largebatch training for visual predictors through AGVM. Further exploration and refinement can enhance the algorithms usability and address potential challenges comprehensively.", "mwIPkVDeFg": "Summary: The paper addresses the question of the minimum communication overhead required to train overparameterized models in a decentralized setup. The authors introduce theoretical properties of decentralized optimization setting a focus on overparameterized models. They propose algorithms that achieve dimensionindependent nearoptimal communication complexity for specific models. Strengths: 1. Theoretical Contributions: The paper provides a comprehensive analysis of communication complexity lower bounds for overparameterized problems offering novel insights into decentralized optimization. 2. Algorithm Design: The proposed algorithms showcase innovative approaches utilizing linear compression and quantization contributing to reducing communication complexities efficiently. 3. Rigorous Analysis: The paper includes detailed proofs for the theoretical results ensuring the validity and robustness of the proposed algorithms. 4. Practical Relevance: The application of the algorithms to neural networks and numerical experiments on real datasets enhances the practical significance of the research. Weaknesses: 1. Complexity: The complexity of the algorithms and theoretical analysis might be challenging for readers without a strong background in decentralized optimization. 2. Limited Evaluation: While the paper introduces novel algorithmic approaches a more indepth evaluation on a variety of datasets and models could provide a broader perspective. 3. Societal Impact Discussion: A discussion on the societal implications or ethical considerations arising from decentralized optimization in machine learning could enhance the papers relevance. Questions and Suggestions: 1. Could the authors provide a more intuitive explanation of the proposed algorithms for readers less familiar with decentralized optimization concepts 2. How do the introduced algorithms compare with existing decentralized optimization techniques in terms of convergence speed and scalability 3. How sensitive are the proposed algorithms to hyperparameters and are there any considerations for tuning them effectively 4. What are the potential realworld applications of the dimensionindependent communication complexity achieved in your work Limitations and Suggestions: The paper lacks a detailed discussion on the limitations and potential societal implications of the research. To improve the authors should consider including a section that addresses any negative societal impacts ethical considerations or limitations specific to their work. Providing suggestions for mitigating potential drawbacks would enhance the papers completeness.", "v1bxRZJ9c8V": " Summary: The paper proposes an uncertaintyaware continuoustime dynamical model for interacting objects using latent Gaussian process ordinary differential equations. It decomposes the dynamics into independent and interaction components and shows improved reliability in longterm predictions over neural network alternatives. It utilizes variational sparse Gaussian process inference techniques and demonstrates the benefits of disentanglement of dynamics components.  Strengths: 1. Innovative Contribution: Development of an uncertaintyaware model for continuoustime dynamics of interacting objects is novel. 2. Structured Approach: The model decomposes independent and interaction dynamics effectively resulting in interpretable predictions. 3. Methodological Rigor: Utilization of Gaussian processes for calibrated uncertainty estimates and comparison against stateoftheart models. 4. Empirical Validation: The extensive experiments across various scenarios demonstrate the models superiority in handling missing information and improving prediction reliability.  Weaknesses: 1. Complexity: The models complexity might limit scalability to very large datasets due to the use of Gaussian processes. 2. BlackBox Interpretation: While effective the models blackbox nature may hinder full interpretability of the learned dynamics. 3. Limited Comparison Metrics: Additional metrics beyond MSE and expected loglikelihood could provide a deeper understanding of model performance. 4. Assumption Limitation: The paper could have addressed potential limitations or assumptions made in the experimental setup.  Questions and Suggestions: 1. Clarification on Scalability: How does the computational complexity scale with a large number of interacting objects Can the model handle realworld complex systems with thousands of entities 2. Interpretability: How can the model provide insights into the learned interactions and dynamics for better interpretability and understanding 3. Addressing Assumptions: Could the authors explicitly state and evaluate any key assumptions made in the model or experiments to enhance transparency and robustness in realworld applications 4. Potential Applications: Are there specific realworld scenarios or use cases where the proposed model could have a significant impact and have those been explored or considered in the study  Limitations: The authors could further elaborate on the implications of their work address scalability challenges in larger systems and provide more insights into interpretability and potential societal impacts for a more comprehensive evaluation.  Constructive Suggestions for Improvement: 1. Enhance Model Explainability: Consider additional techniques or methodologies to enhance interpretability and explainability of the models predictions. 2. Address Scalability Concerns: Explore strategies to mitigate scalability challenges when dealing with extremely large datasets or systems. 3. Investigate RealWorld Applications: Conduct experiments in specific realworld scenarios to showcase the effectiveness and practical utility of the proposed model. 4. Discuss Ethical Implications: Include a dedicated section discussing potential ethical implications and biases that may arise from implementing the model in realworld applications.", "lAN7mytwrIy": " 1) Summary: The paper introduces ElasticMVS a selfsupervised multiview stereopsis framework that leverages geometric proximity cues to improve dense surface predictions without requiring groundtruth 3D training data. The key contributions include a novel elastic part representation that encodes physically connected part segmentations with varying scales and shapes and a selfsupervised MVS framework for learning the representation and estimating perview depth. Extensive evaluations demonstrate superior performance in terms of reconstruction completeness accuracy efficiency and scalability compared to both supervised and selfsupervised approaches.  2) Strengths:  Novel Approach: Introducing a novel elastic part representation and leveraging geometric correlations to guide multiview correspondences is a significant and innovative contribution.  Superior Performance: The extensive evaluations show that ElasticMVS outperforms stateoftheart methods in both accuracy and completeness especially in challenging largescale reconstruction tasks.  Thorough Experiments: The paper includes comprehensive evaluations on DTU and Tanks and Temples datasets demonstrating the effectiveness and robustness of the proposed framework.  Memory Efficiency: ElasticMVS is shown to be more memoryefficient compared to other learningbased MVS methods which is a valuable practical consideration.  Weaknesses:  Computational Complexity: The high computational complexity for calculating patch similarities could be a potential limitation impacting the scalability and efficiency of the framework.  Long Training Time: The training time for calculating the patch similarity for each pixel is highlighted as a weakness suggesting a need for improvement in terms of efficiency.  3) Questions  Suggestions for Authors:  Computational Efficiency: Have the authors considered any strategies to reduce the computational complexity and training time of ElasticMVS Suggestions could include exploring lightweight network architectures or optimizing the patch similarity calculations.  Realtime Applications: How feasible is it to adapt ElasticMVS for realtime multiview stereopsis applications considering the computational demands highlighted in the paper  Scalability: How scalable is ElasticMVS to handle more complex scenarios or larger datasets with a substantial increase in the number of images and viewpoints  4) Limitations: The authors should consider addressing the computational complexity and training time issues to ensure the practical applicability of ElasticMVS in realtime or largescale multiview stereopsis scenarios. Suggested improvements could include exploring lightweight network architectures and optimizing calculations to enhance efficiency and scalability.", "tUH1Or4xblM": " Peer Review  Summary: The paper presents a novel ObjectCentric Layered Representation (OCLR) model for discovering and segmenting multiple moving objects in videos particularly handling occlusions without relying on manual annotations. The model is based on a transformer architecture using optical flow data showing stateoftheart performance on various video segmentation benchmarks.  Strengths:  Innovative Model Design: The OCLR models objectcentric layered representation is a novel approach that effectively handles multiple moving objects and mutual occlusions.  Scalable Synthetic Data Pipeline: The introduction of a pipeline for generating multiobject synthetic training data is a significant strength reducing the need for laborintensive annotations.  Thorough Evaluation: The paper includes comprehensive ablation studies and evaluations on multiple benchmarks demonstrating the models performance and Sim2Real generalization capabilities.  Weaknesses:  Dependency on Flow Information: The reliance solely on optical flow might limit the models performance in scenarios where objects are stationary for extended periods or amidst heavy deformations or interactions.  Global Depth Ordering: The approachs use of a global depth ordering may not handle complex scenarios where object orders alter or mutual occlusions vary dynamically.  Questions and Suggestions: 1. Data Simulation Pipeline: Could the authors provide more details on the synthetic data generation process particularly how they address complexities like heavy deformations and occlusions in simulated videos 2. TestTime Adaptation: How sensitive is the models performance to variations in the number of training frames or the duration of object interactions 3. Handling Stationary Objects: How does the model cope with objects that are stationary for extended periods and are there potential approaches to address this limitation  Limitations: The authors have not fully addressed the potential limitations of heavy object deformations and complex interactions in realworld scenarios. It is suggested to explore more sophisticated data simulation methods and consider incorporating RGB information into the training process to enhance the models robustness.  Conclusion: Overall the paper introduces a promising ObjectCentric Layered Representation model for multiobject segmentation in videos. By addressing the weaknesses highlighted and further exploring the suggested improvements the authors can enhance the robustness and applicability of their approach in challenging realworld scenarios.", "pHdiaqgh_nf": " Review of \"Decoding Complex Images from Brain Signals: A Multimodal Approach\"  Summary The paper addresses the challenge of reconstructing complex image stimuli from fMRI signals using a novel multimodal approach that incorporates brain signals observed images and textual descriptions. The authors leverage a pretrained visionlanguage latent space to overcome data scarcity limitations and develop a pipeline for generating photorealistic and faithful reconstructions of complex images.  Strengths 1. Innovative Approach: The paper introduces a unique approach by incorporating a text modality alongside brain signals and observed images for reconstructing complex images emphasizing semantic representations. 2. Addressing Data Scarcity: The use of a pretrained latent space and models to mitigate data scarcity challenges is a strong point of the study. 3. HighQuality Results: The reconstructions demonstrate a good balance between naturalness and fidelity producing photorealistic images that capture the ground truth content well. 4. Comprehensive Experiments: The paper conducts thorough experiments comparing different configurations and assessing the pipelines performance on various metrics.  Weaknesses 1. Model Complexity: The complexity of the pipeline involving multiple modalities and mappings may raise concerns about interpretability and scalability in realworld applications. 2. Evaluation Metrics: While the paper uses FID and image retrieval rates for evaluation additional perceptionbased metrics or user studies could provide more insights into the quality of reconstructions.  Questions and Suggestions 1. How does the proposed approach compare with existing methods in terms of computational efficiency and scalability for realtime applications 2. Have you considered evaluating the reconstructions with human subjects to assess the perceptual quality and subjective fidelity of the generated images 3. Could the paper provide more insights into the interpretability of the latent space and the semantic relationships captured by the reconstructed images 4. Further exploration of potential biases introduced by leveraging pretrained models and ways to mitigate them could be beneficial.  Limitations The authors could further elaborate on the potential societal impact of their work by discussing the implications of privacy concerns and the need for regulatory measures in the event of broader adoption of brain signal decoding technologies.  Overall Assessment The paper presents a novel and promising approach to decoding complex images from brain signals demonstrating highquality reconstructions with a multimodal framework. While the work showcases significant advancements addressing interpretability scalability and societal implications could further strengthen the studys impact.  Recommendation Considering the innovative contributions and the comprehensive experimental validation I recommend the paper for publication pending revisions addressing the points highlighted above.", "ob8tk9Q_2tN": " Peer Review  Summary: The paper introduces MinAM a novel variant of Anderson Mixing (AM) method to address the memory bottleneck in largescale optimization problems. MinAM is designed to store only one vector pair minimizing the memory requirement. The paper demonstrates the effectiveness of MinAM for both deterministic and stochastic optimization problems showing promising results in experimental evaluations related to logistic regression neural network training on CIFAR and ImageNet datasets.  Strengths: 1. Novelty and Contribution: The introduction of MinAM addresses a critical issue related to memory overhead in largescale optimization problems. The proposed method is wellmotivated and fills a gap in the existing literature. 2. Theoretical Development: The paper provides comprehensive theoretical analysis supporting the convergence properties of MinAM in both deterministic and stochastic optimization settings. The derivation of the restarted MinAM and the eigenvalue estimation technique are particularly notable. 3. Experimental Validation: The experimental results on logistic regression neural network training on CIFAR and ImageNet datasets demonstrate the effectiveness and competitiveness of MinAM compared to other optimization methods.  Weaknesses: 1. Theoretical Clarity: The paper involves complex mathematical derivations and theoretical analysis which might be challenging for readers without a strong background in optimization theory. It is recommended to enhance the clarity of mathematical concepts and assumptions to improve readability. 2. Technical Validation: While the experimental results are promising more detailed comparative analysis with additional stateoftheart optimization methods could provide a more comprehensive understanding of the performance of MinAM. 3. Practical Implementation: It would be beneficial to include practical implementation considerations or guidelines for applying MinAM in realworld optimization scenarios considering factors like computational efficiency and ease of implementation.  Questions and Suggestions for Authors: 1. Clarification: Could you provide intuitive explanations or examples to simplify the understanding of key mathematical concepts and derivations presented in the paper 2. Comparative Analysis: Have you considered comparing MinAM with a broader range of optimization methods including more recent techniques beyond those mentioned in the Related Work section 3. Utility in Realworld Applications: How easily can MinAM be implemented in practical optimization scenarios especially for researchers or practitioners with varying levels of expertise in optimization methods  Limitations: The authors have substantiated the theoretical analysis and demonstrated the efficacy of MinAM in experimental evaluations. However further efforts to enhance the clarity of mathematical concepts and considerations for practical implementation would ensure broader accessibility and applicability of MinAM in realworld optimization problems.", "xjXN3wEvCGG": " Review of Paper Titled \"Learning Temporal Task Specifications from Expert Demonstrations\" 1. Summary: The paper introduces Demonstration Informed Specification Search (DISS) to learn temporal task specifications from expert demonstrations efficiently. It addresses the challenges of learning from demonstrations and proposes algorithms that conjecture labeled examples to reduce surprise and sample tasks consistent with them. The concrete implementation focuses on Deterministic Finite Automata (DFAs) and demonstrates the efficacy of DISS in identifying tasks from limited expert demonstrations. 2. Strengths:  Innovative Approach: The concept of DISS alternating between conjecturing labeled examples and sampling tasks is a novel and logical way to learn temporal task specifications.  Concrete Implementation: Providing a detailed implementation of DISS for learning DFAs adds practical value to the paper.  Empirical Results: The experiments demonstrate the efficiency of DISS compared to traditional enumeration approaches showcasing the effectiveness of the proposed method. 3. Weaknesses:  Complexity of Content: The paper delves deep into technical details making it challenging for readers unfamiliar with the subject matter to grasp the concepts easily.  Lack of Comparative Analysis: While the empirical results show the superiority of DISS over baselines a more indepth comparison with broader stateoftheart methods would enhance the paper. 4. Questions and Suggestions for Authors:  It would be beneficial to provide more insights into how DISS compares with other approaches in terms of scalability and performance.  Could the authors elaborate on the potential application domains where DISS could have significant impacts apart from humanrobot interaction  How robust is the DISS framework to noise in demonstrations or variations in task complexities 5. Limitations:  The authors could elaborate more on the potential societal implications of their work especially with the increasing integration of AI systems in various domains. Its essential to address any negative impacts headon in the paper. Overall the paper demonstrates an innovative approach to learning temporal task specifications and provides significant contributions to the field. Further refinement in presentation clarity and broader comparisons with existing methods would strengthen the papers impact. Addressing the limitations mentioned above would enhance the papers completeness and relevance.", "mhQLcMjWw75": "Peer Review: Summary: The paper introduces a lightweight federated learning (FL) framework that leverages multiple fixed pretrained models as backbones to fuse representations and reduce computation and communication costs. The proposed Federated Prototypewise Contrastive Learning (FedPCL) algorithm utilizes class prototypes to facilitate knowledge sharing among clients. Extensive experiments validate the effectiveness of FedPCL in fusing representations from diverse pretrained models and its performance in various FL scenarios. Strengths: 1. Innovative Approach: The paper introduces a novel lightweight FL framework that integrates fixed pretrained models addressing computation and communication challenges. 2. Novel Algorithm: FedPCL utilizing class prototypes and contrastive learning demonstrates the ability to fuse representations effectively across clients enhancing personalization and knowledge integration. 3. Thorough Evaluation: Extensive experiments across benchmark datasets and comparison with stateoftheart FL algorithms showcase the superiority of FedPCL in terms of stability performance and scalability. 4. Comprehensive Analysis: The paper provides a detailed examination of integration with diverse backbones ablation studies on prototypes and contrastive loss and visualization of fused representations enhancing the understanding of the proposed framework. Weaknesses: 1. Lack of Theoretical Analysis: While the experimental results are promising the paper could benefit from a more indepth theoretical analysis of FedPCLs convergence properties and generalization bounds. 2. Generalizability: The evaluation primarily focuses on vision tasks limiting the generalizability of the proposed framework to other domains. Future work could explore applications in different domains like natural language processing. Questions and Suggestions: 1. Scalability: How does the proposed framework perform as the number of clients scales up in a realworld scenario Consider evaluating the scalability aspects further in future experiments. 2. Privacy Preservation: Have privacy concerns been adequately addressed in the lightweight framework Consider discussing potential approaches to enhance privacy preservation in federated learning. Limitations and Suggestions for Improvement: The authors have not explicitly addressed the potential negative societal impacts or ethical considerations of their work. To enhance the ethical aspect of their research the authors could include discussions on data privacy security and fairness considerations to ensure the responsible deployment of their proposed framework. Overall the paper presents a promising contribution to the field of federated learning addressing key challenges and proposing innovative solutions. Further theoretical analysis generalizability studies and ethical considerations will enhance the impact and applicability of the proposed framework.", "nzuuao_V-B_": " Peer Review  1) Summary: The paper systematically reevaluates stateoftheart input recovery attacks in federated learning focusing on batch reconstruction and model variations. It introduces the concept of angular Lipschitz constant as a measure to assess a models vulnerability to input recovery attacks and demonstrates its effectiveness through empirical evaluation.  2) Strengths:  Thorough reevaluation of existing attack methods on a variety of models in the context of batch reconstruction.  Introduction of the angular Lipschitz constant as a novel measure for assessing model vulnerability.  Empirical demonstration of the proposed measures effectiveness in predicting vulnerability under input recovery attacks.  A detailed explanation of model variations considered and their impact on attack methods.  3) Weaknesses:  Lack of comparison with other defense strategies or benchmarks to showcase the proposed measures superiority.  Theoretical explanation behind the proposed measure could be elaborated further.  The paper could benefit from a clearer delineation of the ethical implications of potential privacy threats and societal impact.  4) Questions and Suggestions for Authors:  Have you considered comparing the performance of the angular Lipschitz constant with other existing defense mechanisms or benchmarks  Can you provide an indepth discussion of the ethical implications and societal consequences of potential privacy breaches resulting from the evaluated input recovery attacks  How robust is the proposed measure under different scenarios or variations in federated learning settings  Please elaborate on the scalability and computational efficiency of the proposed angular Lipschitz constant for practical deployment in realworld applications.  5) Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work. To enhance the paper it is recommended to provide additional insights into the ethical considerations and societal implications along with further suggestions for improvement in addressing potential privacy concerns effectively. Overall the paper presents a valuable reevaluation of input recovery attacks in federated learning introducing a promising measure for assessing model vulnerabilities. Further discussions on the proposed measures robustness scalability and ethical implications would strengthen the papers impact and relevance in the field.", "xT5rDp5VqKO": " Peer Review Summary: The paper explores the use of coincidence detection a classic neuromorphic signal processing method and compares its performance with a stateoftheart deep learning method for pattern recognition. The study demonstrates that the coincidence detection method can be competitive with deep learning challenging the traditional belief that matrixvector operations suffice for efficient processing. The experimental results show promising outcomes indicating a potential for utilizing more advanced continual learning methods running on spiking neural network hardware for improved accuracy. Strengths: 1. Innovative Approach: The paper addresses an important and novel research question by revisiting classic methods on modern hardware providing valuable insights into the potential of neuromorphic signal processing. 2. Experimental Rigor: The comparison with a deep learning method establishes a solid basis for evaluating the effectiveness of coincidence detection backed by clear methodology and results. 3. Relevance: The papers discussion on the application of AI for infection detection adds practical relevance and highlights the societal impact of the research. Weaknesses: 1. Clarity of Methodology: While the paper describes the methodology well some technical aspects such as the handling of spectral data could be further detailed to enhance reproducibility. 2. Comparison Metrics: The comparison with the deep learning method could benefit from more detailed analyses including statistical significance tests and insights into why one method outperforms the other in specific scenarios. 3. Neuromorphic Theory Development: The paper briefly touches on the theory behind neuromorphic signal processing but could delve deeper to provide a more comprehensive understanding. Questions and Suggestions: 1. Exploring Complexity: Have you considered how the performance of coincidence detection would scale with increased complexity of datasets or when dealing with noisy data 2. Optimization Strategies: Could the authors explore different optimization techniques for coincidence detection to further improve its performance and efficiency 3. Interpretation of Results: How do the authors interpret the significant improvements in accuracy with coincidence detection and its implications for future research directions 4. Generalization Capability: To what extent can the results obtained be generalized beyond the specific dataset used in this study Limitations: The paper adequately addresses the limitations by acknowledging the approximation used for coincidence detection. To enhance clarity the authors could provide a more detailed discussion on the implications of this approximation and how it may impact the robustness of their findings. Suggestions for Improvement: Consider incorporating a thorough analysis of the potential negative societal impacts of the research to ensure ethical considerations are more explicitly addressed. Additionally providing more detailed discussions on the theoretical underpinnings of neuromorphic signal processing can enrich the papers contribution and help readers grasp the significance of the findings better.", "zbuq101sCNV": "Summary: The paper proposes a novel endtoend architecture named TANGO for textdriven photorealistic 3D style transfer. The core idea is to disentangle the appearance style as spatially varying bidirectional reflectance distribution function local geometric variation and lighting condition. TANGO leverages the CLIP model for supervision and predicts reflectance effects for any 3D mesh based on a text prompt. The paper showcases superior performance in terms of photorealistic quality consistency in 3D geometry and robustness with lowquality meshes. Strengths: 1. Innovative Approach: TANGO introduces a novel framework for photorealistic 3D style transfer with disentangled appearance components showcasing advancements over existing methods. 2. EndtoEnd System: The paper presents an endtoend neural architecture that automatically predicts material normal map and lighting condition for any bare mesh based on text descriptions without additional taskspecific dataset training. 3. Performance Evaluation: Extensive experiments demonstrate the advantages of TANGO in terms of photorealistic renderings lack of selfintersection in geometry details and robustness to lowquality meshes compared to existing methods. 4. Ablation Study: The ablation study indicates the effectiveness of individual components in TANGO validating the importance of each module for successful stylization results. Weaknesses: 1. Limitation in Shading Model: The paper acknowledges the simplified shading models limitation in representing indirect lighting effects and shadows which could restrict handling complex lighting scenarios in large scenes. 2. Vertex Displacement Comparison: While TANGOs normal displacement approach offers benefits in terms of realism and efficiency the paper acknowledges that vertex displacement frameworks have larger geometry capacity which could be considered a limitation. Questions and Suggestions: 1. Scalability Concerns: How does TANGO perform in terms of speed and computational resources required when dealing with larger scenes or datasets in realworld applications 2. User Interface Considerations: Have the authors explored any userfriendly interfaces or tools that could help artists or users interact with TANGO for better control over the style transfer process 3. Further Evaluation: Could the authors provide more insights into the interpretability of TANGOs predictions potential failure cases and strategies for handling them effectively Limitations and constructive suggestions for improvement: The authors should address the simplified shading models limitations and explore approaches to incorporate more sophisticated lighting effects for handling complex scenes. Additionally it would be beneficial to provide guidance on incorporating userfriendly features and optimizing TANGOs performance for scalability in realworld applications.", "lJx2vng-KiC": "Peer Review Summary: The paper provides exact solutions to the learning dynamics in deep linear networks with rich prior knowledge. The authors explore the impact of different initializations on learning trajectories alignment with task structure and implications for continual learning reversal learning and structured knowledge learning. The contributions include deriving explicit analytical expressions for network function dynamics internal representational similarity and neural tangent kernel alongside discussing the importance of prior knowledge in deep learning. Strengths: 1. The paper addresses an important gap in the theoretical understanding of deep learning dynamics with rich prior knowledge in linear networks. 2. The contributions are significant offering insights into network behavior alignment with task structure and implications for various learning paradigms. 3. The analytical solutions provided for learning dynamics are rigorous and comprehensive catering to a broad class of initial conditions and tasks. 4. The paper is wellstructured with clear sections and explicit references to related work for context and background. Weaknesses: 1. The paper heavily focuses on theoretical derivations and formalisms which might make it challenging for readers unfamiliar with deep linear networks or advanced mathematical concepts to grasp the content easily. 2. While the results are insightful the practical implications for realworld applications or scalability to largescale networks need further discussion. 3. The limitations of the assumptions made in deriving the solutions should be acknowledged and discussed to provide a more nuanced interpretation of the findings. 4. The paper could benefit from more concrete examples or case studies to illustrate how the theoretical contributions can be translated into practical settings. Questions and Suggestions for Authors: 1. Can you provide further insights into how the findings of your work can be translated into practical applications or guide improvements in pretraining schemes for deep neural networks 2. How do you envision further extending your results to more complex network architectures or nonlinear models to enhance the generalizability of the findings 3. Have you considered experimenting with realworld datasets or scenarios to validate the theoretical results and explore potential challenges or discrepancies between theory and practice 4. Could you elaborate on the computational complexity and scalability of the provided solutions especially when applied to larger and more intricate neural networks Limitations: The authors have adequately addressed the limitations by acknowledging assumptions made and potential scalability challenges. Constructive suggestions include incorporating more intuitive examples and extending the theoretical findings to more practical applications for a broader audience.", "zKBbP3R86oc": " Summary: The paper presents a novel method for training deep neural networks with quantized parameters and activations aiming to reduce energy consumption and inference time of models. The proposed method learns different precisions for weights and activations simultaneously leveraging a framework that approximates the optimization of discrete precisions. Experimental results on CNNs trained on CIFAR and ImageNet as well as on reducedprecision GANs show improved performance compared to stateoftheart quantization methods.  Strengths: 1. Innovative Approach: The method of learning heterogeneous precisions at a perparameter level is novel and adds value to the field of network quantization. 2. Modularity and Flexibility: The approach accommodates varying precisions for different weights providing flexibility and potential efficiency gains. 3. Systematic Development: The paper presents a wellstructured framework for optimizing mixedprecision architecture showcasing detailed derivations and experimental results. 4. Performance Improvements: Empirical results demonstrate superior performance in image classification tasks and generation tasks outperforming existing methods.  Weaknesses: 1. Complexity: The methods intricate nature might pose challenges for adoption and implementation in practical applications. 2. Hardware Dependency: While the paper discusses the potential energy savings on specialized hardware the generalizability of these advantages across diverse hardware platforms is unclear. 3. Comparative Analysis: The comparison with existing methods could be further extended to include a broader range of related approaches offering deeper insights into the method\u2019s efficacy.  Questions and Suggestions for Authors: 1. Clarity on Precision Allocation: Can the authors elaborate on how the learned precisions impact the models generalization performance across different applications and datasets 2. Scalability and RealWorld Applicability: How scalable is the proposed method for larger models or diverse network architectures commonly used in realworld applications 3. Robustness: Have the authors evaluated the robustness of the method against noise in the training process or the sensitivity to hyperparameters 4. Interpretability: Considering the sophisticated nature of the method can the authors provide insights into interpretability and explainability especially in the context of heterogeneous precision allocation  Limitations: The authors should address potential limitations related to the practicality and deployment of the proposed method as well as consider discussing the societal impact transparency and interpretability aspects of the approach for a more comprehensive evaluation and responsible development. Overall the paper presents a significant contribution to the field of network quantization offering a promising avenue for improving model efficiency and inference time with heterogeneous precision allocation. Further validations and explanations can enhance the applicability and understanding of the method.", "lQ--doSB2o": " Summary: The paper explores featurelevel adversarial attacks in computer vision for interpretability research. It introduces robust featurelevel attacks and demonstrates their versatility and robustness in generating various types of attacks at the ImageNet scale. The authors use these attacks as interpretability tools to identify bugs in networks and explore spurious associations between features and classes.  Strengths: 1. Conceptual Insight: The paper presents a novel concept of featurelevel adversarial attacks for studying deep network representations. 2. Robust Attacks: The authors introduce methods for generating versatile robust and interpretable featurelevel adversaries. 3. Interpretability: The study shows how these featurelevel attacks can aid in understanding network vulnerabilities and predicting misclassifications.  Weaknesses: 1. Complexity: The papers technical details and experimental setups may be challenging for readers unfamiliar with adversarial attacks and deep learning. 2. Lack of Comparison: While the authors compare their methods with prior works a more extensive comparison with a broader range of existing methods could enhance the papers contribution.  Questions and Suggestions for Authors: 1. Can you elaborate on how your approach compares to traditional pixellevel adversarial attacks in terms of misclassification rates and interpretability 2. Have you considered the computational efficiency of generating these featurelevel attacks for largescale models especially in realtime scenarios 3. Could you provide more insights into the generalizability and scalability of using featurelevel attacks for interpretability research in different domains beyond computer vision  Limitations: The authors have adequately highlighted the limitations of their approach in terms of multioptimization desiderata and efficiency concerns with generator quality. To address these limitations the authors could explore leveraging advancements in generative modeling to improve attack efficiency and quality. In conclusion the paper presents a significant contribution to the interpretability of deep neural networks through featurelevel adversarial attacks. By addressing the weaknesses and considering the questions and suggestions provided the authors can further strengthen their studys impact in the field of adversarial attacks and interpretability research.", "um2BxfgkT2_": " PeerReview:  Summary: The paper introduces Tokenized Graph Transformer (TokenGT) a pure Transformer architecture for graph learning. It treats each node and edge as independent tokens augmented with tokenwise embeddings and demonstrates promising results in both theory and practice. The authors prove its expressive power and compare its performance to GNN baselines and Transformer variants with graphspecific modifications highlighting competitive results.  Strengths: 1. Theoretical Foundation: The paper establishes a strong theoretical basis proving that TokenGT is at least as expressive as kIGN and kWL tests making it more powerful than messagepassing GNNs. 2. Innovative Approach: The approach of treating graphs as tokens with appropriate embeddings is novel and the concept of orthonormal node identifiers and trainable type identifiers is intriguing. 3. Empirical Results: The experimental results especially on the PCQM4Mv2 dataset showcase the effectiveness of TokenGT outperforming GNN baselines and competing with Transformer variants with graphspecific modifications. 4. Comparative Analysis: The comparison with other relevant works and the demonstration of the advantages of TokenGT over existing methods are informative.  Weaknesses: 1. Complexity vs. Performance Tradeoff: The discussion regarding O((nm)2) asymptotic cost and the need for efficient variations of Transformers could be elaborated more clearly. 2. RealWorld Applicability: While the paper demonstrates theoretical and empirical successes further exploration on practical applications and realworld impact could add depth. 3. Reproducibility: While the authors have provided the implementation code more details on hyperparameters and training specifics would enhance reproducibility.  Questions and Suggestions: 1. Scalability: How does performance scale with the size of graphs Have largescale graph structures been tested for performance and efficiency 2. Comparative Analysis: Could a more detailed comparison be provided with other graph learning methods such as GCNs GAT etc. to illustrate TokenGTs advantages more comprehensively 3. Interpretability: Considering the opaque nature of Transformers how can the interpretability of TokenGTs decisions be ensured especially in critical applications like healthcare or finance  Limitations: The paper could benefit from addressing the following limitations:  Scalability: The O((nm)2) cost needs optimization for larger graph structures.  Interpretability: A lack of discussion on ensuring the interpretability of TokenGT could be a limitation in critical applications. Overall the paper presents a strong argument for TokenGTs effectiveness in graph learning backed by solid theory and empirical validation. Addressing scalability interpretability and providing further realworld implications would enhance the papers impact and practicality.", "wTp4KgVIJ5": " Peer Review  Summary: The paper proposes a novel algorithmic framework called Stochastic Sampling Averaging Gradient Descent Ascent (SAGDA) for solving federated minmax learning problems with the aim of reducing communication complexity. The study addresses the challenges of high communication costs in federated minmax optimization by leveraging infrequent communications. The proposed method achieves significant improvements in communication complexity compared to existing stateoftheart methods. The theoretical analyses are supported by numerical experiments on various machine learning problems.  Strengths: 1. Novel Algorithm: The introduction of the SAGDA framework is a significant contribution to the field of federated minmax learning. 2. Theoretical Rigor: The paper presents a thorough theoretical analysis of the proposed algorithm providing clear insights into its convergence properties and communication complexity. 3. Experimental Validation: Numerical experiments conducted on real datasets provide empirical evidence supporting the theoretical claims enhancing the credibility of the proposed method. 4. Comparative Analysis: The comparison with existing baseline algorithms demonstrates the superior performance of SAGDA in terms of convergence speed and sample complexity. 5. Clear and Concise Presentation: The paper effectively organizes the theoretical framework algorithmic details and experimental results in a sequential manner for easy understanding.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers not deeply familiar with federated learning methods. Providing more intuitive explanations could improve comprehension. 2. Limited RealWorld Applications: While the proposed algorithm demonstrates effectiveness in the context of the study further exploration of its applicability to realworld scenarios would enhance the practical impact of the research. 3. Data Complexity Handling: The paper briefly mentions heterogeneity in datasets but could elaborate more on how the proposed method effectively addresses this challenge in realworld diverse data settings.  Questions and Suggestions for Authors: 1. Could the authors provide more detailed insights into how the SAGDA algorithm handles the data heterogeneity and partial client participation common in federated learning scenarios 2. How robust is the proposed algorithm to variations in the number of clients and local update steps Have sensitivity analyses been conducted 3. Can the authors clarify how the algorithmic framework of SAGDA balances model convergence with communication efficiency in practical largescale scenarios 4. Given the theoretical advancements in communication complexity what specific realworld applications or industry use cases could benefit most from the proposed SAGDA method  Limitations: The paper could further address the practical limitations of implementing the SAGDA algorithm in diverse realworld settings perhaps by discussing issues related to scalability computational efficiency and generalizability to different types of federated learning tasks. Overall the paper presents a significant advancement in federated minmax learning with impressive theoretical results and promising empirical validations. Addressing the highlighted weaknesses and questions could further improve the clarity impact and applicability of the proposed algorithm.", "kS5KG3mpSY": " Peer Review Summary: The paper introduces a novel method for learning an informative energybased model (EBM) prior in the latent space of a generator model using adaptive multistage density ratio estimation. The proposed approach breaks down the estimation task into multiple stages to bridge the gap between prior and posterior densities leading to a more expressive and efficient training process. The method is evaluated through experiments on image generation reconstruction and anomaly detection tasks. Strengths: 1. Innovative Approach: The paper introduces a novel method for learning a latent EBM prior in generator models offering a fresh perspective on prior modeling. 2. Thorough Experiments: The empirical evaluation showcases the effectiveness of the proposed method through comprehensive experiments on image generation reconstruction and anomaly detection. 3. Sequential Learning: The adaptive multistage density ratio estimation allows for sequential and adaptive learning which potentially leads to better priors. Weaknesses: 1. Complexity: The methods complexity due to multiple density ratio estimators could raise concerns about scalability and parameter efficiency. 2. Evaluation: While the experiments demonstrate strong performance more indepth analysis and comparisons with stateoftheart methods could enhance the robustness of the findings. 3. Clear Presentation: The clarity of presentation can be improved especially in explaining the technical details and rationale behind the proposed method. Questions and Suggestions: 1. Scalability: How does the method scale with the number of stages in terms of computation and memory requirements 2. Generalization: How does the proposed method generalize to datasets with different characteristics or tasks beyond image generation 3. Comparison: Can the authors provide more insights into the comparison with stateoftheart methods to establish the superiority of the proposed approach 4. Interpretability: How interpretable are the learned latent EBM priors and can they provide insights into the underlying data distribution Limitations: The authors should ensure that the parameter efficiency and scalability of the method are adequately addressed to avoid prohibitive resource requirements especially with an increasing number of stages. Careful consideration should also be given to parametrization to maintain efficiency. Overall the paper presents a promising methodology with strong empirical results. Addressing the areas of complexity scalability and clarity could strengthen the papers contribution to the research community.", "tbId-oAOZo": " Summary and Contributions The paper introduces a novel sparse endtoend multiperson pose regression framework QueryPose which directly predicts multiperson keypoint sequences from input images. This framework encodes each human instance with learnable spatialaware partlevel queries and enhances them through the Spatial Part Embedding Generation Module (SPEGM) and the Selective Iteration Module (SIM). By eliminating handcrafted postprocesses and incorporating bipartite matching QueryPose outperforms dense endtoend methods on MS COCO and CrowdPose datasets.  Strengths  Innovative Approach: Introducing sparse learnable partlevel queries is a unique and effective method.  Novel Modules: The SPEGM and SIM contribute significantly to enhancing local spatial features and refining queries.  StateoftheArt Results: QueryPose outperforms existing methods on both MS COCO and CrowdPose datasets.  Weaknesses  Complexity: The proposed framework involves multiple modules which might increase complexity and computational cost.  Limited Comparison: The comparative analysis lacks comparison with more recent methods limiting the comprehensive evaluation.  Theoretical Insights: The paper would benefit from more detailed theoretical insights to better explain the mechanisms behind the proposed modules.  Questions and Suggestions 1. How does the proposed framework handle occlusions in crowded scenes 2. Can the authors provide insights into the potential application of QueryPose in realtime or resourceconstrained environments 3. Have the authors considered incorporating domain adaptation techniques to improve performance across different datasets  Limitations The authors did not explicitly address any potential negative societal impacts or ethical considerations arising from their work. To enhance the ethical framework of the research the authors could include a section discussing the implications of deploying such technologies and the measures to mitigate any risks associated with privacy biases or misuse.", "q4IG88RJiMv": " PeerReview:  1) Summary: The paper introduces a novel concept of relating the arc length of the optimal ROC curve to f divergences. By establishing a connection between these two concepts the authors propose a variational objective for estimating the arc length accurately using samples. This leads to the development of a novel classification procedure that maximizes an approximate lower bound of the maximal AUC. Experimental results on CIFAR10 datasets demonstrate the effectiveness of the proposed twostep procedure in achieving good AUC performance in imbalanced binary classification tasks.  2) Strengths:  Innovative Concept: The paper introduces a novel approach by linking the arc length of the ROC curve to fdivergences offering a fresh perspective in the field of classification metrics.  Theoretical Soundness: The mathematical derivations and theoretical analyses of the proposed concepts are wellstructured and supported by rigorous mathematical formulations and proofs.  Experimental Validation: The paper provides experimental results on CIFAR10 datasets to demonstrate the practical applicability and effectiveness of the proposed methodology in realworld classification tasks.  Clarity and Presentation: The paper is wellorganized and effectively communicates complex mathematical concepts in a clear and concise manner.  3) Weaknesses:  Computational Complexity: The paper briefly touches upon the computational complexity of the proposed methods but a more detailed discussion or comparative analysis with existing methods in terms of computational efficiency could enhance the practical applicability of the proposed approach.  Evaluation Metrics: While the paper focuses on AUC performance a comparison with other traditional classification metrics or an analysis of the limitations of AUC in specific scenarios could add depth to the evaluation of the proposed methodology.  4) Questions and Suggestions for Authors:  Experimental Validation: Are there plans to extend the experimental validation to other datasets or realworld applications to further demonstrate the generalizability of the proposed approach  Robustness Analysis: Have you considered conducting sensitivity analysis or robustness checks to evaluate the stability of the proposed method under different data distributions or noise levels  Practical Deployment: How would you foresee the practical deployment of the proposed methodology in realworld scenarios considering factors such as interpretability computational requirements and scalability  5) Limitations: The authors have not provided a detailed discussion on how they plan to address the limitations and potential societal impact of their work. I recommend including a section addressing these aspects to ensure balanced consideration of the broader implications of the proposed methodology and encourage socially responsible research practices.  Overall Evaluation: The paper presents a novel and theoretically sound approach linking ROC curves to fdivergences offering promising results in classification tasks. Strengthening the discussion on computational complexity conducting additional experiments and addressing potential societal impacts would further enhance the impact and relevance of this work.", "v2es9YoukWO": " Summary: The paper introduces the Super Kernel Flow Network (SKFlow) a CNN architecture designed to enhance optical flow estimation by addressing occlusion challenges. SKFlow utilizes super kernels to capture larger receptive fields and complement local evidence in predicting occluded motions. Extensive experiments demonstrate SKFlows effectiveness achieving stateoftheart performance on benchmarks like the Sintel dataset.  Strengths: 1. Innovative Approach: Introducing super kernels for optical flow estimation is a novel and effective approach to handle occlusions. 2. Performance: SKFlow outperforms existing methods ranking 1st on the Sintel benchmark showing strong performance gains in occluded areas. 3. Efficient Design: By utilizing conical connections and hybrid depthwise convolutions SKFlow achieves compelling results with a modest increase in computation.  Weaknesses: 1. Limited Dataset Exploration: The paper could benefit from exploring potential limitations or performance variations on a wider array of datasets beyond Sintel and KITTI. 2. Complexity: SKFlows architecture may be complex for implementation by practitioners without detailed guidance or code examples.  Questions and Suggestions for Authors: 1. Can the authors provide insights on how SKFlow can be adapted to handle other challenging aspects of optical flow besides occlusions 2. How does SKFlow perform in realtime scenarios and are there any tradeoffs between accuracy and speed that need to be addressed 3. Could the authors provide more detailed comparisons with other stateoftheart methods especially in terms of model interpretability and generalization across different datasets  Limitations: The authors have adequately discussed limitations. One constructive suggestion could be to explore potential negative societal impacts that may arise from relying heavily on automated optical flow estimation systems such as biases in data or misuse of the technology.  Overall the paper presents a significant advancement in optical flow estimation by introducing the SKFlow architecture demonstrating superior performance in handling occlusions while maintaining a good balance between accuracy and computation cost. Further exploration into the generalization capability and potential realworld applications of SKFlow could enhance the impact of the research.", "vjKIKdXijK": " Summary: The paper presents an implementation of the Hessian approach for certifying convexity of differentiable functions comparing it with the disciplined convex programming (DCP) approach. The authors show how to compute computational graphs of the Hessians for a class of functions and check them for positive semidefiniteness. They demonstrate the effectiveness of the Hessian approach in certifying convexity for a broader range of differentiable functions than the DCP approach.  Strengths:  Novel Approach: Introducing the Hessian approach as an alternative to DCP for certifying convexity is innovative and adds value to the field of optimization and machine learning.  Technical Depth: The paper delves into detailed technical aspects such as matrix calculus and computational graph analysis showcasing a deep understanding of the subject matter.  Comparative Analysis: The comparison with the DCP approach provides a comprehensive evaluation of the proposed methods efficacy highlighting its advantages in certifying convexity for differentiable functions.  Weaknesses:  Complexity: The technical nature of the paper may make it challenging for readers without a strong background in optimization or convex analysis to grasp the content easily.  Limited Practical Examples: While the theoretical underpinnings are wellcovered the paper could benefit from more practical examples or use cases to demonstrate the realworld applications of the proposed approach.  Clarity: Some sections could be further simplified or clarified to enhance readability and accessibility for a broader audience.  Questions and Suggestions:  Clarification: Can the authors provide more intuitive examples or illustrations to help readers better understand the significance and practical implications of the Hessian approach  Scalability: How does the proposed Hessian approach scale in terms of computational complexity when applied to larger and more complex optimization problems  Extension: Are there plans to extend the application of the Hessian approach to scenarios involving nondifferentiable functions or nonstandard optimization problems  Limitations: The paper could benefit from a clearer discussion of potential negative societal impacts or ethical considerations arising from the application of the proposed Hessian approach. Addressing such limitations and considering ethical implications can enhance the overall contribution and relevance of the research.  Suggestions for Improvement: To enhance the papers impact and readability the authors could consider incorporating more practical examples providing additional context for nonexpert readers and discussing the broader implications of their work beyond the technical aspects. Additionally addressing the limitations and societal impacts of the research more explicitly would contribute to a more comprehensive evaluation of the proposed approach.", "pIYYJflkhZ": "Summary: The paper introduces SoftPatch a memorybased unsupervised anomaly detection method to address labellevel noise in image sensory anomaly detection. The method efficiently denoises data at the patch level using noise discriminators and memory banks softening the anomaly detection boundary. Extensive experiments demonstrate that SoftPatch outperforms existing methods on benchmark datasets and is robust to noisy data providing a new perspective for further research in unsupervised anomaly detection with noisy data. Strengths: 1. Novel Contribution: Addressing labellevel noise in image anomaly detection is a significant and original contribution. 2. Robust Methodology: SoftPatch efficiently denoises data at the patch level enhancing model robustness and performance in noisy environments. 3. Extensive Experiments: The paper provides comprehensive experimental results demonstrating the effectiveness of SoftPatch on benchmark datasets. Weaknesses: 1. Parameter Sensitivity: The paper lacks detailed analysis on the sensitivity of key parameters such as LOFk and threshold \u03c4 which could impact the methods performance. 2. Noisy Data Evaluation: While the method is designed to combat noisy data an indepth analysis of the impact of different noise levels on SoftPatchs performance could provide additional insight. 3. Comparison with Baselines: Limited comparison with more recent or diverse anomaly detection methods beyond the ones mentioned could further strengthen the evaluation. Questions and Suggestions: 1. InDepth Parameter Analysis: Could the authors provide insights into how variations in LOFk and threshold \u03c4 impact the methods performance across different noise scenarios 2. Noisy Data Impact: How does the presence of noise impact anomaly detection accuracy in different scenarios and are there specific noise levels where SoftPatch excels or struggles 3. Extended Comparison: Consider comparing SoftPatch with a broader range of anomaly detection methods to showcase its superiority more convincingly. Limitations: The paper does not explicitly discuss the potential negative societal impact of the proposed method or address the ethical considerations of unsupervised anomaly detection. Authors could include a section discussing the ethical implications of using anomaly detection in realworld applications to enhance the papers societal impact.", "zuL5OYIBgcV": " Peer Review: 1. Summary: The paper explores the concept of building highperforming \"nondeep\" neural networks through the introduction of parallel subnetworks in contrast to the typical deep architectures. The authors present ParNet an architecture showing that networks with a depth as low as 12 can achieve outstanding performance on various benchmark datasets such as ImageNet CIFAR10 CIFAR100 and MSCOCO. They discuss architectural choices scaling rules and performance comparisons with existing networks like ResNets and RepVGG. Experimental results demonstrate the effectiveness of ParNet in achieving competitive accuracy with reduced depth. 2. Strengths:  Innovative Approach: The concept of utilizing parallel substructures to decrease depth while maintaining high accuracy is a novel and intriguing idea.  Empirical Proof: The empirical validation through experiments on diverse datasets like ImageNet CIFAR and MSCOCO strengthens the credibility of the proposed architecture.  Comparative Analysis: The comparison with existing networks and ablation studies showcase the superior performance of ParNet in terms of both accuracy and speed.  Detailed Technical Description: The paper provides a thorough description of the proposed architecture training methodology scaling approach and results which enhances the clarity and understanding of the work. 3. Weaknesses:  Lack of Indepth Analysis: While the paper extensively describes the architecture and experimental results a deeper analysis of the limitations of the proposed approach or failure cases could add more value to the work.  Validation on Additional Datasets: Extending the evaluation to more datasets or realworld applications could strengthen the generalizability and applicability of ParNet beyond the discussed benchmark datasets. 4. Questions and Suggestions for Authors:  Future Extensions: Have the authors considered exploring the application of ParNet in specific realtime scenarios where low latency is critical such as autonomous driving or robotics  Robustness against Adversarial Attacks: How does ParNet fare against adversarial attacks and have any robustness experiments been conducted to analyze its vulnerability  Scalability: Could the authors discuss potential challenges or insights into scaling ParNet for even larger models or applications beyond the current scope 5. Limitations:  Societal Impact: The paper could further address the potential societal impact of utilizing ParNet in safetycritical systems or decisionmaking processes ensuring ethical considerations and avoiding biases. In conclusion the paper presents a promising approach in the domain of neural network architecture design with ParNet. Addressing the identified areas could enhance the comprehensiveness and applicability of the proposed framework. Overall the paper demonstrates significant potential for impacting the field of deep learning architecture design.", "yP0vpghGoLF": " Summary and Contributions The paper presents a novel analysis of the Past Extragradient (PEG) method for monotone variational inequalities. It extends the lastiterate convergence results of PEG to both constrained and unconstrained cases without relying on the Lipschitzness of the Jacobian. The contribution includes the derivation of new potential functions and the establishment of O(1N) convergence rates for the squared norm of the operator or the squared residual in both cases.  Strengths 1. Novel Analysis: The paper introduces a new analysis through potential functions to achieve convergence results for PEG without assumptions on the Jacobian. 2. Methodological Contribution: The use of performance estimation problems (PEPs) for obtaining convergence results is a methodological highlight. 3. Rigorous Approach: The paper provides detailed theoretical proofs and numerical validation showcasing a comprehensive analysis. 4. Extension to Constrained Case: Successfully extending the lastiterate convergence results to the constrained case is a significant advancement.  Weaknesses 1. Complexity: The papers content is highly technical making it challenging for nonexperts to grasp the methodology and results easily. 2. Multiplicative Constants: The derived upper bounds have multiplicative constants that are noticeably worse than those found numerically indicating room for improving the theoretical guarantees. 3. Practical Applicability: The focus on theoretical analysis may limit the discussion on practical implications and realworld applications.  Questions and Suggestions for Authors 1. Can you provide insights into the practical implications of your theoretical findings for realworld optimization problems 2. How do your results compare with stateoftheart methods in terms of convergence rates and applicability to various optimization scenarios 3. Have you considered conducting experiments to validate the theoretical findings on real data sets or optimization problems 4. Are there any assumptions or simplifications made in your analysis that could be further relaxed or refined for a more comprehensive study  Limitations and Suggestions The paper could further enhance its practical relevance by discussing the applicability of the derived theoretical results to realworld optimization problems. Additionally addressing the discrepancy between the estimated upper bounds and the numerical findings could strengthen the theoretical guarantees provided. Furthermore exploring the implications of the results in practical optimization settings would amplify the impact of the study.", "nX-gReQ0OT": " Peer Review 1. Summary: The paper proposes a novel deeplearning architecture combined with Monte Carlo methods for accurately solving the Schr\u00f6dinger equation in computational chemistry. The new approach achieves significantly lower energy errors at a reduced computational cost compared to existing methods. By combining a PauliNetlike embedding with FermiNet envelopes and introducing various architectural improvements the authors establish a new benchmark for variational ground state energies. The study showcases substantial improvements in accuracy and computational efficiency across different molecules and atoms. 2. Strengths:  Innovative Approach: The introduction of a novel deeplearning architecture that significantly reduces energy errors and computational cost is a substantial contribution to the field.  Performance Benchmark: Establishing a new benchmark for variational ground state energies with improved accuracy and efficiency sets a new standard in the domain.  Experimental Rigor: The systematic breakdown of improvements and the evaluation of the impact of increasing physical prior knowledge provide comprehensive insights into the methodology.  Comparative Analysis: Comparison with existing methods both deeplearningbased and classical demonstrates the superior performance of the proposed approach. 3. Weaknesses:  Complexity: The paper delves deep into technical aspects potentially making it challenging for nonexperts to grasp the intricacies of the methodology.  Validation: The paper could benefit from more detailed discussions on the robustness and generalizability of the proposed architecture across diverse molecular systems. 4. Questions and Suggestions for Authors:  How does the proposed architecture perform when applied to more complex molecular systems beyond the benchmark atoms and molecules mentioned in the study  Could the authors elaborate on potential challenges or limitations of scaling the method to larger molecules or systems  Considering the speed and computational resources required are there plans to optimize the method further for practical use in computational chemistry applications 5. Limitations: The authors could further address the potential negative societal impact of the research findings especially in terms of accessibility and implications for researchers with limited resources. Suggestions for improvement could include the development of userfriendly tools or cloudbased solutions to democratize access to the advanced computational methods. Overall the paper presents a significant advancement in computational chemistry through the development of a novel deeplearning architecture. Addressing certain aspects highlighted for improvement could enhance the impact and accessibility of the research findings in the scientific community.", "p9zeOtKQXKs": " Summary: The paper introduces a unified framework for studying generic metagradient estimations in Gradientbased MetaReinforcement Learning (GMRL). It identifies and analyzes biases in existing stochastic metagradient estimators adopted by many recent GMRL methods including compositional bias and multistep Hessian bias. The authors conduct empirical evaluations on tabular MDP Iterated Prisoner Dilemma using the LOLA algorithm and Atari games using the MGRL algorithm to validate their theoretical findings.  Strengths: 1. Theoretical Contributions: The paper offers important theoretical insights into biases in metagradient estimations in GMRL algorithms specifically compositional bias and multistep Hessian bias. 2. Empirical Validation: The experiments conducted on various environments provide quantitative evidence supporting the theoretical findings and demonstrate the effectiveness of proposed bias correction methods. 3. Comprehensive Analysis: The paper covers a broad range of GMRL settings and conducts thorough ablation studies to validate the proposed theoretical insights.  Weaknesses: 1. Complexity: The paper delves into detailed theoretical analysis and experiments making it challenging for readers unfamiliar with the topic to grasp the content easily. Simplifying explanations or providing more intuitive examples could enhance understanding.  Questions and Suggestions for Authors: 1. Clarification on Experiments: Could the authors elaborate on the reason behind choosing specific environments for the experiments and how they relate to the theoretical insights presented 2. Impact of Bias Correction Methods: How do the proposed bias correction methods impact the convergence rate and stability of GMRL algorithms in practice 3. Extension of Bias Analysis: Is there potential for extending the bias analysis to more complex GMRL scenarios or different types of metaRL algorithms to provide a broader understanding of biases in metagradient estimation  Limitations: The authors could enhance the paper by discussing the potential negative societal impact of biased metagradient estimations and suggest measures to address these issues in future research.", "mCzMqeWSFJ": " Review of the Paper \"Complete and Efficient Graph Neural Networks for 3D Molecular Graph Learning\" 1. Summary and Contributions The paper proposes ComENet a novel method for 3D molecular graph learning that combines quantuminspired basis functions with a message passing scheme to achieve both completeness and efficiency in incorporating 3D information. ComENet guarantees complete 3D information representation and achieves scalability through its time complexity analysis. Experimental results on realworld datasets demonstrate the effectiveness and efficiency of ComENet compared to existing methods. 2. Strengths and Weaknesses Strengths:  The paper addresses a significant challenge in 3D graph representation learning and proposes a novel and efficient solution.  The method guarantees completeness in incorporating 3D information without significant loss.  ComENet demonstrates superior performance and efficiency compared to existing stateoftheart methods.  The experimental evaluations on largescale datasets validate the effectiveness of ComENet.  The thorough analysis of time complexity and proof of completeness enhance the credibility of the proposed method. Weaknesses:  The paper lacks a comprehensive comparison with a wider range of existing methods in the field potentially limiting the insights gained from the evaluation.  More detailed discussions on the limitations of ComENet and potential areas for future research could have further enriched the paper.  The paper could benefit from more clarity and succinctness in presenting technical details to enhance readability and accessibility for a broader audience. 3. Questions and Suggestions for Authors  Could the authors elaborate on the generalizability of ComENet to other domains beyond 3D molecular graph learning  How does the efficiency of ComENet scale with increasing graph sizes and complexities in practical applications  What are the implications of introducing rotation angles for identifying conformers and are there potential applications beyond molecule identification  How does ComENet perform on datasets with noisy or incomplete 3D information and what strategies could be employed to address such challenges 4. Limitations The authors could further address the potential negative societal impact of ComENet by discussing considerations related to bias ethics and data privacy. Additionally providing guidance on interpreting and mitigating any biases inherent in the methods design and application could enhance the ethical implications of using ComENet in realworld scenarios. Overall the paper presents a solid contribution to the field of 3D molecular graph learning with the introduction of ComENet. By addressing completeness and efficiency challenges ComENet shows promise for practical applications in various research domains. Strengthening the completeness of the analysis and broadening the discussion on implications and future directions could further enhance the impact and relevance of the paper.", "xTYL1J6Xt-z": "Summary: The paper introduces FasterRisk a novel approach for efficiently producing highquality risk scores learned from data. The method utilizes a beamsearch algorithm to generate a pool of almostoptimal sparse continuous solutions which are then transformed into separate risk scores through a unique \"star ray\" search technique using multipliers. The algorithm returns a collection of highquality risk scores for user consideration. The experiments demonstrate the computational speed and performance superiority of FasterRisk over existing baselines like RiskSLIM. Strengths: 1. Innovative Approach: The paper introduces a novel methodology for generating risk scores efficiently and effectively. 2. Comprehensive Experiments: The paper conducts extensive experiments to evaluate the performance of FasterRisk against baselines showcasing superior results. 3. Interpretability: The methods emphasis on interpretability and domain knowledge makes it a valuable tool for highstakes decisionmaking. Weaknesses: 1. Complexity: The technical details of the algorithm might be challenging for readers without a strong background in machine learning and optimization. 2. Limited Comparison: While FasterRisk outperforms existing methods in experiments a broader comparison with a wider range of algorithms could enhance the papers comprehensiveness. 3. Implementation Details: The paper includes detailed methodology but lacks practical implementation guidance which might limit adoption by practitioners. Questions and Suggestions for Authors: 1. Can you provide more insights into how FasterRisk can adapt to varying dataset characteristics such as different dimensions or feature correlations 2. Have you considered potential tradeoffs between computational efficiency and model accuracy when deploying FasterRisk in realworld scenarios How does the method handle noisy or imbalanced datasets 3. What considerations have been made regarding the generalizability of FasterRisk across different domains beyond healthcare finance and criminal justice Limitations and Suggestions: The authors could enhance the paper by addressing the potential negative societal impacts of deploying automated risk scores particularly in criminal justice contexts. Moreover providing guidelines on ethical considerations and bias mitigation strategies in the risk score generation process would offer a more comprehensive approach. Overall the paper presents an innovative methodology that could significantly impact various industries. Addressing the highlighted suggestions and limitations would further strengthen the papers contribution and practical relevance.", "peZSbfNnBp4": " Peer Review:  Summary: The paper investigates domain generalization in deep learning models and proposes a model averaging protocol to improve performance stability and reliability in selecting models. The protocol involves maintaining a moving average of model parameters during training to enhance outdomain test accuracy. The study shows that ensembling moving average models further boosts performance outperforming traditional ensembling methods. The paper provides a theoretical explanation for the observed improvements and benchmarks the proposed approach on various datasets with different pretrained models.  Strengths: 1. Novel Contribution: The paper offers a novel approach to addressing domain generalization issues through model averaging enhancing both performance stability and reliability in model selection. 2. Theoretical Explanation: The adaptation of the BiasVariance tradeoff to explain the effectiveness of model averaging and ensembling in improving outdomain performance is noteworthy and adds theoretical depth to the study. 3. Empirical Results: The experimental results especially the benchmarking on different datasets and pretrained models provide robust evidence for the effectiveness of the proposed ensemble of moving average models. The performance gains are substantial and consistent across various scenarios.  Weaknesses: 1. Clarity and Organization: The paper is detailed which is essential for a research study but it can be challenging for readers to navigate and extract key points quickly. Consider improving clarity and organization by refining the structure and enhancing readability. 2. Comparison with Existing Work: While the paper compares results with existing methods a more detailed comparative analysis and discussion with other stateoftheart approaches could enhance the context and impact of the proposed technique. 3. Visualization: While the paper includes some qualitative visualizations more visual aids could aid in understanding complex concepts especially related to model averaging and ensembling protocols.  Questions and Suggestions for Authors: 1. Experiment Reproducibility: Have you considered providing detailed guidelines in the paper to facilitate the reproducibility of your experiments for the research community 2. Scalability Consideration: How scalable is the proposed ensemble of averages approach to larger and more complex datasets or models beyond the ones tested in this study 3. Sensitivity Analysis: Did you perform sensitivity analysis to evaluate the impact of different parameters or initialization conditions on the proposed model averaging protocols performance  Limitations: The authors have provided a comprehensive discussion of the limitations and societal impacts of their work. One suggestion for improvement could be to consider explicitly addressing potential biases or ethical considerations that may arise from the implementation of the proposed methodology to ensure responsible deployment in realworld applications. Overall the paper presents a valuable contribution to the domain generalization problem in deep learning and offers an effective approach that significantly improves model performance. Enhancing clarity providing more detailed comparisons and addressing scalability are areas that could further strengthen the paper.", "maSvlkPHc-k": " Paper Summary: The paper focuses on the issue of discrimination risk in graph feature imputation algorithms particularly in scenarios where marginalized groups have a higher rate of unknown features compared to dominant groups. The authors introduce a theoretical framework to quantify discrimination risk and show how it can amplify unfairness in machine learning models applied to imputed data. They propose a mean aggregation imputation approach to reduce discrimination risk while minimizing reconstruction error. The solution is evaluated on synthetic and realworld credit networks to assess fairness and accuracy.  Strengths: 1. Innovative Problem Statement: The paper addresses an important and novel issue of discrimination risk in graph feature imputation algorithms shedding light on the potential unfairness it can introduce into ML models. 2. Theoretical Foundation: The authors provide a strong theoretical foundation by defining discrimination risk and proving its impact on model unfairness. They also propose a theoretical solution to mitigate discrimination risk. 3. Empirical Evaluation: The evaluation on synthetic and realworld datasets adds practical relevance to the theoretical contributions. The comparison of fairness and accuracy metrics provides valuable insights into the performance of the proposed approach.  Weaknesses: 1. Societal Impact Discussion: The paper could benefit from a more extensive discussion on the societal implications of their work particularly in realworld applications such as automated candidate screening and loan approval where unfairness can have significant consequences. 2. RealWorld Dataset Limitation: The inability to reproduce similar results on the realworld datasets as seen in prior works raises concerns about the generalizability of the proposed solution. Further analysis and explanation are needed to address this discrepancy.  Questions and Suggestions for Authors: 1. Generalizability: How applicable is the proposed mean aggregation imputation framework to a broader range of graph datasets beyond the ones considered in the evaluation 2. Algorithm Optimizations: Are there ways to enhance the computational efficiency of the proposed algorithm without compromising its fairness guarantees 3. Intersectional Fairness: Have you considered exploring fairness at the intersections of different groups to capture multiple axes of discrimination and ensure inclusivity  Limitations: The authors did not extensively discuss the broader societal impact of their work. They should consider providing a more detailed analysis of the potential negative implications especially in highstakes decisionmaking scenarios. To improve they can engage with experts in ethics and social science fields for deeper insight.", "rG0jm74xtx": " Summary The paper introduces a novel approach D3VAE for time series forecasting using generative modeling. The proposed model utilizes a bidirectional variational autoencoder (BVAE) with coupled diffusion denoising and disentanglement techniques to improve forecast accuracy and interpretability. Extensive experiments on synthetic and realworld datasets demonstrate the superiority of D3VAE over existing algorithms.  Strengths 1. Innovative Approach: The incorporation of coupled diffusion denoising and disentanglement in the BVAE framework is a novel and effective strategy for improving time series forecasting. 2. Performance: D3VAE outperforms competitive algorithms with significant margins across synthetic and realworld datasets showcasing its effectiveness. 3. Experiments: Thorough evaluation on diverse datasets and the comparison with multiple baselines enhance the credibility of the proposed approach. 4. Uncertainty Estimation: The approach effectively estimates uncertainty through noise estimation contributing to the reliability of predictions.  Weaknesses 1. Ablation Study Scope: The ablation study could be further expanded to explore the individual impact of each component (diffusion denoising disentanglement) on model performance. 2. Model Complexity: The complexity of the proposed model with multiple components may increase training time and computational resources required which could be a downside in practical applications. 3. Assumptions: The paper assumes the independence of noise and ideal data which may not hold in all scenarios. Addressing this assumption\u2019s implications on model robustness could be beneficial.  Questions and Suggestions 1. Evaluation Metrics: Could additional metrics beyond CRPS and MSE be considered to comprehensively evaluate model performance such as accuracy precision or recall 2. Interpretability: How does disentangling latent variables enhance model interpretability and can these disentangled factors be validated or labeled in realworld time series applications 3. Robustness: Have robustness tests been conducted to assess the models performance under different input noise levels or data distributions Robustness analysis can provide insights into model reliability across diverse scenarios.  Limitations The authors could further elaborate on: 1. Handling Bias: The potential bias introduced by the coupled diffusion model and strategies to mitigate it should be addressed to enhance the models reliability across diverse applications and datasets.  Suggestions for Improvement 1. Robustness Analysis: Conduct additional experiments to evaluate the models robustness under varying noise levels data distributions or potential biases to ensure its stability across different scenarios. 2. Scalability: Investigate strategies to streamline the model complexity and enhance scalability for wider adoption in realworld applications. 3. Potential Negative Impacts: Acknowledge potential negative implications of the proposed approach in specific societal contexts and provide recommendations or safeguards to mitigate such impacts. The paper presents a strong contribution to the field of time series forecasting and addressing these suggestions could further enhance the clarity robustness and applicability of the proposed D3VAE model.", "sGugMYr3Hdy": " Summary: The paper introduces a Bayesian model of Goal Inference to implement pedagogy and pragmatism mechanisms in a teacherlearner setup for learning from demonstrations. The study shows that these mechanisms facilitate communication between agents reduce goal ambiguity and improve learning especially in scenarios with few demonstrations.  Strengths: 1. Innovative Contributions: Introducing pedagogical teaching and pragmatic learning mechanisms based on Bayesian Goal Inference is novel and contributes to improving artificial agent training setups. 2. Clear Explanation: The paper provides a detailed explanation of the pedagogy and pragmatism mechanisms along with the experimental procedures and results making it understandable for readers. 3. Thorough Experiments: The experiments show the effectiveness of pedagogy and pragmatism in reducing goal ambiguity and enhancing learning speed in a multigoal environment.  Weaknesses: 1. Complexity: The paper is quite technical and may be challenging for readers without a strong background in cognitive science or machine learning. 2. Limited Discussion on Scalability: The scalability of the proposed approach to more complex scenarios or realworld applications is not extensively discussed.  Questions and Suggestions for Authors: 1. Verification of Bayesian Goal Inference (BGI): How did the authors validate the accuracy and efficacy of the Bayesian Goal Inference mechanism in inferring goals from demonstrations 2. Impact of Training Regime: How does the performance of the proposed approach vary with different ratios of pedagogical demonstrations to collected trajectories in the learners experience buffer 3. Evaluation Metrics: Have the authors considered any additional metrics or benchmarks to assess the generalization capabilities of the proposed mechanisms across varying environments  Limitations: The authors should consider addressing potential negative societal impacts of these mechanisms such as reinforcing biases or affecting humanmachine interactions negatively. Constructive suggestions for improvement could include incorporating ethical considerations into the experimental design and analysis.", "vdxOesWgbyN": " 1) Summary The paper explores the vulnerability of Split Federated Learning (SFL) to model extraction attacks and proposes five ME attacks in a threat model where gradient query is allowed but prediction query is not. The attacks differ in data assumptions and gradient usage. The paper evaluates the performance of the attacks on VGG11 model on CIFAR10 dataset and provides insights into the impact and implications of these attacks.  2) Strengths and Weaknesses Strengths:  Thorough exploration of the vulnerability of SFL to ME attacks and proposing multiple attack methods.  Extensive evaluation with varying settings and datasets to demonstrate the effectiveness of the proposed attacks.  Clear presentation of the methods results and implications.  Indepth discussion on the impact of the attacks on model IP and resistance to adversarial attacks. Weaknesses:  The paper could benefit from additional discussion on potential defenses against the proposed attacks.  Consideration of realworld scenarios and practical implications of the attacks could enhance the papers relevance.  Error bars for experimental results could provide a more robust analysis.  3) Questions and Suggestions for Authors 1. Have you considered exploring potential defense mechanisms against the proposed ME attacks in SFL 2. How do you plan to address realworld scenarios and practical implications of the attacks in future work 3. Could you provide insights into the possible negative societal impacts of these attacks and how to mitigate them 4. How do you plan to enhance the robustness of your experiments possibly by incorporating error bars for result variability and accuracy estimation 5. Is there a plan to further investigate the tradeoff between ME resistance and data privacy in SFL to optimize both aspects  Limitations Authors could consider:  Further exploring potential defenses against the proposed attacks.  Providing insights into the realworld implications and practical scenarios of these attacks.  Incorporating error bars for better experimental robustness and accuracy estimation.", "vaxPmiHE3S": " Review:  Summary: The paper introduces an Eventbased GRU (EGRU) a model designed to improve the scalability and efficiency of recurrent neural networks by reducing computation required at each time step through activity sparsity and eventbased communication. The contributions include the introduction of EGRU eventbased gradientdescent demonstration of competitive performance on realworld tasks and high levels of activity sparsity during inference and learning.  Strengths: 1. Innovative Approach: The concept of EGRU and its eventbased activitysparse model is novel and addresses a significant challenge in recurrent neural networks. 2. Competitive Performance: The paper demonstrates competitive performance in realworld tasks compared to stateoftheart recurrent network models. 3. Comprehensive Analyses: The paper provides detailed mathematical formulations experimental results and comparisons with existing architectures. 4. Clear Presentation: The paper is wellstructured with clear explanations detailed sections and references.  Weaknesses: 1. Complexity: The continuous and discretetime formulations along with eventbased gradientdescent might be challenging for some readers to follow. 2. Evaluation Scope: While the paper demonstrates competitive performance a more extensive evaluation on a wider range of tasks could strengthen the findings further. 3. Lack of Generalization: The experimental evaluation focuses mainly on specific tasks without comprehensive generalization to different tasks and data types.  Questions and Suggestions for Authors: 1. Generalization: How does the performance and activity sparsity of the EGRU model vary across different types of datasets and tasks 2. Scalability: Have you considered scalability challenges when deploying the EGRU model to larger datasets or more complex tasks 3. Efficiency: In what ways can the EGRU model be optimized further to enhance its computational efficiency and applicability to a broader range of applications 4. Comparison: What are the specific advantages and disadvantages of the EGRU model compared to existing LSTM and GRU architectures in terms of practical implementations and performance  Limitations: The authors have adequately addressed the limitations of their work including potential societal impacts. However further validation of the models realworld applicability and performance across diverse tasks could enhance the studys robustness and practical relevance. Enhancing the discussion on the potential negative societal impacts and mitigation strategies could also be beneficial.  Overall the paper presents a significant contribution to the field of recurrent neural networks with the introduction of the Eventbased GRU model. It provides a detailed and innovative approach to improving scalability and efficiency in RNNs. Further refinement and validation of the model across a wider range of tasks and datasets could strengthen the papers impact and applicability in various domains.", "oR5WIUtsXmx": "Summary: The paper investigates the behavior of coordinateMLPs a type of MLPs used in computer vision for representing highfrequency signals concerning implicit regularization assumptions. It finds that coordinateMLPs do not exhibit the same implicit bias towards lowcomplexity solutions as typical MLPs affecting smooth interpolations and generalizability across signal regions. Through a Fourier lens the paper uncovers that increasing the models bandwidth can suppress lower frequencies hindering smooth interpolations. A novel regularization technique is proposed to mitigate this issue without changing the architecture. Strengths: 1. Theoretical Depth: The paper tackles a complex topic involving neural network behavior through a detailed analysis from a Fourier perspective showcasing a deep understanding of the subject matter. 2. Novel Contribution: The paper provides new insights into the behavior of coordinateMLPs highlighting their limitations and proposing a practical regularization technique to address the identified issues. 3. Empirical Validation: Theoretical findings are supported by empirical experiments providing tangible evidence to support the theoretical arguments developed in the paper. Weaknesses: 1. Limited Scope: The analysis primarily focuses on shallow models limiting the generalizability of the findings to deeper networks commonly used in realworld applications. 2. Complexity of Proposed Regularization: The proposed regularization technique involves manual tuning of loss components potentially making it challenging for practitioners to implement without detailed domain knowledge. 3. Lack of Comparative Analysis: The paper lacks a comparison with existing regularization techniques or other types of networks which could provide a stronger context for the proposed methods effectiveness. Questions and Suggestions: 1. Clarification on Model Depth: How do the findings and insights from shallow models extend to deeper coordinateMLPs commonly used in realworld applications Consider discussing the implications of model depth on the identified issues. 2. Scalability and Ease of Implementation: Can the proposed regularization technique be easily scaled to deeper networks and are there automated methods for tuning the regularization hyperparameters to improve usability and practicality Limitations: While the paper thoroughly addresses the limitations in the context of shallow networks it could further enhance its impact by exploring the implications of these limitations on the general applicability of coordinateMLPs in broader and more complex neural network architectures. Consider elaborating on potential negative societal implications such as the reduced generalizability of coordinateMLPs in realworld computer vision applications.", "wfKbtSjHA6F": " Summary The paper investigates the effectiveness of \"winning tickets\" sparse subnetworks obtained via magnitude pruning in datalimited regimes for image recognition tasks. The authors show that sparse winning tickets outperform dense networks in lowdata settings with up to 16 improvement. Extensive experiments reveal the robustness and generalizability of sparse winning tickets to synthetic noise and domain shifts especially in longtailed classification scenarios.  Strengths  The paper presents a comprehensive study on sparse winning tickets in lowdata regimes offering empirical evidence and detailed experimental results.  The methodology is welldefined utilizing iterative magnitude pruning (IMP) and augmentation strategies to demonstrate the effectiveness of sparse networks.  Results showcase the superior performance of sparse winning tickets in datalimited scenarios highlighting their potential for improved generalization.  Weaknesses  The paper could benefit from more comparison with existing literature on sparse networks and dataefficient learning methods to provide a broader context for the findings.  Although the experiments are thorough additional analysis on the interpretability of sparse network representations could enhance the insights provided by the study.  The paper would benefit from a clearer emphasis on the practical implications and realworld applications of the proposed approach for broader impact.  Questions and Suggestions for Authors 1. Transferability of Findings: How applicable are the findings of the study to realworld image recognition tasks beyond the datasets used in experiments 2. Scalability: How does the proposed approach scale with increasing dataset sizes or complexities Could the benefits of sparse winning tickets be maintained with larger datasets 3. Interpretation of Results: Can the authors provide more insights into why sparse winning tickets outperform dense networks in the context of generalization and feature learning 4. Reproducibility: Have the authors considered sharing more detailed experimental setups or analysis scripts to enhance reproducibility for the research community  Limitations The authors have adequately addressed the limitations by conducting a thorough analysis of the properties of sparse winning tickets in datalimited scenarios. To further enhance the paper they could provide stronger connections to practical applications and emphasize the scalability and interpretability of their proposed approach for broader adoption.", "muvlhVKvd4": "Peer Review:  Summary: The paper presents a fundamental unified convergence theorem used for deriving expected and almost sure convergence results for a series of stochastic optimization methods. The theorem is not specific to any particular algorithm and provides a plugintype convergence analysis for a wide class of stochastic optimization methods. The paper applies this framework to several classical stochastic optimization methods including stochastic gradient descent (SGD) random reshuffling (RR) stochastic proximal gradient method (proxSGD) and stochastic modelbased methods for nonsmooth nonconvex optimization problems. The results reveal new insights and provide strong convergence guarantees for these methods. Strengths: 1. The paper introduces a novel unified convergence framework that is applicable to a broad range of stochastic optimization methods. 2. The application of the framework to various stochastic optimization algorithms demonstrates its effectiveness in providing new and strong convergence guarantees. 3. The theoretical analysis and proofs are detailed providing a clear understanding of the convergence results derived. 4. The results offer significant contributions to the field by enhancing the understanding of the convergence properties of stochastic optimization algorithms. Weaknesses: 1. The complexity of the theorem and application steps may limit accessibility to readers with less mathematical background. 2. The paper could benefit from more practical examples or case studies to demonstrate the realworld applicability of the proposed convergence framework. 3. The discussion on comparative advantages or drawbacks compared to existing convergence analysis methods is relatively limited. Questions and Suggestions for Authors: 1. Could you provide practical scenarios or application domains where the proposed convergence framework can be particularly useful 2. How does the proposed framework compare to existing convergence analysis methods in terms of computational complexity and practical applicability 3. Have you considered conducting experiments or case studies to validate the convergence guarantees provided by the framework in realworld optimization problems 4. Could you elaborate on the computational implications of implementing the unified convergence theorem in stochastic optimization algorithms to show the practical significance of the results 5. It would be beneficial to discuss the scalability of the framework for largescale optimization problems or highdimensional datasets. Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work by focusing on theoretical convergence analysis. To further enhance the practical relevance of the research the authors could consider integrating empirical evaluations or applications in realworld scenarios to complement the theoretical results.", "u6GIDyHitzF": " Peer Review  1) Summary: The paper addresses the Karmed dueling bandit problem focusing on a batched setting for efficient learning algorithms in scenarios like web search ranking and recommendation systems. The proposed C2B algorithm achieves regret bounds close to the best sequential algorithms under the Condorcet condition with computational experiments demonstrating competitive performance on realworld datasets.  2) Assessment of Strengths and Weaknesses: Strengths:  The paper addresses a relevant and challenging problem in the field of machine learning.  The proposed C2B algorithm provides a novel solution with regret bounds almost matching best sequential algorithms.  The theoretical analysis is welldeveloped with rigorous proofs of key lemmas and theorems.  Computational experiments on diverse datasets validate the algorithms effectiveness and solidify the theoretical findings. Weaknesses:  The paper could benefit from a clearer elucidation of the algorithmic steps in C2B to aid readers in understanding its implementation.  The introduction of additional illustrations or flowcharts could enhance the comprehension of the proposed algorithm.  Further comparison with a broader range of stateoftheart algorithms could enhance the robustness of the studys contributions.  3) Questions and Suggestions for the Authors: 1. Could you provide a more intuitive explanation of how the C2B algorithm operates potentially with illustrative examples to aid readers in grasping its workings more easily 2. Did you consider investigating the scalability of the C2B algorithm for extremely large datasets and if so what were the outcomes 3. Have you explored potential variations of the proposed algorithm or extensions to handle scenarios where the Condorcet winner assumption may not hold  4) Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To improve consider discussing ethical considerations concerning the deployment of dueling bandit algorithms in realworld applications such as user privacy bias in recommendations or unintended consequences of algorithmic decisionmaking. Overall the paper presents a significant advancement in the dueling bandit problem domain but additional clarifications and considerations regarding societal impacts could enhance the papers contribution.", "mT18WLu9J_": " Summary: The paper investigates data poisoning attacks that amplify the risks of privacy leakage of benign training samples by increasing the membership exposure of targeted classes. It presents both dirtylabel and cleanlabel poisoning attacks along with countermeasures and conducts evaluations on computer vision benchmarks.  Strengths: 1. Original Contribution: The paper explores a novel aspect of data poisoning attacks related to privacy leakage offering valuable insights. 2. Comprehensive Experiments: Extensive evaluation on various datasets and models provides a robust analysis of the proposed attacks. 3. Indepth Analysis: The paper thoroughly explains the attack methodologies threat models and potential countermeasures.  Weaknesses: 1. Ethical Considerations: Limited discussion on ethical implications and societal impact despite the sensitive nature of privacyrelated attacks. 2. Theoretical and Experimental Clarifications: Some theoretical results need more detailed explanations and including error bars would enhance result reliability.  Questions and Suggestions for Authors: 1. Data Privacy: How do the proposed attacks align with data privacy regulations and what steps could be taken to ensure compliance with such standards 2. Transferability of Attacks: Have you considered the transferability of the poisoning attacks to different domains or model architectures 3. Realworld Applications: Can you discuss potential realworld scenarios where these attacks could pose significant risks and how can businesses protect against such threats effectively  Limitations: While the authors address the limitations and negative societal impacts to some extent there could be further enhancements: Authors may consider delving deeper into the implications of their work in the broader context of data privacy and ethical AI by providing concrete steps for mitigating the negative impacts and ensuring responsible use of their findings.", "vdh62914QR": "1. Summary: The paper provides a novel analysis of generalization error in blackbox learning using the Zerothorder Stochastic Search (ZoSS) algorithm extending previous work on gradientbased methods. The analysis covers unbounded and bounded possibly nonconvex losses showing that ZoSS achieves generalization error bounds similar to Stochastic Gradient Descent (SGD) under certain conditions. 2. Strengths:  The paper introduces an important analysis of generalization error for blackbox learning filling a gap in understanding the performance of derivativefree optimization methods.  The results demonstrate that ZoSS can achieve comparable generalization guarantees to SGD making it a viable alternative for scenarios where gradient evaluations are not feasible or efficient.  Rigorous theoretical analysis including stability bounds growth recursions and generalization error bounds provide a solid foundation for the study. Weaknesses:  The paper is highly technical and assumes advanced familiarity with optimization theory which may limit accessibility for a broader audience.  While the theoretical results are comprehensive practical implications and empirical validations are lacking.  The abstract and introduction could be more readerfriendly to improve understanding for nonexperts in the field. 3. Questions and Suggestions for the Authors:  Can you provide any insights on how the theoretical findings translate into practical applications or realworld scenarios  Consider including a section discussing potential practical challenges or limitations in implementing ZoSS in various applications.  Have you considered conducting experiments or case studies to validate the theoretical findings and evaluate the performance of ZoSS in realworld settings 4. Limitations: The paper focuses primarily on theoretical analysis and the translation of these findings into practical applications or addressing potential negative societal impacts is limited. A stronger connection between theoretical results and practical implications could enhance the papers relevance and broaden its impact. Consider discussing practical considerations implications and limitations of applying ZoSS in realworld scenarios.", "yZgxl3bgumu": " Peer Review:  1. Summary: The paper presents a novel datadriven framework named PhysGNN which leverages graph neural networks to approximate soft tissue deformation in neurosurgical settings. The framework aims to account for the structural information of the finite element mesh and enhance computational feasibility while accurately predicting tissue deformation. Empirical evaluations demonstrate promising results on two datasets in terms of accuracy and computational efficiency making PhysGNN suitable for neurosurgical applications.  2. Strengths:  Innovative Approach: The proposal of PhysGNN introducing GNNs for tissue deformation predictions is a significant step forward in the domain of neurosurgery.  Accurate Results: Results show the effectiveness of PhysGNN with high accuracy in predicting tissue deformation.  Competitive Performance: The paper compares favorably to existing methods in terms of accuracy and computational efficiency making it a suitable choice for practical neurosurgical settings.  Thorough Experimental Validation: Detailed experimental results and comparisons with existing methods contribute to the reliability and value of the proposed model.  3. Weaknesses:  Lack of Theoretical Analysis: The paper lacks indepth theoretical analysis of the proposed PhysGNN model its complexity and the underlying principles compared to existing methods.  Limited Discussion on Failure Modes: The authors should discuss potential failure modes or limitations of the PhysGNN model to provide a comprehensive understanding of its applicability.  4. Questions and Suggestions: 1. Theoretical Foundation: It would be beneficial to include more theoretical insights into how PhysGNN leverages GNNs to capture tissue deformation. Providing more clarity on the intricacies of the model would enhance the papers technical depth. 2. Failure Analysis: Consider discussing potential scenarios where PhysGNN may not perform optimally such as in cases of highly complex deformations or extreme force applications. 3. Scalability: How scalable is PhysGNN when dealing with a vast number of mesh nodes Discuss the models limitations when processing highresolution meshes. 4. Comparison Metrics: Are there additional metrics beyond Euclidean error that could provide further insights into the models performance in different scenarios Consider exploring other evaluation criteria. 5. EndUser Interface: How userfriendly is PhysGNN for neurosurgeons in practical applications Address any interface design considerations or ease of implementation for endusers.  5. Limitations: While the paper adequately discusses the limitations of using FEM for tissue deformation future works could focus on addressing the potential inaccuracies stemming from MRI image segmentation and soft tissue constitutive laws. Additionally considering Meshless Total Lagrangian Explicit Dynamics methods for simulation could enhance accuracy.  6. Future Directions:  Implementation Accessibility: Make the data and implementation of PhysGNN publicly available for future benchmarking and comparison studies.  Advanced Simulation Algorithms: Consider exploring the effectiveness of Meshless Total Lagrangian Explicit Dynamics algorithms alongside PhysGNN for more accurate soft tissue deformation predictions.  7. Conclusion: In conclusion the paper presents a novel and promising approach with PhysGNN for accurately predicting soft tissue deformation in neurosurgery. By addressing theoretical depth potential failure modes and scalability concerns the framework can be further refined to enhance its applicability in practical neurosurgical settings. Overall the paper makes a significant contribution to the field and provides a solid foundation for future research in the domain of neurosurgical procedures. Further enhancements in theoretical analysis and failure scenarios could strengthen the papers technical impact.", "y5ziOXtKybL": " Peer Review  Summary The paper presents theoretical results on Bayesian neural networks particularly focusing on the posterior consistency and convergence rates in the context of the Besov space. The study extends prior work by considering spikeandslab and shrinkage priors highlighting near minimax convergence rates. The paper also discusses adaptive estimation methods and practical implications for large sample sizes.  Strengths  The paper offers rigorous theoretical analysis on Bayesian neural networks and their convergence properties in the Besov space contributing to the existing literature.  The extension of results to shrinkage priors enhances the practical applicability of the findings addressing computational challenges associated with spikeandslab priors.  The adaptive estimation approach and detailed numerical examples provide valuable insights for implementing the theoretical findings in realworld scenarios.  Providing extensive background on neural networks activation functions and Bayesian methods enriches the papers context and aids in understanding the presented results.  Weaknesses  The paper could benefit from more clarity in articulating the practical implications of the theoretical results for industry applications and addressing the gap between theory and practical implementation.  While the theoretical results are comprehensive the paper could be strengthened by including empirical validation or case studies to demonstrate the effectiveness of the proposed approaches in realworld scenarios.  The complexity of the mathematical notation and technical details might pose a barrier to readers unfamiliar with advanced Bayesian inference methods warranting a clearer explanation for accessibility.  Questions and Suggestions for Authors 1. Could you elaborate on how the theoretical results presented in the paper can be practically implemented in realworld scenarios especially in highdimensional data and complex neural network architectures 2. How do the proposed adaptive estimation methods compare to existing techniques in terms of computational efficiency and accuracy particularly in largescale applications 3. Could you provide more insights into potential extensions of this work such as incorporating additional priors or exploring different activation functions for Bayesian neural networks  Limitations The authors have not explicitly addressed the potential limitations and societal impacts of their work. To enhance the papers completeness and ethical considerations it would be beneficial to include a section discussing these aspects ensuring transparency and responsible research practices. Overall the paper presents valuable contributions to the theoretical analysis of Bayesian neural networks but a clearer link between theory and practical applications as well as addressing limitations and societal impacts would further enhance its impact and relevance.", "rcMG-hzYtR": " Summary: The paper introduces a novel offpolicy actorcritic approach M2TD3 aimed at optimizing worstcase performance in reinforcement learning scenarios where policies trained in simulation environments may suffer significant degradation when transferred to realworld environments due to model misspecification. The proposed method formulates a trilevel optimization problem to optimize performance under uncertainty parameters. Experiments conducted on MuJoCo environments show that M2TD3 outperformed several baseline approaches in terms of worstcase performance.  Strengths: 1. Innovative Approach: The paper introduces a novel method M2TD3 to address the critical issue of policy degradation in realworld environments due to model misspecification. 2. Comprehensive Experimental Evaluation: The thorough experimentation conducted on 19 MuJoCo tasks provides strong empirical evidence of the superiority of M2TD3 over baseline methods. 3. Detailed Methodology: The paper provides an elaborate explanation of the proposed algorithm including its formulation components and optimization strategies making it easy for readers to understand the technical aspects of M2TD3.  Weaknesses: 1. Limited Comparison: While the paper compares M2TD3 with relevant baseline methods it could benefit from further comparisons with additional stateoftheart approaches to reinforce the effectiveness of the proposed method. 2. Limitation Clarity: The paper briefly discusses the limitation of small uncertainty sets affecting M2TD3s performance but could elaborate further on other potential limitations or scenarios where the proposed method might not be as effective.  Questions and Suggestions: 1. Further Experiments: Have you considered conducting experiments on more diverse environments or realworld applications to demonstrate the scalability and generalizability of M2TD3 2. Approach Comparison: How does M2TD3 compare to other existing worstcase optimization methods in terms of computational efficiency and robustness in handling various uncertainty scenarios 3. Sensitivity Analysis: Have you performed sensitivity analysis to evaluate the impact of hyperparameters and design choices on the performance of M2TD3  Limitations: The paper does not explicitly address the potential negative societal impact of the work. Considering the focus on reinforcement learning algorithms it would be beneficial for the authors to discuss the implications of their research on societal applications ensuring ethical and responsible AI development. Suggestions for improvement include integrating discussions on potential biases fairness and ethical considerations in AI decisionmaking processes.", "m6HNNpQO8dc": " Peer Review  Summary: The paper introduces novel activation functions based on probabilistic Boolean logic operations in logitspace aiming to formalize dendritic computations within neural networks. By deriving logitspace operators equivalent to AND OR and XNOR for independent probabilities the authors propose efficient approximations named ANDAIL ORAIL and XNORAIL which can be used as activation functions. These new functions are evaluated on various tasks like tabular classification image classification transfer learning abstract reasoning and compositional zeroshot learning.  Strengths: 1. Novel Contribution: The paper offers a novel perspective by introducing activation functions inspired by probabilistic Boolean logic operations. 2. Theoretical Rigor: The derivation of logitspace operators and subsequent development of efficient approximations demonstrate a strong theoretical foundation. 3. Experimental Validation: Extensive experiments on diverse tasks including image classification transfer learning and compositional zeroshot learning showcase the effectiveness of the proposed activation functions. 4. Ensemble Strategies: The exploration of ensemble strategies using multiple activation functions provides valuable insights into enhancing network performance. 5. Normalization and Transfer Learning: The consideration of normalization techniques and successful performance in transfer learning tasks further enhance the relevance of the proposed activation functions.  Weaknesses: 1. Computational Overhead: The computational expense due to multiple exponent and logarithm calculations in the logitspace operators could pose a limitation in practical implementation. 2. Limited Empirical Analysis: While the paper presents a wide range of experiments a more detailed comparative analysis against existing stateoftheart activation functions could add further depth to the evaluation. 3. Limited Generalization: The study primarily focuses on twodimensional activation functions exploring the generalizability to higher dimensions could strengthen the findings.  Questions and Suggestions: 1. Efficiency Enhancement: Have you considered any further optimization techniques to reduce the computational overhead of the logitspace operators ensuring practical implementation feasibility 2. Comparative Analysis: How do the proposed activation functions compare against established activation functions like ReLU ELU and others in terms of convergence speed and overall network performance 3. Scalability: How scalable are these activation functions to deeper architectures and larger datasets Have you tested their performance on more complex tasks like object detection or natural language processing 4. Interpretability: Can you provide more insights into the interpretability of these probabilistic Booleanlogicbased activation functions in understanding network decisions and feature interactions  Limitations: The authors could enhance the discussion on the limitations and societal impact of their work by explicitly addressing the potential computational burden and implications for realworld applications. Providing suggestions for improving efficiency while maintaining effectiveness would strengthen the practical relevance of the proposed activation functions. Overall the paper presents an innovative approach to activation functions with strong theoretical grounding and promising empirical results. Addressing the computational overhead and scalability concerns would make the findings more accessible and impactful for the broader research community.  This comprehensive review aims to provide constructive feedback for the authors to enhance the quality and impact of their research.", "luGXvawYWJ": " Summary: The paper introduces a novel dataset distillation (DD) approach called HaBa which factorizes a dataset into data Hallucination networks and Bases to improve data efficiency during compression. The method combines adversarial contrastive constraints to increase data diversity and achieve better downstream classification performance while reducing parameters significantly.  Strengths: 1. Novel Perspective: HaBa introduces a unique dataset factorization approach that significantly improves data efficiency. 2. Versatility: The method is compatible with existing DD baselines making it a flexible and effective approach. 3. Performance Improvement: Extensive experiments show significant performance gains in downstream tasks and crossarchitecture generalization compared to stateoftheart methods. 4. Adversarial Constraints: The pair of adversarial contrastive constraints increases data diversity and informativeness of factorized results. 5. Thorough Experiments: The paper presents comprehensive experiments on standard datasets providing strong evidence of the methods effectiveness.  Weaknesses: 1. Complexity: The process of online pairwise combination between hallucinators and bases may increase computation costs. 2. Limited Performance Gain: For large numbers of images further increasing the number may have limited benefits. 3. Overfitting Risk: The few distilled samples may still pose a risk of overfitting in deep models.  Questions and Suggestions: 1. How does HaBa perform in scenarios with varying dataset sizes or types such as natural images with different resolutions 2. Could the authors elaborate more on the computational resources and training time required for implementing HaBa compared to baseline methods 3. How does the adversarial constraint impact the convergence and stability of the training process especially with larger datasets 4. Have the authors considered exploring classwise relationships in the dataset factorization process to potentially enhance performance further  Limitations: The paper may consider discussing the potential computational costs and time overhead associated with the online pairwise combination in training as well as addressing any risks of overfitting with the proposed method. Furthermore exploring additional scenarios beyond the ones tested could provide a broader assessment of HaBas capabilities.", "pZtdVOQuA3": " Summary: The paper proposes a novel rendering algorithm for neural radiance fields based on importance sampling to optimize the fields with just a few radiance field evaluations per ray. The algorithm reparametrizes the sampling process enabling endtoend learning with gradient descent.  Strengths: 1. Innovative Contribution: The reparametrization of the sampling algorithm is a significant contribution allowing for improved optimization of neural radiance fields with reduced computational costs. 2. GradientBased Optimization: The algorithms fully differentiable nature facilitates gradientbased optimization of the fields. 3. Efficiency: By using importance sampling the paper achieves competitive rendering quality with fewer radiance queries reducing memory requirements.  Weaknesses: 1. Evaluation: The paper lacks comprehensive quantitative experiments to demonstrate the algorithms superiority over existing methods. 2. Comparison: While the algorithm claims improved efficiency and rendering quality a more detailed comparison against stateoftheart methods would strengthen the paper.  Questions and Suggestions: 1. Evaluation Metrics: Could you provide additional quantitative metrics to compare the performance of your algorithm with existing methods 2. Scalability: Have you tested the scalability of your algorithm with larger more complex scenes 3. Noise Analysis: How does your algorithm handle noise in rendering especially with varying levels of importance sampling 4. Evaluation with Realistic Data: Have you tested your algorithm on realworld datasets to demonstrate its efficacy outside synthetic settings  Limitations: The authors should address the limitations regarding the lack of detailed evaluations and comparisons with stateoftheart methods. Conducting thorough experiments and providing a comprehensive analysis would enhance the papers credibility.", "wFymjzZEEkH": " 1. Summary: The paper proposes a personalized Federated Learning (FL) method named \"lpproj\" that aims to balance different constraints of interest such as communication efficiency robustness and fairness simultaneously. The method utilizes Lpregularization and lowdimensional random projection to control the deviation between global and local models. The proposed method shows convergence for smooth objectives with square regularizers and provides benefits in terms of robustness fairness and communication efficiency through theoretical analysis and empirical experiments.  2. Assessment of Strengths and Weaknesses:  Strengths:  Novel Contribution: The paper addresses an important problem in personalized FL and introduces a novel method to achieve personalized learning while balancing various constraints.  Theoretical Foundation: The paper provides a robust theoretical analysis covering convergence robustness against adversarial attacks and fairness across clients.  Empirical Validation: Extensive experimental results demonstrate the superiority of the proposed method over stateoftheart techniques in terms of communication efficiency robustness and fairness.  Comprehensive Comparison: The comparison with multiple existing methods provides a clear indication of the effectiveness of the proposed approach.  Weaknesses:  Complexity: The proposed method while effective may be complex for practical implementation and deployment.  Limited Scenarios: The experiments are conducted on a limited set of datasets and scenarios limiting the generalizability of the findings.  Communication Efficiency Metrics: More detailed information on the metrics used to evaluate communication efficiency could enhance the papers credibility.  3. Questions and Suggestions for Authors:  Can you elaborate on the practical implications and computational overhead of implementing the proposed method in a realworld FL setting  How does the proposed method perform when faced with nonlinear models or complex data distributions  To expand the applicability of your work have you considered conducting experiments on a wider range of datasets with varying complexities  4. Limitations: The authors have discussed the limitations of the study particularly in addressing adverse societal impacts and potential negative consequences of their work. To further enhance the studys credibility and ethical standards it is suggested to include a more detailed discussion on potential ethical implications and societal impacts of their proposed method. Transparency and ethical considerations are essential when dealing with advanced technologies like Federated Learning.  Overall The paper represents a significant contribution to the field of Federated Learning offering a novel approach that balances multiple constraints. Strengthening the practical applicability further extending the experimental evaluation and discussing the ethical dimensions of the research could enhance the papers impact and relevance.", "stAKQ6vnFti": " 1) Brief Summary: The paper introduces a novel framework called Contrastive Learning with Lowdimensional Reconstruction (CLLR) to address the issue of scattering instances sparsely in highdimensional feature spaces leading to insufficient capture of similarity between pairwise instances. CLLR uses a sparse projection layer to reduce dimensionality while maintaining instance discrimination effectiveness. Theoretical analysis proves CLLRs effectiveness and empirical results show improvement across various domains compared to existing CL techniques.  2) Strengths and Weaknesses: Strengths:  The paper addresses a critical issue in contrastive learning by proposing CLLR to learn lowdimensional contrastive embeddings effectively.  The theoretical analyses provide a strong foundation for the proposed method demonstrating its significance and necessity.  Empirical results across multiple domains show consistent improvement over stateoftheart methods validating the superiority of CLLR. Weaknesses:  The paper lacks detailed performance comparison with distillationbased CL models which could provide more insights into CLLRs effectiveness compared to alternative approaches.  While the experimental results are promising a more extensive evaluation on a wider range of datasets and tasks would strengthen the argument for CLLRs generalizability and robustness.  The explanation of the ablation study and experimental setups could be more detailed to aid in reproducibility and clarity of the results.  3) Questions and Suggestions for Authors:  Could the authors provide more insights into how CLLR compares to distillationbased CL models in terms of performance and computational efficiency  How robust is CLLR to variations in dataset characteristics and task complexities Further analysis on a broader set of datasets could enhance the generalizability claims.  Could the authors elaborate on the potential computational overhead introduced by CLLR and discuss strategies to mitigate any additional computational costs  4) Limitations: The authors need to provide more explicit details and comparisons regarding the negative societal impact of their work to ensure that ethical implications are adequately addressed. Suggestions for improvement include conducting a comprehensive ethical assessment to identify and mitigate potential negative consequences of CLLR.", "uLhKRH-ovde": " Summary: The paper introduces a communicationefficient distributed gradient clipping algorithm for training deep neural networks in a distributed setting. The algorithm leverages a relaxed smoothness assumption of the loss landscape and enables parallel speedup with reduced communication rounds. The paper offers theoretical proofs for iteration and communication complexity and validates the algorithm through experiments on benchmark datasets in various scenarios.  Strengths: 1. Innovative Algorithm: The paper presents a novel algorithm that addresses the exploding gradient issue in distributed training offering clear benefits such as linear speedup and reduced communication complexity. 2. Theoretical Analysis: The authors provide detailed theoretical analysis including iteration and communication complexity results enhancing the credibility of the proposed algorithm. 3. Empirical Validation: Extensive experiments on different tasks corroborate the algorithms effectiveness in achieving fast convergence in practice substantiating the theoretical claims. 4. Clear Presentation: The paper is wellstructured with clear explanations of technical concepts making it accessible to readers.  Weaknesses: 1. Limited Scope: The analysis is restricted to homogeneous data settings limiting the generalizability of the algorithm. Exploring heterogeneous data scenarios could strengthen the paper. 2. Experimental Setup: The authors could provide more details on hyperparameters convergence criteria and computational resources utilized in the experiments to enhance reproducibility and comparability.  Questions and Suggestions for Authors: 1. Heterogeneous Data Settings: Have you considered extending the analysis to heterogeneous data distributions to evaluate the algorithms performance in more realistic scenarios 2. Hyperparameter Sensitivity: How sensitive is the algorithms performance to variations in hyperparameters such as clipping threshold and synchronization interval length 3. Practical Implications: How does the proposed algorithm compare to existing distributed training methods in terms of implementation complexity and scalability 4. Future Directions: Are there plans to explore applications of the algorithm beyond the scenarios tested in the experiments such as in more diverse deep learning tasks or realworld distributed environments  Limitations: The authors have adequately discussed the limitations of the homogeneous data setting but could enhance the paper by addressing heterogeneous data scenarios and potential societal impacts suggesting future research directions for broader applicability and societal considerations.", "xONqm0NUJc": " Summary The paper introduces Relational Proxies a novel approach for finegrained visual categorization that leverages crossview relationships between global and local views of an object to improve semantic labeling accuracy. The authors provide a rigorous theoretical framework design a specific model architecture and achieve stateoftheart results on multiple benchmark datasets.  Strengths 1. Theoretical Rigor: The paper offers a comprehensive theoretical formulation of the finegrained visual categorization task providing necessary and sufficient conditions for effective learning. 2. Novel Approach: Introducing Relational Proxies to bridge the information gap between viewspecific and crossview relational properties is a unique and promising contribution. 3. StateoftheArt Results: By surpassing existing methods with significant accuracy gains across multiple datasets the proposed approach demonstrates superior performance. 4. Thorough Experiments: The paper includes indepth experimental evaluations ablation studies and visual representations that support the theoretical claims and demonstrate the effectiveness of the proposed method.  Weaknesses 1. Limitation Addressing: The paper briefly mentions limitations around obtaining local views and could benefit from a more detailed discussion on potential challenges of the methodology. 2. Empirical Evaluation: While the theoretical framework is robust a deeper empirical analysis discussing the impact of varying components in different scenarios could enhance the papers completeness. 3. Generalizability: The discussion on the generalizability of the approach beyond the presented datasets could be strengthened considering the diversity of potential applications.  Questions and Suggestions for Authors 1. Methodological Clarification: Can the authors provide a clearer explanation of how the relationagnostic and relationaware components of their model interact during training and inference 2. Scalability Consideration: How does the computational complexity scale with the number of finegrained categories particularly when using relational proxies for metric learning 3. Interpretability: How can the interpretability of the learned relational proxies be enhanced to aid practitioners in understanding the models decisions 4. Robustness Analysis: Have the authors conducted sensitivity analyses to evaluate the methods robustness to variations in hyperparameters and dataset characteristics  Limitations The authors have adequately addressed the limitations of their work particularly around the uninformed nature of obtaining local views. A suggestion for improvement could involve exploring more informed techniques for identifying object parts for improving performance.  Societal Impact The rigorous theoretical foundation of the paper ensures transparency and facilitates the study of finegrained visual categorization. As with all datadriven approaches attention to biases in training data is crucial for mitigating negative societal impacts.", "nDemfqKHTpK": " PeerReview  1) Summary: The paper introduces GraB an algorithm that leverages the herding framework to improve the data ordering in Stochastic Gradient Descent (SGD). GraB aims to provide faster convergence rates by selecting better orderings than Random Reshuffling all while reducing memory and computational overhead. The proposed algorithm is validated through extensive experiments on various machine learning tasks.  2) Assessment of Strengths and Weaknesses: Strengths:  The paper addresses an important problem in model training proposing a novel algorithm based on theoretical grounding.  The theoretical foundation connecting SGD with herding is compelling and the proposed GraB algorithm demonstrates empirical superiority over baseline methods.  Extensive experiments conducted on diverse datasets and models to validate the algorithms performance.  Thorough literature review and clear structured presentation of the problem theory and algorithm. Weaknesses:  The introduction of multiple algorithms and theoretical concepts might be challenging for readers not familiar with the field.  Limited discussion on the scalability of GraB to larger datasets or more complex models.  More comparisons to stateoftheart methods beyond baseline algorithms could enhance the papers impact.  3) Questions and Suggestions:  Can the authors provide more insights on the scalability of the GraB algorithm to larger datasets that are common in realworld scenarios  How does GraB perform in scenarios with highly imbalanced datasets or noisy data  Have the authors considered the computational efficiency when comparing different batch sizes in the experiments  Further clarity on the interpretability of the results in terms of practical implementation and generalization to other domains would enhance the papers impact.  4) Limitations: The authors have not explicitly addressed the potential negative societal impacts of their work. To improve they could consider discussing ethical considerations implications for fairness and bias and potential unintended consequences of accelerated convergence techniques. In conclusion the paper presents a novel algorithm with promising results and solid theoretical foundations. Addressing scalability interpretability and societal impacts would strengthen the papers contributions to the field of machine learning optimization.", "n7Rk_RDh90": " PeerReview of Paper Titled \"3D Concept Grounding Using Neural Descriptor Fields\" Summary: The paper addresses the challenging problem of 3D concept grounding by utilizing RGBD images and paired questions and answers to learn visual concepts. The authors propose a framework that leverages the continuous differentiable nature of neural fields to segment and learn concepts in a 3D space. Through experiments on a collected dataset the authors demonstrate that their proposed framework outperforms existing models on semantic and instance segmentation tasks and 3D aware visual reasoning tasks. Strengths: 1. Innovative Framework: The utilization of neural fields for 3D concept grounding is innovative and differentiates the paper from existing approaches. 2. Differentiable Learning: The incorporation of differentiable learning in the proposed framework allows for joint learning of segmentations and concept grounding. 3. Performance: The experimental results demonstrate the superior performance of the proposed model compared to baseline models across various tasks. Weaknesses: 1. Limited RealWorld Training Data: The limitation of training the model only on synthetic scenes and questions raises concerns about realworld generalization. 2. Complexity: The complexity of the proposed framework may make it challenging for adoption and practical implementation. 3. Evaluation Metrics: The paper lacks a detailed discussion on the evaluation metrics used for comparing the proposed framework with baselines which could affect the interpretability of the results. Questions and Suggestions: 1. Generalization to Real Scenes: How do you plan to expand the training data to include realworld scenes to enhance the models generalization capabilities 2. Societal Impact: Have you considered potential ethical implications or biases that may arise from deploying such advanced visual reasoning systems in realworld applications 3. Scalability: Given the complexity of the proposed framework how do you plan to ensure scalability for realtime applications or deployment in resourceconstrained environments Limitations: The authors should consider expanding their dataset to include realworld scenes to improve the models generalization ability for practical applications. Additionally addressing the potential ethical implications of deploying such advanced systems is crucial for societal impact.  Verdict: The paper presents an innovative framework for 3D concept grounding with promising results on synthetic datasets. Addressing the weaknesses highlighted and considering the suggestions provided could further strengthen the papers contribution and applicability.", "uFSrUpapQ5K": " Summary: The paper introduces two novel estimators for the OffPolicy Evaluation (OPE) problem in the context of deficient support scenarios. These estimators leverage side information about actions to provide reliable evaluations without relying on the unrealistic full support assumption. The paper presents theoretical guarantees experimental evaluations and finitesample concentration inequalities to compare the proposed estimators with traditional methods like IPS and regressionbased estimators.  Strengths: 1. Innovative Contribution: Introducing two novel estimators for OPE in the deficient support scenario is a significant contribution to the field. 2. Theoretical Rigor: The paper provides insightful theoretical analysis concentrative inequalities and alternative assumptions enhancing the understanding of estimator behaviors. 3. Experimental Evaluation: Practical performance comparisons with realworld datasets validate the superiority of the proposed estimators over traditional methods establishing their effectiveness.  Weaknesses: 1. Complexity: The complexity of the proposed estimators especially in computation and interpretation may pose challenges for practical implementation. 2. Hyperparameter Selection: The paper introduces hyperparameters for the novelty estimator without a clear crossvalidation procedure leading to potential concerns about robustness and generalization.  Questions and Suggestions: 1. Evaluation Metrics: Did the authors consider other evaluation metrics apart from Mean Squared Error (MSE) to provide a more comprehensive understanding of estimator performance 2. Practical Implementation: How do the proposed estimators perform in terms of computational efficiency and scalability especially in largescale applications 3. Robustness Analysis: Have robustness analyses been conducted to assess the stability of the estimators under various conditions such as dataset shifts or noise 4. Validity of Assumptions: Given the importance of underlying assumptions have sensitivity analyses been conducted to test the estimators robustness to deviations from these assumptions  Limitations: The paper does not explicitly address the potential negative societal impacts of implementing the proposed estimators. To enhance the papers societal relevance and ethical considerations the authors could include a discussion on potential negative implications privacy concerns or bias issues introduced by their estimators. They should also provide strategies to mitigate any identified negative impacts.", "mXP-qQcYCBN": "Summary: The paper introduces a selfsupervised method called AutoLink for learning structured representations specifically keypoints from unlabelled image collections. AutoLink disentangles object structure from appearance using a graph of 2D keypoints and learns both keypoint locations and pairwise edge weights without supervision. The resulting interpretable graph allows for applications like keypoint and pose estimation with outperforming existing selfsupervised methods. The method paves the way for structureconditioned generative models on diverse datasets. Strengths: 1. Innovative Approach: Autolink presents a novel selfsupervised method for learning structured representations which is a significant contribution to the field. 2. Interpretability: The resulting graph structure is interpretable and applicable to various domains which enhances the methods usability. 3. Experimental Results: AutoLink demonstrates superior performance compared to existing methods in keypoint localization tasks and exhibits robustness across different datasets. Weaknesses: 1. Limited Addressing of Occlusions: The model struggles to handle occlusions and distinguish between left and right sides of objects which could affect its performance in realworld scenarios. 2. Lack of 3D Structure Modeling: The method is 2Dbased and does not model 3D structures limiting its ability to handle certain complexities like occlusion. 3. Complexity: The paper could benefit from clearer explanations or visual aids in some sections where the methods intricacies are described. Questions and Suggestions: 1. Clarity on Edge Weights: Can the authors explain in more detail how the learnable edge weights contribute to the models performance 2. Scalability: How does the complexity of the method scale with larger datasets or more diverse image collections 3. Comparison with Supervised Methods: Have the authors considered comparing AutoLink with supervised approaches for keypoint detection to demonstrate its effectiveness further Limitations: The authors address the risk of potential misuse of keypoints for deep fakes or surveillance applications but could provide more detailed discussions on addressing these ethical concerns and potential negative societal impacts going forward. Suggestions for Improvement: 1. Ethical Considerations: Provide a more thorough discussion on potential misuse and societal impact along with suggestions for mitigation measures. 2. Model Robustness: Conduct experiments to further evaluate the models robustness to variations in background structures and occlusions for practical applicability. Overall the paper presents a promising approach with strong experimental results but a focus on addressing limitations and enhancing model robustness could further improve its impact and practicality.", "xbhsFMxORxV": " Peer Review:  Summary: The paper proposes a novel model called Hyphen a discourseaware hyperbolic spectral coattention network for socialtext classification tasks. By incorporating public discourse represented as Abstract Meaning Representation (AMR) graphs Hyphen leverages hyperbolic geometric representation and Fourier coattention mechanism to enhance socialtext classification. Extensive experiments on fake news detection hate speech rumour and sarcasm tasks show that Hyphen achieves stateoftheart results on various datasets providing explanations alongside predictions.  Strengths: 1. Innovative Model: The fusion of hyperbolic graph representation learning and Fourier coattention mechanism is novel addressing the limitation of existing methods by incorporating public discourse signals. 2. Exhaustive Experiments: The paper conducts extensive experiments on multiple socialtext classification tasks across ten benchmark datasets showcasing the superior performance of Hyphen compared to various baselines. 3. Explainability: The models capability to provide explanations through generated coattention weights is an added advantage enhancing the interpretability of the predictions.  Weaknesses: 1. Complexity: The complexity of the proposed model Hyphen might make it challenging to implement for practitioners without a strong background in hyperbolic geometry and Fourier analysis. 2. Limitation Addressing: The paper does not provide a detailed discussion on addressing limitations such as scalability and potential negative societal impacts that could arise from using public discourse signals.  Questions and Suggestions for Authors: 1. Clarity on Implementation: Can the authors provide more insights into the practical implementation of Hyphen especially concerning the intricacies of hyperbolic geometry and Fourier analysis for researchers less familiar with these concepts 2. Scalability: How does the proposed model scale with large datasets in terms of computational resources and training time Is there a plan to optimize it for broader applicability  Limitations: The authors can further enhance the paper by: 1. Explicitly Addressing Limitations: It would be beneficial to clearly acknowledge and discuss the scalability challenges associated with implementing Hyphen as well as potential negative societal impacts that may arise from leveraging public discourse signals. 2. Improving Interpretability: To improve the interpretability of results authors can explore methods to simplify the model architecture ensuring that practitioners can comprehend and implement Hyphen with relative ease.  Overall Assessment: The paper introduces an innovative model Hyphen for socialtext classification tasks demonstrating superior performance and good explainability. While the model shows promise addressing implementation complexities and enhancing interpretability will improve its practical utility. The authors should focus on addressing limitations and ensuring wider applicability to attract a broader audience interested in social media analytics.", "q-snd9xOG3b": " Summary: The paper investigates the construction of canonical noise distributions (CNDs) for various differential privacy (DP) frameworks focusing on logconcave CNDs and multivariate CNDs. It examines the relationships between tradeoff functions group privacy and mechanism composition showcasing results on existence construction and impossibility of CNDs for different DP guarantees.  Strengths: 1. Theoretical Contributions: The paper provides significant theoretical insights into the construction and properties of CNDs for various DP frameworks. 2. Novelty: The exploration of logconcave and multivariate CNDs fills a gap in the literature on privacy mechanisms. 3. Rigorous Analysis: The paper rigorously proves the relationships between tradeoff functions group privacy and mechanism composition adding to the understanding of privacy guarantees.  Weaknesses: 1. Technical Complexity: The paper is highly technical and may be challenging for readers without a strong background in differential privacy and related concepts. 2. Limited Practical Applications: While the theoretical results are valuable the practical implications may require further exploration or realworld applications. 3. Lack of Empirical Validation: The paper lacks empirical validation or practical implementations of the proposed concepts which could enrich its impact and applicability.  Questions and Suggestions for Authors: 1. Clarifications on Constructed CNDs: Could the authors provide more insights into the practical implications of the constructed logconcave and multivariate CNDs How can these CNDs be practically utilized in DP mechanisms 2. Comparison with Existing Mechanisms: How do the proposed CNDs compare with existing DP mechanisms in terms of efficiency accuracy and applicability in realworld scenarios 3. Scalability and Generalizability: Have the authors considered the scalability and generalizability of the proposed CND constructions across different datasets and applications  Limitations: The authors should address the following limitations: 1. Societal Impact Considerations: The paper should further discuss the potential societal impact of the proposed mechanisms considering aspects such as fairness transparency and unintended biases to ensure responsible deployment of privacy mechanisms.", "qfC1uDXfDJo": " Peer Review: 1) Summary: The paper provides a comprehensive study of the optimization problem associated with fitting twolayer ReLU neural networks with respect to the squared loss. The authors leverage rich symmetry structures to develop new tools to understand how overparameterization eliminates spurious minima. Analytic estimates for loss Hessian spectrum and descent directions are obtained at different minima shedding light on the effectiveness of overparameterization in navigating the loss landscape with gradientbased methods. 2) Assessment of Strengths and Weaknesses: Strengths:  Utilization of novel tools from algebraic geometry representation theory and symmetry breaking to explore the optimization problem associated with neural networks.  Rigorous analytic estimates for the loss and Hessian spectrum at various minima provide a deep understanding of the impact of overparameterization.  The paper offers detailed theoretical insights into how overparameterization influences the transition from spurious minima to saddles enhancing the understanding of neural network optimization landscapes. Weaknesses:  The papers complexity may pose challenges for readers not wellversed in algebraic geometry representation theory and symmetry breaking.  The empirical validation of the theoretical findings could strengthen the practical relevance of the study.  The high level of mathematical formalism and technical details may limit the accessibility to a broader audience outside specialized academic fields. 3) Questions and Suggestions:  Could you provide more intuitive explanations or visual aids to help readers grasp the key concepts especially for those less familiar with algebraic geometry and representation theory  Considering the substantial mathematical depth of the paper how can you bridge the gap to make the implications and applications of your findings clearer to a broader audience including practitioners in deep learning  Have you considered incorporating simulations or experiments to complement the theoretical analysis and validate the conclusions in realworld scenarios 4) Limitations: While the authors acknowledge certain limitations and complexities in their work further elaboration on the potential negative societal impacts or discussions on ethical considerations related to their research could provide a more comprehensive assessment of the studys implications and ramifications. 5) Suggestions for Improvement:  Enhance the readability by using more accessible language and providing illustrative examples to clarify intricate concepts for a broader audience.  Consider including empirical validations or practical applications of the theoretical findings to enhance the studys relevance and tangible impact in realworld scenarios.  Further discussion on ethical considerations societal impacts or potential biases in the research could contribute to a more comprehensive evaluation of the study. Overall the paper presents a rigorous and insightful exploration into the optimization landscape of neural networks leveraging advanced mathematical techniques. To broaden the impact and accessibility of the findings addressing the suggested improvements could enhance the clarity and relevance of the study.", "uPyNR2yPoe": "Summary: The paper introduces ELIGN a selfsupervised intrinsic reward for multiagent systems inspired by selforganization in Zoology. ELIGN aims to improve coordination in decentralized training settings by encouraging agents to align their behaviors with their teammates expectations while being misaligned with adversaries. The efficacy of ELIGN is demonstrated across 6 tasks in multiagent particle and Google Research football environments outperforming sparse and curiositybased intrinsic rewards in most scenarios. Strengths: 1. Novel Contribution: The paper introduces a novel selfsupervised intrinsic reward ELIGN which offers a taskagnostic approach for multiagent coordination in decentralized training environments. 2. Empirical Evaluation: The paper provides a thorough empirical evaluation across various cooperative and competitive tasks demonstrating the superior performance of ELIGN compared to existing intrinsic rewards. 3. Scalability and Generalizability: ELIGN shows scalability and generalizability improving coordination and breaking symmetries across different tasks and team sizes. 4. Clear Explanations and Method Implementation: The paper carefully explains the formulation of ELIGN the training algorithms and provides detailed explanations of the evaluation setups. Weaknesses: 1. Limitations in RealWorld Applicability: The study focuses on specific tasks in artificial environments limiting generalizability to realworld scenarios with complex action spaces and dynamics. 2. Limited Scope: The paper mainly evaluates ELIGN in comparison to sparse and curiositybased rewards with potential benefits of richer reward structures left unexplored. 3. Incomplete Discussion: The paper lacks detailed discussions on potential negative societal impact of the introduced method and future implications on humanAI interactions. Questions and Suggestions: 1. Generalization to RealWorld Scenarios: How do you envision the application of ELIGN in more complex and realistic multiagent settings with larger action spaces and varying dynamics 2. Scalability and Resource Requirements: Could you provide insights into the computational resources required for implementing ELIGN in scenarios with a large number of agents How does the method scale with increased agent complexity 3. Potential Negative Societal Impact: Have you considered any potential negative societal impact of heavily incentivizing predictability in multiagent systems How might this affect diverse realworld collaborative settings 4. Future Research: Could you elaborate on any followup research avenues building upon the introduced concept of expectation alignment How might the interplay between curiositydriven exploration and expectation alignment play out in more complex tasks Limitations: The paper inadequately addresses potential negative societal impacts of promoting predictability through intrinsic rewards. To improve consider discussing ethical considerations and implications for humanAI interactions.", "r5rzV51GZx": " Summary: The paper introduces a certifiably robust collaborative inference framework called CoPur to defend against various attacks during collaborative inference such as distributed adversarial attacks and missingfeature attacks. The proposed method leverages the blocksparse nature of adversarial perturbations on the feature vector and assumes the embedded features lie on an underlying lower dimensional manifold. The paper provides theoretical analyses and empirical evaluations on ExtraSensory and NUSWIDE datasets to demonstrate the effectiveness of CoPur against targeted and untargeted adversarial attacks.  Strengths: 1. Innovative Method:  The paper introduces a novel method CoPur that addresses an important problem in collaborative inference and provides theoretical guarantees of recovery in the presence of adversarial attacks. 2. Theoretical Foundation:  The paper provides a solid theoretical foundation for the proposed method including detailed proofs and discussions on the conditions for exact or approximate feature recovery under different threat models. 3. Empirical Evaluation:  Extensive experiments on realworld datasets showcase the superiority of CoPur over existing defenses demonstrating its robustness against targeted and untargeted attacks. 4. Clear Writing:  The paper is wellwritten and effectively presents the problem proposed method and experimental results in a clear and organized manner.  Weaknesses: 1. Limited Comparison:  While the paper presents a detailed evaluation of the proposed method it lacks comparison with some stateoftheart defense mechanisms commonly used in adversarial attacks which could provide a more comprehensive assessment. 2. Realworld Applicability:  The paper focuses primarily on theoretical analysis and empirical results but it could benefit from a discussion on the practical applicability of CoPur in realworld scenarios and potential implementation challenges.  Questions and Suggestions for Authors: 1. Scalability: Have you considered the scalability of CoPur to handle a large number of agents in practical IoT settings 2. Impact of Noise: How does CoPur perform in the presence of noise and uncertainties in the embedded features apart from adversarial attacks 3. Generalization: Can the proposed feature purification method be generalized to handle different types of feature representations beyond those considered in the current experiments  Limitations: The authors have not fully addressed the potential negative societal impact of their work. To improve they could include a discussion on ethical considerations related to the use of robust defenses in collaborative inference systems.", "xvlaiSHgPrC": " Summary The paper explores the composition properties of differential privacy in concurrent compositions focusing on interactive mechanisms. It significantly generalizes previous work by providing optimal parallel composition properties for various notions of differential privacy including approximate DP R\u00e9nyi DP and zeroconcentrated DP. The results show that adversaries gain no advantage when interleaving queries to independently running mechanisms highlighting interactivity as a free feature granted by differential privacy.  Strengths  The paper addresses an important gap in the literature by studying concurrent compositions of interactive mechanisms expanding the applicability of differential privacy notions.  The theoretical results are significant proving optimal composition theorems for various forms of differential privacy showcasing the robustness of interactivity.  Weaknesses  While the theoretical contributions are substantial the practical implications and realworld applications of the findings could be further elaborated to enhance the papers impact.  The paper might benefit from clearer explanations for the methodologies employed particularly for readers less familiar with the technical details of differential privacy.  Questions and Suggestions for Authors 1. Can you provide more insights into potential practical applications of your results for designing new differentially private algorithms 2. How do the findings of the paper contribute to improving the understanding and implementation of privacypreserving mechanisms in realworld scenarios 3. Have you considered evaluating the performance of the proposed composition theorems in practical settings or conducting experiments to validate their efficacy 4. How do you envision future research building upon your work to address challenges in fully adaptive compositions of differential privacy  Limitations The authors may enhance the paper by discussing potential negative societal impacts or ethical considerations that could arise from the deployment of their findings in realworld applications. They should consider addressing limitations related to the scope and generalizability of their results thoroughly.", "yZcPRIZEwOG": " Summary: The paper introduces a novel approach to policy optimization with linear temporal logic (LTL) constraints. The framework decouples task specification from policy selection offering a systematic alternative to traditional cost minimization in reinforcement learning. The proposed modelbased approach leverages a generative model to guarantee task satisfaction and cost optimality through a reduction to a reachability problem. The paper provides strong theoretical guarantees develops new theoretical tools and empirically demonstrates the effectiveness of the algorithm in various scenarios.  Strengths: 1. Innovative Framework: The decoupling of task specification from policy selection is a novel and promising approach to RL with LTL constraints. 2. Theoretical Rigor: The paper offers strong theoretical guarantees provides a sample complexity analysis and introduces new theoretical tools that may have independent interest. 3. Empirical Validation: Empirical results demonstrate the algorithms effectiveness in achieving strong performance even with composite specifications.  Weaknesses: 1. Highly Technical: The paper is dense with technical details and formalisms making it challenging for readers without extensive background knowledge to grasp the content. 2. Complex Assumptions: The reliance on assumptions such as a generative model and lower bounds on transition probabilities may limit the generalizability and practical applicability of the proposed algorithm. 3. Limited Clarity: Some sections especially the algorithm descriptions would benefit from clearer explanations to enhance understanding.  Questions and Suggestions: 1. Clarification on Generative Model Use: Can the authors clarify the implications and practical feasibility of assuming access to a generative model in realworld applications 2. Scalability Concerns: How does the proposed approach scale with complex environments and larger state spaces Are there any computational limitations or scalability considerations 3. Practical Implementation: How transferable is the proposed algorithm to realworld scenarios Are there any plans to validate the algorithm in more complex environments or industry usecases  Limitations: The authors could further address the practical implications of their assumptions potential limitations in realworld deployment and the computational overhead of the proposed algorithm to enhance the applicability and robustness of their work. Consider providing more concrete suggestions on addressing these limitations for future research. Overall the paper presents an innovative and theoretically sound framework for RL with LTL constraints but further improvements in clarity practical feasibility and scalability may enhance its impact and applicability.", "pluyPFTiTeJ": "Peer Review Summary: The paper addresses the domain generalization problem by proposing a novel optimization approach that minimizes a surrogate penalty under the constraint of optimality of the empirical risk. The method is demonstrated to improve performance on various benchmarks and its effectiveness is shown by outperforming existing methods like CORAL FISH and VRex. Strengths: 1. Innovative Approach: The paper introduces a unique optimization method that focuses on minimizing the penalty without compromising the empirical risk filling a gap in existing domain generalization methods. 2. Theoretical Grounding: The paper is wellsupported by theoretical foundations drawing a connection between ratedistortion theory and domain generalization providing a solid basis for the proposed optimization approach. 3. Empirical Validation: The method is empirically validated on a variety of benchmarks demonstrating significant performance improvements compared to existing approaches like CORAL FISH and VRex. 4. Methodological Rigor: The paper presents a comprehensive methodology including algorithmic details convergence proofs and clear stepbystep descriptions of the optimization process. Weaknesses: 1. Computational Complexity: The method introduces significant computational overhead due to the need for gradients per domain limiting its practicality in realworld applications. 2. Limited Scope of Applications: The paper focuses on penaltybased domain generalization models potentially excluding other approaches that may benefit from the proposed optimization method. 3. Lack of Comparative Analysis: While the paper extensively evaluates the proposed method on various benchmarks more indepth comparisons with other stateoftheart methods would provide a clearer understanding of its superiority. Questions and Suggestions for Authors: 1. Scalability Concerns: How does the proposed method scale with the number of domains and dataset sizes Have you considered any techniques to mitigate the computational complexity 2. Generalizability: How applicable is your optimization approach to different types of datasets beyond image language and graph modalities Have you tested it on more diverse datasets or realworld applications 3. Benchmark Diversity: Could you provide more insights into the specific characteristics of the benchmarks chosen for evaluation and their relevance to realworld domain generalization challenges 4. Comparative Analysis: While the results are promising a more detailed comparison with a broader set of existing domain generalization methods including extensive ablation studies would strengthen the papers impact and contribution. Limitations: The authors have not adequately addressed the computational complexity introduced by the method. A suggestion for improvement could be exploring approximation techniques to reduce this complexity while maintaining the methods efficacy. Overall the paper presents a novel optimization approach for domain generalization supported by solid theoretical grounding and empirical results. Addressing the highlighted weaknesses and questions would further enhance the papers quality and impact in the machine learning research community.", "q-tTkgjuiv5": " Review of Paper: \"Fair Allocation of Graphical Resources\" 1. Summary and Contributions: The paper studies fair allocation of graphical resources where agents utilities depend on weight of maximum matchings. It explores Maximin Share (MMS) fairness and EnvyFree up to one item (EF1). Key contributions include constantapproximation algorithms for homogeneous agents and results for heterogeneous agents highlighting difficulties and solutions for different scenarios. 2. Strengths:  The paper rigorously examines fair resource allocation in graph structures providing insights into MMS and EF1.  Theoretical analysis is thorough with precise definitions theorems and algorithm descriptions.  The consideration of both homogeneous and heterogeneous agent scenarios adds depth to the study. 3. Weaknesses:  The paper lacks practical validations or empirical experiments to support the theoretical findings. Realworld applications could strengthen its impact.  The complexity of the algorithms and proofs may limit accessibility for readers not deeply familiar with the subject matter. 4. Questions and Suggestions for Authors:  How do the proposed algorithms perform in computational simulations compared to existing methods  Could the paper discuss potential applications of the research in diverse fields beyond theoretical contexts  Clarify further how the proposed allocations practically translate to scenarios like studentadvisor pairings or resource distribution in networks. 5. Limitations:  The paper does not extensively address potential negative societal impacts. Authors could consider discussions on fairness bias or inequality exacerbation and suggest mitigation strategies. The paper presents a comprehensive analysis of fair allocation in graphical resources showcasing advancements in MMS and EF1. Strengthening practical relevance and addressing societal implications could enhance the papers impact. Overall the work exhibits strong theoretical exploration in the domain of fair resource allocation.", "xDaoT2zlJ0r": " Summary: The paper proposes a novel approach called First IntegralPreserving Neural Differential Equations (FINDE) to find and preserve first integrals from data using neural networks. It leverages projection methods and discrete gradient methods to predict future states accurately even without prior knowledge of the underlying structures. Experimental results demonstrate the effectiveness of FINDE in various scenarios including Hamiltonian systems and unknown systems.  Strengths: 1. Novel Contribution: The paper introduces a unique method FINDE that addresses the issue of unknown system structures in predicting future states accurately. 2. Comprehensive Explanation: The paper provides clear explanations of the proposed methodology projection methods and experimental setups aiding in understanding the concepts. 3. Theoretical Grounding: The work includes theoretical underpinnings proofs of the methods capabilities and detailed experiments across different types of systems to validate the approach.  Weaknesses: 1. Computational Complexity: The paper alludes to increased computational complexity especially with the discrete version of FINDE. 2. Limited Scope: The experiments were conducted in finitedimensional Eucleadian spaces restricting the generalizability to more complex manifolds. 3. Training Instabilities: Some trials failed due to underflow of the step size indicating potential stability issues in training.  Questions and Suggestions for Authors: 1. Generalization: Have you considered extending the method to operate on manifolds beyond finitedimensional Eucleadian spaces for broader applicability 2. Stability: How do you plan to address the training instabilities reported especially concerning issues like step size underflow 3. Scalability: How could the proposed approach be scaled to handle larger and more complex systems efficiently  Limitations: The authors adequately addressed the limitations of increased computational complexity and reported training instabilities. It is suggested to consider exploring solutions to improve stability and scalability in future iterations of the method.  Suggestions for Improvement: 1. Provide further insights on enhancing the stability of the training process possibly by finetuning hyperparameters or adjusting the optimization strategies. 2. Enhance the discussion on computational complexity to offer clearer understanding and potentially propose optimizations or parallelization strategies to handle the complexity more efficiently. Overall the paper offers a valuable contribution to the field of neural network modeling of dynamical systems and presents a promising methodology that warrants further investigation and refinement. It is recommended to address the identified weaknesses and incorporate the suggestions for improvement to enhance the overall quality of the work.", "xOqqlH_E5k0": " Summary: This paper presents an augmented deep unrolling neural network for snapshot compressive hyperspectral imaging reconstruction. The proposed network includes innovative modules for gradient update and proximal mapping utilizing memoryassistant descent and crossstage selfattention. Additionally a spectral geometry consistency loss is introduced to enhance reconstruction accuracy. The experiments demonstrate the superior performance of the proposed approach over existing methods on various datasets.  Strengths: 1. Innovative Approach: The paper introduces novel modules for gradient update and proximal mapping enhancing the reconstruction process. 2. Extensive Experiments: The performance advantage of the proposed approach is demonstrated through extensive experiments on various datasets showcasing its superiority over existing methods. 3. Clear Presentation: The paper provides detailed explanations of the proposed method along with visualizations and comparisons making it easy to understand.  Weaknesses: 1. Code Availability: The paper lacks the inclusion of code data and instructions needed to reproduce the main experimental results. Ensuring code availability is crucial for replicability and future research.  Questions and Suggestions for Authors: 1. Training Details: Could you elaborate more on the training details such as data splits hyperparameters and their selection process for better reproducibility 2. Ablation Studies: Can you provide more insights into the ablation studies performed especially regarding the significance of each main component in the proposed approach 3. Real Data Evaluation: How did the proposed approach perform on real data compared to synthetic data and were there any notable differences in performance or challenges encountered 4. Future Directions: Have you considered exploring the application of your proposed DUN for other compressive imaging problems in the future  Limitations: The paper lacks the provision of code data and instructions essential for reproducing the main experimental results hindering the reproducibility and transparency of the research. It is crucial to provide these resources for the benefit of the research community.", "toR64fsPir": " Review 1. Summary: The paper introduces a structurepreserving embedding framework for multilayer networks using a novel generative tensorbased latent space model (TLSM). The model aims to embed vertices into a lowdimensional latent space preserving community structures and network relations. The paper offers theoretical consistency proofs and conducts extensive numerical experiments on synthetic and reallife multilayer networks. 2. Strengths:  Novel Model: The proposal of TLSM is innovative and aims to address the challenges in community detection for multilayer networks.  Theoretical Rigor: The paper provides asymptotic consistency proofs adding a strong theoretical foundation to the proposed method.  Extensive Experiments: The paper showcases thorough numerical experiments on both synthetic and reallife datasets demonstrating the effectiveness of TLSM.  Clear Structure: The paper is wellstructured providing a detailed introduction method section theoretical analysis and experimental results. 3. Weaknesses:  Negative Impacts: The authors did not explicitly discuss any potential negative societal impacts of their work which could be a valuable addition.  Resource Details: Lack of information about the total compute resources used could limit reproducibility and evaluation of the computational aspects of the work. 4. Questions and Suggestions: 1. Experimental Reproducibility: It would be beneficial to provide clear instructions on how to reproduce the main experimental results by sharing code data and details on training. 2. Resource Information: Including specifics about the compute resources used can aid in understanding the scalability and computational requirements of the method. 3. Further Validation: Consider additional experiments or sensitivity analyses to further validate the robustness of the proposed method under various conditions. 5. Limitations: The authors have adequately addressed the limitations of the work by admitting the convergence limitations of the optimization algorithm. To enhance this aspect the authors could provide potential strategies for addressing convergence issues. Overall the paper presents a significant contribution to the field of multilayer network analysis. Strengthening reproducibility elements and further discussions on societal impacts would enhance the comprehensiveness of the work.", "rHnbVaqzXne": "Peer Review of \"A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical Data Representation\" Summary: The paper introduces a novel distribution Rotated Hyperbolic Wrapped Normal Distribution (RoWN) as an enhancement of the Hyperbolic Wrapped Normal Distribution (HWN) for representing hierarchical data in hyperbolic space. The authors provide an analysis of the limitations of HWN in modeling local variations within hierarchical datasets and propose RoWN as a solution. Empirical evaluations on synthetic and benchmark datasets demonstrate the effectiveness of RoWN compared to Euclidean normal distribution diagonal HWN and full covariance HWN. Strengths: 1. The paper addresses a significant gap by proposing RoWN to overcome limitations in modeling local variations in hierarchical datasets. 2. The introduction of RoWN is motivated by thorough geometric analysis providing a strong theoretical foundation for the proposed distribution. 3. The empirical evaluations on diverse datasets validate the superiority of RoWN in representing hierarchical data compared to existing distributions. 4. The clarity and organization of the paper make it understandable for readers interested in probabilistic modeling and hyperbolic space representation. Weaknesses: 1. Limited comparison with additional stateoftheart hyperbolic distribution models besides HWN could enhance the robustness and comprehensiveness of the evaluation. 2. The paper could benefit from a more detailed discussion on the computational complexity of RoWN compared to other distributions. 3. While the empirical results are promising additional analysis on scalability and efficiency in realworld applications would further strengthen the practical relevance of RoWN. Questions and Suggestions: 1. Have you considered investigating the scalability of RoWN in handling larger and more complex hierarchical datasets compared to other distributions 2. How does RoWN perform in scenarios with varying degrees of noise in the dataset Additional experiments with noisy datasets could shed light on the robustness of RoWN. 3. Please elaborate on the computational overhead of RoWN compared to HWN and other distributions particularly in terms of sampling efficiency and training speed. Limitations and Recommendations: The authors should consider discussing the potential negative societal impacts of relying solely on hyperbolic space representations and provide insights into mitigating biases that may arise in the applications of RoWN. It is crucial to address the limitations of RoWN in generalizing to Riemannian spaces and optimize the optimization algorithm for the covariance matrix for better performance. Overall the paper presents a valuable contribution in the field of hierarchical data representation in hyperbolic space and further enhancements in terms of comparison with other models scalability and efficiency aspects could enhance the impact and relevance of RoWN in practical applications.", "qw3MZb1Juo": "Peer Review Summary: The paper introduces Federated NotTrue Distillation (FedNTD) an algorithm aimed at addressing the issue of forgetting in federated learning. The study identifies forgetting as a central challenge in federated learning due to the mismatch in data distributions across clients leading to degraded model performance. The proposed FedNTD algorithm leverages the global models predictions on locally available data to prevent forgetting particularly for nottrue classes. Experimental results demonstrate the effectiveness of FedNTD in enhancing model performance without compromising data privacy or requiring additional communication costs. Strengths: 1. Novel Contribution: The paper provides a novel and insightful perspective on the issue of forgetting in federated learning and offers a wellthoughtout solution with the FedNTD algorithm. 2. Performance Validation: The experimental results showcase the stateoftheart performance of FedNTD across various datasets and setups demonstrating its effectiveness in mitigating data heterogeneity and preventing forgetting. 3. Detailed Analysis: The study offers indepth analyses on the issues of forgetting data heterogeneity and the impact of the FedNTD algorithm on improving weight alignment and convergence in federated learning. Weaknesses: 1. Theoretical Grounding: While the experimental results are convincing the paper lacks a deeper theoretical analysis or mathematical proofs to support the proposed algorithm and its mechanisms. More rigorous theoretical underpinnings would strengthen the paper. 2. Validation Methodology: The paper could benefit from a more detailed comparison with existing forgetting prevention methods in federated learning to showcase the advantages of FedNTD more comprehensively. 3. Clarity and Structure: Some sections particularly those involving Propositions and Definitions could be further clarified for better readability and understanding by readers. Questions and Suggestions for Authors: 1. Can you provide additional insights into how the FedNTD algorithm specifically addresses the issue of forgetting in federated learning beyond the experiments conducted in the study 2. How does FedNTD compare with other stateoftheart forgetting prevention methods in federated learning in terms of computational complexity and scalability 3. Have you considered the practical implications of implementing FedNTD in realworld scenarios with larger and more diverse datasets How adaptable is the algorithm to different settings Limitations and Suggestions: The paper adequately addresses the limitations and societal impact of the proposed work. To enhance the paper further the authors could consider incorporating additional discussion on the algorithms potential ethical implications its applicability to diverse populations and any biases that might arise from utilizing the FedNTD algorithm in practical settings. By addressing these points the paper can provide a more comprehensive and wellrounded assessment of the FedNTD algorithms contributions to the field of federated learning.", "oUigTwc7Cw5": " Summary: The paper proposes a model based on efficient coding theory to explain the mosaic organization of retinal ganglion cells (RGCs) in response to natural videos. The model demonstrates that as the channel capacity (number of simulated RGCs) increases new RGC types with specialized receptive fields emerge capturing higher temporal frequencies over larger spatial areas.  Strengths: 1. Contributions: The paper makes a significant contribution by elucidating how efficient coding principles can predict the emergence of different RGC types and their mosaic organization in response to natural videos. 2. Model Development: The extension of the retinal model to account for spatialtemporal processing of natural videos is a novel and innovative approach. 3. Comprehensive Analysis: The paper provides a thorough analysis of the mosaic organization in response to varying channel capacities and inputoutput noise levels offering valuable insights into the retinal encoding process.  Weaknesses: 1. Implementation Complexity: The detailed mathematical derivations and analytical results might be challenging for readers not wellversed in the field potentially limiting the accessibility of the study. 2. Experimental Validation: While the model results are theoretically sound empirical validation with experimental data could strengthen the papers findings and reliability.  Questions and Suggestions: 1. Generalization:  How well does the models prediction of RGC organization align with experimental observations  Have the RGC types identified in simulations been validated against known RGC subtypes in real neural systems 2. Biological Plausibility:  To what extent do the assumptions of the model such as linear filters and separable filters reflect the true biological processes of the retina  How do the noise considerations in the model align with physiological noise levels in the visual system 3. Impact on Neural Coding:  How do the findings of the study contribute to understanding visual processing and neural coding principles in the retina  How can the insights from this study be applied to broader neural circuitry and sensory processing models  Limitations: The authors have presented a detailed analysis based on theoretical modeling. However empirical validation and comparison with experimental data are necessary to confirm the relevance and accuracy of the models predictions.  Suggestions for Improvement: 1. Incorporate experimental validations using real neural data to verify the applicability of the model to biological systems. 2. Enhance the clarity of the manuscript by providing intuitive explanations alongside the mathematical derivations to improve understanding for a wider audience.", "zfo2LqFEVY": " Brief Summary: The paper introduces a novel Multimodal Grouping Network (MGN) for the audiovisual video parsing task aiming to parse videos into modality and categoryaware temporal segments. The MGN explicitly groups semanticaware multimodal contexts to learn compact and discriminative audio and visual representations. The proposed framework achieves superior results over previous stateoftheart methods on weaklysupervised audiovisual video parsing and significantly reduces parameters compared to baselines.  Strengths: 1. Innovative Approach: The MGN introduces explicit grouping mechanisms for multimodal contexts showing improvements in parsing performance. 2. Superior Results: Empirical experiments demonstrate the effectiveness and superiority of the proposed approach over existing baselines. 3. Efficient Model: The MGN is lightweight using only 47.2 of the parameters of baselines while achieving improved results.  Weaknesses: 1. Limited Comparison: The paper could benefit from comparing the proposed MGN with a wider range of existing methods including more recent approaches. 2. Complexity: The proposed model involves multiple modules and stages which might make it challenging to implement and replicate for other researchers. 3. Limited Dataset: The experiments are conducted on the LLP dataset which may limit the generalizability of the results to other datasets or realworld scenarios.  Questions and Suggestions: 1. Dataset Diversity: Have you considered testing the MGN on other audiovisual datasets to evaluate its performance across different scenarios 2. Scalability: How scalable is the MGN to handle larger datasets or realtime applications Have you tested its performance under different scales 3. Interpretability: Can you provide more insights into the interpretability of the learned classaware features and how they contribute to the overall performance of the model  Limitations and Suggestions for Improvement: The authors could provide more explicit discussions on how they plan to address potential biases in the training data and mitigate any negative societal impacts of the model. One suggestion for improvement could be to include rare but important event categories in the dataset to ensure the model can detect such events effectively. This peer review highlights the innovative nature of the proposed Multimodal Grouping Network while also pointing out potential areas for further exploration and improvement. Overall the paper offers valuable contributions to the field of audiovisual video parsing.", "lfe1CdzuXBJ": " Summary: The paper proposes a fair policy in the linear contextual bandit setting for selecting candidates from different sensitive groups. The policy aims to select candidates with the highest relative rank within their respective groups to ensure fairness despite different reward structures. The proposed greedy policy constructs an estimate of relative rank using the empirical cumulative distribution function. The paper provides theoretical analysis simulation results and experiments on US census data to demonstrate the effectiveness of the policy in achieving fair pseudoregret.  Strengths: 1. Conceptual Contribution: Introduces the concept of group meritocratic fairness a strong notion of fairness in linear contextual bandits. 2. Novelty: Addresses the challenging issue of fairness when rewards from different groups are incomparable. 3. Theoretical Rigor: Provides a detailed theoretical analysis of the proposed policy and its regret bound. 4. Simulation Results: Illustrative experiments demonstrate the policys effectiveness in achieving fair pseudoregret. 5. Realworld Application: Demonstrates the policys performance on US census data showcasing practical relevance.  Weaknesses: 1. Simplified Assumptions: Relies on assumptions like independent contexts and continuous rewards which may not always hold in realworld scenarios. 2. Computational Complexity: The greedy policy involves a high computational and memory cost that grows linearly with time. 3. Limitations in Experimentation: While the experiments provide insights they are conducted on simulated data limiting the generalizability of the findings.  Questions and Suggestions: 1. Scalability: How scalable is the proposed policy to a large number of groups and contexts beyond the settings considered in the paper 2. Robustness: How does the proposed policy perform under noisy or biased data scenarios 3. Comparison with Other Fairness Notions: How does the proposed group meritocratic fairness concept compare with existing fairness definitions in the contextual bandit literature Consider discussing implications for individual fairness as well.  Limitations  Suggestions: The paper could strengthen its contribution by:  Addressing Dynamic Contexts: Consider extensions to handle scenarios where contexts change dynamically.  Societal Implications: Discuss the potential societal impacts especially regarding biased decisionmaking in sensitive scenarios. Overall the paper offers a valuable contribution to the fairness in contextual bandits literature with important theoretical insights and practical implications. However addressing the limitations mentioned could enhance the robustness and applicability of the proposed policy.", "wmsw0bihpZF": " Summary: The paper proposes a novel approach for optimizing data collection workflows in machine learning applications. It formulates the data collection problem as an optimal decisionmaking process considering performance targets collection costs time horizons and penalties for not meeting targets. The proposed LearnOptimizeCollect (LOC) framework minimizes future collection costs and generalizes to multiple data sources. Empirical evaluations demonstrate significant reductions in the risk of failing to meet performance targets across various tasks.  Strengths: 1. Original Contribution: The paper introduces a novel approach to optimization in data collection for machine learning addressing a critical aspect often overlooked in practice. 2. Clear Problem Formulation: The problem statement and the proposed LOC framework are welldefined making it easy to understand the methodology. 3. Experimental Validation: Extensive experiments cover a range of tasks datasets and scenarios showcasing the effectiveness of the proposed method over conventional estimation approaches. 4. Robustness Analysis: The paper demonstrates the robustness of the LOC framework to variations in cost and penalty parameters offering insights into its practical applicability. 5. Theoretical Insights: The paper provides theoretical underpinnings for parameter selection and decisionmaking within the data collection process.  Weaknesses: 1. Complexity: The theoretical underpinnings and mathematical models might be challenging for readers not wellversed in optimization and machine learning. 2. Realworld Applicability: While the experiments demonstrate promising results further validation in realworld scenarios with human involvement and practical constraints may be warranted. 3. Ethical Considerations: The paper mentions societal impacts but does not delve deeply into potential negative consequences such as biases or unfair sampling practices in data collection.  Questions and Suggestions: 1. MultiSource Data Collection: Could you elaborate on how the LOC framework handles the optimization problem with multiple data sources especially in scenarios where there are substantial cost differentials between sources 2. Practical Implications: How feasible is the deployment of the LOC framework in industry settings considering the need for accurate performance evaluations and cost estimations 3. Fairness and Privacy: Have you considered integrating fairness and privacy constraints into the optimization process to prevent biased data collection practices This could enhance the societal impact assessment. 4. Realworld Validations: Are there plans to conduct further realworld validations to assess the effectiveness and scalability of the LOC framework across different applications and domains  Limitations: The paper could enhance its discussion on the potential negative societal impacts of data collection practices by incorporating fairness and privacy considerations. Consider providing practical guidelines for ensuring ethical data collection and utilization in machine learning applications.", "zGvRdBW06F5": "This paper presents a framework for ondevice training enabling model adaptation on tiny IoT devices with limited memory resources. The contributions include proposing an algorithmsystem codesign to enable ondevice training with only 256KB of memory introducing QuantizationAware Scaling to stabilize 8bit quantized training and Sparse Update to reduce memory footprint. The paper implements a lightweight training system Tiny Training Engine to support sparse updates and offload autodifferentiation to compile time. Strengths: 1. Novel Approach: The paper offers a pioneering solution for ondevice transfer learning on tiny IoT devices addressing memory constraints effectively. 2. Algorithm Innovation: The proposed QuantizationAware Scaling and Sparse Update techniques demonstrate significant advancements in enabling ondevice training. 3. Experimental Validation: Extensive experiments on multiple datasets and models show the effectiveness of the proposed framework achieving high accuracy with limited memory. Weaknesses: 1. Lack of Comparative Analysis: While the paper demonstrates the effectiveness of the proposed framework a detailed comparison with existing approaches or benchmarks would enhance the evaluation. 2. Lack of Discussion on Training Efficiency: The paper emphasizes memory reduction but lacks indepth discussion on the training efficiency in terms of convergence speed and computational complexity. 3. Lack of Generalizability: The focus on specific tiny IoT devices and vision recognition limits the generalizability of the proposed framework to other modalities or applications. Questions and Suggestions: 1. Can the authors provide insights into the training convergence speed and computational efficiency enabled by the proposed framework 2. How does the proposed framework compare to existing solutions in terms of accuracy memory efficiency and training speed 3. Consider discussing the applicability of the proposed framework to other modalities beyond vision recognition and its potential adaptability to different tasks. Limitations and Societal Impact Suggestions: The authors should further investigate the generalizability of their framework to different modalities and applications to enhance its utility. Additionally providing a detailed analysis of the societal impacts such as energy consumption or data privacy implications would further enrich the papers discussion.", "z9CkpUorPI": " Review of \"NeuroSchedule: An Efficient GNNbased Scheduling Method for HighLevel Synthesis\" Summary: The paper introduces NeuroSchedule a novel Graph Neural Network (GNN)based scheduling method for HighLevel Synthesis (HLS). It addresses the critical issue of scheduling in HLS to enhance performance efficiently. NeuroSchedule formulates the HLS scheduling problem as a learning problem and proposes a GNNbased learning framework. The key contributions include nearoptimal solutions with a significant reduction in runtime compared to the ILPbased method and an improvement in scheduling results over the stateoftheart entropydirected algorithm. Strengths: 1. Innovative Approach: The paper introduces a novel GNNbased approach to solve the HLS scheduling problem addressing both runtime efficiency and solution quality. 2. Experimental Results: The results demonstrate significant improvements in scheduling solutions and runtime efficiency compared to existing methods showcasing the effectiveness of NeuroSchedule. 3. Scalability: The paper addresses the scalability issue by adopting pretraining models enhancing the models performance for various scheduling problems with different settings. Weaknesses: 1. Clarity in Methodology: While the paper presents the NeuroSchedule algorithm and training details further clarification on the finetuning process and specifics regarding data preprocessing and model architecture could enhance understanding. 2. Comparative Analysis: The paper could benefit from a more detailed comparative analysis with additional stateoftheart methods to provide a comprehensive evaluation of NeuroSchedules performance. 3. Discussion on Limitations: The paper lacks a detailed discussion on potential limitations and societal implications of the proposed method which could be essential for research ethics and practical implementation. Questions and Suggestions: 1. How does NeuroSchedule perform when compared to a wider range of scheduling algorithms beyond the ILPbased and entropydirected methods 2. Can the authors provide more insights into the finetuning process including the specific datasets used and the criteria for determining the effectiveness of finetuning from pretraining 3. How does NeuroSchedule address the solution space exploration versus runtime tradeoff especially in highly complex scheduling scenarios 4. Are there any plans to release the code or dataset used in this study to facilitate reproducibility and further research in this area 5. Could the paper include a discussion on the potential negative impacts or limitations of GNNbased scheduling methods to ensure ethical considerations are adequately addressed Limitations: The paper does not adequately address the limitations and societal impact of the proposed work. Authors could enhance the paper by including a section discussing potential negative impacts of GNNbased scheduling methods and providing insights into mitigating such implications for responsible research practices.", "w_jvWzNXd6n": " Summary: The paper proposes SBMTransformer an innovative model that efficiently adapts attention sparsity based on input data overcoming the limitations of the original Transformers computational cost. The model is based on a mixedmembership Stochastic Block Model enabling each attention head to sample a bipartite graph for attention masking with a linear computational cost. The paper presents theoretical proofs of universal approximability along with empirical results showing superior performance over existing efficient Transformer variants and the original Transformer.  Strengths: 1. Innovative Approach: Introducing mixedmembership SBM for attention sparsity is novel and addresses adaptability issues in existing efficient Transformers. 2. Theoretical Foundation: The theoretical proof of universal approximability enhances the credibility of the proposed model. 3. Empirical Results: Demonstrating superior performance on LRA and GLUE benchmarks showcases the effectiveness of SBMTransformer. 4. Efficiency and Flexibility: Endowing each attention head with the ability to adjust sparsity based on data enhances model efficiency and task performance.  Weaknesses: 1. GPU Optimization: Sparse tensor operations on GPUs pose a performance challenge compared to dense counterparts which could limit practical scalability. 2. Complexity: While the approach is innovative the model architecture and training procedures may be complex to implement and understand potentially hindering adoption by practitioners. 3. Evaluation: While the empirical results are promising more detailed analysis on specific aspects such as convergence behavior robustness to hyperparameters and computational efficiency could strengthen the paper.  Questions and Suggestions: 1. Scalability Concerns: How do you envision addressing the GPU kernel optimization challenge to enhance the computational efficiency of the model for realworld applications 2. Implementation Complexity: Could you provide more insights or simplifications for practitioners to better understand and implement the SBMTransformer model effectively 3. Evaluation Rigor: Can you elaborate on the convergence behavior of SBMTransformer robustness analysis to hyperparameters and further insights on its computational efficiency under different settings for better comparison with existing approaches  Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. They could enhance their paper by discussing ethical considerations implications on energy consumption or social biases that might arise from the deployment of SBMTransformer. Including constructive suggestions in this regard would add value to the research.  Suggestions for Improvement: 1. Ethical Considerations: Provide a brief discussion on the ethical implications of introducing advanced AI models like SBMTransformer and suggestions on mitigating potential negative impacts. 2. Robustness Analysis: Include rigorous analyses of model robustness interpretability and generalization across diverse datasets to strengthen the papers empirical foundation. Overall the paper showcases a strong contribution to the field of efficient Transformers with SBMTransformer. Addressing the outlined points would further enhance the quality and impact of the work.", "xs9Sia9J_O": " Peer Review  Summary: The paper investigates the Individual Global Max (IGM) decomposition approach in cooperative multiagent reinforcement learning highlighting its flaws and proposing a novel training paradigm called IGMDA to address the problem of lossy decomposition and error accumulation. The proposed method integrates imitation learning to prevent error accumulation in the training process. The experimental results on the StarCraft MultiAgent Challenge benchmark problem with zero sight view demonstrate the effectiveness of the proposed approach in outperforming stateoftheart IGMbased methods.  Strengths: 1. Novel Contribution: The paper makes a significant contribution by addressing the flaws in the traditional IGM decomposition approach and proposing a novel strategy IGMDA to overcome the problem of lossy decomposition and error accumulation. 2. Theoretical Analysis: The paper presents a rigorous theoretical analysis to support the proposed method providing a clear understanding of the limitations of the conventional IGM approach. 3. Empirical Validation: The empirical validation on the StarCraft MultiAgent Challenge benchmark problem substantiates the effectiveness of the proposed IGMDA approach in improving performance in environments with zero sight view. 4. Experimental Results: The experimental results demonstrate the superiority of the proposed method over existing IGMbased algorithms showcasing its robustness and efficiency in challenging environments.  Weaknesses: 1. Limitation in Realworld Applications: While the proposed method shows promising results in simulated environments the paper lacks discussion on the applicability and scalability of the approach in realworld scenarios where noise and uncertainties are prevalent. 2. Comparative Analysis: The paper could benefit from a more indepth comparative analysis with other relevant stateoftheart approaches beyond IGMbased methods to establish a broader context for the proposed solution. 3. Clarity and Readability: Some sections of the paper particularly the theoretical propositions could be condensed or clarified for better readability ensuring that the core concepts are easily understandable.  Questions and Suggestions for Authors: 1. Could the authors elaborate on the scalability and generalizability aspects of the proposed IGMDA approach beyond the StarCraft MultiAgent Challenge benchmark problem especially in more complex and dynamic environments 2. How does the proposed method handle noisy and incomplete information in realistic multiagent systems and are there measures to address the impact of uncertainties in the training process 3. Considering the focus on zero sight view scenarios how does the approach perform when the vision range is increased and what implications does this have for the practical applicability of the method in varying environmental conditions  Limitations: The authors could enhance the paper by providing a more comprehensive discussion on how the proposed method may have limitations or potential negative societal impacts. Suggestions for improvement could include incorporating ethical considerations and societal implications of utilizing advanced reinforcement learning methods.  Conclusion: Overall the paper presents a valuable contribution to the field of cooperative multiagent reinforcement learning by addressing the shortcomings of the traditional IGM approach. The rigorous theoretical analysis and empirical validation underscore the effectiveness of the proposed IGMDA method. Addressing the weaknesses and incorporating suggested improvements can further enhance the impact and relevance of the research findings.", "qYc8VnmUwbv": "PeerReview: 1. Summary: The paper explores smoothed sequential learning in the context of online classification and regression problems. The authors develop efficient algorithms that achieve statistically optimal regret scaling in various scenarios such as realizable Kwise linear classification and piecewise regression. They introduce novel concepts like the disagreement region and the use of John ellipsoids to attain logarithmic regret bounds. The work bridges the gap between statistical optimality and computational efficiency in smoothed online learning settings. 2. Strengths:  Rigorous Analysis: The paper provides a comprehensive theoretical analysis leveraging concepts like geometric measure theory and convex geometry to develop novel algorithms.  Contributions: The presented algorithms achieve statistically optimal regret bounds for various function classes shedding light on the interplay between smoothness parameters and regret minimization.  Application Scope: The extension of results to noiseless contextual bandits and the application of the perceptron algorithm demonstrate the versatility and practical implications of the proposed methods. 3. Weaknesses:  Lack of Empirical Assessment: While the theoretical developments are solid empirical validation or experimental comparison with existing methods could enhance the practical relevance of the proposed algorithms. Adding empirical results would strengthen the papers impact in realworld applications.  Complexity of Algorithms: The computational efficiency of the introduced algorithms is highlighted but a more detailed discussion on the complexity and scalability of the methods could provide better insights for implementation in largescale scenarios. 4. Questions and Suggestions for Authors:  Clarifications on Algorithm Details: Providing a detailed pseudocode and stepbystep explanation of the algorithms could help readers understand the implementation intricacies better.  Impact on Realworld Applications: How do the proposed algorithms fare in scenarios with highdimensional data or large datasets Discussing the scalability and practical implications in such scenarios would be beneficial.  Comparison with Existing Techniques: Can the authors contrast their approach with existing methods in smoothed online learning Analyzing the strengths and weaknesses concerning other stateoftheart techniques could provide a clearer perspective on the novelty and significance of the proposed algorithms. 5. Limitations and Suggestions for Improvement:  Limitations Addressed: The authors have not explicitly addressed the potential negative societal impact or ethical considerations of their work. It would be constructive to include a brief discussion on the limitations and address any negative societal effects that might arise from the utilization of the proposed algorithms. Overall the paper presents a significant contribution to the field of smoothed sequential learning offering novel algorithms with statistically optimal regret bounds. Incorporating empirical evaluations and addressing algorithm complexity concerns can further enhance the practical applicability and robustness of the proposed methods.", "sFQJ0IOkHF": " Peer Review  Summary: The paper proposes DivBO a diversityaware framework based on Bayesian optimization to solve the Combined Algorithm Selection and Hyperparameters optimization (CASH) problem for ensemble learning. DivBO addresses the issue of neglecting diversity among base learners in recent AutoML systems by introducing a diversity surrogate and a framework guided by a weighted acquisition function. Empirical results on 15 datasets show that DivBO outperforms existing methods in terms of validation and test errors.  Strengths: 1. Novel Contribution: The paper introduces a novel approach to enhance ensemble performance in CASH problems by incorporating diversity into the optimization process. 2. Thorough Experiments: The empirical evaluation includes comparisons with multiple baselines analysis of diversity surrogate fitting and performance assessment on realworld datasets. 3. Clear and Detailed Description: The paper provides a comprehensive explanation of the proposed framework algorithms and experimental setup making it easy to understand the methodology.  Weaknesses: 1. Lack of Comparative Analysis: While the paper presents DivBO as outperforming existing methods there could be a more indepth comparison and analysis of the limitations of each approach to provide a stronger justification for the proposed framework. 2. Theoretical Explanation: While the empirical results are welldocumented a more detailed theoretical explanation of how diversity enhances ensemble performance could strengthen the theoretical foundation of DivBO.  Questions and Suggestions for Authors: 1. Evaluation Metrics: Could the paper elaborate on the choice of evaluation metrics used in the empirical evaluation and how they reflect the performance improvements brought by DivBO 2. Scalability: How scalable is the proposed DivBO framework when dealing with larger datasets or more complex optimization spaces Have you considered the implications of computational efficiency and resource requirements 3. Parameter Sensitivity: Have authors conducted sensitivity analysis on the key hyperparameters of DivBO It could be insightful to explore the impact of varying parameters on the performance and efficiency of the framework.  Limitations: The authors have adequately addressed the limitations such as increased inference latency with ensembles potential overfitting with small datasets and lack of direct support for certain algorithms. To further improve authors could provide suggestions for optimizing inference latency regularization techniques to mitigate overfitting and exploring alternative diversity functions for broader algorithm support. Overall the paper presents a significant contribution to the AutoML field by addressing the diversity issue in ensemble learning for CASH problems. With some enhancements in theoretical explanation comparative analysis and scalability considerations the paper can further strengthen its impact in the research community.", "nxw9_ny7_H": "Summary: The paper proposes a novel method named AugNet for learning data invariances directly from training data using differentiable data augmentation layers integrated within a neural network. The authors demonstrate the methods versatility and effectiveness through experiments on both controlled settings and realworld applications such as image recognition and sleep stage classification. AugNet is shown to outperform stateoftheart automatic data augmentation (ADA) methods in terms of speed and accuracy achieving almost perfect invariance irrespective of the models expressivity. Strengths: 1. Innovative Approach: AugNet introduces a novel method for learning data invariances directly from training data providing a new perspective on addressing the challenges of automatic data augmentation. 2. Versatility: The method is demonstrated to be applicable to a broad class of learning problems extending beyond computer vision tasks. 3. Empirical Evidence: The paper provides thorough experimental validation showcasing the effectiveness of AugNet compared to stateoftheart ADA methods both in controlled settings and realworld applications. 4. Performance: AugNet achieves comparable or superior performance compared to existing approaches in terms of both speed and accuracy showcasing its potential for practical applications. Weaknesses: 1. Computational Complexity: A large number of copies may be required for optimal performance increasing the computational complexity at test time. The tradeoff between performance and speed needs to be carefully considered. 2. Limitations of Augmentations: AugNet is limited to augmentations that can be relaxed into differentiable surrogates and requires a predefined set of possible augmentations. This may restrict its applicability in scenarios where such augmentations are not known a priori. Questions and Suggestions: 1. Can you provide more insights into the scalability of AugNet in terms of the number of samples required for training and the computational resources involved 2. How does AugNet handle situations where the dataset contains unknown invariances that are difficult to capture with predefined augmentations Are there strategies for adaptively learning these invariances 3. It would be beneficial to elaborate on the generalizability of AugNet across diverse datasets and domains beyond the ones studied in the paper. 4. Considering the potential societal impact of AI systems have you assessed the ethical considerations surrounding the use of learned data invariances in realworld applications Limitations: While AugNet demonstrates promising results in learning data invariances the method might face challenges related to computational complexity adaptability to unknown invariances and ethical considerations in practical deployment scenarios. To address these limitations authors should explore strategies for improving computational efficiency enhancing adaptability to unknown invariances and conducting thorough ethical assessments before realworld application.", "lkrnoLxX1Do": "Paper Summary: The paper proposes a selfsupervised image restoration method named SelfIR that leverages both shortexposure noisy and longexposure blurry images for image restoration. The key contributions include introducing a collaborative learning approach for deblurring and denoising utilizing the complementarity of noisy and blurry images and demonstrating superior performance compared to existing methods through extensive experiments on synthetic and realworld images. Strengths: 1. Innovative Approach: The collaborative learning approach of leveraging complementary information from noisy and blurry images for image restoration is novel and offers significant potential in addressing the challenges of lowlight photography. 2. Performance Improvement: The results of SelfIR show superior performance compared to stateoftheart selfsupervised denoising methods and even outperform supervised denoising and deblurring methods in both synthetic and realworld experiments. 3. Practical Implementation: The paper provides detailed implementation details dataset configurations and training protocols enhancing the reproducibility and generalizability of the proposed method. Weaknesses: 1. Limitation in Realworld Data Collection: While the method is evaluated on realworld images the dataset might be limited in size. Expanding the realworld dataset could provide a more comprehensive evaluation. 2. Complex Implementation: The collaborative learning framework and the methodology are technically intricate potentially limiting the accessibility of the method to researchers with less experience in image restoration. Questions  Suggestions: 1. Generalization to Other Use Cases: Have the authors considered how well the proposed SelfIR method would perform in scenarios beyond lowlight photography such as different types of image distortions 2. Exploration of Hyperparameters: Could the authors elaborate on the rationale behind selecting specific hyperparameter values and discuss any sensitivity analysis performed on these parameters 3. Comparison with Other Collaborative Learning Methods: How does the proposed SelfIR method compare with existing collaborative learning approaches in the field of image restoration Limitations  Constructive Suggestions: 1. Addressing Realworld Limitations: The authors should further discuss the limitations of the proposed method in handling more diverse realworld scenarios and suggest potential avenues for improvement in addressing these challenges.", "v6CqBssIwYw": " Summary: The paper introduces InstanceBased Uncertainty estimation for Gradientboosted regression trees (IBUG) to enable any GBRT pointprediction model to produce probabilistic predictions. IBUG leverages the affinity between training instances and uses a knearest neighbor approach to estimate uncertainty achieving competitive probabilistic and point prediction performance compared to existing methods.  Strengths: 1. Novel Contribution: Introduces a simple yet effective method (IBUG) to extend GBRT models for probabilistic predictions. 2. Empirical Validation: Demonstrates superior probabilistic performance on 22 benchmark datasets compared to NGBoost PGBM and CBU. 3. Flexibility: IBUG allows the use of different base GBRT models for improved performance and can model various types of output distributions. 4. Efficiency: Discusses strategies for improving runtime efficiency such as sampling trees and variance calibration techniques.  Weaknesses: 1. Limited Comparison: Limited comparison with only a few existing probabilistic methods like NGBoost PGBM and CBU more comparisons with other stateoftheart approaches could enhance the papers value. 2. Complexity: The method involves tuning several hyperparameters which may need further explanation for practical implementation. 3. Prediction Time: Acknowledges relatively slow prediction times for IBUG which might be a concern in realtime applications or large datasets.  Questions and Suggestions: 1. Could you elaborate on how the proposed IBUG method could be practically implemented especially regarding the tuning of hyperparameters and selecting the optimal k value 2. Have you considered evaluating IBUG against more recent deep learningbased probabilistic regression methods as they are becoming popular in the field 3. How does the method address any potential bias in the estimation of uncertainty using a knearest neighbor approach and how robust is it against outliers in the data  Limitations: The paper adequately addresses methodological limitations but could further discuss any potential societal impact especially in critical applications such as financial forecasting or healthcare. Suggestions include conducting additional sensitivity analyses to assess the robustness of the proposed approach and disclosing any data biases or fairness concerns. By addressing these aspects the paper can further solidify the robustness and societal relevance of the proposed IBUG method.", "xqYGGRt7kM": " 1) Summary: The paper provides a study on streaming algorithms for pure exploration in Stochastic Multi Armed Bandits (MABs) extending the work done by Assadi and Wang. They introduce strong lower bounds that demonstrate the challenges of achieving instancesensitive sample complexity in a single pass. Additionally they propose an algorithm that achieves the instancesensitive bound under specific assumptions.  2) Strengths and Weaknesses: Strengths:  Novelty: The paper addresses an important problem in the context of stochastic MABs providing valuable insights.  Theoretical Contributions: The rigorous analysis and lower bounds presented contribute significantly to the understanding of the samplespace tradeoffs in streaming MABs.  Algorithm Development: The proposed algorithm that achieves instancesensitive sample complexity is a noteworthy advancement.  Clear Presentation: The paper is wellstructured and presents complex concepts in a coherent manner. Weaknesses:  Practical Applicability: The application of the theoretical results to realworld scenarios could be further explored for practical relevance.  Assumptions: The stringent assumptions made in the algorithm may limit its generalizability and applicability.  Empirical Validation: While the theoretical results are strong empirical validation of the proposed algorithm would strengthen the paper.  3) Questions and Suggestions for Authors:  Validation of Algorithm: Have you considered conducting experiments to validate the performance of the proposed algorithm in practical scenarios  Generalizability: How can the algorithm be adapted to accommodate more realistic assumptions about the stream of arms  Impact: What are the potential practical implications of the lower bounds and algorithm proposed in this paper  Future Research: Have you identified any specific avenues for future research based on the findings of this study  4) Limitations: The authors should provide more detailed discussion on the potential negative societal impact of their work if any and propose suggestions for mitigating these impacts in the future.  Overall Assessment: The paper makes significant theoretical contributions to the field of streaming algorithms for pure exploration in MABs. The lower bounds presented are insightful and the proposed algorithm offers a notable advancement. Addressing the weaknesses highlighted and providing more thorough discussions on practical implications and societal impact would further enhance the paper. Further empirical validation and exploration of practical implications would strengthen the overall impact of the work.", "sr0289wAUa": " Summary: The paper introduces ASPiRe (Adaptive Skill Prior for RL) a novel approach that accelerates reinforcement learning by leveraging prior experience from diverse datasets. Unlike existing methods that learn a single skill prior ASPiRe learns a library of specialized skill priors and adapts them to new tasks using an Adaptive Weight Module (AWM). Experimental results demonstrate improved learning efficiency and task success rates over competitive baselines in challenging environments.  Strengths: 1. Innovative Approach: ASPiRe introduces a unique method to accelerate learning by utilizing multiple specialized skill priors and dynamically adjusting weights for policy guidance. 2. Thorough Literature Review: The paper provides a comprehensive review of related works highlighting the novelty of ASPiRe in leveraging composite skill priors for downstream task learning. 3. Experimental Evaluation: The experiments conducted in challenging environments demonstrate the effectiveness of ASPiRe in accelerating reinforcement learning.  Weaknesses: 1. Data Requirement Limitation: The paper relies on separated and labeled datasets for extracting primitive skill priors posing challenges for realworld applications with ambiguous or unlabelled data. 2. Complexity and Scalability: The algorithms complexity may hinder scalability especially when dealing with a large number of primitive skill priors potentially affecting computational efficiency.  Questions and Suggestions for Authors: 1. Data Labeling: Have you explored methods to automatically extract and label primitive skill priors from unlabeled datasets to enhance scalability and practicality 2. Scalability: How does ASPiRe perform with an increasing number of primitive skill priors Have you considered strategies to simplify the dataset labeling process for broader applicability 3. Comparative Analysis: Can you provide further insights on the tradeoffs between utilizing composite skill priors for policy guidance versus direct execution as demonstrated by the MCP approach  Limitations and Suggestions for Improvement: The authors may consider:  Addressing the potential limitations of requiring separated and labeled datasets by exploring unsupervised learning methods for automatic extraction of distinct primitive skill priors.  Providing a more indepth discussion on the scalability and computational efficiency of ASPiRe when handling a large number of diverse skill priors to enhance practicality and realworld applicability.", "tZUOiVGO6jN": " PeerReview: Summary: The paper proposes a new data loading method JOADER for efficiently training parallel DNNs with flexible constraints. The method includes a dependent sampling algorithm a domainspecific cache policy and a treebased data structure. JOADER is evaluated on ImageNet using ResNet models and shows significant improvements in training efficiency without sacrificing accuracy. Strengths:  The paper addresses a significant challenge in parallel DNN training by proposing a novel data loading method that shows substantial performance improvements.  The proposed dependent sampling algorithm is wellmotivated and effectively improves sampling locality while maintaining randomness.  The evaluation on ImageNet with ResNet models demonstrates the effectiveness of JOADER in improving training efficiency without compromising accuracy.  The proposed treebased data structure efficiently implements the algorithm and the domainspecific cache policy further optimizes data handling. Weaknesses:  Some sections of the paper are heavily technical which might make it challenging for readers without a deep background in the field to fully grasp the proposed methods.  While the experimental evaluation is comprehensive more comparative analysis with existing methods like CoorDL could provide stronger evidence of the superiority of JOADER.  The paper could benefit from a clearer explanation of the limitations of the proposed method and the scenarios where it might not be as effective. Questions and Suggestions: 1. How does JOADER compare to CoorDL in terms of performance improvement and efficiency in scenarios with varying training speeds and different starting moments 2. In scenarios with highly correlated workloads how does the proposed dependent sampling algorithm perform compared to traditional approaches 3. Could the authors provide more insights into the scalability of JOADER for larger datasets and more complex models Limitations and Suggestions: The authors could further discuss the potential negative societal impact of their work such as increased reliance on computational resources and the environmental implications associated with accelerated DNN training. Providing recommendations on optimizing resource usage or exploring energyefficient implementations could enhance the societal impact aspect of the paper. In conclusion the proposed method JOADER appears to be a promising solution for improving the efficiency of parallel DNN training. Further clarifications on the limitations more comparative analysis and addressing potential negative societal impacts would strengthen the paper.  This peerreview provides a thorough evaluation of the papers contributions strengths weaknesses as well as insightful questions and suggestions for the authors. It also highlights areas for improvement regarding limitations and societal impact.", "wN1CBFFx7JF": " Review of \"Theoretical Properties of Deep Neural Networks in Modeling Time Series Data\" 1. Summary: The paper investigates the theoretical properties of deep neural networks in modeling nonlinear time series data. By establishing nonasymptotic bounds for prediction error of feedforward neural networks with ReLU activation functions under mixingtype assumptions the authors bridge the gap in theoretical developments related to deep neural networks and time series data. The study provides insights into the impact of temporal dependence on neural network performance through numerical simulations and real data application. 2. Strengths:  Rigorous Theoretical Analysis: The paper provides a thorough theoretical analysis of deep neural networks in modeling time series data addressing temporal dependence.  Clear Presentation: The structure of the paper is wellorganized with a clear explanation of concepts model assumptions results and simulations.  Empirical Validation: Empirical evaluation through simulations and application to macroeconomic data enhances the credibility of the results.  Addressing a Gap: The research addresses the scarcity of theoretical results on deep neural networks in temporally dependent settings filling an important gap in the literature. 3. Weaknesses:  Complexity of Analysis: The theoretical analysis presented may be challenging to understand for nonspecialists due to its mathematical depth.  Model Assumptions: The assumptions made regarding independence mixing condition and boundedness may limit the applicability of the results in practical scenarios.  Limited Application Scenarios: The findings may not generalize well beyond the specific model structures considered limiting the broad practical implications. 4. Questions and Suggestions:  Clarifications:  Could the authors provide intuitive explanations or visual aids to help readers grasp the key concepts and results more easily  Can the authors discuss how the proposed model can be practically applied or extended to handle more diverse time series data with complex patterns 5. Limitations:  Addressing Limitations:  The authors could enhance the discussion on potential negative societal impacts of their work particularly concerning the reliance on and limitations of deep neural networks in modeling time series data.  Providing insights into potential biases or shortcomings of using neural networks in time series forecasting could further enrich the paper. Overall the paper makes a significant contribution to understanding the performance of deep neural networks in analyzing time series data although further clarity in presentation and broader discussions on limitations and societal implications would strengthen its impact.", "m67FNFdgLO9": " Peer Review  Summary: The paper introduces Dense Interspecies Face Embedding (DIFE) as a novel approach for understanding faces of various animal species by extracting common features among them including human faces. The method implements multiteacher knowledge distillation and StyleGAN2 for interspecies face embedding data augmentation for generating pseudopaired images and semantic matching loss for enhancing semantic correspondence. The study demonstrates the effectiveness of DIFE through interspecies keypoint transfer tasks and applications like image manipulation and face parsing using various datasets.  Strengths: 1. Innovative Approach: The paper presents a novel method DIFE for interspecies face embedding using multiteacher knowledge distillation StyleGAN2 and semantic matching loss providing significant contributions to the field. 2. Comprehensive Experiments: The method is thoroughly evaluated on various datasets for interspecies keypoint transfer tasks demonstrating superior performance compared to baselines and showcasing the practical applications like image manipulation and face parsing. 3. Clear Explanations: The paper is wellstructured with clear explanations of the method experimental setup and results making it easy to follow for the readers.  Weaknesses: 1. Lack of Theoretical Analysis: While the paper provides detailed experimental results a deeper theoretical analysis discussing the effectiveness of each component in the proposed method could enhance the understanding of the approach. 2. Limited Comparison: Although the method outperforms baselines in the experiments a more extensive comparison with other stateoftheart methods in interspecies face understanding could further validate the superiority of DIFE.  Questions and Suggestions for Authors: 1. How robust is the proposed method to variations in pose lighting conditions and occlusions in face images Have you conducted experiments to evaluate the performance under such challenging scenarios 2. Can you clarify how the semantic matching loss specifically improves the finegrained correspondence between different species faces Providing more insights into the mechanism of this loss could enhance the understanding of its effectiveness. 3. Considering the potential applications of DIFE in realworld scenarios have you explored any practical use cases beyond the demonstrated image manipulation and face parsing applications Further discussions on the broader implications of DIFE would be insightful for the readers. 4. As a future research direction have you considered extending the proposed method to handle more diverse animal species and addressing potential challenges that may arise due to larger shape differences or limited data availability for certain species  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To improve it would be beneficial to include a section discussing the ethical considerations of interspecies face understanding and propose ways to mitigate any negative implications that may arise. Overall the paper presents a valuable contribution to the field of interspecies face understanding with the proposed DIFE method. Addressing the weaknesses and questions raised here could further enhance the quality and impact of the research.", "m97Cdr9IOZJ": " Review of \"Invertible Neural Networks based on Coupling Flows for Approximating Diffeomorphisms\"  Summary: This paper introduces a novel approach utilizing Invertible Neural Networks (INNs) based on Coupling Flows (CFlows) for the approximation of diffeomorphisms. The main contributions include proving the Ckuniversality of affine coupling flows for compactly supported diffeomorphisms generalizing the results to parametric diffeomorphisms and proposing an effective neural surrogate model called ParaCFlows for contextual Bayesian optimization tasks showcasing its superior performance compared to other models.  Strengths: 1. Theoretical Contributions: The paper presents significant theoretical advancements proving the Ckuniversality and the approximation ability of INNs for diffeomorphisms. 2. Parametric Generalization: The extension to parametric diffeomorphisms and the development of ParaCFlows demonstrate a comprehensive approach to handling complex realworld problems. 3. Novel Model Architecture: The proposed ParaCFlows model leverages dimension augmentation and contextualization enhancing the expressiveness and performance in black box optimization tasks. 4. Thorough Experimental Analysis: The paper conducts comprehensive experiments to validate the proposed approaches providing clear illustrations and comparisons of different models in action sensitivity and contextual optimization tasks.  Weaknesses: 1. Clarity in Theoretical Proofs: The paper could benefit from providing more detailed explanations or visual aids in the theoretical proofs especially for readers less acquainted with complex mathematical concepts. 2. Code and Reproducibility: While mentioning that the code will be available the paper could provide more details on code availability documentation and reproducibility practices to facilitate further research and adoption of the proposed methods.  Questions and Suggestions for the Authors: 1. Generalization to Higher Dimensions: Have you explored the scalability and performance of ParaCFlows in higherdimensional or more complex scenarios beyond the presented benchmarks 2. Robustness Analysis: How robust are the proposed models to noise or perturbations in the data Have you conducted sensitivity analyses to assess the models performance under varying conditions 3. Comparison with StateoftheArt: Could you provide a more elaborate comparison with stateoftheart models or techniques in diffeomorphism approximation or surrogate modeling for optimization tasks to highlight the uniqueness and advantages of ParaCFlows 4. Potential RealWorld Applications: How do you envision the practical deployment of ParaCFlows in realworld applications particularly in complex optimization scenarios or dynamic systems with changing parameters over time  Limitations: The authors might consider enhancing the reproducibility of their work by providing detailed code repositories and documentation. Additionally addressing potential limitations in the experimental setups or generalizability of the proposed models could further strengthen the credibility of the study.", "mhe2C2VWwCW": " Peer Review: 1. Summary and Contributions: The paper introduces a general typology for predictive queries in neural autoregressive sequence models addressing complex probabilistic queries beyond onestep predictions. The authors develop new query estimation methods based on beam search importance sampling and hybrids demonstrating their efficacy on different datasets and language models. The contributions include reducing queries to elementary building blocks proposing a novel proposal distribution and evaluating the methods across various datasets. The experiments highlight the robustness and efficiency of the hybrid method compared to traditional search and sampling approaches. 2. Strengths:  Novelty: The paper addresses a gap in efficiently computing complex probabilistic queries in autoregressive neural sequence models adding to the existing literature.  Methodological Rigor: The authors provide a clear framework detailed description of methods and thorough evaluation across multiple datasets enhancing the reproducibility and generalizability of their findings.  Performance Evaluation: The comparisons between search sampling and hybrid methods are wellstructured demonstrating the effectiveness of the proposed hybrid approach in improving query estimation accuracy.  Experimental Design: The experiments are carefully designed considering different computation budgets and dataset characteristics providing a comprehensive analysis of the proposed methods performance. 3. Weaknesses:  Limited Scope: The studys focus on specific datasets and autoregressive neural models may limit the generalizability of the findings. A wider range of datasets and models could further validate the proposed methods.  Theoretical Analysis: While the practical implications of the methods are wellpresented a deeper theoretical analysis into the computational complexities and convergence properties of the proposed approaches could enhance the papers theoretical contributions. 4. Questions for the Authors:  Have you explored the impact of varying hyperparameters such as learning rates or model architectures on the performance of the query estimation methods  How do the proposed methods scale with larger vocabularies or more complex sequence structures and do they maintain efficiency and accuracy  Can you provide insights into the computational overhead of the hybrid approach compared to traditional search and sampling methods especially for long sequences and diverse queries 5. Suggestions for Improvement:  Generalization: Consider validating the proposed methods on a broader set of datasets and models to enhance the applicability and robustness of the findings.  Theoretical Insights: Include a more indepth discussion on theoretical properties and convergence guarantees of the query estimation methods to complement the empirical results.  Ethical Considerations: Discuss potential negative societal impacts in more detail proposing mitigation strategies to address ethical concerns arising from predictive query applications. 6. Limitations: The authors could enhance their work by providing more detailed insights into the computational scalability and convergence properties of the proposed methods ensuring a comprehensive understanding of their practical implications and theoretical foundations.", "rO6UExXrFzz": " Summary: The paper investigates the capacity control in composing classes of functions by introducing Gaussian noise. It focuses on bounding the composition classs capacity by controlling the uniform covering numbers and provides theoretical results definitions and novel techniques to achieve this. The paper presents results for multilayer sigmoid neural networks and empirical experiments on the MNIST dataset to demonstrate the impact of noise on generalization bounds.  Strengths: 1. Innovative Concept: The paper introduces a novel approach of controlling the capacity of composed function classes through Gaussian noise offering a unique perspective. 2. Technical Rigor: The paper demonstrates a high level of technical rigor by defining new notions of uniform covering numbers providing clear definitions stating theorems and conducting empirical studies. 3. Theoretical Contributions: The paper contributes theoretical results Lemmas Propositions Theorems and Corollaries enhancing the understanding of capacity control in composing function classes.  Weaknesses: 1. Limited Empirical Validation: While the paper provides preliminary empirical results on the MNIST dataset a more extensive and varied empirical analysis could further strengthen the practical relevance and credibility of the proposed approach. 2. Complexity of Exposition: The intricate technical details and formalism in the paper could potentially make it challenging for a broader audience to grasp the key contributions and implications easily. Simplifying the presentation could enhance accessibility.  Questions and Suggestions: 1. Clarity on Noise Impact: Could the paper elaborate more on the specific mechanisms through which adding Gaussian noise impacts the capacity control of the composed function classes 2. Applicability to Realworld Data: How does the effectiveness of noise addition in controlling capacity translate to realworld scenarios with diverse datasets and architectures beyond MNIST Could this methodology be extended to more complex datasets  Limitations: The authors could consider addressing the following limitations: 1. Generalization: The discussion on potential negative societal impact and ethical considerations related to the deployment of noiseaided capacity control in realworld applications could be included for a more holistic analysis.  Suggestions for Improvement: 1. Robust Empirical Validation: Conducting more comprehensive empirical studies on various datasets and network architectures to validate the efficacy and generalizability of the proposed approach. 2. Clarity in Presentation: Simplifying the complex technical aspects and providing more intuitive examples could enhance the accessibility and understanding of the paper for a wider audience.", "zGPeowwxWb": " Summary: The paper introduces a novel approach to model diffusion models as a deep equilibrium (DEQ) fixed point system specifically focusing on the denoising diffusion implicit model (DDIM). By formulating the generative process of DDIM as a DEQ the authors aim to improve single image sampling and model inversion tasks. They demonstrate the benefits of their approach on various datasets and highlight the advantages of parallel sampling faster convergence and efficient differentiation.  Strengths: 1. Novel Approach: The paper introduces a novel perspective on modeling diffusion models showcasing the potential benefits of DEQs for improving sampling processes. 2. Clear Contributions: The paper clearly outlines the contributions of the proposed approach emphasizing benefits in single image generation and model inversion tasks. 3. Empirical Validation: The authors provide empirical results demonstrating the efficacy of their method on datasets like CIFAR10 CelebA and LSUN Bedroom and Churches. 4. Thorough Explanation: The paper provides a detailed explanation of the formulation algorithms and experimental setup making it accessible to the reader.  Weaknesses: 1. Complexity: The approach and its implementation may be challenging for readers not familiar with DEQs and diffusion models potentially limiting the accessibility of the paper. 2. Evaluation: The paper could benefit from additional qualitative analyses of the generated images to complement the quantitative metrics provided. 3. Algorithm Details: While the paper introduces algorithms for inversion using DEQs more clarity on the practical implementation and potential challenges could enhance the understanding.  Questions and Suggestions for Authors: 1. How does the proposed DEQ formulation impact the interpretability of the generated images compared to traditional diffusion models 2. Can the authors provide insights into the computational resources required for implementing the DEQ formulation in practical scenarios 3. How scalable is the proposed approach when dealing with larger datasets or more complex generative tasks  Limitations: The authors have not explicitly addressed the potential negative societal impacts of their work. To improve they could include a section discussing any ethical considerations related to the use of generative models and provide guidelines for responsible deployment.  Overall the paper presents a substantial advancement in the field of generative models by proposing a novel perspective on modeling diffusion models using DEQs. With some refinement in the presentation and additional discussion on practical implications the work has the potential to make a significant contribution to the research community.", "z64kN1h1-rR": " Peer Review 1. Summary: The paper proposes a new offline reinforcement learning algorithm Model Standarddeviation Gradients (MSG) which leverages ensembles of Qfunctions for uncertainty estimation. It addresses a critical flaw in existing ensemblebased algorithms and demonstrates through theoretical analysis and empirical experiments on D4RL and RL Unplugged benchmarks that MSG outperforms stateoftheart methods especially in challenging domains like antmazes. The paper highlights the significance of using independently trained Qfunctions and the impact of ensemble size on performance. 2. Strengths:  Theoretical Grounding: The paper provides a strong theoretical foundation by identifying a critical flaw in shared pessimistic targets for Qfunction ensembles offering a clear rationale for introducing MSG.  Empirical Validation: Empirical results on D4RL and RL Unplugged benchmarks demonstrate the superiority of MSG over existing methods especially in challenging antmaze domains.  Clear Exposition: The paper is wellstructured logically organized and presents its findings in a coherent and understandable manner.  Relevance: Addressing the importance of uncertainty estimation in offline RL and proposing a novel algorithm that significantly outperforms stateoftheart methods in challenging benchmarks is highly relevant to the research community. 3. Weaknesses:  Limitation of Experiments: While the empirical results are promising more comprehensive experiments could strengthen the paper such as additional comparisons with a broader range of stateoftheart algorithms and evaluations across a larger set of offline RL benchmarks.  Computational Efficiency Discussion: The paper briefly touches on the computational bottlenecks of deep ensembles in largerscale settings but could elaborate more on potential solutions or future directions for improving efficiency. 4. Questions and Suggestions for Authors:  Could you provide insights into potential avenues for scaling up the MSG algorithm to handle larger neural network architectures more efficiently  Can you elaborate on the implications of MSGs performance for realworld applications and deployment in complex environments beyond benchmark tasks  How does the MSG algorithm handle nonstationarity and distributional shifts in the offline data and what considerations have been made for generalization to diverse datasets 5. Limitations:  The authors could further discuss the potential negative societal impacts of their work by considering ethical considerations related to reinforcement learning algorithms such as unintended biases or reinforcement of undesirable behaviors. Presenting recommendations for mitigating such impacts would enhance the societal value of the research. Overall this paper provides a valuable contribution to the field of offline reinforcement learning by proposing a novel algorithm that addresses critical flaws in existing methods and demonstrates significant performance improvements in challenging benchmarks. Further experiments discussions on scalability and considerations for societal impact can enhance the completeness and impact of the research.", "r__gfIasEdN": "Peer Review Summary: The paper proposes a novel approach called Generalized Autoregressive ParaphraseIdentification (GAP) to address biases introduced by negative examples in paraphrase identification. The authors advocate for training separate models for positive and negative pairs and automatically ensembling them based on a perplexitybased outofdistribution metric. The study includes insightful observations introduces GAPX for capturing interplay and reports strong empirical results compared to stateoftheart models. Strengths: 1. Innovative Approach: The proposal to train separate positive and negative models for paraphrase identification is novel and addresses the issue of biases effectively. 2. Empirical Validation: The authors provide strong empirical results to support their findings showcasing improved performance in various outofdistribution scenarios over existing models. 3. Clear Writing: The paper is wellstructured and clearly articulates the problem proposed solutions and experimental results. 4. Thorough Evaluation: The authors discuss the strengths and limitations of the proposed method in a detailed manner helping readers understand the implications of the approach. Weaknesses: 1. Interplay Consideration: While the authors address the importance of interplay between positive and negative pairs through GAPX more insights on the impact of neglecting this interplay in GAP could enhance the analysis. 2. Complexity: The methods implementation seems complex which might limit its practical adoption simplifications or practical guidelines could be beneficial. 3. Evaluation Metrics: While F1 and accuracy are used for evaluation other NLPspecific metrics like BLEU score could provide a comprehensive assessment of the models performance. Questions and Suggestions for Authors: 1. Could the authors elaborate on the potential challenges associated with the deployment of the proposed approach in realworld scenarios 2. Have variations in dataset characteristics been explored to understand how the proposed method adapts to diverse data distributions 3. How does the perplexitybased outofdistribution metric compare to established anomaly detection techniques in NLP and are there scenarios where it might be less effective 4. Considering potential computation complexities of training multiple models have the authors provided insights on the computational overhead of implementing GAP and GAPX 5. How does the proposed method handle cases where a pair does not clearly fall into either indistribution or outofdistribution categories Limitations: While the authors provide a thorough examination of the limitations and societal impacts of their work additional suggestions for improvement could include: 1. Incorporating explainability mechanisms to interpret model decisions. 2. Addressing potential biases in dataset selection and evaluation protocols to ensure fairness and robustness. 3. Extending experiments to verify the models generalizability across multiple languages or domainspecific challenges. Overall the paper presents a valuable advancement in the field of paraphrase identification offering a fresh perspective and reinforcing the importance of mitigating biases for improved model performance in diverse scenarios.", "vkhYWVtfcSQ": " PeerReview of \"EnergyBased MultiAgent Surprise Minimization\"  Summary: The paper introduces a new approach called EMIX which utilizes EnergyBased Models (EBMs) for surprise minimization in MultiAgent Reinforcement Learning (MARL). The authors propose an energybased framework to estimate and minimize surprise jointly across all agents in a multiagent system. The theoretical claims are empirically validated in a suite of MARL tasks demanding collaboration under fastpaced dynamics.  Strengths: 1. Innovative Approach: The integration of energybased surprise minimization in the MARL setting is a novel and promising direction in reinforcement learning research. 2. Theoretical Foundation: The paper provides a strong theoretical foundation for surprise minimization through an EnergyBased Model supporting the proposed EMIX framework. 3. Empirical Validation: The authors conduct comprehensive experiments on challenging MARL tasks demonstrating the effectiveness of EMIX in improving performance compared to existing methods.  Weaknesses: 1. Complexity: The proposed EMIX framework although promising may introduce additional complexity to the MARL algorithms potentially impacting scalability and efficiency. 2. Empirical Evaluation: While the empirical results are promising a more detailed analysis of the computational overhead and scalability of EMIX would provide a clearer understanding of its practical utility.  Questions and Suggestions: 1. Scalability: How does the EMIX framework scale with an increasing number of agents in a multiagent system Have you conducted experiments to evaluate the computational requirements and potential limitations in highdimensional action spaces 2. Parameter Sensitivity: Have you analyzed the sensitivity of the EMIX framework to hyperparameters such as the temperature \u03b2 and surprise estimation accuracies A sensitivity analysis could provide insights into the robustness of the approach. 3. Sample Efficiency: How does the EMIX approach impact sample efficiency in MARL tasks Consider discussing the tradeoffs between computational complexity and sample efficiency in the context of surprise minimization.  Limitations: The authors should consider addressing the practical implications of integrating the EMIX framework into realworld systems. Moreover a thorough investigation of computational efficiency and scalability aspects would enhance the practical applicability of the proposed approach.  Future Directions: 1. Efficient Sampling: Consider exploring advanced sampling methods to improve efficiency when sampling from the surprise distribution within the EMIX framework. 2. Scalability: Investigate the scalability of EBMs in highdimensional action spaces through the utilization of densitybased methods and generative models. Overall the paper presents an innovative approach with strong theoretical foundations and promising empirical results. Addressing the scalability and practical implications would further strengthen the contribution of the EMIX framework in the field of MARL.", "xILbvAsHEV": " Summary: The paper introduces a novel approach to learn ExtensiveForm Games (EFGs) by converting them to NormalForm Games (NFGs and applies the \\xce\\xa6Hedge algorithm for equilibria learning. It shows equivalence with Online Mirror Descent (OMD) introduces a new algorithm and achieves sharp regret bounds in EFGs.  Strengths: 1. Innovative Approach: The conversion of EFGs to NFGs and using \\xce\\xa6Hedge is a novel and conceptually appealing approach. 2. Theoretical Development: The paper provides rigorous theoretical analysis proofs and establishes important equivalences with existing algorithms. 3. Efficiency: The proposed algorithms are shown to run in polynomial time addressing the computational inefficiency typically associated with such conversions. 4. Regret Bounds: The derived regret bounds under full feedback and bandit feedback are significant contributions to the field.  Weaknesses: 1. Complexity: The mathematical formalism and derivations might be challenging for readers not familiar with the topic. 2. Clarity: The paper could benefit from improved clarity in explaining the algorithms and their implementations for easier understanding. 3. Practical Implementation: The practical applicability and scalability of the proposed algorithms are not discussed.  Questions and Suggestions: 1. Extension to General \\xce\\xa6 Sets: How would the proposed approach generalize to more complex \\xce\\xa6 sets beyond the behavioral modifications considered in the paper 2. Efficient \\xce\\xa6Hedge: Are there plans to develop more efficient implementations of \\xce\\xa6Hedge for broader applications given the polynomial time complexity 3. Practical Applications: How would the proposed algorithms be practically implemented and scaled for realworld EFG scenarios especially in largescale games 4. Comparison with Existing Algorithms: How does the performance of the proposed algorithms compare with other existing algorithms for learning equilibria in EFGs particularly in terms of convergence and computational efficiency 5. Experimental Validation: Is there any plan to include experimental validation to demonstrate the effectiveness of the proposed algorithms in realgame settings  Limitations: The paper does not explicitly address the practical implementation and realworld applications of the proposed algorithms which could limit its impact. It would be beneficial to include experimental validations in future work to validate the theoretical findings.", "vWUmBjin_-o": " Summary The paper introduces a novel approach for learning equivariant representations by using invariants of linear representations of transformation groups. It proposes a symmetry regularization objective to learn injective embeddings that are regularized towards simple linear actions using group invariants. The approach aims to address limitations of existing equivariant networks that assume linear transformations and require specialized implementations.  Assessment of Strengths and Weaknesses Strengths:  The paper tackles an important problem in deep learning by introducing a novel approach for learning equivariant representations.  The use of invariants of linear representations to induce a geometry on the embedding is a unique and promising idea.  The experimental results demonstrate the effectiveness of the proposed SymReg approach in various tasks showcasing qualitative and quantitative improvements over baseline methods. Weaknesses:  The paper may benefit from further elaboration on the scalability of the approach to larger and more complex datasets.  The explanation of the SymReg objective and its mathematical underpinnings could be made more accessible for readers less familiar with group theory and symmetry principles.  The limitations such as the inability to capture detailed rewardrelated information in certain RL tasks should be further discussed and addressed.  Questions and Suggestions for Authors 1. Clarification:  Could you provide more insights into how the proposed SymReg approach compares to existing methods in terms of computation complexity and scalability 2. Experiments:  Have specific scenarios been considered where the approach may face challenges or limitations such as dealing with highdimensional or diverse datasets 3. Interpretation:  How versatile is the proposed SymReg approach in handling different types of transformations and datasets beyond the ones presented in the experiments  Limitations The authors have not explicitly addressed the potential negative societal impact of their work. To improve the authors could include a discussion on ethical considerations potential biases in the datasets used or implications for marginalized communities. Additionally providing transparency about the limitations and biases in the approach used can foster trust and reliability in the research.  Conclusion The paper presents a promising approach for learning equivariant representations through symmetry regularization. The experimental results showcase the efficacy of the proposed method. Further clarifications on scalability limitations and societal implications could enhance the papers impact and relevance. Overall the paper contributes significantly to the field of representation learning and equivariance opening up new avenues for research and practical applications.", "oiztwzmM9l": " Summary: The paper introduces two novel algorithms STOBNTS and STOBNTSLinear that overcome the limitations of existing neural bandit algorithms by sidestepping the need to invert a large parameter matrix and supporting batch evaluations. The algorithms are proven to be asymptotically noregret under certain conditions for infinitewidth neural network surrogates. Both algorithms demonstrate competitive performance in practical AutoML reinforcement learning and image optimization tasks.  Strengths: 1. Novel Contribution: The introduction of STOBNTS and STOBNTSLinear offers a fresh perspective on improving neural bandit algorithms by using neural networks as surrogate models. 2. Theoretical Guarantees: The rigorous theoretical analysis provides confidence in the algorithms performance and applicability under specific conditions. 3. Empirical Effectiveness: The algorithms show promising results in a variety of realworld experiments demonstrating their practical utility. 4. Clear Explanations: The paper provides detailed explanations of the algorithms their theoretical foundations and experimental setups enhancing understanding and reproducibility.  Weaknesses: 1. Limited Discussion on Negative Impact: The paper lacks a comprehensive discussion on the potential negative societal impacts of promoting more adoption of deep learning methods and suggests further exploration in this area. 2. Limitation on Finitewidth NN Analysis: The theoretical analysis for finitewidth NNs only applies to STOBNTSLinear not STOBNTS which could benefit from additional exploration in future work.  Questions and Suggestions: 1. Clarifications Needed: Can the authors provide more insights into the computational complexity and scalability of the proposed algorithms 2. Comparison with Existing Methods: How do STOBNTS and STOBNTSLinear compare with other stateoftheart Bayesian optimization algorithms beyond the ones mentioned in the paper 3. Reproducibility: Will the authors provide the code implementation and datasets used in the experiments for reproducibility and further research  Limitations: The authors should provide a more explicit discussion on the potential negative societal impacts of promoting more adoption of deep learning methods and address limitations like the lack of analysis for finitewidth NNs in STOBNTS in future revisions. Overall the paper presents innovative algorithms with strong theoretical foundations and empirical performance contributing significantly to the field of blackbox optimization.", "xp5VOBxTxZ": " Summary: The paper explores the interplay between normalization layers weight decay and optimization dynamics in neural networks. It introduces a sharpnessreduction bias showing that gradient descent with weight decay encourages a decrease in the sharpness of the loss surface during training. The authors provide a theoretical analysis and empirical experiments to support their findings.  Strengths: 1. Novel Contribution: The paper offers a unique perspective on the relationship between normalization layers weight decay and optimization dynamics introducing the concept of sharpness reduction bias. 2. Theoretical Analysis: The mathematical analysis presented in the paper is thorough and provides a solid foundation for the proposed sharpnessreduction bias. 3. Experimental Validation: The empirical experiments conducted on matrix completion and CIFAR10 datasets provide evidence supporting the theoretical findings.  Weaknesses: 1. Complexity: The paper is highly technical making it challenging for readers without a strong background in optimization theory to grasp the concepts presented. 2. Limited Practical Implications: While the theoretical insights are valuable for understanding optimization dynamics the practical implications for optimizing neural networks may be limited. 3. Clarity: The paper could benefit from improved clarity in explaining the key concepts and their implications for practitioners in the field.  Questions and Suggestions for Authors: 1. Clarity for Practical Applications: Can the authors provide more insights into how the sharpnessreduction bias can be practically leveraged to enhance neural network training processes 2. Generalization to SGD: How applicable are the findings to Stochastic Gradient Descent (SGD) scenarios especially when considering gradient noise and nonspherical sharpness measures 3. Impact Beyond ScaleInvariant Nets: Have the authors considered exploring the implications of sharpness reduction bias for neural network architectures that are not scaleinvariant on all parameters  Limitations Feedback: The authors have not explicitly addressed potential negative societal impacts in the paper. It would be beneficial to include a section highlighting any ethical considerations or negative consequences that may arise from the proposed methodologies ensuring responsible conduct in AI research.  This detailed assessment should provide constructive feedback to help the authors enhance the clarity and applicability of their research.", "z9cpLkoSNNh": " Summary: The paper introduces a framework for continual learning through feature extraction focusing on resistance to catastrophic forgetting. It presents a novel gradientbased algorithm DPGrad that guarantees good performance and avoidance of catastrophic forgetting in the linear feature case. However it proves the impossibility of continual learning in the general case with nonlinear features.  Strengths: 1. Theoretical Contributions: The paper provides formal definitions and guarantees in the field of continual learning addressing an existing gap in the theoretical literature. 2. Innovative Algorithm: DPGrad is a novel algorithm that efficiently learns in the presence of linear features showing promising results. 3. Clear Exposition: The paper is wellstructured with detailed explanations provided for the proposed framework results and proofs.  Weaknesses: 1. Limited Practical Validation: While the algorithms theoretical soundness is wellestablished more empirical validation on realworld data sets could enhance the papers impact. 2. Lack of Comparison: The paper lacks comparisons with existing stateoftheart continual learning algorithms limiting the assessment of DPGrads comparative performance. 3. Impossibility Result Impact: The impossibility result in the nonlinear case while insightful might raise concerns about the practical applicability of the proposed framework in more complex scenarios.  Questions and Suggestions: 1. Could the authors provide insights on how the proposed algorithm DPGrad compares with existing stateoftheart continual learning algorithms in terms of performance and scalability 2. How could the proposed linear feature extraction framework be extended to accommodate nonlinear features in a practical setting even if without provable guarantees 3. In what scenarios or applications do the authors envision DPGrad being most beneficial considering the limitations highlighted in the paper  Limitations: The paper could benefit from an extended discussion on the practical implications and potential negative societal impacts of its findings to ensure a thorough examination of broader implications on AI systems and societal implications. Suggestions for improvement include integrating ethical and societal considerations within the research framework.", "kjR8GiwqCK": " Summary: The paper introduces the Indexed Minimum Empirical Divergence for Reinforcement Learning (IMEDRL) algorithm to minimize regret in a Markov Decision Problem (MDP) under the average reward criterion. IMEDRL aims to balance exploration and exploitation optimally deriving theoretical bounds that match regret lower bounds for ergodic MDPs. The algorithm exhibits competitive performance in numerical experiments.  Strengths: 1. Innovative Approach: Introducing IMEDRL for reinforcement learning in MDPs under the averagereward criterion is a novel contribution. 2. Theoretical Rigor: The paper establishes a regret upper bound that matches the known regret lower bound proving the asymptotic optimality of IMEDRL demonstrating strong theoretical foundations. 3. Clear Presentation: The paper is wellstructured provides a comprehensive overview of related work assumptions and algorithmic details. The mathematical derivations are articulate and support the claims effectively. 4. Numerical Competitiveness: Numerical experiments demonstrate the superiority of IMEDRL over existing strategies in ergodic environments showcasing the practical effectiveness of the proposed algorithm. 5. Practical Implementation: IMEDRL has a simplified implementation without requiring hyperparameters or confidence parameters reducing the complexity of the algorithm.  Weaknesses: 1. Limited Experimental Validation: While the numerical experiments are insightful extending the evaluation to more complex or realworld datasets would strengthen the practical implications of IMEDRL. 2. Complexity Concerns: The pertimestep complexity of IMEDRL is mentioned to scale as O(S2A) which may pose scalability challenges for larger state and action spaces. 3. Exploration in Communicating MDPs: The performance and optimality in communicating MDPs are not theoretically endorsed suggesting further investigation and clarification in this area.  Questions and Suggestions for Authors: 1. How does the algorithm handle dynamic environments or varying reward distributions over time Is it adaptable to nonstationary scenarios 2. Are there plans to extend the evaluation of IMEDRL to more diverse or challenging environments to assess its robustness and scalability 3. Could the authors elaborate on the potential impact of IMEDRL on realworld applications of reinforcement learning considering its computational efficiency and theoretical optimality 4. It would be beneficial to discuss how the proposed algorithm compares against stateoftheart reinforcement learning approaches beyond the specific strategies evaluated in the experiments.  Limitations: The paper should include a thorough consideration of the negative societal implications or ethical aspects related to the use of IMEDRL especially in complex environments or critical applications to ensure responsible deployment and minimize unintended consequences.", "oWx_9VJgyV7": " Peer Review:  1) Summary: The paper introduces a novel unsupervised paradigm named SNAKE for joint surface reconstruction and 3D keypoint detection based on implicit neural representations. The proposed method entangles shape reconstruction and keypoint saliency achieving stateoftheart performance in various benchmarks. The core idea is to predict shape indicators and keypoint saliency simultaneously using coordinatebased networks enabling superior performance in terms of semantic consistency repeatability and geometric registration.  2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The integration of shape reconstruction with keypoint detection sets SNAKE apart from existing methods and showcases novel advancements in the field.  Superior Performance: The extensive evaluations demonstrate SNAKEs stateoftheart performance across multiple datasets showcasing its effectiveness and versatility.  Semantic Consistency: The model excels in producing semantically consistent keypoints and achieving high repeatability even with sparse input point clouds.  Detailed Methodology: The paper thoroughly explains the network architecture loss functions and training strategies providing a clear understanding of how SNAKE operates. Weaknesses:  Computational Cost: The paper acknowledges that the optimization process during inference for keypoint extraction is computationally expensive potentially limiting realtime applications.  HumanAnnotation Dependency: The reliance on humanannotated data for certain evaluations could introduce biases or limitations in the models generalizability.  Assumption of Object Similarity: While the methods ability to encourage semantic consistency across instances of the same category is highlighted the paper should further discuss limitations in scenarios where objects vary significantly.  3) Questions and Suggestions for Authors:  Could the authors elaborate on the potential scalability issues given the computational cost of the optimization process during inference  How does SNAKE perform when tested on realworld data with inherent noise and occlusions considering the robustness of the method to downsampling and noise  Can the authors discuss the implications of using humanannotated data for semantic consistency evaluations and potential biases in the models generalization  How does SNAKE handle variations in object shapes within the same category especially in scenarios with significant shape differences  4) Limitations: While the paper does acknowledge the computational cost during inference as a limitation further exploration on optimizing the inference process for realtime applications would enhance the practicality of SNAKE. Additionally addressing potential biases introduced by humanannotated data could improve the models generalizability and mitigate negative societal impacts.  Overall the paper presents a novel and effective approach in the field of 3D keypoint detection with promising results and insights. Further exploration of scalability robustness and generalizability aspects could enhance the practical applicability and impact of SNAKE.", "scfOjwTtZ8S": "Peer Review 1. Summary and Contributions The paper introduces an automated reward shaping method called EAGER for reinforcement learning in longhorizon and sparse reward tasks. EAGER leverages question generation and question answering techniques to generate auxiliary objectives and intrinsic rewards without the need for expert intervention. Experimental results on the BabyAI platform demonstrate improved sample efficiency and robustness compared to existing methods like ELLA and RIDE. The paper makes substantial contributions by providing a framework that automatically generates relevant auxiliary objectives and intrinsic rewards based on language input showing improved performance in a challenging RL setting. 2. Strengths and Weaknesses Strengths:  Novel approach leveraging languageconditioned RL for automated reward shaping.  Clever integration of question generation and question answering for generating auxiliary objectives and intrinsic rewards.  Extensive experimental evaluation on the BabyAI platform showcasing improved sample efficiency and robustness compared to existing methods.  Clear description of the method algorithm and experimental setup. Weaknesses:  Insufficient discussion on the generalizability of the proposed method beyond the BabyAI platform.  Limited analysis on the negative impacts of using automated reward shaping in RL tasks.  Lack of comparison with more recent or stateoftheart methods in RL outside the BabyAI environment. 3. Questions and Suggestions for Authors  How does EAGER perform in more complex or realworld environments where language instructions are less structured or ambiguous  Have you considered the impact of automated reward shaping on task performance in scenarios where the agent may exploit the system to achieve rewards without learning genuinely useful behaviors  Can EAGER be easily adapted to different types of RL tasks beyond languageconditioned settings If so what modifications or adjustments might be necessary  Have you explored any interpretability aspects of the auxiliary objectives generated by EAGER and how they affect the agents decisionmaking process 4. Limitations The authors should provide more detailed discussions on the potential negative societal impact of automated reward shaping in RL particularly regarding issues like reward hacking unintended consequences and ethical considerations. Constructive suggestions for improvement include incorporating mechanisms for fair and unbiased training explicitly defining reward functions to prevent unintended behavior and promoting transparency and accountability in the decisionmaking process. Overall the paper presents a valuable contribution to the field by introducing an automated reward shaping method that showcases improved sample efficiency and robustness in RL tasks. By addressing the limitations highlighted above and ensuring a more comprehensive discussion on potential negative impacts the authors can enhance the impact and relevance of their work in the broader RL research community.", "lKFOwaYNQlb": "Summary: The paper introduces Distributed InfluenceAugmented Local Simulators (DIALS) as a solution to training large networked systems with many agents efficiently. It factorsize systems into subregions each with independent local simulators aided by approximate influence predictors. The paper focuses on parallelizing training scalability benefits and mitigating nonstationarity issues in simultaneous learning scenarios. Strengths: 1. Innovative Solution: The paper proposes a novel approach (DIALS) to address challenges in training large multiagent systems efficiently. 2. Thorough Experimental Evaluation: The empirical results demonstrate the superiority of DIALS in terms of convergence scalability and efficiency over traditional global simulators. 3. Theoretical Justification: The paper provides strong theoretical foundations for the proposed method discussing influence distributions and their impact on learning. 4. Detailed Explanations: The paper includes comprehensive details on the theoretical framework algorithm description and experimental setup aiding in understanding the proposed solution. Weaknesses: 1. Complexity of Presentation: The papers technical content is dense and could be challenging for readers unfamiliar with the specific domain of reinforcement learning and multiagent systems. 2. Evaluation Limitation: The experiments lack comparison with other stateoftheart methods limiting the assessment of DIALS performance against existing solutions. 3. Lack of Realworld Application: The paper does not provide concrete examples or demonstrations of applying DIALS to realworld scenarios thus limiting its practical impact assessment. Questions  Suggestions: 1. Scalability and Generalizability: How robust is the DIALS method when applied to diverse types of networked systems beyond the traffic and warehouse domains 2. Computational Efficiency: Have you considered any strategies to optimize the computational overhead associated with maintaining multiple AIPs in terms of memory and processing resources 3. Comparison with Existing Methods: Can you provide a more detailed comparison of DIALS with other prominent methods in the field of multiagent reinforcement learning showcasing its advantages more explicitly Limitations: The authors have addressed the limitations and negative societal impact of their work sufficiently particularly in highlighting the memory demands associated with DIALS. To enhance the papers impact and address potential societal implications the authors could explore strategies to optimize memory usage further and discuss potential applications in critical realworld scenarios.", "mTra5BIUyRV": " Summary: The paper presents a fairranking framework that addresses errors in sociallysalient attributes by introducing a probabilistic noise model. The framework guarantees fairness criteria while maximizing utility under the presence of attribute perturbations. The authors provide theoretical guarantees on fairness and utility show the neartightness of the fairness guarantee and propose a polynomialtime algorithm to solve the optimization program. Empirical evaluations on synthetic and realworld datasets demonstrate that the framework outperforms existing baselines in terms of fairness and fairnessutility tradeoff.  Strengths: 1. Innovative Approach: The paper introduces a novel framework that addresses the impact of errors in sociallysalient attributes on fairranking algorithms. 2. Theoretical Guarantees: The authors provide rigorous theoretical results proving fairness and utility guarantees of their framework. 3. Empirical Evaluation: The framework is empirically evaluated on synthetic and realworld datasets demonstrating its effectiveness compared to existing methods. 4. Clear Presentation: The paper is wellstructured with detailed explanations code availability and informative figures to support the findings.  Weaknesses: 1. Limited RealWorld Data Evaluation: While the framework is evaluated on realworld data the scope of datasets and scenarios considered for evaluation could be expanded to further validate the effectiveness of the proposed approach. 2. Societal Impact Discussion: The paper lacks a detailed discussion on the potential negative societal impacts that could arise from the proposed framework and how to mitigate them.  Questions and Suggestions for Authors: 1. Can the Framework Handle Adversarial Errors Have the authors considered the robustness of their framework against adversarial errors in sociallysalient attributes 2. Further Applications: Have the authors explored potential applications of their framework beyond ranking problems such as in recommendation systems or hiring processes 3. Model Robustness: How does the framework perform when the assumptions of random and independent errors in attributes are violated Are there plans to extend the framework to handle correlated errors 4. Fairness Metrics: How sensitive is the framework to the choice of fairness metrics Have the authors considered other fairness metrics and their implications 5. Future Work: What are the next steps in the research agenda Are there plans to improve the framework or extend its applicability to different domains  Limitations: The authors have not explicitly discussed the potential negative societal impacts of their work. To enhance the papers completeness it is recommended to include a section addressing these limitations and providing constructive suggestions to mitigate any adverse consequences.", "snUOkDdJypm": "1) Summary: The paper addresses the question of lastiterate convergence rate in multiplayer games for the extragradient algorithm and the optimistic gradient algorithm. It focuses on smooth monotone games with closed convex action sets extending previous research that only considered unconstrained settings. The key contribution is showing that both algorithms with constant step sizes achieve lastiterate convergence rates of O(1\\xe2\\x88\\x9aT) to a Nash equilibrium in terms of the gap function. 2) Assessment of Strengths and Weaknesses: Strengths:  Addresses an important open question in a significant area of research.  Tight convergence rates are established for both algorithms in constrained settings matching lower bounds.  Introduces a new concept the tangent residual which plays a crucial role in the analysis.  Contributions expand the understanding of algorithms convergence behavior in multiplayer games. Weaknesses:  The paper is highly technical and may be challenging for readers not wellversed in the specific area of research.  Limited discussion on the practical implications or realworld applications of the findings.  Could benefit from more illustrations or examples to help clarify complex concepts for a broader audience. 3) Questions and Suggestions:  Could the authors provide more insights on the practical implications of the discovered convergence rates How might these findings impact realworld applications of multiplayer game algorithms  It might be beneficial to include some illustrative examples or use cases to enhance the accessibility of the paper to a wider audience.  How do the proposed convergence rates compare with existing algorithms or methods in terms of computational efficiency and convergence speed 4) Limitations and Suggestions for Improvement: The authors have not thoroughly discussed the limitations of their work or potential negative societal impacts. To address this they could include a dedicated section addressing these aspects highlighting any ethical considerations or potential biases in the research and proposing ways to mitigate negative impacts. Additionally suggestions for improvement could involve providing more relatable realworld examples and practical scenarios to enhance the applicability and accessibility of the research findings.", "zqQKGaNI4lp": "Summary: The paper introduces a novel ensemble classification method using the manifoldHilbert kernel for data distributed on a Riemannian manifold. It proves weak consistency and interpolation properties for kernel smoothing regression for this method. By defining weighted random partition kernels the paper establishes consistency for ensemble classification methods on the sphere. The contributions are significant in offering a consistentinterpolating ensemble method for distributions in arbitrary dimensions particularly on manifolds. Strengths: 1. Innovative Approach: The paper introduces a novel ensemble classification method utilizing the manifoldHilbert kernel addressing a gap in theoretical understanding for ensemble methods. 2. Theoretical Rigor: The theoretical foundations are sound with rigorous proofs provided for key propositions and theorems. 3. Broad Impact: The methods potential for diverse applications on Riemannian manifolds and spheres showcases the broad applicability and significance of the research. Weaknesses: 1. Limited Empirical Validation: Lack of empirical validation or realworld applications to demonstrate the effectiveness or practical implications of the proposed method. 2. Complexity: The technical nature of the results and proofs might limit accessibility and understanding for a broader audience potentially restricting the dissemination of the findings. 3. Rate of Convergence: The absence of a discussion on the rate of convergence for the ensemble methods poses a limitation given the importance of understanding the speed of learning in machine learning models. Questions and Suggestions: 1. Empirical Evaluation: Will the authors consider conducting empirical studies to validate the proposed ensemble methods performance on realworld datasets 2. Comparative Analysis: How does the proposed method compare with existing ensemble techniques in terms of performance metrics like accuracy scalability and robustness 3. Interpretability: Can the authors provide insights into how the ensemble method could enhance interpretability compared to other models especially in the context of data distributed on manifolds 4. Practical Implementation: Have the authors considered the computational complexity and scalability of the ensemble method for large datasets in practical applications Limitations: The authors could improve by addressing the following limitations: 1. Rate of Convergence: Consider providing insights into the rate of convergence of the proposed ensemble method which is essential for understanding the speed of learning in practice.", "ofwkaIWFqqv": " PeerReview Summary: This paper introduces the GoaldRiven Actorcritic retroSynthetic Planning (GRASP) framework focusing on improving the sequential decisionmaking policy in retrosynthetic planning. The framework outperforms existing stateoftheart approaches in terms of search efficiency and quality especially in goaldriven retrosynthetic planning. Strengths: 1. Innovative Framework: The proposal of GRASP incorporating goaldriven planning in retrosynthetic pathway design is a significant innovation in the field of chemical synthesis planning. 2. Performance Improvement: The experimental results demonstrate that GRASP outperforms existing approaches achieving higher search efficiency and quality. 3. Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed framework with comparisons to multiple baseline algorithms across various metrics enhancing the credibility of the results. 4. RealWorld Application: The deployment of GRASP in an industry setting to accelerate synthetic pathway planning towards novel compounds adds practical value to the research. Weaknesses: 1. Limited Detail on Training Procedure: The paper lacks indepth explanation of the training procedure of the GRASP framework particularly on how the actorcritic network learns from experiences and the specifics of the MonteCarlo Tree Search integration. 2. Incomplete Experimental Evaluation: While the paper presents promising results more detailed analysis on the scalability of GRASP especially with larger and more diverse datasets could further validate the frameworks effectiveness. 3. Transparency in User Study: The description of the user study could be more transparent by providing information on the number of participants the expertise level of chemists involved and potential biases in the study design. Questions and Suggestions for Authors: 1. Could you elaborate on the specific training process of the GRASP framework detailing how the actorcritic network learns from both positive and negative experiences This information can enhance the reproducibility and understanding of the methodology. 2. Have you considered the impact of dataset size and diversity on the performance of GRASP Extending the experiments to larger datasets with more chemical diversity could showcase the frameworks robustness. 3. Could the authors provide more transparency in the user study methodology including details on participant selection expertise levels and potential biases in the study design Limitations: The paper adequately addresses the limitations and potential negative societal impact of the work. However to further enhance the ethical considerations the authors could consider conducting a thorough analysis of any unintended consequences or biases that may arise from the deployment of GRASP in industrial settings. Additionally providing guidelines on responsible use and potential risks associated with automated chemical synthesis planning could be beneficial. Overall the paper presents a valuable contribution to the field of retrosynthetic planning with the GRASP framework showing promising results. Clearer descriptions of the training process and further exploration of scalability and user study transparency would enhance the impact and understanding of the proposed approach.", "v_0F4IZJZw": " Peer Review: 1. Summary: The paper systematically explores domainadaptive training to reduce the toxicity of language models across dimensions like training corpus model size and parameter efficiency. It introduces SelfGeneration Enabled domainAdaptive Training (SGEAT) that outperforms existing baselines identifies tradeoffs between detoxification and LM quality and evaluates various detoxification methods through automatic and human evaluations. The study covers a wide range of model sizes and training paradigms providing valuable insights for developing effective and parameterefficient detoxification techniques. 2. Strengths:  Comprehensive Approach: The paper covers a wide spectrum of parameters including training corpus model size and parameterefficient training methods showcasing a thorough analysis.  Novel Contribution: The SelfGeneration Enabled domainAdaptive Training (SGEAT) introduces a unique approach demonstrating better data efficiency for detoxification.  Analytical Rigor: The evaluation metrics including automatic and human evaluation as well as the consideration of model quality in addition to toxicity provide a robust analysis of the proposed methods.  Clear Presentation: The papers structure is wellorganized with a clear delineation of contributions and findings making it easy to follow the methodology and results. 3. Weaknesses:  Limited Bias Discussion: While the paper briefly touches on bias concerns a more thorough examination of potential biases within the proposed detoxification methods could strengthen the studys ethical considerations.  Scalability Consideration: Although the study covers a wide range of model sizes discussing the scalability of the proposed methods to even larger models could enhance the practical relevance of the work. 4. Questions and Suggestions for Authors:  Scalability Testing: Have you considered the scalability of your proposed methods to even larger language models beyond the studied 530B parameter size  Ethical Implications: How do you plan to address potential societal biases within detoxification methods particularly considering the impact on marginalized groups  Integration of Insights: How can the findings of this study be integrated into realworld applications to ensure safe and ethical deployment of largescale language models 5. Limitations: The authors should further address potential societal biases within detoxification methods improve the robustness and fairness of the hate speech detector used and consider ways to mitigate biases within pretrained models for more effective detoxification. Overall the paper provides valuable insights into reducing the toxicity of language models through domainadaptive training but further discussion on bias mitigation and scalability would enhance the studys impact and applicability.", "y--ZUTfbNB": " Peer Review  1) Summary: The paper presents algorithmic advancements in designing efficient algorithms specifically tailored for distance matrices focusing on fundamental linear algebraic primitives. The contributions include upper and lower bounds for matrixvector queries faster matrix multiplication for distance matrices and algorithms for constructing distance matrices efficiently. These advancements have implications for various downstream applications including lowrank approximation and eigenvalue computations.  2) Strengths and Weaknesses: Strengths:  The paper addresses fundamental algorithmic challenges related to distance matrices offering significant contributions in terms of upper and lower bounds for matrixvector queries.  The theoretical analysis is thorough and wellstructured providing clear explanations of algorithms and their implications.  The work is grounded in a solid theoretical framework showcasing a good understanding of the underlying geometric structures of distance matrices.  The empirical evaluation provides practical insights affirming the theoretical advancements. Weaknesses:  The paper could benefit from more detailed comparisons with prior works particularly in terms of discussing the novelty and impact of the proposed algorithms.  The exposition of the algorithms could be further improved for better clarity and accessibility to a broader audience.  While the empirical evaluation is valuable more diverse datasets could enhance the generalizability of the findings.  3) Questions and Suggestions:  Could the authors elaborate on the specific downstream applications where the proposed algorithms are expected to have the most significant impact  How do the theoretical lower bounds impact the practical utility of the algorithms especially in scenarios with large datasets  It would be beneficial to discuss potential tradeoffs between efficiency and accuracy in the context of distance matrix computations.  Consider exploring the scalability of the algorithms to even larger datasets and addressing potential bottlenecks in terms of computational complexity.  4) Limitations: The authors could strengthen the discussion around the potential negative societal impacts of the work by considering issues related to algorithmic bias fairness and ethical considerations in algorithm design particularly when deploying algorithms in realworld applications. Overall the paper presents a valuable contribution to the algorithmic landscape related to distance matrices offering novel insights and advancements that could have broad implications for various fields. Addressing the suggested points could further enhance the impact and relevance of the research.", "wjqr6aqkLUV": "Peer Review: Summary: The paper explores the phenomenon of weight condensation in ReLU neural networks with small initialization. It investigates how the condensation dynamics implicitly regularize the network towards a smaller effective size during training. The study illustrates the formation of condensation in multilayer fully connected NNs where the maximal number of condensed orientations in the initial training stage is related to the multiplicity of the activation function. The theoretical analysis and experiments confirm that the maximal number of condensed orientations is twice the multiplicity of the activation function used. Strengths: 1. Original Contribution: The paper addresses an intriguing aspect of neural network training related to weight condensation offering insights into how neural networks are implicitly regularized. 2. Theoretical Analysis: The theoretical analysis conducted in the paper is rigorous and insightful providing a solid foundation for understanding the phenomenon. 3. Experiment Design: The experiments conducted are extensive and cover different scenarios enhancing the robustness of the findings. 4. Relevance: The study addresses an important question regarding network generalization and complexity making it relevant to the field. Weaknesses: 1. Complexity: The paper especially in the theoretical sections may be challenging for readers without a deep background in neural networks and mathematics. 2. Clarity: Some sections particularly regarding the experimental setup and methodologies could be explained more clearly for better understanding by a wider audience. 3. Missing Comparison: While the paper extensively discusses the impact of the activation functions multiplicity on condensation a comparative analysis with existing studies or methods could enhance the papers value. 4. Societal Impact: The paper lacks a discussion on the limitations and potential societal implications of the findings which could be crucial for broader applicability. Questions and Suggestions for Authors: 1. Can you further elaborate on how the results of this study complement or contrast with existing theories or empirical observations in the field of neural networks 2. How robust are the findings regarding weight condensation across different network architectures or datasets Should this be a focus for future research 3. Could the paper benefit from a more detailed discussion on practical implications for optimizing network training or improving generalization 4. Considering the mathematical complexity of the theoretical analysis what strategies would you recommend for simplifying the presentation of such intricate concepts for a broader readership Limitations: The authors have not adequately addressed the potential negative societal impacts or broader implications of their work. To enhance the societal relevance of the study it is suggested to include a brief discussion on how these findings could impact the field of machine learning or related applications and any ethical considerations that may arise. In conclusion the paper presents a significant contribution to the understanding of weight condensation in neural networks. Addressing the highlighted weaknesses and the suggestions provided could further improve the clarity and impact of the study.", "sOVNpUEgKMp": " Summary: The paper proposes an \"Adaptive MultiDistribution Knowledge Distillation\" (AMDKD) scheme to address the crossdistribution generalization issues of deep models for Vehicle Routing Problems (VRPs). The scheme aims to enhance generalization by distilling knowledge from multiple teachers trained on different exemplar distributions into a lightweight student model. Extensive experiments show that AMDKD outperforms baseline methods in terms of generalization on both indistribution and outofdistribution instances.  Strengths: 1. Innovative Contribution: Introduces a novel approach AMDKD to enhance crossdistribution generalization in VRPs using knowledge distillation from multiple teachers. 2. Experimental Results: Extensive experiments demonstrate significant improvements in generalization over baseline methods on various distributions. 3. Adaptive Strategy: The adaptive strategy aids the student model in focusing on difficult distributions enhancing learning effectiveness. 4. Efficiency: The scheme offers competitive results while consuming fewer computational resources for inference. 5. Detailed Methodology: Detailed explanation of the methodology ablation studies and comparisons with existing methods provide a comprehensive evaluation.  Weaknesses: 1. Limited Benchmarks: The comparison with existing generalization methods could be more detailed with additional benchmarks or datasets. 2. Additional Experiments: The paper could benefit from experiments on different problem sizes or variants to strengthen the evaluation. 3. Complexity vs. Performance: The tradeoff between solution quality and computational cost needs to be carefully considered and addressed.  Questions and Suggestions: 1. Question: How does the proposed AMDKD scheme perform when applied to larger or more complex VRP instances  Suggestion: Consider extending experiments to larger problem sizes to evaluate the scalability of AMDKD. 2. Question: Could the adaptive strategy of AMDKD be further finetuned or optimized for specific types of distributions  Suggestion: Discuss potential enhancements or refinements to the adaptive strategy based on the characteristics of different distributions. 3. Question: How does the performance of AMDKD compare with other stateoftheart methods on realworld or industryspecific VRP instances  Suggestion: Explore the generalization performance of AMDKD on realworld datasets to evaluate its practical applicability.  Limitations: The paper could provide more insights into the potential negative societal impacts of the proposed approach and discuss strategies to mitigate them for ethical considerations. Suggestions for improvement include incorporating an ethical impact assessment for future work.  Conclusion: The paper presents a novel AMDKD scheme for enhancing crossdistribution generalization in VRPs. With compelling experimental results and a detailed methodology the paper makes a valuable contribution to the field. Further enhancements could include exploring the performance of AMDKD on larger problem sizes and addressing the tradeoff between solution quality and computational cost effectively.", "xLnfzQYSIue": " Summary: The paper introduces a general analysis of Top Two algorithms for best arm identification in the context of bounded distributions. It proposes new variants using the EB leader and the TCI challenger and proves their asymptotic \u03b2optimality. The study includes theoretical analysis and empirical performance evaluation on realworld data and Bernoulli instances.  Strengths: 1. Novel Contribution: The paper presents a novel analysis of Top Two algorithms for bounded distributions providing theoretical guarantees for their asymptotic \u03b2optimality. 2. Comprehensive Analysis: The study includes a thorough theoretical analysis derivation of stopping rules and assessment of various sampling rules showing a deep understanding of the algorithms dynamics. 3. Empirical Validation: The experimental results on realworld data and Bernoulli instances support the theoretical findings demonstrating the efficacy of the proposed algorithms.  Weaknesses: 1. Limited Experiments: The experimental evaluation is limited in scope with only a few scenarios explored. More extensive experimentation across diverse datasets and problem settings could provide a broader perspective on the algorithms performance. 2. Complexity Concerns: The computational cost of some challenger variants (e.g. RS challenger) might hinder practical implementation potentially limiting the scalability of the algorithms.  Questions and Suggestions: 1. Theoretical Guarantees: Have you considered exploring nonasymptotic guarantees or bounds on sample complexity to better reflect the algorithms empirical performance 2. Empirical Robustness: How do the proposed algorithms perform under scenarios with varying levels of explorationexploitation tradeoff Robustness analysis under different conditions could enhance the practical applicability of the algorithms. 3. Scalability Concerns: Given the computational complexity of certain challenger variants have you investigated methods to mitigate these complexities while maintaining algorithm effectiveness 4. Generalization: How generalizable are the findings to other distribution classes beyond the scope of bounded distributions Exploring the performance in more diverse settings would strengthen the practical utility of the algorithms.  Limitations: The authors address the limitations associated with the computational complexity of certain variants and the potential challenge of scaling the algorithms to larger problem instances. To improve the authors could consider optimizing the computational efficiency further and ensuring scalability to broader application domains.", "sL7XH6-V21e": "Summary: The paper analyzes the influence of the prediction concatenation operation (\"PreConc\") on the consistency of deep forest algorithm compared to random forest. It establishes the consistency analysis based on a simplified version proving that deep forest with deeper layers can be more powerful than shallower layers in certain conditions. The paper presents theoretical advancements in understanding the value of depth over width in the deep forest algorithm. Strengths: 1. The paper provides a comprehensive theoretical analysis of deep forest showcasing the advantages of depth over width in term of consistency and convergence rates. 2. The experiments conducted effectively support the theoretical findings and provide insights into the practical performance of the algorithm. 3. The paper fills a gap in the existing literature by focusing on the theoretical properties and advantages of deep forest compared to random forest. 4. The proposition on the priority of the new feature in the splitting process enhances the understanding of how deep forest functions in practice. Weaknesses: 1. The paper could benefit from a more detailed explanation of the assumptions made and how they might impact the practical application of the algorithm. 2. The complexity of the theoretical analysis might make it less accessible to readers who are not deeply familiar with the underlying concepts and algorithms. 3. More clarity could be provided on how the results of the study could be practically applied in realworld scenarios. 4. The limitations of the study in terms of generalization to different datasets or applications could be more explicitly addressed. Questions and Suggestions for the Authors: 1. Have you considered the potential implications of the assumptions made in the theoretical analysis on the realworld applicability of the algorithm How might these assumptions affect the performance in practical scenarios 2. Could you provide more insights into how the theoretical findings can be practically implemented or optimized for different application domains 3. How do you plan to address the limitations identified in the study in future research Are there plans to conduct more extensive empirical evaluations across a wider range of datasets 4. What are the next steps in your research to further enhance the understanding and performance of the deep forest algorithm in realworld applications 5. How do you plan to disseminate your findings to the machine learning community and potential users of deep forest algorithms", "u4dXcUEsN7B": " Summary: The paper introduces a novel algorithm called MetaSP for Continual Learning (CL) to address the StabilityPlasticity dilemma by exploring example influences on past tasks (Stability S) and new tasks (Plasticity P). By simulating the Influence Function (IF) MetaSP computes example influences and fuses them towards SP Pareto optimality ultimately enhancing model update and rehearsal storage. Experimental results demonstrate the superiority of MetaSP over existing stateoftheart methods in task and classincremental CL benchmarks.  Strengths: 1. Innovative Approach: Introducing example influence exploration and fusion to enhance CL performance is a novel and valuable contribution. 2. Effective Algorithm: The proposed MetaSP algorithm efficiently computes example influence without the need for computationally intensive Hessian inversions demonstrating substantial performance improvements. 3. Thorough Evaluation: Extensive experiments on multiple datasets showcase the superior performance of MetaSP validating its effectiveness in improving Stability and Plasticity in CL tasks.  Weaknesses: 1. Complexity Concerns: Despite the effectiveness of the proposed algorithm the additional computational overhead may limit scalability especially in realtime or resourceconstrained scenarios. 2. Limitation in Small Memory Sizes: The paper mentions a limitation in extremely small memory sizes which could affect the algorithms performance suggesting potential constraints in certain practical applications.  Questions and Suggestions: 1. Validation of Rehearsal Selection: Can the authors provide more insights into the rationale behind the rehearsal selection strategy proposed and how it influences CL performance 2. Privacy Considerations: How does the example influence method impact data privacy especially when selecting examples for rehearsal storage 3. Generalizability: How applicable is the proposed MetaSP algorithm to different CL scenarios and datasets beyond the ones evaluated in the experiments  Limitations: While the paper addresses the limitations of small memory sizes it is essential to further explore the impact of the method on data privacy and address concerns regarding the need for additional storage for rehearsal data.  Suggestions for Improvement: 1. Efficiency Enhancement: Explore ways to optimize the algorithm for faster computational performance without compromising its effectiveness. 2. Robustness Testing: Conduct testing on a wider range of CL scenarios to assess the generalizability and robustness of the proposed MetaSP algorithm. Overall the paper presents a compelling approach to enhancing CL performance through example influence exploration and fusion showcasing promising results that warrant further investigation and potential optimizations for practical implementations.", "rBCvMG-JsPd": "Summary: The paper compares fewshot incontext learning (ICL) and parameterefficient finetuning (PEFT) methodologies for enabling pretrained language models to perform new tasks with limited data. The authors introduce a new PEFT method named (IA)3 that rescales activations leading to improved performance with minimal new parameters. They also propose the TFew recipe based on the T0 model achieving superhuman performance on the RAFT benchmark while requiring lower computational costs. Strengths: 1. Novel PEFT Method: The (IA)3 method introduced for PEFT is innovative and shows impressive performance gains with minimal additional parameters. 2. Clear Methodology and Results: The paper clearly explains the proposed methods presents thorough experimental results and demonstrates superiority over existing approaches. 3. RealWorld Application: Evaluation on the RAFT benchmark showcases the practical applicability and superior performance of TFew in realworld fewshot learning tasks. Weaknesses: 1. Impact of Additional Parameters: While (IA)3 shows improved performance the paper could further discuss the potential implications of introducing additional parameters on model efficiency and complexity. 2. Lack of Comparative Analysis: While the paper focuses on comparing ICL and PEFT there could be a deeper analysis of other existing PEFT methods for a comprehensive comparison. 3. Potential Biases: The paper could discuss any biases in the experiments data processing or methodology to ensure the robustness of the results. Questions and Suggestions: 1. Generalizability: How does the proposed TFew recipe perform on a broader set of tasks beyond classification 2. Scalability: Have the authors considered the scalability of the (IA)3 method to even larger models or diverse types of tasks 3. Adaptation: How does the TFew recipe adapt to complex tasks with different linguistic structures or requirements Limitations: The authors should further discuss and address any negative societal impact associated with promoting superhuman performance in natural language processing tasks. Suggestions for improvement would involve conducting ethical assessments and incorporating considerations for responsible use of advanced AI technologies.", "p6hArCtwLAU": " Summary: The paper introduces TreeMoCo a selfsupervised learning framework that utilizes a TreeLSTM network to encode neuron morphology without labels for classification. The work demonstrates the effectiveness of TreeMoCo in classifying major brain celltypes and identifying subtypes using 2403 highquality 3D neuron reconstructions from mouse brains. The authors compare TreeMoCo with existing methods and highlight its potential to advance quantitative neuron morphology analysis.  Strengths: 1. Innovative Approach: TreeMoCo presents a novel method for learning neuron morphology representation with contrastive learning addressing the limitation of manual annotations in previous approaches. 2. Comprehensive Evaluation: Extensive experiments on 2403 neuron reconstructions showcase the effectiveness of TreeMoCo in celltype classification and subtyping without requiring labeled data. 3. Visual and Quantitative Analyses: The paper includes a thorough visual inspection and quantitative evaluation demonstrating the robustness and interpretability of the learned representations. 4. Generalizability: The potential of applying TreeMoCo to diverse types of tree graph data beyond neuron morphology is discussed indicating broader applicability.  Weaknesses: 1. Global Sensitivity: The sensitivity of TreeLSTM to global changes may impact performance on incomplete neuron reconstructions potentially limiting its robustness in realworld scenarios. 2. Performance Oscillation: The observed fluctuations in performance on small datasets after long training may indicate challenges in model stability and convergence that need to be addressed.  Questions and Suggestions: 1. Data Augmentations: How were the specific augmentations chosen for TreeMoCo and can additional or refined augmentation strategies improve model performance and robustness especially on incomplete reconstructions 2. Global Sensitivity Mitigation: Are there strategies to mitigate the global sensitivity of TreeLSTM perhaps by incorporating attention mechanisms or hierarchical encoding to better handle incomplete or noisy reconstructions 3. Model Stability: Considering the observed performance oscillation after long training epochs have the authors explored early stopping mechanisms or regularization techniques to improve stability and generalization on small datasets 4. Impact on Realworld Applications: How might the findings of TreeMoCo impact realworld applications in neuroscience research particularly in drug discovery or neurological disorder treatments and are there ongoing experimental validations planned to support these applications  Limitations: The authors acknowledge limitations in the sensitivity of TreeLSTM to global changes and possible oscillations in performance on small datasets. To enhance the robustness and stability of TreeMoCo the authors may consider refining data augmentations addressing global sensitivity issues implementing early stopping mechanisms and evaluating the impact on realworld applications more thoroughly.", "znbTxnBPlx": " Summary: The paper presents a novel method for training stabilized supralinear networks (SSNs) representing cortical neural circuits. The method enables the training of SSNs without encountering dynamical instabilities allowing for the networks to be trained at scale and on challenging tasks like MNIST classification and probabilistic inference tasks. The results demonstrate the effectiveness of the proposed method in training realistic corticallike neural networks on various tasks.  Strengths: 1) Novel Methodology: The paper introduces a unique approach to training SSNs that addresses the challenge of dynamical instabilities enabling the training of biologically realistic neural networks on complex tasks. 2) Experimental Validation: The authors successfully demonstrate the effectiveness of the method through experiments on MNIST classification and probabilistic inference tasks showing promising results. 3) Biological Relevance: The focus on biological realism and simulating neural circuits adds value to the field of computational neuroscience offering insights into neuronallevel mechanisms.  Weaknesses: 1) Complexity of Method: The methods complexity involving gradual network growth and architecture changes might limit scalability and computational efficiency. 2) Limited Generalization: The evaluation is mainly focused on SSNs performance on specific tasks like MNIST and probabilistic inference potentially limiting the generalizability of the method to broader machine learning applications.  Questions and Suggestions for Authors: 1) How does the proposed method compare to existing techniques in terms of training time and computational resources required 2) Have you considered exploring the tradeoffs between training stability and network complexity in more depth 3) How adaptable is the method to other types of neural network architectures beyond SSNs  Limitations: The authors have not explicitly addressed how potential negative societal impacts of the research could be mitigated. It is advisable to consider and discuss any ethical implications or societal consequences resulting from the development and application of the proposed method to ensure responsible research conduct. Suggestions for improvement include including an ethical considerations section in the paper and discussing ethical implications in a broader context.  Overall the paper presents a significant contribution to the field of neurocomputational modeling showcasing an innovative method for training biologically realistic neural networks on challenging tasks. Further refinements and considerations for scalability and broader applicability could enhance the impact of this research.", "zLVLB-OncUY": " Review of \"InstanceSpecific Contrastive Optimal Positive Generation via Latent Space Transformation\"  Summary: The paper proposes a novel approach called COPGen for selfsupervised contrastive learning. COPGen aims to generate optimal positive pairs in a fully unsupervised setting by learning instancespecific latent transformations in the latent space of pretrained generative models. The method minimizes the mutual information between the generated positive pairs while preserving semantic consistency. The paper provides a theoretical foundation for COPGen and demonstrates its effectiveness through empirical experiments outperforming other latent transformation methods and realimagebased approaches in selfsupervised contrastive learning.  Strengths: 1. Innovative Approach: The paper introduces a novel method for generating optimal positives for selfsupervised contrastive learning leveraging latent space transformations. 2. Theoretical Foundation: The theoretical analysis provided in the paper strengthens the understanding of the proposed approach and its efficacy. 3. Empirical Results: The experimental results show that using COPGen to generate positives consistently outperforms other methods validating the effectiveness of the proposed approach. 4. Comparative Analysis: The paper comprehensively compares COPGen with existing methods highlighting its superior performance in multiple experiments.  Weaknesses: 1. Data Augmentation Dependency: The ablation study suggests that the performance drops significantly when removing data augmentations. Addressing the need for data augmentation in conjunction with COPGen can be further explored. 2. Computational Overhead: The paper acknowledges the additional computational cost introduced by COPGen. Efficient ways to mitigate this overhead could be discussed.  Questions and Suggestions: 1. Generalization: How does the proposed COPGen approach generalize to different domains and datasets beyond the ones considered in the experiments 2. Scalability: Are there scalability issues when applying COPGen to larger datasets or more complex tasks Providing insights into the scalability of the method would enhance its practicality. 3. Impact of Pretrained Models: How sensitive is the performance of COPGen to the quality and biases present in pretrained generative models used as data sources 4. Further Analysis: Conducting additional experiments to evaluate the impact of hyperparameters or variations in the training process might provide deeper insights into the robustness and generalizability of COPGen.  Limitations: The authors have not extensively addressed potential negative societal impacts of using synthetic data for representation learning. To improve the authors could conduct a comprehensive societal impact assessment and discuss strategies to mitigate biases present in generative models. Overall the paper presents a novel and effective approach for generating optimal positives in selfsupervised contrastive learning. By addressing the weaknesses and questions raised the authors can further strengthen the impact and relevance of their work.", "tadPkBL2gHa": " Summary: The paper presents a novel approach for optimizing academic recruitment processes under uncertainty by selecting a subset of candidates that maximizes overall expected value while considering a penalty for deviating from the target number of positions. The authors propose algorithms analyze their theoretical guarantees and conduct empirical evaluations to validate their results.  Strengths: 1. Original Contribution: The paper addresses a practical problem in academic recruitment with a novel formulation and provides theoretical guarantees for proposed algorithms. 2. Theoretical Analysis: The paper rigorously analyzes the algorithms under different penalty functions showcasing their performance and optimality under specific conditions. 3. Empirical Evaluation: The authors conduct experiments on synthesized data validating the theoretical results and confirming the algorithms effectiveness in practice. 4. Applicability: The proposed approach is intuitive and aligns with current recruitment practices making it potentially feasible for adoption in realworld scenarios.  Weaknesses: 1. Theoretical Complexity: While the paper presents sophisticated theoretical analyses the discussion on practical deployment and scalability in realworld settings is relatively limited. 2. Algorithm Behavior: The empirical results highlight a slight lag in the performance of the proposed algorithm compared to the greedy heuristics due to rounding probabilities implying potential optimization of the algorithms computational efficiency. 3. Ethical Considerations: While briefly touched upon a more detailed discussion on the potential negative societal impacts and recommendations for mitigating them could enhance the papers comprehensiveness.  Questions and Suggestions to Authors: 1. Can you provide more insights into the practical implementation challenges of the proposed algorithms in largescale recruitment processes 2. Have you considered the adaptability of your approach to diverse academic disciplines or organizations with varying recruitment strategies 3. How do you envision the integration of your approach with existing recruitment tools or practices to enhance decisionmaking processes 4. Do you plan to explore the impact of incorporating additional constraints or factors such as diversity considerations into the optimization model in future work  Limitations: The paper addresses the limitations and societal impact briefly. To enhance this aspect:  Provide a more indepth discussion on potential negative consequences and societal implications  Recommend including clear guidelines or mitigation strategies for addressing any adverse effects from adopting the proposed algorithms. Overall the paper presents a valuable contribution to the field combining theoretical rigor with practical implications for optimizing academic recruitment processes under uncertainty. Further enhancements in the discussion of practical implications and societal impacts could strengthen the papers relevance and applicability.", "vhKaBdOOobB": " Peer Review: 1) Summary: The paper proposes a novel hardwarefriendly attention mechanism called DFC attention and introduces a new GhostNetV2 architecture designed for mobile applications. The DFC attention mechanism is based on fullyconnected layers and aims to capture longrange spatial information efficiently while enhancing lightweight convolutional neural networks representational ability. The GhostNetV2 architecture integrates the DFC attention to simultaneously aggregate local and longrange information showing superior performance to existing architectures. 2) Strengths:  Innovative Approach: The integration of DFC attention in GhostNetV2 to capture longrange dependency efficiently is a novel and promising approach.  Extensive Experiments: The paper provides comprehensive experiments demonstrating the superiority of GhostNetV2 over existing architectures in terms of accuracy and computational cost tradeoff.  Clarity: The paper is wellstructured and clearly explains the motivation methodology and results.  Practical Relevance: Considering the focus on mobile applications the proposed architectures efficiency and effectiveness are highly relevant to current technology trends. 3) Weaknesses:  Complexity: The paper delves deep into technical details which might make it challenging for readers who are not wellversed in neural networks and hardware architectures.  Limited Comparison: While the experiments show GhostNetV2 outperforming existing architectures a more detailed comparison with a broader set of recent models could strengthen the analysis.  Theoretical vs. Practical: The discussion on theoretical complexity versus practical implementation could be further elaborated to provide a clearer understanding of the impact on realworld performance. 4) Questions and Suggestions for Authors:  How does the proposed DFC attention mechanism handle scenarios with varying input resolutions  Could you provide insights into potential scenarios or applications where the GhostNetV2 architecture would be particularly advantageous  How does the proposed GhostNetV2 architecture perform in scenarios with limited compute resources or memory constraints  Have you considered conducting user studies or qualitative assessments to evaluate the usability and interpretability of the GhostNetV2 model 5) Limitations: The paper adequately addresses the limitations and potential negative societal impact of the work by ensuring efficiency and optimization for mobile deployment. However to further enhance this aspect the authors could provide discussions on the environmental impact of deploying AI models on mobile devices and potential implications for energy consumption. In conclusion the paper presents a promising contribution to the field of lightweight neural architectures for mobile applications. Strengthening the conceptual clarity expanding comparison analyses and addressing practical implications could further enhance the papers impact and relevance in the research community. I hope these insights are beneficial for the authors in refining and enhancing their work.", "sZAbXH4ezvg": "Summary: The paper introduces the concept of multiscale graph neural networks with implicit layers (MGNNI) as a solution to address weaknesses in implicit graph neural networks (GNNs) related to limited effective range for capturing longrange dependencies and the lack of ability to model multiscale information on graphs. MGNNI aims to expand the effective range and capture information at various scales on graphs. The authors conduct experiments on both node classification and graph classification tasks showing that MGNNI outperforms representative baselines in terms of performance and ability to capture longrange and multiscale information. Strengths: 1. Innovative Approach: Introducing multiscale propagation in implicit GNNs is a novel and innovative approach to address the limitations of previous models. 2. Comprehensive Experiments: The paper conducts thorough experiments on synthetic and realworld datasets to demonstrate the effectiveness of MGNNI compared to baselines providing strong empirical evidence. 3. Theoretical Analysis: The paper provides wellstructured theoretical analyses to support the proposed method giving a solid foundation for the proposed model. Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers not deeply familiar with graph neural networks and mathematical formalisms. 2. Evaluation Metrics: It would be beneficial to include additional experimental evaluation metrics and comparisons with a broader range of stateoftheart methods to strengthen the conclusions further. 3. Clarity: Some sections could benefit from improved clarity in explaining complex concepts particularly for readers less familiar with the topic. Questions and Suggestions: 1. What specific realworld applications do you envision MGNNI being most impactful for 2. Have you considered exploring different aggregation mechanisms within MGNNI to further enhance performance 3. How does the computational efficiency of MGNNI compare to other methods especially in scenarios with large datasets 4. Could you provide more insights into the robustness of MGNNI to noisy or incomplete graph data Limitations: While the paper extensively discusses the limitations of previous implicit GNNs and presents MGNNI as a solution more discussion on potential negative societal impacts or ethical considerations of implementing such models in realworld applications could enhance the paper. Overall the paper presents a significant advancement in the field of graph neural networks with its proposal of MGNNI. With further refinements and addressing the mentioned points the paper can make valuable contributions to the research community.", "tYAS1Rpys5": " Peer Review  Summary: The paper introduces SimulationGuided Beam Search (SGBS) as an inference method for neural combinatorial optimization. SGBS combines MonteCarlo tree search with beam search to improve the quality of solutions found in a given time budget. The paper further hybridizes SGBS with Efficient Active Search (EAS) to enhance search performance. Experimental results show that SGBS and the SGBSEAS hybrid outperform existing methods on wellknown combinatorial optimization benchmarks.  Strengths: 1. Innovative Approach: The introduction of SGBS as a method to enhance the search capabilities of neural combinatorial optimization methods is a significant contribution to the field. 2. Experimental Evaluation: The thorough evaluation of SGBS on various combinatorial optimization benchmarks demonstrates its effectiveness and superiority over existing methods. 3. Efficiency and Simplicity: SGBS strikes a balance between performance and simplicity making it a practical and efficient inference method for neural combinatorial optimization. 4. Integration with EAS: The incorporation of EAS to further enhance the performance of SGBS showcases the synergy between the two methods in improving solution quality.  Weaknesses: 1. Clarity in Explanations: The paper at times gets technical and may be challenging for readers not wellversed in neural combinatorial optimization. Providing clearer explanations and examples could enhance understanding. 2. Reproducibility Concerns: More details on hyperparameter tuning and implementation specifics could be beneficial for researchers looking to replicate the experiments.  Questions and Suggestions for Authors: 1. Could you explain further how the hyperparameters \u03b2 and \u03b3 were chosen for SGBS 2. In realworld applications how adaptable is SGBS to diverse problem instances without significant modifications 3. How does the computational efficiency of SGBS compare with other stateoftheart methods especially in largescale combinatorial optimization problems  Limitations: The authors adequately addressed the limitations and potential negative societal impacts of their work. They could enhance this aspect by providing a more comprehensive discussion on the computational resources required for implementing SGBS and SGBSEAS in practical scenarios ensuring scalability and efficiency.  Suggestions for Improvement: 1. Provide more detailed guidelines on implementing SGBS and SGBSEAS for researchers interested in applying these methods to their combinatorial optimization problems. 2. Consider discussing the computational tradeoffs and resource requirements when scaling up the application of SGBS in largescale combinatorial optimization scenarios. Overall the paper presents a novel and promising approach in the field of neural combinatorial optimization showcasing improved solutions and efficiency with SGBS and the SGBSEAS hybrid. Clarifying explanations enhancing reproducibility details and discussing scalability implications could further strengthen the papers impact and usability in the research community.", "qqHMvHbfu6": " Peer Review  Summary: The paper presents an analysis of the learning dynamics in Lewis signaling games used for simulating language emergence. The authors decompose the standard objective in these games into two components: an information loss and a coadaptation loss. Through empirical experiments they demonstrate that controlling overfitting in the coadaptation loss leads to the development of more structured and generalizable languages. The study provides valuable insights into the impact of learning dynamics on language emergence in computational models.  Strengths: 1. Analytical Framework: The decomposition of the loss function in Lewis games is a novel and insightful approach to understanding the learning dynamics in language emergence simulations. 2. Empirical Validation: The empirical experiments conducted to validate the analytical findings are thorough and welldesigned. The results effectively support the main conclusions of the study. 3. Experimental Rigor: The methodology followed in the experiments such as probing the information and coadaptation losses and controlling the listeners level of convergence is wellstructured and provides meaningful insights into the emergence of structured languages. 4. Generalization and Compositionality Analysis: The evaluation of generalization and compositionality in the emergent languages demonstrates a comprehensive assessment of the language properties which strengthens the validity of the studys findings.  Weaknesses: 1. Complexity: The paper is highly technical which might make it challenging for readers without a deep understanding of machine learning and language modeling to follow the analysis. 2. Clarity: Some sections particularly those detailing the methodological tools and game settings could be further streamlined for better clarity and ease of understanding. 3. Impact of Environmental Factors: The study primarily focuses on the learning dynamics within Lewis signaling games but a discussion on how environmental factors interact with the identified learning dynamics could provide a more comprehensive analysis.  Questions and Suggestions for Authors: 1. How do you envision further extending the findings of this study to more diverse and complex communication environments beyond Lewis games 2. Have you explored potential applications of your findings in practical language processing tasks or conversational AI systems 3. Could you elaborate on the implications of your study for the field of cognitive science specifically in understanding the emergence of structured languages in humans  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To rectify this it is recommended to consider discussing:  The ethical implications of using reinforcement learning for language modeling especially in sensitive contexts.  The potential biases or unintended consequences that may arise from deploying structured languages developed through this method in realworld applications. Overall the paper makes a significant contribution to understanding the learning dynamics in language emergence simulations and with some revisions it has the potential to further enhance its impact in the field of computational linguistics.", "pD5Pl5hen_g": " Peer Review 1) Summary: The paper presents an optimal algorithm for solving the smooth and stronglyconvexstronglyconcave minimax optimization problem. The proposed algorithm achieves the lower bound on gradient evaluation complexity improving upon existing methods. The work is organized into sections covering problem reformulation algorithm development and convergence guarantees. 2) Strengths:  Novelty and Contribution: The paper introduces the first optimal algorithm that matches the lower bound complexity for minimax optimization a significant contribution to the domain.  Technical Depth: The paper dives into detailed mathematical formulations algorithmic frameworks and theoretical proofs showcasing a strong technical foundation.  Methodological Rigor: The authors employ a systematic approach construct algorithms based on previous works and provide clear explanations for the proposed solutions. 3) Weaknesses:  Clarity and Accessibility: While the paper is technically sound the complex mathematical formulations and derivations may pose challenges for readers not deeply familiar with the subject matter. Simplifying certain parts could improve accessibility.  Comparison and Discussion: Although the algorithms effectiveness is highlighted a more indepth discussion comparing it with existing methods could enhance the papers value. 4) Questions and Suggestions for Authors:  Could you provide more intuitive explanations or examples to make the mathematical concepts more accessible to a broader audience  Considering the related works how does the proposed algorithm compare in terms of convergence speed or computational efficiency  Have you considered including practical experiments or simulations to validate the algorithms performance in realworld scenarios 5) Limitations: The paper seems to lack a discussion on the limitations of the proposed algorithm in practical applications or potential societal impacts. Authors should consider addressing these aspects to provide a more comprehensive evaluation of their work. Overall the paper presents a valuable advancement in minimax optimization algorithms. Addressing the suggested points could enhance the clarity and impact of the research. Well done on a thorough and technically rich study.", "msFfpucKMf": "1) Summary: The paper introduces a framework for training robust options for performing arbitrary sequences of subtasks in reinforcement learning settings. It formulates the problem as a twoplayer zerosum game and proposes two reinforcement learning algorithms ROSAC and AROSAC to solve the game. The approach is evaluated on two multitask environments demonstrating superior performance compared to existing baselines. 2) Strengths and Weaknesses: Strengths:  Innovative Approach: The paper presents a novel gametheoretic formulation that tackles the problem of training subtask policies to perform any task robustly.  Algorithm Development: Proposing two RL algorithms ROSAC and AROSAC for solving the game is a significant contribution.  Experimental Evaluation: The approach is rigorously evaluated on two multitask environments showcasing promising results compared to baselines. Weaknesses:  Limited Generalization Discussion: The paper could benefit from a more indepth discussion on the generalization capabilities of the proposed approach to varying environments and tasks.  Complexity: The proposed algorithms may require significant computational resources and the tradeoffs between performance and computational cost could be explored further.  Comparative Analysis: While the results demonstrate the effectiveness of the proposed approach a more detailed comparison with existing stateoftheart methods could provide additional insights. 3) Questions and Suggestions for Authors:  Generalization: How does the proposed approach generalize to unseen tasks and environments outside the evaluated benchmarks  Scalability: Have you considered the scalability of the algorithms to more complex and realworld scenarios where a large number of subtasks and tasks are involved  Interpretability: How do you ensure interpretability of the learned policies especially in realworld applications where the decisionmaking process needs to be transparent  Extension to MultiAgent Setting: Given the relevance of multiagent environments have you considered extending the approach to cooperative or competitive multiagent settings 4) Limitations and Societal Impact: The authors successfully discussed the limitations but could further enhance the discussion by providing insights into potential negative societal impacts. To address this authors might consider conducting impact assessments on the application of their work in sensitive areas such as robotics in military applications. Additionally emphasizing the ethical considerations related to task automation and robot decisionmaking in realworld settings could be beneficial. Overall the paper presents a novel and promising approach to compositional reinforcement learning with clear strengths in methodology and experimental evaluation. Further considerations towards generalization scalability and ethical implications could enhance the robustness and applicability of the proposed framework.", "vy7B8z0-4D": " Summary: This paper introduces a novel method Fused Unbalanced Gromov Wasserstein (FUGW) for intersubject alignment of cortical surfaces based on functional signatures derived from neuroimaging data. The method aims to address the challenges posed by the interindividual variability in brain structure and function. Experimental results demonstrate that FUGW leads to a significant increase in betweensubject correlation of brain activity and provides more precise mapping at the group level outperforming existing methods.  Strengths: 1. Innovative Methodology: The paper introduces a novel approach FUGW leveraging Optimal Transport to address intersubject alignment challenges in neuroimaging data. 2. Comprehensive Evaluation: The experimental results clearly demonstrate the effectiveness of FUGW in increasing betweensubject correlation of brain activity and improving mapping precision. 3. Detailed Explanations: The paper provides thorough details on the formulation of FUGW including the objectives constraints and computation methods. 4. OpenSource Implementation: The availability of an opensource solver for FUGW enhances reproducibility and indicates a strong commitment to scientific transparency.  Weaknesses: 1. Limited Comparison: While the paper compares FUGW with MSM the comparison with a broader range of existing methods could provide a more comprehensive assessment of its performance. 2. HyperParameter Sensitivity: The impact of hyperparameters especially \u03b5 on the results is crucial but challenging to interpret. Further explanation or guidance on selecting optimal values would be beneficial. 3. Small Cohort Size: The study acknowledges the limitation of a small cohort size which can affect the generalizability of the results. Extending the evaluation to larger datasets would strengthen the robustness of the findings.  Questions and Suggestions: 1. Generalizability: How do you plan to validate the performance and generalizability of FUGW on larger datasets beyond the Individual Brain Charting dataset 2. Parameter Optimization: Could you provide more insight into the process of hyperparameter selection especially regarding the sensitivity of \u03b5 and its practical implications for optimal model performance 3. Clinical Applications: What potential clinical applications or realworld scenarios do you envision for FUGW considering its promising results in increasing betweensubject correlation and mapping precision  Limitations: The paper adequately addresses the limitations of a small cohort size but could further strengthen the discussion by proposing strategies for validating FUGW on larger datasets to enhance its robustness and applicability. Overall this paper presents a promising advancement in intersubject brain alignment techniques and addressing the weaknesses and questions raised could further enhance its impact and practical utility in the neuroimaging research community.", "x_HUcWi1aF1": " Summary: The paper introduces a novel strategywise concentration principle for offline multiagent reinforcement learning (MARL) to address the curse of multiagents in games. The proposed algorithm achieves sample complexity improvements for twoplayer zerosum Markov games and multiplayer generalsum Markov games breaking the exponential dependency on the number of actions. Additionally the paper incorporates prespecified strategy classes for efficient learning close to the best strategy.  Strengths: 1. Innovative Strategywise Concentration Principle: Introducing a new strategywise concentration principle is a significant contribution that addresses the curse of multiagents in offline MARL. 2. Efficient Algorithm Design: The algorithms proposed for twoplayer zerosum and multiplayer generalsum Markov games demonstrate computational efficiency and sample complexity improvements. 3. Incorporation of Prespecified Strategy Class: Considering prespecified strategy classes is a novel addition in the MARL setting enabling the utilization of prior knowledge for improved learning.  Weaknesses: 1. Theoretical Clarity: The paper may benefit from clearer explanations or illustrative examples to enhance the understanding of the proposed algorithms and theoretical advancements. 2. Experimental Validation: While the theoretical contributions are significant empirical validation on realworld or simulated environments could further strengthen the papers impact and practical relevance.  Questions and Suggestions for Authors: 1. Algorithm Efficiency: How does the proposed strategywise concentration principle impact the convergence speed of the algorithms compared to existing methods 2. Scalability: Have you assessed the scalability of the proposed algorithms to larger and more complex environments with a high number of actions and players 3. Generalizability: How adaptable are the proposed algorithms to different types of games beyond Markov games and are there any limitations in terms of game structure or characteristics  Limitations: The authors could enhance the paper by addressing the potential negative societal impacts of their work such as ethical concerns related to reinforcement learning algorithms and their realworld applications. Providing insights into the ethical considerations and societal implications would strengthen the papers contribution.", "rnJzy8JnaX": " Summary: The paper proposes a novel method called CrossResolution Knowledge Distillation (ResKD) to enhance video recognition using lowresolution frames efficiently. It challenges the common belief that lowresolution frames are suboptimal for recognition accuracy and demonstrates that ResKD can significantly improve efficiency and accuracy compared to existing methods. The paper conducts an indepth analysis of the causes of performance degradation on lowresolution frames introduces ResKD as a simple but effective solution and benchmarks its performance on various datasets.  Strengths: 1. Innovative Approach: The paper introduces a novel method ResKD that bridges the gap between network architecture and input scale for efficient video recognition using lowresolution frames. 2. Thorough Analysis: The paper conducts a comprehensive analysis of the underlying causes of performance degradation on lowresolution frames providing valuable insights for the video recognition community. 3. Extensive Experiments: The authors extensively benchmark ResKD on multiple datasets and demonstrate its effectiveness generality and scalability showcasing superior performance over competitive methods. 4. Clear Articulation: The paper is wellorganized with clear explanations of the problem statement methodology experiments and results making it easy to follow for readers.  Weaknesses: 1. Validation: The paper could provide additional insights into the validation methodology used for ResKD including comparisons with a wider range of baseline methods or insights into potential failure cases. 2. Theoretical Justification: While the empirical results are strong more theoretical justification or analysis of why ResKD works well could enhance the robustness of the proposed method.  Questions and Suggestions for Authors: 1. How does ResKD perform with different types of video content or action types Are there specific scenarios where it may not be as effective 2. Have you considered the potential impact of noise or artifacts in lowresolution frames on the effectiveness of ResKD How robust is the method to such variations 3. Can you elaborate on the computational complexity and training time required for ResKD compared to other stateoftheart methods Is there potential for further optimization in terms of efficiency 4. Does ResKD have any prerequisites in terms of network architecture or dataset characteristics for optimal performance Clarifying these dependencies could provide more insights for practical implementation.  Limitations: The authors have addressed the limitations of their work regarding efficient video recognition using lowresolution frames. However further exploration into the generalizability of ResKD across diverse video datasets and scenarios could enhance the completeness of the study.", "zofwPmKL-DO": " Summary This paper presents quantum algorithms for logconcave sampling and estimating normalizing constants which are fundamental tasks with broad applications in machine learning physics and statistics. The authors develop innovative quantum algorithms that achieve speedups over classical approaches by exploiting quantum analogs of classical algorithms such as Langevin diffusion and Metropolisadjusted Langevin algorithms.  Strengths 1. Innovative Quantum Algorithms: The paper introduces novel quantum algorithms for logconcave sampling and normalizing constant estimation providing polynomial speedups over classical methods. 2. Rigorous Analysis: The authors provide detailed technical analysis including the design of quantum algorithms theoretical proofs and complexity analysis demonstrating the effectiveness of the proposed algorithms. 3. Theoretical Contributions: Theoretical contributions include proving quantum lower bounds and demonstrating nearoptimality of the quantum algorithms enhancing the understanding of quantum speedups in logconcave sampling tasks.  Weaknesses 1. Experimental Validation: Lack of experimental validation or practical implementation of the proposed quantum algorithms may limit the applicability of the work in realworld scenarios. 2. Clarity in Presentation: The technical details and mathematical derivations are complex making it challenging for readers less familiar with quantum computing to fully grasp the content.  Questions and Suggestions for Authors 1. It would be beneficial if the authors could provide a more intuitive explanation or visualization of the quantum algorithms for logconcave sampling to enhance understanding for readers from diverse backgrounds. 2. Can the authors discuss potential challenges in implementing these quantum algorithms on current quantum hardware and suggest possible strategies to overcome these challenges 3. How do the proposed quantum algorithms compare in terms of resource requirements such as qubit counts gate complexity and noise tolerance when compared to classical counterparts  Limitations and Societal Impact The authors have not explicitly addressed the potential limitations or negative societal impacts of their work in the context of quantum computing. It would be constructive to incorporate a discussion on the limitations such as practical feasibility hardware constraints and ensure ethical considerations are adequately addressed.  Constructive Suggestions for Improvement 1. Include a section in the paper discussing the practical challenges of implementing the proposed quantum algorithms on current quantum hardware and potential solutions to mitigate these challenges. 2. Provide more concrete insights into the scalability of the quantum algorithms proposed in the paper and discuss how these algorithms could be adapted for largerscale applications in the future. 3. Consider including simulations or experiments to validate the performance and efficiency of the quantum algorithms in realworld scenarios providing practical insights for readers and potential users of the algorithms.", "yipUuqxveCy": " Summary: The paper introduces an innovative offline multiagent reinforcement learning (offline MARL) framework that leverages previously collected data without additional online data collection. The method reframes offline MARL as a sequence modeling problem and utilizes the Transformer architecture for scalability. By training a teacher policy to understand agents interactions and distilling its knowledge to student policies the framework achieves superior performance convergence rate sample efficiency and robustness compared to stateoftheart offline MARL baselines.  Strengths: 1. Innovative Framework: The paper proposes a novel approach to offline MARL by formulating it as a sequence modeling problem and utilizing policy distillation. 2. Performance: Demonstrated stateoftheart results on various tasks highlighting the effectiveness of the proposed method over existing baselines. 3. Structured Knowledge Distillation: The introduction of a structural relation distillation objective enhances learning by preserving the relationships among agent features. 4. Thorough Experiments: The paper presents rigorous evaluations including comparisons with multiple baselines ablation studies and analysis of convergence rates sample efficiencies and scalability.  Weaknesses: 1. Scalability Concerns: The reliance on a centralized decision transformer for all agents may pose limitations in terms of scalability when dealing with a large number of agents. Consider discussing strategies to address this potential limitation. 2. Societal Impact: While the paper briefly mentions potential negative applications like drone surveillance a more thorough discussion on ethical considerations and potential negative societal impacts of the proposed method would be beneficial.  Questions and Suggestions for Authors: 1. How does the proposed method handle scenarios with a large number of agents and what strategies can be employed to enhance scalability 2. Can the authors elaborate on the ethical implications of deploying a centralized decision transformer in realworld multiagent systems particularly addressing concerns related to privacy and security 3. It would be helpful to delve deeper into the computational costs associated with training and deploying the proposed framework in practical applications.  Limitations: The authors should further address the scalability concerns associated with a centralized decision transformer and provide insights into mitigating potential negative societal impacts especially in sensitive applications like surveillance.  Suggestions for Improvement: 1. Discuss potential strategies to improve scalability for scenarios with a large number of agents considering the computational overhead of a centralized decision transformer. 2. Include a dedicated section discussing the ethical implications of the proposed method and explore ways to mitigate any negative societal impacts that may arise. 3. Provide a more detailed explanation of the computational resources required during training and inference to enhance the practical applicability of the proposed framework.", "lhLEGeBC-ru": "1) Brief Summary: The paper establishes new polynomial time guarantees for the BurerMonteiro method by solving semidefinite programs in a smoothed analysis setting. It shows that the method can solve SDPs efficiently provides endtoend guarantees and addresses the feasibility and approximate optimality. The main technical contribution lies in the analysis of spurious critical points and the use of tubular neighborhoods for concentration bounds. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The paper introduces new guarantees for the BurerMonteiro method addressing feasibility and optimality efficiently.  Thorough Technical Analysis: Conducts indepth analysis on spurious critical points and leverages real algebraic geometry for concentration bounds.  Detailed Experiments: The experimental results support theoretical findings and demonstrate the methods performance. Weaknesses:  Complexity: The paper is highly technical which may limit accessibility to nonspecialist readers.  Assumptions: Relies on specific assumptions such as compactness and smoothness which may not always hold in realworld applications.  Limited Discussion: Could provide more elaboration on the practical implications and implications for realworld SDP solving. 3) Questions and Suggestions: Questions:  How do the proposed guarantees of polynomial time complexity compare to existing methods for SDP solving  Have considerations been made for noise or uncertainty in the data that may affect the performance of the method Suggestions for Improvement:  Provide a comparative analysis with existing SDP solvers to showcase the advantages and limitations of the BurerMonteiro method.  Consider discussing overfitting or generalization issues when applying the method to realworld datasets with noise. 4) Limitations: The paper adequately addresses the limitations by focusing on the smoothed analysis setting although realworld data may not always satisfy the compactness and smoothness assumptions. To enhance the practical relevance the authors could consider exploring the methods robustness to noisy or uncertain data. 5) General Comments: The paper presents a novel approach to solving SDPs efficiently using the BurerMonteiro method in a smoothed analysis setting. The theoretical foundation and experimental results support the claims made. Clear presentation demonstration of technical prowess and rich discussions make this paper a valuable contribution to the field of SDP optimization. Strengthening the link to practical applications and addressing potential noise remain areas for further exploration and improvement.", "sADLRl2STMe": " Summary: The paper introduces an explicit penalty in deep linear networks to mirror the implicit bias towards lowrank solutions observed in gradient descent optimization trajectories. The penalty when combined with Adam optimizer enables a degenerate singlelayer network to achieve lowrank approximations with comparable generalization error to deep linear networks. The study sheds light on the interplay between implicit and explicit regularization highlighting depth invariance and improved performance across various matrix completion tasks.  Strengths: 1. Innovative Contribution: The paper provides a novel approach to replicate the implicit regularization effects observed in deep networks using an explicit penalty. 2. Thorough Experiments: The experimental setup is extensive demonstrating the effectiveness of the proposed penalty in achieving lowrank solutions and improved generalization across different scenarios. 3. Clarity and Structure: The paper is wellstructured and the explanations about implicit regularization optimization dynamics and experimental results are clear and detailed.  Weaknesses: 1. Limited Comparison: The comparison with other penalties and optimizers is limited to a few combinations. A broader comparison with a wider range of penalties and optimizers could provide more insights into the effectiveness of the proposed approach. 2. Discussion on RealWorld Data: While the paper briefly mentions results on realworld data a more detailed analysis and comparison with existing stateoftheart methods would strengthen the practical relevance of the findings.  Questions and Suggestions for Authors: 1. Can more insights be provided on the underlying mechanism through which the explicit penalty interacts with the optimizer to achieve depth invariance and improved performance 2. How robust are the results to hyperparameter choices such as the penalty coefficient and learning rates and how sensitive is the method to changes in these settings 3. It would be interesting to investigate the scalability and computational efficiency of the proposed approach compared to existing methods especially in largescale applications.  Limitations: Although the paper extensively addresses the limitations and negative societal impact of its work further exploration on the generalizability and applicability of the proposed explicit penalty in realworld applications would enhance the practical relevance of the findings.", "kvtVrzQPvgb": "Summary: The paper proposes a novel aggregation procedure based on social choice theory to rank systems in a multitasks setting in machine learning. It highlights the significance of benchmarks in assessing the progress of new methods in NLP and the limitations of traditional meanaggregation methods. The proposed method is theoretically grounded and is evaluated through extensive numerical experiments on synthetic and real scores from various benchmarks. Strengths:  The paper addresses a crucial issue in benchmarking NLP systems which has been overlooked in prior works.  The proposed method is theoretically sound based on the Kemeny consensus aggregation procedure.  Extensive numerical experiments are conducted to validate the effectiveness and robustness of the proposed method.  The paper provides clear explanations definitions and formulations making the topic accessible to a broad audience in the ML community. Weaknesses:  The complexity of the Kemeny consensus aggregation procedure may pose challenges in realworld applications due to computational limitations.  The paper focuses primarily on NLP benchmarks limiting the generalizability of the proposed method to other domains in machine learning.  While the experimental results are promising they are limited to synthetic and a few specific realworld benchmarks. More diverse datasets could strengthen the empirical validation.  The paper could benefit from discussing potential practical implications and applications of the proposed method beyond just ranking systems in NLP tasks. Questions and Suggestions: 1. Could the authors provide insights into the computational complexity of the Kemeny consensus aggregation procedure and discuss any potential scalability issues in largescale applications 2. How does the proposed method compare to existing stateoftheart aggregation techniques in machine learning benchmarks other than NLP tasks 3. Have the authors considered the interpretability of their ranking results and how different stakeholders could benefit from understanding why a system performs well or poorly in specific tasks 4. How could the proposed methodology be extended or adapted to handle scenarios where multiple criteria need to be considered simultaneously for comprehensive evaluation Limitations: While the paper effectively addresses the limitations of traditional meanaggregation methods in benchmarking NLP systems it could further explore and discuss potential negative societal impacts of using inadequate evaluation procedures. To enhance societal impact assessment the authors could consider incorporating fairness bias and ethical considerations in their evaluation framework.", "pyLFJ9TBZw": " Peer Review of \"Generative Multitask Learning: Improving Robustness to Target Shift through Causal Machine Learning\"  Summary: The paper introduces Generative Multitask Learning (GMTL) a novel approach in causal machine learning that addresses the issue of targetcausing confounding in multitask learning (MTL) settings. By modifying the inference objective GMTL aims to remove spurious dependencies between input and targets thus improving robustness to target shift. Through experiments on the Attributes of People and Taskonomy datasets the paper shows that GMTL enhances performance across multiple MTL methods by mitigating targetcausing confounding.  Strengths: 1. Innovative Approach: Introducing GMTL to mitigate targetcausing confounding is a novel and potentially impactful contribution to both causal machine learning and multitask learning. 2. Empirical Validation: The experiments on real datasets showcase the effectiveness of GMTL in improving robustness to target shift strengthening the credibility of the proposed approach. 3. Clear Exposition: The paper provides a clear and detailed explanation of the problem methodology and results making it accessible to readers from diverse backgrounds.  Weaknesses: 1. Theoretical Grounding: While the proposed approach is interesting some more theoretical grounding on the causal assumptions and implications of the method could enhance the depth of the paper. 2. Generalization: The paper focuses on two specific datasets limiting the generalizability of the findings. Including results from more datasets would strengthen the conclusions. 3. Complexity: The method appears to involve some complexity particularly in the implementation and tuning of the hyperparameter \u03b1. More clarity on practical challenges and potential pitfalls would be beneficial for future adopters.  Questions and Suggestions for Authors: 1. How does the proposed approach handle noisy or incomplete data in the context of multitask learning 2. Can the authors provide insights into the computational overhead or efficiency implications of integrating GMTL into existing MTL methods 3. Could further insights be provided on the interpretability of GMTL results particularly regarding the removal of spurious dependencies and its impact on downstream applications 4. Have you considered exploring additional realworld applications or scenarios where GMTL could be particularly beneficial beyond the datasets evaluated  Limitations: The authors adequately address the limitations and potential negative societal impacts by highlighting the importance of targetcausing confounding in reallife scenarios such as in medicine where dataset biases can have serious societal implications. To further enhance this aspect the authors could provide more concrete recommendations on mitigating biases and ensuring ethical implications are carefully considered in the deployment of GMTL. In conclusion the paper presents a promising approach in GMTL that offers a valuable contribution to causal machine learning and multitask learning but further theoretical grounding generalization to diverse datasets and practical considerations are recommended to enhance the impact and applicability of the proposed method.", "pkfpkWU536D": " Summary: The paper introduces a novel method called Neural Shape Deformation Priors for shape manipulation predicting mesh deformations of nonrigid objects based on userprovided handle movements. The approach leverages transformerbased deformation networks to learn deformation behavior enabling handling of challenging deformations and generalization to unseen deformations.  Strengths: 1. Innovative Approach: Introducing transformerbased local deformation field networks for shape manipulation is a novel and effective approach. 2. Generalization: The method shows good generalization ability to unseen deformations outperforming classical optimizationbased and recent neural networkbased methods. 3. Thorough Experiments: The paper provides comprehensive experiments on the DeformingThing4D dataset with clear comparisons to baseline methods and ablation studies to validate approach choices.  Weaknesses: 1. Limited Dataset Variety: The limitations of the dataset in terms of models and poses may impact the generalization of the approach to outofdistribution models and extreme poses. 2. User Input Scope: Although sparse user input via handles is efficient the inability to handle rotations through handles may limit the versatility of editing capabilities. 3. Training Bias: Potential biases in the training dataset may affect the models generalization addressing this by enriching and diversifying the training data could be beneficial.  Questions and Suggestions for Authors: 1. How do the local latent codes used to learn local deformation fields impact the efficiency and performance of the deformation predictions 2. Could the method benefit from incorporating rotations alongside translations in user handles for more comprehensive shape manipulation 3. Have you considered exploring additional datasets or augmenting the existing one to address limitations in diversity and generalization to outofdistribution models  Limitations: The authors have not fully addressed the potential negative societal impact of their work in the paper. To improve they could provide considerations on preventing misuse of the algorithm for creating fraudulent or offensive content and ensure ethical implications are discussed in the research. Overall the paper presents an innovative approach with strong experimental validation but addressing the dataset limitations and user input scope could further strengthen the work. Ethical considerations regarding potential misuse should also be explicitly discussed.", "lxdWr1jN8-h": "Summary: The paper introduces SymPlan a framework that enhances data efficiency and generalization in endtoend differentiable planning algorithms for 2D robotic path planning problems by leveraging group symmetry. The key contributions include formulating value iteration as a steerable convolution network and implementing equivariant versions of existing planning algorithms leading to substantial improvements in training efficiency and generalization performance. Strengths: 1. The paper addresses an important problem in endtoend planning algorithms and proposes a novel framework to exploit symmetry for improved performance. 2. The technical contributions including the formulation of value iteration as a steerable convolution network and the implementation of equivariant versions of existing algorithms are innovative and impactful. 3. The experiments conducted on different path planning tasks demonstrate significant improvements in training efficiency and generalization validating the effectiveness of SymPlan. Weaknesses: 1. While the paper provides a detailed theoretical framework and implementation the practical details of code data and instructions for reproducing the experimental results are missing. 2. The discussion on potential negative societal impacts and ethical considerations of the proposed framework is lacking. 3. The limitations of the work are briefly highlighted but could be further elaborated to provide a clearer understanding of the constraints and challenges. Questions and Suggestions: 1. Can you provide more insights into the practical implications and realworld applications of SymPlan beyond the presented experiments 2. How does the proposed framework compare against stateoftheart planning algorithms in terms of scalability and performance in more complex scenarios 3. Have you considered the computational costs and resource requirements for implementing SymPlan in real robotic systems 4. What are the implications of incorporating SymPlan in autonomous navigation tasks or industrial applications Limitations: The authors have addressed the limitations of the work but could further emphasize the challenges in scaling SymPlan to more complex environments and domains. Additionally highlighting the potential negative societal impacts and discussing ethical considerations would enhance the papers comprehensiveness and relevance. Overall the paper presents a significant advancement in leveraging group symmetry for improving endtoend planning algorithms with notable technical contributions and promising results. Further clarity on implementation details practical applications and ethical considerations would strengthen the paper.", "tvDRmAxGIjw": " Summary: The paper introduces posttraining quantization (PTQ) for pretrained language models (PLMs) to address issues with quantizationaware training (QAT) such as slow training high memory overhead and data accessibility problems. The proposed modulewise quantization error minimization (MREM) approach efficiently minimizes reconstruction errors by partitioning PLMs into modules. A new model parallel strategy accelerates the training of MREM achieving significant speedups. Experimental results on GLUE and SQuAD benchmarks show the superiority of the proposed PTQ solution over QAT.  Strengths: 1. Innovative Approach: The paper introduces a novel PTQ method MREM to improve PLM quantization performance while maintaining training efficiency. 2. Efficiency Gain: The new model parallel strategy offers nearly theoretical speedup for distributed training significantly reducing training time. 3. Empirical Results: Results on GLUE and SQuAD benchmarks demonstrate the effectiveness of MREM in reducing memory overhead and training time while maintaining performance.  Weaknesses: 1. Limitation of Calibration Data Size: The study finds a limitation in performance improvement with increased calibration data size leading to slower progress. 2. Reduction in Performance with Increased Modules: More modules result in slightly lower performance highlighting the need for balancing granularity and performance.  Questions  Suggestions: 1. Calibration Data Size Impact: Have you explored the potential reasons behind the saturation of performance with increased calibration data size Consider discussing any insights or potential avenues for improvement. 2. Performance Impact of Increased Modules: Can you provide more context on the decision to partition the model into four modules by default Have you explored the performance impact of different module configurations 3. Scalability and Generalization: How scalable is the proposed MREM approach to other PLMs beyond BERT Have you considered any potential challenges or benefits in scaling to different architectures or sizes  Limitations: The authors did not explicitly address issues such as the potential negative societal impacts of their work. It would be beneficial to include a discussion on potential ethical considerations related to the application of their methods especially in scenarios that affect broader societal contexts. In conclusion the paper presents a valuable contribution to the field of PLM quantization with the proposed MREM approach. Addressing the highlighted weaknesses and considering the questions posed could further enhance the papers impact and relevance.  This draft adheres to the requested format providing a structured evaluation of the strengths weaknesses questions and suggestions for the authors alongside considerations on limitations and ethical implications.", "u_7qyNFwkP8": "Peer Review 1. Summary The paper investigates the query complexity of cake cutting in the standard query model focusing on envyfree perfect and equitable allocations with minimal cuts. Through a series of theorems and proofs the researchers establish lower and upper bounds for different fairness notions. They also introduce moving knife procedures and analyze the simulation of these protocols in the RobertsonWebb query model. 2. Strengths and Weaknesses Strengths:  Comprehensive coverage of various fairness notions in cake cutting.  Clear formalization of moving knife procedures and their efficient simulations.  Rigorous proof techniques for establishing lower bounds in query complexity.  Thorough comparison between RW RW and RW query models.  Exhaustive references to related works and thorough discussions on results. Weaknesses:  The paper may be very technical and challenging for readers without a strong background in theoretical computer science.  Limited practical implications discussed as the research mainly focuses on theoretical complexities. 3. Questions and Suggestions for Authors Questions:  How do the findings of this research contribute to the broader field of computational fairness beyond cake cutting specifically  Have you considered realworld applications or scenarios where the theoretical results presented in the paper could be practically implemented Suggestions:  Provide more intuitive explanations or examples to help readers grasp the complex theoretical concepts easier.  Consider discussing potential realworld applications or implications of the theoretical results to enhance the practical relevance of the study. 4. Limitations The authors could further address potential negative societal impacts of their work and include suggestions for improvement by clearly outlining how the theoretical results could be practically used in realworld scenarios to benefit society and address fairness challenges effectively. Overall Evaluation The paper demonstrates a high level of expertise in theoretical computer science and offers valuable insights into the query complexity of cake cutting algorithms. The rigorous analysis and formalization of fairness notions provide a solid foundation for future research in the field of computational fairness. However enhancing the practical relevance and addressing societal impacts could further strengthen the paper.", "w4X7GLThiuJ": " Summary The paper presents an analysis of the alternating mirror descent algorithm for twoplayer bilinear zerosum games with constrained strategy spaces. It explores the behavior of the algorithm in comparison to simultaneous mirror descent providing theoretical bounds and implications for regret in the constrained setting. The study connects alternating mirror descent with the skewgradient flow in the dual space highlighting its performance advantages and potential applications to various game theory contexts.  Strengths 1. Theoretical Analysis: The paper provides a rigorous theoretical analysis of the alternating mirror descent algorithm in the context of constrained bilinear games offering insights into its behavior and performance. 2. Connections to Continuous Dynamics: The study links the algorithm to the skewgradient flow and offers a detailed explanation of how the alternation in actions mirrors continuoustime dynamics which enhances the understanding of the algorithms behavior. 3. Regret Analysis: By defining and analyzing regret in terms of energy functions the paper offers a novel perspective on evaluating the algorithms performance. 4. Generalization and Relevance: The research extends findings from the unconstrained to the constrained setting paving the way for broader applications in multiagent systems and machine learning.  Weaknesses 1. Complexity: The paper involves advanced mathematical concepts which might make it challenging for readers without a strong background in convex optimization and game theory to grasp the content fully. 2. Clarity in Presentation: The text could be further refined for clarity in certain sections especially those discussing technical derivations and formal notations.  Questions and Suggestions for Authors 1. Clarification on Assumptions: It would be beneficial to explicitly state the assumptions made in the analysis and discuss their implications on the algorithms applicability and performance. 2. Empirical Validation: Have the authors considered empirical validation of the algorithms performance in realworld scenarios or simulations Including practical examples could enhance the papers impact and applicability. 3. Scalability: How does the algorithm scale with larger game settings or more complex payoff structures Addressing scalability concerns could provide insights into practical implementation challenges. 4. Comparison with Existing Methods: Can the authors provide a comparison with other stateoftheart algorithms for constrained minmax games to showcase the uniqueness and advantages of the alternating mirror descent approach  Limitations The authors have adequately addressed the limitations and potential negative societal impact of their work by focusing on theoretical analysis however including practical implications and realworld applicability in future works could enhance the societal impact of the research.", "tPiE70y40cv": " Summary The paper proposes a novel approach for constructing coresets over relational data known as the \"aggregation tree with pseudocube.\" The method addresses the challenges of constructing coresets when the input data is in the form of relational tables rather than a matrix. The approach efficiently builds a coreset from the bottom up using an aggregation tree structure. The coreset construction method is proven to yield a qualified coreset for Empirical Risk Minimization problems in machine learning on relational data.  Assessment of Strengths and Weaknesses Strengths: 1. Innovative Approach: The paper introduces a novel method for constructing coresets on relational data addressing the challenges specific to this scenario. 2. Theoretical Foundation: The method is based on sound theoretical principles and provides provable quality guarantees for the constructed coresets. 3. Experimental Evaluation: The paper includes an experimental evaluation comparing the proposed approach (RCORE) with baseline methods demonstrating competitive optimization quality and acceptable running time. Weaknesses: 1. Overlap Issue Resolution: Although the paper discusses the resolution of the overlap issue in constructing coresets for relational data the complexity of this solution could be a potential drawback that needs further evaluation. 2. Simplification of PseudoCube Overlap: The assumption of disjoint pseudocubes when calculating weights for the coreset could oversimplify the problem and may not accurately represent the true characteristics of the data.  Questions and Suggestions for Authors 1. Evaluation Metrics: Could you provide more insights into the criteria used to choose the variables such as \u03b4 in the sampling procedure for resolving the overlap issue 2. Scalability and Generalizability: How does the proposed approach scale with increasing size and complexity of relational datasets Have you considered the generalizability of the method to other machine learning problems beyond those discussed in the paper 3. Further Validation: To enhance the practical applicability have you considered conducting experiments on a diverse range of datasets beyond those used in the evaluation to validate the methods robustness  Limitations The paper adequately addresses the limitations associated with constructing coresets over relational data acknowledging challenges such as addressing overlap between pseudocubes. To improve authors can further explore more efficient methods for handling overlap and validate the approach on a wider variety of datasets to evaluate its performance robustness.  Constructive Suggestions for Improvement 1. Validation on Diverse Datasets: Include experiments on a wider range of datasets with varying complexities and characteristics to assess the methods applicability in different scenarios. 2. Sensitivity Analysis: Conduct sensitivity analysis on parameters such as \u03b4 in the sampling procedure and evaluate their impact on the quality of the constructed coresets. 3. Comparison with StateoftheArt: Compare the proposed method with stateoftheart techniques for coresets construction on relational data to demonstrate its superiority and effectiveness. Overall the paper presents a promising approach for constructing coresets on relational data but further validation and elaboration on key aspects could strengthen the research.", "ylila4AYSpV": " Summary: The paper addresses the design of modern ad auctions focusing on the Rich Ads problem where advertisers can provide multiple rich ad formats and the auctioneers challenge is to select the best set that fits within the given total space maximizing social welfare. The authors propose a greedy and monotone allocation rule that guarantees a 3approximation to the optimal social welfare. They also analyze the Price of Anarchy for a derived auction mechanism.  Strengths:  Innovative Approach: The authors introduce a novel approach to tackle the rich ads allocation problem providing theoretical proofs and algorithmic solutions.  Theoretical Rigor: The paper is wellstructured presenting clear definitions lemmas and proofs to substantiate the claims made.  Empirical Evaluation: Empirical results on realworld data are included showing the practical viability of the proposed algorithms.  Weaknesses:  Complexity: Some sections of the paper are highly technical which may limit the accessibility to readers not deeply familiar with auction theory and mechanism design.  Algorithm Efficiency: The initial monotone space allocation algorithm (ALGB) is critiqued for being inefficient due to the update strategy impacting the overall performance.  Questions  Suggestions for Authors: 1. Clarification Needed: Can the authors provide more insights into the computational complexity of the proposed algorithms particularly in scenarios with a large number of advertisers and rich ad formats 2. Generalizability: How easily can the proposed mechanisms be adapted to accommodate evolving trends in online advertising such as dynamic bidding strategies or changing user preferences 3. Comparison with Existing Methods: How do the proposed mechanisms compare to existing auction strategies used in realworld ad platforms such as Google Ads or Facebook Ads 4. Scalability: Have the authors considered the scalability of their mechanisms for largerscale ad auctions How do the algorithms perform as the number of advertisers and ad formats increases  Limitations: The paper could benefit from more explicit discussions on the practical implementation challenges and potential negative societal impacts that may arise from using the proposed mechanisms in realworld ad auction platforms.  Suggestions for Improvement: Consider expanding the discussion on implementation challenges scalability issues and ethical implications of implementing the proposed auction mechanisms in realworld settings to provide a more comprehensive analysis. Additionally the authors could address the algorithmic efficiency concerns raised in the review to enhance the practical applicability of their proposed mechanisms.", "pZsAwqUgnAs": " Summary: The paper investigates the dynamics of stochastic gradient descent (SGD) on highdimensional convex quadratic problems. It introduces a homogenized stochastic gradient descent (HSGD) framework characterizes learning and risk trajectories and studies the impact of noise on generalization performance. The paper demonstrates that SGD does not offer implicit regularization in convex problems establishes an implicitconditioning ratio to describe the efficiency of SGD and compares the convergence rates of SGD and fullbatch methods. Numerical results and theoretical analyses provide insights into the benefits and limitations of SGD in different settings.  Strengths: 1. Novel Framework: The introduction of HSGD and characterization of learning and risk trajectories provide a unique perspective on SGD dynamics. 2. Theoretical Rigor: The paper presents rigorous mathematical analyses and establishes precise results on the behavior of SGD in highdimensional convex problems. 3. Comparative Analysis: The comparison of SGD to fullbatch methods in terms of convergence rates and efficiency adds significant value to the understanding of optimization algorithms.  Weaknesses: 1. Limited Generalization: The study focuses on highdimensional quadratic problems limiting the generalizability of the findings to more complex scenarios. 2. Complexity: The mathematical formalism and technical details may be challenging for readers without a strong background in optimization theory. 3. Lack of Practical Examples: While theoretical results are detailed practical examples or realworld applications of the findings could enhance the papers relevance.  Questions and Suggestions for Authors: 1. Could you elaborate on how the proposed HSGD framework can be extended to nonquadratic losses and nonconvex problems 2. Have you considered incorporating practical experiments or applications to showcase the implications of the theoretical results in realworld scenarios 3. How do the results of this study align with existing literature on optimization algorithms and regularization techniques in machine learning 4. Can you discuss any potential implications or extensions of your findings for deep learning models beyond convex problems  Limitations: The authors did not explicitly address the potential negative societal impact of their work. To improve the societal impact assessment the authors should consider discussing the broader ethical implications of optimization algorithms efficiency and generalization behavior in realworld applications.  Constructive Suggestions for Improvement: 1. Incorporate practical case studies or experiments to demonstrate the applicability of the theoretical results. 2. Consider discussing the influence of hyperparameters and data characteristics on the observed behaviors of SGD in optimization tasks. 3. Provide a clearer discussion on how the findings of the study can be practically beneficial for optimizing machine learning models in diverse applications.", "nrOLtfeiIdh": " Summary: The paper introduces RECOURSENET a model for providing recourse in machine learning applications when instances are sampled from bad environments. The model consists of a classifier to optimize performance under recourse a recourse trigger and a recourse recommender network to suggest alternative environment settings for improving predictions. The paper provides a novel threelevel training method without assuming access to the physical process that generates the recoursed instances. Experimental results on synthetic and realworld datasets demonstrate the efficacy of the proposed approach.  Strengths: 1. Innovative Framework: The paper introduces a novel framework for providing recourse in machine learning models when instances are sampled from unfavorable environments. 2. Theoretical Characterization: The paper provides theoretical characterizations to identify circumstances under which recourse can enhance prediction accuracy adding depth to the proposed approach. 3. Empirical Evaluation: The experiments conducted on synthetic and real datasets exhibit the superiority of the proposed method over various baselines showcasing its effectiveness.  Weaknesses: 1. Complexity: The papers training methodology involves multiple components and objectives which might be complicated to implement and understand. 2. Limited Evaluation: The experiments are limited to a few datasets and the evaluation metrics provided may not fully capture the models performance in practical settings. 3. Theoretical Clarity: The theoretical aspects could be explained in a more accessible manner for readers without a strong theoretical background in machine learning.  Questions and Suggestions for Authors: 1. Interpretability: How can the model ensure interpretability and transparency of the recourse recommendations made to users in practical applications 2. Scalability: How does the proposed threelevel training method scale with increasing dataset sizes and complexity Are there any computational efficiency concerns 3. Generalizability: Have the authors considered how well the proposed approach generalizes to unseen or more diverse datasets with varying environmental settings  Limitations: The authors could further address the limitations of the proposed approach in terms of applicability to largescale realworld datasets and potential performance degradation in scenarios with a wide range of environment variations. Suggestions for improvement would include conducting experiments on more diverse datasets and providing insights on how to handle scalability challenges.", "l5UNyaHqFdO": " Summary: The paper explores the convergence behavior of the popular optimization algorithm Adam specifically focusing on the relationship between its hyperparameters and convergence properties. The authors observe a discrepancy between the theoretical divergence issue of Adam shown previously and its practical success. By analyzing the order in which hyperparameters are selected in theory and practice the authors propose that Adam can converge within fixed settings. They present theoretical results showing that when the 2ndorder momentum parameter \u03b22 is large and 1storder momentum parameter \u03b21  \\xe2\\x88\\x9a \u03b22  1 Adam converges to critical points without the need for modifications in its update rules. Conversely they also show regions where Adam may diverge to infinity providing insights for hyperparameter tuning.  Strengths: 1. Theoretical Contributions: The paper offers significant theoretical contributions by providing clear mathematical proofs and establishing conditions under which Adam can converge and diverge. 2. Novelty: The proposed method for analyzing Adam without the need for assumptions of bounded gradients breaks new ground and enhances the understanding of the algorithm. 3. Experimental Validation: The inclusion of experiments on various tasks like function optimization image classification and natural language processing adds empirical support to the theoretical findings.  Weaknesses: 1. Complexity: The papers highly technical content may limit accessibility for readers without a strong background in optimization theory. 2. Clarity: The dense technical details and extensive formalism could potentially hinder the understanding of the average reader. 3. Generalizability: The findings are anchored in specific scenarios and assumptions limiting their generalizability to all optimization problems.  Questions and Suggestions for Authors: 1. Clarification: Could the authors provide examples or practical scenarios where the proposed theoretical results could be applied for realworld optimization tasks 2. Comparison: How does the convergence behavior of Adam compare with other optimization algorithms especially in scenarios where the conditions for convergence are less straightforward 3. Practical Guidance: How can practitioners effectively utilize the insights provided in the paper to improve their optimization processes using Adam Can the proposed guidelines be easily implemented in typical deep learning applications  Limitations and Suggestions for Improvement: The authors could enhance the practical utility and broader applicability of their work by:  Providing concrete guidelines or algorithms based on the theoretical results for tuning Adam\u2019s hyperparameters.  Offering more relatable examples or case studies to demonstrate the application of the proposed theoretical insights in realworld optimization tasks.", "p-56bnzZhQ7": " Summary: The paper investigates the computational complexity of learning halfspaces in the presence of Massart noise. It introduces a reduction from the Learning with Errors (LWE) problem to the problem of learning Massart halfspaces providing the first computational hardness result for PAC learning halfspaces in the presence of Massart noise.  Strengths:  Novel Contribution: The paper presents the first computational hardness result for learning Massart halfspaces.  Technical Rigor: The proofs are detailed and thorough covering both the reduction process and the analysis of the resulting distributions.  Relevance: The topic addressed is highly relevant in the context of machine learning theory and complexity analysis.  Weaknesses:  Complexity: The paper is inherently deep and might be challenging for nonexperts in the field to follow.  Assumptions: The results heavily rely on the subexponential hardness of the LWE problem which may limit the direct applicability of the findings.  Questions and Suggestions for Authors:  Could you provide further insights into how the reduction from LWE to Massart halfspaces could impact practical machine learning applications  Have you considered exploring additional assumptions or scenarios where the hardness results could be potentially strengthened or relaxed  Can you discuss how the findings of the research might influence the development of new learning algorithms under noise conditions in practice  Limitations: The authors have adequately addressed the potential limitations of their work and its societal impact. One constructive suggestion could be to further explore the practical implications and applications of the computational hardness results in realworld scenarios.", "r70ZpWKiCW": "Peer Review Summary: The paper proposes a novel framework Gentle Teaching Assistant (GTASeg) for SemiSupervised Semantic Segmentation. The framework aims to disentangle the effects of unreliable pseudo labels on the feature extractor and mask predictor of the student model. By introducing a gentle teaching assistant network the model learns representation knowledge from pseudo labels but ensures that the mask predictor is trained solely on groundtruth labels. Extensive experiments on benchmark datasets demonstrate competitive performance against previous methods. Strengths: 1. Innovative Approach: The proposed framework addresses the challenge of unreliable pseudo labels effectively by separating the learning process for the feature extractor and mask predictor. 2. Thorough Experiments: The paper provides extensive experimental results on benchmark datasets demonstrating the effectiveness of the proposed method compared to previous approaches. 3. Visualization Analysis: The inclusion of visualization results enhances the understanding of the models performance and showcases the superiority of the proposed method in capturing object contours and categories. 4. Component Analysis: The detailed analysis of different components such as the gentle teaching assistant and reweighting strategy provides valuable insights into their contributions to the overall performance. 5. HyperParameter Sensitivity: The evaluation of the method under different hyperparameters and warmup epochs showcases the robustness of the approach. Weaknesses: 1. Cost Implication: The addition of a teaching assistant model might increase training costs which should be considered for practical implementations. 2. Limited Focus on Labeled Data: While the emphasis is on leveraging unlabeled data effectively the paper could benefit from exploring ways to better utilize the limited labeled data in the semisupervised setting. Questions and Suggestions for Authors: 1. The authors mentioned a reweighting mechanism for pseudo labels. Could they elaborate on the rationale behind the choice of Laplace Smoothing for this mechanism and discuss potential alternatives 2. How scalable is the proposed framework to larger or more complex datasets beyond the benchmarks considered in the experiments Are there any limitations in terms of scalability 3. Considering the focus on the separation of learning processes for feature extraction and mask prediction could the authors discuss any potential tradeoffs or challenges that may arise in realworld applications with diverse datasets 4. How sensitive is the model performance to changes in the hyperparameters particularly the EMA hyperparameter and warmup epochs Are there any guidelines for selecting these parameters effectively Limitations: The authors have not extensively addressed the potential negative societal impacts of their work. To improve they could consider discussing any biases or ethical considerations related to the dataset or model implementations. Overall the paper presents a novel and promising approach to semisupervised semantic segmentation. By addressing the challenge of unreliable pseudo labels effectively the proposed framework shows competitive performance. Further exploration of leveraging both labeled and unlabeled data efficiently could enhance the methods applicability and robustness in realworld scenarios.", "lNokkSaUbfV": "Summary: The paper presents Masked Decision Prediction (MaskDP) a selfsupervised pretraining method for reinforcement learning. It utilizes a masked autoencoder to reconstruct missing stateaction tokens enabling the model to generalize across various downstream RL tasks. The empirical study demonstrates MaskDPs effectiveness in zeroshot transfer to new tasks and offline RL outperforming existing methods. Strengths: 1. Innovative Approach: MaskDP introduces a novel selfsupervised pretraining method for RL leveraging masked token prediction for generalization. 2. Generalization: The paper demonstrates the models ability to zeroshot transfer to new tasks and excel in offline RL settings. 3. Empirical Validation: Extensive experiments showcase superior performance in single goal reaching multiple goal reaching and offline RL highlighting the methods efficacy. 4. Comparison to Baselines: Clear comparisons against GPTbased methods and behavior cloning demonstrate MaskDPs superiority. Weaknesses: 1. Detailed Explanations: The paper could elaborate more on the rationale behind specific design choices and model architecture. 2. Scalability Discussion: While the paper mentions scalability with model size more detailed analysis on scalability aspects would enhance the depth of the study. 3. Comparative Analysis: Deeper comparisons with existing methods in terms of computational efficiency and sample complexity could provide a more comprehensive evaluation. Questions and Suggestions for Authors: 1. Can the authors provide insights into the interpretability of MaskDP in understanding the learned representations 2. How does MaskDP perform in scenarios with noisy or incomplete data Any strategies to enhance robustness 3. Have the authors considered incorporating attention mechanisms to enhance the models performance in capturing intricate dependencies 4. It would be beneficial to discuss the computational efficiency of MaskDP compared to existing methods shedding light on training time and resource requirements. Limitations and Suggestions for Improvement: While the paper mentions societal impact briefly a more thorough analysis of potential negative societal consequences bias mitigation strategies and ethical considerations should be included for a comprehensive evaluation. Overall the paper presents a promising approach with significant potential in reinforcement learning pretraining. Refinements in explaining design choices scalability discussions and addressing limitations can further enhance the robustness and applicability of MaskDP.", "rZalM6vZ2J": " Peer Review:  Summary: The paper proposes a novel approach DPPCA for computing principal components from i.i.d. data in d dimensions under (\u03b5 \u03b4)differential privacy. The proposed algorithm overcomes key limitations of existing methods achieving nearly optimal statistical error rates even for n  \\xc3\\x95(d). DPPCA leverages minibatch SGD and private mean estimation to adapt to the gradient geometry and maintain privacy. The work provides theoretical analyses bounds and comparisons with existing methods.  Strengths: 1. Novel Algorithm: DPPCA is a unique approach that combines minibatch SGD and private mean estimation to address the limitations of existing private PCA methods. 2. Theoretical Rigor: The paper provides detailed theoretical analyses error bounds and comparisons enhancing the understanding of the proposed algorithms performance. 3. Privacy Considerations: The focus on differential privacy and the incorporation of (\u03b5 \u03b4) mechanisms demonstrate a strong commitment to data privacy in the context of PCA. 4. Impactful Contributions: The improvements in error rates and privacy considerations make the proposed DPPCA algorithm significant for applications involving sensitive data analysis.  Weaknesses: 1. Complexity: The complexity of the proposed algorithm involving multiple subroutines and mechanisms may limit its practical applicability and implementation in realworld settings. 2. Dependency on Assumptions: The reliance on specific assumptions such as the Gaussianlike tail bound may restrict the algorithms applicability to datasets that do not meet the specified criteria. 3. Terminology Density: The paper is densely packed with technical terminology and mathematical expressions which may make it challenging for nonexperts in the field to fully comprehend the methodology and results. 4. Sensitivity to Hyperparameters: The sensitivity of the algorithms performance to hyperparameters like \u03b5 \u03b4 and other constants might require careful tuning for optimal results adding complexity to its practical implementation.  Questions and Suggestions for Authors: 1. Can you clarify the practical implications and potential application scenarios where DPPCA could offer significant advantages over existing private PCA methods 2. How robust is DPPCA to deviations from the assumptions especially regarding the Gaussianlike tail bound Have you considered scenarios where these assumptions do not hold 3. Could you elaborate on the computational efficiency and scalability of DPPCA especially in scenarios with large datasets or highdimensional data 4. Considering the potential negative societal impacts of differential privacy techniques have you discussed ethical considerations and mitigation strategies in the paper  Limitations: While the paper provides insightful contributions to the field of private PCA the complexity of the proposed algorithm reliance on specific assumptions and the need for careful parameter setting pose challenges for practical implementation and broader applicability. Authors should consider addressing these aspects to enhance the algorithms usability and impact. Overall the paper demonstrates a high degree of theoretical rigor and innovation in the field of private PCA and provides a strong foundation for further research and development in the domain of privacypreserving data analysis. Further clarity on practical implications and considerations can enhance the papers impact and relevance. Great work overall", "lUyAaz-iA4u": " Summary: The paper investigates the dynamics and convergence properties of Stochastic Gradient Descent (SGD) equipped with new variants of stochastic Polyak stepsize (SPS) for solving general convex optimization problems. The proposed modifications address the drawbacks of the original SPS providing convergence guarantees to the exact minimizer without requiring a priori knowledge of problem parameters.  Strengths: 1. Conceptual Innovation: The paper introduces novel modifications to SPS addressing key limitations and providing convergence guarantees to the exact solution in nonoverparameterized regimes. 2. Theoretical Grounding: The work is welltheorized demonstrating strong convergence results and performance improvements over existing adaptive methods like Adam and AdaGrad. 3. Experimental Validation: The numerical evaluation section provides detailed experimentation demonstrating the effectiveness of the proposed DecSPS across various datasets and regularization levels.  Weaknesses: 1. Complexity: The paper is dense with technical details potentially making it challenging for readers unfamiliar with the subject area to understand the methodology and contributions. 2. Limited Comparison: While comparisons with existing methods are insightful further comparative analysis with a broader range of stateoftheart optimization algorithms could strengthen the evaluation.  Questions and Suggestions: 1. Clarity: How can the authors simplify the technical explanations to make the paper more accessible to a wider audience without compromising the depth of the research 2. Generalizability: How do the proposed modifications in DecSPS perform on highly nonlinear and nonconvex objective functions commonly encountered in deep learning applications 3. Scalability: Have the authors considered the scalability of DecSPS to largescale optimization tasks involving massive datasets How might the algorithm perform in parallel or distributed computing environments  Limitations: The paper does not adequately address the potential negative societal impact of the proposed optimization methods. To improve the authors could discuss the implications of their work on fairness bias or ethical considerations in machine learning applications.", "wO53HILzu65": "Summary: The paper presents a comprehensive study on recommender system algorithms aiming to address the challenge of selecting the most suitable algorithm and hyperparameters for a given dataset and performance metric. The authors compare 24 algorithms across 85 datasets and 315 metrics highlighting the datasetdependency of algorithm performance. Based on their findings they introduce RecZilla a metalearning approach that predicts the best algorithm and hyperparameters for new datasets. The paper offers valuable insights into the variability of algorithm performance across datasets and the predictability of performance based on dataset metafeatures. Strengths:  Comprehensive Study: The paper conducts a largescale study covering a wide range of algorithms datasets and metrics enhancing the researchs credibility and generalizability.  Innovative Approach: The introduction of RecZilla a metalearningbased algorithm selection method is a novel contribution that addresses a practical challenge in recommender systems.  Open Source and Transparency: The release of code pretrained models and raw experimental results facilitates reproducibility and encourages further research in the field.  Methodological Rigor: The experimental design including dataset splits hyperparameter search and performance evaluation demonstrates careful methodology and attention to detail. Weaknesses:  Limited Comparison with Existing Approaches: While the paper introduces RecZilla as an innovative method a more indepth comparison with existing algorithm selection approaches could provide further context and insights.  Potential Overfitting: The authors acknowledge the lowdata regime in their metalearning problem highlighting the need for additional training data to avoid overfitting.  Bias in Evaluation: The large number of experiments conducted raises concerns about biases stemming from experiment successfailures which could impact the validity of the results. Questions and Suggestions for the Authors: 1. Can you provide further insights into the decisionmaking process behind selecting the specific subset of algorithms included in RecZilla and the criteria used for metafeature selection 2. How do you plan to address the limitations related to overfitting and biases in the dataset especially concerning the uncertainty of results due to experiment failures 3. Could you discuss the practical implications and potential adoption challenges of implementing RecZilla in realworld recommender system applications Limitations: The authors partially address the limitations related to the lowdata regime in their metalearning problem and potential biases in the evaluation. To improve they can seek more training data to mitigate overfitting risks. Moreover they should carefully monitor and correct biases in the dataset to enhance the reliability of their findings and RecZillas predictions.", "lC5-Ty_0FiN": " PeerReview of \"Mechanisms Underlying Benign Overfitting in Deep Neural Networks\"  Summary: The paper delves into the phenomenon of \"benign overfitting\" in deep neural networks (DNNs) where increasing the networks parameters beyond the number of training samples improves generalization performance. The study investigates the last hidden layer representations in various stateoftheart convolutional neural networks to understand this behavior. The authors find that in wide networks with the last hidden representation wide enough neurons split into groups that are statistically independent but convey similar information. This leads to redundant features that may explain benign overfitting. The paper presents quantitative analyses and experiments to support their findings.  Strengths: 1. Originality: The paper addresses a significant and puzzling aspect of DNNs contributing to the understanding of overfitting in deep networks. 2. Rigorous Analysis: The study is methodical employing experiments and quantitative analyses to support the proposed mechanism. 3. Clarity: The explanations and figures aid in understanding the complex concepts presented in the paper.  Weaknesses: 1. Accessibility: The paper could benefit from simplifying technical terms and concepts for readers not wellversed in advanced neural network theories. 2. Visualization: Some figures may be challenging to comprehend without a more detailed legend or annotation. 3. Generalization: The study focuses on specific architectures and datasets exploring a broader range could enhance the papers impact.  Questions  Suggestions for Authors: 1. Clarity on Key Findings: Could the authors provide a succinct summary of the main findings for better comprehension 2. Extensions: How could these findings be applied to different network architectures or tasks to explore more varied scenarios 3. Future Work: Are there plans to investigate the implications of these findings on the training dynamics or optimization algorithms used in DNNs  Limitations: The authors have addressed potential negative societal impacts and discussed the limitations of their work. To enhance the study considering wider societal implications beyond scientific advancement could enrich the paper. Overall the study offers valuable insights into the mechanisms behind benign overfitting in DNNs and with some clarity enhancements and extensions to broader contexts it has the potential to make a notable contribution to the field of deep learning.", "pn5trhFskOt": " Summary: The paper introduces a new evaluation protocol and a novel method Simultaneous Localization and AudioVisual Correspondence (SLAVC) to address the overfitting and negative sample identification issues in weaklysupervised visual sound source localization. The proposed method outperforms prior works achieves stateoftheart performance and is available for further research.  Strengths: 1. Identification of Issues: The paper effectively identifies critical limitations in existing weaklysupervised visual sound source localization methods and proposes a comprehensive evaluation protocol to address them. 2. Novel Method: The proposed SLAVC method is innovative and demonstrates significant improvements in preventing overfitting and effectively identifying negative samples compared to prior approaches. 3. Thorough Evaluation: The paper conducts extensive experiments on popular benchmarks thoroughly evaluates different components of the proposed method and compares results with prior methods to showcase the effectiveness of the approach.  Weaknesses: 1. Complexity: The method proposed while innovative involves several components and techniques that may make it complex to implement and reproduce for researchers who are not familiar with the field. 2. Evaluation Protocol: While the new evaluation protocol is a strength it may not be directly applicable to all scenarios or datasets potentially limiting its generalizability.  Questions and Suggestions: 1. Clarity on Implementation: Can the authors provide more detailed guidance on implementing the SLAVC method particularly regarding the application of dropout and momentum target encoders 2. Comparative Analysis: How does SLAVC perform against stateoftheart methods in terms of computational efficiency and speed during inference 3. Scalability: Are there scalability issues with the proposed method when dealing with larger datasets or in realtime applications  Limitations: The paper adequately addresses the limitations of existing weaklysupervised visual sound source localization methods and proposes a novel solution with improved performance. To enhance the work the authors could provide more insights into the practical implications of implementing SLAVC in realworld scenarios.", "u5oLvX8x4wH": " Summary: The paper addresses the convergence of stochastic gradient descent (SGD) based optimization algorithms focusing on momentumbased SGD (mSGD). It relaxes the step size decay condition and explores two different step size scenarios: decaying as \\xcf\\xb5n  1\\xe2\\x88\\x9an and a constant step size \\xcf\\xb5n \\xe2\\x89\\xa1 \\xcf\\xb5  0. The contributions include proving gradient norm decay for nonconvex loss functions providing convergence speed estimates and establishing convergence under a constant step size without strong convexity assumption. Experimental results on regression and classification tasks affirm the theoretical findings.  Strengths: 1. Theoretical Rigor: The paper provides detailed mathematical analyses and proofs to support the main results demonstrating a strong theoretical foundation. 2. Novel Contributions: The relaxation of step size conditions and the exploration of different scenarios for mSGD convergence contribute to advancing the understanding of optimization algorithms. 3. Experimental Validation: The experimental results on regression and classification tasks validate the theoretical findings and enhance the practical relevance of the study.  Weaknesses: 1. Clarity: The paper can be challenging to follow due to the complexity of the mathematical derivations and technical language making it less accessible to a broader audience. 2. Limited Discussion: The paper could benefit from a more detailed discussion on the implications of the results for practical machine learning applications and potential future research directions. 3. Engagement: While comprehensive the paper may lack engaging elements that can help communicate the significance of the findings effectively.  Questions and Suggestions for Authors: 1. Theoretical Clarifications: Could the authors provide an intuitive explanation of how the relaxed step size conditions impact the convergence behavior of mSGD for readers less familiar with intricate mathematical details 2. Practical Relevance: How do the proposed relaxation of step size conditions and convergence results translate to improving the efficiency and effectiveness of training neural networks in realworld machine learning applications 3. Comparison with Existing Literature: How does the proposed approach compare with previous studies on optimization algorithms in terms of convergence rates computational efficiency and practical implications 4. Future Directions: In which direction do the authors envision extending this work further to enhance the understanding of optimization algorithms or address specific challenges in largescale machine learning problems  Limitations: The paper could benefit from enhancing the discussion on the practical implications of the proposed relaxation of step size conditions and providing clearer insights into the potential applications and impacts of the research findings for the broader machine learning community.", "sipwrPCrIS": " Summary: The paper introduces a novel approach to analyze feature learning in infinitewidth neural networks through selfconsistent dynamical mean field theory. It presents a method to derive deterministic order parameters that capture the network activity during training providing insights into the evolution of hidden activations neural tangent kernels and output predictions. Experimental validation on CNNs using CIFAR classification tasks demonstrates the efficacy of the proposed methodology.  Strengths: 1. Theoretical Novelty: Introducing a path integral formulation to analyze gradient flow dynamics in infinitewidth networks is a significant theoretical advancement. 2. Detailed Methodology: The paper provides a comprehensive derivation of saddle point equations for kernel order parameters showcasing a deep understanding of network dynamics. 3. Experimental Validation: Experiments on CIFAR datasets demonstrate the practical relevance and predictive power of the proposed theory. 4. Comparative Analysis: Comparisons with approximate techniques like gradient independence assumption and perturbation theory provide a robust evaluation of the proposed methods accuracy.  Weaknesses: 1. Complexity: The paper involves intricate mathematical derivations and technical language which may limit accessibility for readers not wellversed in advanced theoretical concepts. 2. Computational Complexity: The theoretical frameworks computational demands are highlighted as a limitation and optimizing the algorithms efficiency could be a point of improvement. 3. Limited RealWorld Applications: While the methodology is theoretically sound further demonstrations or case studies in realworld applications beyond CIFAR classification could enhance the papers practical impact.  Questions and Suggestions for Authors: 1. Clarity for General Audience: Consider adding a section or appendix that simplifies the core concepts for readers from diverse backgrounds. 2. Endpoint Analysis: How does the DMFT approach handle convergence issues if the network reaches a suboptimal state or plateau during training 3. Generalization: Have you explored the applicability of the proposed theory to other types of neural network architectures beyond fullyconnected and CNN models 4. Scalability: How feasible is it to extend this methodology to larger datasets or more complex tasks that involve varying input data distributions  Limitations: The authors should consider: 1. Addressing Computational Overheads: Develop strategies to reduce the computational complexity of the proposed algorithm to make it more scalable for practical implementation. 2. RealWorld Relevance: Discuss potential limitations of applying the proposed theory to scenarios outside controlled experimental settings and suggest avenues for addressing these challenges.", "yKDKNzjHg8N": " Summary: The paper proposes a new metric SecondSplit Forgetting Time (SSFT) to track the epoch after which an example is forgotten during finetuning on a second data partition. The metric distinguishes between mislabeled rare and complex examples. Through experiments on various datasets the paper demonstrates the effectiveness of SSFT in identifying mislabeled samples improving generalization and detecting failure modes. The metric is robust across architectures and optimizers.  Strengths: 1. Innovative Metric: Introduces a novel metric SSFT to characterize example hardness offering a new perspective and complementing existing metrics. 2. Comprehensive Evaluation: Conducts experiments on multiple datasets and modalities providing a thorough evaluation of the proposed metrics efficacy. 3. Practical Implications: Shows practical applications of SSFT in identifying mislabeled samples improving generalization in noisy data settings and identifying failure modes of machine learning models. 4. Theoretical Analysis: Supports empirical findings with theoretical analysis on overparameterized linear models enhancing the papers academic rigor.  Weaknesses: 1. Lack of Comparison Metrics: While the paper introduces SSFT and baseline methods a direct comparison with existing stateoftheart metrics for characterizing example hardness could enhance the studys value. 2. Limited Theoretical Insights: While the theoretical analysis is a strong point further elaboration on the implications of the theoretical results in practical settings can strengthen the paper. 3. Dataset Specificity: The experiments focus on image datasets generalizing the findings to other types of data could enhance the papers applicability and relevance.  Questions and Suggestions for Authors: 1. Can you elaborate on how SSFT compares with other metrics in terms of computational efficiency and interpretability 2. How do the results vary when training on datasets with different characteristics such as imbalanced classes or noisy features 3. Have you considered extending the analysis to largescale datasets to assess the scalability and applicability of SSFT in realworld scenarios  Limitations: The authors partially addressed the limitations by exploring the robustness of SSFT across architectures and optimizers. To further improve they could include a detailed analysis of SSFTs sensitivity to hyperparameters and learning rates and provide practical guidance for optimizing these parameters. This comprehensive review highlights the strengths weaknesses potential improvements and limitations of the paper on characterizing example hardness through the novel SSFT metric.", "ypXcTtbBsnZ": " Summary: The paper introduces UniCLIP a Unified framework for Contrastive Language\u2013Image Pretraining to address the limitations of prior works in image\u2013text pretraining. UniCLIP unifies contrastive objectives between interdomain and intradomain pairs into a single embedding space by introducing augmentationaware feature embedding MPNCE loss and domaindependent similarity measure. Experimental results demonstrate that UniCLIP outperforms existing pretraining methods on various downstream tasks.  Strengths: 1. Innovative Framework: UniCLIP addresses the shortcomings of prior works by unifying contrastive losses across multiple domains contributing to improved data efficiency and feature diversity. 2. Theoretical Foundation: The paper provides a solid theoretical basis for the proposed methodology with detailed explanations of key components like augmentation encoding and domaindependent similarity scores. 3. Experimental Validation: Extensive experiments showcase the effectiveness of each component of UniCLIP demonstrating superior performance in singleand multimodal downstream tasks.  Weaknesses: 1. Complexity: The theoretical discussions especially on loss functions and domainspecific similarity scores may be challenging to follow for readers without a strong background in contrastive learning methods. 2. Evaluation: While the paper extensively evaluates UniCLIPs performance on various tasks additional comparisons with stateoftheart methods and analysis of computational efficiency could strengthen the experimental validation. 3. Clarity: Some sections especially on augmentation encoding and image projection head types would benefit from clearer illustrations or examples to enhance understanding.  Questions and Suggestions: 1. Could you elaborate on the computational overhead of UniCLIP compared to existing pretraining methods 2. How robust is UniCLIP to variations in dataset sizes and characteristics particularly in scenarios with limited annotated data 3. Have you considered exploring the interpretability of the learned representations in UniCLIP for tasks like image\u2013text retrieval or zeroshot classification  Limitations: The paper adequately addresses the limitations and societal impact of the work. To improve authors could consider discussing potential ethical implications such as biases in data or model decisions and how they plan to mitigate these concerns in future research.", "rQAJmrLmGC6": " Review of the Paper 1) Summary: The paper introduces TURN a novel architecture for zeroresource sounding object localization. It leverages a twostream approach with a localization stream to extract knowledge from image grounding and an alignment stream to align different data domains. TURN achieves competitive performance on benchmarks like VGGSS and MUSIC without using any training data in the sounding object localization field. 2) Assessment of Strengths and Weaknesses: Strengths:  Innovative Approach: The paper introduces a novel architecture for zeroresource sounding object localization which bridges the gap between different modalities effectively.  Effective Knowledge Transfer: Leveraging data from image grounding and audiotext datasets TURN successfully transfers knowledge to the sounding object localization task.  Competitive Performance: Comprehensive experiments show that TURN achieves competitive performance compared to stateoftheart approaches on benchmark datasets. Weaknesses:  Evaluation Metrics: The paper primarily focuses on cIoU and AUC metrics additional metrics or comparisons could provide a more comprehensive evaluation.  Lack of Failure Analysis: While success cases are discussed failure cases and limitations should also be addressed for a balanced assessment.  Complexity: The model architecture and training process while detailed may present challenges in implementation and scalability. 3) Questions and Suggestions for the Authors:  Can you provide insights on the computational complexity of TURN compared to existing models  How does TURN perform with respect to computational efficiency and training convergence  Have you considered realworld applications or scenarios where TURN could be deployed  How does TURN handle scenarios with multiple objects emitting the specified sound 4) Suggestions for Improvement:  Include a discussion on the potential negative societal impact of the proposed solution.  Provide more detailed insights on the limitation of the proposed method in handling complex realworld scenarios.  Consider conducting additional experiments comparing TURN with models utilizing different learning paradigms or architectures for a more thorough evaluation. 5) Limitations: While the paper provides a detailed analysis of the proposed TURN model it falls short in discussing the potential negative societal impact of the technology. Authors should consider including this aspect and provide suggestions for addressing any societal risks that may arise from the implementation of TURN.", "n7XbkHOwKn6": "PeerReview of \"CogVideo: LargeScale Pretrained Transformer for TexttoVideo Generation\" Summary: The paper introduces CogVideo a 9Bparameter transformer model for texttovideo generation. It inherits knowledge from a pretrained texttoimage model and uses a multiframerate hierarchical training strategy to align text and video clips. CogVideo outperforms existing models in both machine and human evaluations. Strengths: 1. Innovative Approach: Leveraging pretrained texttoimage models for texttovideo generation is a novel approach. 2. Scalability: CogVideo is a largescale model that shows promising results in both quantitative and qualitative evaluations. 3. Hierarchical Training: The multiframerate hierarchical training strategy improves alignment between text and video semantics. Weaknesses: 1. Ethical Considerations: While the paper briefly mentions the societal impact and ethical implications of deepfake technology further exploration and mitigation strategies could be discussed. 2. Reproducibility: Lack of released code data and instructions for reproducing experimental results limits the transparency and reproducibility of the study. Questions and Suggestions: 1. Dataset Quality: How was the quality of the 5.4 million captioned videos dataset ensured 2. Training Efficiency: How long did the training process take and what computational resources were used 3. Model Generalization: How well does CogVideo generalize to unseen action classes or scenarios outside the training dataset Limitations: Authors adequately acknowledge the restrictions on input sequence length due to the models scale. One constructive suggestion is detailing potential solutions to circumvent this limitation for future work. Societal Impact: Authors acknowledge the risk of misinformation and suggest training a classifier for discrimination. To further enhance the societal impact assessment authors could explore additional strategies for addressing misinformation risks and ensuring responsible model deployment. In conclusion the paper presents a significant advancement in texttovideo generation with CogVideo. Improving reproducibility further discussing societal implications and addressing limitations can enhance the impact and credibility of the research.", "yam42JWePu": " Summary: The paper introduces LOUPE a visionlanguage pretraining framework that explicitly learns finegrained semantic alignment from raw imagetext pairs through gametheoretic interactions. It also proposes an uncertaintyaware neural Shapley interaction learning module to efficiently compute the interactions. Experimental results demonstrate LOUPEs stateoftheart performance on various visionlanguage tasks including imagetext retrieval object detection and visual grounding.  Strengths: 1. Innovative Approach: Novel perspective of using gametheoretic interactions for finegrained semantic alignment sets this work apart. 2. Efficient Computation: The proposed uncertaintyaware neural Shapley interaction learning module efficiently computes interactions saving computational cost. 3. StateoftheArt Results: LOUPE achieves top performance on various visionlanguage tasks without objectlevel annotations or finetuning. 4. Broad Applicability: Demonstrates the potential of learning finegrained semantics from raw imagetext pairs for a wide range of downstream tasks.  Weaknesses: 1. Limited Generalization: The system is trained on noisy Internet data potentially containing inappropriate content which may hinder generalization to realworld applications. 2. Model Limitations: The dependence on offtheshelf parsers for phrase extraction may introduce inaccuracies in the learned semantic alignment.  Questions  Suggestions for Authors: 1. Data Analysis: How did you mitigate potential biases or inappropriate content in the Internet data used for training 2. Evaluation Metrics: Can you elaborate on the specific criteria used to evaluate the performance of LOUPE in comparison to existing methods 3. Human Supervision: How scalable is the proposed framework in scenarios where human supervision might be necessary for correct labeling or alignment  Limitations  Recommendations: The authors could enhance the paper by elaborating on the strategies employed to address potential negative societal impacts such as privacy concerns or unsuitable content in the training data. They should provide clear guidelines to ensure responsible deployment of their model in realworld applications. Additionally it would be beneficial to outline steps for validation and ethical considerations in utilizing the model.  Overall: The paper presents an innovative approach to learning finegrained semantic alignment in visionlanguage pretraining. The thorough experimentation and performance evaluation showcase the effectiveness of LOUPE. However addressing potential biases and ethical considerations would further strengthen the impact and applicability of the proposed framework.", "wxWTyJtiJZ": "Summary: The paper proposes a consumer choice model for multiplepurchase scenarios in online retailing to optimize product ranking for revenue maximization. The MultiplePurchasewithBudget UCB (MPBUCB) algorithms are introduced to estimate consumer behaviors and maximize revenue in online settings. Experiments on synthetic and semisynthetic datasets validate the effectiveness of the proposed algorithms. Strengths: 1. Original Contribution: The paper addresses a gap in existing literature by considering a more realistic multiplepurchase setting in consumer choice modeling. 2. Algorithm Design: The formulation of the MPBUCB algorithms is thoughtful and can effectively balance exploration and exploitation. 3. Theoretical Analysis: The regret bounds for the proposed algorithms are wellestablished and provide theoretical support for their performance. 4. Experimental Validation: Experiments on both synthetic and realworld data demonstrate the effectiveness of the proposed algorithms in practical scenarios. Weaknesses: 1. Complexity: The model and algorithms presented in the paper may be challenging to implement in realworld online retail systems due to their complexity. 2. Sensitivity to Parameters: The proposed algorithms may be sensitive to the estimation of consumer parameters and may require careful tuning for optimal performance. 3. Limited Comparison: The comparison with baselines focuses mainly on the proposed method and explorethenexploit strategies lacking comparison with a wider range of existing algorithms. Questions and Suggestions: 1. Would the proposed algorithms be computationally feasible for largescale online retail applications with high consumer traffic 2. How robust are the algorithms to variations in consumer behavior patterns and changes in market dynamics over time 3. Have you considered the potential ethical implications of using such algorithms in online retailing especially in terms of consumer privacy and data exploitation Limitations: The authors should further address: 1. Ethical implications of deploying the algorithms in online retail environments ensuring consumer privacy and data protection. 2. The need for easytoimplement solutions for practical deployment in realworld online retail platforms. 3. The importance of continuous monitoring and updates based on evolving consumer behavior trends to maintain algorithm effectiveness.", "v7SFDrS44Cf": " Peer Review of \"FLEXSUBNET: Flexible Neural Models for Learning Submodular Functions\"  Summary The paper proposes FLEXSUBNET a family of flexible neural models for learning submodular functions including monotone nonmonotone submodular functions and monotone \u03b1submodular functions. The proposed models are trained under two supervision regimes: (set value) pairs and (perimeterset highvaluesubset) pairs. The paper introduces a novel permutation adversarial training method for differentiable subset selection. Extensive experiments show that FLEXSUBNET outperforms baselines on synthetic and real datasets.  Strengths 1. Novel Contributions: The paper introduces trainable parameterized families of submodular functions under two supervision regimes providing a flexible and expressive approach. 2. Theoretical Foundation: The paper includes detailed theoretical characterizations of submodular and \u03b1submodular functions extending the current understanding in the field. 3. Methodological Innovation: The proposal of a novel permutation adversarial training method for subset selection addresses a critical issue and enhances the learning process. 4. Experimental Rigor: The experiments on both synthetic and real datasets demonstrating the superior performance of FLEXSUBNET establish the effectiveness of the proposed approach.  Weaknesses 1. Complexity: The models introduced and the optimization procedures outlined are complex which may hinder easy implementation and understanding especially for practitioners not deeply familiar with submodular functions. 2. Baseline Comparison: The comparison with stateoftheart methods could be further strengthened by including additional relevant baselines to provide a more comprehensive assessment of the proposed approachs performance.  Questions and Suggestions for Authors 1. Could the authors provide insights into potential scenarios or applications where the proposed FLEXSUBNET models could be particularly beneficial compared to existing approaches 2. Considering the complexity of the proposed models how does the training time and computational requirements compare to traditional methods for learning submodular functions 3. Have the authors considered conducting experiments on larger and more diverse datasets to further evaluate the scalability and generalizability of FLEXSUBNET 4. Is there any consideration for releasing the code implementation of FLEXSUBNET for the research community to facilitate reproducibility and future research endeavors  Limitations The paper acknowledges the limitations of focusing on just two supervision regimes and suggests extensions for future work. However the discussion on limitations could be expanded to cover potential challenges in model interpretability and scalability to larger datasets. Overall the paper presents a strong contribution to the field of learning submodular functions with innovative methodologies and promising results. By addressing the weaknesses and answering the questions posed the authors can further enhance the impact and credibility of their work.", "vDeh2yxTvuh": " Peer Review  Summary: The paper provides a systematic comparison of two popular flatminima optimizers Stochastic Weight Averaging (SWA) and SharpnessAware Minimization (SAM) across various domains in deep learning. The authors explore the properties of the minima found by each method conduct benchmarking experiments and highlight surprising findings. The study aims to aid researchers in developing better optimizers and practitioners in choosing the right optimizer.  Strengths: 1. Comprehensive Comparison: The paper conducts an indepth comparison of the two optimization methods considering loss surface behaviors generalization performance and flatness measurements. 2. Rigorous Benchmarking: The study covers diverse domains including computer vision natural language processing and graph representation learning providing a holistic view of the optimizers performance. 3. Novel Findings: The paper uncovers interesting insights such as the improved generalization with WeightAveraged SharpnessAware Minimization (WASAM) and the differences in optimization outcomes across different tasks and architectures.  Weaknesses: 1. Limited Investigation on Negative Impacts: The authors do not discuss potential negative societal impacts of their work which is essential for responsible research. Considering ethical implications is crucial for deep learning advancements. 2. Lack of Hyperparameter Tuning Details: Although the authors mention hyperparameter tuning the specific details of the tuning process are not provided. Clear documentation of hyperparameter selections enhances reproducibility and validation of results. 3. Clarity and Organization: The paper includes complex technical details and analysis which could be challenging for readers not deeply involved in the field. Simplifying the presentation of results can improve accessibility.  Questions and Suggestions for Authors: 1. Ethical Considerations: Have the authors explored any potential unintended consequences or negative impacts of promoting specific optimizers in research and industrial applications 2. Hyperparameter Tuning Transparency: Can the authors provide a detailed breakdown of the hyperparameter tuning process Sharing the tuning protocol and outcomes can enhance the studys credibility. 3. Clarity for NonTechnical Audience: Considering potential readers with varied expertise levels could the authors provide more accessible explanations visual aids or summaries for key findings to improve overall comprehension  Limitations: The authors have not thoroughly addressed the potential negative societal impacts of their work. To enhance ethical responsibility they should explicitly discuss these implications and incorporate suggestions for mitigating adverse effects. Additionally improving transparency in hyperparameter tuning details and enhancing readability for a broader audience could strengthen the papers impact and reproducibility.", "pF5aR69c9c": " Reviewer Comments  Summary: The paper introduces a novel constrained optimization method for policy gradient reinforcement learning called MemoryConstrained Policy Optimization (MCPO). The method utilizes a virtual trust region formed by a dynamic virtual policy constructed from past policies to regularize each policy update. Through experiments on various environments including robotic locomotion navigation with sparse rewards and Atari games MCPO consistently outperforms recent onpolicy constrained policy gradient methods.  Strengths:  Innovative Approach: The introduction of a dynamic virtual policy and the attention mechanism for building trust regions is a novel contribution to the policy gradient reinforcement learning domain.  Comprehensive Experiments: The paper provides extensive experimental results demonstrating the effectiveness of MCPO across various tasks highlighting its competitive performance compared to existing methods.  Clear Explanation: The paper is wellstructured with detailed explanations of the proposed method algorithms and experimental setups aiding in understanding the novelty and implementation of MCPO.  Weaknesses:  Lack of Comparative Studies: While the experiments demonstrate the superiority of MCPO over baseline methods additional comparisons with stateoftheart approaches or ablation studies with a more diverse set of baselines could strengthen the evaluation.  Hyperparameter Sensitivity Analysis: The sensitivity of key hyperparameters such as \u03b2 is mentioned but not extensively explored. A deeper dive into the effect of tuning these hyperparameters could enhance the understanding of MCPOs performance.  Questions and Suggestions for Authors: 1. Generalization: How does MCPO generalize to different environments or tasks not covered in the current experiments It would be interesting to see how robust the method is across a wider range of scenarios. 2. Computational Efficiency: Given the attention network and memory storage what are the computational requirements of MCPO compared to baseline methods It would be helpful to have insights into the computational overhead introduced by these components. 3. Effectiveness of Attention Mechanism: Could the authors provide more insights into the effectiveness of the attention mechanism in learning to attend to relevant past policies for constructing the virtual policy Understanding the attention patterns could shed light on the mechanisms role in performance improvement.  Limitations: The authors adequately discussed the limitations of their work such as the introduction of new hyperparameters and the need for further exploration of optimal values. They also acknowledged potential societal impacts related to unsafe exploration and malicious misuse of the method highlighting the need for responsible deployment.  Suggestions for Improvement: To enhance the paper the authors could consider providing:  A broader comparison with additional recent stateoftheart methods in the field.  Further analysis on the computational complexity and scalability of MCPO.  Detailed insights into the attention mechanisms performance and its contribution to the methods success. Overall the paper presents a promising framework for policy gradient reinforcement learning and offers valuable contributions to the field. Addressing the above suggestions could further strengthen the impact and applicability of the proposed MCPO method.", "wfel7CjOYk": "Summary: The paper proposes a novel mechanism AllInOne Neural Composition (FLANC) for resourceadaptive federated learning to address the issue of system heterogeneity in Federated Learning (FL). The mechanism enables the construction of models at different complexities using a shared neural basis allowing clients with varying computation resources to contribute effectively. Extensive experiments on popular FL benchmarks demonstrate the superior performance of FLANC in various heterogeneous setups. Strengths: 1. Innovative Mechanism: The AllInOne Neural Composition mechanism is innovative and addresses a critical issue in FL that has been largely underexplored. 2. Extensive Experiments: The paper provides comprehensive experimental results on diverse datasets and scenarios showcasing the effectiveness and efficiency of the proposed mechanism. 3. Comparison with StateoftheArt: The paper compares FLANC with existing methods highlighting its superior performance especially in dealing with data and system heterogeneity. 4. Thorough Technical Details: The paper provides detailed technical descriptions and algorithmic implementations enhancing the reproducibility of the study. 5. Clear Structure: The paper is wellstructured with sections clearly outlining the problem solution method and experimental results. Weaknesses: 1. Theoretical Analysis: While the practical performance is demonstrated through experiments more theoretical analysis explaining the underlying principles of FLANC would enhance the papers scientific rigor. 2. Limited RealWorld Scenario Discussion: The paper focuses on experimental results but discussing realworld implications and potential applications in various industries would strengthen its relevance. 3. Complexity: The papers complexity might hinder accessibility for a broader audience and simpler explanations or illustrative examples could improve readability. 4. Ethical Considerations: The paper lacks discussions on the ethical implications of the proposed mechanism such as privacy concerns or bias in model training on diverse datasets. Questions and Suggestions: 1. Can you elaborate on the scalability of FLANC to larger and more complex datasets or tasks 2. How does FLANC handle issues related to data privacy and security in federated learning environments 3. Have you considered the implications of FLANC on energy consumption and environmental sustainability in resourceadaptive FL scenarios 4. Could further comparisons with other neural network decomposition techniques or federated learning approaches enhance the papers insights and contributions Limitations and Suggestions for Improvement: To enhance the paper authors could address the limitations by: 1. Providing a more thorough discussion on the potential societal impact of FLANC considering ethical considerations and data privacy concerns. 2. Including case studies or scenarios showcasing the practical application of FLANC in realworld settings to demonstrate its broader impact and usability.", "n3lr7GdcbyD": " Peer Review 1) Summary: The paper introduces a variant of the MonteiroSvaiter (MS) acceleration framework that eliminates the need to solve an expensive implicit equation at each iteration. The new algorithm improves the complexity of convex optimization by a logarithmic factor for Lipschitz pth derivatives matching the lower bound. Additionally the authors develop an MS subproblem solver that does not require knowledge of problem parameters and implement it as either a second or firstorder method using exact linear system solution or MinRes respectively. The proposed method outperforms previous secondorder acceleration schemes on logistic regression but underperforms Newtons method. 2) Strengths and Weaknesses:  Strengths:  The paper addresses an important problem in convex optimization and proposes a novel algorithm that shows promising results on logistic regression.  Theoretical contributions are robust with clear definitions and evidence of complexity bounds and convergence rates.  The comparison with existing methods and the experimental evaluation on various datasets add credibility to the results.  Weaknesses:  Practical performance limitations are acknowledged but not fully understood especially compared to the exceptional performance of Newtons method in some scenarios.  The complexity of the proposed algorithm may restrict its scalability to very large datasets.  The lack of tunable parameters might limit the adaptability of the algorithm to different scenarios. 3) Questions and Suggestions for Authors:  Can the authors elaborate on the potential reasons for Newtons method outperforming the proposed method on logistic regression Is there a plan to investigate this further and potentially enhance the algorithms performance in such cases  Considering scalability limitations have the authors considered methods to adapt the algorithm for large datasets such as utilizing cheap stochastic gradient estimates or other approximations to reduce computational complexity  Given the sensitivity of the algorithms performance to different scenarios as observed in experiments would adding some level of tunability or adaptability improve its robustness 4) Limitations and Societal Impact: The authors have pointed out limitations related to practical performance and scalability. It is crucial for the authors to continue addressing these limitations to ensure the practical applicability of the proposed algorithm. To mitigate potential negative societal impacts the authors should focus on expanding the algorithms capabilities for wider use on realworld datasets while maintaining efficiency and accuracy. Overall the paper presents valuable advancements in optimization algorithms but further work is required to address practical limitations and enhance societal applicability. Further investigations and improvements on scalability adaptability and performance on specific problem domains could significantly strengthen the algorithms utility in realworld applications.", "pBz3h8VibKY": " Peer Review  1) Summary: The paper addresses the problem of InteractionGrounded Learning (IGL) where an agent learns to interact with the environment without explicit reward signals. The proposed approach ActionInclusive IGL (AIIGL) allows for feedback vectors to contain action information and implements a contrastive learning algorithm to ground the latent reward. The paper provides theoretical guarantees and largescale experiments showing the effectiveness of the AIIGL approach.  2) Strengths and Weaknesses: Strengths:  The paper tackles an important problem in reinforcement learning  learning without explicit rewards.  The proposed AIIGL algorithm is theoretically sound providing guarantees for policy optimization.  Experimental results demonstrate the effectiveness of the approach on both OpenML datasets and a simulated BCI experiment.  The contrastive learning approach for grounding latent rewards is a novel and promising solution. Weaknesses:  The paper could benefit from a more detailed discussion and comparison with existing methods in IGL.  The practical implementation details of the proposed algorithm are not extensively covered.  The limitations of the approach could be more explicitly discussed especially regarding performance stability in realworld applications.  3) Questions and Suggestions:  Could a comparative study with existing IGL methods showcase the specific advantages of the AIIGL approach  How does the AIIGL approach handle noisy or inaccurate feedback information especially in realworld scenarios  Can the authors provide more insights into the computational complexity and scalability of the proposed algorithm for largerscale applications  Is the AIIGL approach adaptable to different types of environments beyond those discussed in the paper and if so are there specific considerations to be made  4) Limitations: The authors have not explicitly addressed the potential negative societal impact of the work. It would be beneficial to include a discussion on potential ethical considerations such as the impact on data privacy and the implications of using brain signal data to infer actions and rewards. Providing suggestions for mitigating any negative impacts in the ethical and social context is recommended. Overall the paper presents a novel and wellstructured approach to handling interactiongrounded learning challenges. Addressing the aforementioned points could further enhance the clarity and impact of the research findings.", "odOQU9PYrkD": "Peer Review Draft:  Summary: The paper focuses on addressing the limitation in image segmentation algorithms that fail to account for noise ambiguity in data and disagreements in expertsegmented images. The authors propose a novel approach to solve this issue by decomposing the overall predicted uncertainty into smaller uncertainty components. They achieve this by leveraging a lowrank Gaussian distribution for the logits in stochastic segmentation networks (SSNs). By interpreting the distribution as a factor model the authors introduce latent variables (factors) to control individual uncertainty components. Through factor rotations they structure the uncertainty to achieve simplicity nonredundancy and separability among components. The paper introduces flow probabilities to quantify deviations from the mean prediction aiding in uncertainty visualization. Experimental results across medical imaging Earth observation and trafficscene data demonstrate the effectiveness of the rotation criteria based on factorspecific flow probabilities.  Strengths: 1. Novelty and Contribution: The paper addresses an important issue in image segmentation and introduces a novel method to decompose predicted uncertainty into interpretable components offering a finer control for sampling and adjustment. 2. Clarity and Structure: The paper is wellorganized and provides a clear explanation of the proposed methodology highlighting the significance of factor rotations in structuring uncertainty components. 3. Experimental Validation: The authors provide thorough experimental validation across diverse datasets showcasing the effectiveness of the proposed approach in improving finegrained sample control and uncertainty visualization in image segmentation tasks.  Weaknesses: 1. Technical Details and Complexity: The paper delves into technical details of factor modeling and flow probabilities which might be challenging for readers unfamiliar with the specific domain or methodologies. Simplifying complex information could enhance accessibility. 2. Comparison and Benchmarking: While the paper clearly explains the proposed methodology a comparison with existing stateoftheart methods or benchmarking against alternative approaches could strengthen the evaluation of the proposed technique. 3. Interactive Implementation: The authors acknowledge that the computation time for flowprobabilitybased rotations may limit interactive use. Providing strategies to mitigate this limitation or discussing practical implications for realtime applications would be beneficial.  Questions and Suggestions: 1. Clarification on Rotation Criteria Evaluation: Provide a more detailed explanation of how different rotation criteria impact the sensitivity and specificity of the segmentation uncertainty components. What specific metrics can be used to quantify the efficacy of the rotations 2. Generalization of the Approach: Discuss potential applicability of the proposed method beyond SSNs. How transferable is the structured uncertainty approach to other machine learning tasks or algorithms utilizing multivariate Gaussian distributions 3. Effectiveness in Realworld Scenarios: How does the proposed method perform in scenarios with significant label noise or high ambiguity levels Can the approach be further refined to handle extreme cases of uncertainty in image segmentation  Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. To enhance the social relevance of the research it would be beneficial for the authors to discuss possible limitations or ethical considerations associated with the deployment of their methodology in realworld applications.  Overall the paper makes a significant contribution by introducing a structured approach to handling uncertainty in image segmentation. By addressing the identified weaknesses and addressing the provided questions and suggestions the authors can further enhance the clarity applicability and impact of their research.", "yJEUDfzsTX7": "Summary: The paper introduces a novel contribution in risksensitive reinforcement learning by providing the first regret bounds for reinforcement learning under a general class of risksensitive objectives including the popular CVaR objective. The theory is based on a new characterization of the CVaR objective and an innovative optimistic MDP construction. Strengths: 1. Novel Contribution: The paper addresses an important and emerging area in reinforcement learning by focusing on risksensitive objectives. 2. Regret Bounds: The authors provide the first regret bounds for risksensitive reinforcement learning filling a significant gap in the existing literature. 3. Theoretical Foundations: The paper presents a solid theoretical foundation with detailed derivations of regret bounds and key lemmas. 4. Broad Applicability: The developed theory applies to a wide range of risksensitive objectives beyond CVaR enhancing its practical relevance. Weaknesses: 1. Complexity: The paper involves advanced mathematical concepts and derivations which may be challenging for readers not wellversed in these areas. 2. Limited Empirical Validation: The experiments section lacks detailed realworld demonstrations or broader applications focusing primarily on a simulated Frozen Lake problem. 3. Assumption Clarity: Some assumptions made in the theoretical analysis could be further explained or motivated to enhance understanding. Questions and Suggestions: 1. Clarification on Optimistic MDP Construction: Can the authors provide more insights into the intuition and details behind the construction of the optimistic MDP for the upperconfidencebound strategy 2. Extension to RealWorld Applications: How applicable is the proposed approach to realworld safetycritical scenarios like healthcare or autonomous systems Consider discussing potential practical implications and challenges. 3. Comparison with Existing Algorithms: It would be beneficial to include a more detailed comparison with existing risksensitive reinforcement learning algorithms to showcase the unique strengths of the proposed method. 4. Empirical Evaluation: Consider extending the experimental section with additional scenarios or benchmarks to provide a comprehensive evaluation of the proposed approachs performance. Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To improve they could consider discussing ethical considerations potential risks and biases that may arise from implementing their algorithms in safetycritical domains.", "prKLyXwzIW": " PeerReview: 1. Summary and Contributions: The paper introduces a robust estimation algorithm in the Generalized Method of Moments (GMM) framework to handle adversarially corrupted samples. The algorithm is computationally efficient provides recovery guarantees and is applied to instrumental variables linear regression with heterogeneous treatment effects. The contributions include extending the SEVER algorithm theoretical analysis and experimental validation on real and synthetic datasets. 2. Strengths:  The paper addresses an important problem in statistics and econometrics by developing a robust GMM estimator against adversarially corrupted data.  The algorithm provides computational efficiency recovery guarantees and performs well in experiments showing superiority over baseline methods.  The theoretical analysis is comprehensive providing insights into the methods performance and robustness properties. 3. Weaknesses:  The paper lacks a detailed comparison with existing stateoftheart robust GMM estimators to showcase the specific improvements brought by the proposed algorithm.  The experimental section could benefit from a more indepth discussion on the algorithms behavior across various datasets and scenarios to provide a deeper understanding of its practical implications.  The exposition of the technical content could be challenging for readers unfamiliar with algorithmic robust statistics warranting clearer explanations. 4. Questions and Suggestions for Authors:  Can the authors provide insights into the choice of hyperparameter values in the experiments and their impact on the algorithms performance  How does the proposed algorithm scale with increasing dimensions in realworld datasets compared to traditional GMM estimators  It would be beneficial to discuss the scenarios where the proposed method may have limitations or cases where it may not be suitable for application. 5. Limitations:  The authors should provide a more explicit discussion on the practical limitations and negative societal impacts of their work especially in scenarios where the assumptions of the algorithm may not hold in realworld data settings. Overall the paper presents a significant advancement in robust statistics and econometrics showcasing a promising algorithm for handling adversarially corrupted data. Further refinements in the comparative analysis and potential limitations could enhance the papers completeness and impact.", "u6p_NvZ23qt": " Peer Review  Summary: The paper presents a new generalization bound that applies to linear predictors in Gaussian space under various conditions and continuous losses. The authors recover the \"optimistic rate\" of Zhou et al. (2021) for linear regression with the square loss and extend it to handle model misspecification. The significance of the work lies in its broader application beyond proportional scaling regimes and its potential impact on highdimensional linear regression and classification settings.  Strengths: 1. Novel Contribution: The generalization bound proposed in the paper is a significant contribution to the field extending existing theories to handle various scenarios in linear regression and classification settings. 2. Theoretical Depth: The paper is theoretically sound backed by rigorous mathematical analyses and proofs. The authors effectively blend existing techniques with their novel approach to provide comprehensive results. 3. Clarity and Structure: The paper is wellorganized with clear section headings making it easy to follow the flow of the research and the development of the key ideas. 4. Application Scenarios: The results are tested and validated through numerical simulations showcasing the wide applicability of the theory in diverse feature distributions and labelgenerating processes.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers without a strong mathematical background to fully comprehend the nuances of the proposed generalization bound. 2. Limited Practical Implementation: Although the theoretical contributions are significant the paper lacks discussion on the practical implications and realworld implementation challenges of the proposed technique. 3. Gaussian Assumption: The reliance on Gaussian data assumption may limit the generalizability of the results to datasets with nonGaussian distributions which could impact the practical utility of the theory in realworld applications.  Questions and Suggestions for Authors: 1. Generalization Beyond Gaussian Assumption: Have you considered extending the theory to accommodate nonGaussian data distributions or exploring the robustness of the generalization bound in realistic data scenarios 2. Practical Implementation: How can the proposed generalization bound be practically implemented in machine learning tasks Consider discussing potential challenges and strategies for applying the theory to realworld datasets. 3. Comparative Analysis: How does the performance of your generalization bound compare with existing stateoftheart techniques in terms of prediction accuracy computational efficiency and robustness 4. Future Directions: Are there plans to extend the theoretical framework to multiclass classification or complex network architectures beyond linear models Discuss potential future research directions to broaden the applicability of the proposed theory.  Limitations: The authors have not adequately addressed the limitations of their work particularly the practical challenges of applying the theory in nonGaussian settings. It is recommended to provide constructive suggestions for improving the generalizability and potential negative societal impacts of the research.", "x2WTG5bV977": "Peer Review 1) Summary: The paper proposes a novel metric the diversity coefficient to measure task diversity in fewshot learning benchmarks. The study compares ModelAgnostic MetaLearning (MAML) and transfer learning under fair conditions and shows that in low diversity regimes these methods have equivalent performance. The paper conducts empirical analyses on popular benchmarks MiniImagenet and Cifarfs highlighting the importance of problemcentric approaches in metalearning. 2) Strengths and Weaknesses: Strengths:  Novel metric: The diversity coefficient provides a valuable tool for analyzing the intrinsic variability of tasks in fewshot learning benchmarks.  Fair comparison: Utilizing the same architecture optimizer and model training conditions for MAML and transfer learning enhances the validity of the comparison.  Thoughtful analysis: The studys problemcentric approach offers insights into the performance of metalearning algorithms in low diversity settings. Weaknesses:  Lack of reproducibility: Missing code data and instructions for reproducing the main experimental results limit the studys replicability.  Limited discussion on societal impact: While mentioning potential negative societal impacts is essential this aspect seems overlooked in the current version.  Incomplete documentation: The absence of participant instructions and discussion on participant risks in human subject research may raise concerns regarding ethical considerations. 3) Questions and Suggestions:  Can you provide the code data and instructions needed to reproduce the main experimental results either in supplemental material or as a URL to enhance the papers reproducibility  Given the focus on task diversity have you considered evaluating the generalizability of the diversity coefficient across different fewshot learning benchmarks  How do you plan to address the limited discussion on societal impact in the paper and what steps will be taken to ensure ethical considerations are appropriately addressed in future research endeavors 4) Limitations: The authors adequately discussed the limitations of their work including lack of reproducibility due to missing code and data. Constructive suggestions for improvement include sharing code data and instructions to ensure research integrity and enhancing the ethical considerations and societal impact discussions in future studies. Overall the paper presents a thoughtprovoking analysis of metalearning algorithms in fewshot learning contexts emphasizing the importance of problemcentric frameworks and introducing a novel metric for benchmark evaluation. Addressing the identified limitations will strengthen the papers impact and contribute to the fields advancement.", "nLKkHwYP4Au": " Paper Summary: The paper introduces CAGroup3D a novel twostage fully sparse convolutional 3D object detection framework. It addresses the challenges of detecting 3D objects from unordered sparse and irregular point clouds. The proposed method includes a classaware 3D proposal generation module and a RoIConv pooling module for feature refinement. CAGroup3D achieves remarkable performance gains on ScanNet V2 and SUN RGBD datasets and outperforms existing methods in terms of mAP0.25.  Strengths: 1. Innovative Approach: The classaware local grouping strategy and RoIConv pooling module are novel and effectively address the challenges of 3D object detection from point clouds. 2. Improved Performance: The paper demonstrates significant performance gains on challenging indoor datasets showcasing the effectiveness of the proposed framework. 3. Thorough Experiments: The authors conduct extensive ablation studies demonstrating the impact of individual components and providing insightful analysis. 4. Clear Presentation: The paper is wellstructured with detailed descriptions of the methodology experiments and results. Figures and tables support the explanations effectively.  Weaknesses: 1. Limited Comparison: While the paper compares with stateoftheart methods a more extensive comparison with a wider range of existing approaches could provide a more comprehensive evaluation. 2. Assumptions: The paper assumes certain threshold values and hyperparameters without detailed justification. Providing more insights into the rationale behind these choices could enhance the papers credibility.  Questions and Suggestions for Authors: 1. Could the authors elaborate on how the proposed classaware 3D proposal generation module handles cases where objects of the same class vary significantly in size within a scene 2. How does the proposed RoIConv pooling module perform in scenarios with complex and overlapping objects Have these scenarios been investigated in the experiments 3. Can the authors provide insights into the practical implementation aspects of CAGroup3D specifically regarding memory and computational requirements for realworld applications  Limitations: The paper does not explicitly address the potential negative societal impact of the proposed 3D object detection framework. To enhance the societal relevance of the work the authors could consider discussing the ethical implications and privacy concerns associated with deploying such advanced detection systems.", "wu1Za9dY1GY": "Abstract: The paper investigates the alignment between graph neural networks (GNNs) and dynamic programming (DP) through methods from category theory and abstract algebra. It establishes a deep connection between the two going beyond previous observations and offers novel insights into building betteraligned GNN architectures for edgecentric tasks. The paper presents empirical results on the CLRS algorithmic reasoning benchmark and aims to lay the groundwork for stronger algorithmically aligned GNNs. Strengths: 1. The paper introduces a novel approach using category theory and abstract algebra to establish a connection between GNNs and DP offering a fresh perspective on algorithmic alignment. 2. The theoretical framework presented is robust and wellstructured providing a clear understanding of the GNNDP connection. 3. Empirical results on the CLRS benchmark demonstrate tangible improvements in model performance validating the theoretical findings. 4. The paper addresses an important research gap and has the potential to significantly impact the field of neural algorithmic reasoning. Weaknesses: 1. The complexity of the presented concepts might be challenging for readers without a strong background in category theory or abstract algebra. 2. The practical implications and realworld applications of the proposed framework could be further elaborated to enhance the papers overall impact. 3. While the empirical results are promising a more extensive evaluation across a wider range of datasets and tasks could strengthen the validation of the proposed methodology. 4. The practical implementation and computational efficiency considerations of the novel architecture proposed could be discussed further. Questions and Suggestions: 1. Can the authors provide a more intuitive explanation of the category theory and abstract algebra concepts for readers less familiar with these topics 2. How do the proposed GNN variants compare to existing stateoftheart approaches in terms of computational complexity and scalability 3. How can the insights from this work be further extended to tackle more diverse algorithmic reasoning tasks beyond the ones explored in the CLRS benchmark 4. Considering the interdisciplinary nature of the study how can the findings be communicated effectively to a wider audience across different research fields Limitations: The authors could provide more detailed insights into the potential negative societal impacts of their work such as considerations related to computational resource requirements accessibility of the proposed methodology and ethical implications of advancing algorithmic reasoning. Overall the paper makes a significant contribution to the field of neural algorithmic reasoning and lays a solid foundation for future research in building algorithmically aligned GNNs. With enhancements in accessibility and practical implications the findings could have farreaching applications in various domains.", "ymAsTHhrnGm": " Review of Paper: \"Optimizing strategic decisions using inverse game theory\" Summary: The paper introduces a novel approach to inverse game theory in the context of Stackelberg games by relaxing the assumption of perfect rationality to a quantal response model. The study aims to recover follower utility parameters from observable equilibrium behaviors which are crucial for making strategic decisions. Strengths: 1. Original Research Contribution: The paper addresses an important and understudied problem in game theory. 2. Theoretical Rigor: The work provides a solid theoretical foundation with tight analysis and empirical validation. 3. Innovative Approach: The adoption of a quantal response model for bounded rationality is a novel and practical approach. Weaknesses: 1. Limited Practical Application: The proposed framework PURE may face challenges in realworld applications due to dependencies on certain assumptions and scenarios. 2. Complexity: The theoretical concepts and analysis provided are complex and may be challenging for nonexperts to comprehend easily. 3. Lack of Realworld Validation: While empirical experiments on synthesized games are insightful realworld validation is essential for practical adoption and generalizability. Questions and Suggestions for Authors: 1. How do you plan to validate the efficacy of PURE and PUREExp in realworld scenarios and industry applications 2. Have you considered potential extensions of your research to more diverse game settings beyond Stackelberg games 3. How can the proposed framework adapt to scenarios with unknown system dynamics or incomplete information on follower behaviors Limitations: The authors could enhance the discussion on the practical limitations and potential societal impacts of implementing the proposed framework in realworld scenarios. Providing guidelines for ethical and fair use of the technology in different applications would add value to the research. In conclusion the paper presents a significant step forward in the field of game theory with practical implications for decisionmaking in multiagent systems. Further realworld validation and improvements in application scenarios are essential for actualizing the proposed framework in industry settings.", "o-mxIWAY1T8": " Summary: The paper introduces Semantic Probabilistic Layers (SPL) to address structuredoutput prediction (SOP) tasks. SPL guarantees consistency with predefined logical constraints in neural networks. It combines exact probabilistic inference with logical reasoning improving accuracy in challenges like hierarchical multilabel classification and pathfinding. The study demonstrates that SPLs outperform existing neurosymbolic approaches on various SOP tasks.  Strengths: 1. Novel Contribution: SPL introduces a unique approach to handle complex SOP tasks by ensuring predictions align with logical constraints. 2. Comprehensive Evaluation: The paper conducts empirical experiments across multiple tasks showcasing superior performance compared to existing approaches. 3. Theoretical Backing: The paper provides detailed explanations with theoretical foundations enhancing the understanding of the proposed SPLs. 4. Clear Exposition: The presentation is clear and systematic making it accessible to readers with varying levels of expertise in neural networks and structured output prediction.  Weaknesses: 1. Limited RealWorld Examples: The paper lacks realworld application examples to demonstrate the practical implications of SPLs beyond benchmark tasks. 2. Complexity: SPLs involve intricate computations with probabilistic circuits which may limit their scalability in realworld largescale applications.  Questions and Suggestions for Authors: 1. Practical Applicability: How can SPLs be effectively applied in realworld scenarios especially those involving dynamic and evolving constraints 2. Scalability: Have you tested the scalability of SPLs for largescale structured output prediction tasks with extensive constraints 3. Generalization: Can SPLs generalize well to unseen data distributions and complex logical constraints not encountered during training  Limitations: The authors should explicitly address the limitations and potential negative societal impacts of SPLs to ensure ethical considerations are adequately covered. Suggestions for improvement should include rigorous evaluation of potential biases and unintended consequences of implementing SPLs in practical settings.", "zTQdHSQUQWc": " Summary: The paper introduces a novel Frequency improved Legendre Memory model (FiLM) for longterm time series forecasting aiming to preserve historical information while reducing noise. By applying Legendre Polynomials projections and Fourier transforms the proposed FiLM outperforms stateoftheart models by 20.3 and 22.6 in multivariate and univariate forecasting respectively. The model involves a Legendre Projection Unit (LPU) Frequency Enhanced Layers (FEL) and a mixture of multiscale experts mechanism.  Strengths: 1. Innovative Approach: The incorporation of Legendre projections and Fourier transformations to address the challenge of preserving historical information while reducing noise is unique and shows promising results. 2. Empirical Validation: Extensive experiments on diverse benchmark datasets demonstrate the effectiveness of FiLM in achieving significant improvements over existing models. 3. Model Efficiency: The use of lowrank approximation in FEL for parameter reduction without compromising accuracy is commendable. 4. Robustness Evaluation: Supplemental studies such as parameter sensitivity noise injection experiments and KolmogorovSmirnov tests add depth to the analysis.  Weaknesses: 1. Complexity: The model structure including Legendre projections Fourier transforms and the mixture of experts may make the model complex and challenging to interpret for some users. 2. Training Speed: The paper briefly mentions the training speed of FiLM but lacks a detailed comparative analysis with respect to other models especially for multivariate forecasting tasks. 3. Data Normalization Impact: The effectiveness of the RevIN data normalization technique seems mixed across datasets requiring a more thorough investigation.  Questions and Suggestions for Authors: 1. How does the proposed model handle situations where the historical data contains outliers or sudden shifts 2. Can the authors elaborate on the computational complexity of the FiLM model compared to existing methods especially in terms of training time 3. How does FiLM generalize across different types of time series data considering the diversity in domains like energy traffic economics weather and disease 4. Can the authors provide insights into the interpretability of FiLM particularly how the Legendre projections and Fourier transformations affect the models decisionmaking process  Limitations: The paper puts forth an efficient method for longterm time series forecasting but lacks an indepth discussion on the potential negative societal impacts or ethical considerations of deploying such advanced models. Authors are encouraged to address and acknowledge any limitations or biases that might arise when implementing FiLM in realworld applications.", "oOte_397Q4P": " Summary: The paper introduces a novel approach Sparse Structure Search for Delta Tuning (S3Delta) to automatically search for optimal DT modules within large pretrained models (PTMs) in a parameterefficient manner. S3Delta outperforms manual and random structures achieving significant performance with extremely low trainable parameters. The approach is transferable across tasks and provides insights for future design of DT methods.  Strengths: 1. Novel Approach: S3Delta introduces a new method for automatic DT structure search addressing the efficiency and optimization challenges in adapting large PTMs. 2. Performance: The extensive experiments demonstrate the effectiveness of S3Delta surpassing manual structures with fewer trainable parameters. 3. Transferability: Searched structures are found to be transferable across tasks enhancing the reusability of the method. 4. Visualization: The visualization of the searched structures provides a clear understanding of DT module distributions and preferences.  Weaknesses: 1. Complexity: The bilevel optimization used in S3Delta may require substantial computational resources and memory limiting scalability. 2. Limited Evaluations: More comprehensive evaluations on diverse tasks and models could strengthen the generalizability and applicability of S3Delta. 3. Lack of Comparison: Direct comparison with stateoftheart DT methods could provide better context for assessing the performance of S3Delta.  Questions and Suggestions for Authors: 1. Scalability: How does the computational complexity scale with larger PTMs or more complex search spaces Any strategies to enhance scalability 2. Generalizability: Have you tested the robustness and performance of S3Delta on other PTMs or datasets beyond those mentioned in the paper 3. Comparison: Can you compare the efficiency of S3Delta with other stateoftheart methods in terms of computational overhead and performance gains 4. Impact of Structural Parameters: How sensitive is the performance of S3Delta to hyperparameters or initializations of structural parameters  Limitations:  The paper lacks a detailed discussion on potential negative societal impacts of the proposed method. Authors could include this aspect to provide a more comprehensive assessment of the work.", "pgBpQYss2ba": " PeerReview of the Paper  1) Summary: The paper aims to investigate the statistical complexity of decisionmaking in adversarial settings specifically focusing on a general adversarial decisionmaking framework that covers bandit problems with adversarial rewards and reinforcement learning with adversarial dynamics. The key contribution of the paper is establishing the necessity and sufficiency of the DecisionEstimation Coefficient (DEC) for achieving low regret in adversarial decisionmaking scenarios. The paper draws connections between the DEC and other complexity measures like the Information Ratio and ExplorationbyOptimization objective.  2) Strengths and Weaknesses: Strengths:  Comprehensive investigation into the statistical complexity of decisionmaking in adversarial settings.  Theoretical analysis to connect the DecisionEstimation Coefficient with other complexity measures.  Rigorous establishment of the DECs importance for achieving low regret in adversarial decisionmaking.  Theoretical results are wellsupported by indepth explanation and formal proofs.  Clear and structured presentation of theoretical results algorithms and connections with existing literature. Weaknesses:  The paper heavily emphasizes theoretical analysis with complex mathematical formalisms potentially making it challenging for readers unfamiliar with the domain.  Limited discussion on practical implications or applications of the derived theoretical results.  Lack of empirical validation or realworld case studies to demonstrate the practical relevance of the findings.  3) Questions and Suggestions for Authors: Questions: 1. Can the proposed algorithms and complexity measures have practical implications in realworld decisionmaking scenarios 2. How do the findings of this paper align with existing approaches in adversarial decisionmaking or reinforcement learning 3. Are there any specific scenarios or domains where the derived complexity measures may not be as applicable or relevant Suggestions: 1. Consider providing more intuitive explanations or examples to enhance the accessibility of the paper to a wider audience. 2. Include a discussion on potential applications or scenarios where the derived theoretical results could be practically beneficial. 3. Investigate the computational efficiency of the proposed algorithms for large decision spaces or realtime decisionmaking tasks.  4) Limitations: The paper largely focuses on theoretical investigations and there is a lack of concrete discussion on the practical applications or implications of the derived complexity measures in realworld decisionmaking scenarios. To enhance the completeness of the study it would be beneficial for the authors to consider including discussions or case studies demonstrating the applicability and usefulness of the proposed theoretical findings in practical settings.  This peerreview provides an overview of the paper highlights its strengths and weaknesses poses questions for clarification suggests improvements and addresses the need for practical implications to enhance the relevance of the theoretical findings.", "lCGDKJGHoUv": " Review of Paper: \"On the Generalization Properties of Stochastic Gradient Descent\"  Summary: The paper investigates the generalization properties of stochastic gradient descent (SGD) within the stochastic convex optimization framework. It starts by analyzing onepass withoutreplacement SGD and surprises with instances where the algorithms solution has a considerable generalization gap and empirical risk. It then moves on to study withreplacement SGD which exhibits better convergence rates and controlled generalization gap. The paper concludes with an analysis of multiepoch withoutreplacement SGD providing lower and upper bounds for the convergence rates.  Strengths: 1. Novel Approach: The study uncovers intriguing phenomena in SGDs generalization properties challenging conventional wisdom in statistical learning. 2. Rigorous Analysis: The paper provides detailed theoretical analyses and proofs to support the main results enhancing the credibility of the findings. 3. Clear Exposition: The paper is wellorganized with a logical flow from problem formulation to indepth analyses and conclusions making it accessible to readers.  Weaknesses: 1. Applicability Concerns: The theoretical nature of the study may limit immediate practical applications in realworld machine learning scenarios. Providing examples of potential practical implications could enhance the papers relevance. 2. Limited Empirical Validation: Lack of empirical validation or realworld case studies to support the theoretical findings could weaken the papers impact and applicability.  Questions and Suggestions: 1. Generalization Gap Causes: Can you further explain the reasons behind the significant generalization gap observed in certain instances of onepass withoutreplacement SGD 2. Practical Implications: How do these theoretical insights on SGDs generalization properties translate into practical recommendations for optimizing training processes in realworld machine learning applications 3. Comparison with Existing Methods: How does the proposed analysis compare with other contemporary methods for explaining generalization properties in deep learning models especially those that achieve strong performance in the interpolation regime  Limitations: The paper primarily focuses on theoretical analyses lacking empirical validation or direct applications to realworld machine learning problems. To improve the practical relevance and societal impact of the research integrating empirical studies or case studies could complement the theoretical findings.", "p4xLHcTLRwh": " Review of \"SALSA: A Machine Learning Approach for Cryptanalysis of LWEBased Cryptosystems\"  1) Summary: The paper introduces SALSA a novel machine learning attack on latticebased cryptographic schemes relying on the Learning With Errors (LWE) assumption. It explores training transformers to perform modular arithmetic using LWE samples to distinguish LWE instances from random and recover binary secrets. SALSA demonstrates success in cryptanalysis of smalltomid size LWE instances with sparse binary secrets and offers practical implications for attacking realworld LWEbased cryptosystems.  2) Strengths and Weaknesses: Strengths:  Innovative approach merging machine learning with cryptography.  Demonstrates the feasibility of using transformers for cryptanalysis of latticebased schemes.  Impressive results in recovering secrets for smalltomid size LWE instances with sparse binary secrets.  The paper is wellstructured provides detailed explanations of the techniques and is supported by clear illustrations and algorithms. Weaknesses:  Lack of comparative analysis with traditional algebraic attacks on LWE.  Limited discussion on the potential impact of SALSA on existing latticebased cryptosystems.  Can benefit from a more detailed evaluation of the computational and memory requirements.  The paper could elaborate more on the implications for postquantum cryptographic standards and the relevance of the introduced attack.  3) Questions and Suggestions for Authors:  Could the authors provide further insights into the scalability of SALSA to larger lattice dimensions and more general secret distributions  How does SALSA fare against existing algebraic attacks on LWE especially in terms of efficiency and overall success rate  Have the authors considered implications for future postquantum cryptographic standards based on their findings It would be beneficial to address this in the paper.  4) Limitations: The authors could enhance the discussion on SALSAs potential negative societal impact by addressing the implications of successful cryptanalysis on realworld privacy and security. They may consider suggesting measures to mitigate potential risks associated with the capabilities demonstrated by SALSA.  Conclusion: Overall the paper introduces a novel and promising machine learning approach for cryptanalysis of LWEbased cryptosystems. Addressing the mentioned suggestions could further strengthen the papers contributions and provide valuable insights for advancing the field of postquantum cryptography.", "qHGCH75usg": " Review of \"BYOLExplore: CuriosityDriven Exploration in VisuallyComplex Environments\" 1. Summary: The paper introduces BYOLExplore a curiositydriven exploration algorithm that learns a world model through selfsupervised prediction loss in latent space. It optimizes both the world representation and exploration policy together achieving results surpassing humanlevel performance in challenging tasks in both DMHARD8 and Atari games. 2. Strengths:  Contributions: Innovative approach integrating world model learning and curiositydriven policy optimization in a unified framework.  Performance: Demonstrates superior performance over existing methods on complex exploration tasks in both DMHARD8 and Atari games.  Conceptual Simplicity: BYOLExplores use of selfsupervised learning without additional objectives simplifies the training process.  Generality: Shows efficacy in diverse environments highlighting the methods versatility and effectiveness. 3. Weaknesses:  Experiment Setup: While the results are impressive more comparative analysis with a wider range of stateoftheart methods could enhance the papers impact.  Theoretical Insights: While empirical results are strong additional theoretical analysis or insights into why the method works could further strengthen the papers contribution. 4. Questions and Suggestions:  Intrinsic Reward Prioritization: How robust is the reward prioritization mechanism in different environments or with varying levels of noise  Transferability: Have you tested the transferability of BYOLExplore to realworld applications or more complex simulated environments  Impact of Hyperparameters: How sensitive is the performance of BYOLExplore to hyperparameters particularly in diverse settings 5. Limitations: The paper provides limited discussion on potential negative societal impacts of the proposed method. To enhance ethical considerations it would be beneficial to include a section addressing limitations and potential societal implications of the work. Overall the paper presents a compelling approach to curiositydriven exploration showcasing superior performance across a range of challenging tasks. Further analysis and exploration of the proposed methods capabilities and limitations could enhance its significance in the field of reinforcement learning and exploration algorithms.", "xz-2eyIh7u": " Summary: The paper addresses a linear stochastic bandit problem involving M agents that can collaborate via a central server to minimize regret in the presence of adversarial agents. The key focus is on balancing the explorationexploitation tradeoff while considering adversaries. The proposed algorithm RCLB achieves nearoptimal regret bounds for collaborative linear bandits with adversaries and is further extended to generalize linear bandit models and contextual bandit settings.  Strengths: 1. Novel Contribution: The paper introduces new algorithms that balance exploration and exploitation in the presence of adversaries providing tight regret bounds. 2. Theoretical Rigor: The work demonstrates fundamental lower bounds ensuring the optimality of the proposed algorithms. 3. Extension to General Models: The extension of the algorithms to generalized linear models and contextual bandits showcases the versatility and applicability of the proposed techniques. 4. Communication Efficiency: The proposed algorithms are designed to have logarithmic communication complexity making them suitable for largescale computing systems.  Weaknesses: 1. Limited Empirical Validation: The paper lacks empirical results on realworld datasets or practical scenarios limiting the validation of the proposed algorithms in diverse settings. 2. Complexity: The algorithms and theoretical analyses presented in the paper are quite complex potentially making it challenging for practical implementation and application by nonexperts.  Questions and Suggestions for Authors: 1. Clarification:  Can you provide more insights into the specific applications where the proposed algorithms could offer significant improvements over existing methods 2. Empirical Evaluation:  Could you include experiments on realworld datasets or practical settings to showcase the effectiveness of the algorithms in diverse environments 3. Scalability:  How do the proposed algorithms scale with the number of agents and the dimensionality of the problem Have you considered the computational complexity in largescale scenarios  Limitations: The authors have not extensively discussed the potential negative societal impacts of their work. It would be beneficial to address any ethical considerations arising from collaborative decisionmaking systems in adversarial environments and provide suggestions for mitigating potential risks. Consider including a section addressing ethical implications and presenting strategies to ensure responsible deployment and usage of the proposed algorithms.  Constructive Suggestions for Improvement:  Enhance the empirical validation section with experiments on realworld datasets to offer practical insights into the performance of the proposed algorithms.  Simplify the presentation of the algorithms and theoretical analyses to make the work more accessible to a wider audience.  Consider addressing ethical implications and potential societal impacts in the paper along with strategies for responsible deployment of collaborative decisionmaking systems in adversarial settings. Overall the paper makes significant contributions to the field of collaborative decisionmaking in adversarial environments. Further exploration of practical applications and considerations for ethical deployment could enhance the impact and relevance of the work.", "qtZac7A3-F": " Summary: The paper proposes Discrete Adversarial Training (DAT) to enhance visual representation by leveraging VQGAN to discretize images into visual words for robustness and generalization. DAT significantly improves multiple tasks including image classification object detection and selfsupervised learning achieving stateoftheart results without sacrificing clean accuracy.  Strengths: 1. Innovation: Novel approach of transferring NLPstyle adversarial training to vision models. 2. Effectiveness: Significant improvement demonstrated on multiple tasks establishing a new stateoftheart for robust image classification. 3. Thorough Analysis: Detailed methodology experiments and results are provided showcasing the effectiveness of DAT. 4. Empirical Validation: Comprehensive experiments and comparisons on various tasks and datasets validate the efficacy of DAT.  Weaknesses: 1. Computational Cost: The paper acknowledges the high computational cost of the proposed method limiting practicality on largescale tasks. 2. Theoretical Explanation: Lack of detailed theoretical explanations on the mechanism and the impact of the discretization process. 3. Assumption of Ideal Discretizer: Relying on the assumption of an ideal discretizer (VQGAN) may not always hold true in realworld scenarios.  Questions and Suggestions: 1. Experimental Reproducibility: How robust is the proposed method to variations in hyperparameters and different datasets 2. Scalability: How does the computational cost scale with larger datasets or higherresolution images 3. Generalization: Can the proposed DAT method be extended to different types of visual tasks beyond the ones tested 4. Human Perception: Have you considered human perception studies to validate the effectiveness of DAT in capturing shapebias and textureindependent features 5. Theoretical Framework: Further elaboration on the theoretical aspects behind the effectiveness of DAT would strengthen the paper.  Limitations: The authors need to address the computational cost theoretical explanations scalability and practicality of the ideal discretizer assumption for improved robustness and generalization.  Suggestions for Improvement: 1. Provide guidelines on mitigating the computational cost for broader applicability. 2. Enhance the theoretical framework on discretization and its impact on adversarial training. 3. Discuss potential enhancements or alternative approaches to address the limitations outlined in the paper. Overall the paper presents a promising approach with significant contributions however further exploration on computational efficiency and theoretical underpinnings is recommended for future work.", "xbgtFOO9J5D": " Summary The paper addresses the important problem of fairness and diversity in ranking algorithms focusing on group fairness under the proportional fairness notion. It introduces algorithms for finding the closest fair ranking under different distance metrics like Kendall tau and Ulam metrics. Additionally it proposes a novel metaalgorithm for fair rank aggregation under a generalized fairness framework.  Strengths 1. Novel Contributions: The paper presents original contributions including exact algorithms for closest fair ranking under different metrics and a metaalgorithm for fair rank aggregation. 2. Complexity Analysis: The algorithms time complexities are wellanalyzed and shown to be efficient for practical applications. 3. Theoretical Framework: The paper establishes a solid theoretical framework for studying fairness in ranking and rank aggregation problems.  Weaknesses 1. Evaluation: The paper lacks empirical evaluation to demonstrate the practical effectiveness of the proposed algorithms. 2. Limited RealWorld Application Discussion: While the theoretical contributions are substantial there is limited discussion on realworld applications and implications of the proposed fairness measures.  Questions and Suggestions 1. Algorithm Validations: Can the authors provide empirical validation of the proposed algorithms on realworld datasets to showcase their practical effectiveness 2. Scalability: How well do the proposed algorithms scale with increasing dataset sizes and the number of groups 3. Extension to Other Metrics: Have the authors considered extending their algorithms to handle additional distance metrics beyond Kendall tau and Ulam metrics  Limitations The paper lacks a detailed discussion on the potential negative societal impacts of the proposed algorithms. To address this limitation the authors could include a section discussing the ethical implications of their work and suggesting steps to mitigate any unintended biases or adverse effects.", "qq84D17BPu": "Summary: The paper introduces and solves an \"Equation of Motion\" (EoM) for deep neural networks (DNNs) a differential equation that accurately describes the discrete learning dynamics of DNNs. It addresses the gap between gradient flow (GF) and gradient descent (GD) by deriving a counter term that cancels the discretization error leading to the more precise EoM. The paper applies EoM to scale and translationinvariant layers and shows significant improvements over traditional methods supported by experimental results. Strengths: 1. Innovative Contribution: The paper offers a novel approach to bridging the gap between continuous differential equations and discrete learning dynamics in DNNs by introducing the EoM. 2. Rigorous Analysis: The paper provides a thorough theoretical analysis demonstrating the importance of the counter term in improving the description of GD dynamics. 3. Experimental Validation: The theoretical findings are supported by experimental results validating the effectiveness of the proposed approach. 4. Clarity of Presentation: The paper is wellstructured and the content is clearly explained making it accessible to readers. 5. Relevance: Addressing the gap between continuous and discrete dynamics in DNNs is a significant and relevant research topic in the field of deep learning. Weaknesses: 1. Complexity: The papers theoretical derivations are highly technical and may be challenging for readers without a strong background in mathematical analysis. 2. Limited Scope: The study focuses solely on GD and GF without considering other optimization techniques commonly used in deep learning which could broaden the applicability of the findings. 3. Potential Overfitting: The experimental validation uses a single random seed raising concerns about the generalizability of the results. 4. Oversights: The lack of discussion on stochasticity acceleration methods or adaptive optimizers limits the broader applicability of the proposed framework. Questions and Suggestions for Authors: 1. How does the proposed EoM framework compare to existing methods in terms of computational efficiency and application to realworld deep learning problems 2. Can the authors expand their analysis to consider the impact of stochasticity and optimization techniques beyond gradient descent on the dynamics described by EoM 3. How can the proposed framework be generalized to incorporate different network architectures or loss functions and what are the implications for such extensions Limitations: The authors have not explicitly addressed potential negative societal impacts of their work. To enhance the societal impact consideration it would be beneficial for the authors to discuss the ethical implications of using differential equations to optimize neural network models and suggest guidelines for responsible implementation.", "xqyDqMojMfC": " Review of \"Stochastic Optimization Algorithms for Constrained Nonconvex Stochastic Optimization Problems with Markovian Data\" 1. Summary and Contributions: The paper focuses on stochastic optimization algorithms for constrained nonconvex stochastic optimization problems with Markovian data. It studies the case where the transition kernel of the Markov chain is statedependent relevant in machine learning applications like strategic classification and reinforcement learning. The paper presents projectionbased and projectionfree algorithms establishing oracle complexity results. Empirical evaluations are also conducted on a strategic classification problem with neural networks. 2. Strengths and Weaknesses: Strengths:  Comprehensive study on nonasymptotic oracle complexity for constrained nonconvex optimization with Markovian data.  Novel technique leveraging movingaverage gradient estimation and decomposing noise in the stochastic gradient.  Thorough theoretical analysis supported by empirical validation.  Clear and structured presentation of method assumptions and results. Weaknesses:  The text is highly technical which may hinder readability for those less familiar with the subject matter.  More detailed explanations of the empirical results could enhance the papers practical relevance.  Lack of comparison with existing stateoftheart methods or benchmarks limits the context of the contributions. 3. Questions and Suggestions: Questions:  How do the proposed algorithms compare with existing approaches in terms of convergence speed and efficiency  How do the presented oracle complexity results translate to practical implications in realworld applications  Are the assumptions made realistic and applicable to a broad range of Markovian data scenarios Suggestions:  Incorporate a detailed comparison with other stateoftheart algorithms in terms of both theoretical analysis and empirical performance.  Provide further insights into how the proposed methodologies can be applied and generalized to different machine learning problems beyond strategic classification.  Consider including discussions on the computational efficiency and scalability of the algorithms for largescale datasets. 4. Limitations: The authors could enhance the discussion on potential negative societal impacts of their work by considering how the proposed algorithms may affect fairness transparency or bias in decisionmaking processes. Overall the paper presents a significant contribution to the field of stochastic optimization especially for constrained nonconvex problems with Markovian data. By addressing the above points the authors can further strengthen the papers impact and relevance in the research community.", "wKf5dRSartn": " 1) Summary: The paper introduces a new batch teaching complexity parameter called STDmin which is the first to provide an upper bound of the VCdimension (VCD) on a batch teaching complexity parameter. The paper establishes the STDmin teaching model as fulfilling three postulates of teaching without \"coding tricks.\" Additionally the paper compares STDmin to existing teaching models such as TD RTD NCTD and STD highlighting the strengths and weaknesses of each in terms of ambiguity and unambiguity in teaching. The paper provides indepth theoretical analyses and proofs to support its claims and contributions.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important gap in the literature by introducing the novel batch teaching complexity parameter STDmin.  The theoretical analyses and proofs provided are comprehensive and support the claims made in the paper.  Discusses desirable properties of teaching models and introduces postulates that guide the design of reasonable teaching models.  The comparison of existing teaching models (TD RTD NCTD and STD) with STDmin provides valuable insights into the characteristics of each model.  The paper offers significant implications for the design and analysis of teaching and learning algorithms. Weaknesses:  The paper is highly theoretical and lacks practical applications or empirical validations of the introduced model.  The complexity of the content may be challenging for readers unfamiliar with the topic. The paper could benefit from more accessible language for broader understanding.  Limited discussion on potential realworld applications of the proposed teaching model.  The paper focuses on specific theoretical aspects and may lack broader implications for machine learning practitioners.  3) Questions and Suggestions for Authors:  Can you provide examples or scenarios where the introduced STDmin teaching model could be practically applied and demonstrate its advantages over existing teaching models  Have you considered conducting experiments or simulations to validate the effectiveness of the STDmin model in realworld machine teaching scenarios  How does the ambiguity observed in NCTD and STDmin impact the practical implementation and effectiveness of these teaching models in realworld applications  Could you elaborate on potential future research directions or extensions of your work such as exploring the implications of the postulates defined and unambiguity in teaching models on machine learning applications  4) Limitations: The paper is highly theoretical and lacks a discussion on the potential negative societal impact of the introduced teaching model. Authors are encouraged to consider addressing any possible negative consequences or ethical considerations that may arise from the implementation of STDmin or related teaching models. A broader ethical discussion on the implications of teaching models in machine learning could enhance the papers relevance and holistic impact.", "qqIrESv4f_L": "Summary: The paper introduces a novel framework INSPNet for editing and processing Implicit Neural Representations (INRs) directly without explicit decoding. By leveraging differential operators on INRs the proposed framework enables signal processing on continuous signals without discretization. Additionally the authors present INSPConvNet the first Convolutional Neural Network that operates on INRs for various image processing tasks. Strengths: 1. Innovative Contribution: The paper introduces a novel approach for processing and editing INRs directly which is a valuable contribution to the field. 2. Theoretical Foundation: The authors provide a strong theoretical foundation for their methods including the universal approximation property and invariance proofs. 3. Experimental Validation: Extensive experiments showcase the effectiveness of the proposed framework in both lowlevel image processing tasks and highlevel classification on implicit fields. 4. Comparison with Existing Methods: The paper effectively compares the proposed framework with traditional image processing methods and shows competitive results. Weaknesses: 1. Complexity in Computation: The computation of highorder derivatives might be memory intensive and numerically unstable potentially limiting the scalability of the proposed framework. 2. Scalability Concerns: The paper acknowledges limitations in terms of scalability when reconstructing INRs in a manner that is efficient and stable for large datasets. Suggestions for addressing this issue could enhance the practical applicability. 3. Reproducibility: Details on the computational complexity and efficiency of the proposed method are lacking which could hinder the replicability of the results. Questions and Suggestions: 1. How does the proposed INSPNet framework compare to existing methods in terms of computational efficiency and memory usage 2. Are there specific scenarios or datasets where the INSPNet framework may face challenges or limitations in terms of processing capabilities 3. How does the performance of INSPNet and INSPConvNet scale with increasing complexity of tasks or dataset sizes 4. Could the authors provide more insights into potential applications or extensions of the proposed framework beyond the tasks presented in the paper 5. Would it be feasible to explore ways to optimize the computation of highorder derivatives to improve the scalability of the framework in practical scenarios Limitations: The authors have not fully addressed the scalability concerns related to the computation of highorder derivatives and memory efficiency. A constructive suggestion would be to explore methods to optimize the computation process for largescale applications.", "vfCd1Vt8BGq": " Review of \"InformationTheoretic Bounds for Generalization in Deep Learning\"  Summary: The paper introduces a new informationtheoretic bound based on leaveoneout conditional mutual information (looCMI) for generalization in deep learning models. This bound is designed to be interpretable computable and nonvacuous particularly for deep learning scenarios. Empirical validation on image classification tasks supports the effectiveness of the proposed looCMI bounds.  Strengths: 1. Innovative Contribution: Introduces a novel informationtheoretic bound (looCMI) to address generalization in deep learning effectively. 2. Interpretability and Computability: The looCMI bound is designed to be interpretable and easy to compute enhancing its practical utility. 3. Empirical Validation: Empirical results on image classification tasks demonstrate the effectiveness of the looCMI bounds. 4. Efficient Computation: The looCMI bound reduces computational costs compared to previous CMI bounds.  Weaknesses: 1. Limitations in Realistic Settings: The bound based on looCMI showed vacuous results in certain cases indicating limitations in practical applications. 2. Societal Impact: The paper lacks a discussion on potential negative societal impacts of applying these bounds.  Questions and Suggestions for Authors: 1. Can you provide insights into the scenarios where the looCMI bounds become vacuous and strategies to address this limitation 2. How does the proposed looCMI bound compare with existing methods in terms of computational efficiency and interpretability 3. Have you considered potential societal implications of using these bounds in applications and if so how can these be mitigated or addressed  Constructive Suggestions for Improvement: 1. Discuss Limitations: Clearly address the limitations of the proposed method including scenarios where the bounds may not provide useful insights. 2. Address Societal Impact: Include a section discussing potential negative societal consequences of using these bounds and propose strategies to mitigate such impacts.  Limitations: Authors have not adequately addressed potential societal impacts of their work. It is recommended to include a discussion on the ethical implications and societal consequences of deploying these bounds. This review aims to provide constructive feedback to enhance the quality and impact of the paper.", "uvE-fQHA4t_": " PeerReview of the Paper \"Generalization Bounds Using GradientNorm Dependent PACBayesian Approaches\"  1) Summary: The paper introduces a novel approach for deriving generalization bounds in deep neural network models by relaxing the assumption of uniformly bounded or Lipschitz loss functions. The proposed method utilizes onaverage bounded loss and onaverage bounded gradient norm assumptions to derive a new generalization bound that incorporates the contractivity of logSobolev inequalities. Empirical analysis is conducted on various neural architectures to evaluate the effectiveness of the new bound.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: The paper presents a unique approach by relaxing strict assumptions on loss functions providing a fresh perspective on deriving generalization bounds in deep learning.  Theoretical Foundation: The use of logSobolev inequalities and measure concentration in highdimensional spaces adds depth to the theoretical framework.  Empirical Analysis: The empirical evaluation on different neural architectures enhances the practical relevance of the proposed bound. Weaknesses:  Theoretical Complexity: The discussion on integrating logSobolev inequalities and measure concentration may be challenging for readers not wellversed in probability theory.  Practical Implementation: The theoretical advancements may not directly translate into straightforward practical implementation requiring additional efforts for integration into deep learning systems.  3) Questions and Suggestions for Authors:  Clarification on Implementation: Can the authors provide insights on how the proposed gradientnorm dependent bound can be practically implemented in training and evaluation setups for deep neural networks  Comparison Metrics: How does the proposed generalization bound compare with existing bounds in terms of computational complexity during training and inference phases  Extension Potential: Are there possibilities to extend the methodology beyond the considered settings such as nonGaussian data distributions or alternative loss functions  4) Limitations: The paper adequately addresses the limitations of traditional assumptions in deriving generalization bounds and offers a novel perspective. To enhance the discussion on limitations authors could provide practical guidelines on interpreting the proposed bounds implications in realworld applications with complex neural network architectures.  Overall Assessment: The paper presents a significant contribution to the field by introducing a novel method for deriving generalization bounds in deep neural networks. The theoretical foundations and empirical validations suggest the potential for practical application and future research enhancements. However further elaboration on practical implementation and the relevance of the proposed bound in diverse neural network scenarios could strengthen the paper\u2019s impact.", "yfrDD_rmD5": " Summary: The paper introduces a new technique called TracInWE that aims to address the \"cancellation effect\" observed in existing training data influence methods applied to large models in Natural Language Processing (NLP) tasks. By operating on the word embedding layer instead of the last layer of weights TracInWE mitigates the cancellation effect and enhances the discriminative power of data influence scores. The technique is evaluated on three language classification tasks with transformer models and demonstrates significant improvements over existing methods.  Strengths: 1. Innovative Technique: The proposal of TracInWE to improve training data influence methods by operating on the word embedding layer is a novel approach. 2. Robust Evaluation: The paper includes thorough evaluations on multiple NLP tasks with different models demonstrating the superiority of TracInWE over existing methods. 3. Interpretability: The wordlevel decomposition offered by TracInWE enhances interpretability by identifying influential words in training examples.  Weaknesses: 1. Limited Comparison: The paper lacks comparisons with a broader range of existing methods and fails to address potential alternatives or limitations. 2. Complex Method Description: The intricate technical details and mathematical expressions may pose challenges for readers with less expertise in the field. 3. Limitation of Word Overlap: The reliance on word overlap between training and test examples raises concerns about the applicability of TracInWE in scenarios where semantic relevance without word overlap is crucial.  Questions and Suggestions for Authors: 1. Can the effectiveness of TracInWE be further validated on additional diverse NLP tasks or datasets to generalize the findings 2. How does TracInWE perform on largerscale language models beyond BERT such as GPT models 3. Could the authors discuss the computational efficiency of TracInWE in comparison to other methods especially for models with millions of parameters 4. It would be beneficial to include a discussion on how TracInWE could be extended to handle scenarios with limited word overlap but significant semantic similarity between examples.  Limitations: The authors have not fully addressed the potential negative societal impacts of their work. It is recommended to consider implications on bias fairness and ethical considerations in future work to ensure responsible deployment of the proposed technique.", "t0VbBTw-o8": " Summary: The paper introduces novel graybox certificates for Graph Neural Networks (GNNs) named \"messageinterception certificates.\" These certificates exploit the messagepassing principle of GNNs to intercept adversarial messages by randomly deleting edges andor masking features of nodes. The proposed method provides stronger guarantees against adversarial attacks especially when adversaries can manipulate features of multiple nodes. The certificates are also modelagnostic and consider the underlying graph structure. The paper showcases improvement in certifiable robustness through experiments on various models and datasets.  Strengths: 1. Innovative Approach: The proposed messageinterception certificates offer a novel perspective on enhancing the robustness of GNNs against strong adversaries by considering the underlying messagepassing mechanism. 2. Strong Theoretical Framework: The paper provides a thorough theoretical analysis of the proposed certificates including formal definitions propositions and proofs demonstrating a deep understanding of the problem at hand. 3. Experimental Validation: The method is evaluated on different GNN architectures and datasets showcasing significant improvements in certifiable robustness. The results provide compelling evidence of the effectiveness of the proposed approach. 4. Efficiency: The paper highlights the efficiency of the proposed method in terms of sample complexity demonstrating that the certificates are more sampleefficient compared to existing approaches.  Weaknesses: 1. Complexity: The paper introduces several technical concepts and formalisms which may be challenging for readers less familiar with the domain. Simplifying the presentation of the theoretical framework could enhance the accessibility of the paper. 2. Limited Practical Implementation Details: While the paper discusses theoretical contributions and experimental results more detailed explanations on the practical implementation of the proposed method such as specific algorithms or code snippets could enhance clarity for researchers aiming to apply the method in practice.  Questions and Suggestions for Authors: 1. Practical Implementation: Could you provide more insights into the practical implementation aspects of the proposed messageinterception certificates such as details on how the interception of messages is handled in realworld scenarios 2. Comparative Analysis: It would be beneficial to include a comparative analysis with existing stateoftheart certification methods for GNNs to highlight the advantages of the proposed approach more explicitly.  Limitations: The authors could further address the complexity and practical challenges of applying the proposed method in realworld scenarios including considerations of scalability computational resources and data requirements to provide a more comprehensive assessment of potential limitations and societal impacts of the work.", "sj9l1JCrAk6": " Summary: The paper explores federated learning in practical recommender systems and natural language processing scenarios. It highlights the differentiation of hot and cold data features proposing a novel FedSubAvg algorithm to address the issue of feature heat dispersion. The paper provides theoretical analysis convergence proofs and evaluation results showing the superiority of FedSubAvg over existing methods.  Strengths: 1. Novel Contribution: The paper introduces a new perspective by addressing the differentiation of hot and cold data features offering a unique algorithm FedSubAvg to handle feature heat dispersion. 2. Theoretical Rigor: The study provides rigorous theoretical analysis convergence proofs and establishes the illconditioned nature of existing algorithms. 3. Empirical Evaluation: Evaluation results on various datasets demonstrate the effectiveness of FedSubAvg over other baseline methods showcasing its improved convergence speed and performance.  Weaknesses: 1. Complexity: The detailed theoretical analysis might be challenging for readers unfamiliar with the domain potentially limiting accessibility to a broader audience. 2. Empirical Setup: The specifics of the experimental setups could be further elaborated for clarity including hyperparameters tuning and the rationale behind the dataset choices. 3. Generalization: The discussion on the generalization of the proposed method to different types of datasets and scenarios could be expanded for a more comprehensive view.  Questions and Suggestions for Authors: 1. Explanation Clarification: Could the authors provide a more intuitive explanation of the feature heat dispersion concept for readers with limited mathematical background 2. Privacy Concerns: How does FedSubAvg handle privacy concerns especially in scenarios with sensitive client data 3. Scalability: Have authors considered the scalability of FedSubAvg to much larger datasets with millions of clients What potential challenges might arise in such scenarios  Limitations: Authors should enhance the discussion on the negative societal impacts relating to data privacy and security concerns. Suggestions for improving transparency and data protection measures should also be incorporated. Overall the paper makes significant contributions to federated learning in practical scenarios but addressing the identified weaknesses and expanding on the questions raised could further enhance its impact and accessibility.", "mxnxRw8jiru": "Summary: The paper introduces a novel approach to private highdimensional data generation by optimizing a small set of samples under the guidance of discriminative information for downstream tasks. The method aims to bridge the gap between private generative and discriminative models offering improved sample utility while reducing memory and computational consumption. Strengths: 1. Innovative Approach: Introducing a unique method focusing on optimizing informative samples rather than complex generative models is a significant contribution. 2. Superior Utility: Demonstrated considerable improvement in downstream task utility compared to existing approaches validated on MNIST and FashionMNIST datasets. 3. Scalability: Addressed concerns of scalability and provided insights into training on larger datasets with a focus on practicality. 4. Generalization: Showed generalization capabilities across architectures and demonstrated faster convergence rates compared to existing methods. 5. Code Availability: The code being opensourced promotes transparency and replicability in the research community. Weaknesses: 1. Lack of Visual Quality Guarantees: While focusing on utility the paper acknowledges potential limitations in sample visual quality which might be a concern in applications requiring both utility and visual fidelity. 2. Limited Generalization to Multiple Label Classes: Complexity may increase with a larger number of label classes and performance degradation observed with increased samples per class indicates potential challenges in broader applications. Questions and Suggestions: 1. Tradeoff Analysis: Could the paper provide a more detailed tradeoff analysis between visual quality and task utility to explore potential compromises or avenues for improvement 2. Scalability Verification: Can the scalability of the method be further validated on diverse and larger datasets potentially with varying label classes to assess its performance under more complex scenarios 3. Privacy Guarantees Clarification: Could the authors provide more insights into how the method ensures privacy guarantees during data generation and sharing processes 4. Comparison Metrics: Could the paper include more comprehensive comparisons with a wider range of existing methods to further highlight the uniqueness and effectiveness of the proposed approach 5. Ethical Implications: How does the paper address potential ethical concerns related to privacy data security and societal impacts stemming from the use of private data generation techniques Limitations: The paper adequately addresses the limitations of its method but could further enhance this aspect by providing more specific measures to mitigate the potential negative societal impacts especially in sensitive domains like healthcare and finance. Consider discussing ethical implications and safeguards against misuse of generated private data.", "w-Aq4vmnTOP": "Summary: The paper investigates agnostic PAC learning and refutation in noninteractive local differential privacy (LDP). It characterizes the sample complexity of both learning and refutation in this setting showing their equivalence. The work provides theoretical results and thorough analysis for understanding the relationships between learning and refutation under LDP. Strengths: 1. Theoretical Rigor: The paper offers a high degree of theoretical analysis including characterizing sample complexities and showing equivalences between learning and refutation. 2. Detailed Explanations: The paper provides clear definitions and explanations of the studied concepts making it accessible for readers unfamiliar with the topic. 3. Novel Contributions: The complete characterization of sample complexity for agnostic PAC learning in noninteractive LDP is a significant contribution. 4. Rigorous Methodology: The papers methodology is systematic and wellstructured enhancing the credibility of the results. Weaknesses: 1. Lack of Empirical Validation: The paper seems to lack practical validation or realworld application which might limit its immediate applicability. 2. Complexity: Some sections especially those detailing mathematical proofs might be challenging for readers less familiar with the subject matter. 3. Limited Discussion on Practical Implications: The paper could benefit from a more detailed discussion on the practical implications and potential applications of its theoretical findings. Questions and Suggestions for Authors: 1. Clarification on Practical Relevance: Could the authors discuss the potential practical applications of their theoretical results in realworld scenarios or specific industries 2. Comparison to Existing Methods: How does the proposed approach compare to existing methods in terms of efficiency and effectiveness 3. Impact Assessment: Have the authors considered the potential societal impact of their work particularly in terms of data privacy and security concerns Limitations: The paper adequately addresses the limitations and societal impact of its work particularly emphasizing the strong privacy constraints in noninteractive LDP. However it could include more practical implications and considerations for realworld implementation to enhance its societal relevance.", "tQRoZ9nRgM": " Summary The paper investigates a sequential matching problem in large centralized platforms focusing on matching jobs to workers under uncertainty about worker skill proficiencies. The platform aims to maximize cumulative payoffs over a planning horizon by designing efficient algorithms for sequential matching. The study addresses a key research question on adapting algorithms to workertype distribution shifts in the gig economy.  Strengths  Conceptual Contribution: The paper introduces a novel rateoptimal learning algorithm for the sequential matching problem with theoretical analysis establishing lower bounds on regrets and achievable performances.  Novelty: The work fills a gap in existing literature by addressing a dynamic matching problem with an unlimited supply of workers a feature not explored in prior studies.  Rigorous Analysis: The paper presents a systematic approach to complexity and performance analysis providing insights into the challenges and optimal strategies for the problem at hand.  Weaknesses  Limited Empirical Validation: The paper lacks empirical validation or practical implementation of the proposed algorithm making it challenging to assess the realworld applicability and performance of the approach.  Complexity and Accessibility: The technical nature of the content may limit the accessibility of key findings to a broader audience potentially hindering knowledge dissemination and practical uptake.  Questions and Suggestions for Authors 1. Could you provide insights on the scalability and computational efficiency of the proposed algorithm in realworld deployment scenarios 2. Have you considered conducting simulations or experiments to validate the theoretical findings and algorithm performance in practical settings 3. How can the algorithm adapt to dynamic changes in workertypes over time especially in scenarios where such shifts occur frequently or unpredictably 4. It would be beneficial to clarify the practical implications of the studys findings and how they can inform decisionmaking in operational settings.  Limitations The authors have not explicitly discussed the potential negative societal impact of their work. To improve they could consider conducting a comprehensive impact assessment to ensure ethical implications are addressed transparently.", "pk1C2qQ3nEQ": " Peer Review  1) Summary of the Paper and Contributions The paper introduces a novel uncertainty measure Balanced Entropy Acquisition (BalEntAcq) in the context of active learning with Bayesian deep neural networks. This measure aims to capture the information balance between model uncertainty and label variable entropy providing a diversified selection strategy near decision boundaries. Experimental results on MNIST CIFAR100 SVHN and TinyImageNet datasets show that BalEntAcq outperforms existing methods consistently.  2) Assessment of Strengths and Weaknesses Strengths:  The paper introduces a new uncertainty measure with clear theoretical foundations.  Extensive experimentation demonstrates the effectiveness of BalEntAcq compared to existing methods.  The use of Bayesian deep neural networks adds uniqueness to the proposed approach. Weaknesses:  The theoretical development is dense and may be challenging for readers unfamiliar with the subject matter.  Limited discussion on the interpretability of the proposed measure or visual representation of results.  The paper could benefit from more detailed discussion on the computation and scalability aspects of BalEntAcq.  3) Questions and Suggestions for Authors  Can you provide more intuitive explanations or visualizations to aid in understanding the theoretical development and results  How does BalEntAcq perform in scenarios with highly imbalanced datasets or noisy labels  Have you considered the computational overhead of integrating BalEntAcq into largescale active learning systems  4) Limitations The authors have addressed the limitations of their work including the detailed assumptions and theoretical proofs. To improve transparency consider providing a more detailed discussion on potential negative societal impacts or ethical considerations related to the deployment of BalEntAcq. Overall the paper presents a novel contribution in the field of active learning with Bayesian deep neural networks backed by rigorous theoretical foundations and empirical evaluations. Well done on this research endeavor", "owZdBnUiw2": "Peer Review Summary: The paper introduces a novel approach called Ample and Focal Network (AFNet) for action recognition in videos. AFNet consists of two branches  Ample Branch and Focal Branch along with a Navigation Module to efficiently process more frames with reduced computational cost. The authors demonstrate that AFNet can achieve higher accuracy by dynamically selecting salient frames within the classification network. The proposed method is validated through extensive experiments on five video recognition datasets showcasing its effectiveness and efficiency. Strengths: 1. Innovative Approach: The twobranch architecture of AFNet and the dynamic frame selection within the network offer a unique perspective compared to existing methods. 2. Efficiency and Accuracy: AFNet effectively processes more frames while maintaining high accuracy thanks to the adaptive selection strategy and implicit temporal modeling. 3. Comprehensive Experiments: The paper provides thorough experimental validation on multiple datasets demonstrating the superiority of AFNet over competitive methods. 4. Clear Methodological Description: Detailed descriptions of the architecture design navigation module implicit temporal modeling and spatial redundancy reduction enhance the clarity of the proposed method. Weaknesses: 1. Limited Evaluation on Realworld Deployment: While the paper addresses the computational efficiency a detailed discussion on the feasibility and practicality of deploying AFNet in realworld applications could be beneficial. 2. Insufficient Comparison with StateoftheArt: Limited comparison with stateoftheart methods may hinder a comprehensive evaluation of the proposed approach. Including additional comparisons could strengthen the paper. Questions and Suggestions: 1. Can the authors provide insights into how AFNet could be adapted or finetuned for specific realworld applications in recommendation systems surveillance or autonomous driving 2. How does the proposed Navigation Module handle cases of sudden changes or complex motion patterns in videos and are there any limitations in capturing such dynamics 3. Could the authors elaborate on the scalability and flexibility of AFNet in handling variations in video content such as different resolutions or frame rates Limitations and Suggestions for Improvement: The paper adequately addresses the limitations and potential negative societal impact by emphasizing computational efficiency. To further enhance the ethical considerations the authors could provide a discussion on the environmental impact energy consumption and potential biases in the selection process. Additionally incorporating a section on model interpretability and fairness could enhance the ethical framework of the work. Overall the paper presents a novel and effective approach for action recognition in videos. With minor improvements in discussing realworld deployment enhanced comparisons and implications on ethics and fairness the paper could further solidify its contributions and impact in the field.", "kpSAfnHSgXR": "PeerReview: 1) Summary: The paper proposes binary code box embeddings to model directed graphs efficiently. The motivation behind the work lies in the necessity of capturing key information from graphs through differentiable parameterizations. The proposed models GBCBOX and VBCBOX aim to address the limitations of existing box embeddings in representing cycles in directed graphs while preserving important inductive biases like transitivity. The paper demonstrates the theoretical capacities of the models and evaluates them through graph reconstruction and link prediction tasks. 2) Strengths and Weaknesses: Strengths:  The paper addresses a significant challenge in the field of graph embedding by proposing novel binary code box embeddings.  The work provides a sound theoretical foundation demonstrating the representational capacities of the proposed models.  Methodical experiments on both synthetic graphs and realworld datasets showcase the performance and superiority of the proposed models compared to baseline methods.  The paper offers clear explanations and comparisons with existing models enhancing the understanding of the proposed approach. Weaknesses:  The paper could benefit from more detailed discussions on the practical implications and applications of binary code box embeddings.  While the proposed models show promising results more insights into the interpretability and robustness of the learned representations could be valuable. 3) Questions and Suggestions:  Can the authors elaborate on the potential scalability of the proposed models when applied to larger and more complex realworld graph datasets  How do the binary code box embeddings compare in terms of computational efficiency and memory usage compared to existing embedding approaches  It would be interesting to explore the generalization capabilities of the models across different types of graphs beyond the ones considered in the experiments. 4) Limitations: The authors have discussed the limitations in terms of transitivity and flexibility in modeling cycles. To further enhance the discussion a suggestion would be to incorporate an analysis of the negative societal impacts that might arise from the bias or limitations of the proposed models. This addition would contribute to a more comprehensive evaluation of the work. Overall the paper showcases a significant advancement in graph embedding techniques with the introduction of binary code box embeddings. The theoretical foundations coupled with empirical evaluations support the effectiveness and versatility of the proposed models in modeling complex and cyclical directed graphs. Further refinements and realworld applications could enhance the impact and practicality of the proposed approach.", "r-6Z1SJbCpv": " Peer Review of \"OPTFORMER: A Universal Transformerbased Framework for MetaLearning Hyperparameter Optimization\"  Summary: The paper introduces OPTFORMER a novel textbased Transformer Hyperparameter Optimization (HPO) framework that utilizes metalearning to jointly learn policy and function prediction from extensive tuning data. OPTFORMER can imitate diverse optimization algorithms and provides accurate and wellcalibrated predictions outperforming Gaussian Processes. The framework is evaluated on large datasets showcasing its competitive performance in HPO tasks.  Strengths: 1. Innovative Approach: The paper introduces a unique textbased Transformer framework for HPO addressing limitations of existing methods. 2. Comprehensive Experiments: Extensive experiments on diverse datasets demonstrate the effectiveness of OPTFORMER in imitating various HPO algorithms. 3. Performance Metrics: The paper provides detailed performance analysis including logpredictive likelihood and expected calibration errors showcasing the superiority of OPTFORMER over Gaussian Processes. 4. Clear Presentation: The paper is wellstructured with detailed explanations of the methodology experiments and results making it accessible to readers.  Weaknesses: 1. Model Complexity: The paper lacks discussion on the computational complexity and potential resource requirements of deploying the largescale Transformer model in realworld scenarios. 2. Transfer Learning Evaluation: While OPTFORMERs performance on HPO tasks is demonstrated the paper could benefit from a more indepth analysis of the frameworks transfer learning capabilities on unseen datasets. 3. Comparative Analysis: Although OPTFORMER is compared favorably to traditional HPO algorithms more direct comparison with recent stateoftheart methods in the field could provide a clearer benchmark.  Questions and Suggestions for Authors: 1. Scalability and Computational Overhead: How does the computational cost of deploying a Transformer model like OPTFORMER scale with the size of the HPO dataset and complexity of the optimization tasks Consider discussing potential constraints and optimizations related to computational resources. 2. Generalization and Transfer Learning: How does OPTFORMER perform on unseen datasets or significantly different HPO tasks not included in the training set Further analysis on the generalization capabilities of the model would enhance the robustness of the framework. 3. Comparison with Stateoftheart: Can the authors provide a more direct comparison with recent stateoftheart metalearning and transfer learning approaches for HPO tasks showcasing the unique advantages of OPTFORMER over existing methods 4. Societal Impact and Ethical Considerations: It would be beneficial to include a discussion on the potential societal impact of employing automated HPO frameworks like OPTFORMER and address any ethical considerations related to optimizing machine learning models.  Limitations: The authors have not explicitly addressed potential limitations and negative societal impacts of their work. Considering the ethical implications of automated optimization frameworks the paper could benefit from a brief discussion on these aspects to promote responsible research practices.  Suggestions for Improvement: 1. Ethical Considerations: Include a section discussing the ethical considerations and societal impact of developing autoHPO frameworks for machine learning tasks. 2. Extended Transfer Learning Analysis: Further investigate the transfer learning capabilities of OPTFORMER on diverse and unseen datasets to illustrate its adaptability in practical scenarios. Overall the paper presents a promising advancement in the field of metalearning for HPO with the introduction of OPTFORMER. Addressing the suggested points and limitations can enhance the comprehensiveness and applicability of the framework in various machine learning optimization tasks.", "yNPsd3oG_s": " Summary: The paper addresses the vulnerability of Deep Neural Networks (DNNs) to Trojan attacks encompassing both injected and natural Trojans. The authors propose a novel training method NONE (NONLinEarity) designed to mitigate both types of Trojans. The method involves identifying and correcting linear decision regions in neural networks to remove backdoors efficiently. Extensive experiments on various datasets and attacks show that NONE outperforms existing defenses in terms of attack success rate reduction.  Strengths: 1. Innovative Methodology: The paper introduces a novel training method NONE based on the deep analysis of DNN behaviors impacted by Trojans. 2. Thorough Analysis: The authors provide detailed theoretical explanations empirical observations and experimental results to support their claims. 3. Comprehensive Evaluation: The performance of NONE is extensively evaluated against various Trojans offering a robust assessment of its effectiveness across different attack scenarios. 4. High Impact: The development of a defense method that addresses both injected and natural Trojans fills a gap in the existing literature providing significant implications for DNN security.  Weaknesses: 1. Complexity: The papers technical depth may pose comprehension challenges for readers not wellversed in deep learning concepts and Trojan attacks. 2. Limited Benchmarking: Although performance against existing defenses is compared a more extensive benchmarking on additional datasets and attacks could further validate the methods efficacy. 3. Implementation Details: More insights into the practical implementation aspects of NONE potential deployment challenges and computational requirements could enhance the papers applicability.  Questions and Suggestions for Authors: 1. Clarification: Can you provide additional insights on the computational overhead involved in implementing NONE and how it compares to existing defense methods 2. Generalization: How transferable is the NONE method to other types of machine learning models beyond neural networks and have you considered exploring this aspect further 3. Further Analysis: Have you considered analyzing the interpretability and explainability of models trained using NONE especially in understanding decisionmaking processes  Limitations: The authors could further address the potential negative societal impact of their work by including a section discussing ethical considerations transparency and accountability when deploying defense mechanisms in realworld applications. A detailed discussion on ensuring fairness avoiding biases and adhering to data protection regulations would enhance the papers completeness.  Overall Impression: The paper presents a novel and impactful contribution to the field of DNN security providing a comprehensive analysis innovative methodology and strong experimental validation. Addressing the suggested improvements and ethical considerations would further enhance the papers credibility and relevance in the research community.", "ocViyp73pFO": " Summary: The paper addresses the challenge of disentangling the effects of single interventions from jointly applied interventions especially in the presence of unobserved confounders. It proposes a method to learn the effect of a single intervention from observational data and sets of interventions focusing on nonlinear continuous structural causal models with additive multivariate Gaussian noise. The paper provides identification proofs an algorithm for learning causal model parameters and validates the method on synthetic and real data.  Strengths: 1. Original Contribution: The paper makes significant contributions by addressing a challenging problem in causal inference. 2. Theoretical Rigor: The identification proofs provided demonstrate a deep theoretical understanding of the problem. 3. Methodology: The proposed algorithm for learning the causal model parameters is wellstructured and thorough. 4. Empirical Validation: The empirical validation on both synthetic and real data strengthens the credibility of the method. 5. Related Work Comparison: The comprehensive comparison with existing literature highlights the novelty and significance of the proposed method.  Weaknesses: 1. Complexity: The papers technical details and formal proofs might be challenging for readers unfamiliar with advanced causal inference methods. 2. Clarity: Some sections could benefit from clearer explanations and better organization to improve readability. 3. Limited Explanation of Results: The discussion could be enhanced by providing more insightful analyses of the empirical validation results.  Questions and Suggestions for Authors: 1. Clarity on Assumptions: Can the authors clarify any crucial assumptions made in the methodology and their implications on the results 2. Scalability: How does the proposed method scale with larger datasets and more complex causal structures 3. Implementation Details: Are there implementation considerations (e.g. computational efficiency sensitivity to hyperparameters) that authors can discuss for practical application 4. Generalization: Have the authors explored the generalizability of their method to other types of causal models beyond the ones considered in this paper 5. Reproducibility: How accessible and reproducible are the code and data used in the experiments  Limitations: The authors have appropriately addressed the limitations and potential negative societal impact of their work. To further improve they could consider discussing strategies for mitigating any societal impacts that may arise from the application of their method.", "qf12cWVSksq": " Peer Review of \"Inception Transformer for Vision Tasks\"  Summary: The paper introduces iFormer an Inception Transformer architecture designed to capture both high and low frequencies in visual data by effectively combining convolution maxpooling and selfattention mechanisms. The authors propose the Inception token mixer and a frequency ramp structure to balance the capture of highfrequency details in lower layers and lowfrequency global information in higher layers. Experimental results demonstrate that iFormer outperforms existing models on various vision tasks including image classification object detection and semantic segmentation.  Strengths: 1. Innovative Approach: The integration of convolution and maxpooling into Transformer architecture through the Inception mixer is a novel and promising approach to address the limitations of existing Transformer models. 2. Effective Design Choices: The frequency ramp structure introduced to balance high and low frequencies across layers aligns with human visual processing principles and has shown to improve model performance. 3. Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed iFormer on multiple vision tasks and compares its performance with stateoftheart models showcasing consistent improvements.  Weaknesses: 1. Complexity and Interpretability: While the proposed architecture is innovative the complexity introduced by combining multiple mechanisms may hinder the interpretability of the model. Clear explanations or visualizations of the models decisionmaking process could enhance understanding. 2. Scalability: The scalability of iFormer to larger datasets such as ImageNet21K is mentioned as a limitation. It would be beneficial to explore the performance of iFormer on more diverse and extensive datasets for a comprehensive assessment.  Questions and Suggestions for Authors: 1. Validation on Larger Datasets: Have you considered validating iFormer on larger datasets like ImageNet21K to assess its scalability and generalization to a broader range of tasks 2. Interpretability: How can the models decisionmaking process be interpreted or visualized to provide insight into how it captures both high and low frequencies effectively 3. Comparison with Human Visual Perception: How closely does the frequency ramp structure in iFormer mirror the hierarchical processing of high and low frequencies in the human visual system Can this comparison be further explored  Limitations:  Authors may consider automating the selection of channel ratios in the frequency ramp structure using techniques like neural architecture search to enhance scalability and generalizability.  Overall Impression: The paper presents a compelling and innovative approach to enhancing Transformer models for vision tasks. The thorough evaluation and promising results suggest that iFormer has the potential to advance the stateoftheart in vision processing. Addressing the interpretability and scalability concerns could further strengthen the papers impact and utility in the research community.", "q2nJyb3cvR9": " Summary: The paper presents a new algorithm Single Seed Randomization (SSR) for exploration in reinforcement learning using randomized value functions. SSR utilizes a single random seed per episode and introduces a new clipping operation and a novel Bernsteintype noise magnitude to optimize exploration efficiency. The algorithm achieves a nearoptimal regret bound of O\\xcc\\x83(H \\xe2\\x88\\x9a SAT) in episodic timeinhomogeneous Markov Decision Processes matching the \\xce\\xa9(H \\xe2\\x88\\x9a SAT) lower bound up to logarithmic factors.  Strengths: 1. Innovative Algorithm Design: SSR introduces novel concepts like using a single random seed and a clipping strategy deviating from traditional exploration algorithms. 2. Theoretical Contributions: The paper bridges the gap between existing bounds for randomized exploration algorithms and the minimax lower bound showcasing significant theoretical advancements in reinforcement learning. 3. Thorough Analysis: The paper provides detailed technical discussions outlining the algorithms intricacies and their impacts on regret analysis enhancing the readers understanding.  Weaknesses: 1. Complexity: The technical depth and mathematical rigor might limit the accessibility and comprehensibility of the paper for readers with less expertise in reinforcement learning theory. 2. Practical Implementation: While the theoretical aspects are wellcovered practical implications and realworld considerations of implementing SSR are relatively sparse.  Questions and Suggestions: 1. Clarifications on Algorithm Assumptions: Does the algorithms performance rely on assumptions about the MDP properties or specific noise models 2. Empirical Validation: Have simulations or experiments been conducted to demonstrate SSRs performance against existing algorithms in practical scenarios 3. Robustness: How does SSR perform under noisy or nonideal conditions and are there plans to address robustness in future research  Limitations: The paper could benefit from a more indepth exploration of the algorithms scalability robustness and applicability in various RL scenarios. Addressing these aspects would enhance the practical relevance of the research.", "mhp4wLwiAI-": "PeerReview: Summary: The paper investigates the challenges faced in training deep Graph Convolutional Networks (GCNs) due to poor gradient flow and proposes novel techniques to address this issue. The authors introduce a topologyaware isometric initialization scheme and gradientguided dynamic rewiring to improve the trainability and performance of deep vanillaGCNs. Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed methods in enhancing gradient flow and achieving stateoftheart performance. Strengths: 1. Novel Contribution: The paper addresses a crucial problem in training deep GCNs by focusing on improving gradient flow. The proposed methods topologyaware isometric initialization and gradientguided dynamic rewiring are novel and effective in enhancing the trainability and performance of deep vanillaGCNs. 2. Empirical Evidence: Extensive experiments conducted across multiple datasets provide strong empirical evidence supporting the effectiveness of the proposed techniques. Results show significant performance improvements and validate the importance of healthy gradient flow in training deep GCNs. 3. Thorough Analysis: The paper includes detailed theoretical derivations illustrations and experimental results to support the proposed methods. The analysis of gradient flow Dirichlet energy and layerwise behaviors adds depth to the evaluation of the techniques. Weaknesses: 1. Theoretical Elaboration: While the paper provides detailed descriptions of the proposed methods there could be more explicit explanations of the theoretical foundations behind the topologyaware isometric initialization and dynamic rewiring techniques to aid in better understanding. 2. Comparative Analysis: Although the paper compares the proposed methods with existing techniques a more comprehensive comparison with a wider range of stateoftheart methods could strengthen the evaluation and highlight the superiority of the new approaches. Questions and Suggestions: 1. Clarity on Initialization: Could the authors provide more clarity on the impact of the proposed isometric initialization scheme on the convergence speed and stability of deep GCNs compared to traditional initialization methods 2. Scalability Concerns: How do the proposed techniques scale in terms of computational complexity and memory requirements as the depth of GCNs increases Any insights into the scalability of the methods for extremely deep networks 3. Generalization: Have the authors tested the generalization capability of the proposed methods on unseen or larger datasets to assess their performance in varied settings Limitations: The authors have not explicitly addressed the negative societal impacts or ethical considerations associated with their work. It would be beneficial to include a brief discussion on the limitations and potential societal implications of the proposed techniques ensuring responsible AI development. This review acknowledges the significance of the paper in advancing the understanding of training deep GCNs and commends the authors for their innovative contributions. Improving the theoretical clarity expanding the comparative analysis addressing scalability concerns and discussing potential societal impacts could further enhance the quality and impact of the research.", "pHd0v8W30O": " Summary The paper introduces a new method called latentvariable advantageweighted policy optimization (LAPO) to address the challenge of learning from heterogeneous offline datasets in the context of reinforcement learning. LAPO leverages a latentvariable generative model to represent highadvantage stateaction pairs improving adherence to data distributions and maximizing reward via a policy over the latent variable.  Strengths 1. Novel Contribution: LAPO introduces a unique approach to address the problem of learning from heterogeneous datasets in offline reinforcement learning. 2. Empirical Validation: Extensive empirical evaluations on simulated tasks demonstrate the effectiveness of LAPO compared to several stateoftheart methods showcasing significant performance improvements. 3. Thorough Explanations: The paper provides detailed explanations and illustrations to clarify the challenges and the proposed solution making it accessible to readers.  Weaknesses 1. Complexity: The paper can be challenging to follow due to the technical depth and complexity of the proposed method limiting its accessibility to those not deeply familiar with the subject. 2. Limited Comparison Metrics: While the comparison with existing methods is insightful additional metrics such as computational efficiency and scalability could further strengthen the evaluation. 3. Realworld Validation: The experiments are conducted on simulated tasks realworld validation on physical robots could enhance the practical relevance of the proposed method.  Questions and Suggestions for Authors 1. Could you provide more insights into the computational complexity and scalability of LAPO compared to existing methods 2. Have you considered conducting experiments on real robotic platforms to validate the performance of LAPO in practical scenarios 3. How does LAPO handle cases where the dataset size is limited and may not cover all possible scenarios adequately  Limitations The authors have not clearly addressed the potential negative societal impacts of their work. To improve they could consider discussing implications related to the deployment of offline reinforcement learning in safetycritical environments and the potential biases that may arise from precollected datasets.", "ouXTjiP0ffV": "PeerReview: 1) Summary: The paper introduces a novel paradigm Neural Correspondence Prior (NCP) for computing correspondences between 3D shapes. The approach is fully unsupervised and addresses challenging cases such as nonrigid nonisometric deformations. The key contributions include demonstrating that untrained neural networks produce pointwise features comparable to axiomatic local descriptors introducing the Neural Correspondence Prior effect that improves map quality and proposing a twostage unsupervised algorithm for shape matching. The method achieves stateoftheart results across various tasks. 2) Strengths:  The paper introduces a novel paradigm that tackles challenging 3D shape correspondence problems with impressive results.  The experiments are thorough covering various datasets and comparisons against stateoftheart methods show significant improvements.  The theoretical foundation and empirical validation of the Neural Correspondence Prior effect are wellsupported providing a solid framework for the proposed approach.  The methodology is wellexplained and structured making it easy to follow the algorithmic pipeline. 3) Weaknesses:  The paper could benefit from more detailed explanations regarding the experimental setups particularly in terms of hyperparameter choices baseline methods and limitations of the datasets used.  The theoretical underpinnings of the NCP effect could be further elaborated to enhance the papers theoretical contributions.  It would be beneficial to discuss potential failure cases or limitations of the proposed method to provide a complete picture of its applicability. 4) Questions and Suggestions for Authors:  Have you considered exploratory analyses to understand the sensitivity of the proposed method to different hyperparameters and settings  Can you elaborate on how the proposed method addresses challenges such as overfitting in an unsupervised setting  How generalizable is the NCP effect across different types of 3D shapes and have you tested the method on more diverse datasets to assess its robustness  It would be insightful to investigate the interpretability of the learned features and how they contribute to the overall correspondence quality. 5) Limitations and Suggestions for Improvement: The authors should provide a more detailed discussion on the limitations of the proposed method such as cases where the unsupervised method in the first stage fails to provide meaningful maps. Addressing these limitations could strengthen the practical applicability of the approach. Overall the paper presents a novel and promising approach to 3D shape correspondence with strong empirical results and a wellexplained methodology. Clarifying some aspects and addressing the limitations in more detail could enhance the impact and practical relevance of the work.", "wlrYnGZ37Wv": " Summary: The paper presents Sequencer a novel architecture for computer vision that uses LSTM for sequence modeling instead of selfattention layers. It introduces the BiLSTM2D layer to efficiently mix 2D spatial information and achieves competitive performance with stateoftheart models on the ImageNet benchmark. The architecture shows good resolution adaptability and efficiency making it a practical alternative to existing models.  Strengths: 1. Novel Architecture: The paper introduces a new approach to sequence modeling in computer vision using LSTM providing a fresh perspective in the field. 2. Performance Results: Sequencer demonstrates impressive performance on ImageNet outperforming comparable models with competitive accuracy. 3. Resolution Adaptability: The architecture shows robust resolution adaptability and efficiency maintaining accuracy even with varying input resolutions. 4. Thorough Experiments: The paper includes extensive experiments ablation studies transfer learning and semantic segmentation evaluations showcasing the effectiveness and generalizability of the proposed architecture.  Weaknesses: 1. Throughput: The paper acknowledges that Sequencers throughput is slower compared to other models due to the use of RNNs which can be a limitation in practical applications. 2. Training Efficiency: While it excels in memory efficiency the higher computational complexity of Sequencer due to recursion may impact training efficiency. 3. Documentation: The organization of the paper can be improved for better readability and flow particularly in the sections discussing architecture details and experiments.  Questions and Suggestions for Authors: 1. Generalization: How does Sequencer perform on datasets beyond ImageNet and what is its performance variability with diverse datasets 2. Comparative Analysis: Have you compared Sequencer with other LSTMbased vision architectures or hybrids that combine attention and LSTM for sequence modeling 3. Incremental Learning: How well does Sequencer adapt to incremental learning scenarios where new data is continuously introduced for model updates 4. Inference Speed: Are there strategies or optimizations planned to enhance the inference speed of Sequencer without sacrificing accuracy 5. Open Source: Mention providing the source code is beneficial. Have you ensured clear documentation and ease of use for researchers who might want to implement or experiment with Sequencer  Limitations:  Addressing Throughput Concerns: To improve the practical applicability of Sequencer enhancing the throughput for efficient training and inference should be a focus area for future work. Overall the paper presents an intriguing architecture with promising results. Addressing the highlighted weaknesses and considering the suggested questions could further strengthen the work and its applicability in realworld scenarios.", "vfR3gtIFd8Y": " Summary: The paper introduces a new method for forward variable selection in the context of Gaussian processes with KarhunenLo\u00e8ve decomposed kernels specifically focusing on the Bayesian Smoothing Spline ANOVA kernel. The method offers competitive accuracies training times and inference speeds for tabular datasets with low feature set dimensionality. Experimental evaluations on dynamic datasets showcase superior performance over neural networks in predicting derivatives and capturing correct dynamics.  Strengths: 1. Innovative Contribution: The paper presents a novel method for forward variable selection in Gaussian processes addressing high dimensionality issues. 2. Experimental Evaluations: The study conducts comprehensive experiments on dynamic datasets showcasing the effectiveness of the proposed method compared to neural networks. 3. Thorough Method Description: The detailed algorithm description and implementation details provide clarity on the proposed approach.  Weaknesses: 1. Limited Application: The method is demonstrated on specific dynamic datasets with low feature dimensionality potentially limiting its generalizability to broader applications. 2. Comparative Analysis: While the paper compares the proposed method with neural networks a more extensive comparison with other stateoftheart methods could enhance the studys credibility.  Questions and Suggestions: 1. Generalization: How transferable is the proposed method to datasets with higher feature dimensionality or different characteristics 2. Scalability: Have the authors tested the scalability of the method on larger datasets to assess its performance in realworld applications 3. Model Interpretability: Can the authors provide insights into the interpretability of the selected variable terms and their impact on model performance  Limitations: The authors have adequately discussed the methods limitations and should aim to address broader datasets and comparison with additional stateoftheart methods for more comprehensive validation.  Suggestions for Improvement: 1. Expand the experimental evaluation to include diverse datasets and comparison with a wider range of modeling approaches. 2. Provide a clear roadmap for transitioning the method from lowdimensional tabular data to more complex realworld datasets. 3. Enhance the discussion on the interpretability of the model and the selected variables for practical applications.", "vNrSXIFJ9wz": "Peer Review Summary: The paper proposes a methodology for addressing challenges in Vertical Federated Learning (VFL) by introducing Vertically Federated Participant Selection (VFPS) based on mutual information estimation. The authors present VFMINE as an information estimator and VFPS as a participant selection framework achieving improved speed and selection accuracy in training VFL models compared to traditional approaches. Strengths: 1. Original Methodology: The approach of selecting a subset of important participants in VFL through mutual information is novel and addresses key challenges faced in existing methods. 2. Technical Contributions: The introduction of VFMINE and VFPS showcases technical innovation in efficiently estimating mutual information and optimizing participant selection. 3. Empirical Validation: The empirical results demonstrate the effectiveness of the proposed methodology showing significant speed improvements and maintained model quality. 4. Security Considerations: The paper provides a detailed security analysis ensuring data privacy and protection in the proposed methods. Weaknesses: 1. Limited Experimental Benchmarks: While the evaluation demonstrates the effectiveness of VFPS the comparison with a broader range of baselines and realworld datasets could enhance the robustness of the findings. 2. Theoretical Justification: More indepth theoretical discussions on the choice of mutual information for participant selection and comparison with other relevant feature selection criteria could provide a stronger foundation for the proposed methodology. 3. Scalability Concerns: The paper acknowledges the limitations of largescale federated networks and the potential performance degradation of VFPS in such scenarios. Addressing this scalability issue with potential solutions could strengthen the applicability of the proposed methodology. Questions and Suggestions: 1. Can the authors provide insights into the interpretability of the selected subset of participants by VFPS especially in scenarios where participant importance varies over training stages 2. How does VFPS handle cases where participants exhibit dynamic behavior in contributing to model quality and how could the selection methodology adapt to such scenarios 3. Are there plans to extend VFPS to other metrics or criteria for participant selection beyond mutual information acknowledging possible bias and fairness issues in the selection process Limitations: The paper does not comprehensively address the potential negative societal impact of their work. To improve this aspect the authors could consider conducting an ethical impact assessment and incorporating fairness considerations in participant selection criteria. Overall the paper presents a promising approach to address challenges in VFL offering valuable technical insights and empirical validation. Further enhancements in theoretical justification scalability and interpretability aspects could enhance the robustness and applicability of the proposed methodology.", "pBJe5yu41Pq": " Summary: The paper investigates the impact of explicitly incorporating aleatoric uncertainty in Bayesian neural networks focusing on classification tasks. It introduces the concept of aleatoric uncertainty in Bayesian regression and classification and demonstrates how accounting for aleatoric uncertainty can improve model performance. The paper discusses the limitations of standard Bayesian models in capturing aleatoric uncertainty accurately especially in the context of data augmentation and proposes alternative approaches like tempered likelihood and noisy Dirichlet model to address this issue.  Strengths:  The paper presents a comprehensive exploration of aleatoric uncertainty in Bayesian neural networks for classification.  It provides clear theoretical explanations along with empirical results to support the proposed methods.  The experiments conducted to validate the findings are welldocumented and provide insights into the practical implications of the proposed approaches.  The discussion on the impact of data augmentation on model performance adds valuable context to the research.  Weaknesses:  The paper could benefit from more indepth discussion on the practical implications and scalability of the proposed approaches in realworld applications.  While the theoretical underpinnings are sound practical implementation details and potential challenges in using the proposed methods need further elaboration.  The paper may appear technical and might require additional clarification in certain sections for readers unfamiliar with Bayesian methods.  Questions and Suggestions for Authors: 1. Could you provide more insights into how the proposed methods could be integrated into existing Bayesian neural network frameworks for classification tasks 2. Have you considered evaluating the proposed approaches on a broader range of datasets to assess their generalizability 3. Can you discuss the computational overhead associated with implementing the noisy Dirichlet model in comparison to standard Bayesian approaches  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work. To improve authors could further discuss the implications of model misspecification and computational complexity in practical applications.", "tfkeJG9yAX": " Summary: The paper studies the limits of the expressive power of neural networks by analyzing the approximation properties in Lp norm for various function sets. The main contributions include a general lower bound theorem and its application to H\u00f6lder balls and multivariate monotonic functions. The study sheds light on the differences between approximation in Lp norm and sup norm providing valuable insights into network architecture design constraints.  Strengths: 1. Rigorous Analysis: The paper provides a rigorous mathematical analysis of the approximation error bounds in Lp norm for different function sets. 2. Novel Results: The paper presents new lower bounds on approximation errors resolving open questions and providing valuable contributions to understanding the limits of neural networks approximation capabilities. 3. Clear Structure: The paper is wellstructured with clear explanations definitions and logical flow in presenting the results.  Weaknesses: 1. Lack of Empirical Validation: The paper lacks empirical validation of the theoretical results using practical datasets or experiments which could strengthen the relevance of the findings in realworld settings. 2. Limited Discussion on Generalization: While the focus is on approximation properties a discussion on how the findings relate to the generalization capabilities of neural networks could have added depth to the paper.  Questions and Suggestions: 1. Empirical Validation: Have you considered validating the theoretical findings with empirical experiments on realworld datasets or synthetic functions to demonstrate the practical implications of the derived bounds 2. Comparison with Existing Architectures: How do the derived lower bounds compare with existing network architectures in terms of their approximation capabilities Is there any guidance for network design based on the theoretical findings 3. Extension to Higher Dimensions: Have you considered the scalability of the proposed lower bounds to higherdimensional spaces and the potential challenges or implications of such extensions  Limitations: The paper does not address the potential negative societal impacts of the work explicitly. It would be beneficial to include a section discussing any ethical implications or societal consequences of the findings to provide a more holistic view of the research.", "oDoj_LKI3JZ": " Summary: The paper proposes an algorithm for sparse Gaussian process regression using MCMC to sample from the hyperparameter posterior within the variational inducing point framework. It aims to address the limitations of MLII such as biased estimates and underestimation of predictive uncertainty. The work focuses on the benefits of fully Bayesian schemes in sparse Gaussian process models comparing them with the evidence framework and other benchmarks providing computational analysis and extensive experiments.  Strengths: 1. Innovative Approach: The paper presents a novel algorithm using MCMC for sparse Gaussian process regression within the variational inducing point framework. 2. Thorough Analysis: The work provides a comprehensive comparison with existing benchmarks and schemes offering insights into computational efficiency and predictive performance. 3. Experimental Validation: Extensive experiments on synthetic and realworld datasets demonstrate the effectiveness of the proposed scheme over existing methods. 4. Clear Presentation: The paper is wellorganized and effectively communicates complex concepts related to Gaussian processes and Bayesian inference.  Weaknesses: 1. Complexity in Explanations: The paper contains highly technical language and mathematical derivations making it challenging for readers unfamiliar with Gaussian processes to grasp the content easily. 2. Limited Focus: The paper primarily focuses on the method and its comparison with existing benchmarks lacking detailed discussions on broader implications or practical applications of the proposed algorithm.  Questions and Suggestions for Authors: 1. Clarity in Explanations: Could the authors simplify some of the technical explanations to enhance accessibility for a wider audience 2. Broader Impact: How might the proposed algorithm benefit other domains outside of Gaussian process regression 3. Scalability Concerns: Have scalability issues been thoroughly addressed especially when scaling the method to larger datasets or more complex models 4. Robustness Assessment: How does the proposed algorithm perform in terms of robustness to outliers or noisy data compared to existing methods  Limitations: The authors could enhance the discussion on the potential negative societal impacts of their work by investigating how the proposed method might introduce biases or challenges if applied in critical decisionmaking systems within sectors like healthcare or finance. They should also consider providing means to mitigate any negative societal effects.", "yW5zeRSFdZ": "Summary: The paper delves into the challenges of quantizing Transformerbased models due to large outliers particularly in LayerNorm structures. It introduces an outlier suppression framework consisting of Gamma Migration and TokenWise Clipping to address this issue. Gamma Migration transfers the outlier amplifier to subsequent modules while TokenWise Clipping efficiently finds suitable clipping ranges. Extensive experiments demonstrate that the proposed framework outperforms existing methods achieving fullprecision levels with 6bit quantization for BERT models. Strengths: 1. Indepth Analysis: The paper provides a detailed analysis of outliers their inducement and the impact of clipping on quantization performance. 2. Innovative Framework: The proposed outlier suppression framework with Gamma Migration and TokenWise Clipping effectively addresses the outlier issue without additional computational burden. 3. Extensive Experiments: The paper includes comprehensive experiments across various NLP tasks and models showcasing the superiority of the proposed framework over existing methods. 4. Quantitative Validation: The paper offers quantitative validation of the proposed methods through experiments and comparisons with baseline techniques. Weaknesses: 1. Complexity: The paper could be challenging to follow for readers without a strong background in NLP and quantization due to the technical depth of the content. 2. Generalization: The focus on NLP models limits the generalizability of the findings to other domains or applications. Questions and Suggestions for Authors: 1. Robustness: How robust is the proposed outlier suppression framework to variations in model architectures or tasks beyond NLP 2. Scalability: Can the proposed methods scale effectively to larger models or more complex architectures 3. RealWorld Implementation: Have the authors considered realworld deployment challenges and constraints when implementing their framework 4. Comparison Metrics: Have the authors considered additional metrics beyond accuracy to evaluate the effectiveness of their framework such as inference speed or resource utilization Limitations: The authors could enhance the paper by addressing:  Negative Societal Impact: Consider discussing potential negative societal impacts or ethical considerations related to the deployment of quantized models especially in critical applications like healthcare or finance.", "ttC9p-CtYT": " Peer Review:  1) Summary: The paper presents a new family of quadratic functions based on the normalized advantage functions (NAF) algorithm for solving reinforcement learning problems obtained by the timediscretization of optimal control problems with continuous action spaces. The paper proves the approximation properties of the proposed functions and provides improvements to the NAF algorithm. Experimental results show the efficiency of the proposed algorithms.  2) Strengths:  The paper addresses an important problem of applying reinforcement learning to continuous action spaces.  The theoretical analysis is rigorous with detailed proofs accompanying the proposed functions.  The experimental results validate the effectiveness of the proposed algorithms showcasing their potential for solving optimal control problems.  Weaknesses:  The paper could benefit from clearer explanations in some sections especially for readers not familiar with the NAF algorithm and its extensions.  The methods used for the experimental evaluation could be further detailed to provide a deeper understanding of their implementation and performance.  3) Questions and Suggestions for the Authors:  Could you provide more insights into the specific enhancements introduced in the proposed algorithms compared to the original NAF algorithm  It would be beneficial to elaborate on the computational complexity of the improved NAF algorithms in comparison to existing methods.  How do the proposed algorithms handle cases of highdimensional state and action spaces and are there any known limitations in scaling to such scenarios  4) Limitations: The authors could expand on the potential negative societal impacts of their work notably in terms of the computational resources required for training the proposed algorithms on largescale problems. Suggestions for improvement include conducting a detailed analysis of the energy consumption and environmental impact associated with training these algorithms.  Overall Assessment: The paper makes a significant contribution to the field of continuous deep reinforcement learning offering a new family of quadratic functions and improved algorithms for solving optimal control problems. The rigorous theoretical analysis and supportive experimental results strengthen the credibility of the proposed methods. However enhancing the clarity of the manuscript in certain sections and addressing scalability concerns would further improve the impact of the research.", "zbt3VmTsRIj": " Summary: The paper introduces a novel singleloop algorithm for Inverse Reinforcement Learning (IRL) that aims to accurately estimate both rewards and behavior policies. The proposed algorithm converges to a stationary solution with theoretical guarantees and outperforms existing IRL and imitation learning benchmarks in robotics control problems and transfer settings.  Strengths: 1. Novel Algorithm: The paper introduces a unique singleloop algorithm for IRL that addresses the computational burden of nestedloop structures without compromising reward estimation accuracy. 2. Theoretical Guarantees: The algorithm is shown to converge to a stationary solution with finitetime guarantees. 3. Experimental Results: Extensive experiments demonstrate superior performance compared to existing IRL algorithms on highdimensional robotics tasks and transfer learning scenarios. 4. StateOnly Reward Analysis: The paper discusses the implications and benefits of using a stateonly reward function for transfer learning and counterfactual analysis.  Weaknesses: 1. Comparison: While the paper provides comprehensive comparisons with existing algorithms more detailed comparison metrics and analysis could enhance the discussion. 2. Complexity: The algorithm relies on correct parameterization and involves stochastic gradient estimation which may introduce complexity and potential challenges.  Questions and Suggestions: 1. Generalizability: How well does the proposed algorithm generalize across different types of expert demonstrations and environments 2. Scalability: How does the algorithm perform in scaling up to more complex and larger environments 3. Robustness: Have the authors considered robustness to noisy or biased expert demonstrations and how would the algorithm handle such cases  Limitations: The authors should include a discussion on potential negative societal impacts such as biases from expert demonstrations and provide suggestions for mitigating these impacts to ensure safe and responsible deployment of the algorithm in sensitive applications.", "m8YYs8nJF3T": " PeerReview:  Summary: The paper introduces the Sliced Wasserstein Process a stochastic process defined by the empirical Wasserstein distance between projections of empirical probability measures onto onedimensional subspaces offering a unified framework for studying Wasserstein distances based on onedimensional projections. It establishes distributional limit theorems enabling construction of confidence intervals and validating the bootstrap method for various modified Wasserstein distances. The work is supported by theoretical results simulations and applications to sliced maxsliced and distributional sliced Wasserstein distances.  Strengths: 1. Innovative Contribution: The paper addresses an important challenge of the highdimensional Wasserstein distance computation offering a novel approach through the Sliced Wasserstein Process. 2. Theoretical Rigor: The paper provides a solid theoretical foundation by proving distributional convergence results and addressing sample complexity for a range of Wasserstein distance variants. 3. Unified Framework: By developing a unified approach to analyze various Wasserstein distance variants simultaneously the paper contributes significantly to simplifying the study and application of these distances. 4. Simulation Studies: The inclusion of simulation studies exemplifies the practical relevance of the proposed methodology and shows consistency with the theoretical results.  Weaknesses: 1. Complexity: The paper is highly technical and requires a strong background in mathematics and statistics to comprehend fully. Consider clarifying certain sections or providing more intuitive explanations for nonexperts. 2. Clarity of Presentation: Some sections could benefit from improved clarity and organization to enhance readability and facilitate the understanding of the key insights for researchers without specialized knowledge in this area.  Questions and Suggestions: 1. The paper mentions assumptions like compact support and uniqueness of potentials. Can the authors elaborate on the potential implications of relaxing these assumptions and are there practical scenarios where these assumptions might not hold 2. How does the proposed framework perform in practice compared to existing methods when dealing with realworld highdimensional data sets in machine learning applications 3. Could the authors discuss potential future directions for extending this work to different domains or applications beyond Wasserstein distances leveraging the insights gained from this study  Limitations: The authors could enhance the discussion regarding the potential societal impact of their work especially concerning the application of modified Wasserstein distances in realworld scenarios. Providing insights into the ethical implications and discussing ways to mitigate any negative impacts would further strengthen the paper. Overall the paper makes a significant contribution to the field of machine learning and statistics by offering a unified approach to studying Wasserstein distances in highdimensional contexts. It provides valuable insights but addressing the mentioned weaknesses and questions would further enhance the clarity and impact of the work.", "lblv6NGI7un": " Summary: The paper proposes ERIC an efficient framework for graph similarity computation based on graph edit distance (GED) estimation. ERIC introduces the Alignment Regularization (AReg) to enable the model to capture underlying alignment information between pairwise graphs and a multiscale GED discriminator to enhance the discriminability of learned representations. Extensive experiments demonstrate the effectiveness efficiency and transferability of ERIC compared to stateoftheart methods.  Strengths: 1. Innovative Approach: Introducing the Alignment Regularization and multiscale GED discriminator provides a new perspective on graph similarity computation different from existing methods. 2. Efficiency: ERIC eliminates the need for expensive nodetonode matching modules in inference making it computationally efficient. 3. Empirical Results: Extensive experiments on realworld datasets show that ERIC achieves stateoftheart performance in accuracy and efficiency.  Weaknesses: 1. Theoretical Clarity: While the concept of ERIC is innovative the paper could benefit from clearer theoretical explanations and derivations to help readers understand the reasoning behind the proposed method better. 2. Detailed Implementation: More details on the implementation of ERIC especially regarding the hyperparameters and experimental setup would be beneficial for reproducibility and comparison with future works. 3. Comparison Metrics: While the performance metrics presented in the results are comprehensive additional comparisons with more recent stateoftheart methods could provide a more robust evaluation.  Questions and Suggestions for Authors: 1. Did you explore the impact of hyperparameters in the AReg and GED discriminator components How sensitive is the models performance to changes in these hyperparameters 2. Have you considered incorporating interpretability techniques to understand how learned graph representations influence the similarity computation process 3. How does ERIC scale with larger graph datasets in terms of both performance and computational efficiency  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work by focusing on the technical aspects of the proposed framework. To enhance the societal impact future work could include considerations of ethical implications and potential biases introduced by AIbased similarity computations.", "wUUutywJY6": " PeerReview of \"SoLar: Sinkhorn Label Refinery for PartialLabel Learning with LongTailed Distribution\"  Summary: The paper introduces a novel framework SoLar based on Optimal Transport for the challenging problem of imbalanced partiallabel learning with a longtailed label distribution. SoLar aims to refine pseudolabels to align with the true class prior distribution. The proposed framework incorporates a movingaverage estimation for class prior distribution and a reliable sample selection mechanism achieving superior performance compared to existing stateoftheart methods on standardized benchmarks.  Strengths: 1. Novel Framework: SoLar introduces a unique approach using Optimal Transport for label refinement addressing the challenge of longtailed distribution in partiallabel learning. 2. Theoretical Consistency: The paper provides a theoretical foundation proving the statistical consistency of the model under a certain condition enhancing the credibility of the proposed framework. 3. Extensive Experiments: The paper conducts comprehensive experiments on various benchmark datasets demonstrating the impressive performance of SoLar compared to existing methods. 4. Code Availability: Providing the code and data availability on GitHub enhances reproducibility and encourages further research in the field.  Weaknesses: 1. Complexity: The proposed framework involves several components including Optimal Transport movingaverage class prior estimation and sample selection mechanisms which may increase the complexity and computational overhead of the model. 2. Evaluation: While the paper demonstrates superior results empirically it lacks a detailed discussion on the interpretability of the model and the potential tradeoffs involved in utilizing various components of the framework.  Questions and Suggestions: 1. Model Interpretability: Could the authors provide insights on the interpretability of the optimized label assignments achieved by SoLar especially in scenarios with high label ambiguity and longtailed distributions 2. Scalability: How does the proposed framework scale with larger datasets or more complex label distributions considering the efficiency aspects of the SinkhornKnopp algorithm and the class prior estimation techniques employed  Limitations: The authors might consider offering further details on the scalability and interpretability of the model. Additionally explaining potential negative societal impacts of the research or ethical considerations could enhance the papers completeness.  Suggestions for Improvement: 1. Interpretability Analysis: Include a detailed analysis of the interpretability of the refined label assignments produced by SoLar. 2. Scalability Evaluation: Provide insights into the scalability of the proposed framework especially regarding efficiency with larger datasets and more complex label distributions. 3. Consider Ethical Implications: Reflect on any potential negative societal impacts or ethical considerations related to the research and suggest ways to mitigate them. Overall the paper presents a significant advance in addressing the challenging problem of longtailed distribution in partiallabel learning and provides a strong foundation for future research in the field. With some enhancements in the areas of model interpretability scalability and ethical considerations the papers impact could be further strengthened.", "y8FN4dHdxOE": " Review of the Paper 1. Summary and Contributions: The paper introduces a multimode tensor clustering method FusedOrthALS for simultaneous tensor factorization and clustering in the presence of noise. The key contributions include theoretical analysis of statistical convergence rates for recovery and clustering as well as demonstrating the algorithms accuracy and efficiency through simulations and real datasets. 2. Strengths:  The paper addresses a critical problem of multiway tensor clustering offering a novel algorithm with theoretical guarantees for recovery and clustering consistency.  The incorporation of a fused version of the alternating least squares algorithm with orthogonalization for faster convergence and avoidance of local optima is a notable methodological advancement.  The paper provides a thorough comparison with existing methods showcasing superior performance in both synthetic and real datasets.  The proposed regularization scheme with graphbased penalties demonstrates robustness in clustering challenging structures as supported by numerical experiments. 3. Weaknesses:  The paper is dense with technical details potentially overwhelming readers unfamiliar with tensor decomposition and optimization algorithms.  The exposition of mathematical notations and assumptions may be daunting for readers not wellversed in advanced mathematical concepts.  Although the proposed method shows promising results the explanation of practical implementation aspects could be more elaborate to aid in realworld applications.  Questions and Suggestions for Authors 1. Theoretical Study: Providing further insights on the relationship between convergence rates and various parameters such as cluster size tensor dimensionality and noise level could enhance the theoretical foundation. 2. Implementation Detail: Explain in more detail the practical implementation of the algorithm including initialization strategies and parameter tuning for realworld applications. 3. Robustness Analysis: Investigate the algorithms performance under scenarios with missing data entries as such situations are common in realworld datasets.  Limitations The authors adequately addressed the limitations and potential negative societal impact of their work by not identifying any foreseeable negative social impacts. A constructive suggestion for improvement could be to explicitly mention the ethical considerations and potential biases that may arise from the algorithms application in realworld scenarios. Overall the paper presents a comprehensive exploration of multimode tensor clustering enriched with theoretical analysis and empirical validations making a significant contribution to the field. Clarification of practical implementation and further investigations into specific aspects could enhance the impact and applicability of the proposed method.", "lHy09zPewmD": " Peer Review Summary: The paper introduces a generalization of Bandits with Knapsacks (BwK) model that allows nonmonotonic resource utilization. It presents an MDP policy with constant regret against an LP relaxation when true outcome distributions are known and a learning algorithm with logarithmic regret when the true distributions are unknown. The paper also includes a reduction from BwK to their model and matches existing results. Various theorems lemmas and algorithms are presented demonstrating the effectiveness of the proposed approach. Strengths: 1. Novelty: The paper addresses an important extension of the existing BwK model which is a significant contribution to the field of sequential decisionmaking under uncertainty. 2. Theoretical Rigor: The paper provides a comprehensive theoretical analysis including theorems lemmas and algorithms that are wellstructured and logically sound. 3. Clarity: The writing is clear and precise making it accessible to readers who have a background in bandit models. 4. Algorithm Development: The proposed MDP policy and learning algorithm are welldefined making them easily understandable for implementation. Weaknesses: 1. Empirical Evaluation: The paper lacks empirical validation of the proposed algorithms. Including simulations or realworld case studies would provide practical insights into the performance of the algorithms. 2. Complexity: The paper involves intricate theoretical details that might be overwhelming for readers not deeply familiar with bandit models and optimization frameworks. Simplifying the presentation without sacrificing the core concepts could enhance readability. Questions and Suggestions: 1. Clarity on Resource Drifts: It would be helpful to provide more intuitive explanations or examples to clarify the concept of resource drifts for readers to grasp the significance of this parameter in the model. 2. Implementation Challenges: How feasible and computationally efficient are the proposed algorithms in realworld applications Providing insights into the practical implementation challenges and considerations would enhance the applicability of the research. Limitations: The paper does not address the limitations and potential negative societal impacts of the research. To improve the societal relevance and ethical considerations of the work the authors should include a section highlighting these aspects along with suggestions for mitigating any adverse consequences. Overall the paper presents a solid theoretical advancement in the field of sequential decisionmaking under uncertainty. By addressing the weaknesses and incorporating the suggestions mentioned above the authors can further enhance the impact and applicability of their research.", "lmmKGi7zXn": " Peer Review:  Summary: The paper introduces two novel approaches: \\xe2\\x88\\x9eAE an autoencoder with infinitelywide bottleneck layers for recommendation and DISTILLCF a data distillation framework for synthesizing small yet informative data summaries for collaborative filtering datasets. \\xe2\\x88\\x9eAE outperforms stateoftheart models in recommendation tasks showcasing the superiority of infinitewidth networks in practical scenarios. DISTILLCF complements \\xe2\\x88\\x9eAE by providing highquality data summaries demonstrating strong denoising effects and scalability to large datasets. The experiments show that both approaches significantly outperform existing methods and are effective even with a tiny fraction of the original data.  Strengths: 1. Novel Contributions: Introducing \\xe2\\x88\\x9eAE and DISTILLCF as innovative solutions for recommendation tasks showcasing significant performance improvements over existing methods. 2. Theoretical Basis: Leveraging the Neural Tangent Kernel theory and closedform solutions to develop highly effective and efficient models. 3. Complementary Nature: Demonstrating how \\xe2\\x88\\x9eAE and DISTILLCF synergistically improve each others practicality and performance. 4. Experimental Rigor: Thorough experimentation on various datasets comprehensive evaluation metrics and comparison with baseline and stateoftheart methods.  Weaknesses: 1. Complexity: While the paper presents intricate mathematical formulations and algorithms it might pose challenges for readers not wellversed in neural network theory and optimization techniques. 2. Scalability Discussion: While a scalability analysis is provided more details on handling even larger datasets and practical implementation challenges especially in realworld settings would be beneficial. 3. Generalization to Other Domains: The paper primarily focuses on recommendation systems discussing potential applications of the proposed methods in other domains could enhance the papers impact and relevance.  Questions and Suggestions for Authors: 1. Can you elaborate on the interpretability of the models developed particularly in understanding the latent representations learned by \\xe2\\x88\\x9eAE 2. Have you considered discussing the interpretability and explainability aspects of DISTILLCF especially in generating informative data summaries for downstream applications 3. How robust are the proposed models to changes in the distribution of useritem interactions or shifts in the underlying data characteristics 4. In what ways do the proposed approaches address potential biases in the data and how can they be adapted for handling biased datasets or addressing fairness concerns  Limitations: The authors could further expand on discussing the potential negative societal impact of their work especially concerning privacy risks associated with data handling and the challenges of maintaining data quality while downsampling. It would be beneficial to provide more insight into strategies for mitigating these impacts and ensuring ethical data usage in practical scenarios. Overall the paper presents innovative approaches in recommendation modeling supported by strong theoretical foundations and empirical results. Addressing the highlighted weaknesses and questions could further enhance the clarity robustness and applicability of the proposed methods.", "lSqaDG4dvdt": " Peer Review  Summary: The paper proposes a genetic programming framework EZNAS to automate the discovery of zerocost proxies for neural architecture scoring. The methodology efficiently discovers an interpretable and generalizable zerocost proxy that achieves stateoftheart scoreaccuracy correlation on various datasets and search spaces. The research signifies a promising direction towards automatically discovering zerocost proxies that can work across network architecture design spaces datasets and tasks.  Strengths: 1. Innovative Methodology: The use of genetic programming to automate the discovery of zerocost proxies is innovative and addresses a significant challenge in Neural Architecture Search. 2. Interpretable Results: EZNAS generates interpretable zerocost proxies as expression trees allowing for a clear understanding of the features and operations involved in scoring neural networks. 3. Generalizability: The discovered ZCNASM exhibits strong scoreaccuracy correlation across different datasets and design spaces demonstrating generalizability and potential practical utility. 4. Efficiency: The paper demonstrates the efficiency of EZNAS with the ability to discover the ZCNASM within 24 hours on a single CPU leading to a significant reduction in carbon footprint compared to traditional NAS methods.  Weaknesses: 1. Limited External Validation: The paper lacks external validation or comparison with existing benchmarks outside of the proposed NASBench201 and NDS design spaces which could provide a more comprehensive evaluation of the methodologys effectiveness. 2. Complexity of Methodology: The methodology while innovative may be complex for readers unfamiliar with genetic programming and neural architecture search potentially hindering the reproducibility and adoption of the approach. 3. Limited RealWorld Implementation Discussion: The paper could benefit from a more thorough discussion on the practical implementation and adoption of EZNAS in realworld scenarios beyond the experimental setup.  Questions and Suggestions for Authors: 1. Clarification on Performance Metrics: Could the authors provide a more detailed comparison of EZNASA with existing ZCNASMs in terms of performance metrics such as accuracy efficiency and scalability 2. Scalability Considerations: How does the proposed methodology scale with larger and more complex neural architecture design spaces and what strategies could be implemented to ensure efficient discovery of ZCNASMs in such scenarios 3. Robustness Testing: Have the authors conducted robustness testing of the discovered ZCNASM across different hyperparameters datasets and search spaces to verify its stability and reliability  Limitations: The authors could further address the limitations by considering:  Validation on Diverse Datasets: Validate the proposed methodology on diverse datasets to ensure its effectiveness across various applications.  Negative Societal Impact: Assess and discuss any potential negative societal impacts such as biases or ethical concerns arising from the deployment of EZNAS and provide mitigation strategies. The paper presents a significant contribution to the field of Neural Architecture Search by introducing a novel approach to automating the discovery of zerocost proxies. Addressing the suggested questions and limitations could further enhance the robustness and practicality of the proposed methodology.", "rA2tItoRUth": " Summary: The paper introduces a LanguageGuided Denoising Network (LGDN) for videolanguage modeling to address the challenges of noisy videotext data. LGDN dynamically filters out irrelevant frames under language supervision and selects salient frames for tokenlevel alignment. The proposed model outperforms existing methods on five public datasets and includes detailed ablation studies to highlight the importance of noise removal in videolanguage modeling.  Strengths: 1. Innovative Approach: Introducing a mechanism to filter out irrelevant frames based on language supervision is a novel and effective way to address noise in videotext data. 2. Comprehensive Experiments: Extensive experiments on multiple datasets demonstrate the superior performance of LGDN over existing methods. 3. Detailed Methodology: The paper provides a thorough explanation of the proposed model at different levels facilitating understanding and reproducibility. 4. Impactful Contributions: The results show significant improvements over stateoftheart models indicating the practical relevance of LGDN in realworld applications.  Weaknesses: 1. Complexity: The model architecture involving multiple modules and stages might increase computational complexity which could be a potential limitation in practical deployments. 2. Lack of Qualitative Analysis: While the work is supported by quantitative results additional qualitative analysis or case studies could further illustrate the effectiveness of the proposed approach.  Questions and Suggestions: 1. Can the authors provide insights into the potential scalability of the proposed model for larger datasets or realtime applications 2. How does the proposed model handle various kinds of noise in video data such as motion artifacts occlusions or lighting variations 3. Have the authors considered the computational efficiency and inference speed of LGDN for deployment in realworld video understanding systems  Limitations: The authors should address the computational complexity and inference speed of the proposed model to ensure practical applicability in realtime video applications. Additionally providing qualitative analyses alongside quantitative results would enhance the credibility and understanding of the work.", "lYZQRpqLesi": " PeerReview of \"Alternating Sparse Training for Dynamic Inference\" Summary: The paper introduces a novel Alternating Sparse Training (AST) scheme for dynamic inference. It aims to train multiple sparse subnets at once to enable runtime or posttraining network morphing without additional training costs. By alternating the training of subnets with different sparsity values the paper eliminates the large training overhead associated with joint training schemes and proposes gradient correction to mitigate interference between subnets. Extensive experiments on CIFAR10 CIFAR100 and ImageNet datasets validate the effectiveness of AST in achieving high accuracy and training efficiency. Strengths:  Innovation: The proposed AST scheme is innovative and addresses a crucial issue in dynamic inference without incurring extra training costs.  Experimental Validation: Comprehensive experiments on multiple datasets and models demonstrate the efficacy of AST in achieving high accuracy and reduced training costs. Comparison with stateoftheart methods strengthens the papers claims.  Theoretical Justification: The theoretical analysis of gradient correction and the implicit regularization of SGD add depth to the proposed approach.  Clarity: The paper is wellstructured with clear explanations of the methodology and experiments. Figures aid in understanding the proposed scheme. Weaknesses:  Limited Comparison: While the paper compares with joint training methods a broader comparison with diverse pruning approaches or dynamic inference strategies would enhance the papers contribution.  Lack of Analysis: The paper could provide deeper analysis on the impact of sparsity ratios network architectures or gradient correction parameters on the performance of AST.  Future Directions: While mentioned briefly in the limitations section more detailed discussions on extending AST to support larger numbers of subnets and different hardware platforms could add value. Questions and Suggestions for Authors: 1. Could you elaborate on the criteria used to select subnet architectures for AST How does this affect the final performance 2. How sensitive is AST to hyperparameters like sparsity ratios or the adjustment period between subnets 3. Can you provide insights into the tradeoffs between accuracy and training efficiency when scaling AST to a higher number of subnets 4. Have you considered realworld deployments and hardware constraints when designing AST How would AST adapt to diverse hardware environments Limitations:  The authors could further explore how AST adapts to various hardware platforms to ensure its practical efficiency. Overall the paper presents a significant contribution with the AST scheme for dynamic inference demonstrating high efficiency and accuracy. By addressing the weaknesses and considering the suggestions the authors can further enhance the impact and applicability of their work.", "oNnv9XjClGK": " Review of \"Direct Advantage Estimation (DAE)\"  Summary: The paper proposes Direct Advantage Estimation (DAE) a novel onpolicy method for estimating the advantage function directly in reinforcement learning. The authors show that the advantage function can be interpreted as causal effects leading to the development of DAE which models and estimates the advantage function from sampled trajectories. DAE eliminates the need to learn the value function or Qfunction beforehand and can be integrated into modern actorcritic methods. Empirical evaluations on three discrete control domains demonstrate that DAE outperforms the generalized advantage estimation (GAE) method on various environments when applied to policy optimization.  Strengths: 1. Novel Contribution: The paper presents a novel approach DAE which directly estimates the advantage function offering a unique perspective on causal effects in reinforcement learning. 2. Theoretical Justification: The connection between the advantage function and causal effects is established enriching the theoretical understanding of credit assignment in RL. 3. Empirical Evaluation: Rigorous empirical evaluations on synthetic MinAtar and ALE environments demonstrate the superiority of DAE over baseline methods showcasing its effectiveness.  Weaknesses: 1. Lack of Comparison: While the paper extensively compares DAE with GAE a comparison with other stateoftheart methods could provide a broader perspective on its performance. 2. Clarity in Methodology: The paper could improve the clarity of the algorithmic details and experimental setup for better reproducibility and understanding.  Questions and Suggestions: 1. How does DAE perform when compared to other direct advantage estimation methods or stateoftheart methods in reinforcement learning beyond GAE 2. Can you provide more insights into the computational complexity of DAE compared to traditional methods like GAE  Suggestions for Authors: 1. Enhance the clarity of algorithmic descriptions and experimental setups to facilitate reproducibility. 2. Consider comparing DAE with additional baseline methods to further validate its effectiveness in a broader context.  Limitations: The limitations could include the need for further exploration of DAE in more complex environments and offpolicy settings as well as ensuring scalability to continuous action spaces for wider applicability. Consider addressing these limitations for a more comprehensive evaluation of DAE.", "pNHT6oBaPr8": "1) Summary: The paper introduces Partial Group equivariant CNNs (Partial GCNNs) a novel design for constructing equivariant neural networks that can learn layerwise levels of partial or full equivariance directly from data. These networks are able to adjust their level of equivariance at each layer during training outperforming conventional GCNNs in tasks where full equivariance may be harmful. The authors present experimental results on toy tasks MNIST CIFAR10 and CIFAR100 datasets showcasing the effectiveness of Partial GCNNs in learning different levels of equivariance tailored to the data. 2) Strengths:  Innovative Approach: The concept of Partial GCNNs learning layerwise equivariance levels is novel and addresses a critical issue in traditional CNNs.  Experimental Validation: The comprehensive evaluation on toy tasks and benchmark datasets demonstrates the practical utility and advantage of Partial GCNNs.  Clear Exposition: The paper provides a clear explanation of group theory concepts and the methodology adopted for learning partial equivariance. 3) Weaknesses:  Complexity: The theoretical underpinnings and technical discussions might be challenging for readers unfamiliar with group theory and convolutional neural networks.  Experiments on Limited Datasets: The experiments are confined to a few datasets and it might be beneficial to explore a broader range of datasets for extensive validation.  Practical Implementation: There could be potential challenges in the practical implementation of Partial GCNNs outside of experimental settings. 4) Questions and Suggestions for Authors:  Could the authors elaborate on the interpretability of the learned partial equivariances in realworld applications  How does the computational overhead of learning and implementing partial equivariance affect the scalability of Partial GCNNs in larger models and datasets  Have the authors considered the potential ethical considerations of deploying models that adapt their equivariance levels dynamically 5) Limitations: The authors have not specifically addressed the potential limitations and negative societal impact of their work. They could consider discussing the computational complexity and resources required for deploying Partial GCNNs in practical settings and the impact of model interpretability on decisionmaking processes. Overall the paper presents a significant advancement in the field of convolutional neural networks by introducing a novel approach for learning partial equivariance. The detailed experimental validation and clarity in presentation make the work valuable for researchers in the domain of computer vision and deep learning.", "wYgRIJ-oK6M": " Summary: The paper introduces a novel approach to binarize transformer models to high accuracy levels by utilizing a twoset binarization scheme a novel elastic binary activation function and a multidistillation process. The proposed method achieves stateoftheart results on the GLUE language understanding benchmark showcasing the practicality of fully binarized transformer models compared to fullprecision baselines.  Strengths: 1. Innovative Approach: The paper introduces novel techniques for binarizing transformer models leading to significant accuracy improvements over existing methods. 2. StateoftheArt Results: Achieving competitive results on the GLUE benchmark for fully binarized models approaching the performance of fullprecision BERT baselines. 3. Thorough Experiments: Extensive experiments and ablation studies are conducted demonstrating the effectiveness of each proposed technique. 4. Detailed Methodology: Detailed descriptions of the proposed techniques and their implementation are provided enabling reproducibility. 5. Robust Evaluation: The evaluation includes results from various quantization levels and datasets providing a comprehensive analysis.  Weaknesses: 1. Limited Generalization: The evaluation is primarily focused on language understanding tasks limiting the generalizability of the proposed approach to other domains. 2. Complexity: The optimization and training process for binarizing transformer models may still be considered complex which could limit practical adoption. 3. Lack of Comparison with Other Models: The paper lacks comparisons with other recent techniques for model quantization which could provide a broader perspective on the proposed approachs effectiveness. 4. Societal Impact: The paper does not discuss potential negative societal impacts or ethical considerations arising from the advancements made in binarization technology.  Questions  Suggestions for Authors: 1. Generalization: Have you explored the applicability of your binarization techniques to other transformerbased tasks beyond language understanding 2. Scalability: How does the proposed approach scale with the size of the transformer model Does it exhibit similar performance gains with larger models 3. Comparison: Have you compared your method with stateoftheart quantization techniques for neural networks especially in scenarios beyond transformer models 4. Practicality: How feasible is it to implement your techniques in realworld applications considering the computational overhead and training complexity 5. Ethical Considerations: Have you considered potential biases or negative social implications resulting from deploying fully binarized transformer models in practical applications  Limitations  Future Directions: The authors could further address the limitations and potential societal impacts of their work by highlighting any ethical considerations related to extreme model compression and providing suggestions for responsible deployment to mitigate any negative consequences. Moreover discussing the scalability and efficiency of the proposed techniques in diverse AI applications could enhance the practical relevance of the research.", "rY2wXCSruO": "PeerReview Summary: The paper introduces a novel modality interaction strategy for multimodal 3D object detection by maintaining two modalityspecific representations throughout the process. A DeepInteraction architecture with multimodal interactors and predictors is proposed leading to stateoftheart performance on the nuScenes dataset. Strengths:  The paper addresses a fundamental limitation of existing modality fusion strategies by emphasizing intermodality interaction.  The proposed DeepInteraction architecture is welldesigned and experimentally proven to outperform stateoftheart methods on the nuScenes dataset.  Extensive experiments and comparisons with alternative approaches demonstrate the efficacy and superiority of the proposed method.  Ablation studies on the encoder decoder and LiDAR backbones provide a comprehensive analysis of the proposed approach. Weaknesses:  The paper heavily focuses on technical details and lacks a holistic view of the impact and broader implications of the proposed method.  While the experiments show significant performance improvements a more indepth analysis of failure cases or limitations of the method could provide additional insights.  The paper could provide more details on the computational efficiency and scalability of the proposed approach especially for realtime applications. Questions  Suggestions: 1. How does the proposed modality interaction strategy handle occlusions or challenging scenarios where one modality provides incomplete information 2. Can the proposed method handle dynamic scenes or scenarios where the object of interest moves between modalities 3. How does the proposed method scale with larger datasets or more complex scene representations 4. What are the implications of the DeepInteraction architecture for other related tasks or domains beyond object detection 5. Are there any assumptions made in the design of the method that could limit its applicability to certain realworld scenarios or datasets Limitations: The paper could benefit from further discussing the potential negative societal impacts of their work especially in safetycritical applications like autonomous driving. Suggestions for improvement include incorporating fairness transparency and interpretability considerations into the evaluation and deployment of the proposed method.", "w5DacXWzQ-Q": "PeerReview 1) Summary and Contributions: The paper introduces a collaborative pruning approach for Vision Transformers (ViTs) by incorporating joint importance to effectively prune heterogeneous components. The proposed method aims to balance the reduction across all components by considering interactions leading to improved performance with reduced computational cost. The approach is evaluated on ImageNet and COCO datasets demonstrating superior results over existing pruning techniques. 2) Strengths and Weaknesses: Strengths:  Novel and original approach in incorporating joint importance and interactions for collaborative pruning in ViTs.  Theoretical analysis and optimization function development provide a strong foundation for the proposed method.  Extensive experiments on various models and tasks showcase the effectiveness of the approach outperforming existing stateoftheart methods.  Versatile framework applicability demonstrated on different model sizes and tasks showcasing potential for widespread applications. Weaknesses:  Limited discussion on the computational complexity of the proposed collaborative pruning algorithm which could impact practical applicability on resourcerestricted devices.  The paper lacks a detailed analysis of the impact of hyperparameters or sensitivity analysis which could provide insights into the robustness of the approach.  The method evaluation primarily focuses on synthetic datasets and further validation on realworld datasets or benchmark tasks could enhance the credibility of the results. 3) Questions and Suggestions for Authors:  Could you provide more insights into the computational complexity of the proposed collaborative pruning approach and discuss potential strategies to address this limitation for practical deployment  Have you conducted sensitivity analysis on key hyperparameters to evaluate the robustness of the method If not could you consider including this analysis in future work to enhance the understanding of parameter effects  Would it be possible to extend the evaluation to include realworld datasets or benchmark tasks to further validate the performance of the proposed method in diverse settings  How does the proposed method compare to existing techniques in terms of interpretability and explainability of the pruned models Could you elaborate on the interpretability aspects of the collaborative pruning approach Limitations: The authors could improve the paper by discussing the potential negative societal impact of their work more explicitly and suggesting ways to mitigate these impacts. For example addressing issues related to energy consumption or potential biases could be beneficial. Overall the paper presents a novel approach to collaborative pruning in ViTs and showcases promising results. Addressing the suggested points could further strengthen the paper and contribute to the advancement of the field.", "q6bZruC3dWJ": " Peer Review  Summary: The paper investigates the phenomenon in knowledge distillation where larger teacher models do not necessarily lead to betterperforming student models. The authors introduce the concept of undistillable classes that hinder the transfer of knowledge from teachers to students leading to decreased accuracy in certain classes. To address this issue they propose a novel \"Teach Less Learn More\" (TLLM) framework to identify and exclude undistillable classes during training. Experimental results demonstrate the effectiveness of their approach across multiple datasets and network architectures.  Strengths: 1. Original Contribution: The paper presents a novel perspective on the inefficacy of large teachers in distillation attributing it to the presence of undistillable classes. This provides valuable insights into the dynamics of knowledge transfer. 2. Extensive Empirical Validation: The authors conduct a thorough empirical analysis across various datasets and distillation methods to validate their findings. The experiments support their claims and show the effectiveness of the proposed TLLM framework. 3. Clear Methodology: The TLLM framework is wellexplained and the rationale behind identifying and excluding undistillable classes during training is logical. The methodology is straightforward and effective.  Weaknesses: 1. Literature Review: While the paper addresses related works on teacherstudent relationships in knowledge distillation a more comprehensive comparison with existing methods and discussions on how the proposed approach differs could enhance the paper. 2. Theoretical Justification: While the empirical results are convincing some theoretical analysis or further insights into why undistillable classes occur and how they impact knowledge transfer could strengthen the paper.  Questions and Suggestions: 1. Clarification on Undistillable Classes: Could the authors provide more concrete examples or cases to illustrate how undistillable classes impact the knowledge distillation process in practical scenarios 2. Comparative Analysis: It would be beneficial to compare the proposed TLLM framework with existing stateoftheart distillation techniques to showcase its superiority more explicitly. 3. Future Directions: What are the potential future directions for this research Are there any other applications or extensions of the TLLM framework that the authors are considering  Limitations: The authors have adequately highlighted the limitations and potential societal impact of their work with constructive suggestions for improvement. Overall the paper makes a significant contribution to understanding the limitations of large teachers in knowledge distillation and proposes a practical framework to address these challenges effectively. With minor enhancements in the literature review and theoretical justification the paper could further strengthen its impact in the field.", "qTCiw1frE_l": "Summary: The paper delves into the phenomenon of policy churn in valuebased reinforcement learning highlighting the rapid and largescale changes in the greedy policy during training. It empirically characterizes and explores this phenomenon linking it to exploration and offering insights into its potential impact on agent behavior and performance. Strengths:  The paper offers a novel perspective on policy churn in deep RL shedding light on its potential as a form of implicit exploration.  Empirical analysis is comprehensive showcasing the widespread occurrence of policy churn across different environments and algorithms.  The study provides valuable insights into the relationship between policy churn and exploration offering implications for training stability and performance. Weaknesses:  While the empirical findings are strong deeper theoretical analysis or formal modeling could enhance the interpretability of the results.  The paper could benefit from more extensive comparisons with existing exploration methods to highlight the unique benefits and tradeoffs of policy churn. Questions and Suggestions: 1. Can you further elaborate on the potential practical implications of policy churn in terms of training stability and convergence behavior 2. How does the proposed hypothesis on policy nullspaces relate to existing theories on policy improvement and exploration strategies in reinforcement learning 3. Have you considered exploring the impact of policy churn on transfer learning or generalization to new environments Limitations: The authors have not adequately addressed the potential negative societal impact of their work. It would be beneficial for the authors to discuss how the integration of policy churn could impact algorithmic fairness accountability or transparency in realworld applications. They should provide recommendations for mitigating any adverse effects.", "rTTh1RIn6E": " Peer Review of \"Conditionali: Conditional Independence Model for OutofDistribution Detection\"  Summary: The paper proposes a novel approach Conditionali for outofdistribution (OOD) detection in machine learning using a Conditional Independence Model based on independent component analysis techniques. The method introduces a new generative model that encodes class conditions into the probabilistic model to effectively constrain mutual information between inliers and outliers. By leveraging the HilbertSchmidt Independence Criterion Conditionali achieves superior OOD detection performance compared to existing stateoftheart methods. The paper provides theoretical justifications and empirical results across computer vision and NLP tasks.  Strengths: 1. Innovative Approach: Conditionali offers a unique perspective on OOD detection through conditional independence modeling providing theoretical soundness and practical advantages. 2. Theoretical Justifications: The paper substantiates the proposed model with clear theoretical underpinnings enhancing its credibility and transparency. 3. Empirical Evaluation: Extensive empirical results demonstrate the significant performance boost of Conditionali over existing methods on diverse benchmarks in computer vision and NLP tasks. 4. Memory Bank Architecture: The introduction of a memory bank architecture for endtoend training enriches the technical contributions and practicality of the method.  Weaknesses: 1. Limited Analysis on RealWorld Application: While the proposed method shows promising results in controlled experiments further investigations on realworld scenarios and practical applications are warranted to gauge its robustness in varied settings. 2. Complexity Concerns: The paper touches on intricate theoretical aspects and methodologies potentially making it challenging for readers with lesser expertise in the field. 3. Reproducibility: While the code availability is mentioned additional details on reproducibility practices and potential computational requirements would enhance the practical accessibility of the work.  Questions and Suggestions: 1. RealWorld Application Testing: Have you considered applying Conditionali to practical scenarios or domains beyond the benchmarks studied in the paper to assess its effectiveness in more diverse and complex settings 2. Scalability Concerns: How does Conditionali perform in terms of computational efficiency and scalability particularly when applied to larger datasets or realtime applications 3. Comparison to Generative Models: It would be beneficial to discuss the advantages of Conditionali over traditional generative models for OOD detection and explore scenarios where one approach might be more suitable than the other. 4. Open Questions: What are the key challenges or limitations that need to be addressed for wider adoption and deployment of Conditionali in realworld machine learning systems  Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work in terms of limited generalization on fake OOD data. To mitigate this issue they should explore strategies to enhance generalization capabilities on unseen OOD scenarios. Overall the paper presents a valuable contribution to the field of OOD detection with a novel and welljustified approach paving the way for further exploration and refinement in this critical area of machine learning safety and reliability. [Comments: This review provides a balanced assessment of the strengths and weaknesses of the paper along with thoughtful questions and suggestions for the authors. It highlights the significance of the proposed method while emphasizing areas for improvement and future research directions.]", "vphSm8QmLFm": " PeerReview of \"Global Batch Gradients Aggregation for Efficient Training Mode Switching in Recommendation Models\" Summary: The paper introduces Global Batch gradients Aggregation (GBA) a novel approach for tuningfree switching between synchronous and asynchronous training modes for recommendation models. GBA maintains the same global batch size during switching addressing issues related to gradient distribution and staleness. Experimental results demonstrate GBAs effectiveness in improving AUC metric and training speed compared to existing methods. Strengths: 1. Innovative Solution: GBA offers a unique approach to address the challenges of switching training modes without the need for hyperparameter tuning. 2. Convergence Analysis: The paper provides a detailed convergence analysis to support the effectiveness and stability of GBA. 3. Experimental Validation: Extensive experiments on industrialscale recommendation tasks demonstrate the superiority of GBA in terms of AUC improvement and training efficiency. 4. Realworld Relevance: The application of GBA in a shared training cluster highlights its practical impact on realworld scenarios. Weaknesses: 1. Theoretical Clarity: Some sections particularly the convergence analysis may benefit from better clarity and explanations for nonexperts to fully understand the technical details. 2. Comparison Metrics: The paper could strengthen its analysis by including a broader set of evaluation metrics beyond AUC and training efficiency such as model interpretability or robustness to noisy data. 3. Implementation Details: More insights into the implementation aspects of GBA such as computational complexity or memory requirements could enhance the understanding of its practical feasibility. Questions and Suggestions for Authors: 1. Scalability: How does GBA perform in scenarios with significantly larger clusters or more complex recommendation models Is there a threshold where the efficiency gains start diminishing 2. LongTerm Impact: Have you considered any potential longterm implications of GBA such as its adaptability to emerging recommendation model architectures or evolving hardware technologies 3. Generalizability: Can GBA be generalized to other domains beyond recommendation systems and if so what adaptation might be required 4. User Experience: How userfriendly is the implementation of GBA for practitioners unfamiliar with tuningfree training mode switching Are there any userfacing challenges or considerations Limitations: The paper adequately addresses the limitations and negative societal impacts of the work. To enhance clarity authors might consider adding a section highlighting potential ethical considerations or societal implications of implementing GBA in various settings. In conclusion the paper presents a valuable contribution to the field of distributed training for recommendation models with GBA. Further refinement in theoretical explanations broader experimental metrics and considerations of scalability and generalizability would strengthen the overall impact of the work. Please feel free to elaborate on the suggestions and address any of the questions raised in your revisions. Great work overall", "zdmYnIRXvKS": " Summary: The paper proposes a spiking neural network model based on efficient coding theory aiming to develop biologically plausible networks that mimic the computational functions of biological circuits better. The model includes a variety of features such as diverse neuron properties structured connectivity and timedependent minimization of loss functions.  Strengths: 1. Biological Plausibility: The paper addresses the need for biologically realistic modeling by incorporating features like excitatory and inhibitory units heterogeneous firing thresholds and realistic synaptic currents. 2. Innovative Approach: The introduction of two separate loss functions for excitatory and inhibitory neurons provides a novel perspective that accounts for the functional diversity of neural populations. 3. Comprehensive Analysis: The paper presents detailed derivations and analytical treatment of the network model along with simulations to analyze the effects of various connectivity structures.  Weaknesses: 1. Complexity: The detailed mathematical derivations might make the paper less accessible to readers without a strong background in neuroscience or computational modeling. 2. Evaluation Metrics: The paper lacks concrete evaluation metrics or comparisons to existing models which could strengthen the validation of the proposed approach. 3. Clarity: The lack of clear distinction between the contributions of this work compared to previous studies could be improved in order to highlight the advancements made.  Questions and Suggestions: 1. Could you provide more clarity on how the proposed model compares to existing models in terms of performance metrics or computational efficiency 2. How do the parameters of the network model affect its computational efficiency or response dynamics Could you provide insights into the sensitivity of the model to parameter variations 3. Have you considered incorporating experimental data or validation from real neural recordings to validate the models biological relevance and predictive power  Limitations: The authors could enhance the paper by providing explicit discussions on how the proposed models limitations could impact its application in realworld biological contexts. Additionally addressing the potential negative societal impacts of the research findings could improve the ethical considerations of the work.", "rlN6fO3OrP": " Summary: The paper investigates the vulnerability of continuous prompt learning algorithms to backdoor attacks. It introduces BadPrompt a novel backdoor attack method designed to target continuous prompts. BadPrompt consists of trigger candidate generation and adaptive trigger optimization modules. The research demonstrates that BadPrompt can effectively attack continuous prompts while maintaining high performance on clean test sets outperforming baseline models in various experiments.  Strengths: 1. Original Contribution: The paper makes a significant contribution by being the first to explore backdoor attacks on the continuous prompt learning paradigm. 2. Innovative Approach: BadPrompt introduces a novel algorithm that addresses the challenge of attacking continuous prompts efficiently. 3. Thorough Experiments: The paper conducts extensive experiments on multiple datasets and prompt models to validate the effectiveness of BadPrompt. 4. Clear Exposition: The paper is wellorganized providing detailed explanations of the methodology and experimental results.  Weaknesses: 1. Limited Comparison: While comparing BadPrompt with other baseline methods the paper could benefit from a more extensive comparison with diverse stateoftheart backdoor attack techniques. 2. Incomplete Discussion: The societal impact and limitations sections could be expanded to discuss potential ethical considerations in more depth.  Questions and Suggestions: 1. Question: How does BadPrompt compare to other backdoor attack methods in terms of computational efficiency and scalability 2. Question: Have the authors considered the longterm implications of backdoor attacks in continuous prompt learning for realworld applications and user trust 3. Suggestion: Include a detailed analysis of the interpretability and explainability of the triggers generated by BadPrompt to enhance the papers practical relevance.  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work by discussing the implications of potential malicious use of BadPrompt and proposing future research directions for defending against backdoor attacks.", "mzze3bubjk": " Summary: The paper investigates the computational hardness of highdimensional statistical inference problems by making a rigorous connection between lowdegree and freeenergy based approaches. It defines a freeenergy based criterion the Franz\u2013Parisi criterion and connects it to lowdegree hardness for Gaussian additive models and sparse planted models. The paper establishes that lowdegree hardness implies failure of local Markov Chain Monte Carlo (MCMC) algorithms and provides new lowdegree lower bounds for sparse linear regression. The contributions include conceptual insights into hardness connections and technical tools for proving lowdegree lower bounds.  Strengths: 1. Conceptual Framework: The paper provides a rigorous connection between different notions of computational hardness enhancing the understanding of highdimensional statistical inference. 2. Technical Contributions: By defining the Franz\u2013Parisi criterion and establishing its connection to lowdegree hardness the paper offers concrete tools for studying computational lower bounds in statistical settings. 3. Novel Findings: The results on Gaussian additive models sparse linear regression and implications for MCMC algorithms contribute to the fields advancement.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers not wellversed in the specific field of study. 2. Limited Applications: The focus on highdimensional statistical inference problems may limit the broader applicability and accessibility of the findings. 3. Lack of Practical Demonstration: While the theoretical connections and results are elaborate a practical application or demonstration of the proposed methods could enhance the papers impact.  Questions and Suggestions for Authors: 1. Could the authors provide more concrete examples or case studies to help readers understand how the proposed connections and criteria can be applied in practice 2. How do the results and insights from this study contribute to realworld applications of highdimensional statistical inference problems 3. Have the authors considered the computational complexity of implementing the methods proposed in realworld scenarios Addressing practicality concerns could enhance the papers impact.  Limitations: The authors have not fully addressed the potential negative societal impact of their work. They could consider discussing implications for algorithmic fairness privacy or societal biases in highdimensional statistical inference applications. Overall the paper makes significant contributions by connecting different computational hardness notions and providing new tools for proving lowdegree lower bounds. Addressing practical implications and societal impact considerations could further strengthen the paper.", "q41xK9Bunq1": " Paper Summary: The paper introduces Neural Attentive Circuits (NACs) a generalpurpose neural architecture that combines sparsityinducing modular inductive biases while maintaining the versatility of generalpurpose models. NACs leverage a system of sparsely and attentively interacting neural modules that communicate via connectivity graphs and learn module parameterization and connectivity endtoend without domainspecific assumptions. The paper demonstrates the effectiveness of NACs qualitatively and quantitatively across diverse data modalities showcasing improvements in lowshot adaptation outofdistribution robustness computational efficiency and competitive results in various tasks.  Strengths: 1. Innovative Approach: Introducing a novel architecture that combines generalpurpose and modular inductive biases is a significant contribution. 2. Experimental Rigor: The paper provides thorough and diverse experimental evaluations across various tasks and datasets to validate the effectiveness of NACs. 3. Performance Improvement: NACs show improvements in lowshot adaptation outofdistribution robustness and computational efficiency compared to baselines.  Weaknesses: 1. Domainspecific Language: The paper uses technical terms and concepts that may make it less accessible to readers outside the field. 2. Model Complexity: The complexity of NACs may hinder its scalability and implementation in realworld applications. 3. Absence of Baseline Diversity: The comparison is primarily against Perceiver IOs and the absence of comparison with more diverse baseline models may limit a comprehensive evaluation.  Questions  Suggestions: 1. Scalability: How does the performance of NACs scale with an increasing number of modules Have you tested the models limits in terms of scalability and resource efficiency 2. Generalization: How does NACs perform on tasks requiring complex reasoning beyond the provided benchmarks Could the model generalize effectively in more challenging scenarios 3. Interpretability: Can the authors provide insights into how module configurations are learned by NACs and share detailed analysis of connectivity graph structures across different data modalities  Limitations: The paper could enhance the discussion on the societal impact of potentially complex and resourceintensive models like NACs. Additionally addressing the performance tradeoffs between model complexity and efficiency could provide useful insights for practical deployment. Overall the paper presents a compelling approach with promising results but further analysis on scalability interpretability and societal implications could strengthen its contributions and practical relevance.", "xOK40an4ag1": " Summary The paper investigates the relationship between network dynamics and connectivity in Recurrent Neural Networks (RNNs) trained without constraints on several neuroscience tasks. It identifies lowdimensional functional subspaces in the weight matrix called operative dimensions that are crucial for the networks performance despite a highdimensional connectivity. The study enhances interpretability by relating network structures to dynamics.  Strengths 1. Novel Contribution: The paper introduces the concept of operative dimensions to elucidate the functional relevance of connectivity in unconstrained RNNs. 2. Theoretical Relevance: Offers insights into how connectivity shapes network dynamics enhancing interpretability for neuroscience and machine learning applications. 3. Methodological Rigor: The analyses are thorough with welldefined metrics and clear procedures for identifying operative dimensions. 4. Applicability: The findings pave the way for better understanding neural computations and for improving network performance by highlighting key connectivity components.  Weaknesses 1. Sampling Limitations: The choice of sampling locations for identifying operative dimensions may impact the generalizability of results requiring careful consideration. 2. Task Dependency: The study focuses on specific tasks potentially limiting the generalizability of the findings to broader applications. 3. Evaluation Metrics: The paper lacks a comparative analysis with existing methodologies which could strengthen the validation of the proposed approach.  Questions  Suggestions 1. Sampling Strategy: Could the authors elaborate on the rationale behind selecting sampling locations and discuss potential implications of using alternative strategies 2. Generalizability: How transferable are the findings regarding operative dimensions to different RNN architectures and more complex tasks 3. Method Comparison: Have you considered comparing the effectiveness of operative dimensions with other interpretability methods for RNNs to validate the proposed approach further  Limitations The authors should address potential societal impacts of increased network interpretability especially when applied to sensitive domains. Providing guidance on ethical considerations and data privacy protection is crucial. Overall the paper provides valuable insights into the relationship between network connectivity and dynamics offering a new perspective on interpreting neural computations in RNNs. Strengthening the methodologys robustness generalizability and ethical considerations will enhance the impact of this research.", "wKhUPzqVap6": " Summary: The paper investigates the quality of feature representations learned by deep classifiers in the presence of spurious correlations. It evaluates feature learning methods using Deep Feature Reweighting (DFR) and compares Empirical Risk Minimization (ERM) with group robustness training. The study shows that the quality of feature representations is crucially affected by model design decisions and pretraining strategies. It also demonstrates superior performance on benchmark tasks using ERM training followed by DFR.  Strengths: 1. Comprehensive Study: The paper provides an indepth analysis of feature learning in the presence of spurious correlations across multiple vision and NLP problems. 2. Significant Contributions: By showing that ERM features can be competitive with specialized group robustness methods the paper challenges conventional beliefs about learning under spurious correlations. 3. Practical Relevance: The findings have high practical significance indicating that training the last layer of the model is simpler and computationally more efficient than training the full model to avoid spurious correlations.  Weaknesses: 1. Clarity: Some sections such as the experiment description could benefit from more clarity and conciseness to make it easier for readers to understand the methodology. 2. Presentation of Results: Although the results are insightful more visual aids like diagrams or tables could enhance the presentation and help in better interpretation of the findings.  Questions and Suggestions: 1. Feature Extraction Techniques: Can the authors discuss any alternative feature extraction techniques that could potentially further improve the quality of learned feature representations 2. Impact of Different Architectures: How do different architectures impact the feasibility and effectiveness of feature learning in the presence of spurious correlations A comparative analysis could provide valuable insights. 3. Generalization to Other Domains: To what extent do the observations made in the study generalize to domains beyond the vision and NLP problems considered in the paper It would be interesting to explore the applicability of the findings across diverse domains.  Limitations: The paper does not explicitly address the potential negative societal impact of its work. Including a section that discusses the ethical implications and potential negative consequences of the research findings would be valuable for completeness and responsible scholarship.", "vK53GLZJes8": "PeerReview Summary: The paper challenges the convention that regularization sufficiently addresses the instability and unbounded error issues of Temporal Difference (TD) learning in reinforcement learning. The authors introduce new counterexamples showing the limitations of regularization in mitigating divergence in offpolicy TD methods particularly with linear function approximation and neural networks. They argue that the role of regularization needs reevaluation in TD and emphasize the need for caution in applying regularization in RL methods. Strengths: 1. Innovative Contribution: The paper presents novel counterexamples that challenge the established belief in the effectiveness of regularization in addressing instability in TD learning. 2. Thorough Analysis: The work provides detailed theoretical analysis and concrete examples to support their arguments. 3. Clear Structure: The paper is wellstructured with sections that logically progress from problem introduction to the presentation of counterexamples and implications. 4. Depth of Discussion: The discussion on regularizations limitations and potential negative impact on various methods including EmphaticTD algorithms adds significant value to the existing literature. 5. Relevance: The study focuses on a critical aspect of reinforcement learning regularization which is foundational in practical implementations. Weaknesses: 1. Lack of Empirical Validation: While the theoretical derivations and counterexamples are compelling empirical validation on realworld datasets or tasks could enhance the credibility of the findings. 2. Scope Limited to Linear and Neural Network Approximations: The discussion focuses primarily on linear function approximation and neural networks potentially missing insights into other function approximators commonly used in RL. 3. Inadequate Exploration of Adaptive Regularization: While suggesting the potential for adaptive regularization the paper could benefit from more detailed suggestions or exploration of how such schemes could be practically implemented. Questions and Suggestions to the Authors: 1. Have you considered conducting empirical experiments to validate the theoretical findings on a range of RL tasks or datasets 2. How do the limitations of regularization in TD learning extend to other function approximators beyond linear and neural networks Could different approximators exhibit different regularization behaviors 3. Considering the risks associated with regularization what are your thoughts on potential adaptive regularization strategies that could be explored in future research Limitations: The authors could further explore and discuss potential adaptive regularization schemes to mitigate some of the limitations highlighted in the paper. Additionally evaluating the practical implications of their findings on realworld RL applications would be beneficial for a more comprehensive assessment. Constructive Suggestions for Improvement: 1. Conduct empirical studies to validate theoretical findings. 2. Broaden the investigation to include different function approximators. 3. Explore and propose adaptive regularization strategies for practical implementation in RL algorithms.", "y-E1htoQl-n": " Summary: The paper introduces a novel robust training framework for deep reinforcement learning (RL) agents called WocaRRL to enhance the robustness of RL policies against adversarial attacks under bounded `p constraints. It provides mechanisms to estimate worstcase policy rewards optimize policies under worstcase scenarios and regularize policies to handle state perturbations efficiently. The proposed framework outperforms stateoftheart methods in terms of robustness and sample efficiency across various environments.  Strengths: 1. Innovative Approach: The paper introduces a novel efficient robust training framework for RL agents addressing the limitations of existing methods. 2. Detailed Explanations: The paper provides detailed explanations of the proposed mechanisms making the approach clear and understandable. 3. Experimental Evaluation: The paper includes comprehensive experiments on various environments demonstrating the superior performance and efficiency of WocaRRL compared to stateoftheart methods. 4. Ablation Studies: Conducting ablation studies to show the effectiveness of individual components strengthens the reliability of the proposed framework.  Weaknesses: 1. Complexity: While the proposed framework is efficient and effective the complexity of the approach and the technical terminology used may hinder understanding for readers not wellversed in the field.  Questions and Suggestions for the Authors: 1. Clarification on Evaluation Metrics: Could the authors provide more insights into the specific metrics used to evaluate the robustness of RL policies under adversarial attacks 2. Generalizability: Have the authors considered how well the proposed framework generalizes to different types of environments beyond the ones tested in the experiments 3. Scalability: Is there any discussion on the scalability of the proposed framework to larger and more complex environments or datasets 4. Comparison with StateoftheArt: It would be beneficial to have a more indepth comparison with other robust training methods beyond the ones mentioned in the paper to provide a broader perspective.  Limitations: The paper does not explicitly address the potential negative societal impacts of enhancing the robustness of RL agents against adversarial attacks. Consider including a discussion on ethical considerations and potential implications of deploying highly robust RL agents in realworld applications.", "vF3WefcoePW": "PeerReview Summary: The paper introduces the concept of logic gate networks exploring their effectiveness in machine learning tasks by learning combinations of logic gates such as aANDo and aXORo. The authors propose differentiable logic gate networks to address the nondifferentiability challenge of logic gate networks enabling efficient training with gradient descent. They demonstrate the fast inference speeds and effectiveness of logic gate networks compared to traditional neural networks on tasks like MNIST classification. Strengths: 1. Innovative Approach: The paper introduces a novel concept of logic gate networks and provides a comprehensive explanation of their architecture and training methodology. 1. Efficiency: The proposed logic gate networks achieve significantly faster inference speeds with fewer computations compared to traditional fully connected neural networks showcasing the potential for high efficiency. 1. Experimental Validation: The authors conduct an array of experiments on various datasets including MNIST and CIFAR10 demonstrating the performance superiority and efficiency of their method. 1. Detailed Explanations: The paper provides detailed discussions on differentiable logic gate networks training strategies activation methods and memory considerations enhancing the understanding of the proposed approach. Weaknesses: 1. Lack of Comparison: While the paper discusses the efficacy of logic gate networks against traditional neural networks a more detailed comparison with other stateoftheart methods such as advanced convolutional architectures or attention mechanisms could provide deeper insights into the performance of logic gate networks. 1. Complexity: The paper involves complex concepts such as differentiable logic gates and realvalued logics which may be challenging for a general audience to understand without a strong background in neural networks and logic. 1. Scalability Concerns: The scalability of logic gate networks to larger datasets and more complex tasks is not extensively discussed raising questions about the generalizability of the proposed approach. Questions and Suggestions: 1. Experimental Scope: Have the authors considered experiments on datasets with varying complexities to assess the scalability and generalizability of logic gate networks 1. Comparative Analysis: How does the proposed approach compare to advanced convolutional architectures or transformer models in terms of accuracy efficiency and scalability 1. Memory Efficiency: Given the emphasis on efficiency have the authors explored further optimization strategies to reduce the memory footprint of logic gate networks without compromising performance Limitations: The paper lacks an indepth analysis of potential negative societal impacts or ethical considerations arising from the deployment of logic gate networks. It is recommended that the authors include a section addressing these aspects emphasizing transparency and responsible deployment in future work. Overall the paper presents an innovative concept with promising results in terms of efficiency and fast inference speeds. However further exploration of scalability comparative analysis with advanced architectures and considerations of societal impacts would enhance the comprehensiveness of the work.", "wUctlvhsNWg": " Summary: The paper introduces a personalized online federated multikernel learning algorithm called POFMKL. It leverages random feature (RF) approximation to enable clients to choose a subset of kernels for updating parameters effectively reducing computational complexity and communication costs. The algorithm ensures sublinear regret for each client and the server. Experimental results demonstrate the effectiveness of POFMKL compared to existing online federated kernel learning algorithms.  Strengths: 1. Innovative Algorithm: The proposed POFMKL algorithm is novel personalized and addresses key challenges in online federated kernel learning regarding communication efficiency and heterogeneous data distribution. 2. Theoretical Guarantees: Theoretical analyses provide clear proofs of sublinear regret for both clients and the server showing the algorithms efficacy in dealing with data heterogeneity. 3. Experimental Validation: Comprehensive experiments on real datasets demonstrate the advantages of POFMKL over other algorithms highlighting its accuracy and efficiency.  Weaknesses: 1. Complexity: The paper contains dense technical content that may be challenging for readers unfamiliar with the domain potentially affecting its accessibility and understandability. 2. Limited Comparison: The comparison with existing algorithms could be more detailed addressing a broader set of metrics beyond MSE and runtime to provide a comprehensive evaluation.  Questions and Suggestions for Authors: 1. Clarity for NonExperts: How might you enhance the explanation of complex mathematical concepts and algorithms to make them more accessible to readers with varying levels of expertise 2. Scalability Concerns: Have you considered the scalability of POFMKL for extremely large datasets or a significant number of clients How does the algorithm perform under such scenarios 3. Robustness Analysis: Are there robustness tests conducted to assess the algorithms performance under noisy or incomplete data scenarios  Limitations: The authors can strengthen their paper by addressing the following: 1. Addressing Negative Impacts: Consider discussing and mitigating any potential negative societal impacts of the proposed algorithm such as potential biases or privacy concerns.  Final Note: Overall the paper presents a promising advancement in online federated kernel learning with the POFMKL algorithm. By enhancing clarity expanding the comparison analysis and addressing scalability concerns the authors can further strengthen the impact and accessibility of their work.", "monPF76G5Uv": " Summary: The paper introduces the Globally Normalized Autoregressive Transducer (GNAT) model to address the label bias problem in streaming speech recognition. The model demonstrates a substantial reduction in the word error rate gap between streaming and nonstreaming speech recognition models (over 50 on the Librispeech dataset). The paper provides theoretical and empirical evidence supporting the efficacy of the GNAT model which is developed within a modular framework that encompasses various neural speech recognition models.  Strengths: 1. Innovative Solution: The GNAT model presents an innovative solution to the label bias problem in streaming speech recognition achieving notable improvements in performance. 2. Empirical Results: The theoretical and empirical results provided in the paper support the effectiveness of the GNAT model in reducing the Word Error Rate (WER) gap. 3. Modular Framework: The modular framework introduced enables controlled comparison of different modeling choices and facilitates the creation of new ASR models. 4. Open Source Implementation: The availability of a JAX implementation of the models on GitHub enhances reproducibility and transparency.  Weaknesses: 1. Clarity in Explanations: Some sections of the paper particularly those discussing the various models and components may be challenging to follow for readers not deeply versed in the field of automatic speech recognition. 2. Scalability Concerns: The paper briefly touches on scalability challenges with larger numbers of label contexts and the exponential growth in computational demands. Further discussion or solutions on scalability would strengthen the paper. 3. Decoding Challenges: The limitations of the standard sumpath algorithm in decoding globally normalized models are highlighted calling for additional investigation and enhancements in decoding strategies.  Questions and Suggestions for Authors: 1. Scalability Considerations: How do the authors plan to address the scalability challenges associated with a larger number of label contexts particularly in terms of computational efficiency and memory requirements 2. Decoding Strategies: Can the authors provide more insights into potential approaches for enhancing the decoding of globally normalized models especially with respect to adapting existing heuristics and pruning techniques for optimal performance 3. Model Comparison: Could the paper delve deeper into comparative analyses of the GNAT model against existing globally normalized models and how it stacks up against them in terms of performance and efficiency  Limitations: The paper adequately addresses the limitations of the GNAT model particularly in terms of scalability and decoding challenges. To further improve the authors might explore optimized pruning techniques for globally normalized models to enhance decoding efficiency.", "uOii2cEN2w_": " Summary: The paper introduces a new method called Bayesian Optimisation via DensityRatio Estimation (BORE) and extends it to the batch setting. BORE reformulates the acquisition function using densityratio estimation allowing for scalability and improved performance. The paper provides theoretical analysis introduces BORE which incorporates uncertainty estimates and presents experiments comparing BORE and BORE against other Bayesian Optimisation (BO) baselines across various cases.  Strengths: 1. Theoretical Rigor: The paper provides a comprehensive theoretical analysis of BORE demonstrating regret bounds and efficacy. 2. Novel Contribution: The extension of BORE to a batch setting and the introduction of BORE with enhanced uncertainty estimates are significant contributions. 3. Experimental Validation: Through experiments the paper validates the theoretical results and demonstrates the performance of BORE and BORE against traditional BO baselines on both synthetic and realdata benchmarks.  Weaknesses: 1. Clarity of Presentation: The paper is dense with technical content which may make it challenging for readers unfamiliar with the subject to grasp the concepts easily. Some parts could be explained in a more accessible manner. 2. Empirical Results Visualization: While the experiments are thorough the visualization of results could be more intuitive to enhance the understanding for a broader audience. 3. Implementation Details: More insights into the implementation details computational complexity and practical implications could provide a clearer perspective.  Questions and Suggestions for Authors: 1. Clarification on Experimental Setup:  Can the authors elaborate on the computational complexity of the proposed algorithms compared to traditional GPbased methods  How does the batch setting affect the convergence rates and performance compared to the sequential setting 2. Generalization:  Have the authors considered analysis or experiments on the generalization capabilities of BORE and BORE to different types of optimization problems or functions  Are there plans to investigate the robustness of the proposed algorithms to noise levels in observations or model inaccuracies  Limitations: The paper might benefit from addressing the potential negative societal impact of the algorithms proposed discussing issues related to biased model training ethical considerations or societal implications that might arise from the deployment of these optimization methods.  Constructive Suggestions for Improvement:  Enhance the clarity of the presentation for readers from diverse backgrounds.  Provide a more userfriendly visualization of the experimental results.  Include a discussion on the computational complexity and efficiency aspects.  Consider the broader implications and generalizability of the proposed methods for various applications.", "mTXQIpXPDbh": " Peer Review:  Summary: The paper presents a novel memoryefficient transfer learning framework Back Razor that addresses the challenge of large memory footprints on edge devices by pruning the activation for backpropagation. The proposed method is applied to various backbone models including CNNs and ViTs and is shown to achieve up to 97 sparsity saving 9.2x memory usage without compromising accuracy. The paper includes theoretical analyses and extensive experiments demonstrating the effectiveness of Back Razor on different datasets and tasks.  Strengths: 1. Innovative Approach: The paper introduces an asymmetric pruning scheme for efficient transfer learning addressing a crucial challenge of memory footprints on edge devices. 2. Theoretical Analyses: The inclusion of theoretical analysis adds depth to the proposed method providing a solid foundation for its effectiveness. 3. Extensive Experiments: The experiments cover a wide range of tasks and datasets demonstrating the robustness and versatility of the Back Razor framework. 4. Comparative Analysis: The comparison with existing methods and stateoftheart techniques establishes the superiority of Back Razor in terms of memory efficiency without compromising performance.  Weaknesses: 1. Clarity in Theoretical Section: While theoretical analyses are provided the paper could benefit from simplifying complex concepts for better understanding by a wider audience. 2. Limited Scope in Theoretical Analyses: The theoretical analysis primarily focuses on CNNs while ViTs are mentioned only briefly. Providing more indepth theoretical insights for ViTs could enhance the completeness of the paper. 3. Evaluation Metrics: The paper could provide more context on the specific criteria used for accuracy comparisons and how they impact the practical implications of the proposed method.  Questions and Suggestions for Authors: 1. It would be beneficial to elaborate on the generalizability of the proposed method across different backbone architectures beyond CNNs and ViTs. How adaptable is Back Razor to other types of models like GNNs or RNNs 2. Can the authors shed light on the computational overhead incurred during the sparse activation pruning process How does this overhead compare with traditional training methods in terms of computational resources 3. Could the authors discuss potential realworld applications of Back Razor beyond the scope of edge devices such as in cloudbased environments or distributed systems  Limitations and Suggestions for Improvement: The authors have discussed the limitations of using activation pruning only for backpropagation but could further explore the potential negative societal impact of largescale model pruning. To enhance the ethical implications of the work a discussion on the tradeoffs between model efficiency and societal implications such as bias amplification or privacy concerns could be included. Overall the paper presents a promising framework with significant implications for memoryefficient transfer learning. Addressing the provided suggestions can further strengthen the papers impact and practical relevance in the field of deep learning.", "wGF5mreJVN": "Summary: The paper introduces a navigation approach for intelligent webbased agents to efficiently move towards a target on a graph version of Wikipedia. The method utilizes behavioral cloning of randomly sampled trajectories to learn an effective link selection policy demonstrating success rates of 96 and 92 for navigating nodes 5 and 20 steps apart respectively. The learned embeddings and policy are applied to fact verification and question answering tasks achieving competitive results. Strengths: 1. Innovative Approach: The paper presents a novel method for web navigation using behavioral cloning showing significant improvement in navigation success rates. 2. Scalability: The approach is scalable and applicable to general web settings not limited to Wikipedia demonstrating adaptability. 3. Competitive Results: By combining basic TFIDF search and ranking methods with the proposed approach the paper achieves competitive outcomes compared to stateoftheart methods in downstream tasks. 4. Thorough Experiments: The paper provides detailed analysis of trajectory distribution choices model architectures and evaluation on various navigation tasks showcasing comprehensive experimental results. Weaknesses: 1. Target Encoder Dependency: The reliance on a good target encoder could be a limitation when moving beyond Wikipedia to broader settings where groundtruth target nodes may not be available. 2. External Reranker Requirement: The necessity of a reranker for final decisionmaking on the retrieved sentences poses a limitation as ideally the agent should determine the correct node independently. Questions and Suggestions: 1. Training Efficiency: How does the training efficiency of the proposed method compare with reinforcement learning approaches especially in terms of sample efficiency and computational resources 2. Generalizability: How well does the navigation policy generalize to unseen graphs or web environments with different characteristics and what measures are taken to ensure robustness 3. Impact of Trajectory Distribution: Could the authors elaborate on why the Random Forward Trajectories approach was chosen over other trajectory distribution alternatives and how it affects navigation performance Limitations: The authors should address the potential negative societal impact of their work such as overreliance on automated agents for information retrieval and provide suggestions for mitigating any associated risks.", "z9poo2GhOh6": " Abstract: The paper analyzes large batch stochastic gradient descent with momentum (SGDM) on the least squares problem focusing on scenarios with large sample sizes and dimensions. Convergence dynamics of SGDM to a Volterra equation are studied introducing a stability measurement the implicit conditioning ratio (ICR). The performance of SGDM is compared in both large and small batch regimes providing explicit choices for learning rate and momentum parameters to achieve optimal performance.  Strengths: 1. Theoretical Contribution: The paper provides a thorough analysis of the convergence dynamics of SGDM shedding light on its behavior with large batches in highdimensional settings. 2. Algorithmic Insights: Explicit parameter choices for achieving optimal performance are given enhancing practical applicability. 3. Comparison: Clear comparisons between large and small batch scenarios are drawn elucidating the impact of batch size on convergence rates. 4. Convergence Guarantees: The paper offers theoretical guarantees on the convergence behavior of SGDM based on batch fraction and learning parameters. 5. Rigorous Analysis: The inclusion of mathematical propositions and proofs strengthens the credibility of the findings.  Weaknesses: 1. Clarity in Presentation: The complexity of the mathematical analysis might be challenging for readers not deeply rooted in the subject matter. 2. Empirical Validation: While numerical experiments are mentioned more detailed results and comparisons with existing methods could enhance the practical relevance of the findings. 3. RealWorld Applications: A discussion on the implications of the findings in realworld applications would enhance the papers relevance to practitioners.  Questions and Suggestions for Authors: 1. Can you further elaborate on the practical significance of the identified stability measurement the implicit conditioning ratio (ICR) 2. How do the findings of this study extend to nonorthogonal data matrices or different loss functions beyond the least squares problem 3. Could you provide insights into the scalability of the proposed parameter choices to larger datasets or more complex models 4. In the future consider expanding the empirical validation section to include benchmarks against stateoftheart optimization algorithms in practical machine learning tasks.  Limitations: The paper could benefit from clearer explanations and additional empirical validation to bridge the gap between theoretical analysis and practical applications. Consider providing a more accessible description of the mathematical intricacies to improve readers understanding and highlighting future research directions for broader impact.", "pqCT3L-BU9T": " Summary: The paper introduces Matformer a transformer architecture for learning representations on periodic graphs encoding crystal materials. It addresses the unique challenges of encoding periodic structures in crystal materials specifically focusing on periodic invariance and explicit periodic pattern encoding. Through experiments on benchmark datasets the paper demonstrates that Matformer outperforms baseline methods consistently highlighting the importance of these components for crystal representation learning.  Strengths: 1. Innovative Approach: The paper introduces a novel transformer architecture specifically designed for periodic graph representation learning in crystal structures addressing key challenges in the field. 2. Thorough Experiments: The experimental results showcase the superiority of Matformer over baseline methods on multiple tasks and datasets providing clear evidence of its effectiveness. 3. Clear Definitions and Analysis: The paper provides clear definitions of periodic invariance and periodic pattern encoding along with indepth analysis of their significance in crystal representation learning. 4. Ablation Studies: The ablation studies effectively demonstrate the importance of periodic invariant graph construction and the encoding of repeating patterns in Matformers performance.  Weaknesses: 1. Complexity: The paper delves into technical details and complex mathematical formulations which might make it less accessible for readers without a strong background in the subject. 2. Limited Discussions on Limitations: While the paper discusses the importance of periodic invariance and pattern encoding it lacks an explicit discussion of the limitations and potential negative societal impacts of the proposed method.  Questions and Suggestions for Authors: 1. Clarity for NonExperts: Consider simplifying complex technical details to enhance the readability and accessibility of the paper to a wider audience. 2. Further Exploration: Discuss potential future directions for incorporating angular information more efficiently or addressing any remaining research gaps in periodic graph representation learning.  Limitations:  The paper does not thoroughly address the potential negative societal impacts of their work. Authors may consider discussing and mitigating any potential negative implications to ensure ethical considerations in material discovery research.", "u3vEuRr08MT": " Paper Summary and Contributions The paper empirically investigates exact memorization in causal and masked language modeling across different model sizes and training stages. It explores the impact of dataset size learning rate and model size on memorization dynamics. The key contributions include measuring the rate of memorization with respect to model size characterizing forgetting curves in language models and analyzing the memorization patterns of different parts of speech.  Strengths and Weaknesses  Strengths: 1. Thorough Analysis: The paper provides a detailed analysis of memorization dynamics forgetting curves and the impact of model size dataset size and learning rate. 2. Novel Findings: Uncovering that larger models can memorize faster before overfitting and showing the influence of nouns and numbers as unique identifiers in memorization offer significant insights. 3. Experimental Rigor: Conducting controlled experiments with varying model sizes learning rates and dataset sizes enhances the robustness of the findings.  Weaknesses: 1. Limited Scope: The study primarily focuses on examining memorization dynamics and forgetting curves potentially leaving out broader impacts or applications. 2. Assumptions: The paper assumes exact memorization as the primary metric which may not fully capture the nuanced aspects of model behavior and generalization. 3. Societal Implications: While not a weakness per se the paper could further delve into the implications of memorization dynamics on societal factors like privacy and bias in language models.  Questions and Suggestions for Authors 1. Generalization: How do the findings on memorization dynamics correlate with model generalization and performance on unseen data 2. External Memory: Could incorporating external memory mechanisms in models further improve memorization efficiency 3. Dataset Bias: Have you considered how dataset biases could impact the memorization dynamics observed in the study 4. Future Directions: What are the potential followup studies or extensions to deepen the understanding of memorization and forgetting phenomena in language models  Limitations and Suggestions for Authors The limitations regarding addressing negative societal impacts could be supplemented by:  Providing explicit discussions on potential negative societal ramifications of memorization dynamics.  Offering recommendations for mitigating privacy concerns and biases in language models based on the studys findings.", "x26Mpsf45P3": " Review:  1) Summary: The paper proposes a reinforcement learning principle that approximates the Bellman equations by enforcing their validity only along a userdefined space of test functions. The focus is on modelfree offline reinforcement learning with function approximation aiming to derive confidence intervals for offpolicy evaluation and optimize policies within a prescribed class. The paper provides theoretical guarantees and practical implementations even when the Bellman closure assumption does not hold. The contributions include proposing a method based on approximate empirical orthogonalization of the Bellman residual along test functions along with analyzing the statistical tradeoffs in the choice of test functions.  2) Strengths and Weaknesses:  Strengths:  The paper introduces a novel approach to offline RL with a focus on confidence intervals and policy optimization using a different modelfree principle compared to existing methods.  The theoretical guarantees provided in terms of oracle inequalities and policy optimization offer valuable insights into the statistical aspects of the problem.  The analytical depth in examining the statistical tradeoffs in choosing test functions showcases a thorough analysis.  Weaknesses:  The paper might benefit from a more extensive discussion on the practical implications and empirical validation of the proposed method.  Clearer explanations or examples could enhance the understanding of the proposed algorithms and their practical applications.  While theoretical contributions are notable the practical implementation aspects and scalability to realworld scenarios could be elaborated further.  3) Questions and Suggestions for the Authors:  Can the authors clarify how the proposed method compares to existing approaches in terms of computational efficiency and practical applicability  Could the paper include experiments or case studies to demonstrate the effectiveness of the proposed method in realworld scenarios  It would be beneficial to provide more concrete examples or intuitive explanations to aid readers in understanding the methodology and results.  4) Limitations: The paper could improve by addressing the limitations:  Adequately discussing the potential negative societal impacts of the proposed method and suggesting ways to mitigate any ethical concerns.", "r-CsquKaHvk": "1) Summary: The paper introduces a novel algorithm for survival analysis in a dynamic setting leveraging ideas from temporaldifference learning. The approach estimates a discretetime survival model by exploiting a temporalconsistency condition. The algorithm called TemporallyConsistent Survival Regression (TCSR) is compared with traditional methods on synthetic and realworld datasets demonstrating improved sampleefficiency and predictive performance. 2) Assessment of Strengths and Weaknesses: Strengths:  Introduces a novel algorithm bridging temporaldifference learning and survival analysis.  The approach leverages temporal consistency leading to improved predictive performance.  Thorough theoretical underpinnings and algorithmic details are provided.  Empirical evaluation on synthetic and realworld datasets supports the superiority of the proposed method.  Connection to reinforcement learning and extensions provide a comprehensive discussion. Weaknesses:  Lack of comparison with a broader set of existing dynamic survival analysis methods.  Could benefit from more indepth analysis on the potential negative societal impacts and ethical considerations of the algorithm.  Theoretical proofs and explanations while present could be more accessible to readers unfamiliar with the underlying concepts.  The paper could be strengthened by providing further benchmarks or comparison with more recent methods in survival analysis. 3) Questions and Suggestions for Authors:  Can you provide more insights into how TCSR compares with stateoftheart dynamic survival analysis methods especially those beyond landmarking  How could the proposed algorithm be implemented in practice considering computational efficiency and scalability  Have you considered the potential bias or ethical implications of using algorithms like TCSR in realworld applications especially in sensitive domains such as healthcare  Could you elaborate on the hyperparameter selection process for TCSR and how robust your method is to different parameter settings 4) Limitations: While the authors have thoroughly presented their work further discussion on the potential negative societal impacts and ethical considerations of deploying the TCSR algorithm in practical settings would enhance the papers completeness. It is suggested that the authors address these aspects to ensure responsible and ethical application of their algorithm.", "o8vYKDWMnq1": " PeerReview 1) Summary of the Paper: The paper provides a theoretical study of deep neural function approximation in reinforcement learning (RL) with \u03b5greedy exploration under the online setting. It focuses on understanding deep RL from the perspective of function class and neural network architectures beyond the \"linear\" regime. The work proves that with T episodes scaling the width and depth of the neural network is sufficient for learning with sublinear regret in Besov spaces. The analysis reformulates the temporal difference error in an L2(d\u03bc)integrable space over a certain averaged measure \u03bc and transforms it into a generalization problem under the noni.i.d setting. 2) Assessment of Strengths and Weaknesses: Strengths:  The paper addresses an important topic in the field of deep reinforcement learning  the theoretical understanding of deep RL beyond the \"linear\" regime.  The theoretical analysis is rigorous and the results provide insights into the influence of function class neural network architecture and exploration policy on learning efficiency.  The work bridges the gap between theory and practice in deep RL and offers guidelines for practitioners to consider different architectures for implementations. Weaknesses:  The paper may benefit from more empirical validation to complement the theoretical analysis and confirm the practical applicability of the proposed guidelines.  The complexity of the theoretical derivations might make it harder for nonexperts to follow and apply the results effectively. 3) Questions and Suggestions for the Authors:  Have you considered conducting empirical experiments to validate the theoretical findings and guidelines provided in the paper  How do the results and guidelines proposed in the paper align with existing practical implementations of deep RL algorithms like DQN  Could you provide more intuitive explanations or examples to help readers grasp the significance of the theoretical results for practical applications in deep RL 4) Limitations: The authors have not adequately addressed the limitations and potential negative societal impacts of their work. To enhance the societal impact and ethics of the research the authors could consider discussing potential biases in their theoretical assumptions or the application of their findings in realworld scenarios emphasizing considerations for fairness accountability and transparency in deep RL implementations. Overall the paper presents valuable contributions to the theoretical understanding of deep RL and provides important insights that can guide practical implementations in the field.", "kuJQ_NwJO8_": " Peer Review:  1) Summary: The paper proposes a framework called SUbgraph Retrievalaugmented GEneration (SURGE) for generating knowledgeconsistent dialogue responses using a Knowledge Graph (KG). The method involves retrieving relevant subgraphs from the KG encoding them with the dialogue context and enforcing consistency using graphtext contrastive learning. Experimental results on the OpendialKG dataset demonstrate significant improvements over existing methods in generating informative and knowledgegrounded responses.  2) Strengths and Weaknesses: Strengths:  Clear problem statement and welldefined approach to address the limitations in dialogue generation with factual knowledge.  Systematic experimental evaluation showcases the effectiveness of the proposed method over baselines.  Extensive discussion on knowledge retrieval graph encoding and contrastive learning demonstrating a comprehensive understanding of the problem domain.  Novel metric Knowledgeverifying Question Answering adds a new dimension to evaluation metrics for dialogue systems using knowledge. Weaknesses:  Lack of discussion on realworld application and scalability of the proposed model beyond the academic dataset.  Limited insight into the computational complexity of the model and the practical runtime performance.  Absence of discussion on any potential shortcomings or failure cases of the proposed SURGE framework.  3) Questions and Suggestions: 1. Could you provide more details on how the proposed framework could be scaled for industrial applications handling largescale or dynamic knowledge bases 2. How does the proposed method handle realtime inference scenarios given its reliance on complex subgraph retrieval and contrastive learning 3. Have you considered scenarios where the retrieved knowledge is incorrect or outdated and its impact on the generated responses 4. Can you explain the inherent interpretability and explainability features of the model especially in scenarios where erroneous responses are generated 5. Are there plans for extending the evaluation to more diverse datasets with varying knowledge domains to validate the generalizability of the SURGE framework  4) Limitations: The authors have adequately addressed limitations and potential negative societal impacts of their work. However they can further enhance the paper by providing insights into addressing realworld challenges and ensuring robustness in practical deployment through better explanation and concrete examples.  End of Peer Review ", "t3X5yMI_4G2": " Peer Review of \"Reincarnating Reinforcement Learning: A Computationally Efficient Workflow\" 1) Summary: The paper proposes the concept of reincarnating reinforcement learning (RL) as an alternative workflow to address the inefficiencies of tabula rasa RL in largescale settings. The authors introduce a specific setting policytovalue reincarnating RL (PVRL) and propose the QDagger algorithm to efficiently transfer suboptimal policies to valuebased RL agents. Empirical results demonstrate the superiority of reincarnating RL over tabula rasa RL on Atari games locomotion tasks and realworld problems like balloon navigation. The work emphasizes the potential of reincarnating RL to democratize research improve realworld RL adoption and enable continual improvement of existing trained agents. 2) Strengths:  The paper addresses an important gap in RL research by proposing an alternative workflow reincarnating RL to improve efficiency and democratize research.  The experimental evaluations on Atari games locomotion tasks and balloon navigation demonstrate the effectiveness of the proposed QDagger algorithm in PVRL settings.  The paper is wellstructured methodically detailing the problem the proposed solution experiments results and implications. 3) Weaknesses:  Despite the thorough experimental evaluation the paper lacks comparison with more stateoftheart RL algorithms or other existing reincarnation strategies limiting the assessment of the proposed QDagger algorithm.  The paper could do a better job of discussing potential challenges and limitations of reincarnating RL especially concerning the transferability and generalizability of the approach to various domains. 4) Questions and Suggestions:  Are there plans to extend the evaluation to more diverse and complex RL scenarios to further validate the robustness and scalability of reincarnating RL  Could the authors elaborate on the scalability of the QDagger algorithm to handle more complex environments with higherdimensional action and state spaces  How does the proposed reincarnating RL approach compare to metalearning or transfer learning frameworks in terms of efficiency adaptability and generalization 5) Limitations: The authors have not adequately addressed the potential negative societal impacts of reincarnating RL. Future work should include a comprehensive discussion on the ethical implications embedding mechanisms to prevent biases and undesirable traits and outlining guidelines on responsible use of reincarnating RL technology. 6) Suggestions for Improvement:  Enhance the experimental evaluation by comparing QDagger with a wider range of RL algorithms and reincarnation strategies to provide a more comprehensive analysis.  Discuss the portability and adaptability of reincarnating RL to diverse domains and novel applications to showcase the versatility and potential limitations of the proposed approach. Overall the paper presents an innovative concept of reincarnating RL with promising empirical results. Strengthening the comparison with existing methods discussing broader applications and addressing societal and ethical considerations would further enhance the impact and credibility of the work.", "wwyiEyK-G5D": " Summary: The paper presents a method called REVIVE for knowledgebased visual question answering (VQA) that aims to better utilize regional information in visual representation. The proposed method REVIVE integrates objectcentric visual representation for knowledge retrieval and answering in a transformerbased model. Extensive experiments on the OKVQA dataset demonstrate that REVIVE achieves stateoftheart performance surpassing previous methods by a large margin.  Strengths: 1. Innovative Approach: The paper brings a fresh perspective by focusing on utilizing regional visual information for knowledgebased VQA which leads to significant performance improvements. 2. Comprehensive Evaluation: The thorough empirical evaluation and ablation studies strengthen the credibility of the proposed method. 3. StateoftheArt Performance: REVIVE achieved a significant accuracy improvement over existing methods on the OKVQA dataset. 4. Publicly Available Code: The code availability on GitHub enhances the reproducibility and transparency of the research.  Weaknesses: 1. Limited Comparison: While the paper compares REVIVE with stateoftheart methods a comparison with more diverse datasets could have provided a broader perspective. 2. Lack of RealWorld Applications: The paper could benefit from discussing potential realworld applications or scenarios where REVIVE could be deployed and its impact.  Questions and Suggestions: 1. Generalization: How well does REVIVE perform on datasets outside OKVQA indicating the generalizability of the method 2. Scalability: Does the proposed method scale well with larger datasets in terms of both computational resources and training time 3. Interpretability: Can the model provide insights into why it generates certain answers especially in complex scenarios  Limitations:  Addressing Bias: Authors should ensure that the model does not perpetuate biases and harmful stereotypes. Consider incorporating bias mitigation techniques during training to avoid negative societal impacts. Overall the paper presents a strong contribution to improving knowledgebased VQA through regional visual representation. Addressing the limitations and suggestions provided above could further enhance the impact and applicability of the proposed method.", "qSs7C7c4G8D": " Peer Review  Summary: The paper proposes a unified convergence analysis for federated learning with arbitrary client participation patterns. It introduces a generalized version of FedAvg that amplifies parameter updates at intervals of multiple FL rounds. A novel analysis is presented to capture the effect of client participation leading to convergence upper bounds for various participation patterns both stochastic and nonstochastic. The papers contributions include introducing a novel algorithm providing insights from theoretical and experimental results and discussing convergence rates under different scenarios.  Strengths: 1. Innovative Approach: The paper addresses a significant challenge in federated learning by proposing a generalized FedAvg algorithm that considers arbitrary client participation patterns. 2. Comprehensive Analysis: The unified convergence analysis covers various participation patterns providing theoretical insights and competitive convergence upper bounds. 3. Theoretical Contributions: The papers theoretical analysis and methodology provide a solid foundation for understanding the impact of client participation on convergence rates in federated learning. 4. Experimental Validation: The empirical experiments support the theoretical findings and provide practical validation of the proposed approach.  Weaknesses: 1. Complexity: The paper contains a high level of technical detail and complex mathematical analysis which may hinder the understanding for readers not wellversed in federated learning and optimization theory. 2. Clarity: Some sections of the paper particularly the convergence analysis could benefit from simpler explanations and clearer illustrations to enhance readability for a wider audience. 3. Limited Comparative Analysis: While the empirical results confirm the effectiveness of the proposed method a more detailed comparative analysis with existing approaches could provide a clearer understanding of the advantages of the proposed algorithm.  Questions and Suggestions for Authors: 1. Scalability: How does the proposed algorithm scale with an increasing number of clients and data size Could the performance be affected in scenarios with a large number of clients or highdimensional data 2. Practical Implementation: How feasible is it to implement the proposed algorithm in realworld federated learning settings Are there any specific hardware or software requirements that need to be considered 3. Robustness: Have you considered the robustness of the algorithm to noise in the client data or communication channels How does the performance degrade under noisy conditions  Limitations: The authors should consider addressing the potential societal impact of the proposed algorithm particularly concerning the energy consumption and computational requirements in federated learning settings. Additionally including suggestions for mitigating any negative environmental impacts would enhance the overall contribution of the paper. Overall the paper presents a significant advancement in the convergence analysis of federated learning with arbitrary client participation patterns. However some improvements in clarity and practical applicability could further enhance the impact of the research.", "n6QYLjlYhkG": " Summary: The paper introduces HYPRO a novel hybridly normalized neural probabilistic model aimed at longhorizon prediction of event sequences. It addresses the limitations of existing autoregressive models by combining an autoregressive base model with an energy function. The proposed model significantly outperforms previous models in making longhorizon predictions of future events on realworld datasets.  Strengths: 1. Innovative Model: The combination of an autoregressive model with an energy function is a novel approach to address the challenges of longhorizon event sequence prediction. 2. Efficient Training Objectives: The proposed family of training objectives allows estimating model parameters with low computational cost. 3. Thorough Experiments: The paper conducts experiments on realworld datasets providing comprehensive evaluation metrics and comparisons with baseline models.  Weaknesses: 1. Complexity: The model architecture and training process may be complex and challenging to implement for practitioners. 2. Evaluation Metrics: While the paper provides detailed results additional visualization or comparative analysis could enhance the interpretation of the results. 3. Sparse Discussion on Related Work: While a section mentioning related work exists a more detailed comparison with existing models could provide better context for the proposed approach.  Questions and Suggestions for Authors: 1. Scalability: How does the proposed model perform with larger datasets or more complex event sequences 2. Interpretability: Can the authors provide insights into how the energy function contributes to the prediction results 3. Realworld Applications: Have the authors tested the model in practical scenarios to demonstrate its applicability and effectiveness in diverse domains  Limitations: The authors have not adequately addressed the potential negative societal impacts of their work. To improve they could incorporate an ethical and social impact assessment in their study to ensure responsible AI development. Overall the paper presents a promising approach to longhorizon event sequence prediction with notable contributions in model design and training methods. By addressing the weaknesses and considering the suggestions the authors can enhance the clarity and impact of their research.", "nE6vnoHz9--": " Summary: The paper introduces a novel approach CrossDomain Predictor (CDP) to make Neural Architecture Search (NAS) more efficient by utilizing existing NAS benchmark datasets to train a predictor that can find highperformance architectures in large search spaces. The proposed CDP consists of a progressive subspace adaptation strategy and an assistant space to address domain discrepancies and improve efficiency achieving promising results with significantly lower search costs compared to existing methods.  Strengths: 1. Innovative Approach: The paper introduces a novel concept of crossdomain prediction to overcome the limitations of existing NAS methods contributing to the advancement of the field. 2. Efficiency: The proposed CDP significantly reduces search costs by utilizing existing benchmark datasets making it a more practical and efficient solution for NAS. 3. Experimental Results: The experimental results on ImageNet and CIFAR10 demonstrate the effectiveness and efficiency of CDP in finding highperformance architectures with minimal computational resources.  Weaknesses: 1. Theoretical Clarity: The paper would benefit from clearer explanations of certain theoretical concepts and methodologies especially in Sections 2 3 and 4. 2. Comparative Analysis: While the paper provides insights into the performance of CDP a more comprehensive comparison with a wider range of existing NAS methods could further strengthen the evaluation.  Questions and Suggestions: 1. Data Partitioning: Could the authors elaborate more on how the data partitioning strategy is determined and its impact on the final results 2. Validation Metrics: Are there additional metrics besides KTau that could provide a better understanding of the predictors performance especially when compared to existing methods 3. Scalability: How scalable is the proposed CDP approach for even larger search spaces beyond DARTS Are there any limitations or challenges in scaling up  Limitations: The authors have adequately addressed the limitations and potential negative societal impacts by focusing on the technical aspects of the proposed method. To enhance the discussion it might be beneficial to also consider potential biases introduced by the training data or unintended consequences of using the predictor in realworld applications. Overall the paper presents a promising advancement in NAS methods with the CDP approach demonstrating notable efficiency gains. By addressing some of the weaknesses and incorporating additional clarity and comparative analysis the paper can further enhance its impact in the field.", "ksVGCOlOEba": " Summary: The paper introduces a new compression framework that unifies weight pruning and quantization in a posttraining setting. The approach significantly improves the compressionaccuracy tradeoffs of existing methods. The authors provide theoretical foundations based on the Optimal Brain Surgeon (OBS) framework adapt it for weight quantization (Optimal Brain Quantizer) and present experimental results showing superior performance in various tasks and models.  Strengths: 1. Innovative Approach: The paper proposes a novel compression framework that unifies pruning and quantization demonstrating improved practical performance. 2. Theoretical Rigor: The authors provide a detailed technical foundation based on the OBS framework extending it to quantization with efficient algorithms. 3. Experimental Validation: Extensive experiments are conducted across various tasks and models showcasing the effectiveness of the proposed approach. 4. Efficiency: The development of exact algorithms reduces computational costs enabling practical implementation on modern DNNs.  Weaknesses: 1. Limited Comparison: While the paper compares against existing methods in certain scenarios a more comprehensive comparison across a wider range of settings would enhance the evaluation. 2. Complexity: The detailed technical discussion may be challenging for nonexperts to follow potentially limiting the accessibility of the paper to a broader audience.  Questions and Suggestions for Authors: 1. Scalability: How does the proposed approach scale with larger models and datasets Have you considered the performance of the method on ultralarge models such as GPT3 2. Robustness: How robust is the framework to variations in calibration data size and quality Have you tested the methods robustness under noisy or limited calibration scenarios 3. Implementation Details: Could you provide more insights into the practical implementation aspects such as computational efficiency on different hardware setups 4. Generalization: Have you tested the generalizability of the framework across different neural network architectures and tasks beyond the ones mentioned in the experiments  Limitations: The paper does not adequately address the potential negative societal impacts of the work. To improve the authors could consider discussing the environmental implications of model compression and potential societal implications of accelerated adoption of compressed models. In conclusion the paper presents an innovative and promising framework for posttraining compression of DNNs. Addressing the suggested improvements and limitations will enhance the overall quality and impact of the work.", "pBpwRkEIjR3": "Summary: The paper proposes enhanced bilevel optimization methods using the Bregman distance to solve nonconvexstronglyconvex bilevel optimization problems with nonsmooth regularization. The proposed BiOBreD algorithm is designed for deterministic bilevel problems while SBiOBreD is for solving stochastic bilevel problems. An accelerated version of SBiOBreD ASBiOBreD is also introduced to improve computational complexity. The methods are demonstrated to outperform related bilevel optimization approaches in data hypercleaning and hyperrepresentation learning tasks. Strengths: 1. Novelty and Contribution:  Introduces a new class of efficient bilevel optimization algorithms using Bregman distance.  Offers comprehensive convergence analysis for the proposed methods.  Demonstrates superior performance compared to existing bilevel optimization approaches in hypercleaning and hyperrepresentation learning tasks. 2. Methodological Advancements:  Suggests efficient bilevel optimization approaches that address computational complexity issues.  Presents deterministic and stochastic algorithms to handle nonconvexstronglyconvex bilevel optimization problems.  Introduces an accelerated version using variancereduced techniques for faster convergence. 3. Theoretical Rigor:  Provides thorough convergence analysis to validate the efficiency of the proposed algorithms.  Demonstrates improvements in sample complexity and computational efficiency compared to existing methods. Weaknesses: 1. Clarity in Presentation:  The paper would benefit from clearer section headings and subheadings for easier navigation.  Some parts of the methodology and experiments section could be further clarified to enhance understanding. Questions and Suggestions: 1. Hypercleaning Task:  How sensitive are the results of the hypercleaning task to variations in the corruption rate of the training data  Could the impact of different regularization parameters in the hypercleaning task be explored in more detail 2. Hyperrepresentation Learning Task:  Considering the sparsity constraint on \u03bb in the hyperrepresentation learning task how does the choice of \u03b1 affect the models performance  Can the implications of varying the tuning parameters \u03b1 and C on the learned hyperrepresentations be discussed Limitations: The paper does not adequately address the potential negative societal impact or ethical considerations of the proposed enhanced bilevel optimization methods. Authors could consider adding a discussion on ethical implications and potential societal impacts in future work to ensure responsible AI research. This review provides a detailed evaluation of the strengths weaknesses questions and suggestions for the authors to enhance the quality and impact of their work.", "xvZtgp5wyYT": " Summary: The paper introduces Latent Evolution of PDEs (LEPDE) as a method to accelerate the simulation and inverse optimization of Partial Differential Equations (PDEs). LEPDE learns a global latent representation of the system to efficiently evolve dynamics in a latent space achieving significant speedups and reductions in dimensionality while maintaining competitive accuracy. The paper includes model architecture details learning objectives and experiments demonstrating the effectiveness of LEPDE in 1D and 2D PDE problems compared to stateoftheart methods.  Strengths:  Innovative Approach: The concept of learning a latent representation for accelerating PDE simulations is novel and addresses the efficiency challenges faced by classical and deep learningbased solvers.  Comprehensive Experiments: The paper provides thorough experiments in both 1D and 2D PDE benchmarks to validate the performance of LEPDE.  Effective Results: LEPDE shows significant speedups and dimensionality reductions compared to stateoftheart models while maintaining competitive accuracy demonstrating its effectiveness.  Weaknesses:  Clarity: The paper could improve clarity in explaining certain technical details such as the model architecture and learning objectives to enhance understanding for readers.  Comparative Analysis: While the results are promising a more detailed comparative analysis with discussions on potential limitations or tradeoffs could add depth to the evaluation.  Questions and Suggestions: 1. Latent Dimension Choice: Could the authors explain the rationale behind choosing a specific latent dimension for the experiments How does the latent dimension impact performance and efficiency 2. LongTerm Stability: How does LEPDE ensure the stability of longterm predictions in complex systems exhibiting chaotic behavior like turbulent flows Are there scenarios where stability could be compromised 3. Scalability: To what extent can LEPDE be scaled to handle even larger and more complex PDE systems or higherdimensional problems beyond the 2D cases presented  Limitations: The paper effectively addresses the limitations and potential negative societal impact by focusing on advancements in scientific and engineering domains without discussing broader societal implications. To improve the authors could extend the discussion to include potential applications in broader contexts or implications for industries outside of science and engineering.", "pF8btdPVTL_": " Summary: The paper discusses the phenomenon of \"benign overfitting\" in neural networks focusing on training a twolayer convolutional neural network. The authors show the conditions under which benign overfitting occurs and characterize a phase transition between benign and harmful overfitting based on the signaltonoise ratio. They introduce a signalnoise decomposition technique and provide theoretical results demonstrating the behavior of overparameterized neural networks.  Strengths: 1. Original Contribution: The paper presents a novel analysis of benign overfitting specifically in convolutional neural networks addressing a gap in the current literature. 2. Theoretical Rigor: The authors provide detailed theoretical proofs and establish population loss bounds for overfitted CNN models trained by gradient descent. 3. Algorithmic Analysis: The signalnoise decomposition technique and twostage analysis add depth to understanding the neural network learning process. 4. Phase Transition Demonstration: Demonstrating a sharp phase transition between benign and harmful overfitting based on the signaltonoise ratio is a significant contribution.  Weaknesses: 1. Complexity: The papers technical depth may limit accessibility to general readers or those not wellversed in deep learning theory. 2. Scope Limitation: The study focuses on a specific type of neural network and data model potentially limiting the generalizability of the results to broader neural network architectures.  Questions and Suggestions for Authors: 1. Generalization: Have the authors considered exploring the application of their analysis and techniques to more complex neural network architectures beyond the twolayer CNN studied in the paper 2. Empirical Validation: Will the authors consider providing empirical validation or case studies to demonstrate the practical implications of their theoretical findings on realworld datasets 3. Comparative Analysis: Could the authors discuss how their signalnoise decomposition technique compares with existing approaches or whether it could be extended to address other challenges in deep learning  Limitations: The authors could enhance their study by considering the following:  Practical Implementation: Address how the theoretical findings can be practically implemented in training deep neural networks to improve realworld performance and robustness.  Negative Societal Impact: Explore and discuss potential negative societal impacts of misapplying the findings such as incorrect training procedures or misinterpretation of benign overfitting. Overall the paper presents a valuable contribution to the understanding of benign overfitting in neural networks with theoretical depth and technical rigor.", "lhl_rYNdiH6": " Peer Review  1) Summary: The paper proposes a novel approach named Contrastive Graph Structure Learning via Information Bottleneck (CGI) for recommendation systems. CGI aims to optimize graph structures by adaptively dropping nodes and edges creating multiple views of the original graph and integrating these views using the Information Bottleneck principle to enhance recommendation performance. Extensive experiments on public datasets demonstrate that CGI outperforms existing methods in recommendation tasks.  2) Strengths and Weaknesses: Strengths:  The paper addresses the limitations of existing methods in mitigating popularity bias and handling noisy interactions in recommender systems.  Novel approach of learnable graph augmentation via dropping nodes and edges to create optimized graph structures.  Integration of Information Bottleneck into contrastive learning process to capture relevant information for recommendation tasks effectively.  Extensive experiments conducted to validate the superiority of CGI over strong baselines on multiple benchmark datasets. Weaknesses:  The paper lacks a detailed discussion on the scalability and computational efficiency of the proposed CGI approach compared to existing methods.  The evaluation section discusses performance metrics but additional insights into computational complexity scalability and potential realworld deployment challenges could further enhance the paper.  Future work could aim to address limitations related to handling dynamic datasets or incremental learning scenarios.  3) Questions and Suggestions for Authors:  Can the authors provide insights into the interpretability of the generated graph structures by CGI and how they correlate with recommendation outcomes  Have the authors considered investigating the computational costs and scalability aspects of CGI especially in handling larger datasets or realtime recommendation scenarios  How does CGI perform in terms of algorithm convergence especially in scenarios with dynamic changes in user preferences or item interactions  Are there any plans for future work to explore the adaptability of CGI in scenarios where user preferences evolve over time and how would CGI handle such dynamic environments  4) Limitations: While the paper extensively validates CGIs performance through experiments it lacks indepth discussions on potential negative societal impacts or ethical considerations related to recommendation systems. Authors could enhance the paper by incorporating a section dedicated to addressing the limitations and negative societal implications of their work along with suggestions for minimizing biased or harmful recommendations. Overall the paper presents a valuable contribution to the field of recommendation systems with a novel approach that shows promising results. Further discussions on scalability interpretability and future directions would enrich the paper.", "rAVqc7KSGDa": "1) Summary: The paper introduces a novel stochastic active sets (SAS) approach for scaling Gaussian process (GP) decoders in the context of Gaussian process latent variable models (GPLVMs). The SAS approximation significantly improves the training robustness of GP decoders while reducing computational costs. The paper demonstrates that the SASGP approach outperforms variational autoencoders in terms of learning representations and scalability. 2) Assessment of Strengths and Weaknesses: Strengths:  Introduces a novel SAS approach for scaling GP decoders.  Demonstrates improved training robustness and reduced computational costs compared to existing methods.  Employs active set approximations to link with crossvalidation providing a stochastic estimate of the logmarginal likelihood.  Shows empirical evidence of betterfitted GP decoders using SAS even outperforming variational autoencoders.  Provides historical context and comparisons with related works showcasing the evolution of GP approaches. Weaknesses:  The paper is highly technical and may be challenging for readers without a strong background in GP models and variational inference.  The provided experiments are limited to unsupervised scenarios restricting the generalizability of the approach to broader applications.  Lack of comparison with a wider range of stateoftheart methods potentially limiting the insight into the relative performance of SASGP decoders.  The limitations of focusing on Gaussian likelihood for the SAS approach are acknowledged but alternative likelihoods for discrete data are only briefly mentioned without exploration. 3) Questions and Suggestions for Authors:  Can the SAS approach be extended or adapted for more diverse likelihood functions beyond Gaussian particularly for discrete datasets  The paper mainly focuses on unsupervised scenarios. Have you considered conducting experiments in supervised settings to explore the efficacy of SASGP decoders in classification or regression tasks  Further exploration of the impact of different kernel choices on the SASGP approach could provide insights into enhancing representation learning in diverse domains.  It would be beneficial to include realworld application examples to showcase the practical utility of SASGP decoders in various domains. 4) Limitations and Societal Impact: The authors address limitations related to the Gaussian likelihood and mention the need for more powerful likelihood functions for discrete data. To improve authors could explore and test alternative likelihoods for broader applicability. Additionally discussing potential societal impacts and ethical considerations arising from the use of GP decoders could enhance the papers completeness and relevance.", "wOI0AUAq9BR": " Review of \"Improving SizeGeneralization of Graph Neural Networks with SizeShift Regularization\" Summary: The paper addresses the challenge of poor sizegeneralization performance of Graph Neural Networks (GNNs) when applied to graphs of different sizes than those in the training data. The authors propose a regularization strategy that enhances the sizegeneralization capabilities of GNNs without the need for domainspecific assumptions or access to test data. The approach involves simulating a shift in the size of training graphs using coarsening techniques and enforcing the model to be robust to such shifts. Experimental results demonstrate significant performance improvements on standard datasets showcasing the effectiveness of the proposed regularization method. Strengths: 1. Novel Contribution: The paper introduces a novel regularization strategy that offers substantial performance gains in sizegeneralization without relying on domainspecific assumptions or test data access. 2. Empirical Evaluation: The experimental results on popular benchmark datasets are extensive and clearly demonstrate the effectiveness of the proposed regularization method showcasing performance improvements of up to 30. 3. Model Agnostic: A key strength of the regularization approach is its modelagnostic nature making it applicable to various GNN architectures without necessitating extensive modifications or precomputations. 4. Comparative Analysis: The comparison with existing methods such as domain adaptation Invariant Risk Minimization and more expressive models provides a comprehensive view of the benefits of the proposed regularization approach. Weaknesses: 1. Theoretical Insight: While the empirical results are strong a deeper theoretical analysis or insights on why the proposed regularization method is effective would enhance the academic rigor of the paper. 2. Hyperparameter Sensitivity: The impact of hyperparameter tuning specifically the regularization coefficient \u03bb and coarsening ratios could be further discussed to provide a clearer understanding of their influence on results. Questions and Suggestions for Authors: 1. Can you provide insights into why the proposed regularization method results in significant performance gains across different GNN models 2. How sensitive is the performance improvement to hyperparameter choices and could additional analysis on varying hyperparameters be included to strengthen the experimental findings 3. Have you considered testing the method on datasets with even larger size disparities between training and test graphs to gauge the scalability and effectiveness of the regularization approach Limitations: The authors have adequately discussed the limitations of assuming properties relevant for graph classification do not depend on graph size. To improve they could conduct experiments on datasets with more extreme size disparities and broader graph distributions. Overall the paper presents a strong contribution with notable experimental results. Addressing the theoretical underpinnings and hyperparameter sensitivity would enhance the impact and robustness of the proposed regularization strategy.", "nC8VC8gVGPo": " Peer Review: Summary: The paper proposes a novel learning rule called Local Tandem Learning (LTL) for training deep Spiking Neural Networks (SNNs) efficiently and effectively especially for deployment on analog computing substrates. The LTL rule combines the benefits of ANNtoSNN conversion and gradientbased training methods achieving rapid network convergence on CIFAR10 dataset with low computational complexity. The proposed rule is hardwarefriendly and can address device nonideality issues making it suitable for ultralowpower mixedsignal neuromorphic computing chips. Experimental results demonstrate the effectiveness and scalability of the LTL rule on multiple datasets. Strengths: 1. Innovation: The proposed LTL rule fills a significant research gap in efficient training of deep SNNs for deployment on analog computing substrates. 2. Experimental Rigor: The paper provides comprehensive experimental results on CIFAR10 CIFAR100 and Tiny ImageNet datasets showcasing the superiority of the LTL rule over existing methods. 3. HardwareFriendly: The emphasis on hardwarefriendly implementation and robustness against device nonideality enhances the practical applicability of the proposed rule. 4. Efficiency: The methods low computational complexity and rapid network convergence make it a promising solution for powerefficient pattern recognition tasks. Weaknesses: 1. Comparative Analysis: While the paper compares the proposed LTL rule with existing methods a more detailed comparative analysis with diverse benchmarks and scenarios could further strengthen the evaluation. 2. Theoretical Justification: Providing deeper theoretical insights into why the proposed approach outperforms existing methods could enhance the papers scientific contribution. Questions and Suggestions: 1. Hardware Validation: Could the authors provide insights into potential experimental setups or validation methods to assess the hardware implementation feasibility and performance of the LTL rule 2. Generalizability: How generalizable is the LTL rule across different network architectures and datasets Are there specific scenarios where it might perform less optimally 3. Scalability: How does the scalability of the LTL rule fare with largescale datasets beyond CIFAR and complex network architectures 4. Impact Assessment: Has the paper adequately addressed the potential negative societal impacts or ethical considerations arising from the deployment of SNNs trained using the LTL rule A brief discussion on this aspect would enhance the papers completeness. Limitations: The paper could benefit from a more extensive discussion on the limitations of the proposed LTL rule and suggestions for future research to address those limitations especially concerning scalability performance under varying noise conditions and broader societal implications.", "kRgOlgFW9aP": " Summary: The paper introduces Thompson sampling (TS) for datadriven control of diffusion processes with uncertain drift matrices. It establishes the efficiency of TS in stabilizing the system and minimizing a quadratic cost function showing regret bounds that grow as the squareroot of time. Theoretical results are validated through empirical simulations.  Strengths: 1. Novel Contribution: The paper provides the first theoretical analysis of TS for diffusion process control problems filling an important gap in the literature. 2. Theoretical Rigor: The paper offers comprehensive theoretical analysis establishing stabilization guarantees and regret bounds for TS in continuoustime models. 3. Methodological Soundness: The approach taken in Algorithm 1 for stabilization under uncertainty and Algorithm 2 for efficient control is wellstructured and theoretically grounded. 4. Empirical Validation: Empirical simulations demonstrate the effectiveness of TS showing improved regret compared to existing algorithms.  Weaknesses: 1. Complexity of Analysis: The technical nature of the analysis may make it challenging for readers unfamiliar with advanced concepts in reinforcement learning and control theory. 2. Limited Generalizability: The focus on diffusion processes with uncertain drift matrices might limit the broader applicability of the proposed methods.  Questions and Suggestions for Authors: 1. Theoretical Assumptions: Could you elaborate on the assumptions made in the theoretical analysis especially regarding the properties of the diffusion process and the noise terms 2. Algorithm Implementation: How scalable and practical are the proposed algorithms for realworld applications in terms of computational complexity and implementation challenges 3. Comparison with StateoftheArt: How does the proposed TS approach compare to other stateoftheart algorithms in terms of computational efficiency and performance in more complex scenarios  Limitations: The paper did not address potential negative societal impacts of the proposed algorithms. To improve authors can consider discussing ethical considerations bias mitigation strategies transparency in data usage and implications on societal wellbeing.", "siG_S8mUWxf": "Summary: The paper introduces Subequivariant Graph Neural Networks (SGNN) to address challenges faced by Graph Neural Networks (GNNs) in learning physical dynamics particularly focusing on incorporating symmetry and objectaware information in the model design. SGNN relaxes equivariance to subequivariance introduces a novel objectaware message passing scheme and operates hierarchically to model complex interactions. Experimental results demonstrate significant enhancements in contact prediction accuracy and rollout MSE compared to stateoftheart GNN simulators showcasing strong generalization and data efficiency. Strengths: 1. Innovative Approach: The proposed SGNN introduces novel concepts like subequivariance and objectaware message passing to handle challenges in physical simulation effectively. 2. Hierarchical Modeling: The hierarchical approach to modeling complex physical scenes is a strength enabling a more nuanced understanding of object interactions. 3. Experimental Results: The comprehensive evaluation on Physion and RigidFall datasets along with ablation studies demonstrate the effectiveness of SGNN in enhancing accuracy and generalization. Weaknesses: 1. Complexity: The detailed formulations and technical terms may hinder accessibility for readers with limited background knowledge in GNN and physics. 2. Clarity: The paper could benefit from clearer explanation of key concepts such as equivariance subequivariance and objectaware message passing for better understanding by a broader audience. Questions and Suggestions: 1. Clarification on Objectness: Could the authors provide more insight into how objectness is incorporated in distinguishing interactions within and between objects in the model 2. Scalability: How does the proposed hierarchical modeling scale with increasing complexity of interactions and larger numbers of objects 3. Data Augmentation: Have the authors explored other data augmentation techniques apart from rotation around gravity for enhancing model generalization across various directions Limitations: While the paper extensively examines the limitations and societal impacts of the proposed work focusing on realworld data noise and selfcontact handling a more explicit discussion on ethical considerations and potential biases especially in applications like robotics or physical reasoning could further enrich the discussion. Suggestions for Improvement: To enhance the paper: 1. Enhance the clarity of technical descriptions for better readability. 2. Provide additional visual aids or intuitive examples to illustrate key concepts. 3. Further elaborate on the broader societal implications and ethical considerations of the proposed work.", "wtuYr8_KhyM": "Peer Review Summary: The paper proposes a novel activation function called Adaptive SwisH (ASH) that aims to mimic human neurons adaptive threshold mechanisms to enhance the performance of deep learning models. ASH is designed to provide trainable adaptive and contextaware properties different from traditional activation functions. The study provides mathematical derivations theoretical explanations and experimental validations across various tasks including classification detection segmentation and image generation. Strengths: 1. Innovation: The paper introduces a novel activation function that addresses the limitations of traditional activation functions by incorporating adaptivity and context awareness. 2. Mathematical Rigor: The study provides detailed mathematical modeling and derivations to support the conceptualization and implementation of ASH. 3. Experimental Validation: The experimental evaluation across tasks (classification detection segmentation) using popular datasets demonstrates the superiority of ASH in terms of accuracy sparsity training time and localization. 4. Generalization: The paper convincingly establishes the relationship between ASH and the Swish activation function providing theoretical justification for the latters empirical success. Weaknesses: 1. Clarity: The paper at times delves into intricate mathematical details which might be challenging for nonexperts to follow easily. 2. Implementation Details: More insights on the practical implementation of ASH in different deep learning frameworks or scenarios could enhance the applicability of the proposed activation function. 3. Limitations: While the paper extensively discusses the benefits and improvements brought by ASH a clearer identification and discussion of potential limitations and tradeoffs could provide a more balanced view of the proposed approach. Questions and Suggestions: 1. Practical Implementation: How does the computational complexity of ASH compare to traditional activation functions Are there any specific scenarios or architectures where ASH might be more beneficial 2. Robustness: How does ASH perform under noisy or adversarial input conditions Has the study evaluated the robustness of ASH in realworld scenarios 3. Future Directions: Considering the promising results of ASH what are the potential avenues for further research or extensions of this work Are there any specific tasks or domains where ASH could be particularly impactful Limitations: While the paper extensively discusses the benefits of ASH there is a need to address potential drawbacks practical challenges and negative societal impacts. Including a discussion on these aspects would provide a more comprehensive evaluation of ASHs implications. Overall the paper presents a valuable contribution to the field of deep learning with the introduction of ASH activation function. Addressing the identified weaknesses and exploring the suggested questions can further enhance the clarity and applicability of the proposed approach.", "rF6zwkyMABn": "Peer Review: Summary: The paper presents new algorithms for online learning with general directed feedback graphs achieving nearly optimal regret bounds for strongly observable and weakly observable graphs in both adversarial and stochastic environments. By incorporating new regularization functions and update rules for learning rates the proposed algorithms provide improved regret bounds compared to existing approaches. Strengths: 1. Original Contribution: The study makes a significant contribution by proposing bestofbothworlds algorithms that achieve nearly optimal regret bounds for online learning with feedback graphs addressing a gap in the existing literature. 2. Clear Problem Formulation: The authors provide a comprehensive problem setting incorporating definitions of observability independence number domination number and selfbounding constraints offering a clear understanding of the theoretical framework. 3. Rigorous Analysis: Theoretical proofs and regret bounds provided in the study are detailed and mathematically sound enhancing the credibility of the proposed algorithms. Weaknesses: 1. Assumed Knowledge: The presentation of the proposed algorithms and their analysis heavily relies on familiarity with advanced concepts in online learning and optimization potentially limiting accessibility for readers without a deep understanding in the field. 2. Complexity: The paper contains intricate mathematical derivations and technical details which might be challenging for readers unfamiliar with the specific algorithms and frameworks used in the study. Questions and Suggestions for the Authors: 1. Clarification on Algorithm Implementation: Can the authors provide more insights into the practical implementation of the proposed algorithms How scalable are these algorithms for realworld applications with largescale feedback graphs 2. Comparative Analysis: Are there specific scenarios or settings where the proposed algorithms outperform existing stateoftheart approaches or are there limitations to their applicability that should be addressed Limitations: The paper may benefit from discussing the potential negative societal impacts of introducing highly complex algorithms which might lead to barriers in understanding and implementation. Suggestions for improvements include providing practical examples or case studies to illustrate the realworld implications of the proposed algorithms. Overall the paper presents valuable contributions to the field of online learning with feedback graphs but ensuring readability for a broader audience and addressing potential societal impacts would further enhance the significance and applicability of the research.", "sBrS3M5lT2w": " Summary: The paper addresses the restrictiveness of current assumptions in stochastic gradient descent (SGD) theory for training models in machine learning by providing novel theory that relaxes these assumptions. The work demonstrates the restrictive nature of current assumptions using three canonical models in machine learning and then proposes novel theory that allows for a greater range of applicability of SGD to stochastic optimization problems.  Strengths: 1. Novel Contributions: The paper offers a significant contribution by relaxing restrictive assumptions in SGD theory allowing for a wider range of stochastic optimization problems. 2. Theoretical Rigor: The paper is wellstructured with a clear demonstration of the limitations of existing assumptions and the development of new theory. 3. Innovative Analysis Techniques: Innovative analysis strategies are developed to address issues arising from diverging iterates ensuring the stability of the objective function. 4. Thorough Analysis: The work is thorough covering various scenarios and providing detailed proofs and demonstrations.  Weaknesses: 1. Complexity: The paper is highly technical and might be challenging for readers not wellversed in advanced machine learning theory. 2. Interpretational Challenges: The authors acknowledge some assumptions that lack clear interpretations which may hinder the ease of understanding and applicability of the proposed theory.  Questions and Suggestions for the Authors: 1. Clarify Interpretation: Can the authors provide additional explanations or examples to illustrate the implications and interpretations of assumptions that lack clear interpretations 2. Practical Relevance: How do the proposed relaxations of assumptions impact realworld machine learning applications and practical implementations 3. Comparative Analysis: Have the authors considered comparing their results with existing stateoftheart techniques in stochastic optimization to demonstrate the effectiveness and superiority of their proposed approaches  Limitations: The paper addresses the limitations and negative societal impacts of their work adequately by acknowledging the assumptions and interpretations challenges. Suggestions for improvement include providing clearer interpretations of assumptions and enhancing the practical relevance of the proposed theory.", "nN3aVRQsxGd": " 1) Summary: The paper theoretically characterizes the expressive power of Khop message passing Graph Neural Networks (GNNs) and introduces the KPGNN framework to enhance their expressive capabilities. Specifically the paper distinguishes different kernels of Khop neighbors shows that Khop message passing is more powerful than 1WL and introduces KPGNN to distinguish regular graphs better. The experimental results verify the effectiveness of KPGNN across various datasets.  2) Assessment of Strengths and Weaknesses: Strengths:  The paper provides a novel theoretical analysis of Khop message passing GNNs shedding light on their expressive power.  Introduction of the KPGNN framework to improve expressive power by leveraging peripheral subgraph information is innovative.  Extensive experiments on diverse datasets demonstrate competitive performance and effectiveness of KPGNN. Weaknesses:  Lack of clear delineation between theoretical analysis and experimental results a clearer separation of these sections would enhance readability.  The paper could benefit from a more detailed comparison with existing stateoftheart methods in terms of performance and expressive power.  Some parts of the theoretical analysis and methodology could be elaborated further to enhance clarity.  3) Questions and Suggestions for Authors: 1. Clarity in Methodology:  Can you provide a more detailed breakdown of the computational complexity of KPGNN compared to traditional GNNs for better understanding 2. Indepth Comparison:  Could you discuss the limitations of the KPGNN framework in further detail especially with respect to scenarios where it may not perform as well as expected  How does the KPGNN framework compare to subgraphbased GNNs in terms of computational efficiency and expressive power 3. Future Directions:  Have you considered exploring the applicability of KPGNN in dynamic or evolving graph scenarios and how it may adapt to changing structures over time  Are there plans to investigate the scalability of KPGNN to larger graphs and datasets  4) Limitations: The authors have adequately addressed the limitations of their work particularly highlighting the dependence on selecting appropriate kernels for different scenarios. To further improve offering more concrete suggestions for overcoming these limitations could enhance the overall impact of the work.", "vsNQkquutZk": " Summary: The paper introduces a novel regularization method called WaveBound for time series forecasting. WaveBound provides dynamic error bounds for training loss at each time step and feature to address the overfitting issue in time series forecasting. The method significantly improves performance compared to existing models including stateoftheart methods on six realworld benchmark datasets.  Strengths: 1. Innovative Contribution: The proposed WaveBound method is a novel and effective approach to addressing overfitting in time series forecasting by dynamically adjusting error bounds. 2. Extensive Experiments: The paper conducts thorough experiments on realworld benchmarks and provides detailed performance comparisons showcasing the effectiveness of WaveBound. 3. Comparison to StateoftheArt: WaveBound consistently outperforms existing models including stateoftheart methods demonstrating its superiority. 4. Theoretical Justification: The theoretical analysis including the MSE reduction theorem provides a solid foundation for the proposed method.  Weaknesses: 1. Complexity of Method: The paper could benefit from a clearer explanation of the WaveBound method to enhance the understanding of readers. 2. Limitation Addressing: The paper could delve deeper into discussing potential limitations and negative societal impacts of the proposed method to ensure comprehensive coverage.  Questions and Suggestions for Authors: 1. Clarification: Can the authors provide a more intuitive explanation of how WaveBound dynamically adjusts error bounds for each time step and feature 2. Generalization: How does the proposed method perform in terms of generalization to unseen data over long forecasting horizons 3. Scalability: Have the authors considered the computational scalability of WaveBound for largescale time series datasets 4. Comparison with Traditional Methods: How does WaveBound compare to traditional regularization techniques such as L2 regularization or Dropout in the context of time series forecasting  Limitations: The authors should provide a more detailed discussion on the limitations and potential negative societal impacts of WaveBound. Suggestions for improvement include conducting additional analyses to address these aspects and incorporating them into the paper for a more holistic review.", "tvwkeAIcRP8": " Peer Review Summary: The paper introduces a method for neural reflectance field optimization from singleview images captured under different point lights for multiview scene reconstruction. The method leverages shading and shadow cues to recover 3D geometry and BRDFs of a scene supporting applications like novelview synthesis and relighting. Experimental results on various datasets demonstrate the effectiveness of the proposed method. Strengths: 1. Novel Contribution: The paper addresses a unique problem and proposes a novel method to optimize a neural reflectance field for scene reconstruction from singleview images. 2. Utilization of Shading and Shadow: The method effectively exploits shading and shadow cues for inferring scene geometry enhancing the reconstruction accuracy. 3. Experimental Evaluation: The thorough experimental evaluation on challenging datasets demonstrates the capability of the proposed method in faithfully reconstructing scene geometry. 4. Efficiency and Robustness: The approach efficiently addresses depth discontinuities and is robust in recovering both visible and invisible parts of a scene. Weaknesses: 1. Limitation Discussion: The paper briefly mentions limitations such as the need for known light positions and the influence of complex backgrounds on performance. More detailed discussion and mitigation strategies for these limitations would enhance the paper. 2. Shadow Constraints: While the method successfully reconstructs invisible shapes addressing the challenge of constraining reflectance in those regions using shadow cues could be further elaborated. 3. InterReflection Effects: The omission of interreflection effects in image formation is a limitation. Incorporating a discussion on handling interreflections would strengthen the paper. Questions and Suggestions: 1. Light Calibration: Can the method be adapted to handle scenarios with unknown or varying light positions to reduce the need for manual calibration 2. Complex Backgrounds: How does the method cope with scenes with intricate backgrounds that impact shadow appearance 3. Reflectance Constraints: Are there plans to enhance the method to better constrain the reflectance of invisible regions when relying on shadow cues 4. InterReflection Handling: How might interreflections be incorporated into the model to account for complex light interactions in scenes Suggestions for Improvement: 1. Detailed Limitation Discussion: Elaborate on the limitations with more depth and suggest potential mitigation strategies. 2. Enhanced Shadow Constraints: Develop strategies to improve the methods ability to constrain reflectance properties in invisible regions using shadow information. 3. InterReflection Incorporation: Explore methods to include interreflection effects in the image formation model for more comprehensive scene reconstruction. Limitations: The authors have partially addressed limitations and societal impacts. To improve they should provide more robust discussions on mitigating limitations especially in scenarios with unknown light positions and complex background scenes. Further incorporating strategies to handle interreflections would enhance the models applicability.", "z2cG3k8xa3C": " Peer Review: Summary: The paper investigates the Gaussiansmoothed optimal transport framework assessing its performance in approximating the standard Wasserstein distance for discrete measures. By establishing bounds on approximation properties in the small noise regime the paper introduces precise results showcasing the exponential efficiency of this smoothing technique when measures possess a unique optimal transport plan. Additionally the paper delves into the existence of a phase transition and explores the behavior of the Gaussiansmoothed distance for measures without a unique optimal plan. Strengths: 1. Novel Approach: The paper introduces a novel perspective on the Gaussiansmoothed optimal transport and provides rigorous theoretical foundations supporting its effectiveness in approximating the Wasserstein distance for discrete measures. 2. Theoretical Contributions: The results elucidate the behavior of the smoothing technique under various scenarios offering insights into the impact of unique optimal transport plans on the approximation quality. 3. Mathematical Rigor: The paper demonstrates a high level of mathematical rigor in theorem formulation and proofs enhancing the credibility of the findings. 4. Elegance in Definitions: The definitions of strong cyclical monotonicity and robustness of optimality are elegantly formulated contributing to the clarity of the proposed concepts. Weaknesses: 1. Accessibility: The paper potentially assumes a high level of familiarity with optimal transport theory which may limit its accessibility to researchers not wellversed in the subject. 2. Scope Limitation: The focus solely on discrete measures restricts the generalizability of the findings to more diverse scenarios where measures may not be finitely supported. 3. Lack of Empirical Validation: While the numerical example provides some insight a more extensive empirical validation across varying scenarios could enhance the practical applicability of the proposed framework. Questions and Suggestions for Authors: 1. Could the paper benefit from including a brief overview or primer on optimal transport theory for readers who may not be as familiar with the subject matter 2. How do the findings of this study impact practical applications of optimal transport in machine learning or other domains beyond theoretical considerations 3. Are there avenues for further research exploring the extension of these results to continuous measures or mixed discretecontinuous distributions Limitations: The paper adequately addresses the limitations of its work by acknowledging the focus on discrete measures and the potential lack of generalizability to continuous scenarios. To address this limitation the authors could consider discussing implications for extending the findings to mixed discretecontinuous distributions or providing insights into how the results may translate to realworld applications beyond the theoretical domain.", "vsShetzoRG9": " Summary: The paper introduces INSNET a novel insertionbased text generator that focuses on efficient training and flexible decoding. It addresses the challenges faced by traditional insertionbased models by proposing a unique insertionoriented relative positional encoding and a flexible parallelization algorithm. The experimental results demonstrate the superiority of INSNET in terms of training speed inference efficiency and generation quality compared to existing insertionbased models.  Strengths: 1. Novel Contributions: The paper introduces innovative solutions to improve the efficiency of insertionbased text generation models such as insertionoriented relative positional encoding and adaptive parallelization. 2. Experimental Validation: The paper provides comprehensive experimental results on various datasets showcasing the superiority of INSNET in terms of training efficiency and generation quality. 3. Clear Communication: The paper effectively communicates the challenges proposed solutions and experimental results making it accessible to readers interested in text generation models.  Weaknesses: 1. Complexity: The technical details presented in the paper might be challenging for readers unfamiliar with text generation models which could limit its accessibility. 2. Limited Comparison: The paper lacks a thorough comparison with a broader range of stateoftheart models in the text generation domain which could provide more insights into the performance of INSNET.  Questions  Suggestions: 1. Evaluation Metrics: Could the authors elaborate on the selection of evaluation metrics used in the experiments Are there any specific reasons behind choosing certain metrics over others 2. Scalability: How does the proposed model scale with larger datasets or more complex text generation tasks Have scalability issues been addressed in the experiments 3. Generalization: Can the authors discuss how well INSNET generalizes to different types of text generation tasks beyond the ones evaluated in the paper  Limitations: The authors have effectively addressed the limitations and potential negative societal impacts of their work. However they could further enhance the discussion by considering the implications of the proposed models efficiency and flexibility on the broader text generation community ensuring responsible deployment and usage. Overall the paper presents a promising approach to insertionbased text generation with notable contributions and solid experimental validations. Further improvements in terms of clarity comparative analysis and generalizability could enhance the impact and practicality of the proposed model.", "kb33f8J83c": " Summary: The paper introduces a method named FreeForm CLIP (FFCLIP) that aims to facilitate freeform text prompts for image attribute manipulation by establishing an automatic latent mapping between the visual latent space of StyleGAN and the text embedding space of CLIP. The proposed method consists of semantic modulation blocks for aligning semantics and injecting text prompts into the latent space allowing for one model to handle various text prompts. Experimental results on human portraits cars and churches demonstrate that FFCLIP produces visually realistic and semantically accurate images.  Strengths: 1. Innovative Approach: The paper addresses a critical challenge in image manipulation by proposing an automatic latent mapping method for handling freeform text prompts which is a novel and important contribution. 2. Technical Rigor: The method is welldescribed with clear illustrations and detailed explanations of each step. The proposed semantic modulation blocks for aligning and injecting text semantics are technically sound and wellsupported with comparative analysis. 3. Experimental Evaluation: The paper presents a comprehensive experimental evaluation on multiple datasets showcasing the effectiveness of FFCLIP in producing visually realistic and semantically accurate images. The qualitative and quantitative results demonstrate the superiority of the proposed method over existing approaches.  Weaknesses: 1. Limitation Addressing: The authors did not fully address the disentanglement of the StyleGAN latent code and the potential societal impacts of the manipulation work. These limitations should be explicitly acknowledged and discussed to provide a more thorough analysis of the implications of the research. 2. Experimental Verification: While the paper presents impressive results it would be beneficial to include additional experiments to further validate the generalizability and robustness of FFCLIP especially when encountering text prompts with complex semantic meanings or in scenarios where the training and testing text prompts are dissimilar.  Questions and Suggestions: 1. Model Generalization: How does FFCLIP perform when presented with text prompts that include more complex or abstract semantic meanings beyond those used during training Have you explored the models performance in handling diverse and novel text prompts 2. Disentanglement: Could the authors elaborate on the limitations of not completely disentangling the StyleGAN latent code W and discuss potential strategies to address this issue in future research 3. Bias Mitigation: Given the known biases in CLIP embeddings have you considered incorporating bias mitigation techniques to ensure that the manipulated results are free from biases especially those related to humanlike biases inherent in the text embeddings  Limitations Addressed: While the authors have provided a detailed explanation of their method improved disentanglement of the latent code and addressed potential biases arising from CLIP embeddings could further enhance the methods reliability and societal impact. Its recommended to include these aspects in the discussion to present a more comprehensive assessment of the research work.", "qcRgqCXv1o2": " Review of the Paper 1. Summary: The paper presents a novel method for improving the robustness of graph classifiers against structural attacks by developing a robustness certificate based on orthogonal GromovWasserstein distance. The key contributions include a convex relaxation of resistance distance a convex lower bound for the discrepancy measure and effective algorithms for both attack and certification. Experimental results demonstrate the effectiveness of the proposed method on graph classification tasks. 2. Strengths:  Innovation: The paper addresses an important problem of improving the robustness of graph classifiers through a novel approach involving GromovWasserstein distance and convex optimizations.  Theoretical Foundation: The method is theoretically grounded leveraging concepts such as resistance distance Fenchel biconjugation and matching loss to develop a robustness certificate.  Experimental Validation: The experimental results provide strong evidence of the effectiveness of the proposed method showcasing improvements in classification accuracy and robustness. 3. Weaknesses:  Complexity: The paper delves into sophisticated mathematical concepts and optimizations potentially making it challenging for a broader audience to grasp the intricacies of the proposed method.  Societal Impact: The paper does not directly address the societal impact of the research such as potential uses or implications of the improved graph classification algorithms. 4. Questions and Suggestions for Authors: a) Clarification: Provide more intuitive examples or visualizations to explain the complex mathematical concepts for better understanding. b) Computational Complexity: Discuss the computational complexity of the proposed algorithm and potential scalability issues especially in larger datasets. c) Discuss Societal Impact: Include a section discussing the societal implications of the improved graph classification algorithms potential applications and ethical considerations. 5. Limitations: The paper lacks a dedicated discussion on the limitations of the study and potential negative societal impacts of the improved graph classifiers. It is essential to address these aspects to provide a wellrounded view of the research. Overall the paper presents a sophisticated and novel approach to enhancing graph classifier robustness supported by theoretical foundations and promising experimental results. Addressing the suggested points can further strengthen the papers impact and clarity.", "zkk_7sV6gm8": " Review of \"Learnable Impulse Control Reinforcement Algorithm (LICRA)\" 1. Summary: The paper introduces a novel Reinforcement Learning (RL) framework named Learnable Impulse Control Reinforcement Algorithm (LICRA) to address scenarios where actions incur costs and learning when to act is crucial. The framework combines RL with impulse control to optimize actions considering the cost. LICRA is demonstrated to outperform benchmark RL methods in Lunar Lander Highway environments and Merton portfolio problem in finance. 2. Strengths:  The paper introduces a novel framework LICRA to efficiently handle RL problems with costincurred actions.  The nested structure of LICRA combining RL and impulse control is welldesigned for selecting actions optimally.  The theoretical results and proofs provided support the convergence and optimality of LICRA.  Empirical results demonstrate the superior performance of LICRA against benchmark methods in different environments. 3. Weaknesses:  The paper could benefit from a more detailed explanation of the theoretical foundations such as impulse control convergence proofs and policy selection.  The methodology section could provide more clarity on the specific algorithms and techniques used to implement LICRA.  The comparison with other RL methods could be more extensive highlighting specific advantages of LICRA over existing approaches. 4. Questions and Suggestions:  Can the authors provide more insights into how LICRA handles state spaces of varying complexity and dimensions  How does the combination of RL and impulse control within LICRA impact the algorithms scalability in larger environments  Have you considered the potential computational complexity of LICRA in realtime applications especially in scenarios with high action frequencies 5. Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. They can improve this by discussing implications related to the energy consumption and computational costs of deploying such algorithms in practical resourceconstrained applications. Overall the paper presents a wellstructured and innovative RL framework in LICRA supported by theoretical foundations and empirical validations. Addressing the mentioned weaknesses and questions would enhance the clarity and robustness of the paper.", "noyKGZYvHH": " Peer Review  Summary: The paper introduces coVariance neural networks (VNN) a novel architecture that operates on sample covariance matrices as graphs drawing similarities with graph neural networks and principal component analysis (PCA). The contribution lies in establishing the stability of VNNs to perturbations in the covariance matrix demonstrating superiority over PCAbased methods. The experiments validate the stability and transferability of VNNs on realworld datasets showcasing advantages over traditional PCAbased statistical approaches.  Strengths: 1. Theoretical Framework: The paper provides a solid theoretical foundation linking the stability of VNNs to perturbations in the sample covariance matrix offering a new perspective distinct from existing studies. 2. Empirical Validation: The experiments conducted on realworld datasets validate the theoretical results effectively demonstrating the stability and transferability of VNNs over different dimensions. 3. Innovation: Introducing VNNs that leverage relationships between PCA and GNNs is innovative addressing issues of stability and performance transferability in data analysis.  Weaknesses: 1. Complexity: The computational complexity of VNNs is briefly discussed but a deeper analysis or comparison with existing models could enhance the papers completeness. 2. Limited Comparison: While the comparison with PCAbased approaches is informative including comparisons with stateoftheart methods or other graphbased models would have added more depth to the evaluation. 3. Clarity: The paper although technically rich could benefit from clearer explanations in certain sections to aid understanding particularly for readers less familiar with the topic.  Questions and Suggestions for Authors: 1. Generalization: Have you tested VNNs on a broader range of datasets beyond neural imaging for a more comprehensive evaluation of its effectiveness 2. Hyperparameter Sensitivity: How do hyperparameters impact the stability and performance of VNNs Have you explored sensitivity analyses or optimization strategies 3. Scalability: Have you considered scalability issues with larger datasets or different types of data structures for VNNs How does performance vary with data size and complexity  Limitations: Authors have effectively addressed the limitations and potential negative societal impact of their work by conducting experiments which highlight the stability and advantages of VNNs over traditional methods. To improve authors could explore more diverse datasets and provide comparative analysis to enhance the applicability of VNNs in various domains.", "u8FDFtoMKp2": "Summary: The paper proposes a novel zeroshot transfer learning approach for heterogeneous graph neural networks (HGNNs) called a Knowledge Transfer Network (KTN). The KTN transfers knowledge from labelabundant node types to zerolabeled node types by leveraging relational information in the heterogeneous graph. The KTN outperforms stateoftheart transfer learning baselines across 18 different transfer learning tasks on heterogeneous graphs showing significant performance improvements. Strengths: 1. Novel Contribution: The paper introduces a new and practical problem definition addressing label imbalance in heterogeneous graphs which provides a unique and valuable perspective. 2. Effectiveness: The proposed KTN method consistently outperforms existing domain adaptation techniques demonstrating its effectiveness in zeroshot transfer learning tasks. 3. Generality: The KTN method is shown to be highly generalizable demonstrating significant performance improvements across 6 different HGNN models. 4. Thorough Analysis: The paper provides a detailed theoretical analysis of the relationships between feature extractors in HGNNs enhancing understanding of the proposed method. Weaknesses: 1. Complexity: The paper delves deeply into theoretical and technical aspects which may make it challenging for general readers to grasp the core concepts. 2. Data and Results Presentation: The paper includes extensive technical details and experimental results potentially overwhelming readers with the amount of information presented. Questions and Suggestions for Authors: 1. Clarification: Can the authors provide more intuitive examples or visual aids to help readers better understand the theoretical relationships and transformations within the HGNN models 2. Comparative Analysis: Could the authors further discuss why KTN outperforms other domain adaptation methods and provide insights into the key factors driving this success 3. Scalability: How does the performance of KTN scale with the size and complexity of heterogeneous graphs Are there any scalability challenges that need to be addressed 4. RealWorld Applications: Can the authors elaborate on potential realworld applications where KTN can have a significant impact beyond the specific experimental scenarios described in the paper Limitations: The authors have adequately addressed the limitations of their work focusing on the restriction of the transfer learning method to node types sharing the same task. They plan to extend their work to cover knowledge transfer between different tasks on heterogeneous graphs. Suggestions for Improvement: 1. Societal Impact Assessment: To further enhance the papers contribution the authors could delve deeper into the potential positive societal impacts of their work and discuss any implications related to ethics bias or fairness in AI applications. 2. Practical Implementation: Providing insights into the practical implementation and deployment of KTN in realworld scenarios could add practical value to the paper.", "lxsL16YeE2w": " Summary: The paper introduces UViM a unified modeling approach for computer vision tasks that requires no taskspecific modifications and offers the same functional form for all tasks. UViM consists of a base model trained to predict raw vision outputs guided by a learned discrete code and a language model trained to generate the guiding code. The approach is demonstrated on panoptic segmentation depth prediction and image colorization tasks showing promising results compared to stateoftheart methods.  Strengths: 1. The paper addresses a significant challenge in computer vision by proposing a unified modeling approach. 2. UViM showcases competitive results across diverse vision tasks demonstrating its versatility. 3. The methodology of using a base model and a language model to model complex structured outputs is innovative and wellargued.  Weaknesses: 1. The paper lacks a clear comparison with other existing unified modeling approaches in computer vision. 2. The discussion on practical implications and realworld applicability could be more elaborative. 3. The paper might benefit from additional analysis on the scalability and computational efficiency aspects of the proposed UViM approach.  Questions and Suggestions: 1. Can the authors elaborate on the computational requirements and training time of the UViM model compared to taskspecific models 2. How does the proposed UViM approach handle noisy or incomplete inputs in realworld scenarios 3. Have the authors explored how UViM performs on additional computer vision tasks beyond the three mentioned in the paper 4. It would be beneficial to provide insights on how UViM handles data augmentation and noise in the training data for different vision tasks.  Limitations: The authors have not thoroughly addressed the limitations and potential societal impact of their work. A suggestion is to include a section dedicated to discussing the potential negative consequences and societal implications of deploying UViM in various settings. This can help in addressing ethical concerns and ensuring responsible deployment of the proposed model.", "p9lC_i9WeFE": " PeerReview of Paper: \"Generalization Bounds of Message Passing Neural Networks in Graph Classification and Regression\"   1) Summary: The paper investigates the generalization capabilities of Message Passing Neural Networks (MPNNs) in graph classification tasks. Specifically it studies the generalization error and provides a theoretical analysis of how MPNNs can generalize well from limited datasets when graphs are sampled from random graph models. The paper introduces a novel perspective by modeling graphs as random samples from continuous models and provides convergence results indicating the convergence of MPNNs as the number of graph nodes increases. Additionally the paper presents a detailed framework for the convergence and generalization analysis of MPNNs.   2) Assessment of Strengths and Weaknesses: Strengths:  The paper provides valuable insights into the generalization properties of MPNNs in graph classification.  The theoretical analysis and convergence results offer a unique perspective on the performance of MPNNs.  Introducing the concept of modeling graphs as continuous random samples provides a novel approach to understanding generalization in MPNNs.  The comparison with existing methods and the empirical evaluations add rigor and relevance to the study. Weaknesses:  The complexity and technicality of the analysis might make it challenging for nonexperts to follow.  The practical implications and realworld applications of the findings could be further emphasized.  The limitations of the model and potential negative societal impacts are not specifically addressed.   3) Questions and Suggestions for Authors: Questions: 1. How do the convergence and generalization results of MPNNs in this work contribute to the broader field of graphbased machine learning 2. How do the assumptions made in the paper affect the applicability of the results to realworld scenarios 3. Have the authors considered the computational efficiency and scalability of the proposed framework for practical implementation Suggestions: 1. To enhance the accessibility of the paper consider providing more intuitive explanations and visual representations of the key concepts and results. 2. Address the implications of the findings on realworld applications and suggest potential areas where MPNNs with these properties could be effectively utilized. 3. Consider discussing the computational complexity and scalability of the proposed models to provide guidance for practical usage.   4) Limitations: While the paper conducts a comprehensive analysis of the generalization properties of MPNNs it falls short in directly acknowledging the limitations and potential negative societal impacts of the work. To address this the authors could highlight the following points:  Limitations: The paper could have provided a clearer discussion on the limitations of the proposed model in terms of scalability computational efficiency and applicability to diverse graph datasets.  Potential Negative Impact: Consider discussing any potential negative societal impacts of the use of MPNNs as studied in this work such as biases in the data ethical considerations in model deployment or unintended consequences of algorithmic decisionmaking. By addressing these aspects the authors can provide enhanced transparency and demonstrate a more holistic understanding of the implications of their work.  In conclusion the paper offers valuable insights into the generalization properties of MPNNs in graph classification tasks. By further addressing the limitations societal impacts and practical implications of the findings the authors can improve the clarity and relevance of their work. ", "zXE8iFOZKw": " Summary: The paper introduces the DynamicsAware Hybrid OfflineandOnline Reinforcement Learning (H2O) framework aiming to bridge the simtoreal gaps faced in reinforcement learning (RL) by combining learning from realworld data and simulationbased learning. H2O introduces a dynamicsaware policy evaluation scheme to adaptively penalize Qfunction learning on simulated stateaction pairs with large dynamics gaps. Extensive simulation and realworld experiments demonstrate the superior performance of H2O compared to other crossdomain RL algorithms.  Strengths: 1. Innovative Contribution: The paper introduces a novel framework (H2O) that addresses the simtoreal gap in RL by combining offline and online learning paradigms. 2. Theoretical Analysis: The paper demonstrates a strong theoretical foundation behind the dynamicsaware policy evaluation with adaptive value regularization. 3. Experimental Evaluation: Extensive simulation and realworld experiments validate the effectiveness of H2O showcasing superior performance compared to existing RL algorithms. 4. Insightful Ablation Study: The ablation study provides valuable insights into the impact of each design component of H2O on performance.  Weaknesses: 1. Complexity: The paper is quite dense and may be challenging for readers unfamiliar with RL concepts to grasp the technical details. 2. Lack of Visualization: Visual aids could enhance understanding especially in describing the experimental setups and results. 3. Limited Context Explanation: Some terms and concepts assume prior knowledge which could hinder understanding for readers new to the subject.  Questions and Suggestions: 1. Explain Parameters: Could the paper provide more clarification on the parameters like \u03b2 \u03bb and their impact on the performance of H2O 2. Comparison Metrics: How does H2O perform on specific metrics like convergence speed sample efficiency and robustness compared to the baselines 3. Generalization: How well does H2O generalize to unseen tasks and environments beyond those tested in the papers experiments 4. RealWorld Deployment: Are there plans to test H2O in more diverse realworld scenarios or industries to assess its broader applicability  Limitations: The authors could provide more explicit explanations of the practical implications societal impact and any potential negative consequences stemming from the deployment of H2O in realworld scenarios. Consider addressing ethical considerations and environmental impacts as well.  Suggestions for Improvement:  Expand on the practical implications and societal impact of H2O considering potential ethical and environmental implications.  Include visual aids diagrams or examples to enhance understanding for readers across different expertise levels.  Simplify complex concepts for a wider audience by breaking down technical jargon and providing more context.", "sPNtVVUq7wi": " Summary: The paper proposes CONDOT a multitask approach for conditional optimal transport estimation using partially input convex neural networks. The study focuses on providing a novel method to estimate a family of optimal transport maps conditioned on context variables enabling the prediction of the effect of genetic or therapeutic perturbations on single cells based on observed perturbations separately. The paper demonstrates the effectiveness of CONDOT in various scenarios involving cell responses to treatments and genetic perturbations.  Strengths: 1. Innovative Approach: The paper introduces a novel framework that leverages context information in optimal transport estimation addressing a significant challenge in predicting cell responses to treatments. 2. Algorithm Design: The use of partially input convex neural networks for parameterizing the optimal transport maps is a novel and effective approach. 3. Experimental Validation: The paper presents comprehensive experimental results showcasing the effectiveness of CONDOT in various scenarios. 4. Thorough Evaluation: The evaluation metrics used such as MMD Wasserstein distances and multidimensional scaling provide a robust assessment of model performance.  Weaknesses: 1. Complexity: The proposed CONDOT framework utilizing PICNNs and various modules may present a high level of complexity potentially hindering its practical implementation. 2. Limited Comparison: While the paper provides comparisons with baseline methods such as ICNN OT and CPA a more extensive comparison with other stateoftheart models could enhance the studys contribution. 3. Clarity in Explanations: Some sections of the paper particularly those discussing technical details and initializations might benefit from clearer explanations for readers less familiar with the concepts.  Questions and Suggestions for Authors: 1. Scalability: How does CONDOT scale with larger datasets and more complex contexts Have you tested its performance on datasets with varying sizes 2. Interpretability: Can you provide more insights into the interpretability of the learned optimal transport maps by CONDOT especially in scenarios involving composite contexts 3. Robustness: Have you conducted sensitivity analysis to evaluate the robustness of CONDOT to noise or data perturbations 4. Realworld Applications: How do you envision the practical application of CONDOT in realworld settings such as personalized medicine or clinical studies  Limitations: The paper adequately addresses the limitations and negative societal impacts of the work through rigorous experimentation and evaluation. To further enhance the study providing a more detailed discussion on the computational burden and potential challenges in scaling CONDOT to practical applications could be beneficial. Ultimately the paper presents a valuable contribution to the field of optimal transport theory and neural networks offering a novel approach that could have significant implications for various applications.", "sc7bBHAmcN": "Summary The paper focuses on studying a class of Graph Neural Networks (GNNs) called Subgraph GNNs which model graphs as collections of subgraphs. The authors address two key questions: the upperbound of the expressive power of subgraph methods and the equivariant message passing layers on these sets of subgraphs. They propose a novel analysis of symmetry groups on nodebased subgraph collections connecting Subgraph GNNs to Invariant Graph Networks (IGNs). Moreover they introduce a novel Subgraph GNN architecture SUN that unifies previous architectures and exhibits superior empirical performance on diverse benchmarks. Strengths 1. The paper provides a novel analysis of the symmetry group that enhances the understanding of nodebased subgraph collections distinguishing it from previous works. 2. The proposal of SUN a Subgraph GNN architecture highlights a significant empirical improvement over existing models on diverse benchmarks. 3. The theoretical derivations along with the rigorous exploration of layer formulation for Subgraph GNNs contribute to the understanding of the design space in these networks. Weaknesses 1. The paper lacks a detailed discussion on the computational complexity and scalability of the proposed model especially when dealing with largescale graphs. 2. Evaluation on only a limited set of benchmarks raises concerns about the generalizability of SUN and its performance in various realworld scenarios. 3. The study could benefit from more extensive experiments including comparisons with a wider range of stateoftheart GNN models. Questions and Suggestions for Authors 1. Could the authors provide insights into the computational complexity and scalability of SUN when applied to largescale graphs 2. Have the authors considered expanding the experimental evaluation to include a broader set of benchmarks to ascertain the generalizability and robustness of SUN 3. How does SUN compare to other popular GNN architectures like Graph Attention Networks or Graph Convolutional Networks in terms of performance scalability and interpretability Limitations The paper does not adequately address the potential negative societal impacts of their work. The authors should consider including a discussion on ethical considerations such as privacy concerns fairness and biases inherent in GNNs and provide suggestions on how to mitigate these issues in future research.", "wzJcEb5Mm4": " Summary: The paper proposes a novel logpolar space convolution (LPSC) layer in convolutional neural networks. LPSC utilizes elliptical convolution kernels that adaptively divide the local receptive field into different regions based on relative directions and logarithmic distances. This approach allows LPSC to greatly increase the singlelayer receptive field exponentially without increasing the number of parameters. The paper demonstrates the effectiveness of LPSC through experiments on various tasks and datasets.  Strengths:  Innovative Contribution: The introduction of LPSC as a novel approach to increasing the receptive field without inflating parameter numbers is a significant innovation.  Theoretical Foundation: The paper provides a sound theoretical explanation of the proposed method and relates it to prior research in the field.  Empirical Evaluation: The experiments performed on different tasks and datasets demonstrate the efficacy of LPSC when applied in network architectures such as AlexNet VGGNet ResNet DeepLabv3 and CENet.  Comparison and Analysis: The paper effectively compares LPSC with other convolution methods like dilated convolution providing comprehensive insights into the advantages of LPSC.  Weaknesses:  Complexity: LPSC introduces three new hyperparameters which may increase complexity. The paper could delve deeper into potential drawbacks and ways to mitigate them.  Memory Overhead: The implementation via logpolar space pooling is noted to incur large memory overhead. The authors could explore strategies to reduce this memory consumption.  Adaptation to 1D3D Data: While extending LPSC to 1D and 3D datasets is mentioned in the appendix more detailed discussion on adapting LPSC to such data types within the main text could enhance the comprehensiveness of the paper.  Questions and Suggestions for Authors: 1. How does the LPSC method handle irregularly sampled data or data with varying distances and angles between data points 2. Could the authors provide more insight into the tradeoffs between Lr L\u03b8 and g in LPSC especially in terms of performance versus complexity 3. Given the observed limitations and drawbacks of LPSC what strategies could be devised to address them in future work 4. How can LPSC be further extended to tackle broader applications beyond imagerelated tasks  Limitations:  The paper could address the potential limitations and negative societal impacts of improving the computational efficiency of convolutional neural networks.  Constructive Suggestions for Improvement:  Address potential societal impacts such as biases in AI applications and ethical considerations of using advanced network architectures.  Offer more practical guidelines on tuning hyperparameters and implementation details to aid researchers wanting to adopt LPSC in their work. Overall the paper presents a novel and promising approach to enhancing the receptive field in convolutional neural networks. With minor enhancements in addressing complexity and memory overhead LPSC could significantly contribute to the advancement of network architectures.", "mowt1WNhTC7": " Summary: The paper focuses on analyzing the remaining mistakes made by top computer vision models on the ImageNet dataset specifically in the multilabel context. It undertakes a manual expert review to categorize and assess the severity of these mistakes aiming to provide insights into the longtail errors which can help improve future evaluations of image classification models.  Strengths:  The paper contributes by providing a comprehensive analysis of the mistakes made by stateoftheart models on ImageNet filling a gap in understanding model performance in the multilabel context.  The manual expert review methodology and the categorization of mistakes based on severity add depth to the evaluation process.  The proposal of ImageNetM as a curated evaluation set of major mistakes intended to offer a new benchmark for evaluating model performance is a valuable contribution to the field.  Weaknesses:  The primary limitation lies in the qualitative nature of the analysis as assessments of mistakes categorizations and severity are subjective and dependent on the panelists expertise and interpretation.  While the paper is detailed and meticulous in its analysis more comparison with existing benchmarking methods could provide a clearer context for the proposed ImageNetM dataset.  The validation of ImageNetM on a broader set of models beyond the ViT3B and Greedy Soups could strengthen the generalizability of the findings.  Questions and Suggestions for Authors: 1. Have you considered analyzing the impact of the identified major mistakes on downstream tasks or applications that utilize these models 2. How do you plan to address potential biases in the manual expert review process especially concerning subjective judgments on mistake severity 3. Could you elaborate on the implications of the findings for the broader computer vision community in terms of benchmarking model performance and future research directions 4. How do you plan to ensure the ongoing relevance and maintenance of the ImageNetM dataset for continued model evaluations over time 5. Could you explore ways to quantify the performance improvements or model generalizations resulting from addressing major mistakes identified by ImageNetM  Limitations: The authors have adequately addressed the limitations of their work especially in terms of subjective judgments and analysis dependencies. However to enhance the robustness of the study they could consider including a more diverse panel of reviewers and further validate the proposed ImageNetM dataset across a wider range of models for broader acceptance and application in the field.", "krV1UM7Uw1": "Peer Review: 1) Summary: The paper introduces two novel robust regression algorithms TRIP and BRHT that aim to resist adaptive adversarial attacks efficiently by integrating prior information with robust regression methods. TRIP uses a simple prior and hard thresholding approach while BRHT employs Bayesian reweighting to improve robustness and reduce estimation error. The theoretical convergence of the proposed algorithms is proven under specific conditions and extensive experiments demonstrate their superior performance compared to benchmark algorithms under different dataset attacks. 2) Strengths and Weaknesses: Strengths:  The paper addresses a significant problem of robust regression under adaptive adversarial attacks proposing novel algorithms for improved resistance.  The theoretical analysis provides a solid foundation with established convergence guarantees under mild conditions.  Extensive experiments showcase the effectiveness of the proposed algorithms demonstrating stateoftheart performance compared to existing methods.  The combination of prior information with robust regression methods is innovative and shows promising results in enhancing robustness and reducing estimation error. Weaknesses:  The discussion on the experimental methodology could benefit from more details such as the specific datasets used performance metrics and statistical significance of results.  The paper lacks a comparative analysis with more recent or widely used methods in the field to provide a comprehensive evaluation.  The complexity of the algorithms and notations used may hinder the readability for some readers suggesting the need for clearer explanations and intuitive examples. 3) Questions and Suggestions:  Clarification on Datasets: Could the authors provide more information on the datasets used in the experiments including their characteristics sizes and sources  Comparative Analysis: How do the proposed algorithms compare to popular methods in robust regression under adversarial attacks such as robust deep learning models or ensemble methods  Sensitivity Analysis: Have the authors conducted sensitivity analysis on key parameters such as the weight of the prior information to assess the robustness and generalizability of the algorithms  Scalability: How do TRIP and BRHT perform in scenarios with larger datasets or higherdimensional feature spaces Have scalability issues been considered and addressed 4) Limitations: The authors have adequately highlighted the limitations of their work and acknowledged the need for further refinement in incorporating more accurate prior knowledge and handling data corruption scenarios where both the response variables and features are corrupted. To enhance the societal impact it would be beneficial to consider the broader implications of the proposed algorithms in realworld applications and ensure their usability and interpretability to a wider audience. This manuscript presents significant contributions to the field of robust regression under adaptive adversarial attacks. Strengthening the experimental methodology comparative analysis and scalability considerations can further solidify the impact and relevance of the proposed algorithms. Overall the paper exhibits strong theoretical foundations innovative methodologies and promising experimental validation deserving of publication with minor revisions.  Note to Author: Consider incorporating additional details on datasets expanding comparative analyses providing sensitivity analysis results and addressing scalability and generalizability aspects to strengthen the manuscript further. Keep the clarity and accessibility of the algorithms and notations for a broader audience.", "tWBMPooTayE": " Summary: The paper introduces FreGAN a novel approach designed to address the overfitting and instability issues that arise when training Generative Adversarial Networks (GANs) with limited data. FreGAN raises the models frequency awareness to focus on generating highfrequency signals thereby improving the quality of image generation in lowdata scenarios. It incorporates Haar wavelet transformation a highfrequency discriminator frequency skip connection and highfrequency alignment to fully exploit the frequency information of limited training data.  Strengths: 1. Innovative Approach: FreGAN introduces a novel perspective by integrating frequency information into GAN training to ameliorate image generation quality under limited data. 2. Comprehensive Experiments: The paper provides extensive experimental results showcasing the superiority of FreGAN across various datasets and resolutions especially in scenarios with extremely limited data. 3. Compatibility and Complementary: FreGAN is shown to be compatible with existing regularization and attention mechanism models enhancing their performance and promoting equilibrium between the generator and discriminator.  Weaknesses: 1. Complexity: The proposed method involves multiple components and operations which could make it challenging to implement and interpret for researchers less familiar with frequencybased techniques. 2. Lack of RealWorld Testing: The paper mainly focuses on synthetic datasets and limited realworld application scenarios thus lacking validation in more diverse reallife data settings. 3. Weak Generalization: The limitation section mentions challenges in generating photorealistic images with datasets of wide content variance signaling a potential limitation in generalization.  Questions and Suggestions: 1. Clarification of Frequency Components: Could the authors provide more insights into how different frequency components (LL LH HL HH) contribute to the overall quality of image generation in FreGAN 2. Scalability Concerns: How does the performance of FreGAN scale with increasingly larger datasets Are there scalability issues due to the complexity of wavelet transformations and frequency awareness modules 3. RealWorld Application: Have the authors tested FreGAN on realworld datasets with limited data access such as medical imaging or industrial applications Realworld validations could strengthen the practical relevance of the proposed method.  Limitations: The paper acknowledges limitations related to the generalization of FreGAN in scenarios with imbalanced or widely varied lowdata datasets. To address this the authors may consider:  Incorporating techniques for handling imbalanced data distributions.  Introducing strategies for adapting FreGAN to diverse content variations within limited datasets. Overall the paper presents a promising approach in FreGAN for improving GAN training with limited data but further validations and simplifications may enhance its applicability and scalability.", "xubxAVbOsw": " Summary: The paper introduces a novel method DiversityPromoting Collaborative Metric Learning (DPCML) to address the challenge of user preferences across multiple categories in recommendation systems. DPCML incorporates a multivector user representation strategy and a diversity control regularization scheme to better accommodate diverse user preferences. The paper provides theoretical analysis on the generalization bound of DPCML and demonstrates its effectiveness through empirical studies on various benchmark datasets.  Strengths: 1. Innovative Contribution: Introducing DPCML to address preference bias in imbalanced item category distributions is a significant advancement. 2. Theoretical Analysis: Providing a detailed theoretical analysis and proving the generalization bound of DPCML enhances the credibility of the proposed method. 3. Empirical Evaluation: Conducting comprehensive experiments on multiple benchmarks to demonstrate the superiority of DPCML over existing methods adds substantial value to the paper. 4. Diversity Control Regularization Scheme: The proposed diversity control regularization scheme appears effective in improving recommendation diversity.  Weaknesses: 1. Clarity in Presentation: The paper is comprehensive but dense in technical details which may be challenging for readers without a strong background in collaborative metric learning and recommendation systems. 2. Limitation Alignment: While the paper mentions limitations in applying only to implicit feedback a more explicit discussion on potential negative societal impacts or ethical considerations would enhance the papers scope.  Questions and Suggestions for Authors: 1. Model Interpretability: How can the authors ensure the interpretability of the diverse user representations introduced in DPCML 2. Scalability: Have the authors conducted experiments on largescale datasets to validate the scalability of DPCML 3. User Interaction Studies: Are there any plans to conduct user studies to evaluate the preference accuracy and satisfaction of recommendations powered by DPCML 4. Comparison with StateoftheArt: Are there additional stateoftheart methods specifically newer collaborative metric learning models that the authors plan to compare DPCML against for a more comprehensive evaluation  Limitations: The authors could improve the paper by addressing the limitations not only in terms of technical applications but also concerning potential negative societal impacts. A constructive suggestion would be to include a dedicated section discussing ethical considerations and potential societal implications of DPCML to enhance the papers completeness.", "zYc5FSxL6ar": " Review of \"SynergyOriented LeARning: A LowRank Framework for Modular Reinforcement Learning on MultiJoint Robots\"  Summary: The paper introduces the SynergyOriented LeARning (SOLAR) framework which incorporates muscle synergy principles into modular reinforcement learning to control robots with a large number of degrees of freedom (DoF). The key aspects include discovering synergy structure using unsupervised learning and designing a synergyaware policy architecture. SOLAR is evaluated on various robot morphologies and outperforms stateoftheart algorithms in terms of efficiency and generalizability particularly on robots with high DoF like Humanoid. The experiments demonstrate the effectiveness of SOLAR in multitask zeroshot learning synergy cluster visualization and singletask settings.  Strengths: 1. Innovation and Contribution: The paper introduces a novel approach SOLAR which incorporates muscle synergies to simplify control complexity in modular reinforcement learning. This approach shows significant performance improvements over existing stateoftheart methods. 2. Theoretical Grounding: The integration of muscle synergy concepts from neuroscience into robotics is a novel and theoretically sound approach. It provides a strong foundation for the proposed methodology. 3. Thorough Experiments: The paper conducts comprehensive experiments on various benchmarks providing thorough evaluations of SOLAR in multitask zeroshot learning synergy cluster visualization and singletask settings. The results clearly demonstrate the superiority of SOLAR. 4. Clear Structure and Explanation: The paper is wellstructured and provides clear explanations of the methodology experiments and results. The detailed descriptions of the synergy discovery method and policy architecture enhance the understanding of the proposed framework.  Weaknesses: 1. Lack of Comparison Metrics: While the paper compares SOLAR with existing methods and ablations a more thorough analysis of key metrics such as computational efficiency scalability or robustness could provide a deeper understanding of the proposed framework. 2. Limited Discussion on Negative Impacts: The paper could benefit from a more explicit discussion on potential negative societal impacts or ethical considerations arising from the deployment of SOLAR in realworld scenarios especially in contexts where robotics interacts with humans. 3. Clarity on Learning Dynamics: The paper could provide more insights into the learning dynamics of SOLAR particularly how the synergy hierarchy evolves during training and influences policy learning over time.  Questions and Suggestions for Authors: 1. Could you delve deeper into how the synergy hierarchy evolves during training How does the stability of synergy clusters impact learning efficiency 2. How does SOLAR handle transferring learned policies across different robots with distinct morphologies Is there a mechanism for adapting to new robot structures without retraining from scratch 3. Can you provide insights into potential future applications of SOLAR beyond robotics such as healthcare or assistive technologies  Limitations: The authors have not explicitly addressed negative societal impacts of the proposed method. To enhance the ethical considerations of SOLAR it would be beneficial to conduct an impact assessment and address any potential risks or ethical concerns.  Overall Impression: The paper \"SynergyOriented LeARning: A LowRank Framework for Modular Reinforcement Learning on MultiJoint Robots\" presents a novel and impactful framework for improving the efficiency and generalizability of modular reinforcement learning in robotics. The clear articulation of the methodology extensive experimental evaluations and innovative integration of muscle synergy principles make SOLAR a promising advancement in the field. Strengthening the discussion on societal impacts providing deeper insights into learning dynamics and incorporating additional comparison metrics could further enhance the papers impact and contribution to the research community.", "w6fj2r62r_H": " Peer Review of \"Torsional Diffusion: A Novel Approach for Molecular Conformer Generation\"  Summary: The paper introduces \"Torsional Diffusion\" a novel framework for generating molecular conformers in computational chemistry. The method operates on the space of torsion angles via a diffusion process on the hypertorus and an extrinsictointrinsic score model. The paper demonstrates that on druglike molecules torsional diffusion outperforms existing machine learning and cheminformatics methods in terms of RMSD chemical properties and computational speed. Additionally the model provides exact likelihoods enabling the development of a generalizable Boltzmann generator.  Strengths: 1. Innovative Approach: The paper introduces a novel approach torsional diffusion which leverages insights into torsional degrees of freedom to enhance conformer generation performance. 2. Superior Performance: Torsional diffusion outperforms existing methods in terms of accuracy chemical properties and computational efficiency. 3. Exact Likelihoods: Providing exact likelihoods enables training with groundtruth energy functions enhancing model robustness. 4. Generalizable Boltzmann Generator: The proposed framework can be used to build systemagnostic Boltzmann generators a significant advancement in the field.  Weaknesses: 1. Complexity: The paper presents a complex and specialized framework which may be challenging for readers unfamiliar with the field to understand. 2. Limited Comparison: While the comparison with existing methods is thorough additional comparisons with a wider range of stateoftheart models could further validate the proposed approach. 3. Assumptions: The rigid local structure assumption for training may limit the models applicability to more diverse molecular systems.  Questions and Suggestions for Authors: 1. Clarity: Provide more intuitive explanations or visual aids to help readers understand the intricate concept of torsional diffusion. This could enhance the accessibility of the paper. 2. Validation: Can the authors elaborate on the robustness and generalizability of the proposed method beyond the presented datasets 3. Scalability: How does the proposed framework scale with respect to the size or complexity of molecules Any insights on its performance with larger or more complex molecular systems 4. Future Directions: Are there plans to extend the method to address the limitations of the rigid local structure assumption or explore applications beyond conformer generation  Limitations: The authors have not fully addressed the potential negative societal impact of their work. It would be beneficial to include a discussion on the implications of this advanced method especially concerning any unintended consequences in drug discovery or other chemical applications. Overall this paper introduces a significant advancement in molecular conformer generation with notable improvements in accuracy and efficiency. Ensuring clarity further validation scalability considerations and addressing societal impact can enhance the impact and understanding of the proposed method.", "yQDC5ZcqX6l": " Review of the Paper 1. Summary and Contributions: The paper proposes a novel approach for biclustering based on optimal transport theory focusing on sparse dyadic data such as userrating documentterm and genedisorder pairs modeled using bipartite graphs. Leveraging optimal transport the authors introduce a new biclustering model that outperforms existing methods in terms of efficiency and accuracy. Extensive experimentation demonstrates the effectiveness of the proposed approach across various datasets. 2. Strengths:  Innovative Approach: Introducing biclustering through optimal transport for dyadic data sets is a novel contribution that addresses the limitations of existing methods.  Comprehensive Experiments: The paper conducts thorough experimentation to validate the proposed approach comparing it favorably with other optimal transport biclustering algorithms.  Efficiency: The new biclustering models are shown to be more computationally and memoryefficient providing a significant advantage over existing approaches.  Clear Methodology: The methodology is welldefined with detailed explanations and theoretical underpinnings provided for the proposed biclustering model. 3. Weaknesses:  Theoretical Complexity: The paper delves into complex mathematical formulations potentially hindering the accessibility of the proposed method for readers not wellversed in optimal transport theory.  Limited Practical Use Cases: While the experiments showcase the performance on documentterm matrices additional realworld use cases and applications could strengthen the relevance of the proposed approach.  Comparison Scope: While comparisons with existing optimal transport biclustering methods are made a broader comparison with traditional biclustering algorithms could provide a more comprehensive evaluation. 4. Questions and Suggestions:  Clarifications: Provide additional clarity on the practical implementation and applicability of the proposed framework for realworld scenarios.  Scalability: Investigate how the proposed method scales with even larger datasets and explore potential parallelization strategies for enhanced efficiency.  Robustness: Discuss the sensitivity of the proposed biclustering model to noise and outliers in the data. 5. Limitations: The paper adequately addresses the limitations of the proposed approach particularly in terms of theoretical complexity and practical applicability. To further enhance the paper the authors could provide more concrete examples of realworld scenarios where the proposed biclustering model could be effectively applied. Overall the paper presents a promising approach for biclustering sparse dyadic data through optimal transport with notable strengths in efficiency and performance. By addressing the suggested improvements the authors can enrich the impact and practical utility of their work.", "oQIJsMlyaW_": "Review of Paper  Summary of the Paper: The paper discusses the limitations of current pruning heuristics in deep neural networks and proposes a novel integrated gradient pruning criterion called SInGE inspired by attribution methods. The authors intertwine this new criterion with a finetuning flowchart to preserve DNN accuracy while removing parameters. Through extensive validation on various datasets and architectures the proposed SInGE method outperformed existing stateoftheart DNN pruning methods. Strengths: 1. Novel Contribution: The integration of attribution methods into the pruning process is innovative and potentially impactful. 2. Extensive Validation: The paper conducts comprehensive experiments on various datasets and architectures demonstrating the effectiveness of the proposed SInGE method. 3. Detailed Methodology: The paper provides clear descriptions of the proposed criterion methodology and experiments facilitating understanding and reproducibility. Weaknesses: 1. Evaluation Metrics: It would be beneficial to include additional evaluation metrics such as computational efficiency or training time to provide a more holistic view of the approachs performance. 2. Theoretical Analysis: While the empirical validation is thorough a deeper theoretical analysis or comparison with existing theoretical frameworks could enhance the papers academic rigor. Questions for Authors: 1. How does the proposed SInGE method compare in terms of computational efficiency and training time with existing pruning techniques 2. Have you considered exploring the interpretability of the pruning decisions made by SInGE and the impact on model explainability Suggestions for Authors: 1. Consider including a costbenefit analysis to elucidate the tradeoffs between accuracy preservation and computational complexity. 2. It would be valuable to discuss the potential implications of the SInGE method in realworld applications beyond the experimental validation. Limitations: The authors have not extensively addressed the potential negative societal impact of their work. They should consider discussing ethical considerations and societal implications especially in deployment scenarios involving critical decisionmaking systems. Overall the paper presents a significant advancement in DNN pruning methods. Addressing the identified weaknesses and questions can further strengthen the papers contributions and impact.", "uCBx_6Hc7cu": " Summary: The paper presents a variational inference formulation of autoassociative memories integrating perceptual inference and memory retrieval. It connects Predictive Coding (PC) with Hopfield Networks for associative memory tasks and offers four new autoassociative memory models. The models are evaluated on CIFAR10 and CLEVR datasets comparing them with existing models such as Neural Turing Machines and EndtoEnd Memory Networks.  Strengths: 1. Innovative Formulation: Integration of variational inference with memory retrieval is a novel approach. 2. Theoretical Connection: Drawing links between PC and modern continuous Hopfield networks adds value. 3. Robust Evaluations: Comparative evaluations on CIFAR10 and CLEVR datasets are thorough. 4. Experimental Results: Results demonstrate the effectiveness of proposed models in memory retrieval tasks.  Weaknesses: 1. Complexity: The papers technical depth may limit accessibility for readers without a strong background in neural networks and variational inference. 2. Evaluation Scope: The evaluation is limited to image datasets potentially restricting the generalizability of the proposed models. 3. Performance Comparison: Further comparison with stateoftheart memory models could strengthen the study.  Questions and Suggestions for Authors: 1. Clarifications on Memory Capacity: Could the authors provide insights into the memory capacity and convergence properties of the proposed models 2. Extension to Other Domains: Have the authors considered how these models could apply beyond image datasets potentially in sequential or structured data tasks 3. Optimization Techniques: How do the authors plan to refine the PCbased inference methods considering precision weighting or learning optimization approaches 4. Comparative Analysis: Might additional comparisons with other memoryenhanced models such as Differentiable Neural Computers be beneficial for a complete evaluation  Limitations: The authors have not adequately addressed the memory capacity and convergence properties of their models. It is recommended to conduct further investigations and provide insights in future work to strengthen the research. This review aims to provide constructive feedback to enhance the papers impact and academic rigor.", "peFP9Pl-6-_": " Summary: The paper proposes a new variational framework with an orthogonalspace gradient flow (OGradient) for sampling on manifolds defined by general equality constraints. The method does not require prior knowledge about the manifold and is proven to converge to the target distribution. OGradient is implemented through Langevin dynamics and Stein variational gradient descent and is shown to be effective in various experiments including Bayesian deep neural networks.  Strengths: 1. Innovative Contribution: The paper introduces a novel method OGradient which is wellmotivated and addresses an important problem in constrained sampling. 2. Theoretical Rigor: The paper provides a comprehensive theoretical analysis establishing convergence guarantees and a new Stein characterization of conditional measure. 3. Experimental Validation: The method is demonstrated through a range of experiments across different constrained ML problems showcasing its effectiveness. 4. Comparative Analysis: The paper compares the proposed method with existing approaches highlighting its advantages in terms of convergence and efficiency.  Weaknesses: 1. Complexity: The theoretical derivations and concepts introduced may be challenging for readers unfamiliar with advanced mathematical frameworks. 2. Limited RealWorld Validation: While the proposed method is evaluated on diverse tasks a more extensive validation on a broader range of realworld datasets could further validate its practical utility. 3. Hyperparameter Sensitivity: The paper mentions the need to tune hyperparameters for achieving samples close to the manifold which could be a limitation in practical implementations.  Questions and Suggestions: 1. It would be beneficial to clarify the computational complexity of the proposed OGradient method compared to existing approaches. 2. Can the authors provide more insights into the sensitivity of the proposed method to hyperparameters and suggest strategies for efficient tuning 3. Could the paper discuss potential extensions or modifications of OGradient to incorporate inequality constraints into the framework 4. How does the method perform in scenarios with highdimensional manifolds or more complex constraints  Limitations: The paper adequately addresses the limitations by acknowledging that the obtained samples may not exactly lie on the manifold and suggests including a projection step for improvement. The authors note the restriction to equality constraints and suggest exploring the incorporation of inequality constraints in future research.", "mMuVRbsvPyw": "1) Summary The paper introduces GMMSeg a novel family of segmentation models that combine both generative and discriminative approaches to address the limitations of current discriminative solutions. GMMSeg adopts a hybrid training strategy wherein it uses online optimization via EM for generative GMM classifier and endtoend discriminative representation learning for the feature extractor. The experiments conducted demonstrate that GMMSeg outperforms discriminative counterparts on multiple datasets and network architectures for both closedset segmentation and anomaly segmentation tasks. 2) Strengths and Weaknesses Strengths:  Novel approach that combines generative GMM classifiers and discriminative representation learning.  Hybrid training strategy that effectively utilizes both generative and discriminative aspects.  Demonstrated superior performance compared to discriminative counterparts across various datasets and tasks.  Ability to handle anomaly segmentation without external datasets or postprocessing techniques. Weaknesses:  Lack of comparative analysis with more recent stateoftheart segmentation methods.  Limited details on the specifics of the hyperparameters and training protocols.  The manuscript is dense with technical information making it challenging for readers not wellversed in the field to understand. 3) Questions and Suggestions for Authors  Could the authors provide more insights into the hyperparameters and training protocols used in the experiments as these details are crucial for reproducibility and understanding the practical implementation of GMMSeg  It would be beneficial to incorporate a broader comparison with other recent segmentation methods to showcase GMMSegs performance comprehensively.  Consider adding a paragraph discussing the interpretability aspect of GMMSeg and how the hybrid approach affects model explainability. 4) Limitations While the paper extensively covers the strengths and impacts of GMMSeg it falls short in discussing the potential negative societal impacts or ethical considerations of the algorithm. To enhance the completeness of the work the authors could provide a brief discussion on the societal implications of deploying GMMSeg in realworld applications such as potential biases or unintended consequences. This critical reflection can offer valuable insights for future research and application of the proposed method.", "px87A_nzK-T": " Summary: The paper introduces a novel approach termed \"kernel memory networks\" that leverages kernelbased models to design optimal memory networks capable of storing patterns with maximal noise robustness. The study connects memory modeling with statistical learning encompassing classical and modern memory models like Hopfield networks and sparse distributed memory. The paper also discusses training neural networks to perform classification or interpolation tasks with minimal weight norms showcasing the benefits of kernelbased approaches.  Strengths: 1. Innovative Approach: The paper introduces a novel method kernel memory networks which connects memory modeling with kernel methods offering a fresh perspective on memory storage and retrieval. 2. Theoretical Foundation: The study provides a strong theoretical foundation by deriving optimal models for both heteroassociative and autoassociative memory networks demonstrating clear mathematical structures and connections with existing models. 3. Capacity Analysis: The analysis of storage capacity and noise robustness for both binary and continuous patterns provides valuable insights into the memory models capabilities and limitations.  Weaknesses: 1. Complexity: The paper discusses a wide range of concepts like SVMs kernels and network structures which might be challenging for readers not familiar with these domains. 2. Limited Discussion on Implementation: While the paper is theoretical in nature it could benefit from discussing practical implementation aspects such as computational complexity or realworld applicability. 3. Biological Interpretation Depth: The discussion on the biological interpretations although interesting is brief. To enhance its value a more indepth analysis connecting these concepts to neurobiological mechanisms could be beneficial.  Questions and Suggestions: 1. Clarity on Model Comparisons: Can the authors provide more direct comparisons between the proposed kernel memory networks and existing memory models in terms of performance capacity and computational efficiency 2. Practical Relevance: How transferable are the findings to realworld learning tasks or applications Are there plans to apply the kernel memory networks to practical machine learning problems 3. Biological Insights Expansion: Could the authors elaborate further on how the proposed models could be validated or tested against experimental data or neurobiological observations to enhance the biological relevance of their work  Limitations and Suggestions:  Addressing Limitations: The paper adequately addresses the limitations and societal impact of the work with funding disclosed. To improve authors could explicitly outline ethical considerations or potential risks associated with advanced neural network training methods. Overall the paper presents a novel and theoretically grounded approach to memory modeling with the potential to advance understanding in both machine learning and neurobiology. Improvements on the clarity of complexity implementation discussion and depth of biological interpretations could enhance its impact and relevance.", "szt95rn-ql": " Peer Review 1. Summary: The paper introduces Bursting CorticoCortical Networks (BurstCCN) a novel model that addresses the credit assignment problem in deep learning by integrating cortical network properties. BurstCCN effectively backpropagates error signals through multiple layers with a singlephase learning process. The model is capable of approximating backpropderived gradients and performs well on image classification tasks like MNIST and CIFAR10. 2. Strengths:  The paper is wellorganized providing a comprehensive overview of the model and its contributions.  The authors demonstrate a thorough understanding of cortical features and combine them effectively in BurstCCN for efficient learning.  The models successful performance on image classification tasks showcases its practical applicability. 3. Weaknesses:  Lack of detailed comparisons with existing stateoftheart models or benchmark datasets may limit the contextual understanding of BurstCCNs performance.  The paper could benefit from a more detailed discussion on the computational efficiency or scalability of BurstCCN compared to existing models.  Clearer visual representations of the model architecture and key processes could enhance the readability and understanding of readers. 4. Questions and Suggestions:  Could the authors elaborate on the computational efficiency of BurstCCN compared to traditional backpropagation algorithms in deep learning tasks with large datasets  How does BurstCCN address potential overfitting issues commonly encountered in deep learning models especially with the increasing complexity of tasks  It would be beneficial if the authors clarify the robustness and generalizability of BurstCCN across different neural network architectures and types of tasks. 5. Limitations: While the paper discusses biological plausibility it could further address the negative societal impact of developing highly efficient learning models that mimic brain processes potentially raising ethical concerns. Authors could provide insights into ensuring the responsible development and deployment of such models. Overall the paper presents a compelling model to address complex learning challenges and highlights the intersection of neurobiology and machine learning. Addressing the suggested points could further enhance the clarity significance and impact of this work.  This peer review provides a thorough evaluation of the papers content strengths weaknesses key questions and suggestions for improvement while also highlighting the importance of addressing ethical considerations in research.", "znNmsN_O7Sh": "Summary: The paper proposes the Object Scene Representation Transformer (OSRT) a model for unsupervised learning of 3Dconsistent object decompositions in complex scenes. OSRT introduces the Slot Mixer decoder allowing for efficient rendering of objectcentric representations. The paper demonstrates that OSRT outperforms prior methods in object decomposition and efficiency making it a valuable tool for scene representation learning. Strengths: 1. Contributions: The paper presents a novel model OSRT that efficiently learns 3D object decompositions in complex scenes. 2. Novelty: Introducing the Slot Mixer decoder is a significant advancement enabling fast rendering of objectcentric representations. 3. Experimental Evaluation: Thorough evaluation on various datasets demonstrates the superiority of OSRT in reconstruction quality and scene decomposition compared to existing methods. 4. Efficiency: OSRTs efficiency in rendering novel views is commendable making it orders of magnitude faster than previous models. Weaknesses: 1. Lack of Diverse Evaluation Metrics: While the paper covers reconstruction quality and scene decomposition additional evaluation metrics related to generalization scalability or robustness could provide a more comprehensive assessment. 2. Limitation Awareness: The limitations section could be further expanded to discuss potential pitfalls edge cases or failure modes of OSRT to enhance the transparency of the research. Questions and Suggestions: 1. How does OSRT handle complex scenes with dynamic objects or changing backgrounds Have you considered incorporating temporal information for scene understanding 2. Can OSRT generalize well to unseen object classes or scenes not encountered during training Providing insights on outofdistribution generalization would strengthen the paper. 3. How does OSRT perform in noisy or cluttered scenes where object segmentation might be challenging Have you explored techniques to enhance robustness in such scenarios Limitations: The authors briefly mentioned limitations regarding object segmentation quality and possible failure modes. It would be beneficial to provide more detailed explanations and propose potential solutions or strategies to mitigate these issues for a more robust implementation.", "zD65Zdh6ZhI": " Summary: The paper addresses the computational complexity of computing minimum and minimal \u03b4sufficient reasons over decision trees in the context of explainable artificial intelligence (XAI). It shows that finding such explanations is NPhard for arbitrary decision trees but becomes tractable under specific structural restrictions like bounded split numbers and monotonicity. The authors also demonstrate the practical feasibility of solving these problems using SAT solvers with experimental validation.  Strengths: 1. Original Contribution: The paper addresses an important and timely issue in the field of XAI. 2. Theoretical Depth: The paper provides a rigorous analysis of the computational complexity of \u03b4sufficient explanations. 3. Algorithms and Tractability Results: The identification of structural restrictions enabling tractable algorithms is a significant contribution. 4. Experimental Validation: The demonstration of feasibility using SAT solvers on practical datasets adds practical relevance.  Weaknesses: 1. Limited Practical Evaluation: While the theoretical results are sound a more extensive experimental validation on diverse datasets could enhance the practical impact. 2. Complexity: The paper involves advanced theoretical concepts which may limit its accessibility to a broader audience. 3. Lack of Comparative Analysis: Comparison with existing methods in terms of performance and efficiency could provide further insights.  Questions and Suggestions: 1. Can you provide more insights on the implications of your findings for realworld AI systems 2. How do the identified structural restrictions impact the interpretability of decision trees in practice 3. Have you considered the tradeoffs between computational complexity and the quality of explanations in your work 4. Could additional experiments be conducted to validate the proposed approach across a wider range of scenarios and datasets  Limitations: The authors have not thoroughly addressed the potential negative societal impacts of their work. It is recommended to consider and discuss the ethical implications and potential biases that may arise from the use of algorithmic explanations in decisionmaking processes.  Suggestions for Improvement: 1. Include Discussion on Ethical Implications: Consider discussing the ethical implications and societal impacts arising from the use of algorithmic explanations. 2. Expand the Practical Evaluation: Conduct additional experiments on diverse datasets to validate the effectiveness of the proposed approach further. 3. Enhance Accessibility: Simplify the presentation of complex theoretical concepts to enhance the accessibility of the paper to a wider audience. Overall the paper makes a valuable contribution to the field of XAI presenting significant theoretical and practical insights into the computation of \u03b4sufficient explanations over decision trees.", "xwBdjfKt7_W": " PeerReview:  Summary: The paper investigates the vulnerability of Spiking Neural Networks (SNNs) to adversarial attacks and proposes a regularized adversarial training scheme (RAT) to enhance the robustness of SNNs. The work includes theoretical analysis on perturbation in spike representation proposes constraints on the spiking Lipschitz constant and introduces mixed adversarial training. Experimental validation on image recognition tasks demonstrates the effectiveness of the proposed approach in defending against powerful adversarial attacks.  Strengths: 1. Theoretical Analysis: The paper provides a comprehensive theoretical analysis on the vulnerability of SNNs to adversarial attacks highlighting the importance of constraints on the spike representation. 2. Regularized Adversarial Training: The introduction of the RAT scheme combining weight regularization and adversarial training showcases a novel approach to enhance the robustness of SNNs against attacks. 3. Experimental Validation: The experimental results on image classification tasks demonstrate significant improvements in the robustness of trained models against various adversarial attacks.  Weaknesses: 1. Complexity: The paper delves into theoretical aspects and the proposed regularization scheme making it challenging for readers not deeply familiar with the subject matter to easily comprehend the work. 2. Comparison with StateoftheArt: While the experimental results showcase improved robustness a more detailed comparison with stateoftheart methods could enhance the papers contribution and positioning.  Questions and Suggestions: 1. Evaluation Metrics: Could you provide more insights into the evaluation metrics used in the experiments to assess the models performance under adversarial attacks 2. Generalization: How does the proposed RAT scheme generalize to different datasets or architectures beyond CIFAR10 and VGG11WideResNet16 Extending the analysis to diverse scenarios could bolster the papers applicability. 3. Scalability: Considering the computational overhead involved in the proposed regularization and adversarial training are there strategies to optimize these techniques for scalability in realworld applications  Limitations: The authors could further elaborate on any potential negative societal impacts of the paper especially concerning the computational overheads associated with the proposed training scheme. Addressing these limitations can enhance the papers societal relevance and practical implications. Overall the paper contributes significantly to understanding the robustness of SNNs and presents innovative approaches to bolster their resistance against adversarial attacks. By addressing the raised questions and considering the suggested improvements the paper can reinforce its impact in the field of neuromorphic computing and adversarial defense for safetycritical applications.", "qtQ9thon9fV": " Peer Review  Summary: The paper proposes Fourier Occupancy Field (FOF) a new representation for monocular realtime and accurate human reconstruction. FOF is aligned with input images and efficiently captures 3D geometry using a 2D field with Fourier series expansion. The authors demonstrate a realtime pipeline achieving 30FPS for highfidelity human reconstruction.  Strengths: 1. Novel Representation: FOF provides an innovative approach to 3D geometry representation bridging the gap between 2D images and 3D geometries efficiently. 2. Realtime Performance: The demonstrated high frame rate of 30FPS for realtime human reconstruction is a significant strength. 3. Comparison and Evaluation: Comprehensive comparisons with existing methods and thorough evaluation metrics enhance the credibility of the proposed method. 4. Flexibility and Adaptability: FOFs scalability in terms of sampling rates and simplicity of transformation to meshes make it a promising solution.  Weaknesses: 1. Limitation in Representing Thin Objects: The paper acknowledges a limitation of FOF in representing thin objects due to the Fourier series expansion which could restrict its applicability in certain scenarios. 2. Privacy and Ethical Concerns: The impact section touches upon privacy concerns but more emphasis on addressing potential negative societal impacts could enhance the paper.  Questions and Suggestions for Authors: 1. Scalability: Could the authors elaborate on FOFs scalability to different complexities and poses for human reconstruction 2. Performance vs. Memory: How does the proposed FOF approach compare with existing memoryintensive methods in terms of performance and resource utilization 3. Mesh Generation: Could the authors discuss potential challenges or limitations in mesh extraction from FOF representation especially for complex geometries 4. Future Directions: Are there plans to explore or extend FOFs applicability beyond human reconstruction tasks  Limitations: The authors have addressed the limitations regarding thin objects and privacy concerns adequately. Improvements could include exploring alternative representations for thin objects and conducting a more indepth ethical impact assessment to guide policy decisions. Overall the paper presents a compelling method for realtime human reconstruction with promising results. Strengthening the discussion on scalability mesh extraction and future research directions could enhance the papers impact and contribution to the field.", "p_g2nHlMus": " Peer Review  Summary: The paper proposes a novel approach FewTURE for fewshot learning by decomposing images into patches and utilizing a Vision Transformer architecture pretrained with Masked Image Modeling for selfsupervised learning. The method establishes semantic correspondences between local regions across images refines patch embeddings using token importance reweighting and achieves stateoftheart results on four popular fewshot classification benchmarks. The approach aims to address supervision collapse and enhance generalization for fewshot learning tasks.  Strengths: 1. Innovative Approach: Decomposing images into patches and utilizing Vision Transformers for fewshot learning is a novel and distinctive approach. 2. Effective Utilization of Transformers: Using Transformers solely without conventional convolutional backbones showcases the effectiveness of Transformers in vision tasks. 3. Comprehensive Experiments: The paper provides detailed experiments on popular fewshot classification benchmarks demonstrating the superior performance of the proposed method. 4. Visual Interpretability: The token importance reweighting mechanism provides insights into the most informative patches for classification. 5. Clear Presentation: The paper is wellstructured with detailed explanations and illustrative figures to aid in understanding the proposed methodology.  Weaknesses: 1. Complexity: The methods intricate token importance reweighting process may hinder its practical applicability and implementation in realworld scenarios. 2. Limited Comparison: The paper lacks a thorough comparative analysis with existing stateoftheart methods especially indepth comparisons on various datasets and scenarios. 3. Data Efficiency: Mention of potential challenges in training the method with highly limited data could raise concerns regarding scalability to specialized applications.  Questions and Suggestions for Authors: 1. Can you provide insights into the computational efficiency and scalability of the proposed method especially in scenarios with a significant increase in the number of classes or shots 2. How does the FewTURE approach handle scenarios with varying levels of noise or partial annotations in images 3. Have you considered conducting user studies or qualitative evaluations to assess the interpretability and practical utility of the generated visual explanations 4. It would be beneficial to expand the discussion on the relevance and potential impact of the method in realworld applications beyond fewshot learning tasks.  Limitations: The authors adequately discuss the limitations regarding the methods potential challenges in handling highly limited training data. To further address this issue the authors may consider investigating techniques to improve data efficiency and robustness possibly incorporating prior knowledge or domainspecific constraints. By addressing the weaknesses and open questions improving the methods data efficiency and conducting comprehensive comparative evaluations the authors can enhance the papers impact and practical relevance in the field of fewshot learning.", "pDUYkwrx__w": " Summary: The paper addresses foundational theoretical questions surrounding the privacy loss of the widelyused NOISYSGD algorithm particularly focusing on the behavior of privacy loss with respect to the number of iterations. By providing a tight characterization of the privacy loss the paper reveals that the privacy loss of NOISYSGD does not increase infinitely with the number of iterations as previously believed.  Strengths: 1. Novel Contributions: The paper introduces innovative techniques based on optimal transport and the Sampled Gaussian Mechanism to address fundamental privacy questions in the context of the NOISYSGD algorithm. 2. Theoretical Rigor: The paper rigorously addresses complex theoretical questions surrounding the behavior of privacy loss in machine learning models trained on sensitive data. 3. Detailed Analysis: The paper offers thorough analysis and proofs to support its findings providing a comprehensive understanding of the privacy implications of the NOISYSGD algorithm in various settings.  Weaknesses: 1. Complexity: The technical language and complexity of the proofs may pose challenges for readers not wellversed in the specific techniques and concepts used in the paper. 2. Limited Practical Application: While the theoretical insights are valuable the practical implications of the findings on the realworld application of NOISYSGD may need further exploration and discussion.  Questions and Suggestions for Authors: 1. Clarification on Practical Implementation: Can the authors provide more insights into how the theoretical findings can impact the practical implementation and deployment of differential privacy in machine learning models 2. Generalizability: How easily can the techniques used in this paper be applied to other optimization algorithms beyond NOISYSGD and are there limitations to the generalization of these approaches 3. Comparison with Existing Approaches: How does the proposed methodology in this paper compare to existing approaches in differential privacy analysis and what unique advantages does it offer  Limitations: Regarding limitations and societal impact:  Inclusivity in Accessibility: The complex nature of the papers content may limit accessibility to researchers with diverse backgrounds. Authors could consider enhancing the clarity of the presentation and providing practical examples to improve understanding.  Ethical Considerations: While the paper addresses foundational privacy questions it is essential to consider the broader ethical implications of differential privacy in machine learning and ensure that privacy safeguards are prioritized in datadriven applications. Further discussion on the societal impact and ethical considerations would be beneficial for a comprehensive analysis. In conclusion the paper provides valuable insights into the privacy considerations of training machine learning models on sensitive data and offers novel theoretical contributions that advance the understanding of privacy loss in NOISYSGD. Further exploration of practical implications and broader ethical discussions can enhance the overall impact and relevance of the research.", "ldxUm0mmhl8": " Summary The paper introduces a causal model of thinning for temporal point processes to answer counterfactual questions which are crucial for informing targeted interventions. It proposes a sampling algorithm based on a GumbelMax structural causal model to simulate counterfactual realizations of temporal point processes under alternative intensity functions. The experiments include synthetic and real epidemiological data demonstrating the utility of the algorithm in enhancing targeted interventions.  Strengths 1. Innovative Approach: The paper addresses a critical gap in existing temporal point process models by incorporating counterfactual reasoning adding significant value to the field. 2. Theoretical Grounding: The development of a causal model of thinning based on the GumbelMax structural causal model is wellfounded theoretically and provides a robust framework for answering counterfactual queries. 3. Algorithmic Contribution: The proposed sampling algorithms for inhomogeneous Poisson processes and linear Hawkes processes are novel and wellstructured enabling the generation of counterfactual realizations efficiently.  Weaknesses 1. Complexity and Clarity: The intricacies of the causal model and algorithms might pose challenges for readers unfamiliar with structural causal models thinning processes or Hawkes processes. More intuitive explanations could enhance readability. 2. Limitation of Validation: The lack of validation using observational or interventional experiments limits the concrete assessment of the proposed methodologys effectiveness in realworld scenarios. 3. Generalization: While the paper focuses on specific types of temporal point processes extending the methodology to nonlinear or neural Hawkes processes is suggested for broader applicability.  Questions and Suggestions 1. Validation Concerns: How do the authors plan to address the limitation of validating the counterfactual predictions due to the absence of observational or interventional experiments 2. User Study: Have the authors considered conducting a user study with domain experts to evaluate the usability and effectiveness of the proposed counterfactual realizations in practical settings like epidemiology 3. Generalization: Could the methodology be extended to support spatial point processes or nonlinear Hawkes processes and if so what challenges might arise in this extension  Limitations The authors have not adequately addressed the validation of the proposed methodology due to the absence of observational or interventional experiments. Conducting further validation studies or sensitivity analyses would help validate the effectiveness of the counterfactual approach in enhancing targeted interventions.  Suggestions for Improvement 1. Validation Strategies: Consider exploring simulation studies with known ground truth data or conducting sensitivity analyses to validate the proposed methodologys efficacy in realworld scenarios. 2. UserCentric Evaluation: Conduct user studies with domain experts to gather feedback on the usability and interpretability of the generated counterfactual realizations for targeted interventions. 3. Generalization and Scalability: Extend the methodology to support a wider range of temporal point processes including nonlinear and spatial processes to enhance its applicability across diverse domains.", "rDT-n9xysO": " Summary: The paper proposes a novel approach to tackle TCP congestion control by distilling deep reinforcement learning agents policies into symbolic rules. Their twostage solution involves training a neural network agent through reinforcement learning then distilling its policy into simplified interpretable rules. The proposed symbolic branching algorithm enhances the network context awareness converting the NN policy into a symbolic tree. Experimental validation was done on simulation and emulation environments showing competitive performance with faster execution times.  Strengths: 1. Innovative Contribution: The paper introduces a novel approach that combines deep reinforcement learning with symbolic regression for TCP congestion control addressing interpretability and reliability concerns. 2. Performance Validation: Extensive simulation and emulation experiments were conducted to validate the proposed SymbolicPCC framework showcasing competitive performance compared to existing NN policies. 3. Methodological Advancements: The symbolic distillation workflow and branching algorithm contribute to enhancing agents context sensitivity and performance in diverse network conditions.  Weaknesses: 1. Limited RealWorld Deployment Validation: While simulation and emulation results are promising realworld deployment validation on actual networks could strengthen the papers practical applicability. 2. Complexity: The technical details in the methodology may be overwhelming for readers unfamiliar with RL and symbolic regression. Simpler explanations or visual aids could enhance comprehension.  Questions and Suggestions: 1. Clarification of Symbolic Representation: Can the authors elaborate on the tradeoffs between the complexity of the symbolic rules obtained and the performance gains achieved compared to NN policies 2. Scalability Concerns: How does the proposed method scale with the increasing size or complexity of network conditions Any insights on handling scalability challenges  Limitations: The authors could provide clearer insights into the potential negative societal impacts of the SymbolicPCC framework especially concerning automation bias or lack of human intervention. Addressing such ethical considerations could enhance the papers societal impact. Overall the paper presents a significant advancement in TCP congestion control through a novel symbolic distillation framework but further validation simplification of technical details and considerations on societal impact could strengthen its contribution.", "r4RRwBCPDv5": " Summary: The paper explores the \"double descent\" phenomenon in deep learning networks where large networks trained with zero training error exhibit good generalization on test data contrary to the traditional view on model complexity. It presents a VCtheoretical analysis showing that double descent can be explained using classical VC generalization bounds. The authors illustrate their findings through empirical results on various learning methods including Support Vector Machines Least Squares and Multilayer Perceptron classifiers.  Strengths: 1. The paper addresses an important and current topic in deep learning shedding light on the generalization performance of large neural networks. 2. The use of VCtheoretical analysis to explain the double descent phenomenon is a novel and valuable approach. 3. The detailed explanation of the experimental methodology and results provides a clear understanding of the findings. 4. The paper highlights misconceptions in the machine learning community and offers a fresh perspective on the generalization paradox in deep learning networks.  Weaknesses: 1. The paper could benefit from further exploration of the practical implications of the theoretical findings on realworld applications. 2. The complexity of the VCtheoretical framework used might be challenging for readers without a strong background in statistical learning theory. 3. The limited discussion on the negative societal impacts and ethical considerations of the research is a notable weakness. 4. The absence of error bars and details on the reproducibility of experiments could impact the credibility of the findings.  Questions and Suggestions: 1. Clarification on Negative Impacts: Could the authors elaborate on any potential negative societal impacts of their work and provide guidance on how to mitigate them 2. Reproducibility: Can the authors provide the code data and instructions necessary to reproduce the main experimental results to enhance transparency and reproducibility 3. Practical Relevance: How do the theoretical findings translate into practical guidelines or improvements for deep learning model development and deployment 4. Complexity for NonExperts: Is there a plan to simplify the presentation of the VCtheoretical concepts for a broader audience not well versed in statistical learning theory  Limitations: The authors have not adequately addressed the limitations of their work particularly regarding the technical complexity of applying VCtheoretical frameworks to general deep learning networks. To address this they could provide more detailed guidance on interpretation and broader implications in a more accessible manner.", "yJV9zp5OKAY": " Summary: The paper introduces a novel autonomous data augmentation method named LabelPreserving Adversarial AutoAugment (LPA3) that does not rely on domain knowledge. It aims to create augmentations that preserve label information by optimizing a surrogate objective based on the intermediate layer representations of the modelintraining. LPA3 is demonstrated to bring significant improvements to various machine learning tasks such as supervised semisupervised and noisylabel learning.  Strengths: 1. Novel Contribution: The paper introduces a creative approach to automatic data augmentation without relying on predefined operations or domain knowledge. 2. Theoretical Foundation: The work is well grounded in representation learning principles and information theory providing a strong theoretical basis for the proposed method. 3. Practicality: LPA3 is shown to be effective across a range of tasks demonstrating its applicability in realworld scenarios. 4. Empirical Results: Experiment results demonstrate consistent improvements in efficiency and final performance in comparison to existing methods.  Weaknesses: 1. Limited Analysis: The evaluation section could benefit from more indepth analysis such as ablation studies or additional comparisons with more methods. 2. Complexity: The proposed method involves optimization of intricate objectives which may lead to challenges in implementation and understanding for some readers. 3. Reproducibility: Although the code is available more detailed instructions or demonstrations on reproducibility could enhance the papers impact.  Questions and Suggestions: 1. Could you elaborate on the computational complexity of the LPA3 method in practical applications especially in larger datasets or complex models 2. How does LPA3 handle rare class instances or imbalanced datasets Could the augmentation strategy be customized for such scenarios 3. Considering the proposed surrogate objective are there any limitations or failure cases that need to be addressed for robustness in diverse realworld applications  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work in terms of efficiency and performance improvements. A suggestion for improvement would be to include a brief section discussing ethical considerations related to data augmentation practices. Overall the paper presents an innovative method with strong theoretical foundations and promising empirical results. Further analyses and enhancements could strengthen the papers impact and applicability in machine learning research.", "ucNDIDRNjjv": " Peer Review  Summary: The paper proposes a novel framework Nonstationary Transformers to address the limitations faced by Transformers in time series forecasting on nonstationary realworld data. The framework consists of two interdependent modules Series Stationarization and Destationary Attention which aim to balance series predictability and model capability by incorporating nonstationary information. Experimental results demonstrate the superior performance of Nonstationary Transformers compared to baseline models achieving stateoftheart results on various benchmarks.  Strengths: 1. Originality and Contributions: The paper introduces an innovative approach to tackle the overstationarization problem in time series forecasting addressing a significant limitation in existing methods. 2. Experimental Evaluation: The extensive experiments conducted on realworld datasets demonstrate the effectiveness and generality of the proposed framework showcasing consistent performance improvements across various Transformerbased models. 3. Detailed Analysis and Ablation Study: The paper provides a thorough analysis of the proposed frameworks components via case studies quantitative evaluations and comparisons with existing methods offering valuable insights into the benefits of Series Stationarization and Destationary Attention.  Weaknesses: 1. Theoretical Clarity: The theoretical underpinnings of the proposed framework especially the Destationary Attention mechanism could be further elaborated to enhance reader comprehension. 2. Evaluation Metrics: While the paper primarily focuses on MSE reduction as the evaluation metric incorporating additional metrics such as MAE or quantile loss could provide a more holistic assessment of the models performance.  Questions and Suggestions: 1. Interpretability: Can the authors provide insights into the interpretability of the models predictions especially in realworld applications where understanding the basis for forecasts is crucial 2. Scalability: How does the proposed framework scale with larger datasets or longer time series considering the potential computational complexity and training time implications 3. Robustness: Have the authors considered testing the models robustness to noisy or outlier data points which are common in realworld time series datasets  Limitations: The authors adequately addressed the limitations and potential negative societal impact of their work by emphasizing the importance of maintaining the predictive capability of nonstationary series while addressing the overstationarization problem. Constructive suggestions for improvement have been provided and the paper demonstrates awareness of the limitations for practical implementation. Overall the paper presents a compelling solution to a critical challenge in time series forecasting and provides extensive experimental validation to support its claims. Strengthening the theoretical explanations exploring further interpretability aspects and considering scalability and robustness in future work could enhance the impact and applicability of the proposed framework.", "wo-a8Ji6s3A": " PeerReview: 1. Summary: The paper introduces a novel loopless projection stochastic approximation algorithm (LPSA) for linearly constrained stochastic approximation problems with federated learning as a special case. The authors propose a method that involves probabilistic projection to ensure feasibility. Through a detailed analysis they show that trajectories weakly converge to solutions of specific stochastic differential equations. The work provides insights into biasvariance tradeoffs in choosing projection probabilities and step sizes. Experimental results validate theoretical findings on synthetic datasets under federated learning settings. 2. Strengths:  The paper addresses a significant problem in stochastic optimization specifically in the context of federated learning.  The introduction of a loopless algorithm contributes to simplifying the structure of existing methods like Local SGD.  The theoretical analysis is rigorous and provides novel insights into the asymptotic behavior of the proposed algorithm.  Experiments on synthetic datasets validate the theoretical results enhancing the credibility of the findings. 3. Weaknesses:  The paper is highly technical and may present challenges for readers without a strong mathematical background.  The literature review could be expanded to provide a more comprehensive overview of related work particularly in the field of federated learning.  The presentation of the results especially in the theoretical sections could be made more accessible by providing intuitive explanations alongside mathematical derivations. 4. Questions and Suggestions for Authors:  Can you elaborate on the computational complexity of the LPSA algorithm compared to existing methods  How does the proposed algorithm generalize to more complex optimization problems beyond linear constraints  Could you discuss the potential application of the algorithm in realworld federated learning scenarios with largescale datasets or heterogeneous clients  It would be beneficial to include a discussion on the robustness of the algorithm to noisy or nonconvex optimization problems. 5. Limitations:  While the paper effectively addresses theoretical aspects of the algorithm more discussion on practical implementation challenges computational efficiency and scalability in realworld applications is needed.  The societal impact of the work is not explicitly addressed in the paper. Authors should consider potential ethical implications of their algorithm in widespread deployment scenarios. 6. Suggestions for Improvement:  Add a section addressing the practical considerations such as computational complexity memory requirements and scalability of the algorithm.  Discuss the ethical implications of implementing the algorithm in federated learning scenarios and propose strategies to mitigate any negative societal impacts.  Enhance the readability of technical sections by providing intuitive explanations where possible to aid understanding for a broader audience. Overall the paper presents a valuable contribution to the field of stochastic optimization and federated learning. By addressing the weaknesses and limitations noted above and implementing the suggestions for improvement the authors can enhance the impact and relevance of their work.", "k_XHLBD4qPO": "Summary: The paper addresses the challenge of ClassIncremental Semantic Segmentation (CISS) in Continual Learning (CL) in Computer Vision. The authors propose improvements in methods focusing on the offline training process expressiveness of representations and introducing dropout in the encoderdecoder architecture. They demonstrate improved performance in segmentation accuracy while addressing the issues like catastrophic forgetting and semantic drift. Strengths: 1. Clear articulation of the problem statement and contributions. 2. The paper provides a thorough analysis comparing existing methods and proposing novel improvements. 3. Rigorous experimental validation on the PASCAL VOC 2012 dataset and introduction of a new benchmark setting. 4. Incorporating Information Bottleneck principle for better understanding. 5. Discussing societal impact and ethical considerations. Weaknesses: 1. The paper lacks error bars and should consider providing statistical significance for experimental results. 2. While addressing the limitations the discussion can be expanded to cover the generalization aspects of the proposed model. 3. The comparisons with other stateoftheart models could be more detailed including visual comparisons or additional metrics. Questions and Suggestions for Authors: 1. Can you elaborate on how the proposed method can handle extreme conditions such as an increasing number of classes or complex background variations 2. Have you considered the impact of hyperparameter tuning on the performance improvements shown in the experiments 3. How does the proposed method compare in terms of computational efficiency and training time compared to existing approaches Limitations: The authors have addressed the limitations adequately. However for further improvement they may consider including statistical significance tests to validate their results and provide clearer insights into the generalization capabilities of the model.", "suHUJr7dV5n": " Peer Review  Summary: The paper proposes a new Hessianfree bilevel optimization method named PZOBO (Partial ZerothOrderlike Bilevel Optimizer) that approximates the response Jacobian matrix to improve training stability and efficiency. The proposed algorithm is validated through experiments on various bilevel optimization problems showcasing superior performance compared to baseline optimizers. The paper also provides a convergence rate analysis and theoretical guarantees for the proposed algorithms.  Strengths: 1. Innovative Approach: The introduction of a Hessianfree method that focuses on approximating only the response Jacobian matrix is a novel approach and shows significant improvements in stability and efficiency. 2. Theoretical Contributions: The paper provides theoretical analysis and convergence rate guarantees for the proposed algorithms which is essential for understanding the algorithms behavior and performance. 3. Empirical Validation: Extensive experiments are conducted on different bilevel optimization problems demonstrating the superiority of the proposed method over existing baseline optimizers. 4. Clear Presentation: The paper is wellstructured clearly explaining the problem formulation the proposed method theoretical analysis and experimental results.  Weaknesses: 1. Clarity in Analysis: While the paper covers the theoretical analysis some sections such as the technical assumptions and convergence analysis might be challenging for readers unfamiliar with the field. Simplifying complex mathematical formulations could enhance accessibility. 2. Comparison Metrics: The paper could benefit from including more detailed comparative metrics with baseline algorithms in experiments to provide a comprehensive understanding of the proposed algorithms performance.  Questions and Suggestions for Authors: 1. How does the proposed algorithm handle noise in the gradients especially in the stochastic setting Have you considered incorporating mechanisms to address noisy gradients 2. In the experiments did you encounter any scenarios where the proposed algorithm exhibited limitations or cases where it underperformed compared to baselines Understanding the failure modes can provide insights for future improvements. 3. Can you elaborate more on the generalizability of the proposed method across different types of machine learning problems beyond the ones experimented with in the paper 4. The convergence analysis and theoretical guarantees are crucial. Have you considered providing intuitive explanations or visual aids to help readers grasp the theoretical aspects more easily  Limitations: The authors have adequately discussed the limitations of the paper and potential negative societal impacts.  Suggestions for Improvement: To enhance the paper further it could be beneficial to provide more intuitive explanations for the theoretical analysis include additional comparative metrics in experiments and incorporate mechanisms to handle noisy gradients for robustness in realworld scenarios. Overall the paper presents a significant advancement in the field of bilevel optimization with promising results. With some minor enhancements and additional clarity this work has the potential to make a strong impact in the machine learning community.", "mjUrg0uKpQ": " Peer Review  Summary of the Paper: The paper introduces I2DFormer a transformerbased framework for zeroshot learning (ZSL) that leverages online textual documents such as Wikipedia to learn unsupervised semantic embeddings for object classes. The proposed model aligns images and documents in a shared embedding space by introducing a novel crossmodal attention module that captures finegrained interactions between image patches and document words. The paper demonstrates that I2DFormer outperforms existing unsupervised semantic embeddings in ZSL on multiple datasets providing highly interpretable results where document words are grounded in image regions.  Strengths: 1. Innovation and Contribution: The paper presents a novel approach that uses online textual documents as side information for ZSL improving performance significantly. 2. Novel Module  I2D Attention: The introduction of the I2D Attention module to capture finegrained interactions between image patches and document words is a valuable addition to enhance discriminative features. 3. Comprehensive Experiments: The paper conducts thorough experiments on AWA2 CUB and FLO datasets comparing with stateoftheart (SOTA) methods in ZSL and provides detailed quantitative and qualitative results to support the proposed approach. 4. Interpretability: The qualitative analysis showcasing the interpretability of the models results including document word localization in images adds depth to the study.  Weaknesses: 1. Limited Comparison to Existing Methods: While the paper compares with some unsupervised semantic embedding methods a broader comparison with more related works on ZSL could strengthen the discussion. 2. Limited Discussion on Training Scalability: Considering the potential scalability challenges of training on large datasets with online documents addressing this aspect could enhance the practical applicability of the proposed method. 3. Data Noise Handling: Dealing with the noise in online text documents and its impact on model performance could be further elaborated to provide insights into the robustness of the approach.  Questions and Suggestions for Authors: 1. Scalability: How scalable is the proposed method when training on larger datasets with a vast collection of online textual documents 2. Noise Handling: How does the model handle noise in the online documents and are there mechanisms in place to filter out irrelevant details effectively 3. Future Directions: Can the authors discuss potential future directions for extending the proposed approach to address more complex ZSL scenarios or other domains outside computer vision  Limitations: The authors should further address the potential negative societal impact related to biases present in online content. They could consider incorporating content moderation toolkits or methods to mitigate unintended biases in data collected from online sources. In conclusion the paper presents an innovative approach with significant strengths in improving ZSL performance using online textual documents. Addressing the suggested weaknesses and questions could further enhance the studys impact and practical applicability.", "lme1MKnSMb": " Summary The paper introduces a novel approach using transformers for video compression eliminating the need for complex architectural biases like motion prediction and warping. The Video Compression Transformer (VCT) outperforms existing methods on standard datasets showcasing its ability to handle complex motion patterns solely from data. The approach is easy to implement and the authors provide code for further research.  Strengths  Innovative Approach: The paper offers a unique solution for video compression using transformers diverging from traditional methods.  Performance: Outperforms existing methods on standard datasets showcasing the effectiveness of the proposed Video Compression Transformer.  Simplicity and Ease of Implementation: The approach simplifies the model by removing architectural biases making it easier to implement.  Extensive Experiments: The authors conduct a thorough evaluation on both standard and synthetic datasets providing comprehensive results and comparisons.  Weaknesses  Technical Complexity: The paper involves intricate details of transformerbased models which may hinder understanding for readers not wellversed in this domain.  Comparison to NonNeural Methods: The comparison to nonneural methods like HEVC could have been more extensive for a comprehensive evaluation.  Limitation Analysis: Limited discussion on potential limitations and negative impacts of the proposed approach on society.  Questions and Suggestions for Authors 1. Can the authors provide more insights into the computational requirements and efficiency of the proposed transformerbased video compression compared to traditional methods 2. How does the proposed approach handle issues like latency during video compression and decompression processes 3. It would be beneficial to discuss the tradeoffs in terms of computational complexity and performance compared to existing methods.  Limitations The authors did not adequately address the potential negative societal impacts of their work. It would be constructive for the authors to include a section on the energy consumption implications of their method and suggestions for optimizing for sustainability.  Suggestions for Improvement  Include more detailed comparisons with traditional nonneural video compression methods to provide a comprehensive evaluation.  Discuss more extensively the practical implications and tradeoffs of adopting transformerbased video compression in realworld applications.  Address potential negative societal impacts related to increased computational requirements and energy consumption providing suggestions for mitigation. Overall the paper presents a promising approach to video compression with transformers but further clarity comparisons and considerations on implications will enhance its impact and usability in the field.", "mNtFhoNRr4i": "1) Summary: The paper introduces a novel technique for evaluating hierarchical classifiers by capturing the tradeoff between specificity and correctness using a thresholdbased inference rule. It proposes two new loss functions softmaxmargin and softmaxdescendant and conducts empirical comparisons on the iNat21 and ImageNet1k datasets. The study reveals that the flat softmax classifier outperforms topdown classifiers across all operating points highlights the effectiveness of the softmaxmargin loss function and explores the robustness of different methods to unseen classes. 2) Strengths:  The paper addresses an important problem of hierarchical classification and proposes a novel technique for evaluating hierarchical classifiers contributing to the advancement of the field.  The study provides empirical comparisons of various loss functions on realworld datasets offering valuable insights into the performance of different methods.  The paper is wellstructured and presents results comprehensively making it easy for readers to follow and understand the methodology and findings. 3) Weaknesses:  The experimental results seem to indicate significant dominance of the flat softmax classifier over the proposed methods and existing baselines. This raises questions about the practical relevance and effectiveness of the proposed techniques.  The paper lacks a thorough discussion on the potential reasons behind the observed performance discrepancies and offers limited insights into improving the effectiveness of topdown classifiers.  The studys focus on specific datasets and limited comparison metrics may restrict the generalizability and applicability of the proposed techniques to diverse hierarchical classification problems. 4) Questions and Suggestions:  Could the authors provide more insights into the underlying reasons for the inferior performance of topdown classifiers compared to the flat softmax particularly in the context of nonleaf predictions  Have the authors considered the implications of the observed dominance of the flat softmax classifier on the practical adoption and deployment of hierarchical classifiers in realworld applications  How do the proposed loss functions softmaxmargin and softmaxdescendant address the limitations observed in the performance of topdown classifiers Can the authors provide a detailed comparison and analysis of these new approaches  How do the findings of the study contribute to advancing the understanding of hierarchical classification and what are the potential directions for future research based on these results 5) Limitations: While the paper discusses the tradeoff between specificity and correctness in hierarchical classifiers more indepth analysis and discussion on the practical implications of the observed performance discrepancies and ways to improve the effectiveness of topdown classifiers would enhance the papers insights and applicability.", "t6O08FxvtBY": " Summary The paper introduces a new algorithmic approach termed Bilevel Optimizationoriented Pruning (BIP) to address the gap between pruning accuracy and efficiency in deep learning model pruning. BIP leverages bilinearity to optimize model pruning and retraining showing superior efficiency and accuracy compared to existing methods like Iterative Magnitude Pruning (IMP) and oneshot pruning methods.  Strengths 1. Innovative Approach: The paper presents a novel perspective on model pruning introducing BLO as an optimization framework and BIP as a specialized method. This fresh viewpoint is a significant contribution to the field. 2. Theoretical and Empirical Results: The paper provides theoretical justification for the efficiency of BIP and supports it with extensive experiments across various datasets and model architectures demonstrating superior performance in accuracy and efficiency. 3. Efficiency: BIP offers significant speedups over IMP while maintaining or improving accuracy in finding winning tickets making it a promising algorithm for practical implementation.  Weaknesses 1. Complexity: The complexity of the proposed BIP method might be a barrier to adoption for some practitioners especially in its specialized optimization framework and algorithms. 2. Limited Comparison: While the paper compares BIP to IMP and a few other methods a broader comparison with more stateoftheart pruning techniques would strengthen the evaluation of BIPs performance. 3. Evaluation Metrics: The paper mainly focuses on accuracy and computational efficiency including metrics related to model interpretability or robustness could provide a more comprehensive analysis.  Questions and Suggestions for Authors 1. Scalability: How does the efficiency of the BIP method scale with larger models or datasets 2. Generalization: Have you tested the generalization capabilities of BIP on adversarial robustness or transfer learning tasks to demonstrate its broader applicability 3. Comparison: Could you compare BIP to recently proposed advanced pruning techniques especially those focusing on structured pruning to further benchmark its performance 4. Implementation: How feasible is it for practitioners to implement and deploy BIP in realworld applications  Limitations The authors have effectively addressed the limitations by providing detailed experiments and comparisons to existing methods. To improve they could further explore how BIP fares with more diverse datasets and architectures to validate its robustness across various scenarios.", "wA7vZS-mSxv": " Summary: The paper introduces a novel method to estimate the intrinsic dimensionality of datasets in high dimensions using Normalizing Flows. It addresses the challenges faced by existing methods particularly nearest neighbor statistics in scaling to large datasets and high dimensions. The method is based on backoftheenvelope calculations predicting how the singular values of the flows Jacobian change with different noise magnitudes. The paper demonstrates the effectiveness of the proposed method on various datasets including 64x64 RGB images achieving stateoftheart results.  Strengths: 1. Innovative Methodology: The paper introduces a novel approach to estimate the intrinsic dimensionality using Normalizing Flows addressing scalability issues faced by existing methods. 2. Theoretical Rigor: The method is theoretically wellfounded leveraging backoftheenvelope calculations to predict the behavior of singular values providing a sound basis for estimating the intrinsic dimensionality. 3. Experimental Validation: The paper demonstrates the efficacy of the proposed method through experiments showcasing stateoftheart results on various datasets including highresolution images.  Weaknesses: 1. Clarity of Presentation: The papers complexity and technical depth may present challenges for readers not deeply familiar with the topic. The clarity of presentation could be improved for better accessibility. 2. Limited Comparison: While the paper compares the proposed method with existing approaches such as twoNN and LIDL the comparison could be further elaborated by analyzing a broader range of datasets and scenarios.  Questions and Suggestions for Authors: 1. Clarity Enhancements: Could the authors consider incorporating additional visual aids or intuitive examples to aid in understanding the complex concepts presented in the paper 2. Generalization: How generalizable is the proposed method across various types of datasets and realworld applications beyond the ones tested in the paper 3. Computational Efficiency: Can the authors provide insights into the computational efficiency of their method compared to existing approaches especially when scaling to larger and more complex datasets  Limitations: The authors should consider detailing potential negative societal impacts stemming from the adoption of their method such as implications for privacy biases in the data or unintended consequences in decisionmaking based on the estimated intrinsic dimensionality.", "v9pljSdlUNP": "Summary: The paper provides an indepth study of Stochastic Backpropagation (SBP) for training deep neural networks in image classification and object detection tasks. SBP saves GPU memory and computational costs by calculating gradients from a subset of feature maps during backward propagation. The authors offer best practices for applying SBP showing that it can efficiently train image models with minimal accuracy loss. The study generalizes SBP to imagebased tasks and explores different design choices and strategies. Strengths: 1. The paper addresses a crucial issue of memoryefficient training for deep neural networks offering a novel perspective with SBP. 2. The study provides thorough theoretical formulations and practical implementation guidance for applying SBP effectively. 3. Comprehensive experiments on image classification and object detection tasks validate the effectiveness of SBP in reducing GPU memory usage with minimal accuracy degradation. 4. The paper carefully analyzes different design strategies and their impacts on accuracy and memory savings providing valuable insights for practitioners. 5. The work highlights the potential of SBP for various computer vision tasks and demonstrates its generalizability across different network architectures. Weaknesses: 1. The paper lacks indepth exploration of the potential negative impacts or limitations of the proposed SBP method. 2. The study could benefit from a comparative analysis with existing memoryefficient training techniques to showcase the uniqueness and advantages of SBP more effectively. 3. The evaluation on only two network architectures for image classification and object detection might limit the generalizability of the findings to a wider range of tasks and models. Questions and Suggestions for Authors: 1. How does SBP compare to other memoryefficient training methods like gradient checkpointing or mixedprecision training in terms of both memory savings and training speed 2. Can the authors provide insights into the scalability of SBP to larger datasets or more complex tasks beyond image classification and object detection 3. How does the choice of hyperparameters such as the keepratio for gradients in SBP impact the performance of the models and are there any optimal values based on experiments Limitations: The authors should make a more explicit effort to address any potential negative societal impact by incorporating ethical considerations in the research design and dissemination process promoting transparency and considering biases and fairness in the implementation of their approach. Overall the paper presents a valuable contribution to the field of memoryefficient training for deep neural networks. Addressing the highlighted suggestions and limitations could further enhance the robustness and applicability of the proposed SBP method.", "vKBdabh_WV": " Peer Review  Summary The paper proposes a method called Meta OT that leverages amortized optimization to predict optimal transport maps quickly enhancing computational efficiency in repetitive OT problems. The study explores applications in discrete and continuous settings related to image transport spherical data and color palettes.  Strengths 1. Novel Approach: The Meta OT methodology is innovative utilizing amortized optimization for efficient prediction of OT maps. 2. Experimental Demonstrations: The paper provides thorough experiments demonstrating the effectiveness of Meta OT in discrete OT between MNIST digits spherical data and color transfer tasks. 3. Technical Detail: The paper includes detailed descriptions of the models algorithms and hyperparameters used in the study. 4. Robust Analysis: The analysis covers a range of applications and thoroughly evaluates the impact of Meta OT on various OT problems.  Weaknesses 1. Evaluation Metrics: While the paper delves into the technical details of the model more insight into the choice of evaluation metrics and potential benchmarking against established methods could enhance the analysis.  Questions and Suggestions for Authors 1. Generalization: How does the proposed Meta OT perform in outofdistribution scenarios or with diverse datasets 2. Cost Variability: Have you explored scenarios where the cost function varies for each OT problem and assessed the adaptability of the Meta OT model in such cases 3. Scalability: How does the computational complexity scale with an increasing number of measures or larger datasets 4. Comparison: Could you provide comparative evaluations against stateoftheart OT solvers to showcase the relative advantages of Meta OT  Limitations The authors adequately addressed the limitations and potential negative societal impacts of their work. To further strengthen suggesting exploring alternative modeling formulations and comparison against a broader set of baseline methods could be beneficial. Overall the paper presents a promising methodology in Meta OT for enhancing OT predictions efficiently. Further experiments and comparisons could enrich the thoroughness of the study.", "pGcTocvaZkJ": " Summary: This paper introduces a novel algorithm Censored Quantile Regression Neural Network (CQRNN) which combines ideas from survival analysis with neural networks to perform quantile regression on censored data. The paper addresses the challenge of predicting the target variable directly and characterizing uncertainty without assumptions about the distribution of residual errors. The proposed algorithm optimizes a grid of quantiles using a single neural network leading to improved calibration on real datasets.  Strengths:  Innovative Algorithm: CQRNN offers a novel approach to censored quantile regression with neural networks providing theoretical insights and empirical performance improvements.  Theoretical Analysis: The paper provides a theoretical analysis interpreting the algorithm as a form of expectationmaximisation and demonstrating a selfcorrecting property.  Empirical Results: The experimental evaluation showcases the effectiveness of the algorithm demonstrating better calibration on various datasets compared to existing methods.  Comprehensive Experiments: The paper conducts thorough experiments on synthetic and real datasets covering varied domains sizes and dimensionalities.  Weaknesses:  Efficiency Concerns: The authors acknowledge the inefficiency of the sequential grid algorithm in comparison to CQRNN but could further discuss the tradeoffs between accuracy and computational efficiency.  Limited Theoretical Guarantees: While the theoretical insights are valuable the paper could provide more indepth analysis on the convergence properties and limitations of the proposed algorithm.  Limited Dataset Diversity: The paper tests the algorithm on a limited number of realworld datasets emphasizing the need for broader testing across diverse domains.  Questions and Suggestions: 1. Scalability: How does the proposed algorithm scale with larger datasets or higherdimensional data 2. Robustness: Have sensitivity analyses been conducted to evaluate the algorithms robustness to different hyperparameter settings or data distributions 3. Generalizability: How does CQRNN perform on additional types of realworld datasets beyond those presented in the paper 4. Comparison Metrics: Could the authors discuss the choice of evaluation metrics and consider additional metrics to provide a comprehensive assessment of the algorithms performance  Limitations: The authors could further address the limitations and potential negative societal impact of their work by:  Considering potential biases or unintended consequences that could arise from implementing the algorithm in realworld applications.  Providing suggestions for ensuring fairness transparency and accountability when deploying CQRNN in critical domains like healthcare. In conclusion the paper presents a significant advancement in applying neural networks to censored quantile regression offering promising results and theoretical contributions. Further exploration of scalability robustness and generalizability could enhance the papers impact and applicability in realworld scenarios.", "xuw7R0hP7G": " PeerReview Summary: The paper introduces a novel framework for analyzing the convergence of Stochastic Gradient Descent (SGD) in nonconvex optimization problems by leveraging the concept of admissible potentials and establishing a connection with the rate of convergence of Gradient Flow (GF). The authors provide theoretical results and performance guarantees for GD and SGD algorithms based on the existence of admissible potentials and specific regularity conditions. The analysis is demonstrated through applications to various optimization problems including phase retrieval and matrix square root. Strengths: 1. Theoretical Innovation: The paper introduces a novel framework that provides insights into the convergence behavior of SGD in nonconvex optimization problems based on admissible potentials and convergence rates of GF. This innovative approach offers a fresh perspective on understanding optimization algorithms. 2. Rigorous Analysis: The authors present rigorous mathematical analyses supporting their theoretical results establishing a solid foundation for their conclusions. The paper covers a wide range of mathematical concepts and tools showcasing a comprehensive understanding of the subject matter. 3. Application to Diverse Problems: By applying the proposed framework to various optimization problems such as phase retrieval and matrix square root the authors demonstrate the versatility and utility of their approach across different domains highlighting the practical relevance of their findings. Weaknesses: 1. Complexity of Terminology: The paper involves a high level of technical terminology and mathematical detail which could be challenging for readers without a strong background in optimization theory. This complexity might limit the accessibility and understanding of the presented material for a broader audience. Questions and Suggestions: 1. Clarification on Assumptions: Could the authors provide further insights into the practical implications of the selfbounded regularity conditions (Assumptions 1 2 and 3) on the convergence guarantees of GDSGD How flexible are these assumptions in realworld applications 2. Computational Complexity: In realworld scenarios how does the computational complexity of implementing the proposed framework compare to traditional optimization algorithms especially in largescale optimization tasks 3. Additional Examples: Are there additional optimization problems beyond the phase retrieval and matrix square root where the framework has been applied and what are the general trends observed in the convergence behavior across different scenarios Limitations: While the paper presents a comprehensive analysis of the proposed framework the high level of mathematical rigor and technicality might hinder the wider dissemination of the findings to a nonspecialist audience. Improving the explanation of concepts and providing more intuitive interpretations could enhance the accessibility and impact of the research.  Limitations Addressed: The authors have not adequately addressed the limitations and potential negative societal impact of their work. To address this they can provide a section in the paper discussing the limitations explicitly and suggest areas for further research to mitigate any potential negative consequences thus enhancing the social responsibility of their work.", "sMezXGG5So": " Peer Review  Summary: The paper introduces a novel graph Transformer model NODEFORMER designed for node classification on large graphs. It addresses deficiencies in current graph neural networks by proposing an allpair message passing scheme enabled by a kernelized GumbelSoftmax operator reducing algorithmic complexity to linear w.r.t. node numbers. Extensive experiments demonstrate promising efficacy for node classification tasks and graphenhanced applications even on graphs with up to 2 million nodes.  Strengths: 1. Novelty and Contribution: The introduction of NODEFORMER leveraging a kernelized GumbelSoftmax operator for allpair message passing is a significant contribution to addressing deficiencies in current graph neural networks. 2. Efficiency: The paper effectively reduces the algorithmic complexity to linear w.r.t. node numbers enabling scalable learning of graph structures for large datasets. 3. Experimental Validation: Extensive experiments demonstrate the models efficacy showing superior performance over strong GNN models and stateoftheart structure learning methods on various node classification tasks.  Weaknesses: 1. Explanation Clarity: The papers technical content is dense and may be challenging for readers without a deep understanding of graph neural networks and Transformer models. Simplifying complex technical terms and providing more illustrative examples could enhance comprehension. 2. Theoretical Justification: While theoretical analysis is provided a more detailed explanation of the theoretical underpinnings of the proposed approach would strengthen the paper.  Questions and Suggestions: 1. Scalability vs. Precision Tradeoff: How does the proposed model balance scalability and precision when learning new graph topologies for large networks Elaborating on this tradeoff and discussing potential limitations in extreme scalability scenarios could enrich the paper. 2. Generalization and Robustness: Could the proposed model be evaluated for generalization to different types of graphs including sparse or dense graphs with varying characteristics Robustness analyses under noisy or incomplete graphs could provide valuable insights. 3. Comparative Analysis: Further comparative analysis with different graph neural network architectures and transformerbased models on a wider range of datasets could enhance the evaluation and validate the models effectiveness comprehensively. 4. Potential Extensions: Are there possible extensions or enhancements to the NODEFORMER model that the authors are considering for future work such as incorporating attention mechanisms or hierarchical structures for improved performance  Limitations: The paper does not thoroughly address potential negative societal impacts of the proposed work. Suggestions for improvement include discussing ethical considerations related to the models application potential biases in the data used for training and strategies for mitigating unintended consequences.", "uOJZ_zU9qZm": "Summary: The paper introduces Automatic Propagation (AP) software a framework that generalizes existing automatic differentiation (AD) frameworks to allow custom and composable construction of complex learning algorithms. The AP software enables packaging custom learning algorithms into propagators that automate essential computations across diverse computation graphs. The authors implement Proppo a prototype AP software package built on PyTorch and showcase its utility by implementing Monte Carlo gradient estimation techniques not previously publicly available. Minimalistic experiments demonstrate a significant improvement in gradient estimation accuracy particularly in chaotic systems. Strengths: 1. Innovative Contribution: The development of AP software fills an important gap in existing ML frameworks by enhancing flexibility in composing learning algorithms beyond backpropagation. 2. Practical Application: The implementation of Proppo and demonstration of its utility in complex learning algorithms showcase its feasibility and effectiveness. 3. Technical Depth: The paper provides a thorough technical explanation of the AP framework its components and usage allowing readers to grasp the methodology effectively. Weaknesses: 1. Complexity for Novice Users: The technical content may be challenging for readers unfamiliar with automatic differentiation and message passing frameworks potentially limiting the accessibility of the paper. 2. Limited Experimental Validation: While the paper presents minimalistic experiments to showcase the utility of Proppo additional comprehensive experiments across diverse domains would strengthen the empirical evaluation. Questions and Suggestions for the Authors: 1. Could the authors provide more insight into the scalability of the AP software especially in more extensive and complex machine learning models 2. How does the proposed AP software compare to existing frameworks like Storchastic in terms of usability and performance 3. Have the authors considered incorporating userfriendly features or documentation to facilitate the use of Proppo by researchers with varying levels of expertise Limitations: The authors could enrich their discussion regarding the potential negative societal impact of their work and how they plan to mitigate any associated risks. Emphasizing ethical considerations and implications could add depth to the paper.", "uxWr9vEdsBh": " Summary The paper introduces \"Active Learning via Lagrangian DualitY\" (ALLY) a novel batch active learning method that formulates the problem using constrained learning and addresses batch diversity. The method leverages Lagrangian duality to select informative unlabeled samples based on dual variables. ALLYs performance is evaluated on various classification and regression tasks demonstrating competitive results compared to stateoftheart methods.  Strengths 1. Innovative Approach: The paper introduces a unique active learning method based on Lagrangian duality offering a fresh perspective on the problem. 2. Theoretical Justification: The connection between dual variables and informativeness is wellestablished with theorems and insightful explanations. 3. Empirical Evaluation: Extensive experiments across diverse datasets showcase the effectiveness of ALLY highlighting its competitiveness. 4. Thorough Exposition: The paper is wellstructured providing clear explanations of the method problem formulation and experimental results. 5. Consideration of Dataset Redundancy: Analysis on dataset redundancy adds depth to the evaluation of the methods performance under various conditions.  Weaknesses 1. Comparative Analysis: The paper lacks a direct comparison with some recent active learning methods such as VAAL and WAAL which could provide better context for ALLYs performance. 2. Sparse Discussion on Limitations: While the paper briefly mentions limitations a more comprehensive discussion and exploration of potential drawbacks could enhance the evaluation. 3. Societal Impact: The societal implications and ethical considerations of using active learning particularly in sensitive areas like healthcare could be further discussed.  Questions and Suggestions for Authors 1. Model Capacity and Dataset Redundancy: Did you investigate how the performance of ALLY varies with the capacity of the model used and the redundancy in the dataset Could different model architectures affect ALLYs performance 2. Practical Implementation: How practical is the implementation of ALLY in realworld scenarios considering computational complexity and scalability concerns 3. Interpretability of Dual Variables: Could you explain how the interpretability of the dual variables was validated in the experimental setup What insights were gained from interpreting the informativeness scores of ALLY on the generated samples  Limitations The paper does not extensively address the potential negative societal impacts of active learning particularly in healthcare where obtaining labeled data is challenging. To improve it would be beneficial to include a section discussing the ethical implications and providing suggestions for mitigating any negative effects. Overall the paper presents an innovative active learning method with a strong theoretical foundation and promising experimental results. Further examination of its performance under various conditions and additional exploration of interpretability aspects could enhance its contributions.", "xUK4E1jpV7z": " Review: 1. Summary and Contributions: The paper extends the findings of Chalopin et al. (2018) on unlabelled sample compressibility of extremal classes of VC dimension. It presents a criterion for extremal classes to have unlabelled compression schemes of size VC dimension and demonstrates the embedding of intersectionclosed classes into extremal classes with good compression properties. The study offers insights into compression schemes for various types of concept classes and contributes to the understanding of PAC learnability and generalization in learning theory. 2. Strengths:  Deep insights into the sample compressibility of concept classes.  Theoretical framework with clear definitions and proofs.  Connections to prior work are wellestablished.  Contributions towards resolving the longstanding open problem in learning theory.  The application of the proposed criteria and construction is welldetailed. 3. Weaknesses:  The paper can benefit from more illustrative examples to aid reader comprehension.  Discussions on the practical implications of the theoretical findings could enhance the relevance to realworld applications.  The clarity of the argument could be improved in some parts for better readability. 4. Questions and Suggestions for Authors:  Can the proposed compression schemes be practically implemented and validated on realworld datasets  How do the findings of this study impact the practical implementation of machine learning algorithms  Have you considered evaluating the proposed schemes on benchmark datasets to showcase their effectiveness and efficiency  Could the paper include visual aids or case studies to enhance the understanding of the proposed concepts 5. Limitations: The authors could provide a more explicit discussion on the potential negative societal impacts of the findings to ensure ethical considerations in the application of the proposed compression schemes. Overall the paper showcases significant contributions to learning theory and offers a novel perspective on sample compressibility. With some minor improvements in clarity and realworld implications the study can have a profound impact on the field.", "uAIQymz0Qp": " Review of \"Distributed Morphological Attention Policy for Adaptive Locomotion\" 1. Summary: The paper introduces a novel Distributed Morphological Attention Policy (DMAP) architecture for addressing the problem of learning locomotion in changing body configurations without direct access to perturbation information. The study evaluates DMAP in four locomotion environments and compares its performance with several alternative agents showcasing that DMAP can learn endtoend in all environments and match or surpass the performance of an oracle agent. 2. Strengths:  Innovative Architecture: DMAP presents a novel architecture incorporating principles from biological sensorimotor control which is a strong point of the paper.  Performance Comparison: The thorough evaluation against alternative agents provides comprehensive insights into the effectiveness of DMAP.  Experimental Rigor: The experiments conducted are welldetailed and the comparison with existing techniques adds credibility to the findings.  Clear Presentation: The paper is wellstructured making it easy to follow the methodology and results. 3. Weaknesses:  Lack of RealWorld Validation: While the architecture is promising the study lacks realworld validation or application demonstration limiting the practical implications of the findings.  Societal Impact Discussion: The paper lacks a discussion on the potential societal impact or ethical considerations of deploying such technologies which could be important in promoting responsible AI development.  Comparative Analysis: A deeper analysis comparing DMAP with more recent stateoftheart approaches in continuous control could enhance the relevance and impact of the work. 4. Questions and Suggestions:  Clarification on Adaptation Speed: Can the authors provide more insights into the adaptation speed of DMAP compared to other techniques under different perturbation intensities  Scalability Concerns: How scalable is DMAP to more complex scenarios or largerscale environments beyond the ones considered in the study  Generalizability: Have the authors explored the generalizability of DMAP to other types of locomotion tasks or environments with varying complexities 5. Limitations: The authors could provide clearer insights into the limitations and potential negative societal impacts of their work. Offering suggestions to address societal impacts and ethical concerns related to integrating AI technologies especially in robotics applications would add value to the paper. In conclusion the paper presents a promising architecture for adaptive locomotion with valuable contributions to the field. Addressing the noted weaknesses and providing deeper insights into the questions raised could further enhance the significance and applicability of the presented research.", "x-i37an3uym": " Review  Summary: The paper proposes a ModuleAware Optimization approach for Auxiliary Learning (MAOAL) to address the modulelevel influence in deep learning. It introduces the concept of learnable modulelevel auxiliary importance to optimize the model parameters and auxiliary importance in a bilevel manner. Extensive experiments demonstrate that MAOAL consistently outperforms existing baselines in various scenarios showcasing its potential as a powerful tool for auxiliary learning.  Strengths: 1. Clear Problem Identification: The paper correctly identifies the gap in considering modulelevel influence in auxiliary learning providing a novel solution with MAOAL. 2. Innovative Approach: MAOALs moduleaware optimization introduces a unique perspective to optimize auxiliary learning which can potentially improve model performance significantly. 3. Thorough Experiments: The comprehensive experiments across different tasks and datasets demonstrate the efficacy of MAOAL in comparison to existing methods. 4. Insights Provided: The analysis of modulelevel auxiliary importance offers valuable insights into the behavior of the model under different scenarios enhancing understanding in auxiliary learning.  Weaknesses: 1. Complex Methodology: The bilevel optimization process may introduce computational complexity which is a potential drawback in terms of scalability and efficiency. 2. Evaluation Limited to Specific Tasks: The evaluation mainly focuses on image classification and recommendation tasks limiting the generalizability of the method to a broader range of applications. 3. Limited Comparison on Model Complexity: The impact of model complexity on the usefulness of the method with L2 regularizer might benefit from a more indepth analysis.  Questions and Suggestions for the Authors: 1. Scalability Concerns: How does the bilevel optimization process in MAOAL scale with increasing numbers of modules and auxiliary losses Any insights on optimizing this process for larger models 2. Additional Datasets: Have you considered experimenting with more diverse datasets to showcase the methods applicability across various domains beyond image classification and recommendation 3. Comparison with StateoftheArt: Could you provide a deeper comparison with the stateoftheart methods in terms of computation time and resource utilization 4. Interpretation of Results: Can you provide more insights into why the importance of some modules changes under different scenarios and what implications this has on model performance  Limitations: The authors should provide a more detailed discussion on the potential computational overhead introduced by the bilevel optimization process in MAOAL and consider a broader range of datasets for a more thorough evaluation of the methods effectiveness.", "q16HXpXtjJn": " Summary: The paper presents a comprehensive study on distribution functional estimation in both offline and online settings in the context of the infinitearmed bandit problem. It introduces unified meta algorithms for estimating various functionals beyond the maximum including mean median trimmed mean and maximum achieving optimal sample complexities. The study utilizes Wasserstein distances to derive informationtheoretic lower bounds and reveals a curious thresholding phenomenon in median estimation not captured by previous approaches.  Strengths: 1. Comprehensive Approach: The paper addresses a wide range of distribution functionals and provides unified algorithms for both offline and online settings. 2. Optimal Sample Complexities: Achieving optimal sample complexities for estimating different functionals is a significant contribution. 3. Theoretical Insights: By utilizing Wasserstein distances the paper provides rigorous informationtheoretic lower bounds that offer valuable insights into the problem. 4. Curious Thresholding Phenomenon: The identification of a thresholding phenomenon in median estimation adds an intriguing aspect to the study.  Weaknesses: 1. Complexity: The papers technical content is highly complex and may be challenging for readers not wellversed in the field. 2. Applicability: While the study is theoretically robust practical implications or realworld applications of the proposed algorithms could be further emphasized. 3. Clarity: The paper could benefit from improved clarity in explaining certain technical details and algorithms for ease of understanding by a broader audience.  Questions and Suggestions for Authors: 1. Could the authors elaborate on the practical applications or implications of their proposed algorithms in realworld scenarios to enhance the practical relevance of the study 2. Can the authors provide more insights into how the identified thresholding phenomenon in median estimation could impact algorithm performance or decisionmaking processes in practice 3. It would be beneficial to clarify how the proposed algorithms and the identified thresholding phenomenon could impact the scalability or efficiency of largescale distributed learning systems as mentioned in the introduction.  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work by focusing on theoretical analysis and algorithmic developments. However they could enhance the societal impact by providing more explicit connections to practical applications and benefits.", "qm5LpHyyOUO": " Peer Review of \"MCMAE: Enhancing Vision Transformers with Multiscale Convolution and Masked Autoencoding\" 1. Summary and Contributions The paper introduces a selfsupervised framework called MCMAE that enhances Vision Transformers by combining multiscale convolution and masked autoencoding. MCMAE demonstrates improved performance on tasks like image classification object detection and semantic segmentation. The key contributions include the design of a hybrid convolutiontransformer encoder a blockwise masking strategy and a multiscale decoder. Results show significant performance gains compared to other stateoftheart methods. 2. Strengths and Weaknesses Strengths:  Clever integration of hybrid convolutiontransformer architecture with masked autoencoding.  Detailed experimentations showing performance improvements in various vision tasks.  Efficient training enabled by the blockwise masking strategy.  Strong comparison with existing methods showing superior results. Weaknesses:  The paper lacks a detailed discussion on the generalizability of the proposed approach to other datasets beyond ImageNet.  The explanation of the experimental setup could be clearer for better reproducibility.  Some concepts such as the multiscale decoder could benefit from more detailed descriptions.  The paper doesnt address potential realworld applications or deployment scenarios. 3. Questions and Suggestions for Authors  Can the proposed MCMAE framework be adapted to handle domainspecific datasets or tasks  How does the proposed approach perform with limited training data compared to other methods  Can the authors elaborate on the computational requirements and scalability of MCMAE for larger models  Could the authors explore the interpretability of the learned representations for better model understanding and trustworthiness 4. Limitations The paper adequately addresses potential negative societal impacts by not foreseeing any negative outcomes in the proposed work. To further improve authors could consider discussing ethical implications fairness concerns or environmental impacts related to the deployment of the MCMAE framework. Overall the paper presents a welldeveloped framework with promising results and potential for further enhancements. Solid experimental validations and comparisons make the work valuable to the research community. A more comprehensive discussion on broader applications and model interpretability could add depth to the research findings.", "xxgp42Qz6dL": " Summary: The paper introduces a novel approach called energyguided stochastic differential equations (EGSDE) for unpaired imagetoimage translation (I2I). EGSDE utilizes an energy function pretrained on both the source and target domains to guide the inference process of a pretrained stochastic differential equation (SDE) for realistic and faithful translation. The method outperforms existing scorebased diffusion models (SBDMs) in terms of realism and faithfulness on multiple tasks and datasets.  Strengths: 1. Innovative Approach: EGSDE proposes a novel method that effectively leverages an energy function to improve both realism and faithfulness in unpaired I2I. 2. Empirical Validation: The paper provides comprehensive empirical results comparing EGSDE to a variety of baselines on popular datasets showing superior performance in terms of FID and other metrics. 3. Flexibility and Tradeoffs: EGSDE allows for flexible tradeoffs between realism and faithfulness showing the ability to tune hyperparameters for further improvements.  Weaknesses: 1. Limited Comparison: While the comparisons with existing methods are detailed the paper lacks a comparison with more recent GANsbased methods potentially not covering the full spectrum of stateoftheart techniques in the field. 2. Societal Impact: The potential negative societal impact of the work such as generating misleading fake images is touched upon briefly but could be further elaborated and addressed.  Questions and Suggestions: 1. Complexity vs. Effectiveness: Can the authors provide insights into the computational complexity of EGSDE compared to other methods How does the method scale with larger datasets or more complex image domains 2. Interpretability: Could the authors provide more detailed explanations or visualizations to help readers understand how the energy function guides the generation process 3. Generalization: How well does EGSDE perform on novel or unseen domains Are there any insights on the generalization capabilities of the proposed method  Limitations: The authors should further address the potential negative societal impact of generating realistic but misleading images. To mitigate this impact clear guidelines and ethical considerations should be emphasized in the paper. Overall the paper presents a strong contribution to the field of unpaired imagetoimage translation with EGSDE. Addressing the highlighted points could enhance the clarity and impact of the work.", "qC2BwvfaNdd": "Summary: The paper proposes a framework called DataIQ to systematically stratify examples into subgroups based on outcome heterogeneity in tabular data particularly in healthcare. DataIQ characterizes examples using aleatoric uncertainty and confidence providing robust data characterization principled data collection and reliable model deployment. The paper demonstrates DataIQs effectiveness through experiments on realworld medical datasets. Strengths: 1. Novel Framework: DataIQ introduces a systematic approach to subgroup identification based on aleatoric uncertainty catering to heterogeneous outcomes in tabular data. 2. Robust Characterization: DataIQs subgroups are consistent across different model architectures providing a more stable interpretation than baselines. 3. Practical Applicability: Demonstrates valuable insights for feature acquisition dataset selection and model deployment improving the robustness of ML workflows in realworld scenarios. Weaknesses: 1. Clarity in Formulation: Some sections especially the formulation of uncertainty decomposition during training may be challenging to understand for readers without a strong technical background. 2. Generalization Beyond Tabular Data: While the paper briefly touches on applying DataIQ to text and image data in appendices further exploration of its effectiveness in diverse data modalities could enhance its impact. 3. Limitations Not Clearly Addressed: The paper lacks a detailed discussion on the potential negative societal impacts of its framework neglecting to thoroughly address limitations and ethical considerations. Questions and Suggestions: 1. Clarification on Ambiguous Subgroup: Could the authors provide more insights into how the Ambiguous subgroup could be leveraged in practice for improving model performance and dataset quality 2. Model Applicability: How does DataIQ perform when used with more complex ML models beyond neural networks and gradient boosting Is there a limit to its adaptability with certain architectures 3. Validation on Larger Datasets: Have the findings been validated on larger more diverse datasets to ensure the scalability and generalizability of DataIQs framework Limitations: The paper inadequately addresses the limitations and potential negative societal impacts of its work. To improve the authors should methodically discuss ethical considerations fairness and potential biases that may arise from the adoption of DataIQ in sensitive domains like healthcare.", "oDWyVsHBzNT": " Summary: The paper introduces a modelbased offline reinforcement learning approach called PessimismModulated Dynamics Belief (PMDB). PMDB maintains a belief distribution over system dynamics and evaluatesoptimizes policy through biased sampling from this belief. The proposed approach aims to improve policy learning by considering dynamics uncertainty through biased sampling and iteratively optimizing policy through an alternating Markov game formulation showing stateoftheart performance on benchmark tasks.  Strengths: 1. Innovative Approach: The paper introduces a novel approach PMDB which addresses dynamics uncertainty in modelbased offline RL through biased sampling. 2. Theoretical Rigor: The authors provide a solid theoretical foundation for their approach including formal proofs and discussions on the advantages over existing methods. 3. Empirical Evaluation: Empirical results demonstrate stateoftheart performance across a variety of benchmark tasks. 4. Clarity and Organization: The paper is wellstructured providing clear explanations of the methodology formulation and results.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers not wellversed in reinforcement learning and modelbased approaches. 2. Comparison with Baselines: While the paper compares the proposed approach with previous stateoftheart methods further comparisons with a broader set of baselines could provide a more comprehensive evaluation. 3. Lack of RealWorld Evaluation: The paper focuses on benchmark tasks and it would benefit from discussing potential realworld applications or practical scenarios where PMDB could be applied.  Questions and Suggestions for Authors: 1. Can you provide more insights into how the proposed approach PMDB could be practically implemented in realworld scenarios or domains beyond benchmark tasks 2. How scalable is the PMDB approach in handling largerscale reinforcement learning problems with more complex dynamics and highdimensional state spaces 3. Considering the theoretical foundations discussed in the paper can you elaborate on any potential limitations or challenges in implementation that may arise in practical applications 4. For future work are there plans to explore extensions or modifications to PMDB to enhance its applicability to different types of reinforcement learning tasks or environments  Limitations: The authors could expand their discussion to address any potential limitations and the societal impact of their work. Providing insights into how the proposed approach could be ethically deployed or any potential negative consequences in practical applications would enhance the papers completeness.", "xvLWypz8p8": " Summary: The paper investigates the generalization properties of majority voting on finite ensembles of classifiers by providing stateoftheart marginbased generalization bounds using PACBayes theory. Key contributions include a new margin bound for nonrandomized majority votes applicable to multiclass classification tasks and empirical results showing favorability compared to existing bounds.  Strengths: 1. Theoretical Contributions: The paper introduces a novel margin bound for nonrandomized majority votes expanding the understanding of ensemble classifiers. 2. Empirical Results: The empirical evaluation demonstrates the effectiveness of the proposed approach outperforming existing margin and PACBayes bounds in several scenarios. 3. Comparative Analysis: Comprehensive comparisons with existing PACBayes bounds and marginbased approaches showcasing the superiority of the proposed method. 4. Relevance: The research addresses a pertinent issue in machine learning and ensemble methods contributing to the ongoing debate on margins theory.  Weaknesses: 1. Nondifferentiability: The nondifferentiability of margin loss could pose challenges for direct optimization which might impact the practical application of the proposed method. 2. AlgorithmDependent: The effectiveness of the bounds relies on the choice of learning algorithms which might limit the generalizability of the results across different scenarios. 3. Complexity of Proofs: The detailed theoretical proofs may hinder accessibility for readers not wellversed in the advanced mathematical concepts involved.  Questions and Suggestions: 1. Clarification on Optimality: Can the authors provide further insights into the optimality of the proposed bound especially in realworld scenarios with complex datasets 2. Impact of Dimensionality: How does the proposed method handle highdimensional data and scalability concerns especially in scenarios with a large number of voters 3. Practical Implementation: Have the authors considered practical implications and computational efficiency when applying the proposed bounds in realworld applications Any insights on ease of implementation 4. Further Robustness Analysis: Could the authors expand on the robustness aspects of the proposed marginbased approach particularly in scenarios with noisy or imbalanced datasets  Limitations: The authors should address the implications of nondifferentiability for optimization and provide suggestions for potential practical workarounds to enhance the application of the proposed bounds in realworld machine learning settings.", "prQkA_NjuuB": " Summary The paper investigates the parameterization of deep neural networks satisfying the continuity equation via divergencefree vector fields. It proposes constructing divergencefree neural networks using differential forms to precisely satisfy the continuity equation showcasing applicability in fluid dynamics and dynamical optimal transport maps.  Strengths 1. Innovative Approach: The paper introduces a novel method to enforce exact constraints in neural networks offering a new perspective on parameterizing divergencefree vector fields. 2. Theoretical Foundation: The work is wellgrounded in mathematical theory particularly differential forms enhancing the rigor of the proposed constructions. 3. Practical Relevance: Experimental validations on fluid dynamics and optimal transport demonstrate the practical utility and effectiveness of the proposed models.  Weaknesses 1. Complexity: The technicality of the presented material might limit accessibility to a broader audience potentially necessitating more explanatory visuals or intuitive explanations. 2. Scalability Concerns: The authors acknowledge limitations in scalability due to extensive use of automatic differentiation suggesting avenues for improvement would be beneficial.  Questions and Suggestions 1. Clarity of Notation: Ensure clarity in notations used throughout the paper. For instance defining the terms used in the differential forms and vector field constructions would aid comprehension. 2. Comparison with Existing Methods: How does the proposed framework compare with existing constraint enforcement techniques in terms of computational efficiency and accuracy 3. Generalization to Higher Dimensions: Can the proposed constructions be extended to higher dimensions beyond the 2D and 3D cases discussed in the paper 4. Practical Deployability: How feasible is the implementation of these divergencefree neural networks in realworld applications considering computational costs and model interpretability  Limitations While the authors have discussed scalability concerns they could further address how to enhance the efficiency of their approach for broader applications. Additionally outlining potential negative societal impacts and proposing strategies to mitigate them would strengthen the paper.", "uV_VYGB3FCi": " Summary: The paper introduces a novel method called Code Editing for Neural Image Compression (NIC) that enables flexible bitrate control with a single decoder surpassing existing variablerate methods. The approach is based on semiamortized inference and adaptive quantization allowing for continuous bitrate control ROI coding and multidistortion tradeoff. Experimental results demonstrate the superior performance of Code Editing over current methods.  Strengths: 1. Innovative Methodology: The introduction of Code Editing for NIC is a novel approach that addresses the limitations of existing variablerate methods. 2. Flexible Bitrate Control: The ability to achieve continuous bitrate control with a single decoder is a significant advancement. 3. Experimental Results: The paper provides thorough experimental results that demonstrate the effectiveness of Code Editing in achieving various coding requirements.  Weaknesses: 1. Complexity: The method seems complex and may require sophisticated implementation and understanding which could limit its practical adoption. 2. Limited Training Data: The paper lacks detailed information on the amount and diversity of training data used which could impact the reproducibility and generalizability of the results. 3. Computation Efficiency: There is a mention of efficiency improvement needed implying potential issues with computational speed or overhead.  Questions and Suggestions: 1. Efficiency Enhancement: Can the authors elaborate on potential strategies to improve the efficiency of Code Editing for realtime applications 2. Training Data: Could the authors provide more details on the dataset used for training the models and potential biases or limitations in the dataset 3. Deployment Challenges: What are the main challenges in deploying Code Editing in realworld settings considering its complexity and potential computational demands  Limitations: The authors did not adequately address limitations related to computational efficiency and realtime deployment. To improve it would be beneficial to explore strategies to optimize Code Editing for realtime applications and enhance computational efficiency. Further considering dataset biases and limitations could improve the generalizability of the method.", "yjybfsIUdNu": "Summary: The paper focuses on the reinforcement learning (RL) problem in the presence of multiple environments with different levels of fidelity for a given control task. The authors propose a multifidelity estimator based on control variates to reduce the variance in the estimation of the stateaction value function. They introduce a multifidelity Monte Carlo RL (MFMCRL) algorithm and analyze its performance theoretically and experimentally. Strengths: 1. The paper addresses an important and relevant problem in RL utilizing multifidelity data. 2. The proposed multifidelity estimator is grounded in the method of control variates providing a solid theoretical basis. 3. The paper provides both theoretical analysis and empirical results to validate the proposed MFMCRL algorithm. 4. Experimentation on synthetic MDPs and NAS benchmarks adds value to the findings. Weaknesses: 1. The paper could benefit from a more detailed discussion on the practical implications and realworld applications of the proposed algorithms. 2. While the experimental results are promising a more extensive comparison with existing methods would further strengthen the paper. 3. The paper lacks a detailed discussion on the computational complexity and scalability of the proposed methods. Questions and Suggestions for Authors: 1. How does the proposed MFMCRL algorithm compare to stateoftheart RL algorithms in terms of scalability and computational complexity 2. Have you considered realworld scenarios or case studies to demonstrate the practical utility of the MFMCRL algorithm 3. Could you elaborate on the implications of the variance reduction achieved by the multifidelity estimator on the overall learning process especially in complex environments Limitations: The authors have appropriately addressed the limitations and potential negative societal impacts of their work. To enhance this aspect they could consider highlighting the environmental impact of running computationalintensive experiments and suggest methods for mitigating such impacts. Overall the paper presents a valuable contribution to the field of RL by introducing the concept of multifidelity data in reinforcement learning. Through further refinement and exploration of practical applications the proposed MFMCRL algorithm has the potential for significant advancements in RL research.", "nJWcpq2fco3": "Peer Review of \"Representation Learning Framework for Spatial Trajectories\" 1. Summary: The paper introduces a representation learning framework for spatial trajectories where partial observations of trajectories are represented as probability distributions in a latent space. The framework allows for obtaining trajectory samples for any continuous point in time supporting tasks like interpolation extrapolation and modification of trajectory attributes. Experiments showcase the frameworks superiority over baseline models in prediction tasks particularly in human movement datasets. 2. Strengths:  The paper addresses challenges in trajectory representation learning comprehensively offering a flexible and generalizable framework.  The methods ability to handle uncertainty in trajectories and encode them in a latent space is innovative and promising for downstream applications.  Experimental results demonstrate the frameworks effectiveness in prediction tasks showcasing its advantages over existing baselines.  The incorporation of variational components in representing trajectories and the utilization of different distribution families show methodological rigor and innovation. 3. Weaknesses:  The paper lacks a detailed comparative analysis with existing representation learning methods for trajectories restricting the assessment of the proposed frameworks novelty.  The discussion on the scalability and computational complexity of the framework in realworld applications is limited which may pose challenges for practical implementation.  The paper could benefit from a deeper exploration of the interpretability of learned latent representations and their relevance to downstream tasks and decisionmaking processes. 4. Questions and Suggestions for Authors:  Could the authors elaborate on the interpretability of the learned latent representations and their association with specific attributes of trajectories  How does the proposed framework scale with larger and more complex trajectory datasets and what computational resources are required for training and inference  Considering the realworld applications of trajectory predictions are there specific scenarios or domains where the framework performs exceptionally well or requires further augmentation 5. Limitations: The authors provide a robust analysis of the limitations and societal impacts of their work. To further enhance this aspect they could consider:  Explicitly addressing any potential biases in the training data that might affect the generalizability of the framework to diverse populations or contexts.  Including a section discussing ethical considerations related to trajectory prediction applications such as privacy concerns or unintended consequences of inaccurate predictions. Overall the paper presents a promising trajectory representation learning framework with notable strengths in handling uncertainty and enhancing prediction accuracy. Further clarification on interpretability scalability and ethical considerations would enrich the studys impact and applicability.", "s0AgNH86p8": "Summary: The paper introduces TransBoost a novel deep transductive learning procedure that finetunes pretrained neural models to improve performance on specific test sets. The method is inspired by a large margin principle and significantly enhances ImageNet classification on various architectures achieving stateoftheart transductive performance. TransBoost is compared against SSL and tFSL methods. Strengths: 1. Innovative Contribution: Introducing a novel transductive finetuning procedure that significantly boosts performance on specific test sets. 2. Comprehensive Evaluation: Extensive empirical study across various architectures and datasets demonstrating consistent and significant accuracy gains. 3. Comparison with Baselines: Distinguishing TransBoosts superiority over SSL and tFSL methods. 4. Clear Methodology: Detailed description of TransBoost implementation and optimization procedure (Algorithm 1) for reproducibility. 5. Thorough Analysis: Detailed discussion on the impact of test set size and alternative transductive learning objectives. Weaknesses: 1. Complexity and Efficiency: The quadratic time complexity for large test sets could be a potential limitation. Suggestions for efficiency improvement could enhance practical applicability. 2. Alternative Implementations: While the simplicity of the proposed functions aids clarity exploring more sophisticated implementations may provide further insights into performance enhancements. 3. Lack of Negative Results: As all results presented are positive addressing any limitations or failure cases could provide a more balanced discussion. Questions and Suggestions: 1. Efficiency Improvement: How do you plan to address the time complexity issue for large test sets Could subsampling strategies or alternative loss formulations be explored for more efficient computation 2. Comparative Analysis: Considering the consistent performance gains of TransBoost have you explored any failure cases or scenarios where the method may not provide significant improvements 3. Generalization: How applicable is TransBoost beyond image classification tasks Have you considered exploring its effectiveness in other domains like NLP 4. Further Exploration: Given the success of TransBoost do you plan to investigate the impact of different hyperparameters or loss formulations on performance 5. Benchmarks: Are you considering releasing benchmark datasets or evaluation metrics to facilitate comparisons with future transductive learning methods Limitations: The authors should provide further insights into addressing the quadratic time complexity issue for large test sets and consider exploring alternative implementations for improved efficiency.", "wgRQ1IM4g_w": " Review of \"Stochastic Bandit Optimization with Soft Constraints\"  Summary: The paper presents a study on a stochastic bandit problem with a general unknown reward function and constraint function both lying in a reproducing kernel Hilbert space (RKHS). The focus is on soft constraints that may be violated as long as cumulative violations are small. Leveraging primaldual optimization the paper introduces a general framework incorporating a novel sufficient condition for algorithm design and analysis. The proposed approach aims at achieving a complexityregretconstraint tradeoff in kernelized bandit settings demonstrating superior algorithm performance via numerical experiments.  Strengths: 1. Theoretical Contribution: The paper introduces a novel framework for constrained kernelized bandits with soft constraints addressing a theoretical gap in the field. 2. Algorithm Design: The primaldual optimization framework is a significant contribution enabling a unified approach for efficient algorithm design. 3. Performance Evaluation: The paper provides comprehensive numerical experiments on synthetic and realworld datasets illustrating the effectiveness of the proposed algorithms.  Weaknesses: 1. Clarity: The paper could benefit from improved clarity in explaining the technical details and the connection between theoretical concepts and practical implications. 2. Comparison: While the comparison with existing methods is valuable further detailed analysis and discussion on the limitations and strengths of each approach could enhance the papers depth. 3. RealWorld Impact: The realworld implications of the proposed framework could be more explicitly discussed to highlight its potential applications and societal benefits.  Questions and Suggestions: 1. Exploration Strategies: How do the exploration strategies (e.g. GPUCB GPTS RandGPUCB) impact the algorithm performance and what are the key factors to consider in choosing an exploration strategy 2. Extension to Practical Applications: How applicable is the proposed framework to realworld scenarios beyond the specific examples discussed in the paper Consider discussing potential use cases and practical implications in more detail. 3. Scalability: Are there scalability constraints when applying the proposed approach to larger or more complex datasets Discuss the scalability aspects and potential avenues for addressing scalability issues. 4. Further Analysis: Can you provide insights on the computational complexity tradeoffs between the proposed algorithms and existing methods especially in scenarios with stringent time constraints  Limitations: Authors could enhance by providing a more detailed discussion on the negative societal impacts or limitations of the proposed framework such as computational overhead data privacy concerns or potential biases in algorithmic decisionmaking. Recommendations for improvement include incorporating a thorough ethical evaluation and discussing mitigation strategies to address any negative societal impacts effectively.", "tzNWhvOomsK": " Summary: The paper addresses the challenging problem of zeroshot learning (ZSL) with attributes by developing a novel theoretical framework to quantify the lower bounds on the error of attributebased ZSL methods. The authors provide a comprehensive analysis of the theoretical intrinsic difficulty of the ZSL problem based on available information specifically the classattribute matrix. They establish the first nontrivial lower bounds for attributebased ZSL and show that their analysis can be predictive of the behavior of standard ZSL methods in practice. The contributions include formulating the lower bound as a linear program presenting a closedform expression for binary classification and demonstrating the tightness of the lower bound.  Strengths: 1. Novel Contribution: The paper introduces a novel theoretical framework for quantifying the worstcase error in attributebased ZSL. The development of lower bounds fills a critical gap in understanding the theoretical underpinnings of ZSL. 2. Theoretical Rigor: The paper provides a rigorous mathematical analysis and presents the lower bounds in a computationally tractable manner enhancing the theoretical foundation of ZSL research. 3. Empirical Validation: The authors conducted extensive experiments to compare the lower bounds with the empirical error rates of popular ZSL methods demonstrating the practical relevance of the theoretical findings. 4. Clear Structure: The paper is wellstructured with clear explanations of the problem statement methodology experimental setup and results making it easy for readers to follow the research.  Weaknesses: 1. Complexity: The proposed mathematical framework especially the linear program formulation might be challenging for readers without a strong background in mathematics or machine learning theory. 2. Limited Generalization: The analysis is focused on attributebased ZSL and may not fully generalize to all existing ZSL methodologies limiting the broader applicability of the findings. 3. Empirical Evaluation: While the empirical validation provides insights a more detailed discussion on the limitations and discrepancies between the theoretical lower bounds and practical performance could enhance the discussion section.  Questions and Suggestions for Authors: 1. Are there specific assumptions or constraints in the theoretical framework that might restrict the applicability to realworld datasets beyond the ones considered in the experiments 2. How do the proposed lower bounds compare with existing performance metrics in ZSL such as zeroshot error rates or generalized accuracy measures 3. Have the authors considered exploring the impact of noise and uncertainties in the classattribute matrices on the lower bounds and practical performance of ZSL methods  Limitations and Suggestions for Improvement: The authors should further address the potential negative societal impact by highlighting the limitations of their approach in realworld applications and emphasizing the necessity of interpreting the lower bounds accurately to avoid overestimating the performance of ZSL models. They could suggest ways to mitigate these limitations such as incorporating additional data sources or improving the robustness of attribute representations.", "wOUH1VQ9Rcj": " Summary: This paper introduces a novel Transformed Independent Noise (TIN) condition for causal discovery under measurement error in the context of a linear nonGaussian acyclic model. The TIN condition checks for independence between a specific linear transformation of some variables and others aiming to identify the causal graph among unobserved target variables. By leveraging nonGaussianity and higherorder statistics of data TIN provides informative insights into the graph structure. The paper demonstrates the effectiveness of TIN in identifying the ordered group decomposition of the causal model achieving this without the need for computationally and statistically intensive methods like overcomplete independent component analysis.  Assessment of Strengths: 1. Innovative Approach: The introduction of the TIN condition is a novel and innovative approach to causal discovery under measurement error providing a valuable alternative to existing methods. 2. Theoretical Foundations: The paper provides strong theoretical foundations by drawing on concepts from LiNGAM and nonGaussianity to establish the TIN condition. 3. Experimental Results: The effectiveness and reliability of the proposed method are demonstrated through experiments on both synthetic and realworld data showcasing the practical utility of the TIN condition. 4. Thorough Analysis: The paper carefully analyzes the implications of the TIN condition graphically providing insights into how it can improve identifiability in the latentvariable problem.  Assessment of Weaknesses: 1. Complexity: The paper introduces advanced concepts and mathematical formulations which may be challenging for readers without a strong background in causal discovery and related fields. 2. Limited Applicability: The focus on a specific formulation of the problem restricts the generalizability of the proposed method to broader causal discovery scenarios. 3. Evaluation Metrics: While the paper showcases the effectiveness of TIN through experiments a more comprehensive comparison with existing methods and additional evaluation metrics could provide a clearer understanding of its performance.  Questions and Suggestions: 1. Validation: How does the paper address potential overfitting issues in the experimental results especially considering the complexity of the TIN condition 2. Scalability: How does the proposed TIN method scale with increasing data size and dimensionality Any insights on scalability challenges and potential solutions would be valuable. 3. Human Interpretability: How does the ordered group decomposition identified by TIN translate into actionable insights for practitioners in realworld applications of causal discovery  Limitations: The paper could benefit from a more detailed discussion on the limitations and potential negative societal impacts of the proposed TIN method. Providing insights into these aspects would enhance the overall contribution and ethical considerations of the work. Consider highlighting these aspects further for a more wellrounded evaluation.", "sRKNkpUMQNr": "Peer Review: Summary: The paper proposes a novel energybased framework for compressing generative adversarial networks (GANs) through knowledge distillation aiming to maximize the mutual information between a teacher and a student network. The proposed method introduces an energybased model to represent a flexible variational distribution and achieves outstanding performance in model compression tasks across various GAN architectures and datasets. Strengths: 1. Innovation: The paper introduces a novel energybased framework for GAN compression through knowledge distillation addressing the limitations of existing approaches. 2. Flexibility: The proposed method can be incorporated into various GAN compression algorithms enhancing their performance consistently. 3. Experimental Rigor: Extensive experiments across multiple datasets and models demonstrate the effectiveness of the proposed method showcasing improvements in model compression tasks. 4. Theoretical Foundation: The utilization of a variational approach and a detailed optimization procedure strengthen the theoretical grounding of the method. Weaknesses: 1. Complexity: The optimization procedure involving multiple steps and considerations may pose challenges for implementation and understanding. 2. Lack of Baseline Comparisons: While the paper presents impressive results comparisons with more diverse baseline approaches could provide a better context for evaluating the proposed method. 3. Limited Interpretation of Results: The paper could benefit from a deeper discussion of why the proposed energybased model outperforms alternative methods and the implications of this success. Questions and Suggestions: 1. Clarity: Clarify the computational complexity of the proposed approach compared to conventional GAN compression techniques. 2. Generalizability: Discuss how the energybased model can be adapted to other types of models beyond GANs and its potential implications. 3. Robustness: Address the robustness of the proposed method to hyperparameter settings and variations in dataset characteristics. Limitations: The authors could provide more explicit discussion on how the proposed approach addresses the limitations and potential societal impacts of model compression techniques ensuring transparency and ethical considerations are adequately addressed. Overall the paper presents a significant advancement in GAN compression through a novel energybased framework supported by comprehensive experiments and theoretical underpinnings. Strengthening the discussion around limitations baselines and broader implications could further enhance the impact and clarity of the research.", "uRSvcqwOm0": "Summary: The paper proposes a novel causal model for categorical data based on a new classification model called \"Classification with Optimal Label Permutation (COLP).\" The key contributions include the development of a parsimonious classifier the identification of causal models through comparison of likelihood functions demonstration of improved performance compared to existing methods through experiments with synthetic and real data and the release of an R package containing the proposed algorithm and a benchmark dataset. The models identifiability is proven under specific assumptions and a causal discovery algorithm based on maximum likelihood estimation is provided. Strengths: 1. Innovative Approach: The paper introduces a novel model for causal discovery from categorical data filling a gap in existing literature. 2. Rigorous Methodology: The paper provides theoretical backing for the models identifiability and proposes a systematic causal discovery algorithm. 3. Empirical Validation: Extensive experiments with synthetic and real data demonstrate the effectiveness of the proposed method compared to stateoftheart techniques. 4. Practical Relevance: The availability of an R package and benchmark dataset enhances the practical utility of the proposed method for the research community. Weaknesses: 1. Limited Comparison: While the paper compares the proposed method with existing techniques a more comprehensive benchmarking against a wider range of causal discovery methods could strengthen the evaluation. 2. Assumptions and Limitations: The paper assumes no unmeasured confounders which may not always hold in realworld data. Acknowledging and addressing this limitation would enhance the papers applicability. 3. Scalability: The paper focuses on bivariate causal discovery extending the approach to multivariate scenarios could enhance its versatility and applicability. Questions and Suggestions: 1. Extension to Multivariate Causal Discovery: Have the authors considered extending the proposed method to multivariate causal discovery scenarios This could broaden the models applicability. 2. Handling Unmeasured Confounders: How does the proposed method handle scenarios with unmeasured confounders considering the practical challenges of data collection and the impact on causal inference 3. Scalability and Efficiency: Could the authors discuss potential strategies to enhance the scalability and efficiency of the proposed algorithm for handling larger datasets with numerous categories Limitations: The authors partially address limitations but further exploration of identifiability under scenarios with unmeasured confounders scalability to multivariate causal discovery and efficiency enhancement for larger datasets can enrich the robustness and applicability of the proposed method.", "yHFATHaIDN": " PeerReview:  1. Summary: The paper introduces an innovative algorithm called MABSplit that accelerates the training process of random forests and other treebased learning methods. The algorithm employs multiarmed bandit techniques to efficiently determine featurethreshold pairs for node splitting. The study provides theoretical guarantees and empirical evidence demonstrating significant speedups in training time without compromising generalization performance. The proposed algorithm outperforms existing methods in terms of computational efficiency generalization and feature importance calculations.  2. Strengths and Weaknesses: Strengths:  The algorithm addresses a crucial need for faster training of machine learning models on large datasets essential for modern applications.  The theoretical guarantees and empirical results showcasing up to 100x faster training without loss in test performance are impressive.  The adaptivity of MABSplit to the data distribution and the ability to discard unpromising splits early significantly improve computational efficiency.  The extensive comparison with baseline models and the demonstration of improved generalization performance and feature stability strengthen the credibility of the proposed algorithm. Weaknesses:  Limited discussion on potential drawbacks or challenges encountered during the implementation or experimental setup.  Lack of realworld case studies or diverse datasets to highlight the algorithms applicability in various domains.  The necessity of tuning specific hyperparameters might not be completely avoided as suggested especially in nuanced scenarios.  While comparing with stateoftheart implementations could provide further context the focus on algorithmic improvements is a valid choice.  3. Questions and Suggestions for Authors:  Can you discuss any specific bottlenecks or limitations encountered when implementing MABSplit in practical settings  It would be beneficial to highlight scenarios where MABSplit might not perform optimally or needs additional customization.  Have you considered incorporating the algorithm into realworld applications to assess its performance in diverse data environments  Could the adaptivity of MABSplit potentially lead to overfitting in certain datasets and if so how do you address this concern  4. Limitations: The authors have not thoroughly addressed the potential negative societal impact of their work. To enhance the societal impact assessment the authors could:  Provide insights into any biases or ethical considerations related to the use of accelerated training algorithms in machine learning.  Conduct a broader discussion on the implications of faster model training on societal aspects such as data privacy fairness and transparency. Overall the paper presents a novel algorithm with promising results for expediting the training of machine learning models but further investigations on practical implications and robustness are recommended for a comprehensive evaluation.", "yLilJ1vZgMe": " Review: 1. Summary: The paper addresses the limitations of kernel methods in largescale learning settings by proposing methods to efficiently compute neural kernels for general activation functions beyond ReLU enabling faster approximations without requiring data points on the unit sphere. By providing dual activation expressions for various activation functions the authors present a subspace embedding approach for Neural Network Gaussian Process (NNGP) and Neural Tangent Kernel (NTK) matrices. The proposed sketching algorithm leads to significant speedups compared to exact computations as demonstrated on the Myrtle5 network for CIFAR10 dataset. 2. Strengths:  Novelty and Contribution: The paper makes a significant contribution by extending the capabilities of neural kernels to a broader range of activations beyond ReLU addressing a key limitation in largescale learning settings.  Theoretical Rigor: The theoretical analyses including the derivation of dual kernels for various activation functions and the subspace embedding method demonstrate a strong theoretical foundation.  Empirical Results: The empirical evaluation on CIFAR10 classification showcasing speedups for approximate CNTK computation and the comparison of accuracy between different activations exhibit the practical relevance of the proposed methods. 3. Weaknesses:  Complexity: The technical details and mathematical formulations presented in the paper may be challenging for nonexpert readers to comprehend easily. A more accessible explanation or intuitive examples could enhance readability.  Empirical Evaluation: While the empirical results on CIFAR10 showcase speedups additional experiments on diverse datasets and network architectures could strengthen the generalizability of the proposed methods. 4. Questions and Suggestions for Authors:  How sensitive are the proposed methods to changes in dataset sizes or network architectures  Can the authors provide insights into the scalability of their approach to even larger datasets or deeper neural networks  Does the sketching algorithm introduce any tradeoffs in terms of accuracy compared to exact computations especially for tasks sensitive to kernel precision 5. Limitations:  The paper adequately addresses the limitations in neural kernel computations by providing efficient methods for general activation functions. However a more indepth discussion on potential negative societal impacts or ethical considerations related to the use of these accelerated methods could be beneficial for ethically conscious AI research. Overall the paper presents valuable advancements in neural kernel computations for deep learning models and provides a strong theoretical and empirical foundation. Further experimentation and refinement could enhance the applicability and impact of the proposed methods.  Great work on reviewing the paper If you have any additional questions or need further feedback feel free to ask.", "wlqb_RfSrKh": " Summary The paper introduces a new framework called Selfsupervised amodal Video object segmentation (SaVos) that focuses on inferring the complete shape of occluded objects using video temporal sequences. The methodology leverages the temporal information to predict amodal masks and dense object motion. SaVos achieves stateoftheart performance on amodal segmentation benchmarks and demonstrates strong generalization abilities with testtime adaptation.  Strengths  Innovative Approach: SaVos introduces a novel methodology that efficiently utilizes video temporal sequences to infer amodal masks addressing the challenge of amodal segmentation.  Performance: The framework achieves stateoftheart performance on benchmark datasets and exhibits strong generalization capabilities even after distribution shifts through testtime adaptation.  Generalization: SaVos shows robust performance in different scenarios including cases with unseen object types indicating the effectiveness of the spatiotemporal prior it employs.  Thorough Experiments: The paper presents detailed experimental results on various datasets demonstrating the effectiveness of the proposed framework.  Weaknesses  Type Prior Influence: The paper mentions that the model learns type prior during training which can hinder the generalization to unseen types. More insights on mitigating this influence and enhancing generalization would be beneficial.  Clarity: Some sections of the paper could be clarified further especially regarding the architecture details and the specifics of the testtime adaptation process.  Questions and Suggestions for Authors 1. The paper mentions the efficacy of testtime adaptation for enhancing generalization. To what extent does the testtime adaptation impact the models performance and are there any limitations or tradeoffs associated with this approach 2. How does SaVos handle cases where an object remains occluded throughout the video sequence with no visible parts available for completion Can the framework adapt to such scenarios effectively 3. Could the authors provide more insights into the computational efficiency and scalability of SaVos How does the training and inference times scale with larger datasets or more complex video sequences  Limitations and Suggestions The paper could further elaborate on the potential negative societal impact of the proposed framework especially in terms of automation privacy concerns or biases introduced by the model. Providing suggestions for addressing these limitations and ensuring ethical considerations are adequately incorporated would enhance the papers societal implications. Overall the paper presents an innovative framework for amodal segmentation with compelling results and potential for further advancements in the field. Engaging with the above aspects could strengthen the paper and contribute to its broader impact.", "rwyISFoSmXd": " Summary: The paper proposes a novel framework for quantile fairness in algorithmic decisionmaking. It introduces a method to construct fair prediction intervals under the Demographic Parity fairness constraint with respect to sensitive attributes. The approach utilizes optimal transport and functional synchronization techniques to ensure both distributionfree coverage and exact fairness for the prediction intervals. Empirical results demonstrate the models performance on various benchmark datasets.  Strengths: 1. Novel Framework: The paper introduces a unique framework that addresses the crucial issue of quantile fairness in algorithmic decisionmaking which has received limited attention. 2. Theoretical Guarantees: The paper establishes theoretical guarantees for distributionfree coverage and exact fairness for fair quantiles enhancing the credibility of the proposed method. 3. Empirical Performance: The empirical results exhibit the methods effectiveness in reducing discriminatory bias and outperforming existing fairnessoriented methods.  Weaknesses: 1. Technical Complexity: The paper is highly technical and may pose a challenge for readers unfamiliar with optimal transport and functional synchronization techniques. 2. Limited Experiments: While the empirical evaluation showcases promising results more extensive experiments on diverse datasets could strengthen the generalizability of the proposed method. 3. Comparison with Existing Methods: A more detailed comparison with existing fairness algorithms specifically in the context of quantile fairness would provide a clearer assessment of the proposed frameworks superiority.  Questions and Suggestions for Authors: 1. How does the proposed method compare in terms of computational efficiency with existing fairness algorithms 2. Could the authors elaborate on the practical implications of achieving quantile fairness in realworld decisionmaking scenarios 3. It would be helpful to provide insights into the interpretability of the fair prediction intervals generated by the proposed framework. 4. Have you considered the impact of dataset imbalance on the fairness assessment of the model Addressing this issue could enhance the papers completeness.  Limitations: The authors should elaborate on the potential negative societal impacts that may arise from the use of their algorithmic fairness framework and provide suggestions for mitigating such risks.", "pl279jU4GOu": "1) Summary: The paper introduces a novel strategy for proving the convergence of deep learning architectures to zero training or testing loss using Rayleigh quotients. The analysis focuses on Kurdyka\\xc5\\x81ojasiewicz inequalities and provides a unified view for convergence analysis techniques in various neural network architectures and loss functions. The strategy allows for convergence proofs in scenarios where the number of parameters does not tend to infinity extending beyond the overparameterized regime. 2) Strengths:  The paper addresses an important problem in deep learning regarding the convergence of architectures to zero loss using a novel approach.  The analysis of Rayleigh quotients and Kurdyka\\xc5\\x81ojasiewicz inequalities presents a fresh perspective on convergence techniques in neural networks.  The paper provides detailed examples and mathematical proofs to support the proposed strategy for proving convergence. 3) Weaknesses:  The paper could benefit from more concrete application examples to demonstrate the practical implications of the proposed strategy.  The presentation of the mathematical proofs and concepts could be challenging for readers not familiar with the advanced mathematical terminology used. 4) Questions and Suggestions:  Could you provide more intuitive examples or scenarios where the proposed strategy has practical implications in realworld deep learning applications  How does the proposed strategy for proving convergence compare to existing methods in terms of computational efficiency and ease of implementation  Have you considered conducting experiments or simulations to validate the effectiveness and efficiency of the proposed strategy in practical deep learning tasks 5) Limitations: The paper does not extensively address the practical implications and potential societal impacts of the proposed strategy for proving convergence of deep learning architectures. It would be beneficial to include a discussion on these aspects to ensure the ethical considerations of the research. Consider providing insights or suggestions for addressing potential negative societal impacts to enhance the completeness of the study.", "qbSB_cnFSYn": " Summary The paper introduces Differential Equation GAN (DEQGAN) a novel method that leverages generative adversarial networks to solve differential equations in an unsupervised manner. DEQGAN aims to learn the loss function for optimizing the neural network thus addressing the lack of theoretical justification for loss functions in PhysicsInformed Neural Networks (PINNs). The paper demonstrates DEQGANs efficacy on a suite of twelve ordinary and partial differential equations showcasing significant improvements over traditional PINNs with various loss functions.  Strengths 1. Innovative Contribution: DEQGAN is a novel approach that addresses a critical gap in theoretical justification for loss functions in solving differential equations. 2. Improved Performance: The paper shows that DEQGAN outperforms PINNs using L2 L1 and Huber loss functions achieving lower mean squared errors by multiple orders of magnitude. 3. Competitive Accuracy: DEQGAN achieves solution accuracies comparable to popular numerical methods like RK4 and finite difference methods. 4. Robustness Improvement: Techniques like instance noise and residual monitoring are incorporated to enhance training stability and filter out poorperforming runs effectively.  Weaknesses 1. Theoretical Justification: While DEQGAN shows empirical success a more comprehensive theoretical analysis explaining the discriminators role in \"learning the loss function\" could strengthen the methods foundation. 2. Limited Specifics: The paper lacks detailed information on the application of DEQGAN in realworld scenarios or specific use cases where it can be beneficial.  Questions and Suggestions for Authors 1. Theoretical Underpinnings: Can the authors delve deeper into the theoretical aspects of how the GAN discriminator effectively \"learns the loss function\" in the context of solving differential equations 2. RealWorld Applications: Could the authors provide examples or insights on how DEQGAN could be practically applied in scientific or engineering domains beyond the presented experimental results 3. Evaluation Metrics: How does DEQGAN perform in terms of computational efficiency compared to traditional numerical solvers or other neural networkbased methods 4. Potential Extensions: Have the authors considered extensions of DEQGAN to incorporate additional constraints or domainspecific information to enhance its applicability to diverse problem sets  Limitations The authors have adequately addressed potential negative societal impacts and limitations of their work. To further improve providing additional discussion on the scalability and interpretability of DEQGAN results could strengthen the papers conclusions.", "yRhbHp_Vh8e": " Summary: The paper proposes a novel framework VideoWhisperer for Grounded Video Situation Recognition (GVSR) by incorporating spatiotemporal grounding into structured prediction tasks. The model operates in three stages: (1) learning contextualized embeddings for video features and key objects (2) predicting verbrole pairs and entities through crossattention and (3) generating captions. The model achieves stateoftheart results on VidSitu benchmark by significantly improving entity captioning accuracy and enabling verbrole localization without grounding annotations.  Strengths: 1. Innovative Framework: The incorporation of spatiotemporal grounding into structured prediction tasks is a significant advancement in video understanding. 2. Comprehensive Approach: The model operates on a group of events simultaneously predicting verbs verbrole pairs nouns and grounding onthefly allowing for holistic video understanding. 3. Stateoftheart Results: The model achieves substantial improvements in entity captioning accuracy and localization outperforming previous approaches on the VidSitu benchmark. 4. Ablation Analysis: Comprehensive ablation experiments are conducted to analyze the impact of model design choices role embeddings and masked crossattention in the roleobject decoder.  Weaknesses: 1. Role Prediction Variability: The paper highlights large variance in role prediction results across runs indicating potential instability in certain aspects of the model. 2. Imbalance in Verb Prediction: Overfitting issues related to verb prediction and subsequent performance degradation suggest the need for improved strategies to address imbalances in predicting less frequent verbs. 3. Role Disambiguation Challenges: The models limitations in verb and role prediction and disambiguation are mentioned indicating areas for further optimization to enhance caption quality and diversity.  Questions and Suggestions: 1. Role Embeddings: How were the role embeddings designed to capture distinctive role representations and what impact did they have on the models performance 2. Training Stability: What strategies were employed to address the observed variance in role prediction results during training 3. Linguistic Diversity: How does the model handle linguistic diversity in captions and ensure diverse and contextually appropriate role descriptions 4. Future Research Directions: Are there plans to address the limitations mentioned such as enhancing verb prediction balance and improving role disambiguation in future research  Limitations: The current model faces challenges in verb and role prediction variability imbalance issues and may require improvements in disambiguation and caption diversity to enhance overall performance and address potential biases. Additional research may be needed to address these limitations effectively.", "rwdpFgfVpvN": "Summary: The paper introduces a novel RECtified Online Optimization algorithm (RECOO) for online convex optimization with hard constraints. The algorithm is designed to address both fixed and adversarial constraints achieving optimal regret and violation bounds. Theoretical results indicate that RECOO outperforms existing algorithms in terms of regret and hard constraint violation. Experimental results demonstrate the effectiveness of RECOO in various scenarios including job scheduling in distributed data centers. Strengths: 1. Innovative Algorithm: The paper introduces a novel algorithm RECOO specifically designed for online convex optimization with hard constraints filling a gap in the literature. 2. Theoretical Contributions: The paper provides rigorous theoretical analysis and establishes improved regret and violation bounds compared to existing methods. 3. Experimental Validation: Comprehensive experiments demonstrate the superior performance of RECOO in various scenarios showcasing its effectiveness and practicality. Weaknesses: 1. Complexity: The paper is highly technical making it challenging for readers without a strong background in online optimization to grasp the details easily. 2. Limited RealWorld Application Examples: While experiments are conducted they mainly focus on synthetic and specific scenarios (e.g. job scheduling). Realworld applications outside the scenarios tested are not explored. Questions and Suggestions for Authors: 1. Can the authors provide more intuitive explanations or examples to help readers understand the key concepts of the algorithm and theoretical results better 2. How does the performance of RECOO scale with the dimensionality of the problem and the size of the constraint set in practical applications 3. Have the authors considered comparing the algorithm with stateoftheart methods in other related areas of optimization for a more comprehensive evaluation 4. Did the authors consider incorporating additional scenarios or benchmarks to validate the algorithm such as different types of loss functions or varying levels of constraints Limitations:  The paper does not thoroughly address potential limitations or negative societal impacts of the proposed algorithm. Authors are suggested to include a section discussing any constraints or ethical considerations related to the deployment of the algorithm in realworld applications.  Constructive suggestions to improve this aspect include discussing implications for fairness privacy or environmental sustainability in the algorithms deployment.", "lHj-q9BSRjF": " Peer Review: 1) Summary: The paper investigates how large transformerbased models can perform incontext fewshot learning without explicit training for it. The study explores the relationship between the emergent behavior of incontext learning in transformers and the distributional properties of the training data. Through experiments using the Omniglot dataset the authors show that properties like burstiness rarity of classes and dynamic meanings in training data can promote incontext learning. They also examine the role of architecture indicating that transformers outperform recurrent models in exhibiting incontext learning. Furthermore the study suggests that a skewed Zipfian distribution in the training data enables simultaneity of incontext and inweights learning. 2) Strengths and Weaknesses: Strengths:  Wellstructured and coherent paper with clear objectives and methodology.  The experimental design is robust using the Omniglot dataset to investigate the impact of data distribution on incontext learning.  Findings highlight the interplay between data properties and architecture in driving the emergent behavior of incontext learning in transformers.  Experimental results are supported by insightful discussions and implications for various domains. Weaknesses:  The paper could benefit from more detailed comparisons between transformers and recurrent models to understand the limitations of recurrent models in exhibiting incontext learning.  Clarity could be improved in sections describing the experimental design and results.  The paper lacks a thorough discussion on potential alternative explanations for the observed behaviors and the generalizability of the findings to other domains beyond language. 3) Questions and Suggestions for Authors: Questions:  Can you elaborate on how the findings regarding burstiness rarity of classes and dynamic meanings in training data align with existing theories in machine learning and cognitive science  How do the results concerning the Zipfian distribution in training data relate to the potential biases or shortcomings in the existing pretraining strategies for large language models  Have you considered the implications of your findings for transfer learning tasks and the development of more efficient learning algorithms Suggestions:  Provide additional analyses to better understand the impact of different levels of burstiness and label multiplicity on the efficiency of incontext learning.  Emphasize the practical implications of your findings for future model architectures and data collection strategies in language modeling and other domains.  Consider discussing the scalability of the proposed training strategies to larger and more complex datasets outside the scope of the Omniglot dataset. 4) Limitations: The authors adequately address the limitations and potential negative societal impact of their work by suggesting future research directions and highlighting the opportunities presented by nonuniform training data. To enhance the discussion authors could provide concrete strategies to mitigate any drawbacks associated with relying on skewed distributions for training models. Overall the paper provides valuable insights into the emergent behavior of incontext learning in transformers offering a compelling argument for the importance of data properties and architecture in shaping model capabilities. By addressing the raised questions and suggestions the authors can strengthen the impact and applicability of their research findings.", "zyrBT58h_J": " Summary: The paper introduces a novel Sustainable Online Reinforcement Learning (SORL) framework to address inconsistencies between online and offline training in the context of autobidding in advertising systems. The authors propose a safe and efficient online exploration policy (SER) and a variancesuppressed conservative Qlearning (VCQL) method for offline training. Extensive simulations and realworld experiments demonstrate the superiority of their approach over existing autobidding algorithms.  Strengths: 1. Clarity and Structure: The paper is wellstructured and clearly articulates the problem proposed solution and experimental results. 2. Innovative Contribution: The introduction of the SORL framework to address the IBOO challenge in autobidding is a significant and novel contribution. 3. Theoretical Analysis: The paper provides a strong theoretical foundation with the definition of the IBOO formal proofs and analytical expressions making the proposed framework sound and reliable. 4. Experimental Validation: Extensive simulated and realworld experiments validate the effectiveness of the SORL framework and demonstrate its superiority over existing methods.  Weaknesses: 1. Technical Detail Clarity: Some technical terms and mathematical formulations could be challenging for readers unfamiliar with reinforcement learning and online advertising systems. Further elaboration or simplification in these areas could improve accessibility. 2. Comparison and Baselines: While the experimental results demonstrate the effectiveness of the proposed framework a more comprehensive comparison with a diverse set of baselines in the experiments could strengthen the paper.  Questions and Suggestions for Authors: 1. Clarification on Algorithm Design: Could the authors provide more insights into the decisionmaking process behind choosing the specific algorithms (SER and VCQL) for online exploration and offline training respectively 2. Generalization Potential: How scalable is the proposed SORL framework to different types of online advertising systems beyond the specific settings considered in the paper 3. Longterm Evaluation: Have the authors considered conducting experiments with longer time horizons to evaluate the sustainability and performance of the SORL approach over extended periods  Limitations: The paper could benefit from a more thorough discussion on the potential negative societal impacts of automated bidding strategies in the advertising domain to address ethical considerations. Authors should consider including this aspect in future research for a more comprehensive analysis.", "kHrE2vi5Rvs": " Peer Review:  1. Summary: The paper introduces a novel training scheme called Symmetric Neural Combinatorial Optimization (SymNCO) that leverages universal symmetricities in combinatorial optimization (CO) problems to improve the performance of Deep Reinforcement Learning based CO solvers. The paper addresses the challenges in existing Deep RL based CO methods and proposes a regularizationbased training scheme that exploits rotational and reflectional invariance to enhance generalization capabilities without relying on problemspecific domain knowledge or supervised labels. Experimental results on various CO tasks demonstrate the superior performance of SymNCO over existing methods.  2. Strengths and Weaknesses:  Strengths:  Innovative Approach: The utilization of symmetricities to enhance CO solvers is a novel and promising approach.  Experimental Validation: Extensive experiments on various CO tasks substantiate the effectiveness of SymNCO.  Comparison: The paper provides comparative results with existing methods demonstrating significant performance improvements.  Weaknesses:  Clarity in Explanations: Some sections especially regarding the mathematical formulations and loss functions could be challenging to follow without prior knowledge in the field.  Experiment Details: While the results are impressive providing more detailed information on hyperparameters and implementation specifics would enhance reproducibility.  Impact Evaluation: A deeper analysis of the potential societal impacts of advancements in CO automation would enrich the discussion.  3. Questions and Suggestions for Authors:  Clarification on Symmetry Utilization: Could the authors elaborate on the choice of rotational and reflectional symmetries and their potential impact on scalability and applicability to diverse CO problems  Generalization: How does SymNCO perform on unseen problem instances or different problem classes Are there limitations to its generalization capabilities  Efficiency Analysis: Could the authors provide insights into the computational efficiency of SymNCO compared to existing methods especially in terms of memory usage and convergence speed  4. Limitations: The authors have not thoroughly addressed the potential negative societal impacts of their work particularly concerning implications for employment in industries heavily reliant on CO optimization. A more comprehensive discussion on social impacts including strategies to mitigate adverse consequences like job displacement would enrich the paper.  Final Comments: Overall the paper presents a valuable contribution to the field of CO optimization through the novel SymNCO training scheme. The experimental results showcase the effectiveness of the proposed approach. However addressing the identified weaknesses and providing more clarity on certain aspects would enhance the quality and impact of the paper. The work is commendable and has the potential to significantly advance the domain of Deep RLbased CO optimization.", "nOw2HiKmvk1": " Summary: The paper introduces a new method called Learning with Biased Committee (LWBC) for training debiased classifiers without using spurious attribute labels. LWBC employs a committee of biased classifiers to identify and weight biasconflicting data aiming to reduce the impact of biasguiding samples. The method is validated on realworld datasets and outperforms prior arts without spurious attribute labels and even competes with methods relying on bias labels occasionally.  Assessment of Strengths and Weaknesses:  Strengths: 1. Innovative Approach: LWBC introduces a novel approach using a committee of biased classifiers to tackle bias in training data. 2. Robustness and Performance: The method demonstrates superior performance on realworld datasets even outperforming methods using spurious attribute labels. 3. Comprehensive Experiments: The paper includes extensive experiments on various datasets showcasing the effectiveness of the proposed method. 4. Clear Methodology: The algorithm and training process are welldetailed and supported by formal definitions and implementation details.  Weaknesses: 1. Complexity: The method involves multiple steps and modules (committee knowledge transfer selfsupervised learning) which may increase complexity and computational requirements. 2. Evaluation Metrics: While the paper presents a rich set of evaluation metrics further analysis on the interpretability and generalization of the model could enhance the discussion. 3. Randomness in Training: The use of a bootstrapped ensemble for the committee may introduce performance fluctuation particularly demonstrated on the CelebA dataset.  Questions and Suggestions: 1. Clarity on Implementation Details: Could the authors provide more insight into the convergence and stability of the training process particularly regarding the notorious fluctuation observed on the CelebA dataset 2. Impact of Randomness: How might the reliance on randomness in the bootstrapped ensemble affect the reproducibility and generalizability of the method 3. Scalability: How does the performance and efficiency of LWBC scale with increasing dataset sizes and complexities  Limitations: The paper adequately addresses the limitations such as the potential fluctuation in performance due to the randomness in training on the CelebA dataset. To enhance the discussion the authors could provide guidance on mitigating these fluctuations and improving stability.  Conclusion: Overall the paper presents a promising approach in addressing bias without explicit spurious attribute labels. Further clarity on implementation nuances and addressing scalability concerns could strengthen the contributions of the proposed method.", "tglniD_fn9": " Peer Review:  1) Summary: The paper investigated and addressed the issues with Concept Bottleneck Models (CBMs) that cause performance disparities particularly focusing on the leakage problem in soft CBMs. The authors propose modifications to hard CBMs to match the predictive performance of soft CBMs without compromising their resilience to leakage. The two proposed modifications include introducing a sidechannel to capture latent concepts and using an autoregressive configuration in the concept predictor. Empirical results on synthetic and real datasets demonstrate the efficacy of these modifications in improving concept accuracy intervention accuracy and model interpretability.  2) Strengths and Weaknesses:  Strengths:  The paper effectively addresses a critical issue of leakage in soft CBMs and successfully proposes modifications to hard CBMs for improved predictive performance without compromising interpretability.  The experiments on synthetic and realworld datasets provide solid empirical evidence supporting the efficacy of the proposed modifications.  The thorough explanation of the proposed changes along with clear illustrations and algorithms enhances the understanding of the methodology.  Weaknesses:  While the paper effectively addresses the identified issues a more detailed comparison with existing works or benchmarks could provide a stronger context for the proposed modifications.  The complexity of the proposed modifications especially the autoregressive concept predictor might raise concerns about computational efficiency and scalability in realworld applications.  3) Questions and Suggestions for Authors:  Could the authors provide more insights into the generalizability of their proposed modifications across different types of datasets and tasks  How do the proposed modifications impact model training time and computational resources required for inference  Have the authors considered potential challenges in deploying the proposed modifications in realworld applications especially in terms of interpretability and usability for nonexperts  4) Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work by emphasizing the importance of interpretability and intervenability in machine learning models. One constructive suggestion for improvement could be to include a section discussing the practical implications and ethical considerations of implementing the proposed modifications in decisionmaking scenarios. Overall the paper is wellstructured presents valuable contributions to enhancing CBMs and provides a solid foundation for further research in interpretable machine learning models.", "ldRyJb_cjXa": " Review of \"Star Temporal Classification (STC): Learning from Partially Labeled Sequential Data\"  Summary: The paper introduces the Star Temporal Classification (STC) algorithm to handle partially labeled and unsegmented sequential data. STC addresses the issue of missing labels by using a special star token allowing alignments with all possible tokens whenever a token is missing. The authors implement STC using weighted finitestate transducers (WFSTs) and automatic differentiation with the GTN framework. Extensive experiments in automatic speech recognition and handwriting recognition demonstrate STCs effectiveness achieving close performance to supervised baselines even with up to 70 missing labels.  Strengths: 1. Novel Algorithm: Introducing STC to address missing labels in sequential data is innovative and fills a gap in existing approaches. 2. Implementation and Optimization: Utilizing WFSTs and automatic differentiation for efficient computation of gradients is welldesigned. 3. Extensive Experiments: Comprehensive experiments in speech and handwriting recognition validate STCs effectiveness with varying degrees of missing labels. 4. Performance Gains: Demonstrating significant performance gains over baseline systems in both tasks showcases the practical relevance of STC.  Weaknesses: 1. Complexity: The algorithms complexity and the need for additional hyperparameter tuning could be a barrier for some practitioners. 2. Limited Use Cases: The paper focuses on speech and handwriting recognition exploring applications in a wider range of tasks could enhance the generalizability of STC. 3. Comparison Metrics: Additional comparisons against other stateoftheart methods could strengthen the evaluation of STC.  Questions and Suggestions for Authors: 1. Optimization Impact: Can the authors discuss the computational impact of the proposed optimizations in practical training scenarios 2. Generalizability: Have the authors considered testing STC on other sequence transduction tasks beyond speech and handwriting recognition 3. Comparison Benchmarks: It would be valuable to include comparisons against other advanced techniques in weakly supervised learning to highlight the advantages of STC. 4. Hyperparameter Sensitivity: How sensitive is STCs performance to hyperparameters especially the token insertion penalty \u03bb Is there a systematic way to choose optimal values  Limitations: The authors have not adequately discussed potential negative societal impacts of their work. Future research could explore the implications of using weakly supervised methods like STC in sensitive domains and propose mitigation strategies. Overall the paper presents a promising algorithm with practical implications but addressing the mentioned weaknesses and questions would enhance its impact and contribution to the research community.", "x8DNliTBSYY": " PeerReview of Paper: \"The Neural Tangent Kernel: Conditioning in Challenging Sublinear Setups\"  Summary: The paper explores the conditioning of the Neural Tangent Kernel (NTK) in deep neural networks with sublinear layer widths. The key technical contribution is a lower bound on the smallest NTK eigenvalue for deep networks with minimal overparameterization. The results suggest that even with sublinear widths the NTK remains well conditioned enabling memorization and optimization guarantees. The paper goes on to discuss the methodology proof ideas and implications for memorization and optimization in deep neural networks.  Strengths: 1. Significant Contribution: The paper addresses an important but challenging question regarding the behavior of the NTK in deep networks with sublinear layer widths. 2. Novelty: Providing insight into the conditioning of the NTK in such sublinear setups is a novel and valuable contribution to the field. 3. Rigorous Proof: The paper presents a detailed technical proof showcasing a thorough understanding of the problem and the methodology.  Weaknesses: 1. Complexity: The technical nature of the paper might make it less accessible to a wide audience. Clarifications may be needed for nonexperts in the field. 2. Limited Practical Implications: While theoretical insight is crucial the practical implications of the findings could be discussed further for a broader audience.  Questions and Suggestions for Authors: 1. Generalization Performance: Have you considered investigating how the conditioning of the NTK impacts the generalization performance of deep networks with sublinear layer widths 2. Comparison with Existing Methods: How does the proposed methodology compare with existing approaches in terms of computational complexity and practical implementation in deep learning systems 3. Experimental Validation: Have you conducted any practical experiments to validate the theoretical findings in realworld deep learning scenarios 4. Discussion on Computational Efficiency: Considering the complexity of the methodology have you explored ways to optimize the computational efficiency of implementing the proposed approach in practical settings  Limitations and Suggestions for Improvement: The paper could benefit from:  Discussion on the practical implications and applications of the theoretical findings.  Further exploration of the generalization performance implications of the NTK conditioning in deep networks.  Experimental validation to support the theoretical claims and enhance the practical relevance of the research. Overall the paper is valuable for researchers interested in the theoretical aspects of neural network optimization and memorization capacity but further clarity and practical context could enhance its impact in the broader deep learning community.", "xaWO6bAY0xM": " Peer Review Summary: The paper proposes a novel theoretical framework for designing Lipschitz neural networks for achieving certifiably robust classifiers against adversarial examples in the \\xe2\\x84\\x93\\xe2\\x88\\x9e perturbation setting. The work introduces the concept of representing Boolean functions and order statistics within Lipschitz networks to overcome inherent limitations. The paper presents two fundamental impossibility results regarding the expressive power of standard Lipschitz networks and extends the analysis to advanced architectures like GroupSort and \\xe2\\x84\\x93\\xe2\\x88\\x9edistance nets. A unified Lipschitz network called SortNet is developed with a practical version that enables efficient training leading to better certified robustness across multiple datasets. Strengths: 1. Theoretical Contributions: The work offers valuable theoretical insights on the expressive power of Lipschitz networks focusing on \\xe2\\x84\\x93\\xe2\\x88\\x9e robustness and order statistics. 2. Novel Perspective: The papers novel approach of representing Boolean functions within Lipschitz networks provides a fresh angle to address the limitations of standard architectures. 3. Experiments: Extensive experiments across various datasets showcase the scalability efficiency and enhanced certified robustness of SortNet compared to prior works. 4. Clear Writing: The paper is wellstructured clear and provides detailed explanations with references to appendices for rigorous proofs and further details. Weaknesses: 1. Limited Scope: The focus on \\xe2\\x84\\x93\\xe2\\x88\\x9e robustness may limit the broader applicability of the proposed theoretical framework. Consider discussing implications for other norms. 2. Empirical Validation: While the experiments demonstrate improved performance of SortNet further comparisons with more advanced baselines and realworld scenarios could strengthen the empirical evaluation. 3. Complexity: The theoretical depth of the analysis may make certain sections challenging to follow for readers not deeply familiar with Lipschitz networks and Boolean functions. Consider providing more intuitive explanations or examples. Questions and Suggestions: 1. How can the insights gained from this study be extended to address adversarial robustness for different norm settings such as \\xe2\\x84\\x932 2. Have you considered incorporating other Lipschitz bounding methods to enhance robustness certification without sacrificing efficiency 3. What are the practical implications of the proposed SortNet architecture in realworld scenarios especially considering computational resources and overfitting challenges Limitations: The authors might consider extending the discussion to address the generalizability of the proposed framework to different norm settings and the implications for practical applications. Additionally a comparison with stateoftheart methods in challenging realworld scenarios could provide a more comprehensive assessment of the proposed approach.", "owDcdLGgEm": " Review of \"Estimating Poses of CryoEM Images with Group Synchronization\" Summary: The paper focuses on the problem of estimating poses of CryoEM images under the group synchronization framework. The authors introduce a method that improves upon existing multifrequency vector diffusion maps (MFVDM) by leveraging the relative poses in the group O(2) to estimate the absolute poses of the images. They demonstrate the efficacy of their method through experiments on synthetic datasets and offer a thorough theoretical analysis. Strengths: 1. Innovative Approach: The authors introduce a novel method based on group synchronization to estimate poses of CryoEM images addressing a crucial challenge in structural biology. 2. Theoretical Framework: The paper provides a solid theoretical foundation linking cryoEM symmetries to synchronization problems and demonstrating the necessity of O(2) relative poses for accurate pose estimation. 3. Experimental Validation: The authors validate their method through experiments on synthetic datasets showcasing the improved performance of their method compared to existing approaches. 4. Clear Exposition: The paper is wellstructured and clearly written making it accessible to a wide audience of researchers in the field. Weaknesses: 1. Limited Real Data Validation: The method is evaluated primarily on synthetic datasets. While synthetic experiments are valuable for initial testing further validation on real cryoEM data would strengthen the papers practical relevance. 2. Complexity of the Method: The proposed method involves intricate mathematical concepts and computations which might pose challenges for implementation and practical applications without detailed guidance. Questions and Suggestions: 1. Practical Implementation: How feasible is the implementation of the proposed method in realworld cryoEM pipelines Are there any computational bottlenecks or practical limitations that need to be considered 2. Comparison with StateoftheArt: How does the proposed method compare with stateoftheart techniques in terms of accuracy speed and scalability It would be beneficial to provide a more detailed comparison to established methods. 3. Parameter Sensitivity: Have you investigated the sensitivity of the method to parameters such as noise levels dataset sizes or specific structural characteristics A sensitivity analysis would provide insights into the robustness of the approach. Limitations: The authors could enhance the paper by addressing the following limitations: Societal Impact: The paper does not explicitly discuss societal implications or ethical considerations arising from advancements in cryoEM technology. Acknowledging potential negative impacts and suggesting ways to mitigate them would offer a more comprehensive perspective. Overall the paper presents a novel and promising approach to pose estimation in CryoEM images. By addressing the highlighted areas the authors can further strengthen the impact and clarity of their work.", "lIeuKiTZsLY": " Peer Review: 1. Summary: The paper introduces a novel algorithm to identify causal structures in scenarios where latent variables form a hierarchical graph structure. The proposed algorithm efficiently locates latent variables determines their cardinalities and identifies the latent hierarchical structure by leveraging rank deficiency constraints over measured variables. The paper shows that the proposed algorithm can find the correct Markov equivalence class of the whole graph asymptotically under mild restrictions on the graph structure. 2. Strengths:  The paper addresses a challenging scenario of latent hierarchical structures filling a gap in the current causal discovery algorithms.  The proposed algorithm leverages rank deficiency constraints effectively to identify latent variables and reconstruct the causal structure.  The theoretical analysis and proofs provide a solid foundation for the algorithms correctness and asymptotic performance.  The experimental results demonstrate the superiority of the proposed method over existing algorithms on various types of latent graph structures. 3. Weaknesses:  The paper lacks a detailed discussion on the computational complexity and practical implementation challenges of the proposed algorithm.  While the experimental results are promising a more extensive evaluation on larger datasets and realworld applications would strengthen the validity of the proposed method.  The paper could benefit from including a comparative analysis with more stateoftheart algorithms in the field to showcase the uniqueness and robustness of the proposed approach. 4. Questions and Suggestions for Authors:  How does the complexity of the proposed algorithm scale with the size of the dataset and the complexity of the latent hierarchical structure  Have you considered evaluating the algorithm on realworld datasets to test its performance and generalizability in practical scenarios  Can you provide more insights into the algorithms sensitivity to noise in the data and how it handles deviations from the theoretical assumptions  Have you explored potential extensions or modifications to the algorithm to address more complex causal structures beyond the scope of this paper 5. Limitations:  While the paper addresses theoretical aspects and presents experimental results it could further discuss the limitations of the proposed algorithm in handling realworld complexities and practical implementation challenges.  The negative societal impact of the research is ambiguous in the current context. It is essential to ensure the ethical implications of the algorithm and consider potential biases or implications on decisionmaking in applications. Overall the paper presents a significant contribution to the field of causal discovery and latent variable modeling. With further enhancements in practical implementation and realworld validation the proposed algorithm has the potential to have a substantial impact on causal inference in complex systems.", "wiHzQWwg3l": " Peer Review  Summary: The paper introduces a novel approach Extrapolative Continuoustime Bayesian Neural Networks (ECBNN) to augment unsupervised domain adaptation (UDA) for handling nonstationary streaming data in realtime applications. The key contributions include formulating internal predictive modeling as a continuoustime Bayesian filtering problem developing ECBNN to represent temporal dynamics and achieving trainingfree testtime adaptation with low latency. Empirical results demonstrate the effectiveness of ECBNN in aligning features and improving model performance over time.  Strengths: 1. Innovative Approach: The introduction of internal predictive modeling and ECBNN for UDA in handling nonstationary streaming data is innovative and has the potential to address challenges in realtime applications. 2. Theoretical Framework: The paper provides a solid theoretical foundation for the proposed method incorporating continuoustime Bayesian filtering and a novel particle filter differential equation (PFDE). 3. Empirical Results: Empirical results on synthetic and realworld datasets showcase the effectiveness of ECBNN demonstrating superior performance compared to existing methods. 4. Ablation Study: The ablation study on the Streaming Rotating MNIST\u2192USPS dataset validates the contributions of each component in improving performance enhancing the credibility of the proposed approach.  Weaknesses: 1. Lack of Comparison: While the paper compares ECBNN with baselines in the experiments additional comparison with other stateoftheart methods in the UDA domain would strengthen the analysis. 2. Complexity: The proposed approach although innovative may be challenging to implement and apply in practical scenarios due to its complexity. Providing clearer implementation guidelines and potential tradeoffs would be beneficial.  Questions and Suggestions for Authors: 1. Scalability: How does the proposed ECBNN method scale with increasing dataset sizes and dimensions Have you considered the computational efficiency in handling largescale streaming datasets 2. Robustness: How robust is ECBNN to noise and variability in nonstationary streaming data Have you conducted sensitivity analysis to assess the models performance under varying conditions 3. Generalization: Could ECBNN be extended to handle different types of data streams or applications beyond the ones tested in the paper What are the potential limitations and challenges in applying ECBNN to diverse scenarios  Limitations: The authors should provide a more indepth discussion on the limitations and potential negative societal impacts of adopting ECBNN. Considering aspects related to bias fairness and interpretability in realworld applications would enhance the ethical considerations of the proposed approach. Overall the paper presents a novel and promising approach to address realtime streaming data processing challenges using internal predictive modeling and ECBNN. Clearer exposition of practical implications and robustness analysis would further enhance the significance of the work.", "nVV6S2sb_UL": " Summary: The paper introduces MultiRoundSecAgg a framework for secure aggregation in federated learning that focuses on maintaining user privacy over multiple training rounds. The conventional random user selection strategies in federated learning are shown to have privacy leakages over time due to partial user participation. MultiRoundSecAgg addresses this challenge by introducing a structured user selection strategy that provides longterm privacy guarantees while considering fairness and the average number of participating users.  Strengths:  Novelty: Introduces a new metric to capture longterm privacy guarantees which is a significant contribution to the field of federated learning.  Clear Problem Statement: Clearly articulates the challenge of privacy leakages over multiple rounds and provides a wellthoughtout solution.  Experimental Validation: Demonstrates the performance improvement of MultiRoundSecAgg through experiments on various datasets both in IID and nonIID settings.  Theoretical Guarantees: Provides theoretical guarantees for the proposed framework showcasing the robustness and reliability of the approach.  Weaknesses:  Lack of RealWorld Application: While the experiments are thorough more discussion on the realworld applicability and scalability of MultiRoundSecAgg could enhance the paper.  Limited Comparison: While the benchmarks are provided a more indepth comparison with existing stateoftheart algorithms or more diverse baselines could strengthen the evaluation.  Questions and Suggestions for Authors: 1. What further experiments or scenarios could be explored to showcase the adaptability and scalability of MultiRoundSecAgg in realworld settings 2. How does the proposed framework handle outlier users or erratic behaviors to ensure the longterm privacy and stability of the system 3. Have you considered potential computational overhead or communication costs associated with the multiround privacy guarantees If so how does MultiRoundSecAgg manage these aspects effectively  Limitations: The paper does not specifically address the potential negative societal impacts that could arise from implementing secure aggregation frameworks in federated learning systems. Authors could address these by conducting a more explicit ethical analysis and incorporating ethical considerations in the discussion section for a more comprehensive evaluation. They could also suggest ways to mitigate any potential negative consequences that might emerge from the deployment of MultiRoundSecAgg.", "tTWCQrgjuM": "Summary: The paper proposes a novel MCMC framework to perform Bayesian inference from privatized data addressing the intractable likelihood function and doubly intractable posterior distribution issues. The approach augments model parameters with confidential data updating them alternately to enable exact targeting of the posterior distribution. The work is demonstrated on loglinear and linear regression models under differential privacy mechanisms. Strengths: 1. Innovative Approach: The paper introduces a novel MCMC framework to tackle the challenges posed by privatized data providing a flexible solution applicable to a wide range of statistical models and privacy mechanisms. 2. Theoretical Justification: The authors provide theoretical results on computational complexity acceptance rate and mixing properties. The guarantees on the acceptance rate and computational efficiency enhance the credibility of the proposed framework. 3. Simulation Experiments: The simulation results demonstrate the efficacy of the proposed method showcasing the posterior mean estimates and acceptance rates across different privacy levels. Weaknesses: 1. Limited RealWorld Validation: While the simulation results are promising realworld validation on diverse datasets and privacy mechanisms could further strengthen the practical utility and generalizability of the proposed framework. 2. Complexity Concerns: The theoretical assumptions for geometric ergodicity might be restrictive in practical scenarios. More nuanced investigations on relaxing these assumptions could enhance the applicability of the method. Questions and Suggestions for Authors: 1. How does the proposed MCMC framework handle varying dimensions and complexities of statistical models in realworld applications 2. Have you considered sensitivity analyses to evaluate the robustness of the proposed method to variations in privacy parameters and model specifications Limitations: The authors have not fully addressed the potential negative societal impact of their work. To improve they could include a more comprehensive discussion on the ethical implications of dealing with confidential data and the impact of differential privacy mechanisms on data utility and decisionmaking processes.", "rUb6iKYrgXQ": " Summary: The paper introduces Conditional Robust Optimization (CRO) as a novel approach for datadriven decisionmaking under uncertainty. It proposes a framework for learning uncertainty sets in robust optimization that adapt to side information. The contributions include theoretical connections to Contextual ValueatRisk Optimization (CVO) and empirical illustrations showing the performance improvement of contextual robust optimization in a portfolio optimization problem using realworld data.  Strengths: 1. Innovative Approach: The introduction of Conditional Robust Optimization that integrates contextual information in robust optimization is a significant contribution. 2. Theoretical Grounding: The paper establishes theoretical connections between CRO and Contextual ValueatRisk Optimization providing a solid foundation for the proposed method. 3. Empirical Validation: The use of simulated and realworld data to demonstrate the effectiveness of the approach in a portfolio optimization context adds practical value. 4. Detailed Explanations: The paper provides detailed explanations of the algorithms and methodologies proposed making it accessible to readers.  Weaknesses: 1. Limited Comparison: While the comparison against noncontextual robust optimization models is insightful a broader comparison with existing contextual optimization methods could enhance the papers contribution. 2. Complexity: The integration of deep learning and uncertainty sets may present challenges in reproducibility and interpretability which should be addressed. 3. Data Availability: While using realworld data is commendable more details on data preprocessing and availability could improve reproducibility.  Questions and Suggestions: 1. Clarification on Algorithms: Provide more insights into the inner workings of the algorithms proposed for the benefit of readers aiming to implement them. 2. Scalability: Discuss the scalability of the proposed models to handle larger datasets and more complex scenarios. 3. Generalizability: Address how the proposed approach can be generalized to other domains beyond the portfolio optimization example. 4. Ethical Considerations: Include a discussion on potential ethical implications of using side information in decisionmaking and provide suggestions for ethical considerations.  Limitations: The paper could benefit from a more indepth discussion on the limitations and potential negative societal impacts of using contextual information in decisionmaking. Suggestions for improvement could include incorporating fairness considerations causal inference methods and additional ethical considerations.", "lDohSFOHr0": "Paper Summary: The paper proposes a novel Semisupervised Learning (SSL) approach named NACH which aims to address the challenge of classifying both seen and unseen classes when not all classes have labels in the training data. The approach consists of two main modules: unseen class classification and learning pace synchronization. Unseen class classification is achieved by exploiting pairwise similarity between examples while learning pace synchronization involves adjusting the learning pace between seen and unseen classes using an adaptive threshold with distribution alignment. Experimental results on CIFAR10 CIFAR100 and ImageNet datasets demonstrate significant performance improvements in both seen and unseen classes compared to existing SSL and Novel Class Discovery (NCD) methods. Strengths: 1. Novel Approach: The paper introduces a novel SSL approach that addresses the challenge of classifying unseen classes in the absence of labeled data which is a significant contribution to the field. 2. Comprehensive Experiments: The paper provides extensive empirical results on multiple benchmark datasets demonstrating the effectiveness of the proposed approach compared to existing methods. 3. Detailed Analysis: The paper includes detailed analyses of the proposed components such as unseen class classification and learning pace synchronization along with sensitivity analyses of key parameters enhancing the understanding of the methodology. Weaknesses: 1. Theoretical Guarantees: The paper lacks theoretical guarantees for the proposed approach. Adding theoretical analyses could strengthen the justification of the methodology and provide insights into its underlying principles. 2. Limited Benchmarking: While the experiments cover CIFAR10 CIFAR100 and ImageNet datasets additional datasets or realworld applications could further validate the generalizability and robustness of the proposed approach. Questions and Suggestions for Authors: 1. Have you considered exploring the performance of NACH on additional datasets or realworld applications to validate its applicability across diverse scenarios 2. Can you provide more insights into the computational complexity of the proposed approach especially in terms of scalability to larger datasets or models Limitations and Suggestions for Improvement: The paper should provide theoretical guarantees for the proposed approach to enhance the understanding and trustworthiness of the methodology. Authors should also extend their experiments to include a wider range of datasets or realworld scenarios for comprehensive validation. Addressing Negative Societal Impact: Authors should ensure the robustness and generalizability of the proposed approach across different domains to prevent biased decisionmaking in applications that involve unseen classes. Conducting further studies on the societal impact of the methodology and potential ethical considerations would contribute to mitigating any negative consequences in practical deployments.", "nOdfIbo3A-F": " Review: 1. Summary and Contributions: The paper introduces a Lagrangian graph neural network (LGNN) for learning the dynamics of articulated rigid bodies such as ropes chains and trusses. The contributions include topologyaware modeling of rigid bodies generalizability to arbitrary system sizes and the ability to simulate complex unseen topologies. The paper demonstrates the effectiveness of LGNN in simulating articulated rigid bodies and shows promising results in terms of generalization to different system sizes and topologies. 2. Strengths:  The paper addresses an important problem in simulating the dynamics of articulated rigid bodies using a novel approach with LGNN which can have significant implications in multiple applications.  The demonstration of generalizability to unseen complex structures indicates the robustness of the proposed approach.  The empirical evaluation methodology is wellstructured and the comparison with baselines and analysis of model performance provide a comprehensive understanding of the LGNNs capabilities. 3. Weaknesses:  While the paper effectively introduces the LGNN framework detailed insights into the model architecture hyperparameters and experimental setups are scattered across different sections making it challenging for readers to grasp all technical details easily.  The paper mentions zeroshot generalizability but more indepth analysis and explanations on this aspect could enhance the clarity and insight into LGNNs ability to generalize to complex structures.  The discussion on limitations and future works seem brief and could benefit from more elaboration on potential challenges and directions for future research. 4. Questions and Suggestions for Authors:  Could the authors provide a more detailed overview of the model architecture hyperparameters and training setup in a dedicated section to enhance clarity for readers  How does LGNN compare with existing methodologies and traditional physicsbased approaches in terms of computational efficiency and accuracy  Can the authors elaborate on potential challenges and limitations in deploying LGNN in realworld applications particularly concerning realtime simulations and scalability  Are there specific constraints or assumptions in the LGNN framework that might impact the generalization to extremely complex or realtime dynamic systems 5. Limitations:  The paper could benefit from a more robust discussion on potential negative societal impacts or ethical considerations arising from the adoption of LGNN in various applications to ensure responsible AI development and deployment. Overall the paper presents a novel framework with LGNN for learning the dynamics of articulated rigid bodies showcasing promising results in terms of generalizability and simulation accuracy. Clarifications and enhancements in certain sections and further exploration of potential challenges could strengthen the papers impact and comprehensiveness.", "pCrB8orUkSq": "Summary: The paper addresses the discrepancies in existing experimental protocols for dynamic view synthesis (DVS) from monocular videos. It introduces Effective MultiView Factors (EMFs) to quantify the multiview signal present in input sequences and proposes new evaluation metrics to assess model performance more accurately. The paper also presents a new iPhone dataset with diverse reallife deformation sequences for evaluation. Strengths: 1. Original Contribution: The paper offers a systematic analysis of the discrepancies in current experimental protocols for DVS from monocular videos and proposes solutions with EMFs and new evaluation metrics. 2. Novel Metrics: The introduction of covisibility masked image metrics and percentage of correctly transferred keypoints (PCKT) adds depth to evaluating the quality of both rendering and inferred scene deformation. 3. Experimental Rigor: Extensive evaluation on existing datasets and the newly proposed iPhone dataset demonstrates the limitations of current methods and the need for improvement. 4. Practical Relevance: The discussion on reallife challenges faced during casual capture using smartphones adds practical value to the research. Weaknesses: 1. Complexity: The paper delves into intricate technical details and metrics which may be challenging for a broader audience to follow. 2. Lack of Clarity: At times the paper gets technical without providing sufficient explanation or context making it difficult for readers unfamiliar with the topic to grasp the key points. 3. Limited Comparison: While the paper evaluates existing methods on multiple datasets a more extensive comparison with a wider range of stateoftheart approaches could strengthen the analysis. Questions and Suggestions: 1. Clarity on Implementation: Provide more details on the practical implementation of the proposed metrics especially for readers looking to apply them in their own research. 2. Generalizability: Discuss the potential generalizability of the proposed EMFs and metrics to other related tasks in computer vision. 3. Future Research Directions: Explore potential avenues for future research based on the findings especially in terms of improving the generalization and performance of DVS methods in realworld applications. Limitations: The paper should address the potential negative societal impact of promoting more accurate and realistic dynamic view synthesis as it could have implications for issues like deepfakes and misinformation. Authors could address this by incorporating safeguards to prevent misuse and clearly outlining ethical considerations in their work.", "o3HXEEBKnD": " Summary: The paper introduces a method to address the Single Positive MultiLabel Learning (SPML) problem by proposing a LabelAware global Consistency (LAC) regularization. The LAC regularization leverages manifold structure information to enhance the recovery of potential positive labels. This approach outperforms existing methods on benchmark datasets demonstrating stateoftheart performance in solving SPML tasks.  Strengths: 1. Innovative Regularization Approach: The LAC regularization method proposed in the paper is novel and effective for addressing the SPML problem. 2. Thorough Experiments: The paper provides a comprehensive experimental evaluation on multiple benchmark datasets demonstrating the effectiveness of the proposed method over existing approaches. 3. Clear Problem Definition: The paper clearly defines the SPML problem motivating the need for a new regularization method to handle single positive labels in multilabel learning.  Weaknesses: 1. Complexity of Method Description: The papers detailed description of the method including algorithms and mathematical formulations may be challenging for readers to grasp fully without prior knowledge in the field. 2. Lack of Comparative Analysis: While the experiments showcase the effectiveness of the proposed method a more indepth comparative analysis with existing methods could strengthen the paper.  Questions and Suggestions: 1. Clarification on Threshold Selection: How was the threshold value for the pseudolabeling consistency regularization determined and what considerations were taken into account in selecting this threshold 2. Impact of Memory Queue Size: Could the authors elaborate on how the size of the memory queue influences the models performance and provide insights into the optimal size selection based on experiments 3. Extension to Different Dataset Types: Have the authors considered potential applications or modifications of the proposed method to datasets beyond the ones mentioned in the paper to assess its generalizability  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work. Given the thoroughness and novelty of the proposed method the authors have put forth a strong contribution to the field of SPML although further clarity in presenting the method and additional comparative analysis could enhance the papers impact.", "xdZs1kf-va": " Peer Review: Summary: The paper introduces a novel fully decentralized method I2Q for multiagent reinforcement learning addressing the challenges posed by nonstationarity in independent Qlearning. I2Q models ideal transition probabilities based on the optimal conditional joint policy of other agents enabling agents to perform independent Qlearning. The paper provides theoretical proof of convergence and demonstrates the effectiveness of I2Q in various cooperative multiagent tasks. Strengths: 1. Theoretical Contributions: The paper offers a strong theoretical foundation by proving the convergence of I2Q to the optimal joint policy in deterministic environments. The theoretical analyses are rigorously presented adding credibility to the proposed method. 2. Experimental Validation: Empirical results on a range of tasks including matrix games MPEbased differential games MultiAgent MuJoCo and SMAC demonstrate the effectiveness of I2Q in achieving remarkable improvements over baselines. The experiments cover diverse environments adding depth to the evaluation. 3. Innovative Solution: The novel approach of modeling ideal transition probabilities and conducting independent Qlearning in a fully decentralized manner contributes significantly to the MARL research field. I2Qs robustness to nonstationarity is a valuable contribution. Weaknesses: 1. Lack of Comparative Analysis: While the empirical results show I2Q outperforms baselines a more indepth comparative analysis against a wider range of stateoftheart MARL methods would strengthen the evaluation section. This would provide a better context for understanding the competitiveness of I2Q. 2. Societal Impact Discussion: The paper lacks a discussion on the potential societal impact of the proposed method. Considering the wide application of reinforcement learning in various domains a thorough exploration of any negative societal implications or ethical considerations would be beneficial. Questions and Suggestions: 1. Scalability Considerations: How does I2Q scale with increasing numbers of agents in the environment Are there any scalability limitations especially when the complexity of interactions grows with additional agents 2. Hyperparameters Sensitivity: Could the authors elaborate on the sensitivity of I2Q to hyperparameters such as the learning rate batch size or network architecture Providing insights into parameter tuning could be valuable for practitioners. 3. Generalization Capability: How does I2Q perform in environments where the assumptions of deterministic transitions or fully observable states do not hold Are there plans to extend the method to address challenges in stochastic and partially observable environments more effectively Limitations: The authors have not adequately discussed the broader societal implications or limitations of their work. Enhancing the paper by including a section that addresses potential negative societal impacts and limitations of I2Q would offer a more comprehensive perspective. Overall the paper presents a novel decentralized method I2Q with solid theoretical foundations and promising empirical results. Addressing the suggested improvements would further enhance the quality and impact of the work.", "yhZLEvmyHYQ": " Summary: This paper focuses on active learning with Gaussian Processes (GPs) and addresses the biasvariance tradeoff problem in such contexts. The authors propose two new acquisition functions  Bayesian QuerybyCommittee (BQBC) and Query by Mixture of Gaussian Processes (QBMGP)  that leverage the joint posterior of the GP hyperparameters to optimize bias and variance. Through empirical evaluation across six simulators the authors demonstrate that incorporating the biasvariance tradeoff into acquisition functions improves model performance and reduces unnecessary data labeling.  Strengths: 1. Innovative Approach: The paper introduces new acquisition functions that leverage the joint posterior of hyperparameters addressing the biasvariance tradeoff in active learning with GPs. 2. Empirical Validation: The proposed methods are empirically evaluated across multiple benchmark simulators demonstrating improved performance compared to existing acquisition functions. 3. Thorough Explanation: The paper provides detailed explanations on Gaussian Processes biasvariance tradeoff and acquisition functions making it accessible to readers unfamiliar with the topic.  Weaknesses: 1. Limited Theoretical Analysis: While the empirical results are compelling more indepth theoretical analysis explaining the mechanisms driving the improvement in model performance would strengthen the paper. 2. Evaluation Metrics: The paper lacks a standard metric for comparing overall performance across different simulators making it challenging to quantitatively assess the superiority of the proposed acquisition functions.  Questions and Suggestions for Authors: 1. Theoretical Underpinnings: Could the authors provide additional insights into how the joint posterior of hyperparameters in GPs impacts the biasvariance tradeoff theoretically 2. Comparative Analysis: How do the new acquisition functions compare against stateoftheart methods in active learning with GPs and are there specific scenarios where BQBC or QBMGP outperform traditional methods  Limitations: The authors could further address the limitations through:  Explore computational efficiency improvements for fully Bayesian GPs to handle scalability issues.  Investigate the impact of heteroscedastic noise on the performance of the proposed acquisition functions.  Disclosure of Funding: The authors should transparently acknowledge the funding received for this work as highlighted in the acknowledgment section. The paper addresses an important problem in active learning with GPs and presents innovative solutions. By addressing the weaknesses and providing clarifications the authors can enhance the papers quality and impact.", "xI5660uFUr": "PeerReview: 1. Summary: The paper introduces a novel selective compression method called SCR for deep learningbased variablerate image compression. The method involves generating 3D importance maps adjusting them for different target quality levels and converting them into 3D binary masks to determine essential representation elements for compression. The method is seamlessly integrated into existing compression models enabling variablerate compression with comparable efficiency and reduced decoding time. 2. Strengths:  The paper addresses the issue of model complexity in variablerate image compression methods and proposes a solution that is efficient and effective.  The SCR method is wellmotivated and theoretically sound providing a systematic approach to selective compression.  Extensive experimental results support the claims made in the paper demonstrating comparable compression efficiency and reduced decoding time.  The proposed methods integration with existing compression models with minimal overhead showcases its practical applicability. 3. Weaknesses:  The paper lacks a detailed discussion on the computational complexity of the proposed SCR method which is crucial for understanding its scalability.  The evaluation metrics used in the experiments primarily focus on PSNR and MSSSIM while additional metrics such as visual quality assessments could provide a more comprehensive analysis. 4. Questions and Suggestions:  How does the proposed SCR method compare in terms of encoding time efficiency compared to other variablerate compression methods  Can the authors provide insights into the generalization capabilities of the SCR method across different types of images and content  How does the proposed method handle potential issues such as artifacts or distortions that may arise during compression at different quality levels 5. Limitations: The authors adequately addressed the limitations and potential negative societal impact of their work considering the overall focus on compression efficiency and reduced decoding time. However future research could explore energy consumption implications of the proposed compression method and its integration into realworld applications. In conclusion the paper presents a novel approach to variablerate image compression through selective compression of representation elements. With strong theoretical foundations and promising experimental results the SCR method shows potential for practical implementation in image compression systems. Addressing the minor weaknesses and exploring the suggested questions would further enhance the robustness and applicability of the proposed method.", "tz1PRT6lfLe": " Peer Review  Summary: The paper presents a novel unified framework named SoteriaFL that addresses the challenge of enhancing communication efficiency in private federated learning while incorporating communication compression techniques. The framework combines various local gradient estimators such as stochastic gradient descent SVRG and SAGA along with a shifted compression scheme to achieve better performance tradeoffs in terms of privacy utility and communication complexity.  Strengths: 1. Novel Framework: The proposed SoteriaFL framework is innovative in integrating privacypreserving techniques with communication compression for federated learning. 2. Comprehensive Analysis: The paper provides a detailed theoretical analysis of the framework exploring the tradeoffs between privacy utility and communication efficiency. 3. Performance Comparisons: The comparisons with prior works and the detailed analysis of the proposed algorithms in terms of privacy guarantees utility and communication complexity provide valuable insights.  Weaknesses: 1. Clarity: The technical content can be challenging for readers unfamiliar with the subject matter. Improved clarity in explaining concepts and algorithms could enhance accessibility. 2. Evaluation: While the theoretical analysis is thorough empirical validation through experiments on realworld datasets could strengthen the papers claims and provide practical insights. 3. Implementation Details: More concrete examples or implementation details of the proposed algorithms within the SoteriaFL framework could aid in understanding how to apply them in practice.  Questions and Suggestions: 1. How does the performance of the proposed SoteriaFL framework compare with existing stateoftheart techniques in practical implementations or realworld scenarios 2. Can the authors elaborate on the challenges or computational overhead associated with implementing the proposed algorithms in realworld federated learning environments 3. How scalable is the SoteriaFL framework in largescale federated learning systems with a diverse range of clients and datasets Any insights on handling such scalability challenges  Limitations: The authors have provided a comprehensive analysis of the frameworks performance tradeoffs and technical details. However empirical validation through experiments on diverse datasets can further enhance the papers credibility and provide practical insights into the proposed algorithms realworld applicability.", "kI_kL5vq6Oa": " Review of \"A RiskDriven Approach to Design Perception Systems for Autonomous Systems\"  Summary: The paper introduces a riskdriven approach to designing perception systems for autonomous systems considering the downstream effect of perceptual errors on system safety. The proposed framework is applied to a visionbased aircraft detect and avoid system demonstrating a 37 reduction in collision risk compared to a baseline system. The work leverages concepts from risksensitive domains like finance and robotics to quantify risk and guide the design of safer perception systems.  Strengths: 1. Innovative Approach: The paper introduces a novel riskdriven methodology for designing perception systems addressing a critical aspect of safetycritical autonomous systems. 2. Rigorous Methodology: The inclusion of risk functions risksensitive loss functions and riskdriven data generation techniques showcases a comprehensive and systematic design methodology. 3. Realistic Application: The evaluation on a visionbased aircraft detect and avoid system demonstrates practical applicability and relevance to realworld autonomous systems.  Weaknesses: 1. Complexity: The papers technical depth may pose challenges for readers unfamiliar with concepts such as Conditional Value at Risk and Markov Decision Processes. A clearer explanation or additional illustrative examples may enhance readability. 2. Limited Evaluation: While the 37 reduction in collision risk is significant the evaluation could benefit from comparisons with more diverse benchmarks or scenarios to strengthen the generalizability of the proposed approach. 3. Validation: The study lacks extensive validation or empirical testing beyond simulationbased results. Realworld testing or comparative studies could enhance the credibility of the proposed methodology.  Questions and Suggestions: 1. Model Assumptions: Can the authors elaborate on the assumptions underlying the Markov property in their perception systems How might nonMarkovian systems impact the efficacy of the proposed riskdriven methodology 2. Scalability: How scalable is the proposed riskdriven approach to perception system design Are there computational constraints or tradeoffs in applying this methodology to largerscale or more complex systems 3. Ethical Considerations: Given the critical safety implications have the authors considered any ethical implications or unintended consequences of deploying risksensitive perception systems in realworld autonomous systems  Limitations: The paper does not directly address the societal impact or potential negative implications of deploying riskdriven perception systems. Authors could consider discussing ethical considerations and social implications along with recommendations for mitigating any harmful outcomes. Overall the paper presents a pioneering approach to enhance the safety of autonomous systems through riskdriven perception design. Addressing the identified weaknesses and incorporating the suggestions could further strengthen the research contribution and its practical applicability.", "xL8sFkkAkw": " PeerReview:  Summary: The paper introduces a novel differentiable quantity GradCosine for evaluating the initial state of a neural network. The proposed Neural Initialization Optimization (NIO) algorithm aims to find a better initialization that is agnostic of architecture types. The approach is demonstrated to enhance the classification performance of various neural architectures on CIFAR10 CIFAR100 and ImageNet even showing improvements for large vision Transformer architectures without warmup.  Strengths: 1. Innovation and Contribution: The proposed GradCosine quantity and the NIO algorithm address a critical gap in the automated exploration of neural network initialization methods. 2. Theoretical Foundation: The paper provides a strong theoretical foundation for the derived quantity GradCosine. The analysis of samplewise optimization landscapes and the theoretical insights enhance the credibility of the proposed approach. 3. Experimental Validation: The experimental results on different datasets and architectures demonstrate the effectiveness of NIO in improving classification performance compared to existing initialization methods. 4. Thorough Explanations: The paper offers detailed explanations theoretical proofs and algorithmic descriptions to ensure clarity and ease of implementation of the proposed methods.  Weaknesses: 1. Limitations of the Study: The paper acknowledges the sensitivity of the hyperparameters and the computational resources required for accurate calculation of GradCosine. Providing more guidance on selecting optimal hyperparameters and improving the efficiency of the computation would be beneficial. 2. Comparison with Baselines: While the paper demonstrates the superiority of NIO over existing initialization methods in terms of performance improvements a more comprehensive comparison with a wider range of existing methods could enhance the robustness of the evaluation. 3. Generalizability: Though the experiments cover various architectures and datasets further validation on additional realworld applications or benchmarks will strengthen the generalizability of the proposed approach.  Questions and Suggestions for Authors: 1. Hyperparameter Sensitivity: Have you conducted sensitivity analysis for the hyperparameters in the NIO algorithm Suggestions for efficiently determining the optimal values 2. Comparison Metrics: Could additional metrics or benchmarks be considered to further evaluate the impact of GradCosine and NIO on training dynamics and model performance 3. Scalability: How does the computational overhead scale with increasing model complexity or dataset size Any strategies for improving scalability while maintaining performance  LimitationsImprovements:  The paper could benefit from discussing the potential negative societal impacts of automated techniques in neural network design and initialization. Suggestions include considering fairness privacy concerns and ethical implications in future works.  Conclusion: The paper presents an innovative approach that fills a significant gap in automated neural network initialization. The strong theoretical foundation coupled with promising experimental results positions the proposed GradCosine and NIO algorithm as a valuable contribution to the field of automated machine learning and neural network design. Addressing the highlighted weaknesses and limitations will further enhance the impact and applicability of the proposed methods.", "yfNSUQ3yRo": " Summary: The paper introduces Noise Attention Learning (NAL) to address the challenge of label noise in deep neural networks. NAL incorporates an attention branch to distinguish mislabeled samples from clean ones preventing the model from memorizing mislabeled data. The proposed method is shown to outperform stateoftheart approaches on various datasets in noisy label scenarios.  Strengths: 1. Innovative Approach: NAL introduces a novel method to tackle label noise by dynamically assigning attention weights to samples based on the predictive power of representations. 2. Simplicity and Universality: The paper emphasizes the simplicity and effectiveness of NAL making it a promising tool in realworld applications. 3. Theoretical Justification: The theoretical analysis presented in the paper supports the effectiveness of NAL in preventing memorization of mislabeled samples and maintaining gradient dominance of clean samples.  Weaknesses: 1. Lack of Comparison: The paper does not compare NAL with some stateoftheart methods like RoCL and DivideMix which might limit the comprehensive evaluation of the proposed approach. 2. Complexity: While NAL is presented as a simple method the incorporation of additional components such as target estimation might add complexity requiring careful parameter tuning.  Questions and Suggestions for Authors: 1. Method Comparisons: Can you provide a more detailed comparison against stateoftheart methods like RoCL and DivideMix to showcase the competitiveness of NAL across various noise scenarios 2. Target Estimation Quality: How does the estimated targets accuracy evolve over training epochs and can you discuss their impact on model performance further 3. Scenarios with Hard Samples: Could you delve deeper into how NAL handles hard samples during training and how it differentiates between hard clean samples and mislabeled samples effectively  Limitations: The authors have adequately discussed the limitations and societal impact of their work. However to further enhance the paper it could be beneficial to provide more insights into the computational cost and scalability of NAL in largescale applications. Overall the paper presents a promising approach with strong theoretical backing and empirical evidence. Addressing the suggested points can further enrich the analysis and validation of NALs effectiveness in handling label noise in deep learning applications.", "m2JJO3iEe_5": " Peer Review Summary: The paper introduces a novel extension of randomized smoothing a provable defense against adversarial attacks to fewshot learning models that map inputs to normalized embeddings. The authors analyze the Lipschitz continuity of such models and derive a robustness certificate against `2bounded perturbations for fewshot learning scenarios. Strengths: 1. The paper fills a gap in the field by providing the first theoretical robustness guarantee for fewshot learning classification. 2. The analysis of Lipschitz continuity and derivation of robustness certificates provide a solid theoretical foundation for the proposed defense. 3. The experiments on different datasets validate the theoretical results and demonstrate the effectiveness of the approach. 4. The detailed description of the algorithms and theoretical proofs enhances the clarity and reproducibility of the work. Weaknesses: 1. While the theoretical guarantees are valuable the paper could benefit from further discussion on the practical implications and limitations of the proposed defense system. 2. The computational cost of the certification procedure per image especially the significant standard deviation should be addressed to improve the efficiency of the method. 3. The limitations section could be expanded to discuss potential challenges in realworld application and to suggest avenues for future improvements. Questions and Suggestions: 1. Could the authors provide more insights into how the proposed defense system could be integrated into practical applications or realworld scenarios 2. Can the authors elaborate on any assumptions made during the theoretical analysis and their potential impact on the practical implementation 3. How does the proposed approach compare to existing defense mechanisms in terms of computational efficiency and scalability 4. Considering the significant standard deviation in computational time per image are there strategies to optimize the certification procedure for faster inference without compromising accuracy Limitations: The authors have not adequately addressed the computational complexity and efficiency of the certification procedure. It is recommended to optimize the algorithms to reduce standard deviation and improve scalability for practical applications.", "wJwHTgIoE0P": " Peer Review: Summary: The paper presents a novel approach to learning image representations using synthetic data through a large dataset of twentyone thousand short code snippets that generate diverse synthetic images using OpenGL. The dataset is used for supervised and unsupervised representation learning showing a significant reduction in the gap between pretraining with real and procedurally generated images. The contributions include a study on scaling performance with the number of generative image programs achieving stateoftheart performance for representation learning and analyzing the properties of good image generation programs. Strengths: 1. Innovative Approach: The paper introduces a unique methodology using procedural image programs for representation learning filling a gap in existing literature. 2. Experimental Rigor: The paper provides a detailed experimental evaluation comparing different methodologies showcasing the effectiveness of the proposed approach. 3. Scalability: The scalability of performance with the number of generative programs is well analyzed providing valuable insights. 4. Availability of Resources: The provision of code models and datasets on a public platform adds to the reproducibility and transparency of the research. Weaknesses: 1. Clarity of Presentation: The paper could benefit from a more structured presentation especially in explaining the methodological details and experimental setups. 2. Evaluation Metrics: While the paper uses various metrics a more thorough discussion on the choice of metrics and their implications on results interpretation would enhance the paper. 3. Comparative Analysis: Although the paper compares performance with existing methods more detailed comparative analysis and discussions on the limitations of the proposed approach would strengthen the contribution. 4. Generalizability: The generalizability of the findings beyond the specific datasets and tasks studied could be further discussed. Questions and Suggestions: 1. Can you elaborate more on the practical implications of using procedural image programs for realworld applications or specific use cases where this approach could be particularly beneficial 2. How does the proposed methodology perform on datasets with more complex image structures or in tasks requiring detailed object recognition 3. Have you considered exploring the interpretability of the models learned representations and how they correspond to features in the procedural image programs Limitations and Suggestions for Improvement: The authors should further address the potential negative societal impact of promoting the use of synthetic data over realworld data emphasizing the importance of ensuring the generalizability and ethical considerations of models trained solely on procedural images. In conclusion the paper presents a promising approach to representation learning using procedural image programs. Addressing the identified weaknesses and limitations would enhance the impact and robustness of the research findings. The unique dataset and methodology introduced have the potential to advance the field of image representation learning.", "mMdRZipvld2": " Summary: The paper proposes a method to address the confounding effect of input similarity on representation similarity metrics (RSA CKA) for neural networks. By regressing out the input similarity the deconfounded similarity measures are shown to better capture functional similarities between networks especially in crossdomain scenarios and transfer learning tasks. The method improves consistency of similarity metrics and correlation with outofdistribution accuracy.  Strengths: 1. Novel Methodology: Introducing a simple yet effective approach to address a critical confounding issue in similarity metrics used for neural networks. 2. Comprehensive Evaluation: The paper provides thorough experimental validation demonstrating consistent improvement in various scenarios. 3. Impactful Results: The proposed method enhances the interpretability of neural network representations and aids in understanding model behaviors particularly in transfer learning.  Weaknesses: 1. Assumptions: The linear separability assumption between confounder and representation similarity may limit the applicability in complex scenarios. 2. Open Questions: The paper acknowledges limitations with deeper layers and suggests further exploration to improve functional consistency in these cases. 3. Complexity: The computational complexity especially with the PSD approximation may impact scalability in largescale applications.  Questions  Suggestions: 1. Further Diversification: Can the method be tested on more diverse datasets and network architectures to assess its generalizability 2. Nonlinear Confounders: How can the method be adapted to handle nonlinear confounding effects to improve the applicability to deeper layers 3. Domainspecific Considerations: Have you considered scenarios where the input similarity structure holds relevant information for certain applications to avoid overdeconfounding 4. Visualization: Including visualizations of the representation similarities before and after deconfounding could enhance the clarity of the improvements.  Limitations Addressed: The paper adequately addresses limitations by acknowledging assumptions and potential complexity issues. Constructive suggestions for further exploration are provided to enhance the methods robustness and applicability. Overall the paper presents a significant advancement in addressing confounding effects in representation similarity metrics for neural networks. The methods potential impact on improving model interpretability and transfer learning applications is commendable.", "uytgM9N0vlR": " Summary: The paper investigates the efficacy of graph neural network (GNN) pretraining on small molecular data focusing on selfsupervised pretraining objectives. The authors conduct thorough ablation studies to assess the impact of pretraining objectives data splitting methods input features pretraining dataset scales and GNN architectures on downstream task accuracy. Key findings include the lack of statistically significant advantages of selfsupervised pretraining over nonpretrained methods the marginal improvement with additional supervised pretraining and the impact of hyperparameters on downstream task accuracy.  Strengths: 1. Thorough Experimentation: The paper presents detailed experiments and ablation studies covering various aspects of graph pretraining offering a comprehensive analysis of the impact of different factors. 2. Clear Presentation: The paper is wellorganized with clear sections and tables summarizing experimental setups and results making it easy to follow the methodology and outcomes. 3. Insightful Findings: The findings provide valuable insights into the effectiveness of graph pretraining for molecular representation and highlight the importance of factors such as supervised pretraining features and data splits.  Weaknesses: 1. Limited Impact Demonstration: While the paper provides thorough analyses the actual impact of the research findings on realworld applications or advancements in AIdriven drug discovery may not be clearly emphasized. 2. Potential Confounding Factors: The paper acknowledges limitations in study coverage such as focusing on small molecule graphs and specific pretraining objectives which could affect the generalizability of the conclusions.  Questions and Suggestions for Authors: 1. Further Exploration: Have the authors considered testing the proposed pretraining methodology on a wider range of molecular datasets to enhance the generalizability of the findings 2. RealWorld Relevance: How do the observed results translate to potential applications in drug discovery or other domains It could be beneficial to discuss practical implications based on the findings. 3. Impact of Data Diversity: Given the importance of data diversity did the authors explore the diversity of molecular structures in the pretraining datasets and its impact on downstream task performance  Limitations: The authors have adequately identified and addressed the limitations in their study design and coverage. To enhance the practical relevance and robustness of the findings it is suggested to consider exploring a broader range of datasets architectures and learning objectives in future research.  Suggestions for Improvement: 1. Practical Application Focus: Include discussions on the practical implications of the findings for drug discovery or related applications. 2. Diverse Dataset Evaluation: Consider testing the methodology on diverse molecular datasets to evaluate the robustness of the proposed approach. 3. Generalization Enhancement: Explore a wider range of graph architectures and pretraining objectives to provide a more comprehensive understanding of graph pretraining effectiveness.", "qmy23tNBvbh": " Summary: The paper introduces the use of kernel exponential and deformed exponential family densities for continuous attention mechanisms. The authors investigate the advantages of these densities their theoretical properties and their application to realworld datasets. The experiments show that kernel attention mechanisms outperform unimodal attention particularly highlighting the benefits of kernel sparsemax. The paper provides insights into time warping attention mechanisms and density estimation contributing to the field of continuous attention architectures.  Strengths: 1. Theoretical Contributions: The paper provides new theoretical results for kernel exponential and deformed exponential families showcasing the robustness and flexibility of these densities. 2. Experimental Results: The experiments demonstrate the superior performance of kernel attention mechanisms particularly kernel sparsemax over unimodal approaches across various datasets showcasing the practical utility of the proposed method. 3. Novelty and Innovation: The introduction of kernel exponential and deformed exponential family densities for continuous attention mechanisms presents a novel contribution to the field providing fresh insights and potential extensions for future research.  Weaknesses: 1. Numerical Integration: The reliance on numerical integration for the computations might pose scalability issues with higher dimensionality prompting further investigation into more efficient computational methods. 2. Generalization: While the experiments show promising results additional datasets and varied experimental conditions could further validate the generalizability and robustness of the proposed method.  Questions and Suggestions: 1. Generalization: Have you considered testing the proposed method on a wider range of datasets to assess its performance across diverse applications 2. Efficiency: Could you explore more computationally efficient methods for computations especially with scalability concerns in highdimensional spaces 3. Comparison: How does the proposed method compare to stateoftheart attention mechanisms in terms of computational efficiency and performance metrics on similar datasets  Limitations: The authors should investigate the scalability and efficiency of numerical integration as the dimensionality of the data increases to ensure the practical applicability of the proposed method in realworld scenarios.  Constructive Suggestions for Improvement: 1. Conduct additional experiments on diverse datasets to strengthen the generalizability of the findings. 2. Explore more efficient computational methods to address scalability concerns with higherdimensional data. 3. Compare the proposed method against existing stateoftheart attention mechanisms to highlight its advantages in terms of performance and computational efficiency.", "x7S1NsUdKZ": "1) Summary: The paper introduces the Adaptive Sampling for Discovery (ASD) problem focusing on sequential decisionmaking to maximize the sum of responses by adaptively labeling points from an unlabeled dataset. The proposed informationdirected sampling (IDS) algorithm is analyzed theoretically and tested through simulations and realdata experiments for discovering chemical reaction conditions. 2) Strengths: a. Originality: The ASD problem and proposed IDS algorithm address an important and novel challenge in adaptive sampling for discovery. b. Theoretical Analysis: Rigorous formulation and theoretical guarantees for IDS under linear graph and lowrank models are significant contributions. c. Experimental Validation: The demonstration of IDS through simulation studies and realdata experiments adds value to the papers contributions. d. Comparison with Existing Literature: Thorough comparisons with existing bandit and active learning literature enhance the papers context and relevance. e. Structured Presentation: The paper is wellorganized with clear sections dedicated to problem formulation algorithm design theoretical analysis experiments and discussions. 3) Weaknesses: a. Clarity on Algorithms: The description of the IDS algorithm could be more detailed to aid readers in understanding its workings. b. Experiments Section: Some details on the exact experimental setups parameter choices and performance metrics used in the simulation studies and realdata experiments could provide more clarity. c. Discussion on Limitations: The paper could benefit from a more detailed discussion on the potential limitations of the proposed approach and how these limitations impact the practical applicability of IDS. 4) Questions and Suggestions: a. Have you evaluated the scalability and computational efficiency of the IDS algorithm especially in largescale realworld applications b. How does IDS compare with other stateoftheart adaptive sampling algorithms in discovery tasks c. How sensitive is the performance of IDS to changes in hyperparameters and structural assumptions in each model setting 5) Limitations: The paper adequately addresses the limitations and societal impact. Overall the paper makes significant contributions to the field of adaptive sampling for discovery highlighting the importance of sequential decisionmaking in realworld discovery problems. Clarifications on the IDS algorithm and further exploration of experimental setups could enhance the papers impact and relevance.", "wmwgLEPjL9": "Summary: The paper challenges the prevailing notion in multitask learning (MTL) research that specialized multitask optimizers (SMTOs) outperform unitary scalarization in training neural networks for multiple tasks. The authors present a comprehensive experimental evaluation of various SMTOs on popular MTL benchmarks in supervised and reinforcement learning settings. Surprisingly the results show that unitary scalarization matches or outperforms SMTOs in terms of performance despite being simpler and more computationally efficient. Strengths: 1. Comprehensive Evaluation: The paper presents a thorough experimental evaluation of popular SMTOs on various MTL benchmarks in both supervised and reinforcement learning settings. This extensive evaluation provides valuable insights into the performance of different optimization techniques. 2. Surprising Results: The finding that unitary scalarization can compete with or outperform SMTOs challenges the conventional wisdom in the field of MTL. This unexpected result adds novelty and interest to the study. 3. Technical Analysis: The paper includes a technical analysis of the behavior of different SMTOs shedding light on their potential roles as regularizers. This analysis provides a deeper understanding of the mechanisms at play in MTL optimization. Weaknesses: 1. Limited Hyperparameter Tuning: The authors mention that they conducted experiments under limited compute resources which might have affected the extent of hyperparameter tuning for some methods. Acknowledging this limitation and its potential impact on the results could strengthen the paper. Questions and Suggestions: 1. Hyperparameter Tuning: Have you considered conducting more extensive hyperparameter tuning for all methods to ensure fairness in comparison It might be beneficial to explore the full potential of each optimizer through thorough optimization. 2. Generalization to Other Domains: How do you think the findings of this study might generalize to other domains or datasets outside of the ones tested Have you considered exploring the transferability of these results to diverse MTL applications 3. Impact of Regularization Techniques: Could you delve deeper into the specific regularization techniques used in conjunction with unitary scalarization to achieve competitive performance and contrast them with the regularization mechanisms embedded in SMTOs Limitations: The authors could further enhance the papers quality by acknowledging potential limitations concerning the limited hyperparameter tuning conducted and the need for more indepth exploration to confirm the robustness of the findings across various MTL scenarios.", "vbPsD-BhOZ": " Summary: The paper introduces a novel approach using cellular sheaf theory in the context of Graph Neural Networks (GNNs). It addresses the issues of poor performance in heterophilic graphs and oversmoothing behavior in GNNs by showing the importance of the underlying geometry of the graph. The paper provides theoretical insights into how sheaf structures can improve classification tasks and introduces a new type of model called Sheaf Neural Diffusion (SND) that learns the underlying sheaf from data.  Strengths: 1. Theoretical Contribution: The paper provides a strong theoretical foundation by leveraging cellular sheaf theory to address key issues in GNNs. This unique approach contributes significantly to the field of graph representation learning. 2. Empirical Validation: The experimental results demonstrate the effectiveness of the proposed models on both synthetic and realworld datasets showcasing competitive performance in heterogeneous graph settings. 3. Novelty and Originality: The paper introduces a novel concept of learning sheaf structures in the context of GNNs and provides indepth theoretical analysis on the relationship between sheaf geometry and graph classification tasks.  Weaknesses: 1. Complexity and Accessibility: The paper delves into sophisticated algebraic topology concepts which might make it challenging for readers without a deep understanding of the field to comprehend the content. Providing more intuitive explanations could enhance the accessibility of the work. 2. Generalization: The study mainly focuses on theoretical analysis and empirical validation. Exploring the generalization properties of the proposed models could further strengthen the papers contributions. 3. Ambiguity: Some sections could benefit from clearer explanations particularly in connecting theoretical concepts to practical implementation for a broader audience.  Questions and Suggestions: 1. The paper mentions using sigmas and lambdas to control the asymptotic behavior of the models. Could the authors elaborate more on the significance and interpretation of these parameters in the context of sheaf diffusion models 2. How does the choice of the sheaf structure impact computational efficiency and scalability in practical implementations Any insights on the tradeoffs between model complexity and computational resources would be valuable. 3. Considering the application of sheaf structures beyond graph classification tasks how versatile are these models for other graphrelated problems such as link prediction or graph generation  Limitations: The authors have not explicitly addressed the limitations and potential negative societal impacts of their work. It would be beneficial to include a discussion on any limitations or ethical considerations in future iterations of the paper.", "mkEPog9HiV": " Summary: The paper introduces a novel Neural Sewing Machine (NSM) for structurepreserving 3D garment modeling. It addresses the challenges of generic garment modeling by using sewing patterns as a prior for accurate representation of garment structures. NSM achieves stateoftheart results in 3D garment modeling tasks including reconstruction and manipulation.  Strengths: 1. Innovative Approach: The introduction of NSM utilizing sewing patterns for encoding 3D garment structures is novel and effective. 2. Comprehensive Evaluation: Extensive experiments demonstrate the superiority of NSM in diverse garment shapes and topologies reconstruction and manipulation over existing methods. 3. Structured Learning: The proposed structurepreserving losses ensure accurate representation of garment topology and shape enhancing model performance. 4. Applicability: NSMs capability for controllable garment editing and the interpretability of the sewing pattern embeddings make it highly applicable in garment design and virtual tryon applications.  Weaknesses: 1. Complexity: The models complexity with multiple loss functions and components may introduce challenges in implementation and finetuning. 2. Limited Comparison: The comparison with only three stateoftheart methods for singleview reconstruction restricts the thorough benchmarking of NSM against a wider range of existing techniques.  Questions and Suggestions: 1. Longterm Robustness: How does the proposed NSM adapt to diverse and irregular panel structures seen in some cultural garments 2. Generalization: How well does NSM generalize to unseen garment categories or shapes not included in the training dataset 3. Computational Efficiency: Are there strategies to streamline the modeling process and reduce computational overhead without compromising performance  Limitations: The authors should address the challenges in modeling garments with irregular panels and ensure the models adaptability to diverse cultural garments to enhance its applicability.  Recommendation for Improvement: 1. Include a more comprehensive comparison with a wider array of existing methods to establish a stronger benchmark. 2. Consider future work on adapting NSM to handle irregular panel structures for broader applicability in cultural garments. 3. Further investigate strategies for enhancing computational efficiency without compromising the models performance.", "m7CmxlpHTiu": " PeerReview of \"Selfsupervised Aggregation of Diverse Experts for Testagnostic Longtailed Recognition\"  Summary: The paper presents a novel approach Selfsupervised Aggregation of Diverse Experts (SADE) designed to address the challenge of testagnostic longtailed recognition. The task involves handling models trained on longtailed data and evaluated on various test class distributions that differ from the training set distribution. SADE consists of two main strategies: skilldiverse expert learning and testtime selfsupervised aggregation. The method is theoretically grounded and empirically validated to improve performance on both vanilla and testagnostic scenarios.  Strengths: 1. Innovative Approach: SADE introduces a novel method for handling testagnostic longtailed recognition addressing practical challenges. 2. Theoretical Analysis: The paper provides a sound theoretical analysis of the proposed selfsupervised aggregation strategy. 3. Empirical Validation: Extensive experiments demonstrate the effectiveness of SADE outperforming stateoftheart methods in different scenarios. 4. Rigorous Experimental Setup: The study compares SADE with various baselines across different datasets ensuring comprehensive evaluation. 5. Practical Impact: This work has potential application in realworld scenarios where test distributions may not align with training distributions.  Weaknesses: 1. Limited Evaluation Datasets: While the paper covers four benchmark datasets expanding the evaluation to more diverse datasets could enhance the generalizability of the findings. 2. Complexity and Interpretability: The method involves multiple components that could potentially make it challenging for adoption and understanding by practitioners. 3. Reproducibility: While the source code is provided additional details on hyperparameters tuning and model architectures could further facilitate reproducibility.  Questions and Suggestions: 1. Evaluation on Diverse Datasets: Could the authors provide insights into how the method might perform on datasets with different characteristics or domains to assess its generalizability 2. Sensitivity Analysis: Have the authors conducted sensitivity analysis on key hyperparameters to illustrate the robustness of the approach 3. Comparison with Transfer Learning: How does SADE compare to transfer learning approaches for handling distribution shifts in unseen test data distributions  Limitations: The paper does a commendable job analyzing limitations but providing additional details on the potential societal impact and ethical considerations of the proposed method could further enhance the contribution. In conclusion the paper presents a significant advancement in the field of longtailed recognition offering a novel and effective approach for handling testagnostic scenarios. Addressing the identified weaknesses and open questions could further strengthen the impact and applicability of the proposed method.", "kUOm0Fdtvh": " Summary: The paper introduces AdaFocal a calibrationaware adaptive focal loss that improves the calibration of neural networks during training. It addresses the limitations of fixed \u03b3 values in focal loss by dynamically adjusting them based on the models underoverconfidence. AdaFocal outperforms regular focal loss and other calibration techniques in various image and text classification tasks achieving better calibration while maintaining accuracy. The paper also explores AdaFocals effectiveness in outofdistribution detection.  Strengths: 1. Novel Contribution: The paper introduces a novel technique AdaFocal to address the calibration problem in neural networks which performs consistently better than existing methods. 2. Thorough Evaluation: The method is extensively evaluated on various datasets model architectures and tasks showcasing its effectiveness in improving calibration without sacrificing accuracy. 3. Detailed Explanations: The paper provides a clear explanation of the proposed method the reasoning behind it and the experimental results making it easier to understand and replicate the findings.  Weaknesses: 1. Complexity: The method introduces several new concepts and parameters potentially increasing the complexity of implementation and understanding particularly for those not wellversed in the field. 2. Evaluation Metrics: While the paper evaluates the method on various metrics a deeper analysis on the computational efficiency or memory requirements compared to existing methods would further strengthen the evaluation.  Questions  Suggestions for Authors: 1. Clarification on Hyperparameters: Could the authors provide more insights into how the hyperparameters were selected particularly \u03bb \u03b3minmax and Sth Did they perform sensitivity analysis on these parameters 2. Generalization: How does AdaFocal perform on datasets or tasks outside the scope of image and text classification Have there been any experiments on different domains to demonstrate its generalizability 3. Comparison with StateoftheArt: It would be beneficial to include a more detailed comparison with stateoftheart calibration methods to showcase the competitive advantages of AdaFocal.  Limitations  Suggestions: The authors should provide a more thorough discussion on the potential negative societal impact of their work particularly in applications like autonomous driving or medical diagnosis where calibration of neural networks is crucial. Constructive suggestions for improvement could include conducting ethical impact assessments or addressing potential biases in the dataset used for training AdaFocal.", "thirVlDJ2IL": " Summary: The paper addresses the problem of learning mixtures of spherical Gaussians with contributions including the development of an efficient algorithm that learns the means in low dimensions almost pinning down the critical separation threshold for efficient parameter learning. The authors show the necessity of the separation threshold and extend their results to more general mixtures of distributions.  Strengths: 1. Novel Contribution: The paper addresses an important problem and provides a new algorithm with promising results. 2. Clarity: The paper is wellstructured and provides detailed explanations to aid understanding. 3. Technical Depth: The paper demonstrates a deep understanding of the problem utilizing Fourier analysis and careful algorithm design. 4. Theoretical Grounding: The work is wellgrounded and builds upon existing literature showing an advancement in the field.  Weaknesses: 1. Complexity Concerns: While the algorithm is efficient in some scenarios the complexity may still be a concern in certain cases. 2. Practical Applicability: The focus on theoretical aspects might limit the practical applicability of the proposed algorithm without realworld validation.  Questions and Suggestions for Authors: 1. Can you provide insights into the scalability of the algorithm for large datasets 2. How do you plan to validate the algorithms performance with real dataset experiments 3. Have you considered the impact of outliers and noise on the algorithms performance How robust is it 4. Can you discuss potential applications or realworld scenarios where this algorithm could be beneficial  Limitations: The authors could address the limitations of the work by:  Explicitly mentioning the range of applicability and potential constraints.  Discussing scenarios where the algorithm might not perform optimally.  Offering insights into areas for future research and improvement. Overall the paper makes a significant contribution to the field and addressing the above points could further enhance the clarity and impact of the work.", "vriLTB2-O0G": " PeerReview:  1) Summary: The paper introduces a novel learningbased method for approximating the entire Pareto set in multiobjective Bayesian optimization (MOBO) for expensive optimization problems. The proposed Pareto Set Learning (PSL) method aims to provide decisionmakers with a more comprehensive overview of the Pareto front allowing for flexible tradeoff analysis. The methodology involves developing a scalarization method designing a set model and using Gaussian process models to learn the Pareto set. Experimental results on synthetic and realworld problems demonstrate the effectiveness of the proposed method in comparison to existing MOBO approaches.  2) Strengths and Weaknesses: Strengths:  Innovative approach addressing the limitation of existing MOBO methods by approximating the whole Pareto set.  The paper is wellstructured clearly presenting the methodology and experimental results.  Comprehensive comparison with existing MOBO algorithms demonstrates the superiority of the proposed method.  The inclusion of realworld problem applications strengthens the practical relevance of the research. Weaknesses:  The novelty of the PSL method could be further emphasized through a detailed comparative analysis with existing Pareto set approximation techniques.  The limitations or challenges encountered during the experimentation could have been discussed to provide a more balanced perspective.  Detailed explanation and justifications for parameter choices and settings could enhance the reproducibility and robustness of the study.  3) Questions and Suggestions:  Can the authors elaborate on the computational complexity of the proposed PSL method compared to traditional MOBO approaches  How sensitive is the performance of PSL to variations in the hyperparameters such as the number of preferences sampled and the batch size  It would be beneficial if the authors can discuss potential scenarios or problem types where PSL may not be as effective as expected.  4) Limitations: The authors have made a comprehensive analysis of the limitations and potential societal impacts of their work. One constructive suggestion for improvement could be to explicitly acknowledge potential biases or uncertainties in the decisionmaking process that may arise from utilizing an approximation of the whole Pareto set.  Overall Comments: The paper provides a significant contribution by introducing a novel methodology Pareto Set Learning (PSL) for approximating the whole Pareto set in multiobjective Bayesian optimization problems. The experimental validation on synthetic and realworld problems strengthens the validity and relevance of the proposed approach. Addressing minor weaknesses and suggestions for improvement can further enhance the papers impact and robustness.  Recommendation: Based on the strengths novelty and potential impact of the proposed PSL method I recommend this paper for acceptance after addressing the minor revisions and feedback provided.", "tVHh_vD84EK": " Peer Review of \"Universal SpatioTemporal Sequence Forecasting Framework with Automated Search\"  Summary: The paper introduces a Universal SpatioTemporal Sequence Forecasting Framework with an Automated Search strategy. It addresses the challenge of modeling spatiotemporal dependencies in forecasting tasks by proposing a unified modeling framework named UniST and an automatic structure search strategy called AutoST. The framework consists of Attentionbased network backbones with three distinctive modules: S2T Layer (spatial first) T2S Layer (temporal first) and STS Layer (spatiotemporal synchronous). The contributions include proposing replaceable and unified attentionbased modeling units extensive experiments demonstrating performance superiority and an automated search strategy for optimal spatiotemporal modeling priority.  Strengths: 1. Innovation: The proposed UniST framework and AutoST strategy address a critical issue in spatiotemporal forecasting offering a novel approach that outperforms existing methods. 2. Clear Structure: The paper effectively introduces the problem presents solutions with detailed methodology and provides comprehensive experiments to validate the proposed models. 3. Thorough Experiments: Extensive experiments on realworld datasets demonstrate the effectiveness and superiority of the proposed methods over baseline approaches.  Weaknesses: 1. Clarity in Explanations: Certain technical descriptions especially in the methods section may be challenging for readers to follow without a strong background in the subject matter. Simplifying complex concepts for better understanding would be beneficial. 2. Validation Methods: While the experiments show strong results a deeper analysis of the limitations or edge cases where the proposed method may falter could enhance the paper\u2019s credibility. 3. Comparison Metrics: While the performance of the proposed methods is demonstrated with MAE and RMSE additional metrics could provide a more comprehensive evaluation of the models.  Questions and Suggestions for Authors: 1. Clarification on Search Strategy: How does the AutoST strategy determine the optimal modeling priority and what criteria are used to select the best configuration 2. Impact of Node Quantity: Have you considered the performance impact of varying the number of nodes in the AutoST cells search space How does the complexity of the DAG affect the final results 3. Scalability Concerns: Given the complexity of the proposed framework how scalable is it for deployment in realworld applications with largescale datasets  Limitations: The authors have not fully addressed the potential negative societal impact of their work. To improve consider discussing the energy consumption implications of training complex neural networks and offer suggestions for reducing environmental impact. Overall the paper presents a novel approach with significant potential in the domain of spatiotemporal sequence forecasting. By addressing the highlighted weaknesses and incorporating the suggestions provided the authors can further enhance the impact and clarity of their work.", "ohk8bILFDkk": "Peer Review:  Summary: The paper proposes a novel type of Markov decision process named semiinfinitely constrained Markov decision process (SICMDP) which extends the traditional CMDPs to handle a continuum of constraints. A reinforcement learning algorithm called SICRL is devised to solve SICMDPs by transforming the problem into a linear semiinfinitely programming (LSIP) problem. The authors present theoretical analysis for SICRL including sample complexity and iteration complexity bounds and provide numerical examples to validate the proposed model and algorithm. Strengths: 1. Novel Contribution: Introducing SICMDPs expands the applicability of reinforcement learning to scenarios with a continuum of constraints filling a gap in existing CMDP frameworks. 2. Theoretical Analysis: The paper provides detailed theoretical analysis including sample complexity and iteration complexity bounds enhancing the understanding of SICRLs performance guarantees. 3. Concrete Examples: The numerical experiments conducted on toy SICMDP and discharge of sewage illustrate the practicality and effectiveness of the proposed model and algorithm. 4. Clarity: The paper is wellstructured and explains complex concepts in a clear and coherent manner making it accessible to readers. Weaknesses: 1. Algorithm Limitation: The SICRL algorithm is limited to the tabular case with a predefined offline dataset which restricts its applicability in more practical and general reinforcement learning scenarios. 2. Societal Impact: The paper lacks a discussion on the potential societal impact of the proposed SICMDP model and SICRL algorithm which is essential given the increasing concerns about the ethical implications of AI technologies. Questions and Suggestions for Authors: 1. Generalization Beyond Tabular Case: How do you plan to extend the SICRL algorithm to handle nontabular cases where an offline dataset might not be sufficient Are there plans to explore deep reinforcement learning approaches for SICMDPs 2. RealWorld Applications: Have you considered realworld applications of the proposed SICMDP model and SICRL algorithm beyond the presented examples Providing more diverse application scenarios could strengthen the papers practical relevance. 3. Comparison with StateoftheArt: How does the performance of SICRL compare with existing stateoftheart algorithms for constrained reinforcement learning problems Including comparative analyses with other approaches would add depth to the evaluation of SICRL. Limitations: The authors have not thoroughly addressed the potential negative societal impacts of their work. To improve they could incorporate a discussion on ethical considerations bias mitigation strategies and implications for marginalized communities in future research.  This peer review provides a thorough evaluation of the strengths weaknesses and suggestions for the authors offering constructive feedback to enhance the papers quality and address potential areas of improvement.", "wQ2QNNP8GtM": " Review of the Paper \"Cross Aggregation Transformer for Image Restoration\" 1. Summary: The paper introduces a novel Transformerbased image restoration model called Cross Aggregation Transformer (CAT). The core innovation of CAT is the RectangleWindow SelfAttention (RwinSA) mechanism which utilizes horizontal and vertical rectangle window attention to expand the attention area and aggregate features across different windows. The paper also introduces the AxialShift operation for enhanced window interactions and the Locality Complementary Module to incorporate the inductive bias of CNN into Transformer. Extensive experiments demonstrate superior performance of CAT in various image restoration tasks. 2. Strengths:  Innovative Approach: The proposed RwinSA and AxialShift operations effectively expand the receptive field and enhance feature aggregation across windows without a significant increase in computational complexity.  Comprehensive Experiments: The paper provides extensive experiments demonstrating the superiority of CAT in image restoration tasks over stateoftheart methods both quantitatively and visually.  Availability of Code: The availability of code and models on GitHub enhances reproducibility and allows for further research in this area. 3. Weaknesses:  Clarity and Structure: The papers technical details can be difficult to follow especially regarding the intricate mechanisms like RwinSA and AxialShift operations. Improved clarity in explanations could benefit readers.  Comparison Metrics: While the paper compares CAT with several existing methods a more detailed discussion on how CAT performs under different scenarios or limitations could enhance the evaluation section. 4. Questions and Suggestions for the Authors:  Clarification: Can the authors provide more insights into the specific scenarios where CAT might struggle compared to other methods  Impact of Parameters: How sensitive is CATs performance to variations in key parameters like attention head number window sizes or network depth  Scalability: How does the proposed CAT model scale with extremely highresolution images or large datasets in terms of performance and computational requirements 5. Limitations:  Addressing Limitations: The authors could further emphasize addressing potential negative societal impacts such as environmental concerns due to increased computational complexity and suggest mitigation strategies in future work. Overall the paper presents a promising approach with notable contributions in enhancing Transformer models for image restoration tasks. Strengthening the clarity of technical explanations and providing further insights into model behavior under diverse conditions can enhance the papers impact and understanding.", "rH-X09cB50f": " Summary The paper explores the effectiveness of supervised learning (SL) and selfsupervised learning (SSL) for pretraining in Crossdomain fewshot learning (CDFSL). It investigates the impact of domain similarity and fewshot difficulty on the performance of pretraining schemes. Additionally two pretraining schemes mixedsupervised learning (MSL) and twostage learning are proposed and evaluated on various benchmark datasets.  Strengths 1. Empirical Investigation: The paper conducts extensive experiments and analyses on multiple datasets to provide insights into CDFSL pretraining. 2. Novel Findings: It uncovers new insights such as the advantage of SSL on the target domain over SL on the source domain based on domain similarity and fewshot difficulty. 3. Thorough Comparison: The comparison of various SSL methods and their performance in different scenarios adds depth to the analysis. 4. Novel Pretraining Schemes: The introduction of mixedsupervised learning and twostage pretraining schemes contributes to the advancement of CDFSL methodologies.  Weaknesses 1. Limited Generalization: The findings may be constrained to the specific datasets and models used in the study. 2. Assumptions: The paper assumes predefined domain similarity and fewshot difficulty metrics which might not capture the full complexity of realworld scenarios. 3. Clarity: Some sections particularly the definitions and methodologies could be more clearly explained for better understanding by readers. 4. Evaluation Metrics: The paper could benefit from discussing the choice of evaluation metrics and their implications on the conclusions drawn.  Questions and Suggestions for Authors 1. Data Size Impact: Have you considered how varying the size of the unlabeled target data affects the performance of SSL and SL pretraining schemes 2. Hyperparameter Sensitivity: How sensitive are the findings to the hyperparameters chosen especially in the MSL scheme 3. Comparison with Metalearning: How do the proposed pretraining schemes compare with traditional metalearning approaches in CDFSL 4. Realworld Applications: How do you envision implementing the proposed schemes in practical realworld scenarios with large domain differences  Adequacy of Addressing Limitations The paper adequately addresses the limitations and negative societal impacts of the study by posing questions aimed at strengthening generalizability and practical applications.  Suggestions for Improvement To enhance the paper further authors can consider: 1. Including a discussion on the computational efficiency of the proposed pretraining schemes. 2. Conducting experiments on more diverse and realworld datasets to validate the proposed findings. 3. Providing additional insights into the interpretability of the learned representations through visualizations or case studies. Overall the paper contributes substantially to the understanding of CDFSL pretraining strategies but improvements in clarity generalizability and practical implications could enhance its impact.", "s6ygs1UCOw1": " Summary: The paper introduces a novel algorithm for constructing probably approximately correct (PAC) prediction sets in the context of meta learning. These prediction sets capture uncertainty via sets of labels and can be quickly adapted to new tasks with only a few training examples. The proposed algorithm satisfies the PAC guarantee tailored to the meta learning setting and is evaluated on four datasets across three application domains. It is shown to have smaller prediction set sizes compared to other baselines.  Strengths: 1. Innovative Approach: The paper introduces a novel algorithm for PAC prediction sets tailored to the meta learning setting. 2. Application to Diverse Datasets: The algorithm is tested on four datasets across three different application domains showcasing its versatility. 3. Theoretical Foundation: The paper provides a robust theoretical foundation incorporating concepts from conformal prediction and learning theory. 4. Experimental Results: The results presented demonstrate the efficacy of the proposed algorithm in providing accurate prediction sets with smaller sizes.  Weaknesses: 1. Complexity: The paper delves into technical details and formalisms that might be challenging for readers unfamiliar with the subject matter. 2. Evaluation: While the results show the effectiveness of the proposed algorithm more comparative analysis with existing methods could further strengthen the evaluation.  Questions and Suggestions: 1. Validation of Prediction Sets: How does the proposed algorithm handle imbalanced classes when constructing prediction sets 2. Scalability: How does the algorithm scale with increasing dataset sizes in terms of computational complexity and performance 3. Interpretability: Can the authors provide insights into the interpretability of the prediction sets generated by the algorithm for endusers 4. Extensions: Are there any potential extensions or variations of the algorithm that could further enhance its performance or applicability  Limitations: The paper adequately addresses potential negative societal impacts of the work. However the authors should consider providing clearer guidelines or disclaimers for users to prevent misuse of the guarantees provided by the algorithm.", "kxXvopt9pWK": " Peer Review of \"Denoising Diffusion Restoration Models for Image Restoration\"  1) Summary: The paper introduces Denoising Diffusion Restoration Models (DDRM) an efficient method for solving general linear inverse problems in image restoration. DDRM leverages pretrained denoising diffusion generative models and variational inference to sample from the posterior distribution demonstrating superior performance in image restoration tasks such as superresolution deblurring inpainting and colorization especially on the ImageNet dataset.  2) Assessment of Strengths and Weaknesses: Strengths:  Novel Approach: Introduces an innovative method DDRM that efficiently addresses general linear inverse problems in image restoration.  Empirical Results: Demonstrates superior performance compared to existing methods in terms of reconstruction quality runtime and generalization on various image restoration tasks.  Efficiency: Shows significant speed improvements over nearest competitors making it a promising solution for realworld applications.  Versatility: Applies DDRM across different image datasets and tasks with impressive results. Weaknesses:  Complexity: The methods technical complexity may pose challenges for adoption and implementation by a wider audience.  Limited Comparison: Some comparisons with alternatives like OneNet are missing which could provide a more comprehensive assessment of the proposed method.  Reproducibility: While the code availability is mentioned more details on the reproducibility of results would enhance the papers credibility.  3) Questions and Suggestions for Authors: Questions:  How does DDRM perform compared to methods like OneNet especially in terms of noise handling in measurements  Could the authors provide insights into the computational resources required for implementing DDRM in realworld scenarios Suggestions:  Providing a more detailed comparison with OneNet and possibly other methods to highlight DDRMs unique advantages.  Enhancing the reproducibility section by including information on datasets hyperparameters and model training specifics.  4) Limitations: The paper adequately addresses limitations and potential negative societal impacts by showcasing the efficiency and generalizability of DDRM. However to further improve the authors could include discussions on the potential computational overhead of DDRM for largescale image datasets and suggest ways to mitigate it.  Overall Evaluation: The paper presents a novel and efficient solution DDRM for general linear inverse problems in image restoration. It offers significant advancements over existing methods in terms of speed and quality. Addressing the suggested points would enhance the clarity and impact of the work.", "x5ysKCMXR5s": " Peer Review: 1) Summary of the Paper: The paper introduces a practical algorithm for sparse principal component analysis (PCA) on incomplete and noisy data using a semidefinite program (SDP) relaxation approach. The algorithm aims to recover the true support of the sparse leading eigenvector of an unknown matrix even when observing incomplete and noisy versions of it. The paper provides theoretical conditions for exact recovery and validates the results through synthetic and gene expression datasets. It addresses the gap in sparse PCA research for incomplete data with a focus on applications like singlecell RNA sequence analysis. 2) Strengths and Weaknesses: Strengths:  Novel approach utilizing SDP relaxation for sparse PCA on incomplete data.  Theoretical justification for exact support recovery and experimental validation.  Clear presentation of theoretical results model assumptions and experimental methodologies.  Realworld application on gene expression data demonstrating practical relevance. Weaknesses:  Limited discussion on computational complexity and scalability of the proposed algorithm.  Lack of comparison with more recent methods beyond DTSPCA and ITSPCA.  Insufficient exploration of the impact of different noise levels and missing data patterns. 3) Questions and Suggestions: Questions:  How does the proposed SDPbased algorithm compare with stateoftheart sparse PCA methods designed for complete data  Have you considered the robustness of the algorithm to varying levels of noise and missing data patterns  Is there a tradeoff between model accuracy and computational cost especially in highdimensional settings Suggestions:  Discuss the generalizability of the proposed approach to other domains beyond gene expression analysis.  Provide insights on the interpretability of the sparse PCA results and how they can aid in data analysis.  Consider incorporating discussions on the algorithms convergence properties and potential extensions. 4) Limitations: The authors have adequately addressed the limitations and potential negative societal impacts of their work particularly highlighting the need for further exploration in deterministic missing data patterns and potential scalability issues. To enhance this aspect they could include a dedicated section discussing limitations and societal implications explicitly. In conclusion the paper presents a significant contribution to the field of sparse PCA for incomplete data offering a novel algorithmic approach supported by rigorous theoretical analysis and practical validation. Further enhancements in scalability robustness to noise and generalizability could strengthen the impact of the research.", "qOgSCLE5E8": " Summary: The paper addresses the challenge of fewshot classification focusing on calibrating the distribution of samples from base classes to novel classes to improve model generalization. The proposed method Hierarchical Optimal Transport (HOT) learns an adaptive weight matrix to transfer statistics from base classes to novel samples for improved performance. The experiments demonstrate the effectiveness of HOT in achieving superior results compared to existing methods in both standard and crossdomain fewshot classification tasks.  Strengths: 1. Innovative Methodology: The introduction of HOT as a novel approach for distribution calibration in fewshot learning is a significant and innovative contribution to the field. 2. Hierarchical Approach: The hierarchical structure of the method with a highlevel OT and lowlevel OT brings a principled and adaptive way to transfer statistics capturing correlations effectively. 3. Experimental Results: The experimental results demonstrate the superiority of HOT over existing methods showcasing its effectiveness in both standard and crossdomain fewshot classification tasks.  Weaknesses: 1. Limited Comparison: The direct comparison with only FreeLunch leaves room for a more comprehensive evaluation by comparing with a wider range of stateoftheart methods in fewshot learning to provide a more complete understanding of the proposed methods performance. 2. Complexity: The complexity introduced by the HOT framework might make it challenging to interpret and apply in practical scenarios. Providing more insights on the interpretability and computational efficiency could enhance the paper.  Questions and Suggestions: 1. Interpretability: Can the authors provide more insights into the interpretability of the learned transport plan and how it influences decision making in fewshot classification tasks 2. Scalability: How does the proposed method scale with larger datasets and a higher number of classes Have experiments been conducted on datasets with more classes to assess scalability and generalization to more diverse scenarios 3. Robustness: How robust is the proposed method to noise and outliers in the data Have sensitivity analyses been performed to understand the performance under varying conditions  Limitations: The paper could benefit from a more detailed discussion on the potential limitations and negative societal impacts of their work. It is essential to address ethical considerations and potential biases that could arise from deploying these methods in realworld applications providing suggestions for mitigating such challenges.", "pcgMNVhRslj": " Summary: The paper introduces Alignmentguided Temporal Attention (ATA) as a solution to the dilemma between the sufficiency and efficiency of interactions in temporal modeling for video tasks. ATA uses framebyframe alignments to increase mutual information between neighboring frame representations thereby improving taskrelevant information extraction without adding modelspecific design. Extensive experiments showcase the superiority and generality of ATA on various benchmarks highlighting its potential as a plugandplay module for action recognition tasks.  Strengths: 1. Theoretical Grounding: The paper provides a strong theoretical foundation linking the increase in mutual information through framebyframe alignments to enhanced taskrelevant information extraction. 2. Innovative Concept: The idea of utilizing alignments to guide temporal attention offers a novel approach to resolving the efficiencysufficiency conflict in video temporal modeling. 3. Performance Results: Extensive experiments demonstrate the superiority of ATA over traditional methods showcasing its effectiveness and generality across multiple benchmarks.  Weaknesses: 1. Practical Implementations: The paper lacks a detailed discussion on the practical implementation challenges of the proposed ATA method such as the computational cost of framebyframe alignments on largescale video datasets. 2. Comparison with StateoftheArt: While the paper compares ATA with existing ViTbased approaches it could benefit from a broader comparison with a wider range of stateoftheart methods in the field of video action recognition. 3. Scalability: The scalability of ATA to more complex video tasks or larger datasets remains unclear and warrants further investigation.  Questions and Suggestions: 1. Algorithm Scalability: Have you tested the scalability of ATA on larger video datasets with more complex temporal patterns 2. Comparative Analysis: Can you provide a more detailed comparison with stateoftheart methods beyond ViTbased approaches to highlight the superiority of ATA in a broader context 3. Practical Considerations: How practical is the implementation of framebyframe alignments on realworld video datasets Have you considered strategies to mitigate computational overheads in largerscale applications  Limitations: The paper does not thoroughly address the potential limitations and societal impact of the proposed method. To enhance the discussion it would be beneficial to consider the ethical implications and potential negative impacts of advancing video learning technologies. It would be constructive to include recommendations for addressing these aspects in future research.", "uxc8hDSs_xh": " Review of \"Geometric Scattering Graph Neural Networks for Maximum Clique Problem Approximation\"  Summary: The paper proposes a geometric scatteringbased GNN for approximating solutions to the maximum clique (MC) problem a wellknown NPhard problem in graph combinatorial optimization. The method addresses the oversmoothing issue faced by conventional GNNs by incorporating a scattering transform. The proposed approach outperforms representative GNN baselines in terms of solution accuracy and speed. A twophase strategy is employed including an unsupervised learning model and a constraintpreserving decoder. The paper provides a detailed overview of the theoretical framework the model components and experimental results on various datasets.  Strengths: 1. Theoretical Framework: The paper introduces a novel approach to solve the maximum clique problem by leveraging the geometric scattering transform within a GNN architecture. 2. Innovative Methodology: The incorporation of scattering transform to alleviate the oversmoothing problem is a novel contribution that enhances the performance of traditional GNNs. 3. Experimental Results: Empirical results demonstrate the effectiveness of the proposed method outperforming existing GNN baselines in terms of accuracy and speed.  Weaknesses: 1. Complexity: The paper contains intricate technical details especially in the model architecture and functionality which might make it challenging for some readers to grasp the concepts easily. 2. Evaluation: While the empirical results show improvements over existing methods a more detailed comparison with additional baselines or stateoftheart solvers could strengthen the evaluation.  Questions and Suggestions for Authors: 1. Could the authors elaborate on the computational efficiency of their method compared to traditional solvers like Gurobi in terms of large scale datasets or realworld applications 2. How does the proposed model handle scalability concerns when applied to even larger and more complex graphs beyond the datasets used in the experiments 3. Have the authors considered the robustness and generalization capabilities of their method across diverse graph structures and problem instances  Limitations: The authors have adequately discussed the limitations and societal impact of their work particularly focusing on the technical aspects of GNNs and graph combinatorial optimization problems. To improve the paper could include a section discussing potential ethical implications of applying advanced algorithms like GNNs in realworld scenarios. Overall the paper presents a significant contribution to utilizing geometric scattering in GNNs for solving NPhard problems with compelling experimental results. Further clarity on the methodology additional evaluations and considerations about scalability would enhance the papers impact.", "pELM0QgWIjn": " Summary: The paper presents the design of quasiNewton methods for stronglyconvexstronglyconcave saddle point problems. The proposed random Broyden family updates demonstrate local superlinear convergence rates. The paper also introduces specific Broyden family algorithms with BFGStype and SR1type updates showcasing faster local convergence. Numerical experiments suggest that the proposed algorithms outperform classical firstorder methods in various machine learning applications.  Strengths: 1. Novelty: Introducing random Broyden family updates for saddle point problems is a unique approach. 2. Theoretical Rigor: The paper provides detailed theoretical analyses convergence proofs and convergence rates for the proposed algorithms. 3. Relevance: Addressing saddle point problems which are crucial in various machine learning applications makes the study practically significant. 4. Experimental Validation: Conducting numerical experiments on realworld datasets to validate the proposed algorithms adds credibility to the study. 5. Clear Structure: The organization of the paper into sections with notation preliminaries methods and numerical experiments ensures clarity and easy understanding for readers.  Weaknesses: 1. Complexity: The paper with its extensive theoretical underpinnings and derivations may be challenging for readers less familiar with optimization theory. 2. Clarity: Some parts of the paper could benefit from clearer explanations and more intuitive examples to aid in understanding. 3. Practical Implementation: While the proposed algorithms show promise theoretically practical implementations and computational complexities are not deeply discussed in the paper. 4. Discussion: The discussion could be expanded to address potential realworld applications and implications of the proposed methods.  Questions and Suggestions for Authors: 1. Practical Implementation: How would the proposed algorithms be practically implemented considering the computations involved in approximating the Hessian matrix 2. Comparison: Can the authors further elaborate on how their algorithms compare to existing stateoftheart methods in saddle point optimization 3. Scalability: Have the authors considered the scalability of their algorithms to larger datasets and higher dimensions 4. Error Analysis: Is there an analysis of potential errors or stability concerns when applying the proposed methods to noisy datasets 5. Future Work: What future research directions do the authors envision based on the findings of this study  Limitations: The paper lacks a detailed discussion on the practical implementation aspects and the computational complexities involved. Providing insights into addressing these limitations could enhance the applicability of the proposed algorithms in realworld scenarios.", "m8vzptcFKsT": " Summary: The paper investigates the notion of average robustness in deep neural networks across various settings such as width depth and training modes. It explores how these factors influence the networks robustness in both underparameterized and overparameterized regimes. The findings highlight the impact of width and depth on robustness especially under different initializations and training regimes. The paper makes significant contributions to the current understanding of DNN robustness and presents theoretical developments that align with existing literature.  Strengths: 1. Theoretical Contributions: The paper provides valuable theoretical insights into the average robustness of DNNs across diverse settings bridging the gap in existing literature. 2. Methodological Clarity: The studys definitions and methodology are wellarticulated enhancing the readers understanding of the research findings. 3. Thorough Analysis: The paper thoroughly examines the effects of width depth and initialization on average robustness offering a comprehensive perspective on DNN behavior.  Weaknesses: 1. Empirical Validation: The paper lacks extensive empirical validation to support the theoretical findings. Strengthening the paper with more empirical results would enhance its credibility. 2. Clarity of Results: Some results could be presented in a more structured and visually appealing manner to aid in understanding and interpretation. 3. External Validation: The findings could benefit from comparison with realworld applications or additional validation on different datasets to ensure the generalizability of the results.  Questions and Suggestions for Authors: 1. Empirical Validation: Could the authors provide more empirical evidence to support and validate the theoretical results presented in the paper 2. Comparison: How do the proposed theoretical developments compare with existing empirical studies or realworld applications of DNN robustness 3. Generalizability: Have the authors considered how their findings may generalize to different network architectures or datasets  Limitations: The authors should further validate their theoretical findings with extensive empirical studies on various datasets and network architectures to ensure the practical applicability and generalizability of their results. Consider presenting the results in a more accessible and structured format to enhance the papers readability and impact.", "zFW48MVzCKC": " Summary The paper introduces a novel value factorization framework called QSCAN in the field of cooperative multiagent reinforcement learning (MARL). QSCAN aims to enhance the factorization of the team reward into individual rewards by incorporating subteam coordination ultimately improving learning efficiency. Experimental results demonstrate the superior performance of QSCAN over existing methods in various tasks including matrix games predatorprey challenges the Switch task and StarCraft scenarios.  Strengths 1. Innovative Approach: Introducing subteam coordination in value factorization for MARL is a novel and valuable contribution to the field. 2. Experimental Results: The paper provides detailed empirical evidence showcasing the superior performance of QSCAN compared to existing methods in a variety of tasks. 3. Theoretical Basis: The paper provides a solid theoretical foundation for the proposed QSCAN framework demonstrating its effectiveness in honoring the IndividualGlobalMax (IGM) condition.  Weaknesses 1. Complexity: The paper delves into intricate theoretical concepts and architectures which may make it challenging for readers unfamiliar with the field to follow and grasp the content easily. 2. Limited RealWorld Applications: While the experimental results exhibit success in specific scenarios the paper could benefit from discussing potential realworld applications and practical implications of QSCAN. 3. Assumptions: The paper assumes a certain level of familiarity with MARL terminology and previous works potentially alienating readers who are new to the field.  Questions and Suggestions 1. Generalization: How does QSCAN perform when applied to largerscale scenarios or realworld environments with more complex dynamics 2. Scalability: Does the QSCAN framework scale effectively with an increasing number of agents or more complex tasks What are the computational considerations for scaling up 3. Comparison with PolicyBased Methods: How does QSCAN compare to policybased methods in MARL and are there potential synergies that could be explored 4. Interpretability: Can the learned subteam coordination patterns be interpreted in realworld scenarios and how can they be utilized for practical decisionmaking 5. Future Directions: What are the next steps for extending the QSCAN framework or exploring other organizational paradigms in cooperative MARL  Limitations The paper adequately addresses the limitations and negative societal impact of their work by acknowledging the complexity and theoretical nature of the introduced framework. However to enhance the application potential of QSCAN further research on its scalability interpretability and applicability to realworld settings is recommended.", "qHs3qeaQjgl": " Brief Summary The paper presents a novel algorithm Barbarik3 which addresses the testing problem of constrained samplers over highdimensional distributions with (\u03b5 \\xce\\xb7 \u03b4) guarantees. The existing stateoftheart algorithm has a worstcase query complexity exponential in n making it impractical for use. Barbarik3 provides an exponentially faster algorithm with a query complexity linear in n demonstrating significant improvements in sample efficiency compared to Barbarik2 through empirical evaluations on realworld benchmarks.  Strengths 1. Novel Algorithm: Barbarik3 offers an innovative solution with significantly improved sample complexity compared to Barbarik2. 2. Theoretical Analysis: The paper includes detailed theoretical analysis and proofs ensuring the correctness and efficiency of the proposed algorithm. 3. Empirical Evaluation: Extensive experiments on both scalable and reallife benchmarks validate the effectiveness of Barbarik3 in reducing the sample requirements for testing samplers.  Weaknesses 1. Limited Parameter Support: Barbarik3 restricts the acceptable values of the closeness parameter \u03b5 to a subset compared to Barbarik2 for a given farness parameter \\xce\\xb7 limiting its applicability across a wider range of scenarios. 2. Complexity of Implementation: The complexity of the algorithm and its implementation may pose challenges for adoption in realworld ML applications which require userfriendly and efficient tools.  Questions and Suggestions 1. Clarifications on Parameter Restrictions: Can the authors provide insights into the reasons behind restricting \u03b5 to a narrower range for a given \\xce\\xb7 in Barbarik3 compared to Barbarik2 Are there potential ways to extend the range without compromising the algorithms performance 2. Scalability Challenges: How does Barbarik3 perform in terms of scalability when dealing with even larger instances or more complex distributions beyond the ones tested in the experiments 3. Robustness and Sensitivity Analysis: Have the authors conducted any robustness analysis to see how the algorithm performs under various noise levels or perturbations in the input distributions  Limitations The authors should consider expanding the parameter space support of Barbarik3 to align with or even exceed the capabilities of Barbarik2 ensuring broader applicability and usability of the algorithm in realworld scenarios.", "r0bjBULkyz": "Summary: The paper proposes Active Bayesian Causal Inference (ABCI) a fullyBayesian framework for integrated causal discovery and reasoning with experimental design. The authors focus on the class of causallysufficient nonlinear additive noise models modeled using Gaussian processes. The framework aims to jointly infer a posterior over causal models and queries of interest through Bayesian optimal experimental design. The simulations demonstrate that ABCI is more dataefficient than baselines in learning downstream causal queries with wellcalibrated uncertainty estimates. Strengths: 1. Contribution: The paper introduces an innovative framework that combines causal discovery causal reasoning and experimental design in a fullyBayesian setup. 2. Methodology: The use of Gaussian processes for modeling nonlinear relationships is appropriate and the Bayesian optimal experimental design approach is methodologically sound. 3. Comparisons and Results: The simulations showing the improved data efficiency of ABCI compared to baselines provide empirical support for the proposed framework. Weaknesses: 1. Complexity: The computational challenges associated with the framework are acknowledged but the paper does not delve deep into strategies for addressing these issues. 2. Generalization: The paper focuses on a specific class of models which might limit the general applicability of the proposed framework. 3. Lack of RealWorld Application: The paper could benefit from providing realworld examples or applications to showcase the practical utility of ABCI. Questions and Suggestions for Authors: 1. Computational Efficiency: How do you plan to address the computationally challenging aspects of the ABCI framework when scaling to more complex models or larger datasets 2. Applicability: Have you considered potential realworld use cases or applications to demonstrate the effectiveness of ABCI outside simulations 3. Comparison to Existing Methods: How does ABCI compare to existing methods in terms of scalability and applicability to realworld datasets 4. Robustness and Sensitivity Analysis: Have you conducted sensitivity analysis to assess the robustness of the proposed framework to varying assumptions or model specifications Limitations: While the paper mentions the computational challenges it could further elaborate on potential negative societal impacts ethical considerations or unintended consequences of implementing the ABCI framework to address limitations comprehensively. Suggestions for Improvement: 1. Provide more insights into computational scaling strategies and techniques to address computational challenges. 2. Include realworld case studies or examples to illustrate the practical application of ABCI. 3. Conduct sensitivity analysis and robustness checks to enhance the credibility of the proposed framework. Overall the paper presents a comprehensive framework for active Bayesian causal inference but there is room for further elaboration on computational challenges realworld applicability and methodological robustness.", "yhlMZ3iR7Pu": "Abstract Summary: The paper presents a novel approach to inference in highdimensional data using a denoising diffusion generative model as a prior and an auxiliary differentiable constraint. The method allows for plugandplay modules and various applications in conditional generation image segmentation and solving combinatorial optimization problems. Strengths: 1. The paper presents a unique methodology merging denoising diffusion models with differentiable constraints for inference enabling various applications. 2. The experiments cover a wide range of scenarios including conditional image generation semantic image segmentation and continuous relaxation of combinatorial problems demonstrating the flexibility and effectiveness of the proposed approach. 3. Detailed explanations algorithms and evaluation metrics are provided ensuring reproducibility and understanding. Weaknesses: 1. The paper lacks clarity in explaining the practical implications and potential impact of the proposed method in realworld applications. 2. While the experiments are comprehensive the paper could benefit from a more detailed discussion on the limitations and tradeoffs of the proposed approach. 3. The technical complexity of the paper may pose a barrier to readers unfamiliar with denoising diffusion models and differentiable constraints. Questions and Suggestions: 1. How does the proposed approach compare to existing methods in terms of computational efficiency especially for largescale datasets 2. Can the authors provide more insights into the scalability of the method for handling increasingly complex highdimensional data 3. Are there any plans to test the proposed method on more diverse datasets to assess its generalizability across different domains and applications Limitations: The authors could enhance the discussion on the computational costs and scalability of the proposed method addressing its limitations for realtime applications and suggesting optimizations for improving efficiency.", "yTJze_xm-u6": " Summary: The paper introduces a novel approach for sourcefree domain adaptation using variational model perturbations. The proposed method aims to adapt a pretrained model to target domains without access to source data addressing challenges of distribution shift. By introducing perturbations through variational Bayesian inference the approach aims to retain the discriminative ability of the source model. The paper demonstrates theoretical connections to Bayesian neural networks and utilizes a parameter sharing strategy to reduce learnable parameters. Experimental results on various benchmarks confirm the effectiveness of the proposed method in sourcefree domain adaptation.  Strengths: 1. Innovative Approach: The paper introduces a novel concept of variational model perturbation for sourcefree domain adaptation offering a fresh perspective on the problem. 2. Theoretical Grounding: The paper establishes a theoretical connection to Bayesian neural networks providing a solid foundation for the proposed method. 3. Experimental Validation: Extensive experiments on multiple datasets and evaluation settings demonstrate the effectiveness of the proposed approach. 4. Efficiency Measures: Employing a parameter sharing strategy to reduce learnable parameters enhances the efficiency of the method. 5. Comparative Analysis: The comparisons with existing methods and detailed effectiveness analysis provide a comprehensive evaluation framework.  Weaknesses: 1. Technical Clarity: Some parts of the paper especially in the methodology section may be challenging for readers not deeply familiar with Bayesian inference and neural networks. 2. Experimental Rigor: While the experimental results are promising more insights into the choice of hyperparameters and potential sensitivity analyses could strengthen the empirical findings further. 3. Theoretical Depth: While the theoretical connection to Bayesian neural networks is mentioned further elaboration on how this connection impacts the practical implementation could enhance the papers impact.  Questions and Suggestions for Authors: 1. Clarification on Perturbation Strategy: How sensitive is the models performance to the choice of perturbation intensity or distribution Are there specific guidelines for selecting optimal perturbation levels 2. Handling Uncertainty: How does the approach manage uncertainties introduced by perturbations Could there be potential risks or limitations associated with this uncertainty in realworld deployments 3. Scalability Concerns: Have scalability challenges been explored especially when dealing with larger networks or more complex datasets 4. Combining Approaches: Could a hybrid approach that incorporates perturbationbased adaptation along with traditional finetuning or BN modulation enhance performance in certain scenarios  Limitations: The paper would benefit from a more explicit discussion on addressing potential negative societal impacts or ethical considerations related to the deployment of the proposed method in realworld settings. Addressing these aspects can foster societal trust and responsible deployment of machine learning technologies. Overall the paper presents a promising approach to sourcefree domain adaptation combining theoretical insights with practical applicability. Further clarity robust experimental validations and deeper discussions on implications could strengthen the contribution of the work.", "yKYCwTvl8eU": "Summary: The paper proposes two approaches CCM RES and CCM EYE to mitigate shortcut learning in machine learning models by leveraging domain knowledge represented by known concepts (C) while also accounting for unknown concepts (U). CCM RES fits a model based on known concepts and learns a residual to account for unknown concepts while CCM EYE extends an EYE regularization penalty to the concept space to mitigate shortcuts. The study showcases the effectiveness of these approaches on realworld datasets to enhance model robustness. Strengths: 1. Innovative Approaches: The paper introduces novel methods CCM RES and CCM EYE which address a critical issue in machine learning  shortcut learning by leveraging known and unknown concepts. 2. Theoretical and Empirical Contributions: The paper provides theoretical insights into the proposed methods consistency and demonstrates their effectiveness through empirical experiments on real datasets. 3. Robustness to Changes in Distribution: The experiments show that the proposed methods exhibit robustness to changes in distribution due to shortcuts demonstrating their practical utility. 4. Thorough Discussion: The paper offers a comprehensive discussion on the limitations of the work and potential future directions enhancing the researchs credibility and applicability. Weaknesses: 1. Complexity of Concepts: The reliance on obtaining a reliable C may pose a challenge in practical applications where defining and acquiring such domain knowledge may not be straightforward. 2. Interpretability: While the study mentions the importance of interpreting unknown concepts (U) it does not delve deeply into methodologies to interpret these implicit features which is crucial for model trustworthiness in realworld applications. 3. Generalization: The paper highlights the effectiveness of the proposed methods in experimental settings however it could benefit from further exploration on the generalization of these approaches across diverse domains and datasets. Questions and Suggestions for Authors: 1. How do the proposed methods scale with increasingly complex or highdimensional datasets in terms of both computational efficiency and performance 2. Can you discuss potential tradeoffs in model performance when incorporating unknown concepts (U) compared to solely relying on known concepts (C) Are there scenarios where strictly using C might be preferable 3. Have you considered sensitivity analysis to evaluate the impact of varying hyperparameters such as \u03bb in CCM EYE on model performance and robustness Limitations: The authors have addressed the limitations of relying on known and unknown concepts but could enhance their work by further exploring methodologies for interpreting and validating unknown concepts for improved model interpretability and trustworthiness.", "pm8Y8unXkkJ": " Summary: The paper introduces a new theoretical framework for analyzing classification performance on imbalanced binary classification problems. The key contributions include deriving a generalized Bayes classifier for any performance metric computed from the confusion matrix providing relative performance guarantees under these metrics and developing finitesample statistical guarantees specific to imbalanced binary classification especially under a novel notion called Uniform Class Imbalance. The work is supported by theoretical analysis mathematical proofs and numerical experiments illustrating the implications of the theoretical results.  Strengths: 1. Innovative Contribution: The paper presents a novel approach to addressing the challenges of imbalanced binary classification offering new theoretical insight and analytical framework for performance evaluation. 2. Theoretical Rigor: The work demonstrates a comprehensive theoretical analysis supported by rigorous mathematical proofs providing a robust foundation for the proposed framework. 3. Practical Relevance: By addressing a significant challenge in classification tasks especially in imbalanced settings the paper offers practical implications and insights for developing classifiers that perform well under specific imbalance problems. 4. Numerical Experimentation: The inclusion of numerical experiments enhances the paper by illustrating the practical consequences of the theoretical framework and providing concrete examples to support the findings.  Weaknesses: 1. Complexity of Presentation: The paper delves deeply into theoretical aspects which might make it challenging for readers without a strong background in statistical theory to grasp the concepts easily. 2. Limited Practical Application Discussion: While the theoretical developments are robust there could be more discussion on the direct practical implications of the proposed framework in realworld scenarios. 3. Lack of Comparison: While the paper introduces a new approach a comparison with existing methods or frameworks for imbalanced binary classification could further enhance the papers impact.  Questions and Suggestions for Authors: 1. Can you provide more insights into how the proposed framework can be practically implemented in realworld scenarios to improve classification performance on imbalanced datasets 2. How does the proposed approach compare to stateoftheart methods for imbalanced binary classification both theoretically and empirically 3. Could you elaborate on the computational complexity or efficiency of the framework especially when dealing with largescale or highdimensional datasets  Limitations: The paper adequately addresses the limitations and potential negative societal impacts of their work. One constructive suggestion for improvement could be to include a brief discussion on the ethical considerations and fairness implications of using stochastic classifiers especially in sensitive application areas such as healthcare or criminal justice.", "rrYWOpf_Vnf": " PeerReview of Paper on Statistical Limits of Gradient Descent for Inverse Problems Summary: The paper delves into studying the statistical limits in solving inverse problems using gradient descent with a focus on Sobolev norms. It encompasses a theoretical framework involving a general class of objective functions applicable to specific cases like kernel regression Deep Ritz Method (DRM) and Physics Informed Neural Networks (PINN) for solving partial differential equations (PDEs). The study proves the statistical optimality achievable through gradient descent highlighting the acceleration effect of the Sobolev norm on training. The contributions include lower bounds on solving elliptic inverse problems proving statistical optimality of gradient descent and exploring the acceleration effect of Sobolev loss function. Strengths: 1. The paper addresses a significant topic in the practical application of machine learning methods for solving inverse problems. 2. The theoretical framework presented is wellstructured and explains the statistical optimality of gradient descent. 3. The theoretical findings on Sobolev implicit acceleration add value to the understanding of training algorithms. Weaknesses: 1. The paper lacks a detailed explanation of the empirical methodology and practical implementation of the findings. 2. The presentation could benefit from more illustrative examples to enhance reader comprehension. 3. The paper could be strengthened by providing more comparative analysis with existing methods in solving similar inverse problems. Questions and Suggestions for Authors: 1. Could you provide more insights into the practical implications of the Sobolev implicit acceleration in realworld scenarios 2. Have you considered empirical experiments on larger datasets or more complex inverse problems to validate the theoretical findings 3. How do the results vary when noise levels in observations increase How robust is the proposed methodology in noisy environments Limitations: The authors should consider addressing the limitations of scaling the Sobolev training to higher dimensions and incorporating stochastic noise in gradient descent analysis for improved robustness. Overall the paper makes a significant contribution to the theoretical understanding of gradient descent in solving inverse problems but further empirical validation and practical insights would enhance its impact.", "zSdz5scsnzU": " PeerReview of \"Expansive Latent Space Trees for LongHorizon Goalreaching Scenarios From Visual Inputs\"  Summary: The paper presents a novel latent planning algorithm called Expansive Latent Space Trees (ELAST) for addressing challenges in longhorizon goalreaching scenarios from visual inputs. ELAST utilizes selfsupervised contrastive learning to obtain a latent state representation and a latent transition density model. The algorithm incorporates tree search in the latent state space and demonstrates significant performance improvements in challenging visual control tasks compared to existing baselines.  Strengths: 1. Innovative Approach: The use of treebased exploration in latent spaces for planning is a novel and promising concept. 2. Combination of Techniques: Integrating selfsupervised contrastive learning with motion plannerinspired concepts provides a unique and effective solution. 3. Comprehensive Evaluation: The paper provides thorough experimental evaluations comparing ELAST to existing methods across various control tasks.  Weaknesses: 1. Theoretical Depth: While the practical aspects of ELAST are welldetailed more insights into the theoretical foundations and implications of the approach could enhance the papers contribution. 2. Comparative Analysis: A deeper comparison with existing stateoftheart methods in terms of complexity scalability and performance metrics would have added value to the evaluation section. 3. Clarity in Presentation: Some sections are overly technical and could benefit from clearer explanations and visual aids to improve understanding particularly for readers less familiar with the field.  Questions and Suggestions for Authors: 1. Robustness to Noisy Inputs: How does ELAST perform in scenarios with noisy or unreliable visual inputs Have you tested the models robustness to variations in image quality or occlusions 2. Generalization to Realworld Settings: How well would ELAST generalize to complex realworld robotic scenarios with dynamic environments and uncertainties Any plans for testing on physical robots  Limitations: The authors have not fully addressed the potential negative societal impact of their work. To enhance the societal impact assessment it is advisable to consider potential ethical implications of deploying ELAST in realworld scenarios involving human interaction or sensitive data.  Suggestions for Improvement: 1. Theoretical Exposition: Provide a more detailed theoretical analysis of ELAST including discussions on the algorithms computational complexity convergence properties and scalability. 2. Realworld Validation: Consider validating ELAST in physical robot settings or environments with realworld challenges to assess its practical applicability and robustness. 3. Ethical Considerations: Include a section discussing potential ethical considerations and societal implications of using ELAST in autonomous systems to ensure responsible deployment and address any unintended consequences. Overall the paper presents a promising contribution to the field of latent planning for visual control tasks and further refinements as suggested can enhance the impact and relevance of the work.", "zp_Cp38qJE0": "Summary: The paper focuses on addressing the challenge of transferring fairness under distribution shifts in machine learning models. It conducts a thorough analysis of fairness under different types of distribution shifts and proposes a selftraining algorithm with fair consistency regularization to effectively transfer fairness and accuracy. Experimental results on synthetic and real datasets demonstrate the effectiveness of the proposed approach. Strengths: 1. The paper provides a comprehensive analysis of fairness under distribution shifts addressing a significant gap in the existing literature. 2. The theoretical framework and algorithm proposed are wellmotivated and backed by sound theoretical foundations. 3. The experiments cover diverse scenarios with synthetic and real datasets showcasing the effectiveness of the proposed method in transferring fairness. 4. The fair consistency regularization component is a novel and crucial addition to the selftraining framework contributing significantly to improving fairness in transferred models. 5. The ablation studies provide valuable insights into the importance of transformations fair consistency and other components in achieving fairness under distribution shifts. Weaknesses: 1. The paper lacks discussion on the scalability and computational complexity of the proposed algorithm especially when applied to largescale datasets. 2. The evaluation on real datasets could benefit from a more extensive comparison with stateoftheart methods. 3. The paper could elaborate more on the limitations and challenges of the proposed approach particularly in practical deployment scenarios across varied domains. Questions and Suggestions: 1. How does the proposed algorithm fare in terms of computational efficiency when scaling up to larger datasets Are there any potential bottlenecks in realworld applications 2. Can the proposed approach be generalized to handle more complex distribution shifts beyond the ones considered in the current study 3. Have you considered evaluating the transferability of fairness under different model architectures or hyperparameters in your experiments to gauge the robustness of the proposed method 4. What are the implications of the proposed fair consistency regularization in terms of interpretability and transparency of the transferred models 5. Is there a plan to extend the research to explore the societal implications and ethical considerations associated with transferring fairness in machine learning models under distribution shifts Limitations: The authors could further address the limitations of their work by providing concrete strategies for handling scenarios where the proposed approach may not perform optimally. Additionally discussing the potential negative societal impacts and suggesting ways to mitigate them could strengthen the papers societal relevance.", "sde_7ZzGXOE": " Summary: The paper delves into the theoretical framework of OOD detection using the PAC learning theory. It aims to bridge the gap between empirical OOD detection algorithms and the theoretical foundation for the field. The study investigates the learnability of OOD detection in various domain spaces focusing on conditions that guarantee successful OOD detection in practical scenarios.  Strengths: 1. Original Contribution: The paper makes a significant contribution by providing a theoretical framework for understanding the learnability of OOD detection filling an existing gap in the field. 2. Theoretical Rigor: The authors rigorously explore the probably approximately correct (PAC) learning theory in the context of OOD detection offering insight into necessary and sufficient conditions for successful detection. 3. Relevance to RealWorld Applications: The study focuses on practical implications by studying representative domain spaces such as the separate space and densitybased space to ensure the applicability of the theory in real scenarios.  Weaknesses: 1. Complexity: The paper is highly technical which may limit its accessibility and comprehension for readers outside the specific domain of theoretical machine learning theory. 2. Limited Practical Validation: While the theoretical framework is robust the lack of empirical validation or practical implementations may impact the immediate applicability of the findings.  Questions and Suggestions: 1. Practical Validation: Will the authors consider conducting experiments to validate the theoretical findings and demonstrate the practical applicability of the proposed conditions for successful OOD detection 2. Scalability: How scalable are the proposed theoretical frameworks and conditions when applied to largescale datasets and complex OOD detection scenarios 3. Further Extensions: Are there plans to extend the study to explore the feasibility of nearOOD detection and its implications on the theoretical framework proposed in this paper  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To improve they could consider discussing the societal implications of OOD detection and the potential risks associated with inaccurate or biased OOD detection systems.", "weoLjoYFvXY": " Reviewer Comments: 1. Summary: The paper proposes a novel hierarchical and localized learning algorithm Root Cause Discovery (RCD) for quickly detecting the root cause of failures in complex microservice architectures. The algorithm leverages the distributional invariances across normal and anomalous datasets to identify the root causes efficiently. The authors compare RCD with existing techniques like \\xcf\\xb5Diagnosis AutoMAP and CIRCA demonstrating significant improvements in topk recall and reduced execution time. 2. Strengths:  Innovation: The hierarchical and localized approach of RCD is innovative and addresses the challenges of root cause analysis in complex microservice architectures.  Scalability: RCD outperforms existing techniques in terms of execution time demonstrating scalability in identifying root causes.  Extensive Evaluation: The thorough evaluation on synthetic datasets Sockshop application and realworld data showcases the effectiveness of RCD and provides valuable insights on its performance against baselines. 3. Weaknesses:  Limitations of Existing Methods: The discussion on the limitations of existing methods like \\xce\\xa8PC AutoMAP and CIRCA could be expanded to provide a more comprehensive comparison.  Algorithm Complexity: While the hierarchical and localized approach of RCD is beneficial a more detailed discussion on the algorithms complexity and resource requirements could enhance the paper. 4. Questions and Suggestions for Authors:  How does RCD handle scenarios where the root cause is influenced by latent variables not directly captured in the system  Can the authors provide insights into how RCD adapts to different fault scenarios and system sizes  Could more details be provided on the implementation challenges or potential areas for further optimizationvalidation in a real production setting 5. Limitations: The authors have not explicitly addressed the potential negative societal impacts of their work. To enhance the papers completeness it would be beneficial to consider and discuss any societal implications of automated root cause analysis in cloud applications such as privacy concerns or biased decisionmaking. Overall the paper presents a novel and promising approach to root cause analysis in microservice architectures. With further refinements addressing the identified weaknesses and questions raised this work has the potential to make significant contributions to the field.", "sNcn-E3uPHA": "Peer Review 1) Summary: The paper introduces a text classification algorithm inspired by quantum physics treating text as a superposition of words. The Born rule is utilized to compute the transition probability of a document to a target class. Two implementations are detailed one explicitly calculating wave functions and another embedding the classifier in a neural network. The proposed method is analyzed on benchmark datasets showcasing performance explainability and computational efficiency. 2) Strengths:  Innovative Approach: The quantuminspired methodology stands out for its novelty and potential in enhancing machine learning algorithms.  Comprehensive Analysis: The paper presents detailed explanations equations and empirical results covering various aspects of the proposed method.  Empirical Validation: Demonstration on benchmark datasets and comparisons with traditional classifiers provide a strong basis for the proposed algorithms effectiveness.  Scalability: The discussion on computational complexity and potential scalability including the use of GPUs and sparse matrices demonstrates consideration for practical implementation. 3) Weaknesses:  Complexity: The quantum physics analogies and mathematical formalisms may be challenging for readers unfamiliar with quantum mechanics potentially limiting accessibility.  Lack of Generalization: The paper primarily focuses on text classification more discussions on its applicability to nontextual data and other domains could enhance the papers utility.  Clarity: Some sections especially those discussing hyperparameters and ablation studies may benefit from clearer explanations or visual aids to improve understanding. 4) Questions and Suggestions:  Could the authors elaborate on the interpretability and explainability aspects of the proposed method How does it compare with traditional text classification methods in terms of interpretability  Given the complexities of quantum mechanics are there concerns about practical implementation challenges such as realworld dataset compatibility or computational resources required  Considering the speed and accuracy advantages demonstrated in the results how robust is the algorithm to noise or outliers in data especially in comparison to traditional classifiers  It would be beneficial for the authors to discuss the potential future research directions for applying this quantuminspired approach to broader machine learning tasks beyond text classification. 5) Limitations: The authors have not excessively addressed the potential limitations and societal impacts of their work. To enhance the paper it is suggested to include a brief commentary on these aspects to ensure a wellrounded evaluation. Overall the paper presents an intriguing fusion of quantum physics and machine learning offering a fresh perspective on text classification. Addressing the identified weaknesses and incorporating the suggested improvements would enhance the papers clarity and impact.", "lgj33-O1Ely": " Summary: The paper introduces TotalSelfScan a novel approach for reconstructing a fullbody model from multiple monocular selfrotation videos focusing on the face hands and body. By proposing a multipart network and utilizing partspecific videos the method significantly improves reconstruction quality on faces and hands compared to existing methods.  Strengths: 1. Innovative Approach: The paper introduces a unique method that addresses the challenge of reconstructing fullbody avatars with rich details focusing on different body parts. 2. Rich Details: By using multiple partspecific videos the proposed method enhances the reconstruction quality of faces and hands compared to existing techniques. 3. Thorough Experiments: The paper includes detailed experiments comparisons with baselines and ablation studies to validate the effectiveness of the proposed approach. 4. Code Availability: The authors provide code availability for reproducibility enhancing the transparency of their work.  Weaknesses: 1. Limited Human Motion Modeling: The method may face limitations in modeling posedependent deformations due to the restricted human motions in selfrotation videos. 2. Training Time: The paper acknowledges that the method still requires a relatively long time for training suggesting areas for future improvement. 3. Artifact Evaluation: Error bars for experimental results are missing potentially affecting result reliability and generalizability.  Questions and Suggestions: 1. Posedependent Deformation: Have you considered leveraging multiview human motion datasets to enhance the models ability to generalize to a wider range of human poses 2. Training Time Reduction: Have you explored techniques such as hash encoding to reduce training time for implicit functions as suggested by recent work 3. Consistency Preservation: How do you ensure the method maintains consistent appearances across different parts especially at boundaries in practical scenarios 4. Societal Impact: Given the potential negative impacts of unauthorized image synthesis have you explored additional safeguards or ethical considerations in your research  Limitations Addressed: The paper adequately discusses the limitations of limited human motion modeling and training time providing avenues for further research and improvement.  Constructive Suggestions for Improvement: 1. Include error bars in experimental results for better result interpretation and reliability. 2. Provide details on assets licenses and consent for using data enhancing transparency and ethical considerations. The paper presents an innovative approach with substantial contributions to the field while also highlighting important areas for further research and refinement. Strengthening the experimental evaluations and addressing potential negative societal impacts would enhance the overall quality of the work.", "rjDziEPQLQs": "PeerReview: Summary: The paper introduces a novel algorithm AffineInvariant Cubic Newton (AICN) which combines the benefits of regularized Newton methods and damped Newton methods. AICN is shown to achieve fast global and local convergence rates while being simple explicit and implementable. The paper discusses the theoretical foundations of the algorithm its geometric properties and provides convergence guarantees. Numerical experiments are conducted to showcase the performance of AICN compared to existing stateoftheart methods. Strengths: 1. Novel Algorithm: AICN presents a novel approach by incorporating both regularized and damped Newton methods offering fast global and local convergence guarantees with simplicity and implementability. 2. Theoretical Foundations: The paper provides thorough discussions on selfconcordance affine invariance and geometric properties enhancing the understanding of the algorithm. 3. Convergence Guarantees: The rigorous theoretical analysis provides clear global and local convergence guarantees supported by Lemmas and Theorems. 4. Numerical Experiments: The empirical evaluation particularly in logistic regression and the secondorder lower bound function demonstrates the superior performance of AICN over existing methods. 5. Detailed Explanations: The paper is wellstructured offers detailed explanations and supports its claims with proper notation and mathematical proofs. Weaknesses: 1. Clarity and Readability: The paper contains dense technical content and extensive mathematical derivations making it challenging for the general audience to grasp the key concepts easily. 2. Assumptions and Limitations: The paper could benefit from a clearer discussion on the assumptions made including their impact on the algorithms performance. 3. Comparative Analysis: While the numerical experiments show AICNs superiority a comprehensive comparison with existing methods in various scenarios could strengthen the validity of the algorithm further. Questions and Suggestions for Authors: 1. Can you provide more insights into how the assumptions of selfconcordance and affine invariance influence the practical applicability of the AICN algorithm 2. How does the choice of initialization impact the convergence properties of AICN especially in scenarios where the starting point is far from the optimal solution 3. Have you considered potential extensions or modifications to the AICN algorithm to enhance its performance in specific optimization scenarios or different applications 4. Could you discuss the computational complexity and scalability of AICN compared to existing methods especially in largescale optimization tasks Limitations: The authors could improve the paper by:  Providing a clearer explanation of the assumptions made and their implications on the algorithms performance.  Including a more extensive comparative analysis to highlight the strengths of AICN in different optimization scenarios.  Addressing potential negative societal impacts by discussing ethical considerations related to the use of fast convergence optimization algorithms.", "kK200QKfvjB": " Peer Review  Summary: The paper investigates the loss surface of deep neural networks (DNNs) with L2 regularization. Two reformulations of the loss are proposed: one in terms of layerwise activations and another in terms of covariances of hidden representations particularly for positively homogeneous nonlinearities. The study reveals insights into feature learning sparsity effects and the number of neurons required to reach minima in DNNs. The analysis sheds light on the tradeoffs between network depth and feature learning and provides theoretical implications for training deep networks.  Strengths: 1. Novel Contributions: The paper offers original insights into the loss surface dynamics of DNNs with L2 regularization shedding light on feature learning mechanisms and sparsity effects. 2. Theoretical Rigor: The study is theoretically sound grounded in mathematical formulations and proofs that enhance understanding of deep learning phenomena. 3. Indepth Analysis: The thorough analysis of the impact of L2 regularization on feature learning as well as the exploration of the sparsity phenomenon in DNNs showcases the depth of the study. 4. Clear Explanations: The paper effectively communicates complex concepts providing clear explanations and use of mathematical notations to aid in understanding.  Weaknesses: 1. Complexity: The paper may pose challenges for readers not wellversed in mathematical formulations and theoretical deep learning concepts limiting accessibility. 2. Limited Empirical Validation: While the theoretical analysis is extensive empirical validation on real datasets could have reinforced the findings and practical applicability. 3. Clarity of Presentation: Some sections may be challenging to follow due to the density of mathematical notations and technical details potentially requiring improved clarity in presentation.  Questions and Suggestions: 1. Could you elaborate on how the insights derived from the theoretical analysis can impact practical applications of DNNs such as model optimization or network architecture design 2. Considering the potential complexity of the proposed reformulations in practical implementation are there simplified versions or approximations that can retain the essence of the insights provided while being more computationally feasible 3. How do the findings of the study align with existing research in the field of deep learning and optimization of neural networks Are there any contradicting or complementary results 4. It would be beneficial to include further discussion on how the sparsity effects observed in DNNs can be leveraged for model interpretability or efficiency in realworld applications.  Limitations: The paper does not address the potential negative societal impacts of the research findings explicitly. Authors could enhance the discussion by considering the ethical implications of increased automation potential biases in decision making or environmental sustainability related to the use of deep learning models.  Overall Assessment: The paper provides valuable insights into the loss surface dynamics and sparsity effects in DNNs with L2 regularization offering novel perspectives for understanding feature learning and network optimization. With a strong theoretical foundation the study contributes significantly to the theoretical aspects of deep learning research. Enhancements in clarity and potential empirical validations could further enrich the work and its practical implications.", "u6MpfQPx9ck": "Summary: The paper introduces ProbCover a new deep active learning algorithm aimed at reducing annotation costs for deep models particularly effective in the lowbudget regime. The main contribution lies in developing a theoretical framework to analyze active learning strategies in embedding spaces proposing ProbCover based on this framework and demonstrating its superior performance in various image recognition benchmarks especially in the semisupervised setting. Strengths: 1) Theoretical Analysis: The paper provides a solid theoretical foundation for understanding active learning in the lowbudget regime utilizing a novel perspective based on probability coverage in an embedding space and connecting it with wellknown AL methods like Coreset. 2) Empirical Results: The extensive experimental evaluation on multiple datasets showcases ProbCovers significant improvements over existing AL strategies in the lowbudget regime particularly in semisupervised scenarios bridging the gap between fully supervised and semisupervised methods with minimal labels. 3) Ablation Study: The ablation study investigating ProbCovers performance using RGB space distances and showcasing robustness beyond the initial set selection adds depth to the analysis and underscores the importance of embedding spaces. Weaknesses: 1) Practical Complexity: The NPhardness of the Max Probability Cover problem as discussed in the paper might pose challenges in practical implementation due to computational complexity. 2) Limited Realworld Application: While the paper excels in theoretical analysis and experimental results a more indepth discussion on the broader practical implications and realworld applications of ProbCover could enhance its impact. Questions and Suggestions: 1) Theoretical Clarifications: How does ProbCover handle noisy or ambiguous labels in the lowbudget regime It would be beneficial to discuss the algorithms robustness to such scenarios. 2) Scalability: How does ProbCover scale with larger datasets or higher dimensional feature spaces Investigating its performance in more complex scenarios can provide insights into its scalability. 3) Comparison with StateoftheArt: How does ProbCover compare with the latest advancements in active learning especially in the context of largescale datasets or complex modalities like text or timeseries data Limitations: The paper adequately addresses the limitations and potential negative societal impacts of their work by providing a robust theoretical foundation and valid empirical evaluations to support their claims. A suggestion for improvement would be to include a brief discussion on any unintended bias that may arise from the AL algorithms decisionmaking processes and ways to mitigate them.", "pd6ipu3jDw": " PeerReview Feedback  Summary: The paper proposes the Agent Transformer Memory (ATM) network to address the challenges of partial observability in multiagent reinforcement learning (MARL) tasks. The core contributions of ATM include leveraging a transformerbased working memory mechanism and introducing an EntityBound Action Layer to enhance the learning acceleration and performance improvement in various MARL environments. Extensive experiments on challenging environments such as SMAC and LevelBased Foraging validate the effectiveness of ATM in boosting existing MARL algorithms.  Strengths: 1. Innovative Contributions: The paper introduces novel concepts such as the transformerbased working memory and EntityBound Action Layer to enhance the performance of MARL in partial observable environments. 2. Comprehensive Experiments: The extensive experimentation on challenging environments like SMAC and LevelBased Foraging validates the effectiveness of ATM in accelerating learning and improving performance across various tasks. 3. Thorough Explanation: The paper provides detailed descriptions and illustrations of the proposed concepts making it easy to understand the components and operation of the ATM network.  Weaknesses: 1. Clarity on Computational Complexity: The paper could benefit from a clearer discussion on the computational complexity implications of introducing the transformer into the MARL framework especially concerning scalability to largerscale environments. 2. Comparative Analysis: While the experimental results demonstrate the superiority of ATM over existing methods a comparative analysis with other transformerbased MARL approaches could provide a more comprehensive evaluation of the proposed models performance.  Questions and Suggestions for Authors: 1. Could you provide more insights into how ATM handles the scalability issue concerning the computational complexity of the transformer block with increasing historical observation trajectories 2. Given the effectiveness of the transformer in sequential reasoning tasks have you explored the potential of ATM in capturing longterm dependencies in multiagent interactions over extended periods 3. How does ATM mitigate potential negative societal impacts such as bias amplification or unintended consequences in realworld applications of MARL especially in safetycritical domains like autonomous driving or healthcare  Limitations: While the paper presents a robust framework with promising results it would be beneficial for the authors to address the scalability and potential negative societal impacts of ATM in more depth to ensure the robustness and ethical implications of deploying MARL systems in practice.", "r9b6T088_75": " 1) Summary: The paper proposes a novel approach called DegradationAware Unfolding HalfShuffle Transformer (DAUHST) for hyperspectral image (HSI) reconstruction in coded aperture snapshot spectral compressive imaging (CASSI) systems. The method incorporates a principled DegradationAware Unfolding Framework (DAUF) to estimate parameters from the compressed image and physical mask and a novel HalfShuffle Transformer (HST) to capture local contents and nonlocal dependencies. Experiments demonstrate that DAUHST outperforms stateoftheart methods in terms of performance and computational efficiency.  2) Strengths:  Innovative Approach: The paper introduces a novel approach combining DegradationAware Unfolding Framework and HalfShuffle Transformer showcasing innovative thinking.  Performance Improvement: Experimental results show that DAUHST outperforms existing methods significantly in terms of quality of HSI reconstruction while being computationally efficient.  Thorough Analysis: The paper provides detailed technical descriptions and analyses improving the understanding of the proposed method.  Weaknesses:  Complexity and Speed: The paper mentions a tradeoff between performance improvement and an increase in model complexity and inference speed. This could be a limiting factor in practical implementations.  Evaluation Comparison: While the paper compares the proposed method with existing stateoftheart techniques a more indepth analysis of the drawbacks of the compared methods could provide a more comprehensive evaluation.  RealWorld Impact: The paper briefly touches on potential negative societal impacts but further exploration of ethical considerations and implications of the technology would enhance the discussion.  3) Questions and Suggestions for Authors:  Can you elaborate on the specific computational costs and memory requirements of DAUHST compared to other methods  How scalable is DAUHST for largescale applications or realtime processing  Have you considered the implications of model interpretability and explainability in the context of HSI reconstruction applications  4) Limitations and Social Impact: The authors should further consider the tradeoffs between performance complexity and inference speed. Additionally a more comprehensive discussion on potential societal impacts and ethical considerations of deploying advanced HSI reconstruction techniques would enhance the papers contribution.", "qmm__jMjMlL": "Summary: The paper introduces TARGETVAE a translation and rotation groupequivariant variational autoencoder framework designed for learning semantic representations of objects invariant to pose and location in an unsupervised manner. The model disentangles latent variables into semantic rotation and translation components enabling accurate unsupervised pose and location inference. Comprehensive experiments demonstrate significant improvements over previous methods enhancing clustering in semantic latent space and accurate object representation even in heavily rotated and translated images. Strengths: 1. Innovative Contribution: Introduces a novel approach to learning invariant object representations in images addressing limitations of prior methods. 2. Comprehensive Experiments: The paper includes welldesigned experiments showcasing the effectiveness of TARGETVAE in accurately inferring rotations and translations without supervision. 3. Clear Methodological Explanation: The paper provides a detailed explanation of the proposed method including the neural network architecture and training setup. 4. Potential for Future Applications: The authors highlight the potential of TARGETVAE for future applications such as object generation pose prediction and object detection. Weaknesses: 1. Limited Comparison: While the paper compares TARGETVAE with spatialVAE and other VAE methods it could benefit from a wider comparison with additional stateoftheart models to showcase the superior performance of TARGETVAE more comprehensively. 2. Implicit Assumptions: The assumptions made in the method especially related to rotation and translation handling need to be discussed more explicitly to ensure a robust understanding by the readers. 3. Expansion on Dataset Diversity: While experiments on MNIST and cryoEM datasets are informative including a broader range of datasets would enhance the generalizability and robustness of the proposed approach. Questions and Suggestions: 1. Generalization: Have you tested the model on datasets with more complex and diverse images to evaluate its generalization capability 2. Computational Complexity: To scale the model for realworld applications have you considered the computational complexity and efficiency of the proposed architecture 3. Parameter Sensitivity: How sensitive is the performance of TARGETVAE to its hyperparameters and model configurations Limitations Acknowledgement: The authors have thoroughly addressed the limitations and potential negative societal impacts of their work. To enhance this aspect a discussion on data biases and ethical considerations could provide more depth. In conclusion the paper presents an innovative approach with promising results. By addressing the weaknesses and considering the suggestions the authors can strengthen the robustness and applicability of their methodology.", "nyn2ewuF-g9": " Summary: The paper investigates the dilemma of balancing between payoff and model rewards in collaborative machine learning. It proposes an allocation scheme to distribute payoff fairly among parties collaborating to enhance ML models. The scheme can derive from two approaches based on desirable properties of payoffs or underlying payoff flows between parties. The proposed solution is shown to be effective in various scenarios of collaborative ML with different budget constraints.  Strengths: 1. Innovative Approach: The paper addresses a significant issue in collaborative ML and introduces a novel allocation scheme balancing payoff and model rewards. 2. Clear Problem Definition: The challenges of fairness in collaborative ML are articulately presented providing a solid foundation for the proposed solution. 3. Theoretical Rigor: The paper lays out a comprehensive framework supported by mathematical proofs and formal properties to ensure fairness in reward allocations. 4. Empirical Demonstration: Empirical results validate the proposed scheme across different scenarios showcasing its effectiveness in realworld applications.  Weaknesses: 1. Complexity: The paper entails intricate mathematical formulations and technical details that may pose challenges for nonexperts to grasp easily. 2. Lack of Realworld Validation: While the proposed scheme is empirically evaluated a realworld validation on diverse datasets would strengthen the practical relevance of the findings. 3. Assumption Clarity: The paper makes several assumptions and it would be helpful to clarify the grounds on which these assumptions are made.  Questions and Suggestions for Authors: 1. Clarity in Assumptions: Could the authors elaborate on the assumptions made in the paper and discuss their implications on the proposed solution 2. Scalability: How scalable is the proposed allocation scheme to largerscale collaborative ML projects involving numerous parties with diverse datasets 3. Comparison: How does the proposed scheme compare with existing approaches in terms of fairness computational efficiency and scalability 4. Practical Implementation: Could the authors provide insights on the practical implementation of the proposed scheme in realworld collaborative ML settings 5. Future Work: Are there any specific avenues for future research or potential extensions to the current work that the authors are planning to explore  Limitations: The paper demonstrates the proposed allocation schemes effectiveness in collaborative ML scenarios through empirical results. However addressing the following limitations could further enhance the paper: 1. Addressing Practical Constraints: The authors could delve deeper into discussing practical constraints challenges and potential negative societal impacts arising from implementing their proposed scheme. Overall the paper presents a valuable contribution to the field of collaborative machine learning with a wellstructured proposal backed by theoretical analysis and empirical validation. Addressing the identified weaknesses and limitations can further strengthen the works merit and applicability.", "q9XPBhFgL6z": " Summary: The paper addresses the pressing need for a legal and regulatory framework regarding harm caused by autonomous systems. It critiques existing definitions of harm in philosophy literature and proposes a qualitative definition based on causal models and actual causality. The paper argues that its definition can handle complex scenarios involving autonomous systems interacting with humans.  Strengths: 1. Novel Contribution: The paper makes a significant contribution by proposing a formal definition of harm based on causal models contrasting with existing definitions. 2. Application Relevance: The focus on addressing harm caused by autonomous systems is timely and relevant given the increasing integration of such systems in various domains. 3. Thorough Analysis: The paper provides detailed explanations examples and comparisons with other approaches enhancing the understanding of the proposed definition.  Weaknesses: 1. Complexity: The papers highly technical content and formal definitions may be challenging for readers without a strong background in philosophy or causal models. 2. Conceptual Clarity: Some sections could benefit from greater clarity to make the content more accessible to a broader audience. 3. Lack of Empirical Examples: While theoretical examples are used inclusion of empirical case studies could enhance the practical applicability of the proposed definition.  Questions and Suggestions: 1. Clarification on Default Utility: Provide more insight into how the choice of default utility impacts the determination of harm in various scenarios. 2. Practical Implications: How can the proposed definition be practically applied in realworld scenarios involving autonomous systems to determine liability and responsibility 3. Comparison with Existing Legal Frameworks: Consider discussing how the proposed definition aligns with or differs from existing legal frameworks addressing harm caused by technologies.  Limitations: The authors should ensure that realworld applicability and practical implications of their research are adequately addressed. Addition of empirical case studies and discussions on the potential societal impact of adopting the proposed definition would enhance the papers overall robustness.", "oPzICxVFqVM": " Summary: The paper presents theoretical results regarding the relationship between scorebased generative models and the Wasserstein distance. It proves that scorebased models minimize the Wasserstein distance providing upper bounds on this metric under certain assumptions. The paper also conducts numerical experiments to validate the theoretical findings on synthetic datasets.  Strengths: 1. Theoretical Contributions: The paper provides rigorous theoretical results by leveraging optimal transport theory to demonstrate the relation between scorebased models and the Wasserstein distance. 2. Novelty: The use of optimal transport theory to analyze generative models is novel and provides a fresh perspective on understanding the behavior of scorebased models. 3. Experimental Validation: Numerical experiments on synthetic datasets support the theoretical findings enhancing the credibility of the results presented in the paper.  Weaknesses: 1. Complexity: The mathematical nature of the paper might make it challenging for readers unacquainted with advanced concepts in optimal transport theory to follow. 2. Practical Implications: While the theoretical results are valuable the paper could benefit from discussing practical implications and applications of the findings in realworld scenarios to enhance its impact.  Questions and Suggestions for Authors: 1. Clarity: Can the authors provide more intuitive explanations of the theoretical results for readers not wellversed in optimal transport theory 2. Generalization: How applicable are the findings to highdimensional realworld datasets given the difficulty of estimating the onesided Lipschitz constant in practice 3. Future Work: Are there plans to extend the analysis to address the convergence of the Wasserstein distance in scenarios where the initial and final distributions differ  Limitations: The authors could further address the challenge of estimating the onesided Lipschitz constant in highdimensional settings and discuss the practical implications and applications of their theoretical results to realworld data and models.  Funding Disclosure: The research was supported by the University of WisconsinMadison and NSF Award DMS2023239.", "sYDX_OxNNjh": "PeerReview: 1) Summary: The paper proposes a novel framework Recurrent Skill Training (ReST) to address the issue of exploration degradation in unsupervised skill discovery in reinforcement learning. By training skills one after another recurrently with a state coverage based intrinsic reward ReST aims to improve state coverage and skill diversity. Experimental evaluations on 2D navigation and robotic locomotion tasks demonstrate the superiority of ReST over parallel training approaches in terms of state coverage and skill divergence. 2) Assessment: Strengths:  The paper addresses an important issue in unsupervised skill discovery and provides a novel solution with clear motivation and methodology.  The explanation of exploration degradation and the proposed algorithm ReST are wellelaborated and supported with insightful analysis and illustrative examples.  Experimental results are comprehensive showcasing the effectiveness of ReST in achieving better state coverage and meaningful skill diversity compared to baseline methods.  The comparison with existing approaches and the discussion of limitations contribute to the paper\u2019s thoroughness. Weaknesses:  The paper lacks a detailed comparison with a wider range of stateoftheart unsupervised skill discovery methods potentially limiting the comprehensive evaluation of ReST.  While the proposed algorithm is welldescribed a more indepth discussion on the generalization of ReST to realworld and more complex scenarios could add value to the paper.  The discussion on the scalability challenges of ReST to continuous latent may require more exploration and potential solutions. 3) Questions and Suggestions for Authors:  How does the proposed ReST algorithm compare to other stateoftheart unsupervised skill discovery methods beyond DIAYN and DADS Could additional comparisons enhance the robustness of the evaluation  Can you provide insights on how ReST could be adapted or extended to handle continuous latent spaces for scalability in more complex tasks Are there potential tradeoffs in doing so  Could the authors elaborate on the practical implications and potential realworld applications of ReST in various domains beyond the experimental settings presented in the paper 4) Limitations: The authors adequately acknowledge the limitations of the proposed algorithm such as higher sample and computational complexity compared to previous approaches and the current lack of scalability to continuous latent. Future work should focus on optimizing sample efficiency computational complexity and extending ReST to handle continuous latent for broader applicability in hierarchical control tasks. In conclusion the paper presents a significant contribution to the field of reinforcement learning by addressing exploration degradation in unsupervised skill discovery. The methodology experimental results and analysis are strong with opportunities for further research to enhance the algorithms scalability and realworld applicability.", "kMiL9hWbD1z": " Summary: The paper proposes RTFormer an efficient dualresolution transformer for realtime semantic segmentation. RTFormer achieves a better tradeoff between performance and efficiency than CNNbased models by leveraging GPUFriendly Attention with linear complexity and introducing crossresolution attention. Extensive experiments show that RTFormer achieves stateoftheart results on Cityscapes CamVid and COCOStuff and promising results on ADE20K.  Strengths:  Innovative Approach: Introducing a dualresolution transformer for realtime semantic segmentation is a novel approach.  Efficiency: Leveraging GPUFriendly Attention and crossresolution attention for improved inference efficiency on GPUlike devices.  Experimental Results: Achieving stateoftheart performance on multiple benchmark datasets demonstrating the effectiveness of RTFormer.  Weaknesses:  Lack of Comparative Studies: The paper lacks indepth comparison with existing transformerbased methods for semantic segmentation.  Theoretical Foundation: More detailed explanation and theoretical validation of the proposed attention mechanisms could strengthen the paper.  Questions and Suggestions for Authors: 1. How does the computational complexity of RTFormer compare to existing CNNbased networks and other transformerbased methods 2. Have you considered exploring the realtime capabilities of RTFormer on more diverse datasets beyond those mentioned in the paper 3. Could you provide insights into the interpretability of the attention mechanisms used in RTFormer for semantic segmentation tasks 4. How does RTFormer perform in scenarios with limited computational resources such as edge devices  Limitations: The authors should further investigate and discuss the potential negative societal impact of their work particularly concerning the use and implications of realtime semantic segmentation in applications like autonomous driving and mobile applications. Suggestions for improvement could include adding a section dedicated to ethical considerations and societal impact. Overall the paper presents a promising approach to realtime semantic segmentation but further clarity comparative analysis and consideration of societal impact are recommended for enhancing the quality and impact of the research.", "pMumil2EJh": "Summary: The paper proposes a Temporal Polynomial Graph Neural Network (TPGNN) for accurate multivariate time series forecasting. The main contributions include introducing a novel dependence learning module that captures dynamic correlations and achieving stateoftheart forecasting performance on benchmark datasets. Strengths: 1. Novel Methodology: Introducing the TPGNN model to capture dynamic variable correlations in time series data is innovative and addresses the limitations of existing approaches. 2. Theoretical Analysis: The paper provides a theoretical analysis demonstrating the effectiveness of the proposed method in approximation and explores the gap between optimal and learned graphs. 3. Experimental Validation: Extensive experiments on synthetic and realworld datasets show that TPGNN outperforms existing methods in both shortterm and longterm forecasting tasks. 4. Ablation Study: The ablation experiments demonstrate the importance of key components in the TPGNN model and show that each design is essential for improved performance. Weaknesses: 1. Limited Realworld Validation: While the method is extensively validated on synthetic and benchmark datasets more realworld validation on diverse and complex datasets would strengthen the papers practical relevance. 2. Baseline Comparison: Although the paper claims superior performance over existing methods a more detailed comparison with additional stateoftheart models might further strengthen the findings. 3. Complexity: The proposed polynomial order analysis reveals that a higher order may adversely affect model performance raising concerns about the scalability and generalizability of the approach. Questions and Suggestions for Authors: 1. Dataset Diversity: Have you considered testing the TPGNN model on a broader range of realworld datasets especially those with varying complexities and characteristics outside of traffic and energy systems 2. Additional Baselines: Have you compared the proposed TPGNN model with more recent and diverse baseline methods beyond those mentioned in the paper to ensure a comprehensive evaluation 3. Scalability: How does the performance of TPGNN scale with increasing dataset sizes especially in scenarios with a high volume of multivariate time series data points Limitations: The authors could further discuss the potential negative societal impacts and ethical considerations that might arise from the use of advanced forecasting models like TPGNN. Suggestions for addressing this limitation include incorporating privacypreserving techniques and ensuring transparency in model deployment and decisionmaking processes. Constructive Suggestions for Improvement: To enhance the papers quality the authors could address the limitations mentioned provide a more detailed comparison with advanced and diverse baseline methods expand the realworld validation efforts and discuss the scalability and generalizability of the proposed TPGNN model. Additionally incorporating ethical considerations and societal impacts in the discussion section would enrich the papers value.", "vGQiU5sqUe3": " Summary: The paper presents a novel approach that integrates contrastive representation learning with goalconditioned reinforcement learning (RL). By casting contrastive representation learning methods as RL algorithms in their own right the paper demonstrates improved performance on goalconditioned RL tasks without the need for auxiliary perception losses data augmentation or separate representation learning objectives. The proposed method contrastive RL outperforms existing noncontrastive methods both in online and offline RL settings across a range of tasks.  Strengths: 1. Innovative Integration: The paper proposes a unique integration of contrastive representation learning and goalconditioned RL presenting a novel framework that leads to improved performance without the need for additional training objectives. 2. Empirical Results: The paper extensively evaluates the proposed contrastive RL method across various goalconditioned RL tasks and compares it to existing baselines. The empirical results consistently demonstrate superior performance of the proposed method. 3. Simplicity and Efficiency: The contrastive RL method offers a simpler alternative to existing methods while achieving comparable or better results. This simplicity along with the efficiency demonstrated during training adds practical value to the proposed approach.  Weaknesses: 1. Experimental Analysis: While the empirical results are strong the paper could benefit from a more detailed analysis of the methodologys limitations and failure cases across different task settings. 2. Generalization: The paper primarily focuses on goalconditioned RL tasks raising questions about the generalizability of the proposed contrastive RL framework to other RL domains. Investigating the performance on a broader set of RL problems would enhance the papers applicability.  Questions and Suggestions for Authors: 1. Generalization: How do you envision the application of contrastive RL to nongoalconditioned RL tasks and what challenges may arise in extending this framework to such domains 2. Scalability: Have you explored the scalability of the proposed method to largescale environments or complex tasks and are there any computational challenges that may arise in scaling up this approach 3. Comparison with Other Contrastive Methods: Have you compared the proposed contrastive RL approach with other strategies that directly incorporate contrastive learning into RL algorithms and how does it fare in terms of performance and computational efficiency  Limitations: The authors have not addressed the limitations and potential negative societal impacts of their work explicitly. To improve the paper it would be beneficial to include a discussion on potential ethical considerations related to the deployment of the proposed methodology in realworld applications.", "uOdTKkg2FtP": " Summary The paper introduces two novel methods offteam belief learning (OTBL) and offteam offbelief learning (OTOBL) to address issues faced by the offbelief learning (OBL) algorithm in the context of zeroshot coordination (ZSC) in the game of Hanabi. The methods aim to mitigate covariate shift and testtime issues experienced by OBL ultimately improving performance in cooperative settings.  Strengths 1. Innovative Approach: The paper introduces novel methods (OTBL and OTOBL) to address critical issues in ZSC and OBL showcasing significant potential in improving algorithm performance. 2. Empirical Evaluation: The methods are empirically evaluated in variants of Hanabi providing clear evidence of their effectiveness. 3. Thorough Explanation: The paper offers detailed explanations of the methods their implementation and experimental setups enabling clear understanding.  Weaknesses 1. Complexity: The papers concepts and methodologies are intricate potentially requiring advanced understanding and expertise. 2. Lack of Comparative Analysis: While the methods show improvements a direct comparison with other stateoftheart approaches could further validate their superiority. 3. Theoretical Justification: The paper lacks indepth theoretical justification for the proposed methods which could strengthen the validity and reliability of the approaches.  Questions and Suggestions 1. Clarification on Experimental Setup: Can the authors provide more details on the training setups hyperparameters and settings used in the empirical evaluation for better reproducibility 2. Scalability: How do the proposed methods scale with larger problem sizes and different game environments 3. Impact on Convergence: How do the introduced methods affect the convergence rate of the algorithms especially compared to traditional approaches  Limitations The authors should ensure they address the heuristics nature of the methods proposed provide more clarity on assumptions made and delve deeper into the theoretical grounding to strengthen the robustness of the proposed solutions.  Conclusion In conclusion the paper presents valuable contributions in addressing critical challenges in ZSC and OBL offering innovative solutions in the form of OTBL and OTOBL. By addressing the weaknesses and providing more theoretical grounding and broader context the authors can further enhance the impact and credibility of their work.", "nxl-IjnDCRo": " PeerReview of \"Analysis of the Denoising and Generative Capabilities of Diffusionbased Deep Generative Models\"  Summary: The paper focuses on analyzing the behavior of Diffusionbased Deep Generative Models (DDGMs) regarding their denoising and generative capabilities. It proposes to divide DDGMs into denoiser and generator components and introduces a new model  Denoising AutoEncoder with Diffusion (DAED). Through empirical experiments on three datasets the paper validates the proposed approach and discusses the impact on performance and generalizability.  Strengths: 1. Originality and Contribution: The paper makes a significant contribution by examining the transition point in DDGMs functionality from generating to denoising and proposing the DAED model. 2. Empirical Validation: The experimental results provide substantial support for the proposed approach showcasing the benefits of splitting DDGMs into denoising and generative parts. 3. Clarity and Organization: The paper is wellstructured with clear explanations of concepts methodologies and results. This enhances the understanding of the proposed approach.  Weaknesses: 1. Limited Theoretical Analysis: While the empirical results are robust the paper lacks indepth theoretical analysis to support the division of DDGMs into denoiser and generator components. More theoretical insights could strengthen the proposed model. 2. Comparison with Existing Models: The paper could benefit from a more detailed comparison with existing models in terms of performance metrics and computational efficiency to highlight the uniqueness of the proposed DAED model.  Questions and Suggestions for Authors: 1. Have you considered exploring the theoretical foundations for the split of DDGMs into denoising and generative parts further Providing theoretical insights could enhance the credibility of the proposed model. 2. How does the proposed DAED model compare with existing stateoftheart generative models in terms of performance metrics and computational efficiency A comparative analysis could highlight the advantages of the DAED approach. 3. Can you elaborate on the potential challenges in implementing the DAED model in realworld applications Discussing scalability and computational requirements could provide insights into the practical feasibility of the proposed model.  Limitations: The authors could further elaborate on the potential negative societal impacts of their work by considering ethical considerations related to the use of generative models for data generation and manipulation.  Overall Assessment: The paper presents a novel approach in analyzing DDGMs denoising and generative capabilities supported by robust empirical results. While the theoretical foundation could be strengthened the proposed DAED model shows promising results in enhancing performance and generalizability. Further refinements and theoretical insights could strengthen the papers contribution to the field of generative modeling.", "oW4Zz0zlbFF": "Summary The paper investigates the generalization performance of gradientbased meta learning with an overparameterized model focusing on the challenging bilevel structure and analyzing the meta linear regression setting. The authors aim to understand the phenomenon of \"benign overfitting\" in meta learning and provide theoretical contributions to this field. Strengths 1. Novel Contribution: The paper provides a unique analysis of the generalization performance of overparameterized gradientbased meta learning filling a gap in the existing literature. 2. Theoretical Analysis: The theoretical framework developed in the paper offers insights into the delicate interplay among data heterogeneity model adaptation and benign overfitting in gradientbased meta learning tasks. 3. Empirical Validation: The numerical simulations conducted provide empirical evidence to support the theoretical claims made in the paper. Weaknesses 1. Limited Scope: The analysis is limited to the meta linear regression setting which may restrict the generalizability of the findings. Suggestions for extending the analysis to nonlinear cases would enhance the papers relevance. 2. Complexity: The paper involves intricate mathematical derivations and technical language which may make it less accessible to a broader audience. Improving the clarity of presentation could enhance the papers impact. Questions and Suggestions for Authors 1. Extension to Nonlinear Models: Have the authors considered how their findings might be extended to nonlinear models commonly used in meta learning such as neural networks Providing insights on this aspect would enhance the practical applicability of the results. 2. Impact of Data Distribution: How does the robustness of the proposed model hold up when faced with realworld data distributions that may be more complex than the assumed subGaussian distributions Addressing the impact of different data distributions would strengthen the validity of the findings. 3. Comparison with StateoftheArt Techniques: How does the proposed analysis compare to stateoftheart techniques in the meta learning field Providing a comparison would demonstrate the novelty and significance of the proposed framework. Limitations The authors have not explicitly discussed the potential negative societal impact of their work. To address this they could consider outlining ethical considerations related to potential algorithmic biases or unintended consequences arising from the deployment of their meta learning model. Additionally providing recommendations on how to mitigate these risks would enrich the papers discussion on limitations.", "suplyBhTDjC": " Review of \"Clairvoyant Multiplicative Weights Updates for Convergence to Coarse Correlated Equilibria in General Games\" 1. Summary and Contributions: The paper introduces the Clairvoyant Multiplicative Weights Updates (CMWU) algorithm for convergence to Coarse Correlated Equilibria (CCE) in general games. CMWU utilizes predictive strategies based on tomorrows behavior to achieve constant regret in all normalform games. By leveraging a fixedpoint computation within a simplified framework CMWU offers an uncoupled learning dynamic that converges with a \\xce\\x98(log TT) rate to a CCE outperforming existing techniques. The paper presents detailed theoretical proofs and analyses to support the effectiveness of CMWU in game theory applications. 2. Strengths:  Novel Contribution: Introduces a unique algorithm CMWU showing improved convergence to CCE compared to existing methods.  Theoretical Depth: Provides thorough theoretical analysis and proofs to support the efficiency and effectiveness of CMWU.  Algorithm Simplicity: Demonstrates an efficient and simple operational process for CMWU dynamics allowing for practical implementation.  Relevance and Timeliness: Discusses the significance of the algorithm in the context of game theory online learning dynamics and AI applications showcasing its potential impact. 3. Weaknesses:  Complexity Concerns: The reliance on a fixedpoint computation raises concerns about computational complexity and practical implementation.  Limited Practical Validation: While the theoretical analysis is robust empirical validations or realworld applications could strengthen the papers findings.  Clarity in Presentation: Some sections particularly on technical details and proofs might be challenging for nonexperts to grasp easily. 4. Questions and Suggestions:  How does the computational efficiency of CMWU compare to other existing algorithms practically  Have you considered conducting experiments or simulations to validate the theoretical results of CMWU in realworld scenarios  Can you provide more insights into potential extensions or applications of CMWU beyond general games to enhance the papers impact and practical relevance 5. Limitations: The paper adequately addresses the limitations and societal impact concerns by emphasizing that CMWU is designed for gametheoretic purposes and may not directly translate to rational behavioral assumptions in all scenarios. Authors can further elaborate on the implications of using CMWU in practical applications and highlight areas where ethical considerations are crucial. Overall the paper presents an innovative algorithm CMWU with significant theoretical advancements in game theory showcasing potential for practical implementations and further explorations in the field. Strengthening empirical validations and discussing practical implications in realworld applications could enhance the papers overall impact and appeal to a broader audience.", "sof8l4cki9": " Review: 1) Brief Summary: The paper addresses the Fast Equilibrium Conjecture for stochastic gradient descent on scaleinvariant losses using neural networks with normalization layers trained by SGD with Weight Decay (WD). It proves that hidden progress in the model occurs in eO(1(\\xe2\\x8c\\x98)) steps leading to the convergence to an equilibrium where further training no longer improves accuracy. The analysis includes theoretical derivations formulations and experimental validation on CIFAR10 datasets. 2) Strengths and Weaknesses: Strengths:  Rigorous theoretical analysis leveraging continuous and discretetime dynamics for SGDWD.  Novel rescaling techniques introduced to adapt the optimization framework to scaleinvariant losses.  Empirical validation on CIFAR10 dataset confirming the theoretical predictions.  Clear exposition of assumptions and technical details for better understanding. Weaknesses:  The paper could benefit from a more detailed discussion on the experimental results to provide deeper insights.  Clarity in explaining the rescaling methods could be enhanced for better comprehension.  Potential overreliance on technical jargon and complexity could limit broader accessibility. 3) Questions and Suggestions for Authors:  Could the authors elaborate more on the practical implications of the Fast Equilibrium Conjecture in realworld deep learning applications  Can the authors discuss the computational complexity implications of the proposed rescaling techniques  How do the results and insights presented in the paper contribute to advancing the field of neural network optimization  A discussion on the implications of the findings for other optimization algorithms or network architectures could enhance the papers significance. 4) Limitations: The paper effectively addresses the limitations and potential negative societal impacts by focusing on theoretical derivations and empirical validation in a controlled experimental setup. Further the authors are encouraged to explore the broader implications and potential applications of their findings for realworld scenarios to enhance societal impact. Overall the paper significantly contributes to the theoretical understanding of neural network optimization explicitly demonstrating the convergence behavior of SGD with normalization layers on scaleinvariant losses. Further clarification and discussion on the practical implications of the findings would enrich the paper and increase its impact.", "zJNqte0b-xn": " Summary: The paper by Zhang et al. addresses the challenging task of translating minmax optimization algorithms from the Euclidean setting to Riemannian manifolds in machine learning applications. They introduce the Riemannian Corrected Extragradient (RCEG) method and study its convergence properties for geodesically stronglyconvexconcave objectives. The paper establishes lastiterate convergence results and extends these findings to the stochastic and nonsmooth cases.  Strengths: 1. Novelty and Contribution: The paper addresses an important problem in machine learning related to optimization on Riemannian manifolds offering new insights and algorithms. 2. Theoretical Soundness: The paper provides a rigorous theoretical analysis proving convergence guarantees for the RCEG method in various settings including geodesically stronglyconvexconcave and convexconcave cases. 3. Comprehensive Analysis: The work covers both deterministic and stochastic settings offering a detailed assessment of algorithm performance under different conditions. 4. Quality of Experiments: The numerical experiments supplement the theoretical results demonstrating the efficacy of the proposed algorithms in practical applications such as robust principal component analysis.  Weaknesses: 1. Complexity of Presentation: The paper requires a high level of familiarity with optimization theory and Riemannian geometry which might make it less accessible to a broader audience. 2. Experimental Scope: The experiments focus solely on the robust principal component analysis problem limiting the generalizability of the findings to a wider range of machine learning applications. 3. Lack of Comparative Analysis: The paper lacks a comparison with existing stateoftheart algorithms or a discussion on how the proposed method performs compared to other approaches.  Questions and Suggestions for Authors: 1. Generalizability: Have you considered how the proposed RCEG method could be applied to other machine learning tasks beyond robust principal component analysis 2. Algorithm Efficiency: Have you analyzed the computational complexity and scalability of the RCEG method especially in largescale optimization problems 3. Sensitivity to Noise: How robust is the RCEG method to noisy gradient estimates in practical optimization scenarios Have you considered alternative strategies for handling noise in the stochastic variant of the algorithm 4. Realworld Applicability: Can you provide insights into how the proposed algorithm can be applied to realworld datasets and scenarios considering factors like data sparsity and high dimensionality  Limitations: The authors have not fully addressed the potential negative societal impact or ethical considerations of their work. It would be beneficial to include a discussion on ethical implications and societal consequences of deploying optimized machine learning algorithms in diverse applications.", "tIqzLFf3kk": " Summary: The paper investigates the behavior of rank in deep neural networks focusing on the concept of rank deficiency and its implications for network performance. The study combines theoretical analysis and empirical evidence to demonstrate monotonic decreasing rank behavior with deepening network layers. It establishes that network ranks decrease due to chain rules of differential operators and matrix multiplications supported by numerical tools for efficient validation. Furthermore it uncovers an independence deficit phenomenon where categories can be predicted based on the confidence of other categories potentially impacting network performance in generalized data domains.  Strengths: 1. Theoretical Foundation: The paper establishes a solid theoretical foundation for the rank behavior of deep neural networks deriving insights from basic principles of differential operators and matrix multiplication. 2. Empirical Validation: The study provides empirical evidence through numerical tools to validate the1rank diminishing behavior of deep neural networks across various architectures such as CNNs MLPs and Transformers on ImageNet. 3. Novel Findings: Uncovering the independence deficit phenomenon provides a novel perspective on network behavior and sheds light on potential performance limitations.  Weaknesses: 1. Limited Discussion of Related Work: While the paper briefly mentions related studies on rank behavior in specific network architectures and implicit selfregularization a deeper discussion and comparison with these works could enhance the context and understanding of the research. 2. Complexity and Accessibility: The papers theoretical discussions and numerical tools may be challenging for readers unfamiliar with advanced mathematical concepts potentially limiting its accessibility to a broader audience.  Questions and Suggestions: 1. Clarification on Empirical Results: Can the authors provide a more detailed explanation of how the empirical results validate the theoretical rank diminishing behavior observed in the deep neural networks studied 2. Generalizability: How transferable are the findings on rank behavior across different network architectures beyond the ones explored in the empirical analysis  Limitations: The authors could improve the discussion of potential negative societal impacts by considering how rank deficiency in deep neural networks may affect outcomes in realworld applications. Additional insights on addressing such limitations could enhance the papers societal relevance.", "lMrpZ-ycIaT": " Summary The paper proposes novel selfsupervised learning frameworks for neuroimaging data inspired by natural language processing (NLP) techniques. The frameworks aim to pretrain deep learning models on a largescale neuroimaging dataset and evaluate their performance in mental state decoding tasks. Three main frameworks are introduced: autoencoding causal sequence modeling (CSM) and SequenceBERT. The results show that models pretrained in a learning framework based on causal language modeling outperform others with generally improved decoding performance over baseline models.  Strengths 1. Innovative Approach: The introduction of selfsupervised learning frameworks for neuroimaging data inspired by NLP techniques is a novel and promising approach. 2. Comprehensive Evaluation: The paper thoroughly evaluates the pretrained models on benchmark mental state decoding datasets demonstrating better performance compared to models trained from scratch. 3. Public Availability: The authors make their code training data and pretrained models publicly available enhancing reproducibility and promoting further research in the field.  Weaknesses 1. Limited Comparison: The paper lacks a comparative analysis with existing methods in the field of neuroimaging data pretraining and mental state decoding. 2. Societal Impact: While the authors briefly address potential negative societal impacts a more indepth discussion or analysis on ethical considerations and data privacy concerns could be beneficial.  Questions and Suggestions for Authors 1. Generalization: How do the pretrained models perform when dealing with diverse datasets with varied acquisition protocols and demographics 2. Interpretability: Have the authors considered incorporating interpretability techniques to understand the decisions made by the pretrained models in mental state decoding 3. Future Work: What are the next steps in this research Are there plans to extend the study to other related domains or improve the frameworks further  Limitations The authors should consider including more indepth analyses on attribution methods for mental state decoding and comparing the proposed selfsupervised learning frameworks with contrastive learning techniques commonly used in other domains for a more comprehensive evaluation.  Improvements To enhance the paper further the authors can provide a more extensive discussion on the broader societal impacts and ethical considerations associated with their work particularly regarding data privacy and potential biases in pretraining models on public neuroimaging data. Overall the paper presents a valuable contribution to the field of neuroimaging and selfsupervised learning with significant potential for advancing mental state decoding research.", "rQ1cNbi07Vq": " Summary The paper introduces the application of the Shapley value metric to the frequency domain to quantify the contribution of individual frequency components on convolutional neural networks (CNNs). It aims to address the brittleness of CNNs caused by the reliance on highfrequency components (HFCs) not perceivable to humans. By quantifying the impact of each frequency component through Shapley values the paper explores the instance disparity the adversarial robustness of adversarial training (AT) the fairness problem in AT between different classes and the attack bias on different frequency components. Furthermore a Shapleyvalue guided data augmentation technique named Classwise Shapley valueguided Augmentation (CSA) is proposed to improve robustness showing positive results on image classification benchmarks.  Strengths  Innovative Approach: Introducing Shapley value to quantify the impact of frequency components in the frequency domain is novel and provides finegrained insights.  Thorough Analysis: The paper provides comprehensive analysis on the instance disparity adversarial robustness fairness in AT and attack bias on different frequency components.  Effective Technique: The proposed CSA data augmentation method based on Shapley values shows promising results in improving robustness on various models.  Weaknesses  Complexity: The theoretical approach with the use of Shapley value and intricate metrics may make the concept challenging for nonexperts.  Dataset Limitation: The experimental results are limited to specific datasets and models potentially limiting the generalizability of the findings.  Lack of Comparison: Comparison with existing methods or approaches could strengthen the validation of the proposed method.  Questions and Suggestions 1. Can you elaborate on any specific challenges faced during the application of Shapley values to the frequency domain and how they were overcome 2. How do the findings of the instance disparity impact the generalization of the proposed CSA method across different datasets or models 3. Have you considered exploring the applicability of your approach to other neural network architectures beyond CNNs such as Transformers or GNNs  Limitations The authors have appropriately addressed the limitations focusing on the specific nature of the research and its potential impact on the robustness of machine learning models. To further improve the authors could enhance the discussion on the scalability and wider applicability of their Shapleybased approach.", "p_BVHgrvHD4": "Summary: The paper proposes a novel informationtheoretic framework to analyze the sample complexity of data generated by deep neural networks. It introduces the concept of regret and sample complexity under this framework and studies two types of deep network architectures. The main contributions are novel sample complexity bounds for ReLU neural networks and deep networks with bounded sum of weights. Strengths: 1. Novel Framework: The paper introduces a fresh perspective through the informationtheoretic framework offering a unique approach to analyze deep neural networks data requirements. 2. Theoretical Contribution: The derivation of sample complexity bounds for different deep network architectures is a significant theoretical contribution. 3. Rigorous Analysis: The paper provides detailed mathematical proofs and establishes relationships between error metrics and information gain. 4. Addressing Current Gaps: The paper addresses shortcomings in existing theoretical analyses and offers new insights for understanding the sample complexity in deep learning. Weaknesses: 1. Lack of Empirical Validation: The paper solely focuses on theoretical analysis and lacks empirical validation of the proposed frameworks effectiveness in practical scenarios. 2. Complexity: The paper involves intricate mathematical proofs and concepts which may make it challenging to grasp for readers without a strong background in information theory and machine learning. 3. Limited Discussion on Practical Implementation: The paper does not delve into practical algorithmic implementations or discuss the feasibility of achieving the derived sample complexity bounds in realworld applications. Questions and Suggestions: 1. How do the proposed sample complexity bounds compare to existing bounds in terms of practical relevance and applicability to realworld deep learning systems 2. Could the authors provide examples or case studies to illustrate how their informationtheoretic framework could be utilized in actual machine learning tasks 3. Have the authors considered extensions of their framework to address other areas in machine learning such as semisupervised learning or reinforcement learning Limitations: The paper does not discuss the practical aspects of implementing the proposed framework or provide empirical validation. Considering the gap between theoretical analysis and practical application the authors could address this limitation by including experiments or case studies to validate the frameworks effectiveness in realworld scenarios.", "uuaMrewU9Kk": " Abstract The paper presents a novel approach for multitask reinforcement learning (RL) with diverse offline datasets. The proposed technique aims to leverage the relation of multiple tasks and common skills learned across those tasks to effectively deal with complex realworld problems. The model utilizes a task decomposition method Wasserstein autoencoder and qualityweighted loss to decompose tasks into achievable subtasks and enable joint learning of skills and tasks. Additionally the model augments datasets with imaginary trajectories relevant to highquality skills to enhance performance for offline RL agents. Experimental results showcase the robustness and outperformance of the proposed approach compared to stateoftheart algorithms in robotic manipulation and drone navigation tasks.  Strengths 1. Innovative Approach: The paper introduces a unique skillbased multitask RL model utilizing task decomposition and qualityaware joint learning offering an effective solution for offline RL with heterogeneous datasets. 2. Experimental Validation: Extensive experiments demonstrate the superiority and robustness of the proposed model over existing algorithms in diverse robotic manipulation and drone navigation settings. 3. Detailed Methodology: The paper provides a comprehensive explanation of the method including learning skill and task embeddings task decomposition and data augmentation enhancing clarity and replicability.  Weaknesses 1. Complexity: The models complexity might hinder its scalability and applicability to largescale problems potentially requiring additional computational resources. 2. Lack of Comparative Analysis: While the experimental results show the effectiveness of the proposed model a more indepth comparison with a wider range of existing algorithms could have provided a broader perspective.  Questions and Suggestions for Authors 1. Could you elaborate on how the proposed method can handle scalability issues when applied to realworld problems with highdimensional state spaces 2. Have you considered incorporating interpretability features for the model to enhance transparency and facilitate trust in its decisionmaking process 3. How does the proposed model generalize to tasks beyond robotic manipulation and drone navigation especially in scenarios with diverse dynamics and constraints  Limitations The paper adequately addresses the limitations and potential negative societal impact of the work. To further improve authors could consider discussing the ethical implications of deploying RL agents in realworld applications and potential unintended consequences of the proposed model.", "toleacrf7Hv": " Summary: The paper proposes a criterion based on the Brenier formulation of Optimal Transport (OT) to select the best potential for mapping probability distributions in a stochastic setting. The criterion aims to minimize the quadratic error to the ground truth OT map. The authors showcase the effectiveness of their method through experimental evaluations on synthetic datasets and Domain Adaptation applications.  Strengths: 1. Innovative Approach: The use of the Brenier formulation to guide the selection of potential in OT is novel and potentially impactful. 2. Theoretical Foundation: The paper provides a strong theoretical foundation for the proposed criterion demonstrating its formal validity. 3. Experimental Validation: The experiments on synthetic datasets and Domain Adaptation tasks provide concrete evidence of the effectiveness of the proposed method. 4. Clarity and Organization: The paper is wellstructured clearly explaining the problem methodology and results.  Weaknesses: 1. Complexity and Accessibility: The paper delves into intricate mathematical and computational details which might make it challenging for readers not specialized in the field to grasp fully. 2. Limited Experiment Variety: While the synthetic experiments and Domain Adaptation case study are insightful more diverse and complex scenarios could enhance the papers validity and generalizability. 3. Algorithmic Details: More details on the implementation and computational complexities could benefit practitioners seeking to apply the proposed methodology.  Questions and Suggestions for Authors: 1. Clarification on Regularity Assumptions: Can you provide more insights into the limitations of the regularity assumptions and how they impact the applicability of the proposed criterion in realworld scenarios 2. Generalization to Different Cost Functions: Have you considered extending the methodology to cover a broader range of cost functions beyond the quadratic cost and if so what obstacles might arise 3. Scalability Concerns: How does the proposed method scale with increasing dataset size or dimensionality especially for the Domain Adaptation use case where realworld datasets can be large and highdimensional  Limitations:  Addressing Negative Societal Impact: The paper does not explicitly address any direct negative societal impacts. To enhance ethically responsible research practices considerations of potential limitations and implications for society should be included in the discussion.  Suggestions for Improvement:  Consider discussing potential societal implications and ethical considerations related to the algorithms use to ensure responsible deployment and mitigate possible negative consequences.  Enhance the accessibility of the paper by providing more detailed explanations of the methodology for broader comprehension without sacrificing scientific rigor. Overall the paper presents a significant advancement in the field of Optimal Transport and offers promising new directions for future research. Efforts to enhance clarity expand experimental diversity and address scalability concerns would further strengthen the impact of this work.", "lbQTJN42uea": " Summary The paper presents a novel subquadratictime algorithm for solving Kronecker regression problems specifically focusing on the Tucker decomposition of tensors. By leveraging leverage score sampling and iterative methods the algorithm achieves significant improvements in running time making it suitable for realworld applications.  Strengths  The paper addresses a significant problem in tensor decomposition offering a subquadratictime solution for Kronecker regression with practical applications.  The use of leverage score sampling and iterative methods showcases a thoughtful approach to improving efficiency.  The incorporation of blockdesign matrices and extension to Kronecker ridge regression demonstrates the versatility and scalability of the proposed algorithm.  The experiments and comparisons presented provide solid empirical evidence of the algorithms effectiveness.  Weaknesses  The complexity of the algorithm and the technical details may pose a challenge for readers unfamiliar with the field.  The paper could benefit from further insights on the algorithms theoretical guarantees and the implications for different data scenarios.  Questions and Suggestions for Authors 1. Could you provide a more intuitive explanation of how leverage score sampling and iterative methods contribute to the efficiency of the algorithm 2. How does the algorithm handle noisy or incomplete data and are there any specific considerations for robustness in practical applications 3. Could you elaborate on the implications of the algorithms speed and accuracy on downstream tasks as mentioned in the abstract 4. In what ways could the algorithm be further optimized or extended especially considering different types of tensors or data structures  Limitations The authors have effectively addressed the limitations and potential negative societal impacts of their work by highlighting the algorithms speed and accuracy. However it may be beneficial to include a discussion on potential computational challenges or ethical considerations associated with applying the algorithm in practice.  Suggestions for Improvement To enhance the paper the authors could consider providing more accessible explanations of key concepts and highlighting the practical implications of the algorithm in different realworld scenarios. Additionally further discussions on handling noise scalability and applications beyond tensor decomposition could enrich the papers impact and broaden its audience appeal.", "wiBEFdAvl8L": " PeerReview of \"GLIPv2: Unifying Localization and VisionLanguage Understanding\"  1) Summary: The paper introduces GLIPv2 a model aiming to bridge localization tasks with VisionLanguage (VL) understanding tasks by proposing a unified pretraining approach. GLIPv2 unifies localization pretraining and VisionLanguage Pretraining (VLP) through three pretraining tasks: phrase grounding regionword contrastive learning and masked language modeling. Experimental results demonstrate GLIPv2s high performance across various localization and understanding tasks showcasing strong zeroshot and fewshot adaptation capabilities.  2) Strengths and Weaknesses: Strengths:  The paper addresses a significant challenge in the field by unifying localization and VL understanding tasks showing excellent performance across both task types.  Innovative approach with three pretraining tasks aimed at enhancing grounding capabilities.  Experimental results demonstrate competitive performance on various tasks showcasing strong adaptation abilities.  Clear methodology and detailed description of the model architecture.  Code availability on GitHub for reproducibility. Weaknesses:  Limited discussion on the scalability of the proposed model.  The paper could benefit from more indepth analysis of the data sources and potential biases.  The comparison to existing tasks and models could be expanded to provide a more comprehensive evaluation.  3) Questions and Suggestions:  Could the authors provide insights into the scalability of GLIPv2 particularly when applied to larger datasets or more complex tasks  How does the proposed model handle potential bias or unintended private information in the largescale web data utilized for pretraining  Further exploration into the models interpretability and explainability aspects could enhance the papers impact.  More detailed comparisons with existing methods could provide a more robust evaluation framework.  4) Limitations: While the paper demonstrates strong performance and innovative contributions additional considerations on scalability potential biases and model interpretability are essential for addressing limitations. Constructive suggestions for future work could include more extensive evaluations on diverse datasets additional analysis on model explainability and mitigation strategies for potential societal impacts. In summary the paper presents a promising approach in addressing the unification of localization and VisionLanguage understanding tasks through GLIPv2. Emphasis on addressing scalability biases and interpretability challenges could further enhance the impact and applicability of the proposed model.", "s776AhRFm67": " Summary: The paper presents an oracleefficient algorithm RoBoost for boosting the adversarial robustness of barely robust learners. The main contributions include defining barely robust learners proving the equivalence between barely robust learning and strongly robust learning and proposing an algorithm to boost barely robust learners to strongly robust learners.  Strengths:  The paper presents a novel algorithm RoBoost for boosting the robustness of barely robust learners.  The theoretical analysis provides a clear understanding of the relationship between barely robust learning and strongly robust learning.  The experiments demonstrate the effectiveness of RoBoost in improving the robust accuracy of blackbox learning algorithms.  Weaknesses:  The paper lacks detailed explanations of the practical implications of the proposed algorithm in realworld scenarios.  The experimental section could be enhanced with more diverse datasets and a comparison with other stateoftheart algorithms.  The paper assumes a certain level of familiarity with the theoretical concepts of robust learning which might make it less accessible to readers new to the field.  Questions and Suggestions: 1. Can you elaborate on the potential realworld applications of the proposed boosting algorithm RoBoost 2. How does the performance of RoBoost compare with other boosting algorithms or adversarial training methods in terms of computational efficiency 3. Have you considered conducting experiments on more challenging datasets or different types of perturbation sets to further validate the effectiveness of the algorithm 4. Could you provide more insights into the interpretability of the models generated by RoBoost and how they compare with the baseline models  Limitations: The paper could benefit from addressing the following limitations:  Lack of detailed discussion on the practical implications and application scenarios of the proposed algorithm.  Limited experimentation on diverse datasets and perturbation sets to provide a comprehensive assessment of the algorithms performance and generalizability. To improve the authors should consider expanding the experimental evaluation offering more practical insights and enhancing the readability for a broader audience.", "skgJy0CjAO": " Summary: The paper introduces LogiGAN an unsupervised adversarial pretraining framework designed to enhance logical reasoning abilities of language models. It identifies logical reasoning phenomena in the text corpus trains models to predict masked logical statements and uses an adversarial GeneratorVerifier architecture inspired by reflective thinking in human learning. LogiGAN implements a sequential GAN approach to enable logic learning and demonstrates improved performance on 12 datasets requiring general reasoning abilities.  Strengths: 1. Innovative Approach: The paper introduces a novel approach to enhancing logic abilities in language models addressing an important gap in the field. 2. Theory Grounded in Cognitive Psychology: The authors draw on theories of reflective thinking from cognitive psychology to support their methodology. 3. Extensive Experiments: The paper conducts comprehensive experiments and ablation studies to validate the effectiveness of LogiGAN.  Weaknesses: 1. Complexity: The technical details and terminology used in the paper can be overwhelming for readers not familiar with deep learning and sequential GANs. 2. Evaluation: While the paper demonstrates performance improvements a more detailed comparison with existing methods for logical reasoning enhancement could provide a stronger evaluation.  Questions and Suggestions: 1. Data Preprocessing: How was the corpus preprocessed to filter out nonlogical content and ensure the quality of the training data 2. Scalability: How does LogiGAN perform in terms of scalability with larger datasets or more complex logical reasoning tasks 3. Limitations: Have the authors considered potential biases or limitations in using heuristics for identifying logical reasoning phenomena How could these be addressed  Limitations: The authors could provide more discussion on the potential societal impact of advancing logic abilities in language models including ethical considerations related to decisionmaking based on logical reasoning.  Suggestions for Improvement: 1. Clarity: Simplify technical explanations and clarify complex concepts for a wider audience to improve comprehension. 2. Comparison: Include a more thorough comparison with existing methods for logical reasoning enhancement to highlight the uniqueness and effectiveness of LogiGAN. Overall the paper presents a promising approach for enhancing logical reasoning abilities in language models and opens up avenues for further research in this domain.", "ufRSbXtgbOo": " Summary The paper introduces the problem of multiagent performative prediction (MultiPfD) where agents learn a common decision vector influenced by their decisions. It formulates MultiPfD as a decentralized optimization problem and introduces the DSGDGD scheme to address it. Key contributions include proving conditions for the existence of a unique MultiPS solution analyzing the convergence of DSGDGD and validating through numerical experiments.  Strengths 1. Novelty: Addresses a relevant and emerging problem of multiagent performative prediction. 2. Theoretical Rigor: Provides necessary and sufficient conditions for the existence of the MultiPS solution and analyzes convergence rates with thorough theoretical proofs. 3. Practicality: Introduces the DSGDGD scheme as a decentralized approach to tackle MultiPfD and validates it with numerical experiments using synthetic and real data. 4. Clarity and Structure: The paper is wellstructured with a clear problem statement method description and results presentation.  Weaknesses 1. Assumptions and Limitations: Reliance on strong assumptions like synchronous updates and strongly convex loss functions may limit the applicability of the proposed approach in more practical scenarios. 2. Complexity: The technical details and mathematical proofs might make it challenging for readers not deeply familiar with optimization theory to understand the paper fully. 3. Clarity of Experiments: While numerical experiments are conducted more detailed results and analysis could enhance the clarity and impact of the findings.  Questions and Suggestions 1. Clarification: Can the authors provide more intuition or examples to help understand the implications of the theoretical results 2. Scalability: How scalable is the proposed DSGDGD scheme to a larger number of agents or more complex networks 3. Comparison: How does the proposed approach compare with stateoftheart methods for similar multiagent prediction problems 4. RealWorld Application: Have there been any considerations for practical implementation challenges or realworld applications of the proposed solution  Limitations The authors might benefit from addressing the following limitations:  Scalability Concerns: Address the scalability of the proposed DSGDGD scheme especially in scenarios with a large number of agents or complex network topologies.  RealWorld Applicability: Discuss potential challenges in implementing the proposed approach in realworld settings and suggest ways to mitigate these challenges for wider adoption. Overall the paper makes a significant contribution to the field of multiagent performative prediction but could benefit from addressing the identified weaknesses and limitations.", "sexfswCc7B": " Summary: The paper investigates the tendency of largescale neural language models to generate consecutive sentencelevel repetitions and proposes a trainingbased method named DITTO to mitigate this issue. The study reveals the prevalence of repetition and selfreinforcement effects in language models and demonstrates that DITTO not only reduces repetitions but also improves generation quality. Extensive experiments on openended text generation and summarization tasks validate the effectiveness of the proposed method.  Strengths: 1. Indepth Analysis: The paper conducts a quantitative investigation into the reasons for sentencelevel repetitions providing detailed insights into the issues faced by language models. 2. Novel Contribution: The proposal of the DITTO training method is innovative and addresses the repetition problem effectively without sacrificing perplexity. 3. Experimental Validation: Extensive experiments on both text generation and summarization tasks demonstrate the generality and effectiveness of the proposed method. 4. Compatibility: The paper shows compatibility with popular stochastic decoding strategies enhancing the applicability of the proposed method.  Weaknesses: 1. Limitation in Analysis: While the quantitative investigation is thorough the paper could benefit from further analysis linking the repetition phenomenon to model embedding or neural network architecture. 2. Societal Impact Discussion: The paper lacks a detailed discussion on the potential negative societal impacts of language models with repetitive tendencies.  Questions and Suggestions: 1. Can the authors provide insights into how the proposed DITTO method can be further improved or optimized for specific tasks or datasets 2. Have the authors considered the computational overhead or training time implications of implementing DITTO on larger language models or datasets  Limitations: The authors have not adequately addressed the potential negative societal impact of repetitive language models. To improve the authors could consider discussing ethical considerations related to generating repetitive content and potential consequences when used in realworld applications.  Overall: The paper presents a comprehensive analysis of the repetition issue in language models and proposes an effective training method to alleviate this problem. By addressing some limitations and discussing societal impacts the paper can further enhance its contributions to the field of natural language processing.", "s_PJMEGIUfa": " Summary: The paper introduces a novel approach called LanguageInterfaced FineTuning (LIFT) that leverages pretrained language models (LMs) for solving nonlanguage downstream tasks without altering the model architecture or loss function. LIFT converts labeled samples into sentences finetunes pretrained LMs on these sentences and performs inference using the model. The paper conducts an extensive empirical study on a range of classification and regression tasks demonstrating the effectiveness of LIFT in achieving good performance comparable to baselines.  Strengths: 1. Innovative Approach: LIFT introduces a novel method to adapt pretrained LMs to nonlanguage tasks without architectural modifications which is a significant contribution. 2. Extensive Empirical Study: The paper conducts a thorough empirical study on various datasets showcasing the efficacy of LIFT across different tasks and settings. 3. Empirical Results: The results demonstrate competitive performance of LIFT compared to baselines highlighting its potential for practical applications. 4. Calibration and Data Generation: The paper explores important aspects such as calibration of predictions and data generation capabilities of LIFT adding depth to the analysis. 5. Improvements: The paper suggests techniques like twostage finetuning and data augmentation to enhance the performance of LIFT demonstrating a commitment to advancing the proposed method.  Weaknesses: 1. Limitations Acknowledgment: While limitations are briefly mentioned a more detailed discussion on the scalability of LIFT to highdimensional feature spaces and large class sizes would enhance the assessment. 2. Societal Impact Discussion: The paper does touch upon fairness concerns briefly but a more indepth examination of societal ramifications and potential bias introduced by LIFT would strengthen the paper. 3. Memory Efficiency: The paper mentions memory inefficiency of LIFTGPT models but more exploration or potential solutions in this regard would add value.  Questions and Suggestions: 1. Suitability for HighDimensional Data: How does LIFT perform with highdimensional features in regression tasks 2. Societal Impact: Can the authors propose methods to mitigate potential biases introduced by the use of feature names in prompts within LIFT 3. MemoryEfficient LMs: Have you considered employing memoryefficient LMs to address the memory inefficiency issue of LIFTGPT models 4. Ensemblelike Behavior: Can you delve deeper into the behavior of LMs and LIFTGPT models concerning ensemble methods or decision tree characteristics  Limitations: The paper addresses limitations but could benefit from a more detailed exploration of scalability to highdimensional data and societal impact considerations with proposals for improvement in these areas.  Overall: The paper presents a novel approach LIFT for adapting pretrained LMs to nonlanguage tasks supported by an extensive empirical study. Strengthening the discussion on limitations societal impact and memory efficiency can enhance the papers impact and relevance.", "sQiEJLPt1Qh": " PeerReview:  1) Summary: The paper proposes tighter bounds and a polynomial time algorithm for representing any continuous piecewise linear (CPWL) function using Rectified Linear Unit (ReLU) neural networks. The existing literature indicates an exponential growth in the number of hidden neurons needed to represent a CPWL function especially with respect to the pieces and linear components. The paper establishes quadratic and bilinear bounds that are independent of the input dimension thereby reducing the neural complexity significantly. Various theorems and algorithms are presented to support these findings.  2) Strengths and Weaknesses: Strengths:  The paper makes significant contributions by providing improved bounds and a polynomial time algorithm for representing CPWL functions with ReLU networks.  The theoretical framework is welldeveloped with clear definitions theorems and algorithms.  The authors successfully challenge and improve upon existing exponential growth complexity in representing CPWL functions offering more efficient solutions.  Rigorous proofs and comparisons with prior work demonstrate the novelty and advancement of the proposed methodology. Weaknesses:  The paper is highly theoretical and might be challenging for nonexperts in the field to grasp the technical details.  Limited empirical validation or application of the proposed algorithm could reduce the practical relevance of the work.  The complexity of the papers content might require a high level of mathematical background to fully understand the contributions.  3) Questions and Suggestions for Authors:  Can you provide more intuitive examples or visualizations to aid readers in understanding the implications of the improved bounds and algorithm proposed  Have you considered any practical applications or case studies to demonstrate the utility of your results in realworld scenarios  How do your results compare with stateoftheart methods in terms of computational efficiency and performance when dealing with CPWL functions in various applications  Are there any potential extensions or variations of your approach that could be explored to further enhance the efficiency or applicability of representing CPWL functions with neural networks  4) Limitations: The paper lacks exploration of practical applications or empirical validations of the proposed method. To enhance the impact and relevance of the work authors should consider conducting experiments on real datasets or scenarios to demonstrate the practical significance of their findings. Additionally addressing potential negative societal impacts if any is crucial for ensuring responsible research conduct. Overall the paper presents valuable contributions to the field of representing CPWL functions using ReLU neural networks. Clarity and contextualization of the theoretical advancements with practical examples or applications could further enhance the impact of the research.", "nYrFghNHzz": " Summary: The paper proposes a novel method Supervised Clustering approach using Adaptive Fusion (SCAF) to cluster treatments with similar effects and estimate Individualized Treatment Rules (ITRs) simultaneously. The method addresses the challenge of large treatment spaces with heterogeneous effects. It utilizes convex optimization with a fusion penalty and achieves superior performance in simulations and a real data application on cancer treatments.  Strengths: 1. Innovative Approach: The paper introduces a novel method SCAF to address complex challenges in precision medicine efficiently. 2. Theoretical Grounding: The theoretical guarantees provided for the methods consistency and convergence lend credibility to the proposed approach. 3. Clear Presentation: The paper is wellstructured with clear descriptions of the problem methodology and experiments enhancing readability and understanding.  Weaknesses: 1. Complexity: The paper presents a highly technical and specialized topic which might hinder comprehension for readers outside the specific domain of precision medicine and machine learning. 2. Limited Comparison: While the method outperformed existing approaches in simulations and real data a more extensive comparison with a broader range of methods could enhance the papers robustness.  Questions and Suggestions for Authors: 1. Computational Efficiency: How does the proposed algorithm scale with increasing dataset sizes especially for realworld applications with extensive patient data 2. Interpretability: How can the clustering results be interpreted in a clinical setting aiding doctors in decisionmaking How can these results be visualized effectively 3. Generalizability: Have you considered how the method could be extended or adapted for different diseases or treatment contexts beyond cancer  Limitations: The authors have adequately addressed the limitations and potential negative societal impact in Section 6. A constructive suggestion for improvement in this area could be to further elaborate on the ethical considerations specific to the use of patient data and the implications for patient privacy. Overall the paper presents a promising approach with strong theoretical underpinnings and practical applications in precision medicine. Addressing the suggested points could further enhance the impact and understanding of the proposed method.", "sjaQ2bHpELV": " Review: 1) Summary: The paper investigates online reinforcement learning for optimizing policy spaces with mixed scopes in a causal setting. It introduces novel algorithms to achieve sublinear regret compared to optimal agents deployed in the environment. The contributions include developing an online learning algorithm for identifying optimal policies in unknown structural causal models introducing a parametrization for SCMs with finite latent states and a computationally efficient algorithm. The paper addresses challenges in learning optimal policies with mixed scopes and provides theoretical foundations and algorithms for policy optimization. 2) Strengths:  Theoretical Contributions: The paper makes significant theoretical contributions by addressing the challenges of optimizing policies with mixed scopes in unknown structural causal models.  Algorithm Development: The novel online learning algorithms developed showcase the innovation in addressing complex optimization problems.  Clear Presentation: The paper is wellstructured and provides a detailed explanation of the problem contributions and methodologies used. 3) Weaknesses:  Complexity: The paper delves into complex theoretical concepts and algorithms which may make it less accessible to a broader audience.  Lack of Empirical Evaluation: Limited empirical evaluation is provided to demonstrate the effectiveness of the proposed algorithms in realworld scenarios. 4) Questions and Suggestions for the Authors:  Can you provide more insights on the practical applications of the proposed algorithms in healthcare or other domains  How do your algorithms handle scalability issues when dealing with largescale causal models with numerous variables and interactions  Have you considered conducting empirical studies to evaluate the performance of your algorithms on realworld datasets to showcase practical applications and compare with existing approaches 5) Limitations: The authors should consider conducting empirical studies to validate the effectiveness of the proposed algorithms in practical scenarios and provide insights into their performance scalability and usability in realworld applications. 6) Constructive Suggestions for Improvement:  Include more empirical evaluations: Conduct experiments on realworld datasets to showcase the performance and practical applicability of the proposed algorithms.  Simplify explanations: Consider providing more intuitive examples and graphics to make the concepts more accessible to a wider audience.  Discuss broader implications: Elaborate on the potential societal impact and ethical considerations of using these algorithms in decisionmaking processes. Overall the paper presents valuable contributions to the field of online reinforcement learning and optimization of policy spaces with mixed scopes. Extending the work with empirical validations and addressing scalability concerns will enhance the practical relevance and impact of the research.", "lMMaNf6oxKM": " Summary The paper proposes a recipe for building a General Powerful Scalable (GPS) Graph Transformer with linear complexity and stateoftheart results on various benchmarks. It categorizes different types of encodings as local global or relative and introduces the first architecture with linear complexity in the number of nodes. The paper provides a modular framework GRAPHGPS1 supporting multiple types of encodings for efficiency and scalability.  Strengths 1. Innovative Approach: The paper introduces a new GPS graph Transformer with linear complexity addressing the limitations of previous models. 2. Thorough Analysis: The paper provides a detailed analysis of positional and structural encodings messagepassing components and global attention mechanisms. 3. Extensive Evaluation: The paper conducts an extensive ablation study evaluating the contributions of different components and the GPS model demonstrates competitive results on various benchmarks. 4. Scalability: The GPS architecture is shown to be scalable to large graphs with several thousand nodes outperforming existing models. 5. OpenSource Implementation: Availability of the GRAPHGPS package for further research and validation.  Weaknesses 1. Sensitivity to Hyperparameters: The paper notes the sensitivity of graph Transformers to hyperparameters indicating a lack of a universal solution for all datasets. 2. Limited Challenging Datasets: The paper acknowledges the need for challenging datasets to test longrange dependencies where linear attention architectures could excel. 3. Risk of Overfitting: Results show the model suffers from overfitting on some benchmarks indicating the need for regularization.  Questions and Suggestions for Authors 1. How does the GPS model handle noisy or incomplete graph data 2. Could the authors provide insights into the computational resources required for training the GPS model on large datasets 3. How does the GPS architecture compare in terms of interpretability compared to other graph neural networks 4. Consider discussing the computational efficiency and memory requirements of the proposed model especially for deployment on resourceconstrained devices.  Limitations The authors have adequately addressed the limitations highlighting the sensitivity to hyperparameters and the lack of diverse challenging datasets. A suggestion for improvement could be to provide guidance on hyperparameter tuning strategies for better generalizability.  Societal Impact The authors have briefly touched on the potential societal impact of their work emphasizing the need for examination in specific application domains. A suggestion could be to include a more detailed discussion on ethical considerations and potential biases in the use of their model in realworld applications. Overall the paper presents a novel and scalable approach to graph Transformers with promising results but further exploration on hyperparameters interpretability and ethical implications could enhance the overall impact.", "ywxtmG1nU_6": " Summary: The paper introduces Equivariant Hierarchybased Graph Networks (EGHNs) as an enhancement to Equivariant Graph neural Networks (EGNs) for characterizing multibody physical systems. EGHNs incorporate Equivariant Matrix Message Passing (EMMP) EPool and EUnPool to capture spatialdynamical hierarchy substructure discovery and global information fusion. The authors evaluate EGHN on multiobject dynamics simulation motion capture and protein dynamics modeling tasks showing improved performance over existing EGN methods.  Strengths: 1. Innovative Contribution: EGHN introduces a novel hierarchical modeling approach to capture complex multibody systems more effectively. 2. Theoretical Justification: The paper provides a thorough explanation of the components of EGHN and their equivalence properties. 3. Experimental Evaluation: Considerable experimental results validate the efficacy of EGHN across various applications demonstrating superior performance compared to existing methods. 4. Ablation Studies: The ablation studies provide insightful analysis on the importance of hierarchical modeling equivariance and connectivity loss in EGHN.  Weaknesses: 1. Clarity in Presentation: The paper contains detailed technical descriptions that may be challenging for nonexperts to follow easily. 2. Evaluation Complexity: The evaluation could be more comprehensive by including comparison with a wider range of baselines and datasets. 3. Implementation Details: The implementation details are mentioned but could benefit from additional clarity or code references for reproducibility.  Questions and Suggestions: 1. Clarification on Evaluation Metrics: It would be helpful to provide a detailed explanation of the evaluation metrics used in the experiments and why they were chosen. 2. Scalability and Efficiency: How does the computational complexity of EGHN scale with the size of the input systems Are there considerations for efficiency and scalability in largerscale applications 3. Generalization: Have you evaluated EGHN on different types of physical systems beyond the ones mentioned in the paper How does EGHN generalize to new scenarios or unseen data 4. Robustness to Noise: How does EGHN perform in scenarios with noise or uncertainties in data like missing connections or inaccurate measurements  Limitations: The paper could benefit from addressing the following limitations:  Scalability: The authors could further discuss the scalability of EGHN to larger systems and its computational efficiency.  Robustness: Include an analysis of the models robustness to noise and uncertainties in the input data.  Interpretability: Discuss methods for interpreting the results of EGHN to ensure transparency in its predictions. Overall the paper provides a strong foundation for advancing the modeling of multibody physical systems and further improvements in clarity evaluation scalability and robustness can enhance its impact in various scientific domains.", "xatjGRWLRO": " Summary: The paper introduces a selfsupervised pretraining method SSPL specifically tailored for largescale 3D scenes especially outdoor LiDAR point clouds. The proposed method leverages local feature reasoning and introduces a ClusterInfoNCE loss for efficient contrastive learning globally. The paper demonstrates improved performance on downstream tasks of 3D object detection and semantic segmentation compared to existing approaches.  Strengths: 1. Novel Methodology: The paper proposes a novel selfsupervised pretraining method designed for largescale 3D scenes filling a current gap in the field. 2. Empirical Results: Demonstration of significant performance improvements in 3D object detection and semantic segmentation tasks on both indoor and outdoor scenes. 3. Thorough Experimental Evaluation: Extensive experiments on diverse datasets and tasks showcase the effectiveness and superiority of the proposed approach over existing methods. 4. Detailed Technical Contribution: The paper provides detailed explanations of the methodology including local and global contrastive reasoning volume feature extraction and implementation details.  Weaknesses: 1. Limited Comparison: The paper compares the proposed method with only a few baseline techniques which may not provide a comprehensive understanding of its performance. 2. Theoretical Foundations: While the empirical results are strong the paper lacks a deeper theoretical analysis or justification for the specific choices made in the methodology. 3. Generalization: The paper focuses mainly on specific downstream tasks like 3D object detection and semantic segmentation but could benefit from exploring the generalizability of the proposed method to a wider range of tasks.  Questions and Suggestions: 1. Clarity on Evaluation Metrics: Can the authors provide more insights into the choice of evaluation metrics used for performance comparison especially in relation to existing benchmarks and standards 2. Scalability and Efficiency: How scalable is the proposed method in terms of computational efficiency and memory utilization particularly when dealing with extremely largescale scenes or datasets 3. Impact of Hyperparameters: How sensitive is the performance of the method to different hyperparameter settings and what strategies are employed to tune them effectively 4. Interpretability: Could the authors provide more insights into the interpretability of the learned features and how they correlate with the semantic information in 3D scenes  Limitations: The authors have not adequately addressed the potential negative societal impacts of their work. Considering the rapid advancements in autonomous systems ensuring the safety and ethical implications of such technology should be a crucial aspect for future research. To enhance the paper the authors could discuss how the proposed method could contribute to addressing societal concerns like privacy equity and environmental impact in the context of autonomous systems. Overall the paper presents a promising contribution to the field of selfsupervised learning for 3D perception tasks with strong empirical validation. Strengthening the theoretical underpinnings and expanding the scope of evaluation could further enhance the significance of the proposed approach.", "kZnGYt-3f_X": " Peer Review  Summary: The paper introduces a novel approach called Hilbert Distillation (HD) and Variablelength Hilbert Distillation (VHD) to distill knowledge from 3D models to 2D models. The proposed methods leverage the structural information in highdimensional data to improve the performance of 2D models. Extensive experiments on activity and medical imaging datasets show that the proposed approach outperforms existing distillation techniques in crossdimensionality distillation tasks.  Strengths: 1. Innovative Approach:  Introducing the use of Hilbert curve for distillation is a novel and creative approach to transferring knowledge between 3D and 2D models.  The proposed Variablelength Hilbert Distillation method addresses the challenge of focusing on activation features enhancing the effectiveness of the distillation process. 2. Experimental Rigor:  The paper provides detailed experimental results on two different datasets demonstrating the superiority of the proposed methods over existing distillation techniques.  Conducting hyperparameter tuning experiments and analyzing the influence of different hyperparameters adds depth to the evaluation. 3. Thorough Exposition:  The paper is wellstructured and provides detailed explanations of the proposed methods with clear illustrations and algorithm descriptions.  Background on related work and motivation for the study are wellacknowledged in the introduction setting a context for the proposed approach.  Weaknesses: 1. Limited Benchmarking:  The paper would benefit from a more extensive comparison with a broader range of stateoftheart distillation methods to provide a more comprehensive evaluation.  It could be beneficial to showcase the scalability and generalizability of the proposed method by testing on a wider variety of datasets and tasks. 2. Theoretical Clarity:  Some sections in the methodology could be further refined for better clarity especially in explaining the Variablelength Hilbert Distillation concept and the approximations used in the method. 3. Reproducibility Challenges:  The practicality and reproducibility of the proposed method could be enhanced by providing opensource implementation or code availability for the community to validate the results.  Questions and Suggestions for Authors: 1. Scalability and Generalizability: Have you considered testing the proposed approach on a more diverse set of datasets or tasks to assess its scalability and generalizability 2. Robustness of Distillation: How robust is the proposed method to variations in the architecture of 3D and 2D models Have you tested the method on models other than ResNet and VGG for a more comprehensive evaluation 3. Code Availability: Will the authors provide access to the code or implementation details of the proposed method for reproducibility and future research purposes  Limitations: The paper should enhance its discussion on the limitations of the proposed method such as limitations in specific application contexts potential negative societal impacts or inherent biases. Providing insights into these aspects can strengthen the ethical considerations related to the study. Overall the paper presents an innovative approach with promising results but further enhancements in benchmarking theoretical clarity and reproducibility can elevate the impact and credibility of the research.", "yb3HOXO3lX2": "Summary: The paper provides a formal definition of reward hacking where optimizing a proxy reward function may lead to poor performance according to the true reward function. The authors introduce the concept of unhackability exploring conditions under which increasing proxy return will not decrease true return. They analyze deterministic policies and finite sets of stochastic policies to establish necessary and sufficient conditions for simplifications an important case of unhackability. The results highlight the tension between narrowly defining tasks and aligning AI systems with human values. Strengths: 1. Original Contribution: The paper introduces a novel concept of reward hacking and provides formal definitions to explore the phenomenon comprehensively. 2. Rigorous Analysis: The authors conduct a thorough analysis to identify conditions under which hacking can occur presenting thoughtprovoking results. 3. Theoretical Insights: Through formal proofs the paper sheds light on the challenges of specification and alignment in reinforcement learning. 4. Relevance: The discussion on the implications for specifying AI behavior and the limitations of using reward functions provides valuable insights for future research directions. Weaknesses: 1. Opacities in Theoretical Framework: Some definitions proofs and conditions presented in the study can be challenging to follow limiting accessibility for a broader readership. 2. Limited Generalizability: The study focuses on finite MDPs and Markov reward functions potentially limiting the generalizability of the findings to realworld applications. 3. Concerns of Strictness: The strict definition of hackability may raise questions about the practical implications and realworld applicability of the studys findings. 4. Empirical Validation: While theoretical insights are valuable empirical validation of the findings could enhance the studys robustness and applicability to practical scenarios. Questions and Suggestions: 1. Can the authors discuss potential strategies to apply their theoretical insights in realworld reinforcement learning systems 2. Could the paper benefit from practical examples illustrating the implications of reward hacking in different AI applications 3. How do the authors recommend practitioners and researchers address the identified risks of reward hacking in practice 4. Could the study explore the interplay between reward optimization proxy reward functions and human values alignment in more detail to provide practical guidance Limitations: The paper does not address the potential negative societal impact of reward hacking explicitly. The authors could consider providing concrete suggestions for mitigating such impacts such as ethical considerations for deploying AI systems.", "uOQNvEfjpaC": " Summary of the Paper: The paper presents a method for weaklysupervised openworld purely visual phrasegrounding called \"What is Where by Looking\" (WWbL) which localizes objects in images without predefined object categories or bounding boxes during training. The method combines pretrained visionlanguage models (CLIP and BLIP) and achieves stateoftheart performance in weakly supervised segmentation and phrase grounding tasks.  Strengths: 1. Innovative Approach: The method introduces a novel task (WWbL) and combines pretrained models effectively to achieve impressive results. 2. Empirical Evaluation: The paper demonstrates the effectiveness of the method with thorough experimental evaluations on multiple datasets showcasing stateoftheart performance. 3. Detailed Explanations: The paper provides detailed insights into the methodology network architecture training and evaluation procedures enhancing the reproducibility of the work.  Weaknesses: 1. Limited Discussions on Failure Cases: The paper lacks indepth analysis of failure cases or scenarios where the method may not perform well which could provide valuable insights for further improvements. 2. Lack of Comparison Metrics: While the results are impressive the paper could benefit from comparing not only with existing stateoftheart methods but also illustrating the limitations of the proposed method. 3. Potential Overfitting: The paper does not extensively discuss the risk of potential overfitting especially when using complex pretrained models and large amounts of data.  Questions and Suggestions for Authors: 1. Interpretability of CLIP and BLIP Scores: Can the authors provide insights into how the CLIP and BLIP scores are interpreted in the localization process 2. Scalability and Efficiency: How scalable is the proposed method to handle realtime applications and are there any considerations for optimizing the efficiency of the methodology 3. Generalization to Video Sequences: Have you considered extending the method to localize objects in video sequences by tracking detected regions over time  Limitations and Suggestions for Improvement: The authors should address the following limitations and suggestions for improvement:  Cultural Bias: Address the cultural bias issue by expanding the dataset to include diverse concepts and environments.  Discriminatory Behavior: Implement bias mitigation strategies to eliminate any discriminatory behavior in the method.  Conclusion: Overall the paper introduces a novel and effective method for weakly supervised purely visual phrasegrounding showcasing impressive results across multiple tasks. Addressing the highlighted weaknesses and limitations will enhance the robustness and applicability of the proposed methodology.", "wjSHd5nDeo": " Peer Review  1) Summary: The paper proposes a multiplesample importance weighted autoencoder (IWAE) target for training lossy neural image compression (NIC). The authors identify the impact of the uniform variational posterior on gradient estimators and propose MSNIC as an enhancement over sota NIC methods. Experimental results demonstrate improved performance and richer latent representation.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem in NIC and proposes a novel solution.  The analysis of the impact of the uniform variational posterior on gradient estimators is insightful.  Experimental results show improvement over sota methods. Weaknesses:  The proposed methods RD performance improvement might be marginal in some cases.  Limited evaluation on recent sota methods and the negative result on models with spatial context models warrant further investigation.  3) Questions and Suggestions for the Authors:  Can you provide more detailed insights into why the performance improvement with MSNIC is limited especially when using models with spatial context  Have you considered further experiments or modifications to enhance the RD performance particularly in challenging scenarios  How does the proposed MSNIC approach compare to other recent techniques in neural image and video compression  4) Limitations:  While the paper addresses various aspects of the proposed MSNIC approach further evaluations on different datasets and models could strengthen the findings.  Exploring the theoretical limitations of the method and potential negative societal impacts can enhance the papers contribution.  5) Limitations and Societal Impact Handling: The authors have not adequately addressed the limitations and potential negative societal impacts of their work. To improve considerations regarding broader implications implications for bias fairness privacy or ethical considerations should be incorporated. Overall the paper makes a valuable contribution to the field of lossy neural image compression with the proposed MSNIC approach. Further work on performance enhancement and extensive evaluations can strengthen the findings. Integrating ethical considerations and societal impact analysis will enhance the papers significance.", "w0QoqmUT9vJ": " Abstract The paper proposes a unified framework kOSAN to study subgraphenhanced Graph Neural Networks (GNNs) and introduces datadriven methods for sampling subgraphs. The study addresses the expressivity limitations and empirical performance improvement of these architectures compared to standard GNNs. The proposed approach is evaluated on various datasets showcasing improved predictive performance while reducing computation time.  Summary The paper introduces the kOSAN framework to study subgraphenhanced GNNs addressing the expressive power and limitations of these architectures. It introduces a datadriven method for sampling subgraphs and empirically demonstrates improved predictive performance on benchmark datasets compared to nondatadriven approaches. The study provides theoretical insights model comparisons and computational efficiency analysis.  Strengths 1. Theoretical Contributions: The paper provides a theoretical framework to analyze the expressive power of subgraphenhanced GNNs offering new insights into the limitations and capabilities of these architectures. 2. DataDriven Sampling: The approach of learning to sample subgraphs in a datadriven manner is innovative and provides a novel perspective on enhancing GNN performance. 3. Empirical Evaluation: Extensive empirical studies are conducted on multiple datasets demonstrating improved predictive performance and reduced computation time with datadriven subgraph sampling. 4. Clear Structure: The paper is wellstructured with precise explanations illustrative examples and comprehensive experiments to support the proposed methods.  Weaknesses 1. Expressivity vs. Performance: There seems to be a discrepancy in the empirical results compared to the theoretical findings regarding the impact of subgraph size on predictive performance suggesting further investigation on generalization ability.  Questions and Suggestions for Authors 1. Further Investigation: Can the authors provide insights into the discrepancies observed between theoretical findings (expressivity improvement with larger subgraphs) and empirical results (drop in performance with larger subgraphs) 2. Generalization: How do the proposed methods handle generalization challenges when faced with different types of graphs beyond the benchmark datasets 3. Scalability: Have scalability and convergence issues been thoroughly addressed when learning to sample subgraphs for largescale datasets  Limitations The authors should address the potential negative impact of the observed drop in performance with larger subgraphs compared to the theoretical expectations. Suggestions for improvement include adjusting the model to better handle varying subgraph sizes and types for improved generalization.", "o8H6h13Avjy": " Peer Review 1. Summary: The paper introduces a novel approach called MExMI that focuses on tackling Model Extraction (ME) and Membership Inference (MI) attacks on ML models trained in the cloud. By combining ME and MI in an iterative manner the authors show significant improvements in ME attack accuracy and MI query cost reduction. Experimental results demonstrate that MExMI outperforms existing PAME attacks by up to 11.14 in ME fidelity and achieves high accuracy in MI with minimal query budget. 2. Strengths:  The paper addresses a critical challenge in MLaaS security offering a comprehensive framework to enhance ME and MI attacks.  The proposed approach showcases substantial improvements in accuracy precision and recall compared to stateoftheart algorithms.  Extensive experimental evaluation on diverse datasets validates the effectiveness of the MExMI framework. 3. Weaknesses:  The paper lacks a detailed explanation of the methodology in clear steps making it challenging for readers to follow the technical aspects.  The discussion on potential defenses against MExMI could be further elaborated to provide a holistic view of the frameworks vulnerabilities.  The practical implementation and scalability of the framework in realworld scenarios are not thoroughly discussed. 4. Questions and Suggestions for Authors:  How does the MExMI framework account for potential adversarial scenarios where the adversary has limited knowledge of the victim model  Could the authors provide insights into the computational complexity and resource requirements of implementing MExMI on largescale ML models  Are there plans to extend the MExMI framework to address emerging threats in MLaaS environments such as adversarial samples and model evasion attacks 5. Limitations: The paper adequately discusses the limitations and negative societal impacts of model extraction and membership inference attacks. To further enhance the ethical considerations the authors could include a section on potential mitigation strategies to protect user privacy and data confidentiality in MLaaS settings. Overall the paper presents a valuable contribution to the field of adversarial machine learning particularly in the context of cloudbased ML services. With some clarification and expansion on methodological details and practical implications the paper could greatly enhance its impact and reach within the research community.", "q5h7Ywx-sS": " Summary: The paper introduces \"Stars\" a novel method for constructing extremely sparse and highquality similarity graphs using twohop spanners. Stars significantly reduces the number of pairwise similarity comparisons required for graph construction leading to substantial improvements in efficiency and runtime without compromising quality. The method is theoretically grounded and practically validated through empirical evaluations on real and synthetic datasets.  Strengths: 1. Novel Methodology: Stars introduces a unique approach to constructing sparse similarity graphs using twohop spanners addressing the scalability challenges associated with large datasets. 2. Theoretical Guarantees: The paper provides rigorous theoretical analysis proving the efficiency and effectiveness of Stars in constructing highquality similarity graphs. 3. Empirical Validation: Extensive empirical evaluations demonstrate the superior performance of Stars compared to baseline methods showcasing significant improvements in efficiency and effectiveness for downstream tasks like clustering. 4. Scalability: Stars is shown to be scalable enabling graph construction for datasets with tens of trillions of edges while maintaining high performance levels.  Weaknesses: 1. Complexity: The paper contains intricate technical details and a high level of mathematical formalism which may make it challenging for nonexpert readers to grasp the core concepts easily. 2. Limited Implementation Details: While the overall system implementation and design are briefly discussed more detailed insights into the practical implementation of Stars could enhance the papers applicability for researchers and practitioners. 3. Clarification on Graph Quality: It would be beneficial to provide additional clarity on how the quality of the constructed graphs is assessed beyond the number of comparisons and runtime improvements.  Questions  Suggestions: 1. Generalization: How well does Stars generalize to different types of datasets and similarity metrics beyond those mentioned in the paper 2. Robustness: Have you tested Stars under noisy or incomplete datasets to assess its robustness and reliability in practical scenarios 3. Comparison Metrics: Are there additional metrics beyond VMeasure that could be used to evaluate the performance of Stars for clustering tasks 4. Practical Implementation: Could you elaborate on any realworld applications or case studies where Stars has been successfully applied to demonstrate its practical utility 5. Future Work: What are the next steps in the research agenda for improving and extending the Stars methodology considering the limitations or challenges identified in the current study  Limitations: The paper could benefit from clearer explanations of the implementation details and practical implications of Stars for users with varying levels of domain expertise. Enhancing the accessibility of the technical content and providing more concrete examples or use cases could potentially improve the overall impact and understanding of Stars in the research community.", "yJE7iQSAep": "Peer Review: Summary: The paper discusses the effectiveness of state space models (SSMs) as a deep learning layer particularly focusing on the S4 model and its variants such as DSS and the proposed S4D. The research systematically explores how to parameterize and initialize diagonal SSMs showcasing the simplicity and efficiency of the S4D model compared to the original S4 model. The study includes mathematical analysis design choices empirical studies in various domains and comparison with existing models. Strengths: 1. Novel Contribution: The paper addresses an important topic in deep learning by exploring the effectiveness of diagonal state space models providing a new method (S4D) that simplifies the complex processes of the original S4 model. 2. Comprehensive Analysis: The research provides a thorough mathematical analysis systematic categorization of design choices controlled empirical studies and comparisons with existing models across multiple domains showcasing a deep understanding of the subject. 3. Empirical Results: The study presents stateoftheart results in image audio and medical time series domains along with high performance on the Long Range Arena benchmark demonstrating the effectiveness of the S4D model. Weaknesses: 1. Complexity: The paper while comprehensive may be challenging for readers unfamiliar with state space models and deep learning terminology. There is an extensive discussion of mathematical concepts and models that may be hard to follow for those outside the specific research area. 2. Lack of Visualizations: The absence of visual aids or diagrams to illustrate the models parameterizations and results may hinder the readers understanding especially when dealing with complex mathematical formulas and algorithms. 3. Limited Discussion on Limitations: The paper lacks a thorough discussion on the limitations of the proposed method and potential negative societal impacts that may arise with the widespread adoption of diagonal state space models. Questions and Suggestions for Authors: 1. Can you further elaborate on the implications of the findings in realworld applications beyond the benchmark performance metrics 2. Have you considered providing visual aids or interactive demos to help readers better understand the complex mathematical concepts and models discussed in the paper 3. Could you discuss how the S4D model may impact computational efficiency and scalability in comparison to existing models especially when deployed in largescale applications Limitations: The authors have not adequately addressed the limitations and potential negative societal impacts of their work. They can enhance their paper by including a section addressing ethical concerns potential biases in data and consequences of widespread adoption of diagonal state space models. Constructive Suggestions for Improvement: 1. Include a section discussing ethical considerations societal impact and bias mitigation strategies related to the deployment of state space models in various applications. 2. Incorporate visual aids or interactive tools to enhance the clarity and accessibility of the complex mathematical concepts presented in the paper. 3. Provide practical usecase scenarios to illustrate how the S4D model can benefit specific industries or applications enhancing the relevance and applicability of the research.", "qj-_HnxQxB": "1) Summary: The paper introduces FINE (Functional Indirection Neural Estimator) a neural framework designed to improve outofdistribution generalization in tasks involving geometric transformations. FINE performs analogymaking and indirection in functional spaces allowing models to rapidly recognize hidden rules and apply them to new scenarios. The study demonstrates that FINE outperforms competing models in IQ tasks using image datasets such as MNIST Omniglot and CIFAR100. 2) Strengths:  Innovative Approach: The paper introduces a novel concept by performing analogymaking and indirection in functional spaces showcasing a unique perspective in addressing OOD generalization.  Comprehensive Experiments: Extensive experiments are conducted to validate the effectiveness of FINE across various datasets and tasks demonstrating superior performance in outofdistribution scenarios.  Thorough Explanation: The paper provides detailed explanations of the proposed methodology including the memoryaugmented architecture of FINE memory reading key components like encoder and memory update as well as the similarity metric and model analysis. 3) Weaknesses:  Complexity Concerns: The use of multiple trainable weight matrices and memory components for each layer of the backbone network could potentially lead to a high parameter count which may hinder scalability when dealing with very deep networks.  Backbone Architecture: The optimal design of the backbone network remains a key aspect that needs further exploration as different backbones yield varying performance results suggesting the need for a thorough analysis to guide backbone architecture selection.  Limited Modality Exploration: While FINE demonstrates effectiveness in visual IQ tasks the paper acknowledges potential limitations in tasks involving alternative modalities indicating a need for further research to expand FINEs applicability to diverse scenarios. 4) Questions and Suggestions for Authors:  Could the authors provide insights into potential strategies to address the scalability issue resulting from a large number of parameters in FINEs memoryaugmented architecture  Given the varying performance results with different backbone architectures could the authors offer guidelines for selecting an optimal backbone network for FINE across different types of tasks and datasets  Is there ongoing research to explore the application of FINE in tasks involving modalities beyond visual images such as textual or audiobased IQ problems 5) Limitations and Suggestions for Improvement:  While the paper acknowledges limitations around the complexity of FINEs memory and the need for optimized backbone architectures further exploration into parametersharing techniques and rank limiting strategies may help mitigate these challenges. 6) Limitations: The authors could enhance the paper by providing more detailed discussions on addressing the potential negative societal impact of the proposed methodology especially in terms of computational resource utilization and model complexity. Overall the paper presents a compelling approach through FINE for enhancing outofdistribution generalization in IQ tasks opening avenues for future research in expanding its applicability and addressing scalability concerns.", "lWq3KDEIXIE": " Summary: The paper introduces a novel approach for improving tractable probabilistic models using local probability estimates. It proposes an optimization problem and derives a gradientbased method for learning the parameters of a model that balances global data and local estimates. Experimental results demonstrate the significant improvement in model quality by incorporating inconsistent local statistics.  Strengths: 1. Innovative Approach: Introducing a method to enhance tractable probabilistic models using local estimates is a novel contribution. 2. Theoretical Underpinning: The paper provides a solid theoretical foundation by formulating the learning problem deriving gradients and presenting a detailed algorithm. 3. Empirical Evaluation: Controlled experiments on benchmark datasets support the effectiveness of the proposed approach in enhancing model quality. 4. Practical Relevance: Addressing the common scenario of having limited full data but abundant local estimates adds practical relevance to the research.  Weaknesses: 1. Limited Explanation of Algorithm: The paper could provide a more detailed stepbystep explanation of the algorithm for better understanding. 2. Clarity on Experimental Setup: Further clarification on the experimental setup including hyperparameters selection and evaluation metrics can enhance the reproducibility and interpretation of results.  Questions and Suggestions: 1. Model Robustness: How does the proposed method handle scenarios where local estimates are vastly different from the true underlying distribution 2. Computational Complexity: Can the authors further discuss the computational complexity of the algorithm and potential scalability issues 3. Comparison with Existing Methods: How does the proposed method compare with traditional techniques in terms of model quality and computational efficiency 4. Generalizability: Have the authors explored the generalizability of their approach to other types of probabilistic models beyond cutset networks  Limitations: The authors should consider explicitly addressing any potential negative societal impact of their work especially concerning privacy considerations when dealing with local statistics. Suggestions for improvement may include incorporating privacypreserving mechanisms or conducting a sensitivity analysis on the impact of privacy breaches.", "nEJMdZd8cIi": " Summary The paper introduces a novel approach projUNN for efficient learning using recurrent or deep feedforward networks with unitary matrices. By leveraging rankk updates the method maintains performance at nearoptimal training runtimes. The paper proposes Direct (projUNND) and Tangent (projUNNT) projected Unitary Neural Networks which parameterize full Ndimensional unitary or orthogonal matrices with a training runtime scaling as O(kN). The effectiveness of projUNN is demonstrated in both recurrent neural network and convolutional neural network settings.  Strengths 1. Innovative Approach: The proposal of rankk updates in maintaining performance with nearly optimal training runtimes is innovative and significant for improving training efficiency. 2. Comprehensive Experiments: The paper presents a thorough set of experiments across various tasks such as toy models copy memory Permuted MNIST and CNNs showcasing the effectiveness of the proposed method. 3. Theoretical Foundation: Solid theoretical background and mathematical explanations support the proposed method and make it accessible to a broader audience.  Weaknesses 1. Lack of Comparison to StateoftheArt: While benchmarking against prior unitary neural network algorithms is performed a direct comparison to stateoftheart nonunitary models is missing limiting a complete understanding of the performance gains. 2. Complexity in Implementation: The detailed mathematical derivations and runtime analysis might pose a challenge for practical implementation by researchers unfamiliar with the underlying mathematical concepts. 3. Incomplete Analysis: The paper could benefit from a more detailed analysis of the limitations and potential areas for improvement in terms of scalability and adaptation to different network architectures.  Questions and Suggestions for Authors 1. Clarification on Convolutional Experiments: Can the authors elaborate on the choice of Resnet architecture for the CNN experiments and provide a more indepth analysis of why unitary constraints might result in accuracy drops compared to nonunitary models 2. Scalability and Generalization: How does the proposed method generalize to larger datasets and more complex network architectures beyond the experiments presented in the paper Are there insights into the scalability of projUNN in practical applications  Limitations The authors should ensure the algorithms numerical stability particularly in handling matrix inversions and eigendecompositions of lowrank matrices over multiple update steps to prevent potential numerical instabilities that may arise with prolonged training. Strategies to mitigate these instabilities should be explored further for robustness and reliability.  In conclusion the paper presents a novel method for efficient training of unitary neural networks with promising results. Addressing the suggestions and limitations provided would enhance the papers impact and practical utility in the field of deep learning.", "pGLFkjgVvVe": " Review of \"A Geometrically Enriched Latent Model for Offline Reinforcement Learning\"  1. Summary: The paper introduces a novel method for uncertainty estimation in modelbased offline reinforcement learning by combining parametric and nonparametric techniques. They propose a latent spacebased metric using Riemannian geometry of generative models showcasing improvements in modelbased offline approaches on continuous control and autonomous driving benchmarks.  2. Strengths:  Innovative Approach: The combination of parametric and nonparametric uncertainty estimation methods through Riemannian geometry is novel and promising.  Theoretical Rigor: The paper provides a welldefined framework analytical expressions and efficient estimation methods for the proposed metric.  Experimental Validation: Extensive experiments on continuous control and autonomous driving benchmarks demonstrate the effectiveness of the proposed method over contemporary approaches.  3. Weaknesses:  Complexity: The paper heavily relies on advanced mathematical concepts like Riemannian geometry which may hinder understanding for readers not familiar with these topics.  Execution Speed: The paper notes a significant slowdown due to geodesic distance computation improving computational efficiency could enhance practical applicability.  Limited Comparison: More extensive comparisons with other stateoftheart methods could further highlight the superiority of the proposed approach.  4. Questions and Suggestions:  Clarity and Accessibility: Could the authors provide more intuitive explanations or examples to make the complex concepts more accessible to a broader audience  Scalability: How does the proposed method scale with increasing dataset sizes or more complex environments  Robustness: Have the authors considered the robustness of the proposed metric to different noise levels or model misspecifications  5. Limitations: The authors have not thoroughly addressed the potential negative societal impacts of their work. To improve they could include a discussion on ethical considerations and the implications of deploying their method in realworld applications. Overall the paper presents a valuable contribution to the field of offline reinforcement learning emphasizing the importance of uncertainty estimation and providing a novel methodology. With some improvements in presentation clarity computational efficiency and broader comparisons this work could have a significant impact on future research in the domain.", "se2oxj-6Nz": " PeerReview 1. Summary and Contributions: The paper proposes a targeted adversarial attack in the restoration process to enhance object detection performance in adverse imaging conditions. By generating pseudo ground truth using a novel ADAMlike adversarial attack the restored images achieve better object detection results. The study showcases superior results in image dehazing and lowlight enhancement tasks over conventional methods and domain adaptation techniques. The proposed approach targets the balance between visual quality and detection performance without developing new restoration algorithms or object detection models. 2. Strengths:  The paper addresses a critical gap by focusing on improving object detection performance through targeted adversarial attacks in the restoration process.  The proposed method is theoretically sound and backed by extensive experiments demonstrating improved detection performance while maintaining visually satisfactory restoration results.  The comparison with existing methods and the clear demonstration of the effectiveness of the proposed pipeline add credibility to the study.  The use of established metrics and datasets along with a systematic evaluation strengthens the experimental findings. 3. Weaknesses:  The paper could benefit from a more elaborate discussion on the limitations and challenges of the proposed approach especially regarding the generalization and robustness of the restoration models in realworld scenarios.  The technical details of the proposed algorithm especially the ADAMlike adversarial attack and its comparison with baseline methods could be further elaborated for better understanding and reproducibility.  The paper lacks a thorough discussion on the computational complexity of the proposed method which could be crucial for practical implementation considerations. 4. Questions and Suggestions:  How does the proposed method address potential adversarial attacks in the restoration process and ensure robustness against such attacks  Can the authors elaborate on the scalability of the proposed approach across different restoration algorithms and object detectors for broader application scenarios  How does the proposed method handle the tradeoff between restoration quality and detection accuracy in complex imaging conditions with multiple objects and diverse backgrounds 5. Limitations and Suggestions for Improvement: The paper should provide a more explicit discussion on the generalizability and robustness of the restoration models and the potential negative societal impact of enhanced object detection performance. Future work could focus on incorporating interpretability and robustness enhancements in the proposed method to ensure ethical deployment in practical settings. Overall the paper presents a novel approach to enhancing object detection performance through targeted adversarial attacks in the restoration process. Through addressing some of the weaknesses and expanding on the limitations the authors can further strengthen the impact and relevance of their work in the field of image processing and computer vision.", "rG7HZZtIc-": "1) Summary: The paper introduces Decoupled Dynamic Neural Radiance Field (DNeRF) a selfsupervised approach for segmenting and decoupling dynamic objects from the static background in 3D scenes using monocular videos. The method utilizes separate neural radiance fields for moving objects and the static background with a novel loss function to promote correct separation. Additionally a shadow field network is introduced to handle dynamic shadows. The paper provides experimental validation showing superior performance compared to stateoftheart approaches in decoupling dynamic and static 3D objects occlusion and shadow removal and image segmentation for moving objects. 2) Strengths:  Innovative Approach: The paper introduces a novel method to address the challenging task of decoupling dynamic objects from the static environment in 3D scenes.  Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed method through experiments on synthetic and realworld datasets showcasing its effectiveness.  Selfsupervised Learning: The method utilizes a selfsupervised learning approach allowing it to learn dynamic and static components separately without the need for pretrained object detection or segmentation modules.  Clear Presentation: The paper is wellstructured with detailed explanations of the methodology losses and experimental results. 3) Weaknesses:  Limitations of the Approach: The paper highlights limitations such as the reliance on accurate camera registration challenges with reflective surfaces and ambiguities in handling textureless moving objects. These limitations could impact the generalizability of the method.  Complexity: The method involves several components like the composite neural radiance field supervision losses and shadow field network which may increase the complexity of implementation and understanding.  Evaluation Metrics: While the paper reports quantitative metrics like LPIPS SSIM and PSNR additional metrics or comparisons with more existing methods could enhance the evaluation. 4) Questions and Suggestions:  Can the authors elaborate on how the method handles scenes with highfrequency viewdependent radiance changes caused by reflective surfaces  Could the authors discuss potential extensions or modifications to address limitations related to accurate camera registration and challenges with textureless moving objects  How does the proposed method perform in scenarios with complex lighting conditions or dynamic shadows that are not well captured by the shadow field network 5) Limitations: The authors have adequately addressed some limitations of their work such as challenges with camera registration and the ambiguous interpretation of textureless moving objects. To further enhance the mitigation of these limitations the authors could explore alternative loss functions like the beta distribution or Laplacian distribution and evaluate their impact on decoupling accuracy. Additionally providing detailed insights into handling highfrequency viewdependent radiance changes caused by reflective surfaces would improve the robustness of the approach.", "pp7onaiM4VB": " Summary: The paper introduces a novel machine learning framework for addressing transfer learning challenges in EEG data analysis. The proposed framework combines domainspecific batch normalization on the symmetric positive definite (SPD) manifold with a tangent space mapping model resulting in an endtoend trainable model  TSMNet. The study demonstrates stateoftheart performance in intersession and intersubject transfer learning for EEGbased braincomputer interfaces (BCIs).  Strengths: 1. Innovative Framework: The paper introduces a novel approach combining domainspecific batch normalization and tangent space mapping providing endtoend training for transfer learning in EEG data. 2. StateoftheArt Performance: Achieved stateoftheart results in intersession and intersubject transfer learning exceeding domainspecific methods. 3. Interpretability: The models intrinsic interpretability is highlighted aiding in understanding the learned patterns and sources contributing to the classification. 4. Thorough Experiments: Extensive experiments on diverse EEG datasets show the effectiveness of the proposed method providing detailed results across datasets and scenarios. 5. Ablation Study: The analysis of the effects of different components (SPD manifold DSBN) through an ablation study adds depth to the evaluation and highlights their importance.  Weaknesses: 1. Computational Complexity: The paper mentions limitations related to computational complexity particularly in highdimensional SPD feature spaces which may hinder scalability in certain applications. 2. Limited Application Scenarios: The study focuses on offline evaluations and could benefit from exploration in online UDA scenarios for unseen target domains to further enhance practical applicability.  Questions and Suggestions for Authors: 1. Could the authors elaborate on the feasibility and potential challenges of implementing the proposed framework in realtime or online EEG data processing scenarios given the computational complexities mentioned 2. How does the proposed framework generalize to other types of neuroimaging data beyond EEG such as fMRI or MEG data Is there a plan to extend the framework to different modalities for broader applicability 3. How does the proposed TSMNet model handle potential issues of overfitting or limited generalizability especially in scenarios with small datasets or highly variable subject characteristics  Limitations: The authors have adequately acknowledged the limitations of their work particularly in terms of computational complexity and the focus on offline evaluations. To further address potential negative societal impacts continued exploration in the clinical application of EEG for areas like sleep staging seizure detection or pathology identification could be valuable. Consideration of privacy and ethical implications in data collection and analysis should also be emphasized.  Constructive Suggestions for Improvement: 1. Online Learning Extension: Investigate the viability of adapting the proposed framework for online UDA scenarios for realtime EEG processing applications. 2. CrossModality Generalization: Explore generalized applications across different neuroimaging modalities beyond EEG to enhance the frameworks versatility and transferability. 3. Robustness Validation: Conduct robustness testing to evaluate the models performance under varying data conditions noise levels or data distributions to validate its reliability in realworld settings.", "mWaYC6CZf5": " Summary: The paper addresses the issue of representation collapse in Sparse MixtureofExperts (SMoE) models and proposes a novel routing algorithm that estimates routing scores on a lowdimensional hypersphere. The proposed method aims to alleviate representation collapse and achieve more consistent routing behaviors. Extensive experiments on crosslingual language model pretraining and finetuning demonstrate the effectiveness of the proposed approach in improving performance and routing consistency.  Strengths: 1. Novel Contribution: The paper introduces a novel routing algorithm to address representation collapse in SMoE models which fills a gap in existing research. 2. Comprehensive Analysis: The work includes extensive experiments and detailed analyses to validate the effectiveness of the proposed method. 3. Consistent Gains: Experimental results across various benchmarks consistently show that the proposed method outperforms baseline SMoE models. 4. Clear Exposition: The paper is wellstructured and clearly explains the proposed method experimental setup and results.  Weaknesses: 1. Lack of Comparison with StateoftheArt: While comparisons with SMoE baselines are provided a comparison with other stateoftheart routing algorithms in similar contexts could strengthen the paper. 2. Validation: The paper could benefit from additional validation experiments or comparisons with different model architectures to further support the claims.  Questions and Suggestions for Authors: 1. Dimension Reduction: How was the specific value of dimension reduction chosen (N2) and were other values considered or tested 2. Scalability: How does the proposed algorithm handle scalability issues when increasing the number of experts in largescale models 3. FineTuning: Have you investigated the impact of the proposed routing algorithm on finetuning performance over a larger set of downstream tasks 4. Robustness: How robust is the proposed algorithm to noise or variations in training data across different runs  Limitations: The paper could benefit from addressing the following:  Societal impact: While the paper mentions the potential positive impact of SMoE models in reducing computational costs it could further explore and discuss any potential negative societal impacts such as model biases or unintended consequences. Consider highlighting steps taken to mitigate such risks. Overall the paper presents a promising approach to addressing representation collapse in SMoE models with solid experimental validation. Further exploration and validation in diverse scenarios could enhance the robustness and applicability of the proposed routing algorithm.", "nJJjv0JDJju": " Summary The paper proposes a novel method that enhances the performance of diffusion modelbased solvers for solving inverse problems by introducing a manifold constraint. The method is theoretically proven to stabilize the solution and prevent errors from accumulating. Extensive experiments in image inpainting colorization and CT reconstruction demonstrate the superiority of the proposed method over existing stateoftheart methods both theoretically and empirically.  Strengths 1. The paper addresses a significant issue in diffusion models and proposes a novel and theoretically sound method to improve their performance. 2. The proposed approach is wellmotivated supported by rigorous theoretical analysis and validated through extensive experiments across various applications. 3. The presented results indicate that the proposed method outperforms existing stateoftheart methods both diffusion modelbased and supervised learningbased showcasing its efficacy.  Weaknesses 1. The paper can benefit from further discussions or experiments regarding the scalability and robustness of the proposed method when dealing with highdimensional data. 2. The limitations section could have been more detailed addressing potential challenges in realworld applications and providing insights on mitigating them. 3. More comparisons with alternative approaches such as GANbased solvers could provide additional context for the efficacy of the proposed method.  Questions  Suggestions for Authors 1. How does the proposed method perform in scenarios with limited training data or noisy measurements 2. Have you considered exploring the computational efficiency of the proposed method in more depth particularly in terms of scaling to larger datasets 3. Further explanations on how the proposed method addresses potential societal impacts such as deepfakes or bias amplification would enhance the discussion section.  Limitations The limitations section could be strengthened by providing more detailed insights into potential challenges in deploying the proposed method in realworld applications. Authors may consider discussing robustness issues scalability concerns and implications for ethical and societal aspects to provide a more comprehensive overview.  Suggestions for Improvement 1. Enhance the discussion on potential negative societal impacts outlining strategies to address ethical concerns and prevent misuse of the proposed method. 2. Consider including robustness analysis using diverse datasets and noise levels to ensure the generalizability and robustness of the proposed method. 3. Provide detailed insights into computational efficiency and scalability for practical deployment of the proposed method. Overall the paper presents a compelling approach to improving diffusion modelbased solvers for inverse problems. Addressing the highlighted areas of improvement can strengthen the papers contribution and impact within the research community.", "kY1RbKE7DWE": " Summary: The paper proposes an Anchorchanging Regularized Natural Policy Gradient (ARNPG) framework for policy optimization in Markov decision processes (MDPs) with multiple reward value functions. The framework aims to optimize policies based on criteria like proportional fairness hard constraints and maxmin tradeoff. The proposed algorithms achieve global convergence with exact gradients and demonstrate superior performance compared to existing policy gradientbased approaches.  Strengths: 1. Novel Framework: The ARNPG framework is innovative in incorporating firstorder methods for multiobjective MDP policy optimization. 2. Theoretical Guarantees: The paper provides theoretical guarantees on the convergence of the proposed algorithms. 3. Empirical Evaluation: Extensive experiments showcase the superiority of ARNPGguided algorithms over existing methods in various settings. 4. Detailed Algorithms: Clear explanations of the proposed algorithms and their application in different scenarios add to the papers rigor.  Weaknesses: 1. Complexity: The paper delves deep into theoretical aspects which might make it challenging for readers not wellversed in optimization theory. 2. Limited Discussion on Negative Impacts: The paper lacks a thorough discussion on the negative societal impacts or limitations of the proposed framework.  Questions and Suggestions for Authors: 1. Clarifications Needed: Provide more intuitive explanations in the introduction section to make it easier for a broader audience to understand the significance of the work. 2. Comparison: Could the authors discuss in more detail how the proposed ARNPG framework compares to existing stateoftheart methods in terms of computational efficiency and scalability 3. Future Directions: It would be valuable if the authors could discuss potential future applications or extensions of the ARNPG framework beyond the scenarios considered in this study.  Limitations: The authors could enhance the paper by:  Providing a detailed discussion on the potential negative societal impacts or ethical considerations of deploying the proposed framework.  Including suggestions for mitigating any limitations in the experimental setup or the theoretical assumptions made in the study.", "wjClgX-muzB": " Summary: The paper introduces a new variational inference approach called Support Decomposition Variational Inference (SDVI) for probabilistic programs with stochastic support. SDVI breaks down a program into subprograms with fixed support constructing separate subguides for each subprogram. This methodology significantly aids in constructing suitable variational families leading to substantial improvements in inference performance.  Strengths:  Innovative Approach: The introduction of SDVI which breaks programs into subprograms with static support is a novel and promising approach to addressing variational inference challenges for models with stochastic support.  Theoretical Rigor: The paper provides a thorough theoretical foundation for SDVI outlining the decomposition of programs into straightline programs and the construction of variational guides.  Empirical Evaluation: Extensive experiments on synthetic and realworld datasets demonstrate the effectiveness of SDVI in comparison to existing variational inference methods.  Code Availability: The implementation of SDVI in Pyro and the availability of code for reproducing the experiments contribute to the papers reproducibility and applicability.  Weaknesses:  Complexity: The intricacy of the SDVI methodology might pose a challenge for practical implementation and understanding by readers without a strong background in variational inference and probabilistic programming.  Assumptions: The paper assumes familiarity with probabilistic programming variational inference and related concepts potentially limiting its accessibility to a broader audience.  Questions and Suggestions: 1. Is the SDVI methodology computationally efficient for largescale probabilistic models with complex dependencies 2. How robust is SDVI to different stochastic behaviors in probabilistic programs 3. Could the authors provide more insights into the scalability of SDVI for highdimensional models and realworld applications 4. Have potential negative societal impacts of the SDVI methodology been considered such as biases in inference or unintended consequences in decisionmaking processes  Limitations: The authors have not explicitly addressed the potential negative societal impacts of their work. Constructive suggestions for improvement include conducting a thorough impact assessment to mitigate biases ensure fairness and consider ethical implications.", "uRTW_PgXvc7": " Summary: The paper introduces a novel approach SpatioTemporal Adapter (STAdapter) for parameterefficient imagetovideo transfer learning. The goal is to enable large pretrained image models to perform well on video tasks by reasoning about dynamic video content with minimal parameter updates. The proposed STAdapter demonstrates superior performance on video action recognition tasks compared to existing methods while significantly reducing the number of updated parameters.  Strengths: 1. Innovative Approach: The paper addresses an important research gap in crossmodality transfer learning and proposes a novel solution STAdapter to efficiently adapt pretrained image models for video tasks. 2. Comprehensive Experiments: The authors conduct extensive experiments on popular video action recognition datasets to validate the effectiveness of the proposed method showcasing its superior performance. 3. Benchmarking: The paper provides a detailed benchmark of various finetuning methods and stateoftheart video models highlighting the superiority of the STAdapter in achieving a balance between efficiency and accuracy. 4. Efficiency and Scalability: STAdapter is designed to be lightweight easily implementable and scalable for deployment making it practical for realworld applications.  Weaknesses: 1. Limited Evaluation Scenarios: While the paper extensively evaluates the proposed method on action recognition tasks a broader range of video understanding tasks could enhance the generalizability of the findings. 2. Comparison with Baselines: The discussion could benefit from a more detailed comparison with existing methods particularly in terms of computational efficiency and training time.  Questions and Suggestions for the Authors: 1. Could you elaborate on the specific design choices and hyperparameters used in the STAdapter to achieve efficient spatiotemporal reasoning 2. How does the proposed method perform on more complex video understanding tasks beyond action recognition Are there any plans to explore other video understanding domains 3. Have you considered the impact of varying frame rates or spatial resolutions on the performance of the STAdapter especially in scenarios with diverse video properties  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work focusing on technical aspects. However considering the broader implications of transfer learning efficiency on resource consumption and environmental sustainability could offer valuable insights for future research. Overall the paper presents a significant contribution to the field of crossmodality transfer learning and video understanding showcasing the potential of the STAdapter for parameterefficient adaptation of large pretrained image models. Further exploration and validation across a wider range of video tasks and datasets could strengthen the findings presented in the paper.", "x4JZ3xX5mtv": "PeerReview 1) Summary: The paper introduces a novel singleimage view synthesis framework to address the \"seesaw\" problem between explicit and implicit methods. It proposes a nonautoregressive model with explicit and implicit renderers aiming to better preserve seen contents and complete unseen regions. The loss function encourages complementarity between the two renderers resulting in highquality view synthesis outperforming autoregressivebased methods in terms of both efficiency and effectiveness. 2) Strengths and Weaknesses: Strengths:  The paper clearly articulates the problem statement and motivation behind the proposed framework.  Innovative approach of bridging explicit and implicit methods to address the \"seesaw\" problem.  Comprehensive methodology incorporating depth estimation view synthesis network design and loss functions.  Solid experimental validation on RealEstate10K and ACID datasets showcasing superior performance over stateoftheart methods.  Clear comparison tables and visualizations for a thorough understanding of results. Weaknesses:  Lack of discussion on the generalizability of the proposed framework to other datasets or scenes.  Limited analysis on the scalability of the method to larger and more complex scenes.  The paper could benefit from an indepth discussion on failure cases or scenarios where the proposed method underperforms. 3) Questions and Suggestions:  How does the proposed framework perform under diverse lighting conditions or varying object textures  Have you considered the potential impact of noise or occlusions in the input image on the view synthesis results  Would adding additional regularization techniques further enhance the performance and robustness of the model  Could the framework be extended to handle dynamic scenes with moving objects or changing backgrounds 4) Limitations: The authors have addressed the limitations associated with the proposed work. To further improve they could explicitly detail the generalizability of the framework across different datasets and scenes ensuring a broader application scope. Overall the paper presents an innovative solution to an existing problem in view synthesis with strong experimental validation and methodological contributions. Addressing the suggested questions and limitations would strengthen the paper for potential publication.", "nyCr6-0hinG": "Peer Review 1) Summary: The paper introduces MetaSchedule a novel domainspecific probabilistic programming language abstraction for constructing rich search spaces of tensor programs to optimize deep learning deployment. It allows domain experts to collaboratively analyze and propose probabilistic choices for program transformations leading to an endtoend learningdriven framework for optimized tensor program selection. Experimental results demonstrate the effectiveness of MetaSchedule in enhancing system performance and versatility. 2) Strengths and Weaknesses:  Strengths:  Innovative approach: MetaSchedule proposes a unique probabilistic language abstraction for constructing search spaces allowing flexible customization by domain experts.  Modular and composable design: The paper demonstrates the modularity of search space composition enhancing ease of use and adaptability.  Performance results: The experimental results showcasing competitive speedups and performance gains over existing frameworks validate the efficacy of the proposed approach.  Weaknesses:  Lack of realworld deployment analysis: The paper could benefit from a discussion on the practical implications and realworld deployment challenges that MetaSchedule might face.  Comparative analysis: A more indepth comparison with existing frameworks and discussing scenarios where MetaSchedule outperforms or underperforms could add more insight.  Complexity and learning curve: The complexity of implementing custom probabilistic programs and understanding the framework may hinder adoption by less experienced users. 3) Questions and Suggestions:  How does MetaSchedule handle input model variations and adapt to changing hardware environments  Can the paper provide more details on the interpretability and scalability of MetaSchedule for largescale deep learning models  Is there ongoing research or future plans to address the scalability and ease of integration of MetaSchedule with industrystandard deep learning frameworks 4) Limitations and Suggestions for Improvement:  While the paper addresses the limitations of search space constriction more discussion on potential negative societal impacts such as bias or unintended consequences would enhance the ethical considerations of using MetaSchedule.  Consider expanding on the practical tradeoffs and challenges in deploying MetaSchedule in diverse production environments addressing scalability and complexity issues proactively. Overall the paper presents a valuable contribution to automatic tensor program optimization with the MetaSchedule framework offering a novel approach that empowers domain experts to customize search spaces efficiently. Further refining the frameworks usability scalability and ethical considerations will augment its potential impact in the deep learning community.", "tmer8WAEzV": " Summary: The paper introduces a novel algorithm Constrained Riemannian Hamiltonian Monte Carlo (CRHMC) for efficient sampling from illconditioned nonsmooth constrained distributions in very high dimensions. The algorithm incorporates constraints into the Riemannian version of Hamiltonian Monte Carlo maintaining sparsity and achieving a mixing rate independent of condition numbers. It outperforms existing packages by orders of magnitude on benchmark datasets from systems biology and linear programming showcasing a significant speedup for sampling from large human metabolic networks.  Strengths: 1. Novel Contribution: Introduces a novel algorithm CRHMC for efficient highdimensional sampling addressing constraints and maintaining sparsity with a significant speedup observed. 2. Empirical Results: Demonstrates superior performance over existing packages on benchmark datasets showcasing substantial improvements in sampling rate and total sampling time. 3. Technical Rigor: Provides detailed explanations of the algorithm challenges faced solution strategies and theoretical underpinnings contributing to the transparency and depth of the research.  Weaknesses: 1. Complexity: The paper is highly technical and some sections may be challenging for nonexperts to grasp easily reducing accessibility for a broader audience. 2. Limitations in Comparison: The comparison with existing algorithms appears limited to specific cases and a more comprehensive evaluation against various methods could enhance the papers robustness. 3. Clarity in Notations: Some notations and terms used may require clearer definitions or explanations to facilitate better understanding particularly for readers less familiar with the subject area.  Questions and Suggestions for Authors: 1. Scalability: Have you tested the algorithms scalability beyond the mentioned dimensions to assess its performance in even higher dimensions 2. Further Validation: Have realworld applications been explored outside the mentioned domains to showcase the algorithms versatility 3. Potential Extensions: Are there plans to extend the algorithm to handle additional types of constraints or to enhance its adaptability to different problem settings  Limitations: The authors have addressed the limitations and societal impact of their work adequately particularly focusing on the technical aspects of the algorithms performance and efficiency. To improve additional consideration of broader implications including ethical and societal impacts could be beneficial.  Suggestions for Improvement: 1. Broader Impact Statement: Include a dedicated section discussing the broader societal implications and ethical considerations of the algorithms implementation. 2. Enhanced Clarity: Simplify some technical sections without sacrificing depth to improve readability for a wider audience. 3. Extended Comparison: Include a more comprehensive comparison with a diverse set of existing algorithms to strengthen the validity of the presented results.", "wcBXsXIf-n9": " Peer Review  Summary: The paper proposes a novel deep neural network classifier that maximizes the margin in both Euclidean and angular spaces simultaneously. The proposed method enforces samples to gather around classspecific centers selected from the vertices of a regular simplex. Experimental results show stateoftheart accuracies for both open set and closed set recognition tasks.  Strengths: 1. Innovative Approach: The paper introduces a unique method that combines margin maximization in both Euclidean and angular spaces offering a novel perspective in deep neural network classification. 2. Simplicity: The proposed method does not require setting hyperparameters for classical classification problems making it userfriendly and easy to implement. 3. Performance: The experimental results demonstrate that the proposed method achieves stateoftheart accuracies for open set recognition and competitive results for closed set datasets.  Weaknesses: 1. Limited Generalization to LargeScale Datasets: The Dimension Augmentation Module (DAM) introduced to handle the dimension issue may need further improvement as it partially addresses largescale dataset limitations. The paper acknowledges this limitation but more robust solutions or further research may be needed. 2. Missing Error Bars: While some experiments were repeated the inclusion of error bars especially for pivotal results would have strengthened the statistical significance and credibility of the reported findings.  Questions and Suggestions for Authors: 1. Scalability: Have you considered scalability issues when applying the proposed method to larger datasets or more complex models How does the performance scale with an increase in dataset size or model complexity 2. Comparison with Baselines: Can you provide a more detailed comparison with existing baseline methods especially contrasting strengths and weaknesses in terms of model complexity and performance across different datasets 3. Interpretability: How interpretable are the learned embeddings especially in the context of identifying misclassifications or edge cases in the classification tasks Can you provide insights into how the proposed method handles such scenarios  Limitations: The authors have adequately addressed the limitations notably discussing the dimension issue and proposing DAM as a solution. To enhance the robustness of the approach exploring alternative methods to improve performance on largescale datasets could be beneficial. Overall the paper presents an innovative approach to deep neural network classification with promising results. Addressing the highlighted weaknesses and questions could further strengthen the research.", "q-LMlivZrV": " Summary: The paper introduces a novel contrastive learning method called \"Contrastive Leave One Out Boost\" (CLOOB) which combines modern Hopfield networks with the InfoLOOB objective to address the explaining away problem faced by CLIP in zeroshot transfer learning tasks. CLOOB outperforms CLIP across various architectures and datasets in these tasks.  Strengths: 1. Innovative Approach: The use of modern Hopfield networks and the InfoLOOB objective is novel and addresses the explaining away problem effectively. 2. Thorough Theoretical Justifications: The paper provides detailed theoretical justifications for the optimization approach aiding in the understanding of the methodology. 3. Comparative Analysis: The comparison of CLOOB with CLIP on multiple pretraining datasets and downstream image classification tasks adds significant value to the paper. 4. Experimental Validation: The experiments demonstrate the superior performance of CLOOB in zeroshot transfer learning backed by statistical significance analysis.  Weaknesses: 1. Complexity: The paper delves deep into mathematical concepts and neural network mechanisms which may be challenging for readers less familiar with these areas. 2. Limited Evaluation: The evaluation primarily focuses on zeroshot transfer learning leaving room for expanding analysis to other types of tasks or metrics for a more comprehensive assessment. 3. Lack of RealWorld Applications: Although the method shows improvement in tasks realworld applications and scenarios where CLOOB can be practically implemented are not discussed.  Questions and Suggestions for Authors: 1. Clarity on Practical Implementation: Could the authors provide insights into how CLOOB could be practically implemented in industries or realworld scenarios for effective use 2. Robustness Analysis: Have robustness tests been conducted to evaluate the stability of CLOOB in different scenarios or datasets 3. Scalability: How does CLOOB fare in terms of scalability when dealing with larger datasets or more complex tasks 4. Comparative Analysis: Could the authors discuss in more detail the reasons behind CLOOB consistently outperforming CLIP in zeroshot transfer learning What specific aspects of CLOOB contribute to this success  Limitations: The papers limitations mainly revolve around the complexity of the proposed methods which might hinder practical implementation and realworld adoption. To address these the authors could consider providing more guidance on practical implementation and conducting further analyses on scalability and robustness.", "xnI37HyfoP": "Summary: The paper proposes a new algorithm for completing nonnegative tensors aiming to achieve the informationtheoretic sample complexity rate. The algorithm combines insights from defining a new norm for nonnegative tensors using integer linear programming and a variant of the FrankWolfe algorithm. The paper provides theoretical proofs and computational experiments to demonstrate the effectiveness and scalability of the algorithm. Strengths: 1. Novel Contribution: The paper introduces a new algorithm for completing nonnegative tensors that aims to achieve the informationtheoretic rate filling an existing gap in tensor completion research. 2. Theoretical Rigor: The paper provides detailed theoretical analysis defining a new norm for nonnegative tensors proving convergence properties and presenting results related to the informationtheoretic rate. 3. Applicability: The algorithms effectiveness and scalability are demonstrated through computational experiments showing practical relevance in scenarios like healthcare image processing and recommender systems. Weaknesses: 1. Complexity: The paper relies heavily on mathematical derivations potentially making it challenging for readers unfamiliar with advanced tensor and optimization concepts to grasp the full scope of the algorithm. 2. Practical Implementation: While the algorithms theoretical aspects are wellexplained further details on the practical implementation aspects such as runtime complexity and potential realworld applications could enhance the papers impact. Questions and Suggestions: 1. Clarity in Presentation: Consider simplifying the technical content to ensure broader accessibility to readers outside the specialized field. 2. Comparison: Providing a more detailed comparison with existing stateoftheart tensor completion algorithms would strengthen the papers contribution validation. 3. Impact on the Field: Elaborate on the potential implications of the new norm for nonnegative tensors and how it could influence future research directions in tensor completion algorithms. Limitations: The authors have not directly addressed the potential negative societal impact of their work. It would be beneficial if the paper includes a discussion on ethical considerations related to the application of their algorithm ensuring responsible implementation to mitigate any unforeseen negative consequences.", "lHuPdoHBxbg": " Peer Review  Summary: The paper introduces a novel method called CoarsetoFine Autoregressive Networks (C2FAR) for modeling the probability distribution of univariate numeric random variables in a hierarchical and discretized manner. C2FAR generates increasingly finer intervals of support by conditioning each distribution on previouslygenerated coarser intervals. The method is applied to probabilistic forecasting using deep learning techniques and achieves impressive results in improving output distributions for time series forecasting compared to existing methods. The author discusses the methods benefits experiments evaluations and comparisons with baselines.  Strengths: 1. Innovative Methodology: The C2FAR approach is conceptually novel and addresses the challenges posed by diverse timeseries data types effectively. 2. Comprehensive Analysis: The paper provides a detailed comparison with existing methods a thorough explanation of experimental setups and insightful discussions on results. 3. Realworld Applications: The practical application of C2FAR in various time series use cases like forecasting anomaly detection interpolation and compression is explored demonstrating its versatility.  Weaknesses: 1. Complexity: The complexity of the C2FAR model might be a drawback impacting computational efficiency and potentially making it challenging to implement in resourceconstrained environments. 2. Data Availability: While the paper mentions the availability of code and data on a GitHub repository the limited discussion on the dataset characteristics and preprocessing steps hinders replicability and a deep understanding of the experiments. 3. Evaluation Metrics: The evaluation section focuses on the performance of the proposed method but lacks a comprehensive comparison with diverse metrics or benchmarks outside the forecasting domain.  Questions and Suggestions for the Authors: 1. How does the scalability of the C2FAR model fare with increasingly larger datasets especially in terms of training times and computational resources 2. It would be beneficial to include an analysis of potential failure points or limitations of the C2FAR approach when dealing with noisy or incomplete time series data. 3. Could the authors elaborate on the interpretability of the C2FAR model outputs especially in the context of providing actionable insights for endusers  Limitations: The authors have not explicitly addressed the potential negative societal impact of their work. To improve the societal impact assessment it is recommended to conduct an indepth analysis of potential ethical implications algorithmic biases and model transparency to ensure responsible deployment and usage of the C2FAR methodology. Overall the paper presents a significant advancement in time series forecasting with the C2FAR method showcasing promising results. Addressing the highlighted weaknesses and suggestions can enhance the clarity and robustness of the paper.", "xqyEG7EhTZ": " Summary: The paper introduces Tempo a novel approach to optimize memory usage for training Transformerbased models. Tempo offers efficient replacements for GELU LayerNorm and Attention layers reducing memory consumption and enhancing training performance. The evaluation on BERTLARGE GPT2 and RoBERTa models demonstrates significant improvements in batch sizes throughput and memory footprint reduction.  Strengths: 1. Novel Approach: Tempo introduces innovative techniques tailored for Transformerbased models addressing memory constraints effectively unlike prior methods. 2. Performance Improvement: The results showcase substantial enhancements in batch sizes training throughput and memory utilization demonstrating the efficacy of Tempo. 3. Thorough Evaluation: The paper conducts comprehensive experiments on different models and hardware setups providing detailed insights into the performance gains achieved by Tempo.  Weaknesses: 1. Clarity of Description: Some sections particularly technical details like inplace function implementations might be challenging to follow for readers not deeply familiar with the topic. 2. Comparison with Existing Techniques: While Tempo is compared against baselines a more indepth comparison with other memory optimization techniques could provide a broader perspective.  Questions and Suggestions for Authors: 1. Further Evaluation: Have you considered testing Tempo on a wider range of Transformerbased models to assess its generalizability 2. Scalability: How does the proposed approach scale with increasingly larger models or varied hyperparameters 3. Overhead Analysis: Can you provide more insights into the computational overhead introduced by Tempos techniques and how it scales with model complexity  Limitations: Authors could elaborate further on:  Societal Impact: Discuss any potential negative societal impacts or ethical considerations arising from the deployment of Tempo.  Environmental Concerns: Provide insights on how Tempos efficiency improvements might contribute to reducing the environmental footprint of training Transformer models. Considering these suggestions may help strengthen the papers impact and provide a more holistic view of the proposed approach and its implications.", "nE8IJLT7nW-": " Peer Review  1) Summary: The paper introduces a novel approach called Peripheral Vision Transformer (PerViT) that incorporates peripheral inductive biases to deep neural networks for image recognition. By modeling peripheral vision through multihead peripheral attention the proposed network shows significant improvements in image classification over baselines on the ImageNet1K dataset. The study demonstrates how PerViT efficiently mimics human visual processing strategies by partitioning the visual field into diverse peripheral regions.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: The paper introduces a novel method PerViT to model peripheral vision in deep neural networks for visual recognition.  Efficacy: The proposed method shows performance improvements in image classification over baselines demonstrating the efficacy of incorporating peripheral inductive biases.  Experimental Evaluation: The paper systematically investigates the inner workings of the model providing qualitative and quantitative analysis of the learned attentions and their impact on machine perception. Weaknesses:  Limited Benchmarking: The evaluation is primarily focused on ImageNet1K which may limit the generalizability of the findings on other datasets or tasks.  Complexity Concerns: The paper mentions the quadratic complexity w.r.t. input resolution indicating potential computational inefficiency. Suggestions for optimizing the computational efficiency would strengthen the work.  Lack of Comparative Analysis: While the proposed PerViT is compared with baselines more extensive comparative analysis against a wider range of stateoftheart models would provide a stronger context for the significance of the proposed method.  3) Questions and Suggestions for Authors: Questions:  How does the performance of PerViT compare with other models that focus on incorporating biological or perceptual principles in neural networks  Have you considered analyzing the interpretability of the model further to understand how the networks decisions align with human vision processes Suggestions:  Provide additional experiments on different datasets to demonstrate the generalizability of the PerViT method.  Consider conducting further investigations on the computational efficiency of the proposed method to make it more scalable for practical applications.  4) Limitations: The authors should further address the computational efficiency concerns raised in the paper exploring methods to reduce the quadratic complexity of PerViT with respect to input resolution. They should also consider experimenting with multiresolution pyramidal designs to potentially enhance the models efficacy.  Overall the paper presents an interesting and innovative approach with compelling results. Addressing the highlighted limitations and exploring the suggested improvements would enhance the impact and relevance of the work in the field of computer vision.", "ocg4JWjYZ96": " Summary The paper introduces an empirical influence function (EIF) to address generalization errors in deep metric learning models. EIF efficiently identifies and quantifies training samples contributing to errors facilitating relabelling to mitigate issues. Experimental results show EIF outperforms traditional methods especially in identifying mislabeling mistakes.  Strengths 1. Novel Contribution: Proposing EIF for debugging generalization errors is a valuable and original contribution to the field of deep metric learning. 2. Experimental Validation: Thorough experiments demonstrate EIFs effectiveness in identifying influential samples and recommending relabelling of noisy training data. 3. Clear Presentation: The paper is wellstructured with detailed sections outlining the problem setting approach experiments and results.  Weaknesses 1. Complexity of Method: The empirical influence function might require a steep learning curve for adoption due to its technical intricacies. Simplification could enhance accessibility. 2. Limited Benchmarking: Comparison with more stateoftheart methods or more extensive datasets could provide a clearer picture of EIFs performance against various scenarios. 3. Societal Impact Discussion: The paper lacks a detailed discussion on the societal implications of their work especially in terms of potential biases or ethical considerations.  Questions and Suggestions for Authors 1. How does EIF compare to other stateoftheart deep metric learning debug techniques 2. Could the authors elaborate on any potential biases or ethical considerations in the dataset relabelling process 3. Can the authors provide insights into the scalability of EIF for larger datasets in terms of computational resources and time complexity  Limitations The paper could benefit from a more explicit discussion on addressing any potential negative societal impacts including biases introduced through dataset relabelling. Authors should consider providing recommendations for mitigating these impacts in future research.", "pfEIGgDstz0": " Review of Paper: \"Neural Deformation Pyramid for NonRigid Point Cloud Registration\" 1. Summary: The paper introduces an innovative method called Neural Deformation Pyramid (NDP) for addressing the challenging problem of nonrigid point cloud registration in computer vision and computer graphics applications. The approach decomposes nonrigid motion hierarchically using a pyramid architecture with MultiLayer Perceptrons (MLPs) at each level. By gradually increasing the frequency of sinusoidal encoding as the pyramid level goes down the method achieves multilevel rigid to nonrigid motion decomposition leading to significant speedups compared to existing methods. 2. Strengths:  The proposed Neural Deformation Pyramid (NDP) offers a novel and effective hierarchical motion decomposition approach leading to stateoftheart nonrigid point cloud registration results on benchmark datasets.  The method is wellmotivated and the hierarchical decomposition strategy enhances interpretability and speed of optimization.  The experimental evaluation is comprehensive demonstrating the superiority of the NDP method over existing approaches under both nolearned and supervised settings. 3. Weaknesses:  While the paper showcases impressive results a more thorough comparison with a wider range of existing methods could provide a more comprehensive assessment of the proposed approach\u2019s efficacy.  The discussion on limitations while present could be further expanded to address potential challenges or shortcomings that might arise in practical applications. 4. Questions and Suggestions for Authors:  How does the proposed method handle varying levels of noise and outliers in the input point clouds and are there specific strategies employed to ensure robustness  It would be insightful to elaborate on the computational resources required for training and inference with NDP particularly in comparison to existing methods to provide a more holistic perspective on the methods efficiency. 5. Limitations:  The authors could present a more detailed discussion on the potential negative societal impacts of their work. Enhancing transparency on ethical considerations and addressing any potential biases in the data or models used would be beneficial. Overall the paper presents a significant contribution to the field of nonrigid point cloud registration with its innovative NDP approach. Further enhancements in addressing limitations and societal implications could enrich the papers impact and relevance.  This peerreview provides a balanced evaluation of the papers strengths weaknesses and suggestions for improvement aiming to assist the authors in enhancing the clarity and impact of their work.", "u4ihlSG240n": " Summary: The paper introduces OmniVL a novel foundation model that supports both imagelanguage and videolanguage tasks using a unified architecture. It utilizes a transformerbased visual encoder for image and video inputs and incorporates a decoupled joint pretraining approach to enhance spatial and temporal learning. OmniVL introduces a unified visionlanguage contrastive (UniVLC) loss to leverage various data types and achieve stateoftheart results on a variety of downstream tasks.  Strengths: 1. Innovative Approach: Introducing a foundation model supporting both image and video tasks bidirectionally is a significant contribution. 2. Decoupled Joint Pretraining: The method of first pretraining on imagelanguage and then conducting joint pretraining with videolanguage is an effective strategy. 3. Performance: Achieving stateoftheart or competitive results across various downstream tasks is commendable. 4. Unified VisionLanguage Contrastive Loss: Leveraging multiple types of data for training facilitates better representation learning and transfer to downstream tasks.  Weaknesses: 1. Evaluation: The paper could benefit from more detailed evaluation metrics and comparisons with a broader range of existing methods. 2. Lack of Analysis: The paper lacks indepth analysis of model behavior failure cases or qualitative assessments of the learned representations. 3. Clarity: Some parts of the method and experiments sections are complex and might be difficult for readers less familiar with the topic to follow.  Questions and Suggestions: 1. Could the authors provide insights into how the bidirectional help between image and video tasks is realized within the model architecture 2. How does the performance of OmniVL vary with different model scales or variations in the pretraining corpus 3. Could the authors discuss the choice of downstream tasks for evaluation and why certain tasks were selected over others 4. Are there constraints or challenges in scaling OmniVL to larger datasets or more complex tasks and how could these be addressed  Limitations: The paper adequately addresses limitations such as the lack of zeroshot capability for visual question answering and commonsense reasoning required for certain tasks. Suggestions for improvement include enhancing architecture design for zeroshot capability and ensuring responsible model deployment given the potential biases in webcrawled data. Overall the paper presents a notable advancement in visionlanguage models with promising results in supporting both image and video tasks simultaneously. Further insights into model interpretability and robustness as well as addressing the identified weaknesses can strengthen the impact and relevance of the work.", "wmdbwZz65FM": "Summary: The paper addresses the challenge of posterior collapse in training autoregressive sequence variational autoencoders (VAEs) by proposing an adversarial word dropout method. The method aims to improve sequence modeling performance and the informativeness of the latent space. The authors show theoretical insights into the effect of word dropout on the ELBO and mutual information and conduct experiments on standard text benchmark datasets to validate their approach. Strengths: 1. The paper provides a comprehensive and systematic analysis of the problem of posterior collapse in autoregressive sequence VAEs. 2. The theoretical basis for the proposed adversarial word dropout method is wellexplained and logically presented. 3. The experiments conducted on various datasets demonstrate the effectiveness of the proposed approach in achieving a better balance between sequence modeling performance and latent space informativeness. Weaknesses: 1. The paper lacks a detailed comparison with other stateoftheart methods for addressing posterior collapse in sequence VAEs. 2. Limited insight is provided into the broader applicability of the proposed adversarial method beyond the specific scenarios evaluated in the paper. 3. The discussion on limitations could be more explicit in addressing potential challenges or drawbacks of the proposed approach. Questions and Suggestions: 1. Could the authors provide more insights into the scalability and generalizability of the proposed adversarial word dropout method to different types of sequential data beyond the text datasets used in the experiments 2. Have the authors considered how their approach could be extended or adapted for applications in fields outside of text modeling such as speech recognition or image generation 3. It would be valuable for the authors to provide a more detailed comparison with related work in the field of posterior collapse and sequence VAE training methods highlighting the unique advantages of their proposed approach. Limitations: The authors have not extensively discussed the potential negative societal impacts of their work. To improve they could include a section discussing ethical considerations and implications of their method especially in generating and manipulating text data that could be exploited for misinformation or malicious purposes.", "rcrY85WLAKU": "Summary: The paper introduces tensor network embeddings focusing on Gaussian random tensors to reduce the computational cost in tensorrelated tasks like decomposition and regression. They propose a systematic way to design embeddings for general tensor structures achieving accuracy and efficiency. The paper also presents an algorithm leveraging these embeddings showcasing its effectiveness in various applications like CP decomposition and tensor train rounding. Strengths: 1. Innovative Approach: Introducing Gaussian tensor network embeddings for general structures is novel and addresses a crucial need in tensorrelated tasks. 2. Theoretical Grounding: The paper is welltheoretically grounded with rigorous analysis and proofs provided to support the proposed embeddings and algorithms. 3. Practical Implementation: Realworld applications like CP decomposition and tensor train rounding are utilized to demonstrate the efficacy of their approach. 4. Comparative Analysis: The experiments comparing their method with existing techniques like approximate leverage score sampling and tensor train embeddings provide valuable insights into the effectiveness of their approach. Weaknesses: 1. Complexity: The papers content is highly technical and may be difficult for nonexperts in tensor networks and linear algebra to follow completely. 2. Practical Validation: Despite theoretical solidity further validation with more diverse datasets and benchmarks would enhance the applicability of the proposed method. 3. Limited Generalization: The focus on specific tensor structures may limit the generalizability of the proposed embeddings to more diverse scenarios. Questions and Suggestions for Authors: 1. Have you considered the potential challenges in implementing these complex algorithms in realworld scenarios with largescale data and limited computational resources 2. How do you envision addressing the limitations of specific tensor structures in expanding the application of your proposed embeddings to a wider range of scenarios 3. Can you provide more insights into the impact of accuracy requirements on the performance and scalability of your algorithms in practical applications Limitations: The authors should provide more insights into the potential negative societal implications of their work especially concerning the computational complexity and practical implementation challenges that may arise in realworld scenarios. They should further consider the environmental impact of increased computational demands and explore avenues to mitigate any negative consequences.", "nE8_DvxAqAB": " Summary: The paper introduces Egocentric VideoLanguage Pretraining (EgoVLP) to address the domain gap between existing 3rdperson view datasets and 1stperson view videos captured by wearable cameras. The authors create EgoClip dataset from Ego4D and propose the EgoNCE pretraining objective. They also introduce EgoMCQ benchmark for validation and demonstrate strong performance in egocentric downstream tasks.  Strengths: 1. Novel Contributions: Pioneers Egocentric VLP creating EgoClip dataset proposing EgoNCE pretraining objective and introducing EgoMCQ benchmark. 2. Thorough Experimental Evaluation: Extensive experimental validation on various downstream tasks demonstrates the superiority of Egocentric VLP. 3. Methodology: Carefully designed data curation strategies effective sampling strategies in EgoNCE and relevant benchmarks. 4. Comprehensive Analysis: Ablation studies and comparisons with stateoftheart methods strengthen the results. 5. Clear Presentation: Wellorganized structure with detailed explanations and illustrative figures for better understanding.  Weaknesses: 1. Limited Discussion on LongTerm Dependencies: The paper acknowledges not considering longterm temporal dependencies in Ego4D videos as a limitation but further exploration could enhance the approach. 2. Privacy and Bias Considerations: While mentioned briefly more discussion on privacy and potential biases in usergenerated Egocentric VLP datasets would add depth.  Questions and Suggestions for Authors: 1. Data Representativeness: How representative are the activities in EgoClip of realworld scenarios and how diverse is the dataset 2. Generalizability: How does the proposed EgoNCE objective generalize to other domains beyond egocentric videos 3. Scalability: How scalable is the EgoVLP framework to even larger datasets and more complex tasks 4. Practical Applications: Can you elaborate on the practical implications of Egocentric VLP in robotics and augmented reality applications  Limitations: The authors could further explore longterm temporal dependencies in Ego4D videos for more robust representations. Additionally discussions on privacy concerns and potential biases in usergenerated data should be extended and addressed.  Societal Impacts: While Egocentric VLP has promising applications in augmented reality and robotics caution must be exercised regarding privacy and unintended biases in usergenerated data. Further considerations on privacy and societal impacts should be emphasized in future studies. Overall the paper presents a significant contribution to the field of Egocentric VLP with welldesigned experiments and benchmarks to support its claims. Addressing the identified weaknesses and questions will further enrich the research.", "wKd2XtSRsjl": " Peer Review  Summary: The paper proposes a novel metric called Mutual Information Divergence (MID) as a unified metric for multimodal generative models. MID uses Gaussian crossmutual information to measure the alignment between text and image modalities in generative models. The paper extensively evaluates the proposed metric against existing metrics on tasks like texttoimage generation and image captioning showing superior performance in terms of consistency sample parsimony and robustness. The experiments cover various benchmarks and human judgment correlations demonstrating the effectiveness of MID.  Strengths: 1. Novel Metric Development: The introduction of MID as a unified metric using Gaussian crossmutual information is a novel and valuable contribution to multimodal generative model evaluation. 2. Theoretical Analysis: The paper provides detailed theoretical analyses of the proposed method discussing its mathematical underpinnings and its relationship with other established metrics like KullbackLeibler divergence. 3. Extensive Evaluation: The paper conducts comprehensive experiments comparing MID with existing metrics on multiple benchmarks and human judgment correlations demonstrating its superiority in performance. 4. Consistency and Robustness: MID shows consistency across datasets making it a reliable metric for evaluating generative models. Robustness analysis with feature extractors also adds to the strength of the proposed metric.  Weaknesses: 1. Limited Discussion on Ethical Considerations: While briefly mentioning societal biases associated with pretrained models the paper lacks a thorough discussion on potential ethical implications of using the proposed metric. Suggestions for addressing such biases would enhance the impact of the work. 2. Clarity in Technical Details: Some technical details may be challenging to comprehend for readers unfamiliar with the domain potentially hindering the accessibility of the paper. Simplifying complex concepts without losing crucial information could improve readability.  Questions and Suggestions: 1. The paper mentions outofdistribution detection using Mahalanobis distances. Could you provide more information on how this aspect was incorporated into the evaluations and its impact on the results 2. How does the proposed MID metric handle scenarios of mode collapse or when the diversity in generated samples is limited Insights into how MID addresses diversity in generative models would be beneficial.  Limitations: The paper could benefit from a more thorough discussion on addressing biases in pretrained models like CLIP along with potential fairness transparency and accountability issues. Providing insights or recommendations on mitigating these biases would enhance the societal impact of the work.", "nLGRGuzjtoR": "PeerReview: Summary: The paper investigates concept removal methods in neural network models trained on text data to address the undesirable encoding of linguistic or sensitive concepts. It critiques existing posthoc and adversarial approaches by demonstrating their counterproductivity in removing concepts entirely without affecting taskrelevant features. The analysis combines theoretical insights and empirical evaluations on synthetic and realworld datasets highlighting the limitations of these methods. Strengths: 1. Theoretical Analysis: The paper presents a rigorous theoretical analysis showing the fundamental limits of probes and classifiers in concept removal providing valuable insights into the failure modes of existing methods. 2. Empirical Evaluations: The empirical evaluation on synthetic and realworld datasets supports the theoretical findings enhancing the credibility of the study. 3. Novel Contributions: The study sheds light on the shortcomings of posthoc and adversarial concept removal methods offering practical implications and suggesting alternative directions for future research. Weaknesses: 1. Limited Comparison: While the paper critiques posthoc and adversarial methods a comparative analysis with alternative approaches or additional experiments would enhance the comprehensive evaluation of concept removal techniques. 2. Complexity: The detailed theoretical analysis might be challenging for readers not wellversed in the intricacies of neural networks and concept removal potentially limiting the papers accessibility. Questions and Suggestions for Authors: 1. Can you further elaborate on the applicability of your theoretical findings in practical scenarios and discuss potential implications for tasks beyond text classification such as image recognition or speech processing 2. How do the results change when using different pretrained models or architectures and do you foresee any variability in the effectiveness of concept removal across diverse model types 3. Have you considered exploring hybrid approaches that combine elements of existing methods with novel techniques such as federated learning or transfer learning to improve concept removal efficiency Limitations and Suggestions: The paper adequately addresses potential negative societal impacts by advocating caution in employing concept removal methods for sensitive applications. To further enhance ethical considerations authors could provide clearer recommendations on mitigating biases or unintended consequences in model training and deployment. Overall the paper makes a significant contribution by exposing limitations in current concept removal approaches and proposing insightful alternatives for researchers and practitioners in the field. Further clarity on practical implications and the potential for broader applications could strengthen the studys impact.", "vMQ1V_z0TxU": " Summary: The paper proposes an informative hierarchical VAE to alleviate the issue of \"posterior collapse\" in hierarchical VAEs which limits their performance in unsupervised outofdistribution (OOD) detection. Additionally the paper introduces a novel score function Adaptive Likelihood Ratio for unsupervised OOD detection. Experimental results show that the proposed method outperforms existing stateoftheart approaches in unsupervised OOD detection.  Strengths: 1. Thorough Analysis and Insightful Discussion: The paper provides a comprehensive analysis of the challenges related to \"posterior collapse\" and presents a clear theoretical explanation for its impact on hierarchical VAEs in OOD detection. 2. Novel Contributions: The development of the informative hierarchical VAE and the Adaptive Likelihood Ratio score function represent significant contributions to improving OOD detection in unsupervised settings. 3. Experimental Validation: The experimental results demonstrate the superiority of the proposed method over existing approaches showcasing its effectiveness in addressing the limitations of hierarchical VAEs.  Weaknesses: 1. Clarity in Presentation: Although the paper delves into technical details the complexity of the methodology might be challenging for readers unfamiliar with hierarchical VAEs and OOD detection. 2. Evaluation Metrics: While AUROC and AUPRC are common metrics a discussion on additional evaluation metrics or the rationale behind their selection could enhance the papers impact.  Questions and Suggestions for Authors: 1. Clarification on Implementation Details: Providing more insights into the hyperparameters used during training (e.g. learning rate adjustments optimizer parameters) could help in replicating the results. 2. Generalization: Have you tested the proposed approach on other datasets beyond the ones mentioned in the paper Understanding the generalizability of the method would strengthen its applicability. 3. Comparison with StateoftheArt Models: How does the proposed method compare with the most recent stateoftheart models in OOD detection including ensemblebased methods or other advanced techniques  Limitations: The authors could further elaborate on the potential societal impacts of improving unsupervised OOD detection such as ensuring the robustness and safety of machine learning systems across various domains. It would be beneficial to address any potential negative implications arising from the deployment of these advanced detection techniques. Overall the paper presents a significant advancement in unsupervised OOD detection using informative hierarchical VAE and Adaptive Likelihood Ratio. By addressing weaknesses in existing methods the proposed approach demonstrates promising results and opens avenues for future research in enhancing machine learning reliability.", "mMT8bhVBoUa": " Peer Review  1) Summary: The paper presents a novel method called Gaussian Wasserstein Inference (GWI) for generalized variational inference in infinitedimensional function spaces. The method combines Gaussian measures and Wasserstein distance to obtain a variational posterior using deep neural networks facilitating uncertainty quantification akin to Gaussian processes. The authors demonstrate the effectiveness of GWI on UCI regression datasets Fashion MNIST and CIFAR 10 achieving stateoftheart performance.  2) Strengths:  Original Contribution: The paper introduces a unique approach combining Gaussian measures and Wasserstein distance for inference in function spaces providing a principled uncertainty quantification method.  Comprehensive Background: The paper gives a thorough overview of Bayesian deep learning variational inference and Gaussian processes setting the context for the proposed GWI method.  Experimental Results: The authors showcase the efficacy of GWI through experiments on various datasets demonstrating superior performance compared to existing methods.  Technical Rigor: The methodology is wellfounded and the mathematical derivations are meticulously explained elucidating the theoretical underpinnings of GWI.  3) Weaknesses:  Implementation Complexity: The paper lacks a detailed discussion on the computational complexity and scalability of the proposed method especially concerning large datasets and highdimensional function spaces.  Comparative Analysis: While the paper compares GWI with existing Bayesian neural network methods a more extensive comparison with a broader range of techniques including nonBayesian approaches would enhance the assessment of GWIs performance.  Clarity in Exposition: Some sections particularly those detailing the parameterizations of prior and variational measures could benefit from clearer explanations or illustrative examples to aid in better understanding.  4) Questions and Suggestions for Authors:  Clarification on Scalability: How does GWI scale with respect to the dimensionality of function spaces and dataset sizes Additionally are there any limitations or challenges in applying GWI to more extensive datasets  Extension to NonGaussian Distributions: Have you considered extending GWI to handle nonGaussian distributions for the prior and variational measures How might this impact the computational tractability and performance of the method  Implications on Practical Applications: Could you elaborate on potential realworld applications where GWI could offer significant advantages over traditional Bayesian inference methods particularly in scenarios where uncertainty quantification is crucial  5) Limitations: The authors have not fully addressed the potential limitations of their work such as concerns related to computational complexity scalability and the handling of nonGaussian distributions. To enhance the paper it would be beneficial to include a discussion on overcoming these limitations and providing practical solutions to mitigate negative societal impacts. To improve the authors could offer more insights on these aspects providing a balanced perspective on the strengths and weaknesses of GWI and its implications on broader applications.", "zrAUoI2JA2": "Paper Summary: The paper introduces uHuBERT a selfsupervised pretraining framework that leverages both multimodal and unimodal speech data for speech processing tasks. By utilizing modality dropout during pretraining uHuBERT demonstrates the capability of achieving performance on par or better than modalityspecific models and zeroshot modality generalization for multiple speech tasks. Strengths: 1. The paper addresses the significant challenge of utilizing multiple speech modalities for improved performance and zeroshot generalization. 2. Introduces a novel selfsupervised pretraining framework uHuBERT which shows promising results in processing audio visual and audiovisual speech inputs. 3. Extensive experiments and comparisons with stateoftheart models demonstrate the effectiveness of the proposed model. 4. The paper provides thorough explanations and comparisons with related works offering valuable insights into the research field. Weaknesses: 1. While the paper presents compelling results the methodology and experiments may benefit from further clarity in explaining the intricacies of modality dropout and pretraining dynamics. 2. It is essential to provide more indepth discussions on potential limitations of the proposed framework especially in realworld applications. Questions and Suggestions: 1. Could the authors elaborate on the computational efficiency and scalability of uHuBERT especially in deploying the model for practical applications 2. How does uHuBERT handle conflicting information or noise across different modalities during pretraining and finetuning stages 3. Are there any tradeoffs or sensitivities to hyperparameters that need to be considered while implementing uHuBERT in diverse speech processing tasks 4. Could the authors shed light on the interpretability of the learned modalityagnostic features and codebooks in uHuBERT for better understanding and model transparency Limitations: The authors should further address the potential negative societal impacts of promoting selfsupervised learning frameworks like uHuBERT particularly concerning bias amplification or unforeseen consequences in various applications. Constructive suggestions for addressing the limitations could involve integrating fairness and explainability metrics into model evaluation and providing ethical considerations in future work. Overall the paper presents a compelling approach to multimodal speech processing with uHuBERT and offers valuable insights into advancing speech models for diverse modalities and tasks.", "p62j5eqi_g2": " Summary The paper introduces a novel blackbox adversarial attack against deep clustering models using Generative Adversarial Networks. The attack is evaluated on various stateoftheart deep clustering models and realworld datasets demonstrating a significant reduction in clustering performance. Additionally the paper explores unsupervised defense approaches such as robust deep clustering models and deep learningbased anomaly detection to mitigate the attack. Furthermore the attack is extended to a productionlevel MLasaService (MLaaS) API Face exhibiting its disruptive potential on realworld applications.  Strengths 1. Innovative Contribution: The paper presents a significant contribution by proposing the first blackbox adversarial attack against deep clustering models highlighting the vulnerability of such models to adversarial attacks. 2. Thorough Analysis: The study conducts an extensive evaluation of the attack on multiple deep clustering models and realworld datasets showcasing its effectiveness and transferability across models. 3. Empirical Validation: The experiments are welldesigned providing clear results that validate the efficacy of the proposed attack and the limitations of existing defense mechanisms.  Weaknesses 1. Limited Defense Evaluation: While the study touches upon defense approaches a more indepth analysis of defense mechanisms and their limitations could further enhance the papers comprehensiveness. 2. Complexity of Attack Model: The complexity involved in parameterizing the GAN for the attack may limit practical application and more guidance on simplifying this process could improve the feasibility of the proposed attack. 3. Lack of Ethical Considerations: The paper does not directly address ethical considerations or potential societal impacts of successful adversarial attacks which could be an important aspect to cover in future work.  Questions and Suggestions for Authors 1. Clarification of Threat Model: Can the authors provide further details on the threat model assumptions especially concerning the availability of training dataset knowledge and its impact on the attacks effectiveness 2. Scalability: Have the authors considered the scalability of the attack both in terms of computational resources required for GAN training and its applicability to larger or more complex datasetsmodels 3. Robust Defense Strategies: Could the authors discuss potential directions for developing more robust defense strategies against adversarial attacks on deep clustering models considering the limitations observed in the study  Limitations The authors should enhance the discussion on negative societal impacts of successful adversarial attacks by incorporating ethical considerations and discussing potential measures to prevent such impacts.", "ripJhpwlA2v": "Summary: The paper introduces the concept of group invariant learning (groupIL) to address spurious correlations in machine learning models when environment annotations are unknown. It highlights the insufficiency of existing groupIL methods in preventing classifiers from depending on spurious correlations and proposes two criteria namely the falsity exposure criterion and label balance criterion to judge the sufficiency of groupIL methods. Based on these criteria a new method called SpuriousCorrelationstrata Invariant Learning with Labelbalance (SCILL) is designed and proven to be effective in solving spurious correlations through theoretical analysis and empirical experiments on synthetic and real data benchmarks. Strengths: 1. The paper addresses an important problem in machine learning  handling spurious correlations to improve model robustness. 2. The theoretical analysis is comprehensive introducing novel criteria for evaluating groupIL methods and offering a new method SCILL to address existing limitations. 3. The experimental results demonstrate the superior performance of SCILL over existing methods validating the proposed approach. Weaknesses: 1. The paper assumes specific causal structures that might limit the generalizability of the findings to more complex scenarios. 2. The methodological implementation and practical usability of SCILL could be further discussed to provide insights for realworld applications. 3. The paper could elaborate more on the limitations and potential challenges in deploying SCILL in practical settings. Questions and Suggestions: 1. Can the proposed criteria especially the falsity exposure and label balance criteria be applied to other machine learning tasks beyond groupIL methods 2. How does the robustness of SCILL compare to other stateoftheart methods in handling spurious correlations in realworld datasets with more complexities 3. Have you considered providing a more detailed explanation of how SCILL could be implemented in practical machine learning workflows and its computational efficiency in comparison to existing methods 4. Could the authors explore extensions or variations of SCILL to address different types of spurious correlations or other sources of bias in machine learning models Limitations: The paper adequately addresses the limitations and potential societal impact of their work by discussing the theoretical analysis and empirical experiments conducted. However providing more insight into the scalability and practical implications of SCILL in diverse applications could further improve the paper.", "uyEYNg2HHFQ": " Summary The paper introduces a novel method to sample model weights from hyperrepresentations showcasing superior performance in generative downstream tasks like model initialization ensemble sampling and transfer learning. The method uses layerwise loss normalization to enhance model generation quality and offers several sampling strategies for efficient weight synthesis. Experimental results demonstrate the efficacy of the proposed approach across various image datasets and tasks surpassing strong baselines like training from scratch and pretrained models in finetuning.  Strengths 1. Innovative Contributions: The paper introduces a novel approach for generating diverse and highperforming model weights from hyperrepresentations showcasing promising results in tasks like model initialization ensemble sampling and transfer learning. 2. Thorough Methodology: The paper provides detailed explanations of layerwise loss normalization and various sampling strategies demonstrating a robust experimental setup across multiple image datasets. 3. Empirical Validation: Results show that the proposed method outperforms baseline methods in terms of model performance and efficiency especially in generating highquality ensembles.  Weaknesses 1. Complexity in Explanations: Some sections especially in the Methods and Experiments may be challenging to follow due to technical jargon and intricate details. Simplifying the language or providing more intuitive explanations could enhance readability. 2. Limited Architectural Generalization: The experiments demonstrate promising results for unseen architectures but further analysis and validation are required to assess the methods scalability and generalization capabilities across a wider range of model architectures. 3. Data Accessibility: The paper does not fully address the potential negative impact of requiring large collections of trained models while the training data may not always be accessible. Ensuring data privacy and ethical considerations should be more explicitly discussed.  Questions and Suggestions 1. Could the paper elaborate on how the method addresses potential biases or ethical considerations in utilizing large collections of pretrained models without access to the original data 2. How does the proposed method compare to existing techniques like HyperNetworks or Bayesian HyperNetworks in terms of efficiency and scalability for generative purposes 3. Given the detailed experimentation could the paper explore further the computational cost and scalability aspects of the method especially when applying it to larger datasets or models 4. Have the authors considered potential application domains beyond neural networks where hyperrepresentation learning could be beneficial and how could the method be adapted for wider applications  Limitations The authors do not explicitly address the potential negative societal impact of requiring access to trained neural models which may pose data privacy concerns. It is recommended to reinforce data ethics and privacy standards ensuring compliance with regulations and respecting user data confidentiality and rights in future work.", "kQgLvIFLyIu": "Peer Review Summary: The paper introduces a novel coreset construction for the linesets kmedian problem and its variants. It defines an \u03b5coreset that approximates the sum of Euclidean distances from each set of lines to its closest center with a small size of O(log n) weighted linesets. The main technique involves a reduction to a fair clustering of colored points to colored centers. The paper also presents a coreset construction for colored sets clustering along with theoretical proofs and experimental results. Strengths: 1. Novelty: The paper introduces a novel coreset construction for a challenging problem in geometric clustering. 2. Theoretical Rigor: The paper provides detailed definition properties and proofs related to the coreset construction sensitivity and algorithms involved. 3. Experimental Results: The inclusion of experimental results on both synthetic and realworld data sets adds credibility and practical relevance to the work. 4. Algorithm Design: The proposed algorithms for coreset construction and sensitivity computation are welldefined and structured. 5. Potential Impact: The paper identifies promising future research directions and discusses the potential application of the findings in realworld systems. Weaknesses: 1. Clarity: The paper could benefit from clearer explanations and definitions especially in complex parts related to sensitivity and coreset construction. 2. Visualization: Including visual aids such as diagrams or figures could enhance the understanding of the proposed techniques particularly for readers not familiar with the domain. 3. Comparative Analysis: While the paper includes comparisons with uniform sampling and existing methods further comparisons with stateoftheart techniques in coreset construction could strengthen the evaluation. Questions and Suggestions for Authors: 1. Definition Clarity: Can you provide more intuitive examples or illustrations to explain the concepts of sensitivity and coreset construction to readers with varied backgrounds 2. Algorithm Complexity: How does the time complexity scale with the size of input data and the number of clusters k in practice Are there any optimizations or tradeoffs considered for largescale data sets 3. Experimental Validation: How do the empirical results compare with theoretical guarantees in terms of the approximation quality Are there any outlier cases where the proposed coreset construction performs significantly better or worse than existing methods Limitations: The authors should explicitly address the potential negative societal impact of their work such as unintended bias in clustering or misuse of the algorithms for discriminatory purposes. Providing ethical considerations and guidelines for responsible deployment would enhance the papers societal relevance and ethical rigor.", "uCXNOeL0TG": "Summary: The paper introduces the MultiWorker Restless MultiArmed Bandit (MWRMAB) problem and proposes a solution that focuses on fairness in allocating intervention tasks among heterogeneous workers while maximizing rewards. Their approach combines Whittle index computation with balanced allocation to achieve fair and efficient planning. Empirical evaluation demonstrates its effectiveness compared to baseline algorithms in terms of reward accumulation fairness scalability and execution time. Strengths: 1. Novel Contribution: The paper introduces a novel problem setting and proposes a comprehensive solution addressing both fairness and reward maximization in intervention planning. 2. Theoretical Rigor: The formulation is welltheorized with clear mathematical definitions theorems and algorithms provided to justify the proposed approach. 3. Empirical Evaluation: Extensive empirical evaluation showcases the effectiveness scalability and efficiency of the proposed solution compared to various baselines demonstrating the practical relevance of the approach. 4. Ethical Considerations: The authors address limitations and potential negative societal impacts ensuring ethical research conduct. Weaknesses: 1. Clarity in Presentation: The content in some sections appears repetitive and could benefit from more concise and structured presentation to enhance readability. 2. Baseline Comparison: While the comparisons against different baselines are thorough a more detailed analysis of the differences in results between the proposed method and each baseline could provide a deeper understanding of the approachs advantages. 3. Documentation of Experiments: While the experimental methodology is clearly outlined additional details on data splits hyperparameters and error bars could further enhance the reproducibility of the results. Questions and Suggestions for Authors: 1. Could you elaborate on how the proposed solution can be adapted to handle dynamic environments where worker costs and effectiveness change over time 2. How do the fairness constraints impact the convergence and stability of the proposed algorithm especially in scenarios with a large number of arms or workers 3. Have you considered the potential sensitivity of the algorithm to the choice of fairness threshold \\xe2\\x9c\\x8f and how it may affect the tradeoff between fairness and reward optimization Limitations and Potential Negative Societal Impact: The authors have successfully addressed limitations and potential negative societal impacts of their work by discussing fairness scalability and efficiency aspects. A constructive suggestion for improvement could be to include a more detailed analysis of the algorithms sensitivity to different parameters and realworld settings. Overall the paper presents a strong contribution to the field of intervention planning under heterogeneous worker constraints with wellstructured research methodology and promising empirical results. Further enhancing clarity and addressing the suggestions provided can strengthen the papers impact and contribution to the research community.", "p9_Z4m2Vyvr": " Peer Review Summary: The paper proposes a novel method called clusterwise Amortized Mixing Coupling Processes (AMCP) for efficient clustering using optimal transport techniques. The method sequentially generates clusters utilizing intracluster mixing and intercluster coupling strategies reducing computational complexity and improving adaptability. Experimental results on synthetic and realworld datasets demonstrate the effectiveness and efficiency of the proposed method. Strengths: 1. Novel Contribution: The paper introduces a novel method that combines optimal transport with amortized clustering addressing scalability and adaptability in clustering tasks. 2. Efficiency: By avoiding pairwise distance calculations between clusters and using clusterwise learning AMCP reduces computational complexity and improves efficiency in clustering large datasets. 3. Experimental Validation: Extensive experiments on synthetic and realworld datasets demonstrate the superiority of AMCP in terms of clustering performance and computational efficiency. 4. Thorough Methodology Explanation: The paper provides detailed descriptions of the proposed methods components including IntraCM and InterCC offering a clear understanding of the clustering process. Weaknesses: 1. Evaluation Metrics: While the paper reports clustering performance metrics like loglikelihood accuracy and NMI additional evaluation metrics such as precision recall or F1 score could provide a more comprehensive assessment of the clustering quality. 2. Comparative Analysis: The paper could benefit from a more indepth comparison with existing stateoftheart clustering methods to showcase the specific advantages of AMCP over other approaches. 3. Method Complexity: The intricate process presented in AMCP involving intracluster mixing intercluster coupling and multiple EM steps might pose challenges in practical implementation and interpretation for users less familiar with advanced clustering techniques. Questions and Suggestions: 1. Scalability: How does AMCP handle extremely large datasets with potentially thousands of data points and very highdimensional features 2. Generalizability: Has the model been tested on a wider range of realworld datasets beyond those mentioned in the paper to assess its adaptability to various data distributions 3. Implementation Considerations: Are there plans to develop a userfriendly software package or library for practitioners to easily apply the proposed AMCP method in their clustering tasks Limitations: The authors should further discuss the potential negative societal impact of their work and provide recommendations to mitigate any unintended consequences such as biased clustering or data privacy concerns. Overall the paper presents a significant contribution to the field of clustering with a novel approach that shows promise in enhancing efficiency and adaptability. However further validation and comparison with existing methods as well as addressing practical implementation challenges would strengthen the papers impact in the research community.", "pz2UcXyX0Cj": " Peer Review:  Summary: The paper introduces a new framework called CausalityDriven Hierarchical Reinforcement Learning (CDHRL) to automatically discover highquality hierarchical structures in complex environments by leveraging causality. The proposed method significantly enhances exploration efficiency compared to existing methods in complex tasks such as 2DMinecraft and Eden. The framework consists of causality discovery and subgoal hierarchy construction processes that iteratively boost each other to improve hierarchical structure discovery.  Strengths: 1. Innovative Approach: The paper introduces a novel framework CDHRL that leverages causalitydriven exploration for discovering hierarchical structures in complex environments addressing a key challenge in hierarchical reinforcement learning. 2. Experimental Validation: The extensive experiments conducted on 2DMinecraft and Eden demonstrate the effectiveness of CDHRL in improving exploration efficiency and outperforming existing stateoftheart methods such as HAC LESSON and MEGA. 3. Detailed Methodology: The paper provides a thorough explanation of the proposed framework including causalitybased hierarchical structure causality learning subgoalbased policy training and intervention sampling making it easy for readers to understand the methodology.  Weaknesses: 1. Complexity: The paper delves into intricate details of the framework which may make it challenging for readers to grasp the key concepts and contributions easily. 2. Clarity: Some parts of the paper could benefit from clearer explanations and perhaps more illustrative examples to aid in understanding the concepts better. 3. Limited Scope: The authors acknowledge limitations in the application area of CDHRL for environments with discrete state variables which might reduce the generalizability of the proposed method.  Questions and Suggestions for Authors: 1. Could the authors provide more comparative analysis with existing methods to showcase the superiority of CDHRL in different types of complex environments 2. How does CDHRL perform in environments with continuous state variables and are there plans to adapt the framework for such settings 3. Could the authors elaborate on the computational complexity of CDHRL compared to existing methods Is there a tradeoff between performance and computational resources  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work. However to improve they could provide clearer suggestions on how to broaden the application scope of CDHRL and potentially address the limitations in purely imagebased environments. Overall the paper presents a novel and promising framework for hierarchical reinforcement learning. By addressing the weaknesses mentioned above and providing clarifications the authors can further enhance the impact and understanding of their work.", "nax3ATLrovW": " Peer Review Summary: The paper proposes a novel approach for integrating heterogeneous and diverse information sources in Electronic Design Automation (EDA) using Circuit Graph and Circuit Graph Neural Network (Circuit GNN). The proposed approach is designed to handle multiple EDA tasks and stages efficiently. Experimental results show that the solution achieves stateoftheart performance in logic synthesis and global placement chip design stages. Strengths: 1. Innovative Solution: The integration of topological and geometrical information in Circuit Graph is a novel approach that addresses the limitations of existing methods. 2. Efficiency: The paper demonstrates the efficiency of Circuit GNN in handling diverse tasks on circuits at different stages. 3. Performance: Experimental results show significant performance improvements in congestion prediction and net wirelength prediction tasks. 4. Transferability: The transfer task results highlight the potential of Circuit GNN for knowledge transfer across different tasks. Weaknesses: 1. Limited Comparison: While the paper compares the proposed approach with traditional machine learning methods and existing EDAcustomized models a more comprehensive comparison with a broader range of stateoftheart techniques could provide deeper insights. 2. Complexity: The intricate technical details of the solution might make it challenging for readers without a strong background in EDA and neural networks to fully grasp the methodology. Questions and Suggestions: 1. Scalability: How does the proposed Circuit GNN solution scale with larger and more complex circuit designs 2. Robustness: Have robustness tests been conducted to assess the performance of the model under varying conditions or noisy data inputs 3. Interpretability: How interpretable are the models predictions and representations especially in cases where decisions impact critical design factors Limitations: The authors briefly mentioned limitations regarding the gap between novel machine learning algorithms and commercial EDA tools. To address this the authors could collaborate with industry partners to ensure the practical applicability and adoption of their proposed methodology. Societal Impact: The authors acknowledged the limited societal impact of their solution. One suggestion for improvement could be to proactively consider potential ethical implications that may arise from the application of their technique and address them in the discussion section. Overall the paper presents a significant contribution to the field of EDA with promising results. With some enhancements in comparison scalability and interpretability the proposed methodology could have even broader applicability and impact in the industry. This review provides constructive feedback for the authors to consider for further strengthening their paper.", "rTvH1_SRyXs": " Summary: The paper focuses on the unification of eight popular post hoc explanation methods by demonstrating that they all perform local function approximation of blackbox models. It introduces the Local Function Approximation (LFA) framework and provides insights into the theoretical underpinnings of diverse explanation methods. The work makes substantial contributions by advancing the conceptual understanding of post hoc explanations and offering practical guidance on selecting and designing new explanation methods.  Strengths: 1. Conceptual Framework: The paper introduces a novel conceptual framework (LFA) that unifies diverse explanation methods offering clarity and coherence to the field. 2. Theoretical Insights: The work provides valuable theoretical insights such as the no free lunch theorem for explanation methods enhancing the understanding of the limitations of existing methods. 3. Empirical Validation: Empirical validation on realworld datasets and models supports the theoretical findings enhancing the credibility and applicability of the proposed framework. 4. Practical Guidance: The guiding principle for selecting among methods based on model recovery and perturbation tests provides a practical roadmap for users to choose appropriate explanation methods.  Weaknesses: 1. Incomplete Addressing of Disagreement: While the paper explains the reasons for disagreements among explanation methods it does not delve deeply into potential resolutions for mitigating these disagreements. 2. Limited Consideration of Discrete Data: The discussion primarily focuses on continuous and binary data with a brief mention of discrete data. Further exploration and guidance for explanation methods on discrete data are needed. 3. Lack of User Studies: The paper suggests that humancomputer interaction research and user studies are important for understanding interpretability but it does not provide any empirical evidence or methodologies for conducting such studies.  Questions and Suggestions for Authors: 1. Further Exploration: Could the authors delve deeper into the practical implications of the proposed LFA framework in realworld scenarios to showcase its effectiveness in decisionmaking processes 2. User Studies: How do the authors envision incorporating user studies or feedback mechanisms to validate the interpretability and effectiveness of the proposed unification framework in real applications 3. Extension of Theoretical Framework: Are there plans to extend the theoretical framework to encompass more explanation methods beyond the eight discussed in the paper to provide a more comprehensive understanding of the field  Limitations: The authors may consider elaborating on potential negative societal impacts of the unification framework and offer strategies to mitigate any unintended consequences to ensure responsible use in sensitive domains.", "zAuiZpZ478l": "Peer Review: Summary: The paper introduces the hierarchical lattice layer (HLL) as an extension of the TensorFlow Lattice library for partially monotone regression. It addresses the issues of memory consumption and training constraints associated with the lattice layer. The experiments show that HLL maintains prediction performance on real datasets compared to the lattice layer without sacrificing performance. The paper covers theoretical aspects layer design inference and training algorithms and complexity analysis. Experimental results are presented for both small and large datasets showcasing the performance of MLP HLL and TL LatticeRTL. Strengths: 1. Novel Contribution: Introduces HLL as a solution to the memory consumption and training algorithm constraints of the lattice layer. 2. Theoretical Clarity: Clearly explains the key ideas hierarchical relationship between vertices and auxiliary neural network. Theoretical propositions are logically derived. 3. Experimental Rigor: Conducts experiments on a variety of datasets to demonstrate the effectiveness of HLL in comparison to MLP and TL LatticeRTL. Follows good practices in hyperparameter tuning and model comparison. 4. Detailed Explanations: Provides comprehensive insights into the methodology feature encoding and results analysis. Clear delineation of different neural network models and their implications. Weaknesses: 1. Limited Comparison: The comparison is restricted to HLL MLP and TL LatticeRTL. Including more stateoftheart methods for partially monotone regression could strengthen the analysis. 2. Clarification Needed: Further clarification on the calibration layer comparison (as mentioned in the Appendix) would enhance the manuscripts completeness. 3. Societal Impact Discussion: Lacks discussion on the societal impact of the proposed method on fairness interpretability and ethical implications in machine learning applications. Questions and Suggestions for Authors: 1. How does the proposed HLL layer perform compared to cuttingedge methods outside the TensorFlow Lattice library for monotone regression 2. Can the authors provide practical guidelines for selecting the optimal hyperparameter values for HLL based on their experimentation 3. Have you considered ways to make HLL applicable for larger m values without the need for combining multiple layers 4. How does HLL perform on datasets with missing values or noisy inputs Any techniques for handling such scenarios in the model proposed Limitations: The authors should consider addressing the negative societal impacts of the proposed method such as potential biases in decisionmaking systems trained using partially monotone regression and providing suggestions for mitigating them in future research. Consider incorporating fairness and interpretability aspects into the evaluation of the models societal implications.", "xbJAITw9Z6t": " Peer Review 1) Summary: The paper presents an unsupervised learning (SUL) algorithm aimed at performing completely unsupervised clustering of MNIST digits with high accuracy. The algorithm utilizes stacked energy layers inspired by primary visual cortex models learned through a fast minibatch version of the KSubspaces algorithm. The network architecture is optimized through supervised metalearning demonstrating biologically plausible unsupervised learning. 2) Strengths and Weaknesses: Strengths:  Biological Plausibility: The biological inspiration behind the algorithm is commendable.  High Accuracy: The algorithm achieves a clustering error rate of 2 comparable to supervised techniques.  Innovation: The use of energy layers intricate architectures and metalearning represent novel contributions. Weaknesses:  Limited Dataset: The evaluation is concentrated on the MNIST dataset lacking robustness across diverse datasets.  Complexity: The algorithms complexity might hinder practical implementation and scalability.  Evaluation Method: Lack of error bars for experimental results and smallscale experiments limit generalizability. 3) Questions and Suggestions for Authors:  Generalizability: How does the algorithms performance fare on datasets beyond MNIST  Complexity: Have you considered simplifying the architecture for better scalability and interpretability  Robustness: How sensitive is the algorithm to hyperparameters and what steps have been taken to address this 4) Limitations: The authors should address the limited dataset coverage the need for error bars in results and the potential challenges in scaling the complex algorithm for practical applications. Suggestions include testing on diverse datasets providing comprehensive evaluations and optimizing for simplicity. In conclusion the paper presents a promising SUL algorithm with notable strengths but requires further validation on diverse datasets and considerations for scalability and interpretability. Addressing these aspects would enhance the reproducibility and applicability of the proposed algorithm. Kindly incorporate these recommendations for enhancing the quality and impact of your work. Great job overall on the research and its presentation.", "rJjJda5q0E": "Summary: The paper presents a novel theoretical framework for analyzing Thompson Sampling in contextual bandits with binary losses and adversariallyselected contexts. The key contributions include introducing a lifted information ratio definition adapted to contextual bandits providing regret bounds in terms of entropy and extending the analysis to logistic bandits. The results represent advancements in the state of the art for contextual bandit problems. Strengths: 1. Conceptual Contribution: The adjustment of the information ratio to the contextual setting is a significant conceptual contribution. 2. Theoretical Advances: The paper presents novel regret bounds and analysis techniques for Thompson Sampling in contextual bandits addressing an important problem in the field of sequential decision making. 3. Versatile Approach: The flexibility of the technique is demonstrated by extending the results to logistic bandits with Lipschitz continuous logits showcasing the broad applicability of the framework. 4. Rigorous Analysis: The proofs and results are presented rigorously demonstrating a strong theoretical foundation. Weaknesses: 1. Complexity: The complexity of the proposed lifted information ratio and its relation to standard informationtheoretic measures may make the analysis challenging to interpret for readers not wellversed in the area. Questions and Suggestions: 1. Could the authors provide more intuitive explanations or examples to help elucidate the concept of a lifted information ratio and its practical implications in contextual bandit problems 2. Are there any empirical results or simulations to showcase the practical performance of the proposed techniques in comparison to existing methods 3. Considering the computational aspects of posterior sampling have the authors explored the implications of approximate sampling methods on the regret guarantees Limitations: The paper does not address potential negative societal impacts explicitly. To improve the authors could consider discussing any ethical concerns related to the application of the proposed techniques in realworld scenarios and suggest ways to mitigate them.", "z0M3qHDqH20": " Peer Review  Summary: The paper proposes HUMUSNet a hybrid architecture that combines the efficiency of convolutions with the power of Transformer blocks for accelerated MRI reconstruction. The proposed network achieves stateoftheart results on the fastMRI dataset and outperforms existing methods. The architecture involves a multiscale feature extractor and iterative unrolling for reconstruction. Experimental results on various datasets demonstrate the effectiveness of HUMUSNet.  Strengths: 1. Innovative Architecture: The hybrid design of HUMUSNet combining convolutions and Transformers addresses the limitations of existing models. 2. StateoftheArt Results: Demonstrating new stateoftheart performance on the fastMRI dataset and comparable results on other MRI datasets showcase the effectiveness of the proposed approach. 3. Thorough Experimental Validation: The paper provides comprehensive experimental results ablation studies and comparisons with existing models demonstrating the superiority of HUMUSNet. 4. Detailed Explanations: The paper effectively explains the architectural choices detailing each component and its contribution to the overall network design.  Weaknesses: 1. Compute and Memory Intensive: The paper points out the high computational cost of Transformers for large input resolutions without providing solutions or optimizations to address this challenge. 2. FixedSize Inputs: The limitation of HUMUSNet requiring fixedsize inputs is mentioned as a drawback without proposing a more flexible design to address this issue.  Questions and Suggestions for Authors: 1. Dataset Variability: How does HUMUSNet perform on datasets with more diverse anatomies and acquisition patterns beyond the ones used in the experiments 2. Generalizability: Have you tested the generalizability of HUMUSNet on unseen datasets or MRI acquisition scenarios to assess its robustness 3. Hyperparameter Sensitivity: Did you explore the sensitivity of HUMUSNet performance to different hyperparameters or network architectures 4. Scalability: How scalable is HUMUSNet for potential deployment in realtime clinical settings considering the computational demands of the network  Limitations: The authors briefly touch upon the limitations of the fixedsize inputs but could provide more extensive discussion on potential negative societal impacts such as accessibility of advanced MRI reconstruction technology in resourceconstrained regions. In conclusion the paper presents a novel and effective approach for accelerated MRI reconstruction. Addressing the highlighted weaknesses and questions would enhance the robustness and applicability of the proposed HUMUSNet architecture.  Note to Authors: Consider addressing the limitations and questions raised for a more comprehensive evaluation of the proposed method. Further exploration of generalizability scalability and societal impact could strengthen the impact of your work.", "yjWir-w3gki": " Summary The paper proposes a theory for continuoustime modelbased reinforcement learning generalized to arbitrary discount functions specifically focusing on nonexponential discounting schemes. The authors derive a Hamilton\u2013Jacobi\u2013Bellman (HJB) equation characterizing the optimal policy and illustrate solving it using a collocation method with deep learning function approximation. Additionally the paper introduces an approach for inverse reinforcement learning to estimate parameters of the discount function based on decision data.  Strengths 1. Innovative Approach: The paper addresses an important gap between reinforcement learning and human discounting behavior by proposing a model that broadens the scope of discount functions. 2. Theoretical Rigor: The derivation of the HJB equation for general discount functions and the algorithmic approach to solve it demonstrate a deep understanding of the underlying principles. 3. Application: The use of simulated problems to validate the proposed method showcases its practical applicability and potential for realworld scenarios. 4. Comprehensive Background: The paper provides a thorough background on survival analysis discounting optimal control and related works enhancing the understanding of the proposed methodology.  Weaknesses 1. Complexity: The paper presents a complex mathematical framework which might be challenging for readers not wellversed in the technical details of reinforcement learning and discount function modeling. 2. Limited Empirical Validation: While the simulated experiments demonstrate the effectiveness of the proposed method actual empirical validation on realworld datasets or scenarios is lacking.  Questions and Suggestions for Authors 1. Validation: Have you considered conducting experiments with realworld datasets to validate the effectiveness of your approach in practical applications 2. Scalability: How scalable is your method to handle problems with larger state spaces or continuous action spaces 3. ModelFree Approaches: Have you explored modelfree approaches for dealing with general discount functions and if not do you plan to investigate this in the future 4. Ethical Considerations: How do you plan to address any negative societal impacts that might arise from using behavioral experiments to analyze human discounting behavior  Limitations The authors have not explicitly addressed potential negative societal impacts of their work. To enhance the ethical considerations of their research it would be beneficial for the authors to collect only anonymized data and consider data privacy regulations in any future applications of their method. Overall the paper presents a novel and theoretically sound approach that can contribute significantly to the field of reinforcement learning and human decisionmaking understanding. Strengthening empirical validation addressing scalability concerns exploring modelfree alternatives and incorporating ethical considerations would enhance the impact and applicability of the research.", "mrt90D00aQX": " Summary: The paper introduces Federated Learning (FL) with Domain Generalization (DG) to address the generalization issue to unknown target data within a decentralized and privacypreserving setting. The proposed model FedSR incorporates L2norm and conditional mutual information regularization to encourage a simple yet effective representation learning framework. Extensive experiments indicate that FedSR outperforms existing FL methods significantly in domain generalization tasks.  Strengths: 1. Innovative Approach: Introducing DG to FL in a decentralized setting is a novel contribution. 2. Simple  Effective: The use of L2norm and conditional mutual information regularizers is straightforward yet results in significant performance gains. 3. Theoretical Foundation: The paper provides theoretical connections between the regularization objectives and representation alignment. 4. Thorough Experimental Evaluation: Extensive experiments across various datasets validate the effectiveness of the proposed method and compare it with relevant baselines.  Weaknesses: 1. Simplicity vs. Complexity: While the simplicity of the approach is a strength it might limit the models ability to handle complex scenarios. 2. Applicability Concerns: The paper focuses on implicit alignment within the source domains which may limit generalization to unseen target domains.  Questions and Suggestions: 1. Clarification on Expected Performance: Can the authors elaborate on the performance expectations or metrics for FedSR in comparison to the stateoftheart centralized methods 2. Scalability Concerns: How does the complexity or performance of FedSR scale with an increased number of clients or domains in a realworld FL scenario 3. Privacy Considerations: How does the proposed approach ensure privacy and data confidentiality across decentralized clients especially in sensitive domains like medical data  Limitations and Impact: The authors should explicitly address the limitation regarding the models inability to generalize to unseen target domains due to inherent constraints in enforcing invariance among source domains. Providing strategies to enhance generalization across a broader range of domains would be valuable for future work. Overall the paper presents a promising avenue for incorporating domain generalization into Federated Learning while respecting privacy constraints. Addressing scalability privacy assurances and generalization limitations could further strengthen the impact of the proposed method.", "msBC-W9Elaa": " Summary: The paper proposes a novel covering technique localized for the trajectories of SGD leading to dimensionindependent generalization bounds for nonconvex loss functions. The authors demonstrate improvements over existing stateoftheart rates in various contexts including multiindex linear models multiclass SVMs and Kmeans clustering.  Strengths: 1. Novelty: The proposed localized covering technique introduces a fresh perspective on deriving generalization bounds for SGD in the nonconvex regime. 2. Theoretical Rigor: The paper provides detailed theoretical analysis including Lemmas Propositions Theorems and Corollaries ensuring a solid theoretical foundation for the contributions. 3. Applicability: The results are applied to significant machine learning models like multiindex linear models multiclass SVMs and Kmeans clustering showcasing the practical relevance of the findings.  Weaknesses: 1. Complexity: The paper is highly technical which may make it challenging for a broader audience to grasp the key contributions and implications. 2. Clarity: Some sections contain dense mathematical notation which might hinder the understanding for readers not deeply familiar with the subject matter. 3. Practical Implications: While theoretical bounds are important the practical implications of the improved theoretical rates in realworld scenarios should be better articulated.  Questions and Suggestions for Authors: 1. Clarification: Can the authors provide more intuitive explanations or examples to help readers better understand the implications of the proposed covering technique 2. Generalization Beyond SGD: Have the authors considered how their approach might extend to optimization algorithms beyond SGD particularly those with varying step sizes 3. Comparison: How does the proposed method compare to algorithmindependent approaches in terms of computational complexity and practical applicability  Limitations: The authors have not adequately addressed the potential negative societal impact of their work. To improve they could discuss the implications of their findings in terms of fairness bias and interpretability in machine learning models.  Suggestions for Improvement: The authors should consider:  Including more illustrative examples to enhance the accessibility of the paper.  Providing insights on the computational efficiency and scalability of their method.  Discussing the practical utility and impact of the proposed technique on realworld machine learning tasks.  Acknowledgment: The acknowledgment section appropriately cites the funding sources supporting the research. However a broader acknowledgment of potential limitations or future areas of research could enhance the completeness of this section.", "lKULHf7oFDo": " Summary: The paper presents a novel approach to fairness in federated learning by introducing corestable fairness through concepts from game theory and social choice theory. The proposed CoreFed protocol optimizes a corestable predictor that is robust to lowquality data and prioritizes fairness with guarantees of proportionality and Paretooptimality. The authors validate their approach empirically on realworld datasets showing higher corestable fairness compared to FedAvg while maintaining similar accuracy.  Strengths: 1. Innovative Concept: The introduction of corestable fairness in federated learning is a novel and valuable contribution. 2. Theoretical Framework: The application of game theory and social choice theory concepts provides a strong theoretical foundation. 3. Clear Structure: The paper follows a logical structure clearly presenting the problem proposed solution theoretical analysis and empirical results. 4. Empirical Validation: Empirical results on realworld datasets support the effectiveness of the CoreFed protocol in achieving corestable fairness.  Weaknesses: 1. Complexity: The paper may be challenging for readers unfamiliar with game theory and social choice theory potentially limiting its accessibility. 2. Limited Scope: The empirical evaluation is based on a few datasets and model architectures which could restrict the generalizability of the findings. 3. Technical Terminology: The paper contains highly technical terms and concepts that may require a high level of expertise to fully understand.  Questions and Suggestions for Authors: 1. Clarity on Utility Functions: Provide more detailed explanations or examples to help readers understand the utility functions and how they are applied in the algorithms. 2. Scalability Concerns: Discuss the scalability of CoreFed to larger datasets with numerous agents and heterogeneous data distributions. 3. Comparison with Existing Fairness Methods: Can the authors provide a more indepth comparison with existing fairness methods in federated learning to highlight the advantages of corestable fairness  Limitations: The authors should consider addressing the scalability of their approach to larger datasets and more diverse agent scenarios to enhance the applicability of their proposed method.  Ethical Implications: Authors should consider discussing any potential negative societal impacts of their work especially regarding bias reduction and fairness challenges while implementing corestable fairness in realworld applications.", "mjVmifxpKqS": " Review of the Paper: \"MultiBatch Reinforcement Learning with NearOptimal Regret Bounds\"  Summary: The paper addresses the problem of multibatch reinforcement learning modeled by finitehorizon Markov Decision Processes (MDPs). The key contribution lies in proposing an efficient algorithm to achieve nearoptimal regret bounds while minimizing batch complexity. The novel approach allows for policy updates in batch formats addressing the tradeoff between exploration and exploitation with minimal adaptivity requirements. The proposed technique shows promise in reducing policy switches and achieving low regret with improved batch complexity.  Strengths: 1. Novel Contribution: The paper introduces a new approach to multibatch RL with a focus on batch complexity addressing the tradeoff between exploration and exploitation. 2. Efficient Algorithm: The design of an efficient algorithm to achieve nearoptimal regret bounds using batch updates enhances the scalability and applicability of the proposed framework. 3. Theoretical Framework: The work provides solid theoretical foundations including the introduction of tight confidence regions and policy elimination techniques for efficient exploration of policies. 4. Clear Exposition: The paper is wellstructured with clear explanations of the problem proposed solutions and technical contributions.  Weaknesses: 1. Experimental Evaluation: The paper lacks empirical evaluations on realworld datasets or benchmarks limiting the practical validation of the proposed algorithm. 2. Clarity in Complexity Analysis: Some parts of the complexity analysis could be explained more intuitively for readability and understanding by a wider audience. 3. Relation to Existing Works: The paper could provide a clearer comparison with existing works in the field of reinforcement learning showcasing the novelty and improvements over prior art.  Questions and Suggestions for Authors: 1. Could you elaborate on the practical implications of your proposed multibatch reinforcement learning framework How does it compare to existing RL algorithms in realworld applications 2. Can you provide insights into the scalability and efficiency of the proposed algorithm when dealing with larger state and action spaces or complex environments 3. How adaptable is your approach to different types of MDPs or reward functions with nonstandard properties Have you considered the generalizability of your framework 4. Have you considered integrating any practical case studies or use cases to demonstrate the effectiveness and applicability of your algorithm in realworld scenarios  Limitations: The authors should consider incorporating empirical evaluations on benchmark tasks or realworld applications to validate the effectiveness and efficiency of their algorithm in practical settings. Empirical validation would enhance the credibility and applicability of the proposed framework in realworld RL scenarios.", "xnuN2vGmZA0": "1) Summary: The paper introduces VITA a novel paradigm for Video Instance Segmentation (VIS) that leverages objectoriented information for offline video understanding. VITA uses object tokens distilled from an image object detector to build relationships between objects in a video without relying on dense spatiotemporal backbone features. This approach enables complete offline inference for long and highresolution videos achieving stateoftheart performance on various VIS benchmarks. 2) Strengths:  The paper addresses a significant challenge in Video Instance Segmentation by proposing a new paradigm that achieves stateoftheart results on benchmark datasets.  VITAs approach of using objectcentric tokens for video understanding shows practical advantages over previous offline methods in handling long sequences and highresolution videos.  The methods ability to effectively distill objectwise context and build relationships between objects without relying on dense spatiotemporal features is innovative and impactful. 3) Weaknesses:  The paper could benefit from more detailed comparisons with existing methods both qualitatively and quantitatively to further highlight the strengths of VITA.  While the practical advantages of VITA are discussed a more indepth discussion on potential limitations in specific scenarios or challenging datasets would enhance the paper. 4) Questions and Suggestions for Authors:  How does VITA compare in terms of computational efficiency and inference speed compared to other stateoftheart models especially for processing long videos  Could the authors provide insights on the interpretability of the object tokens and how they contribute to the models performance in understanding complex video scenes  Have the authors considered evaluating VITA on more diverse and challenging datasets to assess its generalization capabilities beyond the provided benchmarks 5) Limitations:  The authors should consider explicitly discussing the limitations of VITA in processing an unlimited number of frames and in identifying complex behaviors in very long video sequences. Suggestions for addressing these limitations could be valuable for future research. Overall the paper presents a novel approach to offline Video Instance Segmentation with significant advantages over existing methods. By addressing potential limitations providing additional comparisons and discussing possible future directions the authors can further strengthen their work.", "xNeAhc2CNAl": " Summary and Contributions The paper investigates extreme compression techniques particularly ultralow bit precision quantization for fitting large NLP models on resourceconstrained devices. It performs a comprehensive systematic study on the impact of key hyperparameters and training strategies from previous works and proposes a simple yet effective compression pipeline named XTC. The XTC method successfully achieves stateoftheart results on GLUE tasks by compressing a 5layer BERT without pretraining knowledge distillation.  Strengths  The paper presents a thorough investigation of extreme compression techniques and provides clear insights into the effectiveness of different optimization strategies.  The proposed XTC method simplifies the compression process and outperforms previous stateoftheart methods in terms of model size reduction and accuracy.  The experimental setup is detailed with evaluations done on realistic training budgets and various learning rates.  Weaknesses  The paper lacks a comparative analysis with a wider range of existing compression methods to provide a more comprehensive evaluation.  The detailed experimental methodology could be better organized to enhance clarity and readability for readers.  The paper could benefit from more detailed discussions on the limitations of the proposed XTC method and areas for future exploration.  Questions and Suggestions 1. Could you provide more insights into how the XTC method would perform in scenarios where training data size is limited 2. Have you considered the potential tradeoffs between extreme compression and model interpretability or generalizability How might these factors impact the applicability of the XTC method in realworld scenarios 3. It would be helpful to include a discussion on the computational resources required for implementing the XTC method especially on edge or embedded devices.  Limitations The paper addresses the limitations of extreme quantization methods but could further elaborate on the potential negative societal impacts such as biases introduced during compression or implications for privacy and security. Suggestions for improvement include conducting a more indepth analysis of societal implications and proposing strategies to mitigate any negative effects. Overall the paper provides valuable insights into extreme compression techniques for NLP models and introduces an effective XTC method. Further discussions on the applicability in different scenarios and potential societal impacts would enhance the relevance and impact of the research.", "xijYyYFlRIf": " Summary: The paper introduces GAUDI a generative model designed to capture complex and realistic 3D scenes and render views from these scenes. The model optimizes a latent representation that disentangles radiance fields and camera poses allowing for both unconditional and conditional generation of 3D scenes. GAUDI outperforms existing methods on multiple datasets and offers potential applications in machine learning and computer vision tasks.  Strengths: 1. Scalability and Performance: GAUDI scales 3D scene generation to thousands of indoor scenes with high performance avoiding issues like mode collapse or canonical orientation problems during training. 2. Innovative Approach: The disentanglement of radiance fields and camera poses along with the novel denoising optimization objective sets GAUDI apart from previous works and contributes to its superior generation performance. 3. Stateoftheart Results: The model achieves stateoftheart performance across multiple datasets demonstrating its effectiveness in unconditional and conditional generative tasks.  Weaknesses: 1. Complexity: The complex optimization process and network architecture might make the model challenging to understand and implement for those less familiar with the field. 2. Dependency on GroundTruth Data: The reliance on groundtruth depth information during training may limit the models applicability to realworld scenarios where such data might not be available.  Questions and Suggestions: 1. Interpretability: Could the authors provide more insights into how the disentangled latent representations capture the underlying information of radiance fields and camera poses 2. Generalization Capability: How does GAUDI perform when applied to unseen or diverse datasets with varying scene complexities 3. Training Stability: Have the authors considered any strategies to address potential training instabilities or convergence issues especially when dealing with largescale datasets  Limitations: The authors have not explicitly addressed potential negative societal impacts or ethical considerations arising from the implementation or application of GAUDI. Considering the increasing importance of ethical AI it would be beneficial to include a discussion on these aspects to ensure responsible deployment of the model in realworld settings.", "lzZstLVGVGW": "PeerReview: Summary: The paper introduces Earthformer a novel spacetime Transformer for Earth system forecasting that utilizes a Cuboid Attention mechanism to address the challenges of highdimensional spatiotemporal data. The proposed model achieves stateoftheart performance on synthetic and realworld datasets for tasks such as precipitation nowcasting and El Ni\u00f1oSouthern Oscillation (ENSO) forecasting. Strengths:  Innovative Approach: The paper presents a novel Cuboid Attention mechanism that efficiently handles highdimensional spatiotemporal data contributing to the field of Earth system forecasting.  Experimental Results: Extensive experiments on synthetic datasets and realworld benchmarks demonstrate the effectiveness and superiority of Earthformer over existing models like Rainformer and traditional CNN and RNNbased approaches.  Hierarchical EncoderDecoder Architecture: The hierarchical encoderdecoder architecture along with the exploration of different cuboid attention patterns showcases a systematic approach to model design.  Global Vectors: The introduction of global vectors enables communication among local cuboids enhancing the models understanding of global dynamics and improving forecasting accuracy. Weaknesses:  Limited Application: The paper focuses heavily on technical aspects and experimental results providing limited insights into the practical application and deployment of Earthformer in realworld scenarios.  Unclear Comparison: While the paper claims superior performance over existing models a more indepth comparison with these models in terms of computational complexity scalability and interpretability would strengthen the paper.  Limited Evaluation Metrics: The evaluation relies primarily on MSE and CSI and expanding the evaluation to include additional metrics or comparisons with more diverse baselines could provide a comprehensive analysis. Questions and Suggestions: 1. Clarity in Communication: Could the authors enhance the paper by clearly articulating how Earthformer addresses key limitations of existing models emphasizing the unique benefits of the proposed Cuboid Attention mechanism 2. Scalability Considerations: How does the computational complexity of Earthformer compare to traditional CNN and RNN models especially in realtime forecasting scenarios where speed is crucial 3. Generalization: Have the authors investigated the generalizability of Earthformer to other Earth system forecasting tasks beyond precipitation nowcasting and ENSO forecasting Limitations: While the paper provides a comprehensive evaluation of Earthformers performance it could be enhanced by addressing the following limitations:  Uncertainty Modeling: The deterministic nature of Earthformer could limit its ability to capture uncertainty in forecasts. Incorporating probabilistic forecasting capabilities could enhance the models predictive power.  Lack of Physical Constraints: The purely datadriven approach of Earthformer raises concerns about overlooking valuable physical constraints in Earth system dynamics. Future work could focus on integrating physical knowledge in the model architecture. Overall the paper presents a valuable contribution to the field of Earth system forecasting and addressing the outlined suggestions could further strengthen the clarity applicability and impact of the proposed Earthformer model.", "tX_dIvk4j-s": " Summary The paper introduces VisCo Grids a gridbased surface reconstruction method incorporating Viscosity and Coarea priors as alternatives to Implicit Neural Representations (INRs). It aims to address the shortcomings of INRs such as control over inductive bias inference cost and training time. Experimental results demonstrate that VisCo Grids achieve comparable accuracy to stateoftheart INR methods on a standard 3D reconstruction dataset.  Strengths 1. Innovative Approach: Proposing VisCo Grids as an alternative to INRs with the incorporation of geometric priors is a novel and significant contribution. 2. Thorough Experiments: The paper conducts comprehensive experiments including comparisons with various stateoftheart methods and ablation studies to demonstrate the effectiveness of VisCo Grids. 3. Clear Explanations: The paper provides detailed explanations of the methodology loss functions and experimental setups enhancing clarity for readers.  Weaknesses 1. Limitations of Grids: The limitation of fixed degrees of freedom in grid methods compared to the flexibility of neural networks for adjusting to detailed areas could impact the models adaptability. 2. Incomplete Comparison: While the paper compares VisCo Grids with INR methods a comparison with more traditional approaches for surface reconstruction could enrich the evaluation.  Questions and Suggestions 1. Generalization: How well does VisCo Grids generalize to various types of objects with different complexities and shapes 2. Edge Cases: How does VisCo Grids handle edge cases or challenging scenarios in reconstruction tasks 3. Efficiency vs. Accuracy Tradeoff: Are there ways to balance the training time improvements with the potential loss of accuracy in VisCo Grids compared to INRs  Limitations The authors have not explicitly addressed potential negative societal impacts of their work. To enhance societal responsibility consider discussing the ethics and implications of utilizing highquality 3D reconstruction in potentially harmful applications.  Constructive Suggestions 1. Incorporate a dedicated section discussing ethical considerations social implications and potential misuses of advanced 3D reconstruction technology. 2. Conduct further experiments to evaluate VisCo Grids on a wider range of datasets and provide a more indepth comparison with both traditional and modern surface reconstruction methods. Overall the paper presents a promising advancement in surface reconstruction and addressing the above points could strengthen its impact and relevance in the field.", "oDRQGo8I7P": " Paper Summary and Contributions: The paper introduces Riemannian ScoreBased Generative Models (RSGMs) as an extension of ScoreBased Generative Models (SGMs) to Riemannian manifolds enabling modeling distributions on nonEuclidean spaces. RSGMs incorporate the geometry of data by defining a forward noising process directly on the manifold and induce a manifoldvalued reverse process. The paper demonstrates the effectiveness of RSGMs on various manifolds including spherical data from earth and climate science datasets.  Strengths: 1. Novel Contribution: Introducing RSGMs to extend generative modeling to nonEuclidean spaces. 2. Theoretical Foundation: The paper provides a strong theoretical basis for RSGMs including convergence bounds on compact manifolds. 3. Empirical Results: Demonstrates the superiority of RSGMs over baselines on various tasks and datasets. 4. Methodological Detail: Detailed explanation of the methods employed including the forward noising process on manifolds and approximate sampling techniques.  Weaknesses: 1. Complexity: The paper delves into intricate mathematical and theoretical concepts that may be challenging for readers not wellversed in the field. 2. Empirical Evaluation: While the empirical results show the superiority of RSGMs a more comprehensive comparison with additional baselines could strengthen the analysis. 3. Reproducibility: Detailed descriptions of implementation specifics hyperparameters selection and code availability could improve reproducibility.  Questions and Suggestions: 1. Can you provide more insights into the scalability of RSGMs with increasing manifold dimensions 2. How does RSGM compare with other stateoftheart generative models designed for manifoldvalued data 3. Considering the complexity of the proposed method could a simplified version with reduced mathematical formalism be presented for a broader audience 4. How sensitive is RSGM to the choice of hyperparameters and are there any guidelines for tuning them effectively  Limitations: The paper could benefit from explicitly addressing the potential negative societal impact of their work such as any biases or unintended consequences in the generative modeling process. Providing ethical considerations and suggestions for addressing any limitations or biases would enhance the manuscript.", "zz0FC7qBpkh": " Paper Summary and Contributions The paper introduces a new notion of invariance called Mirror Reflected IRM (MRI) as an improvement over Invariant Risk Minimization (IRM) to enhance generalization to outofdistribution data. The study identifies and addresses a fundamental design flaw in IRM. The authors propose MRI as a complementary concept providing a practical version MRIv1 that outperforms IRMv1 particularly in imagebased nonlinear problems.  Strengths of the Paper 1. Identification of Design Flaw: The paper effectively identifies a fundamental flaw in the IRM formulation which limits its applicability. This is a crucial contribution to the field. 2. Novel Concept of MRI: Introducing a new notion of invariance (MRI) that bypasses the limitations of IRM demonstrates innovative thinking and theoretical advancement. 3. Empirical Validation: The empirical results showing significant performance improvements of MRIv1 over IRMv1 in imagebased nonlinear problems strengthen the papers contributions.  Weaknesses of the Paper 1. Complexity of Exposition: The paper is highly technical and dense making it challenging for readers not familiar with the domain to follow the arguments and implications easily. 2. Lack of RealWorld Applications: While the theoretical contributions are solid the paper lacks realworld application scenarios to demonstrate the practical usefulness of MRI which could limit its impact beyond theoretical frameworks.  Questions and Suggestions for Authors 1. Clarification on Algorithm Implementation: Can the authors further elaborate on the actual implementation and computational complexity of MRIv1 in comparison to IRMv1 to provide insights into its practical feasibility 2. Generalization Beyond Linear Problems: How transferable are the findings of MRIv1 beyond linear problems to more complex realworld data scenarios 3. Comparison Metrics: Apart from risk and accuracy are there other metrics or evaluation methodologies that the authors recommend for assessing the degree of invariance achieved by MRIv1  Limitations The paper thoroughly addresses the limitations of IRM and introduces a novel concept MRI to improve invariance in machine learning models. To further enhance the study the authors could conduct additional realworld application studies demonstrating the efficacy of MRI in practical scenarios with diverse datasets.", "m_JSC3r9td7": " Summary: The paper introduces DeepMed a novel method for causal mediation analysis using deep neural networks (DNNs). It aims to estimate Natural Direct and Indirect Effects accurately in mediation analysis without relying on sparsity constraints. The paper provides theoretical advancements in the efficiency of estimating causal effects and adapts to lowdimensional structures of nuisance functions by utilizing DNNs. Extensive synthetic experiments are conducted to validate the performance of DeepMed and real datasets on machine learning fairness are analyzed to demonstrate its practical applicability.  Strengths: 1. Innovative Methodology: DeepMed proposes a unique approach to causal mediation analysis by leveraging deep neural networks expanding the scope of semiparametric causal inference. 2. Theoretical Advancements: The paper provides novel theoretical results on achieving semiparametric efficiency without sparse DNN architectures advancing the understanding of DNNbased causal inference. 3. Extensive Experiments: Comprehensive synthetic experiments are conducted to evaluate DeepMeds performance highlighting its superiority over other methods in various scenarios. 4. Real Data Application: The real data analysis on machine learning fairness showcases the practical utility of DeepMed and its consistency with prior findings.  Weaknesses: 1. Limited RealData Application: While the analysis on fairness datasets is insightful a more extensive realworld application could strengthen the practical relevance of DeepMed. 2. Implicit Regularization Concern: The observation of potential biases due to implicit regularization in DNN training processes (as seen in synthetic experiments) needs further investigation and discussion. 3. Gap Between Theory and Practice: Despite theoretical advancements the paper points out discrepancies between statistical guarantees and empirical observations emphasizing the need for research bridging this gap.  Questions and Suggestions for Authors: 1. Further RealData Analysis: How can DeepMed be extended to more diverse realworld datasets to showcase its generalizability in various domains 2. Discussion on Implicit Regularization: Can the authors provide more insights into the implications of implicit regularization in DNN training for causal inference methods and potential strategies to mitigate its impact 3. Addressing the Gap: In what ways can the paper address the current gap between theoretical guarantees and empirical performance to enhance the practical relevance of DeepMed  Limitations: The paper adequately addresses the limitations by highlighting potential issues of unmeasured confounding and the negative societal impact of causal inference methods if applied without proper scrutiny. It suggests future research directions to address these limitations and ensure the responsible application of DeepMed.", "wiGXs_kS_X": " Summary: The paper introduces Logical Credal Networks (LCNs) a new probabilistic logic model that allows probability bounds and conditional probability bounds on arbitrary propositional and firstorder logic formulas without strong restrictions. The LCN outperforms existing approaches in aggregating multiple sources of imprecise information.  Strengths: 1. Contribution: The paper presents a significant advancement in probabilistic logic allowing flexibility in representing imprecise knowledge and maintaining a Markov condition similar to Bayesian networks. 2. Novelty: The introduction of LCNs fills a gap in existing formalisms by allowing cyclic dependencies between variables and addressing the limitations of previous approaches. 3. Algorithmic Development: The paper provides an exact inference algorithm for computing lower and upper bounds on the posterior probability of a query formula showcasing the practical applicability of LCNs. 4. Experimental Evaluation: The experimental results demonstrate the superiority of LCNs over existing methods in random LCNs and benchmark problems like credit card fraud detection highlighting the effectiveness of LCNs in handling complex realworld scenarios.  Weaknesses: 1. Complexity: The paper acknowledges that solving problems with larger instances becomes infeasible due to the exponential complexity of the nonlinear programs involved indicating a limitation in scalability. 2. Runtime: LCNs are noted to be slower compared to competitors due to the complexity of the nonlinear programs suggesting a need for further optimization for efficiency. 3. UniqueAssessment Requirement: The Bayesian and credal network models perform poorly due to the uniqueassessment requirement leading to increased false positive predictions highlighting a limitation in these approaches.  Questions and Suggestions for Authors: 1. Scalability: Have you explored strategies to address the scalability issue of the LCNs for larger problem instances 2. Efficiency: Are there ongoing efforts to optimize the runtime of LCNs for improved performance without compromising accuracy 3. Model Comparison: Could you provide a more detailed comparison of the limitations of Bayesian networks and credal networks regarding the uniqueassessment requirement in the context of your study  Limitations: The authors have not adequately addressed the scalability and runtime efficiency concerns of their approach. To improve consider optimizing algorithms for better scalability and efficiency in solving larger instances.", "x56v-UN7BjD": " Summary The paper introduces new algorithms and convergence guarantees for privacypreserving nonconvex Empirical Risk Minimization (ERM) on smooth ddimensional objectives. The authors develop an algorithm that achieves an improved gradient bound and differential privacy while finding a gradient of norm \\xc3\\x95 (d(N)(23)) in O(N(43)d(23)) gradient evaluations.  Strengths 1. The paper addresses an important and timely issue of privacy in machine learning models. 2. The authors develop novel algorithms with improved convergence guarantees for nonconvex ERM. 3. The use of sensitivity analysis and treeaggregation techniques demonstrates innovative approaches to privacypreserving machine learning.  Weaknesses 1. The complexity and technicality of the presentation might make it challenging for some readers to understand. 2. The limitations and negative societal impacts of the proposed algorithms are not thoroughly discussed. 3. The paper lacks empirical evaluations or practical demonstrations of the proposed algorithms.  Questions and Suggestions 1. Can the authors provide more insights into how the proposed algorithms perform in realworld scenarios or on benchmark datasets 2. How do the proposed algorithms compare with existing stateoftheart privacypreserving methods in terms of both utility and privacy guarantees 3. Are there any specific scenarios or applications where the proposed algorithms may be particularly effective or inefficient  Limitations The authors have not adequately addressed the limitations and potential negative societal impact of their work. To improve they could include discussions on the computational efficiency scalability deployment challenges and ethical implications of the proposed algorithms.", "mE1QoOe5juz": " Review:  1) Summary: The paper proposes a new learning framework that leverages the tiered structure of realworld userinteraction applications by maintaining two policies one for risktolerant users and the other for riskaverse users. The paper investigates the benefits of separating the policies for different user groups achieving constant regret for riskaverse users under specific settings.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important area in reinforcement learning and provides a novel framework for handling different user groups.  The theoretical analysis is thorough providing clear explanations and establishing benefits of leveraging tiered structures.  The papers contributions are welldefined and supported by technical results and proofs. Weaknesses:  The complexity of the framework and technical details might make it challenging for nonexperts to follow.  The assumptions made in modeling the scenarios (e.g. equal frequency of interactions) could impact the generalizability of the results to realworld applications.  The paper lacks empirical validation through experiments on real datasets which could enhance the practical relevance of the proposed framework.  3) Questions and Suggestions for Authors:  Can the proposed tiered structure framework be easily applied to realworld userinteraction systems considering the practical challenges and complexities of such applications  How sensitive is the frameworks performance to deviations from the assumptions made in the theoretical modeling and are there strategies to mitigate these sensitivities  Could the authors provide insights on the computational complexity and scalability of the proposed algorithms to handle largescale datasets with diverse user groups  4) Limitations and Suggestions for Improvement:  The paper lacks a discussion on the potential negative societal impacts or ethical considerations related to implementing the proposed framework. Authors should address these aspects to ensure responsible deployment in realworld applications.  Overall the paper offers a valuable contribution to the reinforcement learning research field by exploring the benefits of tiered structures in userinteraction applications. Addressing the weaknesses and incorporating suggestions could enhance the clarity and practical relevance of the work.", "tjFaqsSK2I3": " Peer Review of the Paper: \"Unified Sequence Interface for Core Computer Vision Tasks\" 1. Summary: The paper proposes a novel approach to unify core computer vision tasks through a shared pixeltosequence interface leveraging the success of language models in natural language tasks. The model unifies object detection instance segmentation keypoint detection and image captioning tasks by transforming the inputs and outputs into sequences of tokens. A single model architecture and loss function are used to train the neural network for all tasks eliminating the need for taskspecific customization. Experiments on the COCO dataset demonstrate competitive performance across all tasks compared to specialized models. 2. Strengths:  Novel Concept: The idea of unifying diverse vision tasks under a single interface is innovative and could simplify model design and facilitate transfer learning.  Experimental Results: The paper provides thorough experimental results on the COCO dataset showing competitive performance across tasks.  Clarity: The paper is wellstructured and clearly explains the methodology and experimental setups. 3. Weaknesses:  Generalizability: The paper focuses on a limited set of vision tasks limiting the scope of potential applications. Expanding the approach to a broader range of tasks could enhance its relevance.  Scalability: While the proposed model achieves competitive performance scalability concerns related to model size and training on larger datasets are not extensively addressed.  Inference Speed: The potential inference speed implications particularly for longer sequences compared to specialized systems should be discussed further. 4. Questions and Suggestions for Authors:  It would be interesting to know how the proposed unified interface compares in terms of computational efficiency during training and inference compared to taskspecific models.  How robust is the model to variations in the types and complexities of tasks included Could the approach be extended to more tasks and generalized effectively  Have the authors considered assessing the models performance on different datasets with varying complexities to evaluate its generalizability further 5. Limitations: The authors could further elaborate on:  Addressing scalability concerns by exploring methods for training on larger datasets and enhancing model size.  Detailed discussions on potential improvements for inference speed including the exploration of nonautoregressive modeling for efficiency. In summary the paper presents a promising approach to unify core computer vision tasks under a shared interface. By addressing scalability generalizability and efficiency aspects the authors can enhance the impact and applicability of their work. Overall the paper is wellwritten and the methodology is wellexplained with strong experimental validations. With some improvements in addressing the identified weaknesses and limitations the paper has the potential to make a significant contribution to the computer vision field.", "qwjrO7Rewqy": " Review of the Paper  1) Summary of the Paper: The paper proposes a novel graph neural network method to efficiently estimate extended persistence diagrams (EPDs) on graphs by leveraging insights from neural algorithmic reasoning. The motivation behind this work is to address the computational bottleneck in computing EPDs for large and dense graphs. The proposed model decomposes the EPD computation into edgewise predictions and aligns with the sequential algorithms using a UnionFind algorithm. Empirical validation demonstrates the effectiveness of the method in approximating EPDs with better supervision and efficiency on various graph representation learning tasks.  2) Assessment of Strengths and Weaknesses:  Strengths:  The paper addresses an important challenge in computing EPDs for large and dense graphs proposing an innovative approach that significantly accelerates the computation without sacrificing accuracy.  The work is wellmotivated and technically sound leveraging insights from neural algorithmic reasoning and aligning the neural network model with the EPD computation algorithm.  The proposed method demonstrates convincing empirical results showcasing its effectiveness on downstream graph learning tasks such as node classification and link prediction.  The paper presents clear and detailed explanations of the problem methodology and experimental setups aiding in the understanding of the proposed approach.  Weaknesses:  While the technical contributions are wellpresented the paper could benefit from a more detailed discussion on the limitations of the proposed method to provide a comprehensive view.  The theoretical analysis and comparison with existing methods could be further elaborated to strengthen the novelty and impact of the proposed approach.  3) Questions and Suggestions for the Authors:  Could the authors elaborate on potential future directions or extensions of the proposed method such as exploring the applicability in other domains or enhancing the models scalability to even larger graphs  Can the authors provide more insights into the interpretability of the approximated EPDs and their implications for realworld applications  How do the computational efficiency gains of the model translate to scenarios where realtime processing or iterative learning is required and how robust is the method to varying input graph characteristics or noise  4) Limitations and Societal Impact: The authors should explicitly discuss the limitations of their work such as generalizability to diverse graph structures potential biases in the learning process and any negative societal impacts arising from the automated decisionmaking based on the approximated EPDs. Providing such reflections can enhance the ethical considerations and promote responsible AI development.  Overall Evaluation: The paper presents a wellstructured and technically sound approach to efficiently estimate EPDs on graphs using a novel graph neural network model. With strong empirical validation and innovative insights into approximating complex mathematical computations the work contributes significantly to the field of graph representation learning. Further discussions on limitations societal implications and future directions would enrich the impact of the research.", "pkzwYftNcqY": " PeerReview of the Paper 1. Summary: The paper introduces computationally efficient nonparametric tests for twosample independence and goodnessoffit problems using Maximum Mean Discrepancy (MMD) Hilbert Schmidt Independence Criterion (HSIC) and Kernel Stein Discrepancy (KSD) with incomplete Ustatistics. Aggregated tests are proposed enhancing test power without significant computational cost. The tests exhibit minimax optimality for Sobolev balls. 2. Strengths:  Novel Contribution: The proposed tests leverage incomplete Ustatistics for efficient hypothesis testing addressing the kernel selection problem.  Efficiency: The tests demonstrate computational efficiency without compromising test power compared to stateoftheart lineartime tests.  Theoretical Guarantees: The paper provides nonasymptotic uniform separation rates for the proposed tests and derivative quantile bounds.  Reproducibility: The authors offer code availability for reproducibility enhancing research transparency.  Thorough Background: Detailed background sections on testing problems related work and contributions provide context for the proposed tests. 3. Weaknesses:  Clarity: The paper could benefit from clearer delineation between theoretical derivations and practical implementation details.  Potential Overload: The paper contains extensive technical details that may overwhelm readers unfamiliar with kernel methods and Ustatistics.  Comparative Analysis: While the experiments illustrate the efficacy of the proposed tests a more comprehensive comparison with additional benchmarks could enhance the evaluation. 4. Questions and Suggestions for Authors:  Elaborate Comparison: Could the authors provide more insights on how the proposed tests compare with a broader range of existing tests beyond those presented in the experiments  Practical Implementation: How well do the proposed tests perform in realworld scenarios with noisy and highdimensional data compared to theoretical settings  Generalization: Are there any extensions or variations of the proposed tests for handling specific types of data distributions or modeling assumptions  Interaction with Established Methods: How does the proposed methodology complement or challenge the existing paradigm of kernelbased tests in nonparametric hypothesis testing 5. Limitations: The paper adequately addresses limitations by providing comprehensive theoretical backgrounds rigorous derivations and experimental validations. However ensuring a balance between theoretical detail and practical relevance is essential for wider adoption and impact. This review acknowledges the significant theoretical contributions of the paper while suggesting enhancements in clarity comparative analysis and practical implications to further strengthen the research findings.", "v6NNlubbSQ": " Peer Review for the Paper on Nonparametric Uncertainty Quantification  Summary: The paper proposes a novel method Nonparametric Uncertainty Quantification (NUQ) for uncertainty quantification in machine learning models. The key contributions of the paper are the principled approach to measuring uncertainty the disentanglement of aleatoric and epistemic uncertainties scalability and outperformance on various tasks like text and image classification. The method based on NadarayaWatson estimation shows robustness in both low and highresource settings with comprehensive empirical evaluations on datasets like MNIST SVHN CIFAR100 and ImageNet.  Strengths: 1. The paper provides a wellstructured and detailed description of the proposed NUQ method making it accessible to readers. 2. The methods performance is demonstrated through a thorough empirical evaluation on various datasets showcasing its superiority over existing approaches. 3. The discussion on disentangling aleatoric and epistemic uncertainties is insightful and addresses a key challenge in uncertainty estimation.  Weaknesses: 1. The paper could benefit from more indepth comparisons with a wider range of stateoftheart methods for uncertainty quantification especially in the context of different dataset characteristics. 2. The discussion on the scalability of the method could be further elaborated with practical considerations on computational resource requirements. 3. While the theoretical framework is sound more practical insights on the methods realworld applicability and performance in complex scenarios could enhance the papers impact.  Questions and Suggestions for Authors: 1. Can you provide more insights into the computational complexity of the proposed NUQ method especially when applied to large datasets like ImageNet How does it compare to existing scalable uncertainty quantification techniques 2. Have you considered the potential interpretability of the uncertainty scores generated by NUQ and their implications for decisionmaking in critical applications like medicine and autonomous driving 3. How does the proposed method generalize to multiclass classification problems and have you explored scenarios with imbalanced class distributions that are common in realworld datasets  Limitations: The authors have not explicitly addressed the potential negative societal impacts of their work. To enhance the papers societal implications and transparency it would be beneficial to include a brief discussion on how the proposed method could impact decisionmaking processes and possible ethical considerations especially in safetycritical domains.  Suggestions for Improvement: Consider discussing the societal implications of uncertainty quantification methods in safetycritical applications and recommend future research directions that focus on addressing ethical considerations and humancentric decisionmaking processes in machine learning models. Overall the paper presents a novel and promising approach to uncertainty quantification but could benefit from addressing certain weaknesses and expanding the discussion on scalability and societal impact. The empirical results and theoretical foundations are strong indicating the methods potential significance in advancing uncertainty quantification in machine learning models.", "w6tBOjPCrIO": " Summary: The paper introduces a Modelbased Counterfactual Data Augmentation (MOCODA) framework to address challenges in reinforcement learning (RL) when dealing with insufficient empirical data coverage in complex multiobject environments. MOCODA leverages a locally factored dynamics model to generate counterfactual transitions for RL agents enabling generalization to unseen states and actions. The proposed method demonstrates zeroshot generalization and is applied successfully in challenging robotics manipulation tasks where standard offline RL algorithms fail.  Strengths: 1. Innovative Approach: Introducing MOCODA to enhance RL generalization capabilities in complex multiobject domains is a significant contribution. 2. Theoretical Foundation: The paper provides a solid theoretical basis regarding locally factored dynamics models and their ability to generalize outofdistribution. 3. Experimental Validation: The experiments conducted on both the 2D Navigation and HookSweep2 domains provide clear evidence supporting the effectiveness of MOCODA in enabling RL agents to learn policies for outofdistribution tasks.  Weaknesses: 1. Complexity: The paper introduces several concepts and frameworks which might be challenging to grasp for readers not wellversed in the subject matter. 2. Assumption Dependency: The success of MOCODA relies heavily on the assumption of locally factored transition models which may not hold uniformly across all scenarios. 3. Implementation Challenges: The practical implementation of MOCODA especially in more complex environments may require significant expertise and computational resources.  Questions and Suggestions for Authors: 1. How does the proposed MOCODA framework perform in scenarios where the assumption of locally factored transition models is not strictly met 2. Could the paper elaborate on the computational overhead and runtime requirements of implementing MOCODA in realworld applications 3. Can the authors discuss the robustness of MOCODA to noise or inaccuracies in the learned dynamics models and parent distributions 4. Could the authors provide more insights on how MOCODA could be extended to handle online RL settings where exploration is possible  Limitations: The paper could further address:  Potential biases or ethical considerations arising from the use of MOCODA in decisionmaking applications.  The societal impact of improving RL generalization especially in domains where RL agents interact with human users or sensitive data.  Suggestions for Improvement: 1. Consider providing more detailed explanations and examples to facilitate understanding especially for readers less familiar with the domain. 2. Include discussions on the realworld applicability and scalability of MOCODA beyond the experimental domains explored. 3. Enhance the discussion on potential societal implications and ethical considerations of using MOCODA in practical RL applications. Overall the paper presents a novel approach with promising results and further clarifications and enhancements could strengthen its impact and applicability in diverse domains.", "o8nYuR8ekFm": " Summary: The paper introduces a novel compression approach based on quantizing parameters in a linear subspace to improve generalization bounds for deep neural networks. By training in a random linear subspace and performing learned quantization the authors achieve low compressed sizes for neural networks leading to stateoftheart generalization bounds for various tasks including transfer learning.  Strengths: 1. Innovative Approach: The paper presents a novel compression technique that significantly improves generalization bounds for deep learning models. 2. Theoretical Contributions: The work offers insights into the role of model size equivariance and implicit biases of optimization in deep learning generalization. 3. Rigorous Experiments: Rigorous experiments and comparisons are conducted on multiple datasets to support the proposed approach. 4. Comprehensive Analysis: The paper thoroughly explores datadependent and dataindependent generalization bounds as well as implications for transfer learning equivariance and stochastic training.  Weaknesses: 1. Technical Complexity: The paper presents complex mathematical concepts and methodologies that may be challenging for readers to grasp. 2. Lack of Practical Implications: While the theoretical contributions are robust the practical implications for realworld applications could be further elaborated. 3. Limitations Not Clearly Addressed: The authors should more explicitly discuss the limitations of their approach and how they impact the broader applicability of the proposed method.  Questions and Suggestions for Authors: 1. Clarification on Model Compression: Can the authors provide more insights into the practical implications of their compression approach for realworld applications 2. Explanation of DataDependent Bounds: How do datadependent priors contribute to the overall understanding of generalization and what are the potential limitations of relying solely on datadependent bounds 3. Comparison with Existing Methods: How does the proposed compression approach compare with existing model compression techniques in terms of computational efficiency and performance  Limitations: The authors have not extensively addressed the practical implications and potential negative societal impacts of their work. It would be beneficial to include a section discussing these aspects to provide a more holistic view of the research.  Suggestions for Improvement: 1. Include a section explicitly addressing the practical implications of the proposed compression approach for industry applications. 2. Discuss potential societal impacts of the research and provide recommendations on how to mitigate any negative consequences. 3. Enhance the clarity of the paper by simplifying complex mathematical explanations to make the content more accessible to a wider audience.", "wuunqp9KVw": " Summary: The paper introduces a novel endtoend probabilistic method for pluralistic image completion aiming to generate visually realistic and diverse results by employing a unified probabilistic graph model and Gaussian Mixture Models (GMM). The method divides the image completion process into subprocedures and introduces taskrelated constraints for visual reality and diversity. Empirical results demonstrate the superior performance of the proposed method compared to stateoftheart approaches.  Strengths: 1. Innovative Approach: The use of a probabilistic graph model and GMM for pluralistic image completion is novel and offers a structured and interpretable way to handle constraints for both visual reality and diversity. 2. Interpretability: The method offers enhanced interpretability due to the explicit use of GMM primitives for result diversity providing insights into the reasoning behind diverse completion results. 3. Comprehensive Experiments: The paper includes thorough experiments on benchmark datasets providing both qualitative and quantitative comparisons with stateoftheart methods to validate the effectiveness of the proposed method.  Weaknesses: 1. Complexity: The methods technical complexity and mathematical formulation might pose challenges for readers without a strong background in probabilistic modeling and image completion. 2. Limited Abstraction: The paper may benefit from including a more abstract overview at the beginning to help readers grasp the key concepts before delving into the detailed technical aspects.  Questions and Suggestions for Authors: 1. Clarification on GMM Parameters: How do you determine the optimal number of primitives k for GMM to balance diversity and computational complexity in practice 2. Scalability Concerns: How does the proposed method scale with larger image sizes or more complex datasets and what computational constraints might arise in such scenarios  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work and no specific concerns have been identified.  Constructive Suggestions for Improvement: 1. Simplification: Consider providing intuitive insights or simplified explanations alongside the technical details to make the paper more accessible to a wider audience. 2. Visualization Enhancement: Include more visual aids or examples to demonstrate the effectiveness of the proposed method particularly in terms of result diversity and interpretability. Overall the paper presents a promising approach to pluralistic image completion and contributes significantly to the field. Streamlining technical details and enhancing clarity could further strengthen the impact and accessibility of the work.", "u6OfmaGIya1": " Summary: The paper presents a novel learning paradigm called SECOND THOUGHTS aiming to realign language models with human values. It proposes a method that models human edits using insert delete and replace operations along with reinforcement learning finetuning. Extensive experiments on benchmark datasets demonstrate superior performance in value alignment and coherence compared to existing methods like MLE finetuning Data Filtering Safe Beam Search PPLM and InstructGPT.  Strengths: 1. Innovative Approach: The paper introduces a unique approach by leveraging text edits to model human values offering a novel perspective in value alignment for language models. 2. Extensive Experiments: The paper conducts thorough experiments on multiple benchmark datasets showcasing the effectiveness of SECOND THOUGHTS in value alignment and coherence. 3. Human Evaluation: The inclusion of human evaluations adds credibility to the methodology and results validating the alignment performance based on human judgments. 4. Value Transfer Learning: The study on the transferability of alignment across different datasets demonstrates the generalization ability of SECOND THOUGHTS.  Weaknesses: 1. Complexity: The algorithms complexity involving dynamic programming reinforcement learning and editing operations may hinder its practical applicability and scalability. 2. Limited Scalability: The paper discusses the limitations of LM sequence length which could impact the scalability of the proposed method for realworld applications. 3. Selective Comparison: While comparisons with existing methods are made a more comprehensive comparison with a wider range of approaches could provide a deeper understanding of SECOND THOUGHTS strengths and limitations.  Questions and Suggestions for Authors: 1. Clarification: Can the authors provide more insights on the practical implications and feasibility of deploying SECOND THOUGHTS in realworld scenarios 2. Scalability: How does the method handle the scalability challenges especially when dealing with larger datasets or different types of text inputs 3. Bias Mitigation: How does SECOND THOUGHTS address potential biases in human value judgement that may influence the models alignment decisions  Limitations: The authors should ensure the proposed method can address biases in human value judgments to mitigate potential negative societal impacts. Furthermore incorporating mechanisms to adapt to evolving human values and ensuring generalizability across diverse user backgrounds would enhance the societal impact of their work.", "yoBaCtx_a3": " Summary: The paper presents an algorithm for learning switched linear dynamical systems from noisy observations. The algorithm reformulates the problem to improve complexity demonstrating practical benefits over existing methods. The approach is evaluated on microbenchmarks handwriting recognition acrobot and cartpole datasets.  Strengths:  Novel Problem Formulation: The paper introduces a novel approach for identifying switched linear systems with improved complexity compared to existing methods showcasing strong theoretical insights.  Theoretical Guarantees: The paper establishes theoretical guarantees through Lemmas and Theorems providing a sound foundation for the proposed algorithm.  Experimental Evaluation: The comprehensive experimental evaluation on synthetic and realworld datasets demonstrates the effectiveness of the proposed approach.  RealWorld Applications: The application on handwritten alphabet recognition and mechanical systems highlights the practical relevance of the algorithm.  Weaknesses:  Technical Complexity: The algorithms complexity and technical details might be challenging for a broader audience to grasp easily potentially limiting its accessibility.  Lack of Comparison Metrics: While the approach is compared against a MILP solver more metrics for comparison with established methods could enhance the evaluations comprehensiveness.  Limited Discussion on Errors: The paper discusses prediction errors but could delve deeper into the significance of these errors in practical applications.  Questions and Suggestions: 1. Scalability: How does the algorithm scale in terms of computational resources when applied to larger datasets with higher dimensions and more modes 2. Generalizability: Have you evaluated the algorithms performance with varying levels of noise in the input data to assess robustness 3. Algorithms Robustness: How does the algorithm handle outliers and noise in realworld data sets particularly in applications like handwriting recognition and acrobot modeling 4. Interpretability: How interpretable are the identified modes for realworld applications How could you enhance the interpretability of the learned models  Limitations: The paper adequately addresses the limitations highlighting the challenges in handling large SARX models addressing backtracking for diverse solutions and the need for leveraging additional information for more efficient tree exploration.  Suggestions for Improvement: 1. Provide a clearer explanation of the algorithms intuition and implementation details for better understanding by a wider audience. 2. Discuss further the implications of prediction errors on the practical utility of the learned models. 3. Consider extending the analysis to include additional benchmark datasets or comparing against a wider range of existing algorithms for a more comprehensive evaluation. The paper presents a promising algorithm with strong theoretical underpinnings and practical applications but could benefit from improvements in clarity broader comparison metrics and robustness validations for realworld scenarios.", "nSe94hrIWhb": " Peer Review: 1. Summary: The paper introduces two novel algorithms CoralTDA and PrunIT to address the computational cost limitations of topological data analysis (TDA) on large complex networks. CoralTDA leverages the (k1)core of a graph to compute k persistence diagrams achieving significant reductions in graph order. PrunIT introduces a pruning algorithm to remove dominated vertices without altering persistence diagrams. Experimental results demonstrate substantial computational gains up to 95 enabling the application of TDA on large networks. 2. Strengths:  Novel Contribution: The paper presents two innovative algorithms CoralTDA and PrunIT addressing the high computational costs of TDA on large network graphs with impressive computational gains.  Effectiveness: Experimental results demonstrate substantial reduction in graph sizes and computational times validating the effectiveness of the proposed algorithms.  Theoretical Foundation: The paper provides a solid theoretical basis for the algorithms demonstrating their efficiency in reduing graph sizes without compromising the persistence diagrams. 3. Weaknesses:  Clarity and Structure: The papers organization can be improved for better readability especially in explaining the algorithms and their theoretical basis.  Experimental Setup: More details on the experimental setup and comparisons with existing methods would enhance the papers empirical validation.  RealWorld Applicability: While the algorithms show promising results in reducing computational costs further validation on diverse and realworld datasets could strengthen the practical utility of the proposed methods. 4. Questions and Suggestions:  Could the authors elaborate more on the choice of the experimental datasets and provide insights into the generalizability of the proposed algorithms to diverse realworld applications  How do the computational gains achieved by CoralTDA and PrunIT compare to existing TDA and graph reduction approaches if any Providing a comparative analysis could enhance the evaluation of the proposed methods.  It might be beneficial to include discussions on potential challenges or limitations in implementing the algorithms in practical applications and suggestions for overcoming them. 5. Limitations: The authors have generally addressed the limitations of their work through experiments and theoretical proof. To enhance societal impact consideration they could include a discussion on the potential implications and ethical considerations of applying TDA on large datasets particularly in sensitive domains. Overall the paper presents innovative algorithms that overcome the computational barriers in applying TDA to large complex networks. With minor improvements in clarity and additional comparative analyses the paper could further strengthen its contributions and practical implications.", "pV7f1Rq71I5": " Peer Review  Summary: The paper presents a new constant memory scheme for estimating the entropy of a distribution with improved sample complexity compared to previous work. The authors introduce a modification to an existing estimator to reduce the sample complexity dependence on 1epsilon from cubic to quadratic. They propose an \"intervalbased\" algorithm that further enhances the sample complexity achieving a significant improvement over prior memoryefficient bounds. The theoretical analysis and algorithm design are detailed showing a promising advancement in the field of streaming algorithms for statistical inference problems.  Strengths:  Theoretical Contributions: The paper makes substantial contributions by significantly improving the sample complexity of entropy estimation in lowmemory streaming algorithms showcasing a novel approach to bias correction and intervalbased estimation.  Algorithm Design: The proposed algorithm is welldescribed with a clear outline of the methodology and computational steps involved. The use of negative binomial random variables and interval partitioning demonstrates a sophisticated technique to handle the entropy estimation problem efficiently.  Theoretical Analysis: The paper provides a rigorous theoretical analysis including the derivation of bias correction calculation of sample complexities and the proof of error bounds demonstrating a deep understanding of the subject matter.  Detailed Explanations: The authors provide detailed explanations of the algorithms theorems and Lemmas making it accessible to readers not deeply familiar with the topic.  Weaknesses:  Complexity: The complexity of the proposed algorithm and the analyses provided might be challenging for readers without a strong background in streaming algorithms or statistical inference.  Practical Implementation: While the theoretical contributions are substantial the practical implications of the proposed scheme and its realworld applications could be further explored or discussed.  Comparison: A detailed comparison with existing stateoftheart methods particularly in terms of practical performance metrics or benchmarks would have provided a more comprehensive evaluation of the proposed algorithms effectiveness.  Questions and Suggestions for the Authors: 1. Can you elaborate on the practical implications of your work How can the proposed algorithm be applied to realworld data analysis scenarios 2. Have you considered conducting experimental evaluations to validate the performance of your algorithm in comparison to existing methods especially in terms of computational efficiency and accuracy 3. Could you discuss the potential scalability of your algorithm to larger datasets or more complex distribution scenarios How does the sample complexity scale in such cases  Limitations: The paper does not thoroughly address the potential negative societal impact of the research. To improve the ethical considerations of the work the authors should consider discussing the implications of using lowmemory streaming algorithms in the context of privacy data security or fairness.  Suggestions for Improvement:  Include a brief discussion on the ethical implications of using lowmemory streaming algorithms addressing privacy concerns and ensuring fairness in algorithmic decisionmaking.  Consider mentioning possible future research directions or extensions of the proposed algorithm for practical implementation in various domains.", "swIARHfCaUB": " Review Summary: The paper introduces a delayed variant of the online FrankWolfe (OFW) method for online convex optimization to handle arbitrary delays in receiving gradients. The proposed Delayed OFW is shown to achieve impressive regret bounds for both convex and strongly convex losses. The theoretical analysis is rigorous and supported by simulation experiments to validate the performance of the proposed algorithms under different delay scenarios.  Strengths: 1. Innovative Contribution: The delayed variant of OFW addresses a practical limitation in online learning settings by allowing for arbitrary delays in receiving gradients. 2. Theoretical Rigor: The paper provides thorough theoretical analysis establishing regret bounds for convex and strongly convex losses demonstrating the effectiveness of the proposed Delayed OFW. 3. Simulation Experiments: The inclusion of simulation experiments validates the theoretical findings and demonstrates the performance of the proposed algorithms under varying delay conditions.  Weaknesses: 1. Clarity of Presentation: The papers technical nature and formalism may pose challenges to readers unfamiliar with the specific concepts or algorithms. Clarifying certain complex notations and assumptions could enhance readability. 2. Comparative Analysis: While the paper presents the proposed Delayed OFW and compares it with existing methods a more indepth analysis and discussion of the specific strengths and weaknesses of each algorithm could provide more insights.  Questions and Suggestions: 1. Is there any specific scenario or application domain where Delayed OFW may provide significant advantages over existing methods 2. How sensitive is the performance of Delayed OFW to variations in parameter settings such as the learning rate or delay thresholds 3. What computational complexities are associated with implementing the Delayed OFW compared to standard OFW especially in scenarios with high delays  Limitations: The paper effectively addresses limitations and potential negative societal impacts by focusing on algorithmic improvements in online learning settings. To strengthen societal impact discussions authors could consider exploring practical implications on realworld applications or addressing ethical considerations related to data privacy or algorithm fairness.  Suggestions for Improvement: Authors could enhance the accessibility of the paper by providing more intuitive explanations or visual aids to help readers grasp complex algorithms and theoretical proofs. Additionally exploring practical case studies or experiments in realworld scenarios could showcase the applicability and benefits of the Delayed OFW method. Overall the paper shows promise by introducing a novel algorithmic solution for online convex optimization under delays backed by a strong theoretical foundation and simulation validation. Further enhancements in presentation and practical demonstrations could increase the papers impact and appeal to a broader audience.", "v9Wjc2OWjz": " Summary The paper addresses the problem of estimating a rank1 signal corrupted by structured rotationally invariant noise when the noise statistics are unknown and Gaussian noise is assumed. It makes contributions towards understanding the impact of this mismatch on inference algorithms focusing on a rigorous analysis of a Bayes estimator and an Approximate Message Passing (AMP) algorithm both assuming a Gaussian setup. Theoretical results and numerical simulations uncover surprising behaviors such as the Gaussian AMP being outperformed by the Bayes estimator in terms of meansquare error in certain scenarios.  Strengths 1. Novelty and Contribution: The paper addresses an important problem in signal processing with substantial theoretical and practical implications. 2. Rigorous Analysis: Theoretical analyses of the Bayes estimator and AMP algorithm provide insightful characterizations of their performances under mismatched scenarios. 3. Surprising Phenomenology: The revelation of unexpected outcomes in numerical simulations adds depth to the study and highlights intriguing aspects of algorithmic performance. 4. Clear Exposition: The detailed description of theoretical frameworks assumptions models of mismatch and numerical results facilitates understanding and interpretation.  Weaknesses 1. Clarity of Discussion: The intricate technical details and analytical formulations may be challenging for readers without a strong background in signal processing and statistical inference. 2. Limited Empirical Validation: While the numerical simulations suggest interesting findings further evaluation with realworld data or practical applications could enhance the applicability of the study.  Questions and Suggestions 1. Clarification of Assumptions: Could the authors elaborate on the practical implications of the assumption regarding the signal norm being \\xe2\\x88\\x9aN How does this assumption affect the generalizability of the results 2. Algorithmic Complexity: Given the surprising findings on the AMP algorithms performance under mismatched scenarios are there considerations regarding the computational complexity or scalability of the proposed methods 3. Generalization: How transferable are the conclusions drawn in this study to realworld scenarios with diverse noise structures and signal characteristics beyond the presented models of mismatch  Limitations The authors have adequately discussed the limitations and societal impacts of their work by addressing the implications of mismatched noise statistics assumptions in inference algorithms. Constructive suggestions for improvement involve expanding empirical validation efforts and exploring the implications of their findings in broader practical contexts.", "pnSyqRXx73": " Brief Summary: The paper investigates the stochastic gradient descent (SGD) algorithm driven by different random sequences including i.i.d noise and random walk on a graph. It analyzes the algorithm asymptotically using the notion of efficiency ordering comparing the performance of Markov Chain Monte Carlo (MCMC) samplers. The study shows that input sequences more efficient for MCMC sampling lead to smaller errors for SGD algorithms in the limit implying reduced mean square error (MSE) for more efficient chains. The findings are particularly relevant for applications like decentralized optimization and swarm learning where SGD is used in a random walk fashion. The paper presents a unified framework to compare gradient descent methods under different input sequences and extends the analysis to popular SGD variants such as accelerated SGD and Adam.  Strengths: 1. Innovative Contribution: The paper introduces a novel concept of efficiency ordering for SGD algorithms based on asymptotic analysis providing new insights into the performance of different random sequences driving the algorithm. 2. Theoretical Depth: The study presents a rigorous analysis connecting efficiency ordering from MCMC literature with covariance matrices in SGD algorithms highlighting the secondorder properties of Markov chains and their impact on optimization. 3. Methodological Rigor: The use of theoretical proofs propositions lemmas and numerical simulations adds credibility to the research demonstrating a comprehensive approach to investigating the problem.  Weaknesses: 1. Complexity: The technical depth of the paper might make it challenging for readers without a strong background in optimization theory and stochastic processes to grasp the content fully. 2. Generalization: While the results are insightful the application to realworld scenarios could be further elaborated to enhance the practical relevance of the findings. 3. Limited Empirical Validation: While the numerical experiments support the theoretical claims additional realworld datasets or scenarios could strengthen the practical implications of the study.  Questions and Suggestions for Authors: 1. Clarity on Assumptions: Further clarity on the assumptions made (A1)\u2013(A5) could help readers better understand the constraints under which the efficiency ordering holds. 2. Impact of NonMarkovian Sequences: How do the findings extend to more complex nonMarkovian processes and what implications do they have on the efficiency ordering concept 3. Application Scenarios: Can the authors provide specific usecases or scenarios where the efficiency ordering of SGD algorithms can lead to tangible improvements in performance or convergence rates 4. Comparison to Existing Literature: How does the proposed efficiency ordering metric compare and complement existing metrics used to evaluate optimization algorithms such as convergence rates or performance bounds  Limitations: The authors have adequately addressed the limitations and potential negative societal impact of their work however suggesting applications in diverse realworld scenarios and providing further empirical validation could enhance the practical relevance of the findings.", "yoLGaLPEPo_": " Peer Review  Summary The paper proposes a novel hyperbolic feature augmentation (HFA) method to combat overfitting in the hyperbolic space by generating discriminative features. It introduces a metaneuralODE distribution estimation scheme and derives an upper bound of the augmentation loss. The method significantly improves hyperbolic algorithms performance in scarce data regimes as demonstrated in fewshot learning and continual learning tasks.  Strengths 1. Original Contribution: The paper introduces a novel method HFA for augmenting features in the hyperbolic space contributing to the field of hyperbolic learning. 2. Innovative Approach: The use of metaneuralODE for distribution estimation and the derivation of an upper bound of augmentation loss are innovative approaches that effectively combat overfitting. 3. Comprehensive Experiments: The paper conducts thorough experiments on fewshot learning and continual learning tasks comparing the proposed method with existing approaches and providing detailed results and visualizations. 4. Theoretical Insights: The theoretical derivations and explanations of the methods mechanisms such as using wrapped normal distributions and gradient flow networks are wellelaborated.  Weaknesses 1. Complexity: The method involving neural ODEs and distribution estimation might be challenging for readers unfamiliar with these concepts potentially limiting its accessibility. 2. Evaluation: While the experimental results are comprehensive more detailed comparisons with stateoftheart methods and sensitivity analysis on parameters could further strengthen the evaluation.  Questions and Suggestions for Authors 1. Can you provide insights into the scalability of the proposed method in terms of computational resources and time complexity 2. How does the choice of hyperparameters affect the performance of the method Can you provide guidance on parameter selection 3. Could the authors provide a comparison with other hyperbolic augmentation methods if available to showcase the superiority of the proposed HFA method  Limitations The authors have adequately addressed the limitations of their work particularly in terms of making assumptions about data hierarchy structures. To further improve authors may consider exploring data with nonuniform hierarchical structures and multiple hyperbolic spaces. Overall the paper presents a significant contribution to the hyperbolic learning domain with wellconducted experiments and theoretical foundations. Addressing the weaknesses and questions raised can enhance the clarity and impact of the work.  This peer review provides an analysis of the strengths and weaknesses of the paper along with questions and constructive suggestions for the authors. Please consider incorporating this feedback when revising the paper.", "zvNMzjOizmn": "PeerReview: 1. Summary: The paper introduces the Amortized Langevin Dynamics (ALD) and the Langevin Autoencoder (LAE) as novel approaches to efficiently sample from posterior distributions in deep latent variable models. ALD replaces traditional datapointwise MCMC iterations with updates of an encoder. The authors theoretically prove the validity of ALD as an MCMC algorithm and demonstrate its effectiveness in sampling from target distributions on synthetic datasets. They also present results showing LAEs superiority over variational autoencoders and other methods based on variational inference in image generation tasks. 2. Strengths:  The paper addresses a significant challenge in deep latent variable models and proposes a novel solution that can potentially revolutionize the field.  The theoretical foundations of ALD are robust represented by the theoretical proof of its validity as an MCMC algorithm.  The experimental section thoroughly evaluates the proposed methods on various datasets demonstrating their effectiveness compared to existing approaches.  The paper is wellorganized with a clear introduction methodology results and related works sections. 3. Weaknesses:  The paper lacks clarity in explaining some technical terms making it challenging for readers not wellversed in the field to understand the proposed methods fully.  While the theoretical basis of ALD is wellargued more indepth discussions on potential limitations and challenges in practical implementation could enhance the papers comprehensiveness. Questions and Suggestions: 1. Can the authors elaborate on the computational complexity of ALD compared to traditional MCMC approaches especially considering different dataset sizes and model complexities 2. How does the proposed LAE scale with larger and more complex datasets beyond the ones used in the experiments Have there been any scalability tests conducted 3. Given the theoretical constraints on the structure of the encoder in Theorem 1 are there practical considerations when applying ALD to realworld DLVMs with hierarchical latent structures 4. Could the authors discuss potential limitations and challenges in using ALD for scenarios with highdimensional data such as healthcare applications or natural language processing Limitations: While the paper acknowledges some limitations in the practical application of the proposed methods further exploration of scaling challenges and realworld applicability would provide a more comprehensive view. Suggestions for Addressing Limitations:  Conduct further investigations on algorithm scalability to larger and more diverse datasets.  Consider realworld scenarios and domainspecific challenges to refine the proposed methods for broader applicability.  Provide detailed guidelines or tools for practitioners to effectively implement ALD and LAE in various DLVM applications. Overall the study presents innovative contributions to the field of deep latent variable models and MCMC algorithms. Addressing the highlighted points would enhance the papers impact and practical relevance.", "yCJVkELVT9d": " Summary: The paper critically evaluates the robustness of current adversarial defenses for Graph Neural Networks (GNNs) by introducing adaptive attacks highlighting that existing defenses are overestimated due to being evaluated against nonadaptive attacks. The study systematically assesses 7 popular defenses using adaptive attacks revealing that most defenses do not significantly outperform an undefended baseline. The paper emphasizes the necessity of custom adaptive attacks for accurate evaluation and introduces a robustness unit test with perturbed graphs to assess the defenses.  Strengths: 1. Methodological Rigor: The papers approach of introducing adaptive attacks and addressing the lack of robustness estimates in GNN defenses is commendable. 2. Thorough Analysis: The study evaluates a diverse set of defenses systematically providing detailed insights on their performance against adaptive attacks. 3. Transparent Methodology: The paper clearly outlines the procedure for designing strong adaptive attacks making the process replicable. 4. Novel Contribution: The introduction of a robustness unit test using perturbed graphs offers a practical method for assessing the reliability of defenses.  Weaknesses: 1. Dataset Limitation: The study focuses on popular datasets potentially limiting the generalizability of the findings. Exploring the effectiveness of defenses on diverse datasets could enhance the study. 2. Evaluation Metrics: While the AUC metric is effective incorporating other evaluation metrics such as precision recall or F1 score could provide a comprehensive assessment of defense robustness. 3. Sensitivity to Hyperparameters: The study reports tuning hyperparameters of defenses indicating sensitivity. Addressing this issue by proposing strategies for more stable defense architectures would be beneficial.  Questions and Suggestions for Authors: 1. Generalizability: Have you considered testing the defenses on a wider range of datasets to assess their robustness across various graph structures 2. Evaluation Metrics: Could additional metrics beyond AUC enhance the evaluation process providing a detailed understanding of defense performance 3. Sensitivity Analysis: How can the sensitivity to hyperparameters in defenses be mitigated to ensure consistent robustness across different scenarios  Limitations: The papers dataset focus and reliance on AUC as the primary evaluation metric suggest a potential need for broader dataset exploration and additional evaluation measures to ensure a comprehensive assessment of defense robustness.  Suggestions for Improvement: 1. Introduce sensitivity analysis for key hyperparameters in defenses to enhance stability and generalizability. 2. Consider benchmarking against a broader range of datasets to evaluate defense effectiveness across diverse graph structures. 3. Explore alternative evaluation metrics to complement AUC and provide a more comprehensive understanding of defense performance.", "kcQiIrvA_nz": "Peer Review: Summary: The paper introduces a novel approach for protecting the copyrights of opensourced datasets through untargeted backdoor watermarking. Existing dataset ownership verification methods using poisononly backdoor watermarks are identified to introduce new security risks due to their targeted nature. To address this the authors propose an untargeted backdoor watermarking scheme and demonstrate its effectiveness in protecting against existing backdoor defenses. Extensive experiments using benchmark datasets validate the proposed method. Strengths: 1. Originality: The paper tackles an important issue in the realm of protecting dataset copyrights with a new perspective on untargeted backdoor watermarking. 2. Clarity: The authors present a structured and clear explanation of their proposed method including the problem formulation background methodology and experimental results. 3. Experimental Rigor: The authors conducted comprehensive experiments using benchmark datasets to validate the effectiveness and resistance of their method to existing backdoor defenses. 4. Indepth Analysis: The thorough technical discussions on dispersibilities bilevel optimization and dataset ownership verification add depth and clarity to the proposed approach. Weaknesses: 1. Lack of Comparison: The paper lacks detailed comparison with other stateoftheart methods for dataset protection aside from backdoor attacks which may limit the context for evaluating the novelty and effectiveness of the proposed approach. 2. Limitations of Experiments: While the experiments conducted are thorough a more detailed analysis of the computational complexity scalability and realworld application scenarios could provide a more holistic evaluation of the proposed method. 3. Assumptions Clarifications: Certain assumptions made in the formulation of the problem and methodology should be explicitly stated and discussed to enhance the clarity and validity of the proposed approach. Questions and Suggestions for Authors: 1. Have you considered the implications of potential false positives or false negatives in the dataset ownership verification process using your proposed untargeted backdoor watermarking scheme 2. How does the proposed method perform in scenarios with highdimensional and complex datasets such as those in natural language processing or medical imaging where the potential impact of watermarking could be more significant 3. Can you provide insights on the tradeoffs between the level of watermark effectiveness stealthiness and dispersibility in the context of protecting opensourced datasets Limitations and Societal Impact: The authors have adequately addressed the limitations of their work highlighting the potential negative societal impact from misuse by backdoor adversaries. Enhancing the discussion on mitigation strategies against such misuse and ensuring transparent communication with relevant stakeholders could further strengthen the societal impact of the research. Conclusion: In conclusion the paper presents a novel approach to dataset ownership verification using untargeted backdoor watermarking demonstrating effectiveness and resistance to existing defenses. Addressing the mentioned weaknesses and considering the provided questions could enhance the papers clarity applicability and overall impact in the field of dataset protection. Note: This peer review provides constructive feedback and suggestions aimed at improving the quality and impact of the research paper.", "uloenYmLCAo": " Summary: The paper introduces the BlockRecurrent Transformer a novel architecture that combines the benefits of attention and recurrence. By processing sequences in blocks and utilizing a recurrent cell the model achieves improved language modeling perplexity over very long sequences compared to conventional transformers. The implementation demonstrates superior performance on PG19 (books) arXiv papers and GitHub source code datasets over a Transformer XL baseline while being computationally efficient.  Strengths:  Innovative Architecture: The BlockRecurrent Transformer introduces a unique approach by combining attention and recurrence demonstrating significant improvement in perplexity over long sequences.  Efficient Implementation: The model exhibits linear complexity with respect to sequence length making it computationally efficient while offering enhanced performance.  Empirical Demonstrations: The paper provides thorough experimental validations on multiple datasets showcasing the models effectiveness in various tasks.  Weaknesses:  Lack of Detailed Comparison: While the paper compares the BlockRecurrent Transformer to baselines and showcases its benefits a more detailed comparison with related works could enhance the papers impact and analysis.  Complexity vs. Interpretability: The paper mentions that complex reasoning might not be evident in the models behavior highlighting a potential tradeoff between complexity and model interpretability.  Questions and Suggestions for Authors: 1. Can you provide more insights into how the model handles longrange dependencies and captures meaningful context across different domains 2. How does the efficiency and performance of the BlockRecurrent Transformer compare to other stateoftheart architectures especially those combining attention and recurrence 3. Have you explored the potential implications of incorporating recurrence in transformer architectures for tasks beyond language modeling  Limitations: The paper adequately addresses the limitations and potential societal impact of the work. One constructive suggestion for improvement could be to further discuss model interpretability and ethical considerations related to deploying advanced language models in realworld applications. Overall the paper presents a compelling approach that could significantly advance natural language processing tasks on long sequences. Additional context and comparisons could strengthen the papers contributions further."}